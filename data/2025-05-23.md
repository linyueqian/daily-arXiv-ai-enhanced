<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 220]
- [cs.CV](#cs.CV) [Total: 218]
- [cs.AI](#cs.AI) [Total: 77]
- [cs.SD](#cs.SD) [Total: 14]
- [cs.LG](#cs.LG) [Total: 210]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 18]
- [eess.IV](#eess.IV) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/pdf/2505.16988)
*Rui Ye, Keduan Huang, Qimin Wu, Yuzhu Cai, Tian Jin, Xianghe Pang, Xiangrui Liu, Jiaqi Su, Chen Qian, Bohan Tang, Kaiqu Liang, Jiaao Chen, Yue Hu, Zhenfei Yin, Rongye Shi, Bo An, Yang Gao, Wenjun Wu, Lei Bai, Siheng Chen*

Main category: cs.CL

TL;DR: MASLab is a unified codebase for LLM-based multi-agent systems (MAS) to address redundancy, unfair comparisons, and high entry barriers by integrating 20+ methods, providing benchmarks, and streamlining implementation.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified codebase for LLM-based MAS leads to redundant efforts, unfair comparisons, and high entry barriers for researchers.

Method: MASLab integrates 20+ validated methods, offers a unified environment with benchmarks, and implements methods in a shared structure.

Result: Extensive experiments on 10+ benchmarks and 8 models provide a clear view of MAS methods.

Conclusion: MASLab aims to evolve with the field and invites open-source contributions to advance LLM-based MAS research.

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [2] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/pdf/2505.15916)
*Juvenal Domingos Júnior, Augusto Faria, E. Seiti de Oliveira, Erick de Brito, Matheus Teotonio, Andre Assumpção, Diedre Carmo, Roberto Lotufo, Jayr Pereira*

Main category: cs.CL

TL;DR: BR-TaxQA-R is a new dataset for QA in Brazilian tax law, using RAG with OpenAI embeddings and GPT-4o-mini. It outperforms commercial tools in relevancy but lags in correctness and fluency. Human expert validation is emphasized.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate, reference-based QA in Brazilian tax law, leveraging AI while ensuring legal validity.

Method: Implemented a RAG pipeline with OpenAI embeddings for retrieval and GPT-4o-mini for generation, comparing segmentation strategies and benchmarking against commercial tools using RAGAS metrics.

Result: Custom RAG pipeline excels in Response Relevancy but commercial models score higher in Factual Correctness and fluency.

Conclusion: Human expert evaluation is crucial for legal validity in AI-generated tax answers, despite trade-offs between grounding and fluency.

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [3] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/pdf/2505.15918)
*Aliakbar Nafar, Kristen Brent Venable, Zijun Cui, Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: The paper explores using LLMs to derive probabilistic knowledge for Bayesian Networks, showing their effectiveness in parameterizing BNs and reducing biases when combined with minimal real-world data.


<details>
  <summary>Details</summary>
Motivation: To investigate the understudied capability of LLMs in generating probabilistic knowledge for real-world events and their potential to enhance Bayesian Network modeling.

Method: Querying LLMs for conditional probabilities of events in BNs across domains like healthcare and finance, comparing results to baselines like random distributions and next-token probabilities.

Result: LLM-derived distributions provide meaningful probabilistic estimates and can refine expert priors, reducing biases when combined with small datasets.

Conclusion: The study presents a promising approach for automatically constructing BNs using LLMs and minimal data, while establishing baselines for evaluating LLM performance in probabilistic knowledge extraction.

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [4] [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/pdf/2505.15922)
*Dong Won Lee, Hae Won Park, Cynthia Breazeal, Louis-Philippe Morency*

Main category: cs.CL

TL;DR: A framework uses a pretrained LLM to decompose session-level feedback into fine-grained rewards for aligning dialogue agents, improving conversation quality without manual reward shaping.


<details>
  <summary>Details</summary>
Motivation: Aligning dialogue agents typically requires granular feedback, which is costly. This work aims to simplify alignment using only session-level feedback by leveraging LLMs for reward decomposition.

Method: Two variants: text-only (using dialogue transcripts) and multimodal (adding behavioral cues). The LLM decomposes global feedback into turn-level rewards, distilled into a lightweight model for RL fine-tuning.

Result: Outperforms state-of-the-art methods in human evaluations, showing LLMs can effectively decompose rewards without manual intervention.

Conclusion: LLMs are powerful reward decomposers, eliminating the need for manual reward shaping and detailed human feedback.

Abstract: We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.

</details>


### [5] [Citation Parsing and Analysis with Language Models](https://arxiv.org/pdf/2505.15948)
*Parth Sarin, Juan Pablo Alperin*

Main category: cs.CL

TL;DR: The paper proposes using open-weight language models to improve citation parsing for better global knowledge sharing, addressing inequalities in research indexing.


<details>
  <summary>Details</summary>
Motivation: To address global inequalities in knowledge production by improving citation network tracking, especially for the Global South, and countering colonial biases in research indexing.

Method: Assembled a dataset of plaintext and annotated citations, then evaluated open-weight language models (like Qwen3-0.6B) for citation markup accuracy.

Result: Open-weight models achieved high accuracy in citation parsing, with Qwen3-0.6B performing well in minimal passes, suggesting potential for small, robust models.

Conclusion: The tool could enhance citation network fidelity, improving research indexing, discovery, and metascientific research globally.

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [6] [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/pdf/2505.15960)
*Ryo Kamoi, Yusen Zhang, Nan Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang*

Main category: cs.CL

TL;DR: FoVer automates step-level error labeling for training Process Reward Models (PRMs) using formal verification tools, enabling generalization to diverse reasoning tasks without human annotation.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost of human annotation for step-level error labels and the limited scope of existing PRMs to math reasoning.

Method: Proposes FoVer, using formal verification tools (e.g., Z3, Isabelle) to automatically annotate step-level errors for training PRMs on formal logic and theorem proof tasks.

Result: PRMs trained with FoVer outperform baselines and match or exceed state-of-the-art PRMs trained on human-annotated data, showing cross-task generalization.

Conclusion: FoVer provides a scalable and accurate method for training PRMs, reducing reliance on human annotation and expanding applicability to diverse reasoning tasks.

Abstract: Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.

</details>


### [7] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/pdf/2505.15962)
*Linxi Zhao, Sofian Zalouk, Christian K. Belardi, Justin Lovelace, Jin Peng Zhou, Kilian Q. Weinberger, Yoav Artzi, Jennifer J. Sun*

Main category: cs.CL

TL;DR: LMLMs store factual knowledge in both internal weights and an external database, enabling editable and verifiable knowledge without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Current neural language models lack transparency and control over factual knowledge, making updates and verification difficult.

Method: LMLMs use pre-training to store knowledge in weights and an external database, masking retrieved facts to encourage targeted lookups.

Result: LMLMs match performance of larger models while offering editable and verifiable knowledge.

Conclusion: LMLMs introduce a new paradigm for managing factual knowledge in language models.

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [8] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/pdf/2505.16592)
*Shijia Zhou, Siyao Peng, Simon Luebke, Jörg Haßler, Mario Haim, Saif M. Mohammad, Barbara Plank*

Main category: cs.CL

TL;DR: The paper explores the interaction between stance and media framing in climate change memes, introduces the CLIMATEMEMES dataset, and evaluates AI models on stance and frame detection tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how stance and media framing interact in climate change memes, a largely unexplored area.

Method: An interdisciplinary approach using the CLIMATEMEMES dataset (1,184 memes) and evaluating AI models (LLaVA-NeXT, Molmo) on stance and frame detection.

Result: VLMs perform well on stance detection but struggle with frames, where LLMs outperform. Human captions enhance performance.

Conclusion: The study highlights AI limitations in nuanced frame and stance analysis, suggesting further research is needed.

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [9] [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/pdf/2505.15993)
*Anirudh Maiya, Razan Alghamdi, Maria Leonor Pacheco, Ashutosh Trivedi, Fabio Somenzi*

Main category: cs.CL

TL;DR: LLMs struggle to provide strategic explanations for Sudoku puzzles, highlighting challenges in human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to solve and explain Sudoku puzzles, a key task for human-AI collaboration.

Method: Evaluated five LLMs on solving and explaining Sudoku puzzles.

Result: Limited puzzle-solving success; none provided strategic or intuitive explanations.

Conclusion: Significant improvements needed for LLMs to be effective in collaborative decision-making.

Abstract: The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.

</details>


### [10] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/pdf/2505.16000)
*Mehrdad ghassabi, Pedram Rostami, Hamidreza Baradaran Kashani, Amirhossein Poursina, Zahra Kazemi, Milad Tavakoli*

Main category: cs.CL

TL;DR: A study enhances a small language model's medical knowledge for Persian by using crawled online data and fine-tuning, showing improved accuracy in medical QA.


<details>
  <summary>Details</summary>
Motivation: Address the lack of curated medical datasets in Persian and improve small language models' performance in specialized, low-resource domains.

Method: Curated a Persian medical corpus from online sources, fine-tuned a baseline model, and evaluated its performance.

Result: The fine-tuned model achieved higher accuracy in medical question answering and better responses than the baseline.

Conclusion: Leveraging open-access online data can enrich small language models in medical fields, offering a practical solution for Persian AI applications.

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [11] [Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://arxiv.org/pdf/2502.20073)
*Haochen Sun, Shuwen Zhang, Lujie Niu, Lei Ren, Hao Xu, Hao Fu, Fangkun Zhao, Caixia Yuan, Xiaojie Wang*

Main category: cs.CL

TL;DR: The paper introduces Collab-Overcooked, a new benchmark for LLM-powered Multi-Agent Systems (LLM-MAS), built on Overcooked-AI. It extends existing benchmarks with multi-agent collaboration tasks and process-oriented evaluation metrics, revealing gaps in LLM collaboration capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating fine-grained collaboration in LLM-MAS, the paper proposes Collab-Overcooked to assess LLM agents' abilities in interactive environments.

Method: The benchmark is built on Overcooked-AI, featuring 30 open-ended tasks and natural language communication. It introduces process-oriented metrics to evaluate collaboration.

Result: Experiments with 11 LLMs show strong goal interpretation but weaknesses in active collaboration and adaptation.

Conclusion: The paper highlights LLM-MAS strengths and weaknesses, offering insights for improvement and evaluation on a unified, open-sourced benchmark.

Abstract: Large language models (LLMs) based agent systems have made great strides in
real-world applications beyond traditional NLP tasks. This paper proposes a new
LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on
the popular Overcooked-AI game with more applicable and challenging tasks in
interactive environments. Collab-Overcooked extends existing benchmarks from
two novel perspectives. First, it provides a multi-agent framework supporting
diverse tasks and objectives and encourages collaboration through natural
language communication. Second, it introduces a spectrum of process-oriented
evaluation metrics to assess the fine-grained collaboration capabilities of
different LLM agents, a dimension often overlooked in prior work. We conduct
extensive experiments over 11 popular LLMs and show that, while the LLMs
present a strong ability in goal interpretation, there is a significant
discrepancy in active collaboration and continuous adaptation which are
critical for efficiently fulfilling complicated tasks. Notably, we highlight
the strengths and weaknesses in LLM-MAS and provide insights for improving and
evaluating LLM-MAS on a unified and open-sourced benchmark. The environments,
30 open-ended tasks, and the evaluation package are publicly available at
https://github.com/YusaeMeow/Collab-Overcooked.

</details>


### [12] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/pdf/2505.16002)
*Sasha Boguraev, Christopher Potts, Kyle Mahowald*

Main category: cs.CL

TL;DR: LLMs aid linguistic syntax theories; causal interpretability reveals abstract mechanisms. Focus on English filler-gap dependencies shows LLMs converge on similar analyses, uncovering overlooked factors for theory refinement.


<details>
  <summary>Details</summary>
Motivation: Enhance linguistic theory by using LLMs as evidence sources, applying causal interpretability to uncover abstract mechanisms.

Method: Distributed Interchange Interventions to analyze English filler-gap dependencies in LLMs.

Result: LLMs converge on similar abstract analyses, revealing overlooked factors (frequency, filler type, context) impacting linguistic theory.

Conclusion: Mechanistic analyses of LLMs can advance linguistic theory by refining understanding of syntax mechanisms.

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [13] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/pdf/2505.16003)
*Roland Daynauth, Christopher Clarke, Krisztian Flautner, Lingjia Tang, Jason Mars*

Main category: cs.CL

TL;DR: SLMEval, a new calibration method for LLM-as-a-Judge, improves correlation with human judgments in real-world tasks and reduces costs.


<details>
  <summary>Details</summary>
Motivation: Prior calibration methods for LLM evaluators perform poorly in open-ended tasks, lacking generalization to real-world scenarios.

Method: Proposes SLMEval, an entropy maximization-based calibration method using minimal human preference data to reweight evaluator scores.

Result: SLMEval achieves strong correlation (e.g., 0.57 Spearman) with human judgments and reduces costs by 5-30x compared to GPT-4-based evaluators.

Conclusion: SLMEval effectively addresses the limitations of prior methods, offering scalable and cost-efficient evaluation for real-world tasks.

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [14] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/pdf/2505.16008)
*Wenrui Yu, Yiyi Chen, Johannes Bjerva, Sokol Kosta, Qiongxiu Li*

Main category: cs.CL

TL;DR: LAGO is a graph-based method for few-shot cross-lingual embedding inversion attacks, leveraging language similarity to improve attack transferability.


<details>
  <summary>Details</summary>
Motivation: Address privacy vulnerabilities in multilingual NLP systems by modeling linguistic relationships, unlike prior independent language treatments.

Method: Uses a graph-based constrained optimization framework with syntactic and lexical similarity constraints, combining Frobenius-norm regularization and linear inequality/total variation constraints.

Result: LAGO improves attack transferability by 10-20% in Rouge-L scores, outperforming baselines with as few as 10 samples per language.

Conclusion: Language similarity is crucial for inversion attack transferability, highlighting the need for language-aware privacy-preserving multilingual embeddings.

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [15] [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/pdf/2505.16972)
*Tianduo Wang, Lu Xu, Wei Lu, Shanbo Cheng*

Main category: cs.CL

TL;DR: Speech Back-Translation improves multilingual ASR by generating synthetic speech from text, reducing transcription errors by 30%.


<details>
  <summary>Details</summary>
Motivation: Extending ASR to diverse languages with limited resources is challenging.

Method: Uses TTS models to convert text corpora into synthetic speech, scaling data volume massively.

Result: Generated 500,000+ hours of synthetic speech, reducing transcription errors by 30%.

Conclusion: Speech Back-Translation is scalable and effective for multilingual ASR enhancement.

Abstract: Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.

</details>


### [16] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/pdf/2505.16014)
*Yash Saxena, Anpur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur*

Main category: cs.CL

TL;DR: METEORA replaces traditional RAG re-ranking with a rationale-driven selection, improving accuracy and robustness while reducing chunk usage.


<details>
  <summary>Details</summary>
Motivation: Address limitations of similarity-based RAG pipelines, such as lack of explainability and robustness against adversarial content.

Method: Uses a two-stage approach: preference-tuned LLM generates rationales for chunk selection, followed by three-stage selection (local, global, context expansion) and verification.

Result: Improves generation accuracy by 33.34%, uses 50% fewer chunks, and enhances F1 score from 0.10 to 0.44 in adversarial settings.

Conclusion: METEORA offers explainable, robust, and efficient RAG with significant performance gains.

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [17] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/pdf/2505.16022)
*Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, Yulan He*

Main category: cs.CL

TL;DR: NOVER is a reinforcement learning framework that eliminates the need for external verifiers, enabling incentive training across diverse text-to-text tasks and outperforming models like DeepSeek R1-Zero.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on external verifiers, limiting applicability to domains like math and coding. Reward models are costly to train. NOVER addresses these limitations.

Method: NOVER uses standard supervised fine-tuning data without external verifiers, enabling incentive training for text-to-text tasks.

Result: NOVER outperforms models like DeepSeek R1 671B by 7.7% and offers flexibility for optimizing large language models.

Conclusion: NOVER provides a general, cost-effective solution for incentive training, expanding its applicability beyond domains with verifiers.

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [18] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/pdf/2505.16023)
*Sheshera Mysore, Debarati Das, Hancheng Cao, Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: The paper analyzes user interactions with LLMs like Bing Copilot and WildChat, identifying common collaboration behaviors (PATHs) and their correlation with writing intents, impacting LLM alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how users actively collaborate with LLMs in writing tasks, moving beyond passive acceptance to refine and co-construct text.

Method: Large-scale analysis of user interactions with Bing Copilot and WildChat, identifying prototypical behaviors (PATHs) and correlating them with writing intents.

Result: A small set of PATHs explains most user-LLM interaction variation, with significant correlations between specific writing intents and behaviors.

Conclusion: Findings highlight the need for LLM alignment to better support dynamic, collaborative user interactions.

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [19] [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/pdf/2505.16036)
*Burak Erinç Çetin, Yıldırım Özen, Elif Naz Demiryılmaz, Kaan Engür, Cagri Toraman*

Main category: cs.CL

TL;DR: The paper conducts a broad ethical evaluation of 29 open-source large language models, focusing on robustness, reliability, safety, and fairness, across English and Turkish. Results show prioritization of safety and fairness, with reliability as a concern, and larger models like Gemma and Qwen performing best.


<details>
  <summary>Details</summary>
Motivation: To address gaps in ethical evaluation breadth, language coverage, and model diversity in generative large language models.

Method: Evaluated 29 open-source models using a novel data collection method (LLM-as-a-Judge) across four ethical aspects in English and Turkish.

Result: Optimization efforts prioritized safety and fairness; reliability remains a concern. Larger models (e.g., Gemma, Qwen) performed better ethically.

Conclusion: Ethical evaluation is feasible across languages, and larger models tend to perform better. The study guides safer model development by filling evaluation gaps.

Abstract: Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.

</details>


### [20] [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/pdf/2505.16061)
*Yu Zhang*

Main category: cs.CL

TL;DR: Analysis of NLP research impacts from 1979-2024 shows language modeling has the widest influence, while linguistic foundations lag. External and internal impacts align, except for ethics/bias/fairness, which are policy-heavy.


<details>
  <summary>Details</summary>
Motivation: To understand how NLP research from top conferences (ACL, EMNLP, NAACL) is consumed academically and publicly.

Method: Analyzed citations from research articles, patents, media, and policy documents (1979-2024).

Result: Language modeling is most influential; ethics/bias/fairness gain policy attention but fewer academic citations. Patents favor practical NLP, while media/policy focus on societal impacts.

Conclusion: NLP research impacts vary by topic and audience, with language modeling dominating and societal issues gaining external traction.

Abstract: We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.

</details>


### [21] [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/pdf/2505.16212)
*Anfeng Xu, Tiantian Feng, So Hyun Kim, Somer Bishop, Catherine Lord, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: LLMs show promise in correcting ASR errors for child speech but struggle with contextual info and autoregressive ASR outputs.


<details>
  <summary>Details</summary>
Motivation: Improving ASR accuracy for children's conversational speech, which remains a challenge despite recent ASR advancements.

Method: Experiments on two children's conversational speech datasets using zero-shot and fine-tuned ASR outputs with LLMs.

Result: LLMs improve zero-shot and CTC-based ASR outputs but not autoregressive ASR or contextual scenarios.

Conclusion: LLMs are promising for child speech ASR correction but face limitations with autoregressive models and context integration.

Abstract: Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.

</details>


### [22] [TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling](https://arxiv.org/pdf/2504.07053)
*Liang-Hsuan Tseng, Yi-Chang Chen, Kuan-Yi Lee, Da-Shan Shiu, Hung-yi Lee*

Main category: cs.CL

TL;DR: TASTE introduces text-aligned speech tokenization and embedding to bridge the modality gap between speech and text, improving spoken language modeling.


<details>
  <summary>Details</summary>
Motivation: To enhance human-LLM interaction by addressing the underexplored effectiveness of speech tokens in joint speech-text modeling.

Method: Uses attention-based aggregation and speech reconstruction to align speech tokens with text transcriptions.

Result: TASTE reduces token sequence length while preserving paralinguistic info, outperforming other SLMs in speech continuation tasks.

Conclusion: TASTE is the first end-to-end method for text-aligned speech tokenization, offering improved performance in spoken language modeling.

Abstract: Recent efforts target spoken language models (SLMs) that not only listen but
also speak for more natural human-LLM interaction. Joint speech-text modeling
is a promising direction to achieve this. However, the effectiveness of recent
speech tokens for joint modeling remains underexplored. To address this, we
introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that
directly addresses the modality gap by aligning speech token with the
corresponding text transcription during the tokenization stage. We propose a
method that can achieve this through a attention-based aggregation mechanism
and with speech reconstruction as the training objective. We conduct extensive
experiments and show that TASTE can preserve essential paralinguistic
information while dramatically reducing the token sequence length. With TASTE,
we perform straightforward joint spoken language modeling by using Low-Rank
Adaptation on the pre-trained text LLM. Experimental results show that
TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;
while significantly outperform other pre-trained SLMs on speech continuation
across subjective and objective evaluations. To our knowledge, TASTE is the
first end-to-end approach that utilizes a reconstruction objective to
automatically learn a text-aligned speech tokenization and embedding suitable
for spoken language modeling. Our demo, code, and model are available at
https://mtkresearch.github.io/TASTE-SpokenLM.github.io.

</details>


### [23] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/pdf/2505.16078)
*Lujun Li, Lama Sleem, Niccolo' Gentile, Geoffrey Nichil, Radu State*

Main category: cs.CL

TL;DR: The paper evaluates prompt engineering and supervised fine-tuning for transformer-based text classification, focusing on smaller models' performance and efficiency in industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: The inefficiency and resource demands of large decoder-only models like Llama prompt exploration of smaller models for text classification tasks.

Method: Comprehensive evaluation of prompt engineering and supervised fine-tuning methods, tested on email, legal document, and long academic text classification.

Result: Examines smaller models' performance and VRAM efficiency, offering insights for local deployment in industrial settings.

Conclusion: Smaller models can effectively handle text classification tasks, with practical implications for industrial applications.

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [24] [On the reliability of feature attribution methods for speech classification](https://arxiv.org/pdf/2505.16406)
*Gaofei Shen, Hosein Mohebbi, Arianna Bisazza, Afra Alishahi, Grzegorz Chrupała*

Main category: cs.CL

TL;DR: Standard feature attribution methods are unreliable in speech processing, except for word-aligned perturbation in word-based tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding the determinants of outputs from large-scale pre-trained models, especially in speech processing, where input characteristics complicate feature attribution.

Method: Study the impact of input type, aggregation, perturbation timespan, and task characteristics on the reliability of feature attribution methods.

Result: Standard feature attribution methods are generally unreliable in speech, except for word-aligned perturbation in word-based tasks.

Conclusion: Feature attribution in speech requires task-specific methods, with word-aligned perturbation being effective for word-based tasks.

Abstract: As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.

</details>


### [25] [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/pdf/2505.16081)
*KMA Solaiman*

Main category: cs.CL

TL;DR: BiasLab is a dataset of 300 political news articles annotated for ideological bias, with labels for sentiment toward Democrats and Republicans, and rationale indicators. It includes human and GPT-4o annotations, tasks for perception drift and rationale classification, and aims to support explainable bias detection in NLP.


<details>
  <summary>Details</summary>
Motivation: To create a dataset for studying perceived ideological bias in political news, enabling explainable modeling and transparent NLP systems.

Method: Articles were annotated by crowdworkers and GPT-4o, with inter-annotator agreement analyzed. Tasks include perception drift prediction and rationale classification.

Result: The dataset reveals asymmetries in bias perception, especially for subtly right-leaning content, and provides baseline performance for modeling tasks.

Conclusion: BiasLab supports research on explainable bias detection and human-in-the-loop interpretability, with released data and tools for further study.

Abstract: We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.

</details>


### [26] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/pdf/2505.16088)
*Gagan Bhatia, Maxime Peyrard, Wei Zhao*

Main category: cs.CL

TL;DR: The paper addresses the issue of BPE tokenizers fragmenting calendar dates, introduces a metric for date fragmentation, releases a benchmark for temporal reasoning tasks, and reveals how LLMs abstract date fragments for reasoning.


<details>
  <summary>Details</summary>
Motivation: BPE tokenizers often split dates into meaningless fragments, hindering temporal reasoning. The work aims to measure and address this fragmentation.

Method: Introduces a date fragmentation ratio metric, releases DateAugBench for temporal tasks, and uses probing and attention analysis to study LLMs' date abstraction.

Result: Excessive fragmentation reduces accuracy by up to 10 points on uncommon dates. Larger models abstract date fragments faster, and LLMs follow a unique reasoning path (year→month→day).

Conclusion: The study highlights the impact of date fragmentation on temporal reasoning and reveals emergent abstraction mechanisms in LLMs, differing from human interpretation.

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [27] [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/pdf/2505.16102)
*Yash Kumar Atri, Thomas H Shin, Thomas Hartvigsen*

Main category: cs.CL

TL;DR: bRAGgen, an adaptive RAG-based model, improves bariatric surgery care by integrating real-time medical evidence and outperforms state-of-the-art models in generating accurate responses.


<details>
  <summary>Details</summary>
Motivation: Healthcare disparities hinder patient access to timely, evidence-based information in bariatric surgery care.

Method: Introduces bRAGgen, a self-updating RAG model, and bRAGq, a validated dataset of bariatric surgery questions.

Result: bRAGgen outperforms other models in generating clinically accurate responses.

Conclusion: bRAGgen enhances MBS care by ensuring current and accurate information.

Abstract: While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.

</details>


### [28] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/pdf/2505.16104)
*Yue Li, Xin Yi, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang*

Main category: cs.CL

TL;DR: HSR is a lightweight method to restore safety in pruned LVLMs by hierarchically realigning critical attention heads and neurons.


<details>
  <summary>Details</summary>
Motivation: Pruning LVLMs often degrades safety performance, necessitating a solution to restore safety without compromising efficiency.

Method: HSR quantifies safety contributions of attention heads, identifies critical ones, and selectively restores key neurons within them.

Result: HSR consistently improves safety performance across various models and pruning strategies.

Conclusion: HSR is the first method explicitly addressing safety restoration in pruned LVLMs, offering a practical solution for deployment.

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [29] [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/pdf/2505.16107)
*Bo Li, Gexiang Fang, Wei Ye, Zhenghua Xu, Jinglei Zhang, Hao Cheng, Shikun Zhang*

Main category: cs.CL

TL;DR: The paper introduces MPL, a framework using multiple programming languages (PLs) for structured information extraction (IE), improving on existing Python-only approaches. It includes a 'function-prompt' method for better code-style input simulation and shows strong experimental results.


<details>
  <summary>Details</summary>
Motivation: Existing IE research focuses on Python for code-style inputs, ignoring other PLs like C++ and Java, which may offer structural advantages. MPL aims to leverage multiple PLs for better IE performance.

Method: MPL incorporates various PLs during supervised fine-tuning (SFT) and introduces 'function-prompt' with virtual running to simulate code-style inputs more effectively.

Result: Experiments on diverse datasets confirm MPL's effectiveness, outperforming Python-only approaches.

Conclusion: MPL demonstrates the benefits of using multiple PLs for IE, with released code for further research.

Abstract: Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.

</details>


### [30] [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/pdf/2505.16118)
*Haotian Lan, Yao Gao, Yujun Cheng, Wei Yuan, Kun Wang*

Main category: cs.CL

TL;DR: A dual-method LLM framework combines unsupervised expectation extraction from UGC and supervised fine-tuning to quantify travel expectations, revealing leisure/social factors dominate engagement.


<details>
  <summary>Details</summary>
Motivation: Social media's UGC is key for travel decisions, but current analytical methods lack scalability.

Method: Dual-method LLM framework: unsupervised expectation extraction from UGC and survey-informed supervised fine-tuning.

Result: Leisure/social expectations drive engagement more than natural/emotional factors.

Conclusion: LLMs advance tourism analytics, enabling personalized experiences and social travel promotion, with broader applications in consumer behavior and marketing.

Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.

</details>


### [31] [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/pdf/2505.16125)
*Hyopil Shin, Sangah Lee, Dongjun Jang, Wooseok Song, Jaeyoon Kim, Chaeyoung Oh, Hyemi Jo, Youngchae Ahn, Sihyun Oh, Hyohyeong Chang, Sunkyoung Kim, Jinsik Lee*

Main category: cs.CL

TL;DR: KoBALT is a Korean benchmark with 700 linguistically-motivated questions across five domains, designed to evaluate LLMs more robustly by addressing conventional benchmarks' limitations.


<details>
  <summary>Details</summary>
Motivation: To advance LLM evaluation in Korean, a morphologically rich language, by providing a linguistically deep and typologically grounded benchmark.

Method: Introduces expert-curated questions with minimal n-gram overlap to mitigate data contamination, evaluating 20 LLMs across linguistic domains.

Result: LLMs show significant performance disparities (61% general accuracy), with strengths in semantics (66%) and weaknesses in phonology (31%) and morphology (36%). Human evaluation validates KoBALT's discriminative effectiveness.

Conclusion: KoBALT fills gaps in linguistic evaluation for diverse languages and offers a robust framework for assessing Korean LLMs' true linguistic competence.

Abstract: We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.

</details>


### [32] [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/pdf/2505.16128)
*Yue Zhou, Barbara Di Eugenio*

Main category: cs.CL

TL;DR: LLMs exhibit biases in associating solution veracity with demographics, showing Attribution and Evaluation Biases, with African-American and Asian groups disproportionately affected.


<details>
  <summary>Details</summary>
Motivation: To uncover and analyze biases in LLMs' association of solution correctness with demographics, despite explicit alignment against stereotypes.

Method: Experiments across five human value-aligned LLMs on math, coding, commonsense, and writing problems, identifying Attribution and Evaluation Biases.

Result: LLMs attribute fewer correct solutions to African-American groups in math/coding and least prefer Asian authorships in writing. Biases are deeply embedded, as shown in visualization code.

Conclusion: Demographic bias in LLMs extends beyond surface stereotypes, posing risks for educational and evaluation deployments.

Abstract: Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.

</details>


### [33] [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/pdf/2505.16129)
*Hyang Cui*

Main category: cs.CL

TL;DR: A generation-based evaluation paradigm using LLMs for MTQE outperforms direct scoring and non-LLM metrics, advocating hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: Direct scoring methods for MTQE with LLMs show low correlation with human judgments, prompting the need for better evaluation methods.

Method: Proposes using decoder-only LLMs to generate high-quality references, then scoring semantic similarity with sentence embeddings.

Result: Outperforms direct scoring baselines and non-LLM metrics in extensive evaluation across 8 LLMs and 8 language pairs.

Conclusion: Supports hybrid approaches combining fluent generation and semantic assessment for improved MTQE.

Abstract: Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.

</details>


### [34] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/pdf/2505.16134)
*Menschikov Mikhail, Alexander Kharitonov, Maiia Kotyga, Vadim Porvatov, Anna Zhukovskaya, David Kagramanyan, Egor Shvetsov, Evgeny Burnaev*

Main category: cs.CL

TL;DR: Study explores positional bias in large language models across five languages, revealing model-driven variations, challenges in prompt engineering, and unexpected interactions with syntax and uncertainty.


<details>
  <summary>Details</summary>
Motivation: To understand how positional bias in large language models interacts with linguistic diversity, model uncertainty, syntax, and prompting across typologically distinct languages.

Method: A cross-linguistic study analyzing positional bias in five languages (English, Russian, German, Hindi, Vietnamese) by examining model behavior, uncertainty, syntax, and the impact of explicit positional guidance.

Result: (1) Positional bias varies by model and language (e.g., Qwen2.5-7B favors late positions). (2) Explicit positional guidance reduces accuracy. (3) Aligning context with bias increases entropy, but minimal entropy doesn't predict accuracy. (4) LLMs impose dominant word order in free-word-order languages like Hindi.

Conclusion: Positional bias is model-specific and context-dependent, challenging assumptions and prompt-engineering practices, with implications for multilingual NLP applications.

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [35] [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.16142)
*Shicheng Xu, Liang Pang, Yunchang Zhu, Jia Gu, Zihao Wei, Jingcheng Deng, Feiyang Pan, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: RLKD, a reinforcement learning-based distillation framework, improves reasoning in smaller LLMs by aligning student and teacher reasoning structures, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard supervised fine-tuning (SFT) fails to capture the implicit multi-branch reasoning structure of teacher models, limiting student model performance.

Method: Proposes RLKD, using a Generative Structure Reward Model (GSRM) to measure structural alignment and reinforcement learning to distill reasoning.

Result: RLKD outperforms SFT-RL pipelines, even with minimal training data, enhancing student reasoning.

Conclusion: RLKD effectively distills complex reasoning structures, unlocking greater potential in student models.

Abstract: Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.

</details>


### [36] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/pdf/2505.16160)
*Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, Heyan Huang*

Main category: cs.CL

TL;DR: The paper introduces EduBench, a diverse benchmark for educational scenarios, with synthetic data and multi-dimensional evaluation metrics. A small-scale model trained on this dataset performs comparably to state-of-the-art large models.


<details>
  <summary>Details</summary>
Motivation: The application of large language models in education is underexplored and under-optimized, prompting the need for a tailored benchmark and evaluation framework.

Method: The authors create a benchmark with synthetic data (9 scenarios, 4,000 contexts) and propose 12 multi-dimensional metrics. Human annotation validates model-generated responses. A small-scale model is trained and tested.

Result: The small-scale model achieves performance comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on the test set.

Conclusion: This work lays a foundation for developing and evaluating education-oriented language models, with code and data publicly available.

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [37] [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/pdf/2505.16162)
*Mingbo Song, Heming Xia, Jun Zhang, Chak Tou Leong, Qiancheng Xu, Wenjie Li, Sujian Li*

Main category: cs.CL

TL;DR: KNN-SSD improves domain generalizability in Self-Speculative Decoding by using KNN search to match skipped layers with domain inputs, achieving 1.3x-1.6x speedup in LLM inference.


<details>
  <summary>Details</summary>
Motivation: Self-Speculative Decoding's layer-skipping method is sensitive to domain shifts, reducing acceleration performance.

Method: Introduces KNN-SSD, which uses KNN search to align skipped layers with domain-specific inputs.

Result: Achieves 1.3x-1.6x speedup in LLM inference across various models and tasks.

Conclusion: KNN-SSD effectively addresses domain sensitivity in Self-Speculative Decoding, enhancing inference speed without compromising quality.

Abstract: Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.

</details>


### [38] [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/pdf/2505.16164)
*Mengyang Qiu, Zoe Brisebois, Siena Sun*

Main category: cs.CL

TL;DR: LLMs can approximate human averages in phonemic fluency tasks but fail to replicate human variability and diversity.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can simulate individual differences in human cognitive tasks like phonemic fluency.

Method: Evaluated 34 LLM configurations against 106 human participants in phonemic fluency tasks, analyzing outputs for diversity and structure.

Result: Some LLMs matched human averages but none replicated human variability; outputs were less diverse and structurally rigid.

Conclusion: LLMs have limitations in simulating human cognition and behavioral variability.

Abstract: Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.

</details>


### [39] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/pdf/2505.16170)
*Yuqing Yang, Robin Jia*

Main category: cs.CL

TL;DR: LLMs can retract incorrect answers but do so rarely. Retraction is linked to their internal belief, and fine-tuning improves performance.


<details>
  <summary>Details</summary>
Motivation: To understand when and why LLMs retract incorrect answers, given their parametric knowledge.

Method: Constructed datasets to evaluate retraction, analyzed internal belief indicators, and conducted steering experiments.

Result: LLMs retract infrequently, influenced by internal belief. Fine-tuning enhances retraction accuracy.

Conclusion: Retraction behavior in LLMs is tied to internal belief, and fine-tuning can improve their ability to acknowledge mistakes.

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [40] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/pdf/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju, Gondy Leroy, David Kauchak, Arif Ahmed*

Main category: cs.CL

TL;DR: The study compares generative AI for simplifying health texts, identifies missing information, and evaluates methods to reintegrate it, finding that adding all missing entities improves text quality.


<details>
  <summary>Details</summary>
Motivation: To improve health information comprehension by addressing gaps in AI-simplified texts.

Method: Simplified 50 health texts using GPT-4, compared five approaches to identify and reintegrate missing elements, and evaluated using cosine similarity and ROUGE scores.

Result: Adding all missing entities yielded better text regeneration than other methods, though current tools lack ranking value.

Conclusion: Reintegrating all missing entities enhances simplified health texts, but better ranking methods are needed.

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [41] [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/pdf/2505.16178)
*Ying Zhang, Benjamin Heinzerling, Dongyuan Li, Ryoma Ishigaki, Yuta Hitomi, Kentaro Inui*

Main category: cs.CL

TL;DR: The paper explores how mixed training (combining fact-storing and fact-recalling examples) improves fact recall in language models compared to two-stage training, identifying shared parameters as key to generalization.


<details>
  <summary>Details</summary>
Motivation: Understanding why mixed training outperforms two-stage training in promoting robust fact recall in language models.

Method: Introduces cross-task gradient trace to analyze shared parameters influenced by both training stages, tested on synthetic datasets with Llama-3.2B and Pythia-2.8B models.

Result: Mixed training encourages a larger, more centralized set of shared parameters, enhancing generalization of factual knowledge.

Conclusion: Shared parameters are crucial for enabling language models to generalize factual knowledge across tasks.

Abstract: Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.

</details>


### [42] [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/pdf/2505.16188)
*Zirui He, Mingyu Jin, Bo Shen, Ali Payani, Yongfeng Zhang, Mengnan Du*

Main category: cs.CL

TL;DR: A novel supervised steering method for LLMs uses sparse autoencoders and linear classifiers to control behavior effectively in open-ended generation tasks.


<details>
  <summary>Details</summary>
Motivation: Controlling LLM behavior reliably in open-ended settings is challenging, requiring interpretable and targeted interventions.

Method: Uses sparse autoencoders for latent representations, linear classifiers to identify task-relevant subspaces, and supervised steering vectors constrained to these subspaces.

Result: Achieves higher success rates in steering tasks (sentiment, truthfulness, politics) with minimal quality degradation, using a small subspace.

Conclusion: The approach enables more targeted and interpretable control of LLM behavior with minimal impact on generation quality.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.

</details>


### [43] [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/pdf/2505.16189)
*Sophie Wu, Jan Philip Wahle, Saif M. Mohammad*

Main category: cs.CL

TL;DR: The paper explores the link between emotion, embodiment, and everyday language using large-scale natural language data, revealing correlations between body part mentions (BPMs), emotional charge, and health outcomes.


<details>
  <summary>Details</summary>
Motivation: To investigate how body-related language reflects emotions and health in natural communication.

Method: Analyzed corpora of BPMs in blog posts and tweets, including annotated emotional context, and used word-emotion lexicons.

Result: BPMs are common (5-10% of posts), emotionally charged, and correlate with poorer health outcomes.

Conclusion: Body-related language offers insights for NLP, affective sciences, and wellbeing research.

Abstract: This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.

</details>


### [44] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/pdf/2505.16193)
*Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou*

Main category: cs.CL

TL;DR: The paper explores improving Multimodal Sentiment Analysis (MSA) using Multimodal Large Language Models (MLLMs) via In-Context Learning (ICL), addressing zero-shot limitations and optimizing demonstrations for better performance.


<details>
  <summary>Details</summary>
Motivation: Zero-shot MLLMs underperform in MSA, raising doubts about their sentiment perception. The study aims to validate MLLMs' capability by refining ICL.

Method: Extends zero-shot to ICL, optimizing demonstrations' retrieval, presentation, and distribution. Identifies and counteracts a sentimental bias in MLLMs.

Result: Achieves 15.9% and 11.2% accuracy improvements over zero-shot and random ICL baselines, respectively, across six MSA datasets.

Conclusion: MLLMs can excel in MSA with proper ICL configuration, overcoming zero-shot limitations and biases.

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [45] [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/pdf/2505.16216)
*Jisu Kim, Youngwoo Shin, Uiji Hwang, Jihun Choi, Richeng Xuan, Taeuk Kim*

Main category: cs.CL

TL;DR: The paper introduces MIDAS, a multilingual idiom dataset, to evaluate LLMs' idiom processing, revealing a hybrid approach combining memorization and reasoning.


<details>
  <summary>Details</summary>
Motivation: Idioms challenge LLMs due to their unique properties, yet the mechanisms of idiom processing in multilingual settings are underexplored.

Method: MIDAS, a large-scale dataset of idioms in six languages, is used to comprehensively evaluate LLMs' idiom processing abilities.

Result: LLMs use a hybrid approach (memorization + reasoning) for idioms, especially compositional ones, suggesting interplay between knowledge retrieval and inference.

Conclusion: Idiom understanding in LLMs involves both internal knowledge and reasoning, highlighting the need for further exploration of these mechanisms.

Abstract: Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.

</details>


### [46] [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/pdf/2505.16222)
*Jiwon Moon, Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, Kyomin Jung*

Main category: cs.CL

TL;DR: LLMs as code evaluators are prone to biases when assessing semantically equivalent code with superficial variations, impacting fairness and robustness.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM judges can fairly evaluate code with superficial variations, given their increasing use in code evaluation tasks.

Method: A comprehensive study defining six types of bias, tested across five programming languages and multiple LLMs, including prompting for test cases.

Result: All tested LLM judges exhibited biases, leading to inflated or unfairly low scores, even with test case generation.

Conclusion: Current LLM-based code evaluation methods lack robustness, necessitating improved approaches to handle superficial variations fairly.

Abstract: With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.

</details>


### [47] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/pdf/2505.16227)
*Bohao Wu, Qingyun Wang, Yue Guo*

Main category: cs.CL

TL;DR: The paper explores efficient and scalable methods for personalized jargon detection, comparing lightweight fine-tuning (LoRA) and personalized prompting, achieving superior performance with minimal annotated data.


<details>
  <summary>Details</summary>
Motivation: To make technical documents accessible to diverse readers without requiring extensive annotation or computational resources.

Method: Two strategies: (1) LoRA-based lightweight fine-tuning, and (2) personalized prompting. Hybrid approaches combining limited data with user signals are also tested.

Result: Personalized LoRA outperforms GPT-4 by 21.4% in F1 score and exceeds baselines by 8.3%, using only 10% of annotated data.

Conclusion: The study provides a scalable, low-resource solution for personalized jargon detection, advancing user-adaptive NLP systems.

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [48] [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/pdf/2505.16232)
*Ali Sarosh Bangash, Krish Veera, Ishfat Abrar Islam, Raiyan Abdul Baten*

Main category: cs.CL

TL;DR: MuseRAG automates originality scoring of creative ideas using LLMs and RAG, matching human accuracy and enabling scalable creativity research.


<details>
  <summary>Details</summary>
Motivation: Manual originality scoring is labor-intensive and error-prone; automation is needed for large-scale creativity research.

Method: Uses LLMs with RAG to retrieve and cluster ideas, then computes frequency-based originality metrics.

Result: Matches human annotators in clustering (AMI=0.59) and scoring (r=0.89), with strong validity.

Conclusion: MuseRAG enables scalable, human-aligned originality scoring for creativity research.

Abstract: An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.

</details>


### [49] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/pdf/2505.16234)
*Wei Zhang, Zhenhong Zhou, Junfeng Fang, Rongwu Xu, Kun Wang, Yuanhe Zhang, Rui Wang, Ge Zhang, Xinfeng Li, Li Sun, Lingjuan Lyu, Yang Liu, Sen Su*

Main category: cs.CL

TL;DR: LIFEBench evaluates LLMs' ability to follow length instructions, revealing their limitations despite advanced reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with explicit length instructions, prompting the need for a benchmark to assess this overlooked aspect.

Method: LIFEBench includes 10,800 instances across 4 tasks in English and Chinese, testing lengths from 16 to 8192 words.

Result: Most LLMs perform well with short lengths but fail beyond a threshold, often not reaching vendor-claimed maximums. Reasoning LLMs outperform specialized models.

Conclusion: LIFEBench highlights critical limitations in LLMs' length instruction following, guiding future improvements.

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [50] [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.16237)
*Derong Xu, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Maolin Wang, Qidong Liu, Xiangyu Zhao, Yichao Wang, Huifeng Guo, Ruiming Tang, Enhong Chen, Tong Xu*

Main category: cs.CL

TL;DR: Align-GRAG improves graph-based RAG by addressing retrieval inefficiencies and representation gaps, using dual alignment for better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with hallucinations and outdated info; graph RAG enhances context but faces retrieval and representation challenges.

Method: Align-GRAG uses a reasoning-guided dual alignment framework to optimize graph retrieval and representation, integrating it with LLMs.

Result: Experiments show effectiveness in tasks like common sense reasoning and knowledge graph reasoning.

Conclusion: Align-GRAG successfully addresses graph RAG limitations, improving accuracy and efficiency in LLM outputs.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.

</details>


### [51] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/pdf/2505.16241)
*Viet-Anh Nguyen, Shiqian Zhao, Gia Dao, Runyi Hu, Yi Xie, Luu Anh Tuan*

Main category: cs.CL

TL;DR: SEAL is a novel jailbreak attack targeting Large Reasoning Models (LRMs) using adaptive encryption to bypass safety mechanisms, achieving an 80.8% success rate on GPT o4-mini.


<details>
  <summary>Details</summary>
Motivation: LRMs show superior reasoning but may introduce severe security vulnerabilities, which are underexplored. Existing jailbreak methods lack balance between effectiveness and robustness.

Method: SEAL employs a stacked encryption approach with dynamic strategies (random and adaptive) to overwhelm LRMs' reasoning and evade safety mechanisms.

Result: SEAL achieves an 80.8% attack success rate on GPT o4-mini, outperforming baselines by 27.2%.

Conclusion: SEAL demonstrates significant effectiveness in bypassing LRM safety mechanisms, highlighting the need for stronger defenses against such attacks.

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [52] [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/pdf/2505.16245)
*Vijeta Deshpande, Debasmita Ghose, John D. Patterson, Roger Beaty, Anna Rumshisky*

Main category: cs.CL

TL;DR: Diverse-NS is a length-controlled self-learning framework that improves language model diversity without shortening outputs, using minimal preference data.


<details>
  <summary>Details</summary>
Motivation: Common diversity metrics and reward models bias toward shorter outputs, limiting expressiveness.

Method: Diverse-NS generates and filters preference data balancing diversity, quality, and length, training with only 3,000 pairs.

Result: Applied to LLaMA-3.1-8B and Olmo-2 models, it enhances lexical and semantic diversity with minor quality trade-offs.

Conclusion: Smaller models can teach diversity to larger ones, and addressing length bias leads to more expressive outputs.

Abstract: Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective "diversity teachers" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.

</details>


### [53] [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/pdf/2505.16252)
*Hwiyeong Lee, Uiji Hwang, Hyelim Lim, Taeuk Kim*

Main category: cs.CL

TL;DR: The paper evaluates localized unlearning in large language models, finding that parameter locality doesn't strictly determine effective knowledge removal.


<details>
  <summary>Details</summary>
Motivation: Address the uncertainty around localized unlearning's effectiveness in removing target knowledge while preserving general knowledge.

Method: Revisits existing localized unlearning approaches and conducts controlled experiments to assess causal contributions of local parameter updates.

Result: Parameter locality isn't inherently indicative of effective knowledge removal; the required parameter modifications aren't strictly determined.

Conclusion: Challenges the core assumption of localized unlearning, suggesting a need for reevaluation of its principles.

Abstract: Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.

</details>


### [54] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/pdf/2505.16258)
*Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee*

Main category: cs.CL

TL;DR: IRONIC is a framework using Multi-modal Coherence Relations for zero-shot sarcasm detection, outperforming baselines by incorporating cognitive insights.


<details>
  <summary>Details</summary>
Motivation: Current methods lack efficiency in mimicking human cognitive processes for sarcasm detection in multi-modal inputs.

Method: IRONIC leverages Multi-modal Coherence Relations (referential, analogical, pragmatic) for in-context learning.

Result: Achieves state-of-the-art performance in zero-shot Multi-modal Sarcasm Detection.

Conclusion: Highlights the importance of integrating linguistic and cognitive insights into multi-modal reasoning.

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [55] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/pdf/2505.16270)
*Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, Jingrui He*

Main category: cs.CL

TL;DR: The paper introduces Transformer Copilot, a framework where a Copilot model refines a Pilot model's performance by tracking and rectifying mistakes during fine-tuning, achieving up to 34.5% improvement with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To enhance fine-tuning by leveraging the model's learning signals, akin to human reflection on past errors, rather than just minimizing generation loss.

Method: Introduces Mistake Log to track errors, designs a Copilot model for logits rectification, and employs joint training and fused inference paradigms.

Result: Experiments on 12 benchmarks show performance improvements up to 34.5% with marginal computational overhead.

Conclusion: Transformer Copilot effectively enhances model performance, scalability, and transferability by systematically addressing recurring errors.

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [56] [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/pdf/2505.16277)
*Sheng-Fu Wang, Laurent Prevot, Jou-an Chi, Ri-Sheng Huang, Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: The paper explores using spontaneous speech corpora to evaluate LLMs' ability to predict production variables like speech reductions and prosodic prominences, finding spoken genre training data improves prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' cognitive characteristics by evaluating their ability to predict behavioral and physiological variables in language processing, extending this to production variables from speech.

Method: Extract production variables from spontaneous speech corpora, test models trained on different pretraining datasets (written, spoken, mixed) for predicting these variables, and fine-tune them.

Result: Fine-tuned models predict production variables well above baselines, with spoken genre training data yielding more accurate predictions than written genres.

Conclusion: High-quality speech corpora can serve as effective benchmarks for LLMs, highlighting the importance of spoken data in training.

Abstract: The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.

</details>


### [57] [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/pdf/2505.16281)
*Shijie Zhang, Renhao Li, Songsheng Wang, Philipp Koehn, Min Yang, Derek F. Wong*

Main category: cs.CL

TL;DR: HiMATE, a hierarchical multi-agent framework, improves machine translation evaluation by leveraging MQM error typology, self-reflection, and agent discussion, outperforming baselines in human-aligned judgments.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based evaluation methods struggle with error span identification and severity assessment, lacking fine-grained MQM hierarchy exploitation.

Method: HiMATE uses a hierarchical multi-agent system based on MQM typology, incorporating self-reflection and asymmetric agent discussion to reduce hallucinations.

Result: HiMATE outperforms baselines, achieving an 89% F1-score improvement in error span detection and severity assessment.

Conclusion: HiMATE enhances machine translation evaluation by addressing fine-grained error analysis, with significant empirical advantages.

Abstract: The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.

</details>


### [58] [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/pdf/2505.16293)
*Rishabh Maheshwary, Masoud Hashemi, Khyati Mahajan, Shiva Krishna Reddy Malay, Sai Rajeswar, Sathwik Tejaswi Madhusudhan, Spandana Gella, Vikas Yadav*

Main category: cs.CL

TL;DR: Notes Writing improves iterative RAG by generating concise notes from retrieved documents, reducing noise and enhancing reasoning. It boosts performance by 15.6% on average.


<details>
  <summary>Details</summary>
Motivation: Challenges in iterative RAG include lengthy contexts and irrelevant information buildup, hindering reasoning and performance. Existing methods lack scalability or are limited to single-round RAG.

Method: Proposes Notes Writing, a method to generate concise notes from retrieved documents at each step, reducing noise and retaining essential information. It’s framework agnostic and integrates with various iterative RAG methods.

Result: Demonstrated effectiveness across three iterative RAG methods, two models, and four datasets, yielding a 15.6% average improvement with minimal token increase.

Conclusion: Notes Writing enhances iterative RAG by improving context handling and reasoning, offering a scalable and effective solution.

Abstract: Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.

</details>


### [59] [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/pdf/2505.16297)
*Seongryong Jung, Suwan Yoon, DongGeon Kim, Hwanhee Lee*

Main category: cs.CL

TL;DR: ToDi, a token-wise distillation method, adaptively combines FKL and RKL for better knowledge transfer from large to small models, outperforming uniform divergence approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are resource-intensive, and conventional knowledge distillation (KD) methods like FKL and RKL fail to address token-level prediction discrepancies.

Method: Proposes Token-wise Distillation (ToDi), which dynamically combines FKL and RKL per token using a sigmoid-based weighting function derived from teacher-student probability log-ratios.

Result: ToDi consistently outperforms uniform or less granular distillation baselines on instruction-following benchmarks.

Conclusion: ToDi is an effective and practical solution for precise distribution alignment in knowledge distillation.

Abstract: Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.

</details>


### [60] [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/pdf/2505.16303)
*Haochen Shi, Tianshi Zheng, Weiqi Wang, Baixuan Xu, Chunyang Li, Chunkit Chan, Tao Fan, Yangqiu Song, Qiang Yang*

Main category: cs.CL

TL;DR: InferenceDynamics is a scalable LLM routing framework that efficiently selects top-performing models for tasks, improving outcomes and resource use.


<details>
  <summary>Details</summary>
Motivation: Current LLM routing lacks scalability and adaptability for large, specialized model pools and evolving domains.

Method: Proposes InferenceDynamics, a multi-dimensional routing framework modeling model capabilities and knowledge, tested on RouteMix dataset.

Result: Demonstrates effectiveness in benchmarks like MMLU-Pro, GPQA, BigGenBench, and LiveBench, achieving superior task performance.

Conclusion: InferenceDynamics enhances LLM ecosystem utilization; code will be public to foster research.

Abstract: Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.

</details>


### [61] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/pdf/2505.16307)
*Chenzhuo Zhao, Ziqian Liu, Xingda Wang, Junting Lu, Chaoyi Ruan*

Main category: cs.CL

TL;DR: PMPO is a prompt optimization framework using token-level loss to improve LLM performance without costly output generation or human input, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods are limited by scalability due to reliance on costly output generation or human preferences, especially for smaller models.

Method: PMPO refines prompts using token-level cross-entropy loss, identifies low-quality segments via masking, and rewrites them by minimizing loss over examples.

Result: PMPO outperforms prior methods across tasks, achieving high accuracy on BBH, GSM8K, AQUA-RAT, and improving AlpacaEval 2.0 win rates by 19+ points.

Conclusion: PMPO is effective, efficient, and broadly applicable for prompt optimization without needing output sampling or human evaluation.

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [62] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/pdf/2505.16325)
*Yuyang Jiang, Chacha Chen, Shengyuan Wang, Feng Li, Zecong Tang, Benjamin M. Mervak, Lydia Chelala, Christopher M Straus, Reve Chahine, Samuel G. Armato III, Chenhao Tan*

Main category: cs.CL

TL;DR: CLEAR introduces a clinically-grounded framework for evaluating radiology reports with expert-curated labels and attribute-level comparisons, outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics lack granularity and interpretability for nuanced clinical differences in radiology reports, leading to suboptimal evaluation.

Method: CLEAR evaluates reports by assessing the presence/absence of conditions and their description across five key attributes. It also introduces CLEAR-Bench, a dataset annotated by radiologists.

Result: CLEAR achieves high accuracy in extracting clinical attributes and provides metrics strongly aligned with clinical judgment.

Conclusion: CLEAR offers a comprehensive, clinically interpretable evaluation of radiology report quality, improving upon prior methods.

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [63] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/pdf/2505.16330)
*Wenqing Wu, Chengzhi Zhang, Tong Bao, Yi Zhao*

Main category: cs.CL

TL;DR: The paper explores optimal section combinations in academic papers for novelty assessment using language models, finding introduction, results, and discussion most effective.


<details>
  <summary>Details</summary>
Motivation: Existing novelty assessment methods focus on limited word or entity combinations, while novelty content is distributed across sections. Identifying optimal section combinations can improve automated novelty evaluation.

Method: Uses NLP to categorize papers into IMRaD sections, then tests different section combinations as inputs for PLMs and LLMs, comparing predictions to human novelty scores.

Result: Introduction, results, and discussion are optimal for novelty assessment; entire text is ineffective. Introduction and results are most critical.

Conclusion: Optimal section combinations improve novelty prediction; introduction and results are key. Code and dataset are publicly available.

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [64] [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/pdf/2505.16348)
*Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong-woo Kwak, Kuan-Hao Huang, Jinyoung Yeo*

Main category: cs.CL

TL;DR: MEMENTO is a framework to evaluate embodied agents' memory utilization for personalized assistance, revealing limitations in current LLMs like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Current embodied agents lack effective memory utilization for personalized assistance, especially in dynamic, real-world tasks.

Method: MEMENTO uses a two-stage memory evaluation process to assess agents' understanding of personalized knowledge in object rearrangement tasks.

Result: Experiments show significant performance drops (30.5%) in LLMs like GPT-4o when referencing multiple memories, particularly in tasks involving user patterns.

Conclusion: The findings highlight the need for improved memory utilization in embodied agents for personalized assistance, guiding future research.

Abstract: Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO

</details>


### [65] [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/pdf/2502.15401)
*Xuetao Ma, Wenbin Jiang, Hua Huang*

Main category: cs.CL

TL;DR: The paper introduces a curriculum ICL strategy for LLMs, focusing on problem-solving logic for example selection and ordering, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods rely on simple features for example relevance, which fail to capture intrinsic connections, limiting reasoning capabilities.

Method: Proposes a curriculum ICL strategy: selects examples via problem-solving logic analysis, orders them from easy to hard using curriculum learning, and fine-tunes a model for logic assessment.

Result: Outperforms prior ICL methods in performance and efficiency, enhancing LLMs' complex reasoning.

Conclusion: The curriculum ICL strategy, guided by problem-solving logic, effectively improves LLMs' reasoning, with plans for public release.

Abstract: In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.

</details>


### [66] [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/pdf/2505.16349)
*Pierre Achkar, Tim Gollub, Martin Potthast*

Main category: cs.CL

TL;DR: XSum is a modular pipeline for multi-document summarization in scientific papers, using Retrieval-Augmented Generation (RAG) to generate coherent summaries with proper citations.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of scientific publications makes it hard for researchers to stay updated, necessitating efficient summarization tools.

Method: XSum uses a question-generation module to retrieve relevant information and an editor module to synthesize coherent summaries.

Result: XSum outperforms existing methods on the SurveySum dataset, improving metrics like CheckEval, G-Eval, and Ref-F1.

Conclusion: XSum offers a transparent, adaptable framework for scientific summarization with broad domain applications.

Abstract: The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum

</details>


### [67] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/pdf/2505.14679)
*Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang*

Main category: cs.CL

TL;DR: ULTRAEDIT is a scalable, efficient method for lifelong learning in LLMs, enabling fast, lightweight edits with minimal overhead and superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current model editing methods in supporting practical, scalable lifelong learning for LLMs.

Method: ULTRAEDIT uses lightweight linear algebra operations for parameter shifts and a lifelong normalization strategy to adapt to distributional shifts.

Result: Achieves 7x faster editing speeds, uses less VRAM, and supports up to 1M edits with high accuracy.

Conclusion: ULTRAEDIT is a highly efficient and scalable solution for lifelong model editing, outperforming existing methods.

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [68] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/pdf/2505.16381)
*Songlin Yang, Yikang Shen, Kaiyue Wen, Shawn Tan, Mayank Mishra, Liliang Ren, Rameswar Panda, Yoon Kim*

Main category: cs.CL

TL;DR: PaTH introduces a data-dependent position encoding scheme using Householder transformations, outperforming RoPE in synthetic and real-world language tasks.


<details>
  <summary>Details</summary>
Motivation: RoPE's position encoding lacks input dependency, limiting transformer expressivity. PaTH addresses this by making transformations data-dependent.

Method: PaTH uses accumulated products of Householder transformations, each input-dependent. It includes an efficient parallel training algorithm and a FlashAttention-style blockwise implementation.

Result: PaTH shows superior performance over RoPE and other baselines in synthetic benchmarks and real-world language modeling.

Conclusion: PaTH's data-dependent approach enhances transformer expressivity and performance, making it a promising alternative to RoPE.

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [69] [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/pdf/2505.16385)
*Kaiyu He, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao*

Main category: cs.CL

TL;DR: The paper investigates how LLMs acquire cross-lingual abilities, proposing a word-level translation task and identifying two behaviors (co-occurrence and semantic pivot) to improve LLMs' performance.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how LLMs develop cross-lingual abilities for better interpretability and performance.

Method: Proposes a Word-Level Cross-Lingual Translation Task, traces LLMs' intermediate layers, and identifies co-occurrence and semantic pivot behaviors.

Result: Identifies two distinct behaviors in LLMs and validates a semantic pivot-aware pre-training dataset to enhance cross-lingual ability.

Conclusion: The study provides insights into LLMs' interpretability and offers a method to improve their cross-lingual performance.

Abstract: Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.

</details>


### [70] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/pdf/2505.16392)
*Benjamin Vendeville, Liana Ermakova, Pierre De Loor*

Main category: cs.CL

TL;DR: The paper introduces a test collection for detecting and classifying errors in simplified texts, addressing gaps in current ATS evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current ATS metrics fail to correlate with errors, and manual inspections reveal diverse issues, highlighting the need for a better evaluation framework.

Method: Proposes a taxonomy of errors, creates a parallel dataset of simplified scientific texts with human annotations, and evaluates existing models for error detection.

Result: Provides a dataset and taxonomy to improve error evaluation in ATS, aiding in model development and text quality.

Conclusion: The contributions enable better error evaluation and model reliability, enhancing the quality of automatically simplified texts.

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [71] [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/pdf/2505.16408)
*Muhammad Farid Adilazuarda, Chen Cecilia Liu, Iryna Gurevych, Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper examines challenges in adapting cultural values in LLMs, finding WVS data alone insufficient and proposing augmented data for better cultural distinctiveness.


<details>
  <summary>Details</summary>
Motivation: Addressing biases and limited training data in LLMs for cultural value adaptation, questioning the effectiveness of WVS data.

Method: Systematically investigates WVS-based training, augmented with Wikipedia and NormAd narratives for cultural adaptation.

Result: Augmented data improves cultural distinctiveness but has variable effects on downstream tasks.

Conclusion: Cultural value alignment is complex; task-specific behavior requires nuanced data beyond surveys.

Abstract: Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.

</details>


### [72] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/pdf/2505.16410)
*Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen*

Main category: cs.CL

TL;DR: Tool-Star is an RL-based framework enabling LLMs to autonomously use multiple tools for reasoning, addressing data scarcity and enhancing multi-tool collaboration.


<details>
  <summary>Details</summary>
Motivation: Leveraging RL to improve multi-tool collaborative reasoning in LLMs, which remains a challenge.

Method: Introduces Tool-Star with tool-integrated data synthesis, quality normalization, and a two-stage training framework (cold-start fine-tuning and multi-tool self-critic RL).

Result: Effective performance on 10+ reasoning benchmarks, demonstrating improved tool collaboration.

Conclusion: Tool-Star successfully enhances LLMs' multi-tool reasoning via systematic data synthesis and RL training.

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [73] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.16415)
*Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz*

Main category: cs.CL

TL;DR: ARC-JSD is a new method for efficient and accurate context attribution in RAG models without fine-tuning or surrogate modeling, outperforming previous methods in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for context attribution in RAG models are computationally intensive and often require fine-tuning or human annotation, limiting scalability and practicality.

Method: Introduces ARC-JSD, a Jensen-Shannon Divergence driven approach, to identify essential context sentences efficiently without additional training.

Result: Demonstrates superior accuracy and computational efficiency on benchmarks like TyDi QA, Hotpot QA, and Musique. Also reveals key attention heads and MLP layers for context attribution.

Conclusion: ARC-JSD offers a scalable and efficient solution for context attribution in RAG models, with insights into model internals.

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [74] [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/pdf/2505.16418)
*Yoichi Aoki, Soichiro Murakami, Ukyo Honda, Akihiko Kato*

Main category: cs.CL

TL;DR: The paper explores the impact of diversity-enhancing methods on ad text generation, focusing on their relationship with ad quality, unlike prior work on summarization and translation.


<details>
  <summary>Details</summary>
Motivation: Diverse ad texts are crucial for audience engagement, but the effectiveness of diversity methods in ad generation is understudied compared to other NLP tasks.

Method: The study examines diversity-enhancing methods, their hyperparameters, input-output formats, and models to assess their impact on ad quality.

Result: Not explicitly stated in the abstract, but the research aims to uncover the relationship between diversity and ad quality.

Conclusion: The paper highlights the need to study diversity methods specifically for ad text generation due to its unique requirements and style.

Abstract: In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.

</details>


### [75] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/pdf/2505.16421)
*Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li*

Main category: cs.CL

TL;DR: WebAgent-R1 is an RL framework for multi-turn web interactions, improving task success rates significantly over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL for LLMs focuses on single-turn tasks; multi-turn web interactions are complex and underexplored.

Method: WebAgent-R1 uses asynchronous trajectory generation guided by binary rewards, tested on WebArena-Lite.

Result: Boosts success rates (Qwen-2.5-3B: 6.1% to 33.9%; Llama-3.1-8B: 8.5% to 44.8%), outperforming state-of-the-art.

Conclusion: Thinking-based prompting and warm-up training (behavior cloning) are key; variants (R1-Zero, R1-CoT) offer insights for future work.

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [76] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/pdf/2505.16425)
*Jing Bi, Pinxin Liu, Ali Vosoughi, Jiarui Wu, Jinxi He, Chenliang Xu*

Main category: cs.CL

TL;DR: A framework translates procedural text into visual instructions using linguistic structure, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of conveying complex actions and spatial relationships through text in NLP.

Method: Decomposes text into goal statements and steps, using a constituency parser, discourse coherence model, and a new evaluation protocol.

Result: Outperforms baselines on three datasets (HTStep, CaptainCook4D, WikiAll) in generating accurate visuals.

Conclusion: Advances grounding procedural language in visuals, with applications in education and multimodal understanding.

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [77] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/pdf/2505.16429)
*Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun Yin, Fei Jiang, Wei Lin, Rui Yan*

Main category: cs.CL

TL;DR: RecInter is an agent-based simulation platform for recommender systems with dynamic user-platform interactions, improving realism and credibility.


<details>
  <summary>Details</summary>
Motivation: Traditional A/B testing is resource-heavy, and offline methods fail to capture dynamic user-platform interactions. Existing simulations lack mechanisms for user actions to reshape the environment.

Method: RecInter features a robust interaction mechanism where user actions update item attributes in real-time, supported by Merchant Agents and advanced profiling. It uses LLM fine-tuned on Chain-of-Thought data.

Result: The platform achieves high-fidelity simulation, replicating emergent phenomena like Brand Loyalty and the Matthew Effect, proving its credibility.

Conclusion: RecInter provides a realistic and evolving testbed for recommender systems research, addressing gaps in existing simulation methods.

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [78] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/pdf/2505.16460)
*Ikhlasul Akmal Hanif, Eryawan Presma Yulianrifat, Jaycent Gunawan Ongris, Eduardus Tjitrahardja, Muhammad Falensi Azmi, Rahmat Bryan Naufal, Alfan Farizki Wicaksono*

Main category: cs.CL

TL;DR: The paper explores multilabel emotion classification across 28 languages, comparing fine-tuning transformer models and classifier-only training. The best results come from using prompt-based encoders with CatBoost classifiers, achieving an average F1-macro score of 56.58.


<details>
  <summary>Details</summary>
Motivation: To improve multilabel emotion classification performance across multiple languages by evaluating different training strategies and model architectures.

Method: Two main strategies: fully fine-tuning transformer models (XLMR, mBERT) and classifier-only training on prompt-based encoders (mE5, BGE). Evaluated various settings like loss functions, encoders, and classifiers.

Result: Classifier-only training on prompt-based encoders outperformed fully fine-tuning. The best model was an ensemble of BGE models with CatBoost, achieving an average F1-macro score of 56.58.

Conclusion: Prompt-based encoders with classifier-only training, especially when ensembled, yield superior performance for multilabel emotion classification across diverse languages.

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [79] [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/pdf/2505.16467)
*Vera Neplenbroek, Arianna Bisazza, Raquel Fernández*

Main category: cs.CL

TL;DR: LLMs infer demographics from conversation cues, leading to biased responses. This study shows how stereotypes influence LLMs and proposes a mitigation method using internal representation intervention.


<details>
  <summary>Details</summary>
Motivation: To understand and address how LLMs infer and act on demographic stereotypes, even when users explicitly state their identity, which can harm minority groups.

Method: Used controlled synthetic conversations to analyze LLMs' latent user representations, examining model internals and generated responses to targeted questions.

Result: LLMs infer demographics from stereotypes, persisting even when users state otherwise. A linear probe intervention effectively mitigates this bias.

Conclusion: Greater transparency and control over LLMs' user identity representations are needed to prevent stereotype-driven biases.

Abstract: Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.

</details>


### [80] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/pdf/2505.16483)
*Shuzheng Si, Haozhe Zhao, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Bofei Gao, Kangyang Luo, Wenhao Li, Yufei Huang, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun*

Main category: cs.CL

TL;DR: CANOE is a framework improving LLM faithfulness in generation tasks without human annotations, using synthesized QA data and Dual-GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs provide faithful responses in information-seeking systems is critical for reliability.

Method: Synthesizes short-form QA data, uses Dual-GRPO (rule-based RL) with tailored rewards to optimize generation tasks.

Result: CANOE enhances faithfulness across 11 tasks, outperforming advanced LLMs like GPT-4o.

Conclusion: CANOE effectively improves LLM faithfulness without human input, demonstrating superior performance.

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [81] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/pdf/2505.16491)
*Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia*

Main category: cs.CL

TL;DR: The study explores sentiment encoding in Llama models, identifying mid-layers as key for sentiment analysis and reducing memory usage by 57%.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs capture sentiment and improve sentiment analysis beyond prompting techniques.

Method: Probe classifiers analyze sentiment encoding across layers and scales, focusing on binary polarity tasks.

Result: Sentiment is concentrated in mid-layers, boosting accuracy by 14% over prompting; last token isn't always most informative.

Conclusion: Layer-specific probing enhances sentiment analysis, improving utility and reducing memory needs.

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [82] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/pdf/2505.16505)
*Runcong Zhao, Chengyu Cao, Qinglin Zhu, Xiucheng Lv, Shun Shao, Lin Gui, Ruifeng Xu, Yulan He*

Main category: cs.CL

TL;DR: Concise-SAE is a training-free framework that improves instruction following in language models by editing relevant neurons using natural language, evaluated on the FreeInstruct benchmark.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of instruction following in complex narrative contexts, where existing benchmarks fall short.

Method: Proposes Concise-SAE, which identifies and edits instruction-relevant neurons using natural language instructions, without labeled data.

Result: Achieves state-of-the-art instruction adherence across tasks without degrading generation quality.

Conclusion: Concise-SAE effectively improves instruction following in narrative-rich settings, validated by the FreeInstruct benchmark.

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [83] [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/pdf/2505.16514)
*Yuting Huang, Meitong Guo, Yiquan Wu, Ang Li, Xiaozhong Liu, Keting Yin, Changlong Sun, Fei Wu, Kun Kuang*

Main category: cs.CL

TL;DR: The paper introduces the AppealCase dataset to address the overlooked appellate process in LegalAI, proposing five novel tasks and evaluating 20 models, all of which perform poorly on judgment reversal prediction.


<details>
  <summary>Details</summary>
Motivation: Appeals are crucial for error correction and fair trials but are understudied in LegalAI. The AppealCase dataset aims to fill this gap.

Method: The dataset includes 10,000 matched first- and second-instance civil case documents with detailed annotations. Five novel tasks are proposed and evaluated using 20 models.

Result: All models score under 50% F1 on judgment reversal prediction, showing the task's difficulty.

Conclusion: The AppealCase dataset encourages further research on appellate case analysis to improve judicial consistency.

Abstract: Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.

</details>


### [84] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/pdf/2505.16518)
*Lovisa Hagström, Youna Kim, Haeun Yu, Sang-goo Lee, Richard Johansson, Hyunsoo Cho, Isabelle Augenstein*

Main category: cs.CL

TL;DR: The paper introduces CUB, a benchmark for evaluating context utilisation manipulation techniques (CMTs) in retrieval-augmented generation (RAG), revealing their limitations in handling diverse real-world contexts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic comparison of CMTs and their inconsistent performance in knowledge-intensive tasks like question answering and fact checking.

Method: Developed CUB, a benchmark for testing CMTs on three context types, evaluated seven state-of-the-art CMTs across three datasets and nine language models.

Result: Most CMTs struggle with diverse real-world contexts and perform better on simple synthetic datasets than realistic ones.

Conclusion: Highlights the need for holistic testing and development of CMTs capable of handling multiple context types.

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [85] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/pdf/2505.16520)
*Giovanni Servedio, Alessandro De Bellis, Dario Di Palma, Vito Walter Anelli, Tommaso Di Noia*

Main category: cs.CL

TL;DR: The paper challenges prior findings on LLMs' truthfulness encoding by creating a more realistic dataset and evaluating two open-source LLMs, revealing partial validation of past results but highlighting generalization challenges.


<details>
  <summary>Details</summary>
Motivation: Factual hallucinations in LLMs undermine reliability and trust. Prior studies used synthetic datasets, limiting realism and generalization. This work aims to address these gaps.

Method: Introduced strategies for sampling true-false factoid sentences from tabular data and generating realistic datasets from Question Answering collections. Evaluated two open-source LLMs.

Result: Previous findings were partially validated, but generalization to LLM-generated datasets proved challenging.

Conclusion: The study provides groundwork for future research on LLM factuality and offers practical evaluation guidelines.

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [86] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/pdf/2505.16522)
*Zhouhao Sun, Zhiyuan Kan, Xiao Ding, Li Du, Yang Zhao, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: A multi-bias benchmark and CMBE method are proposed to address LLMs' poor generalizability due to multiple biases, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with generalizability due to multiple biases in data, and existing benchmarks fail to address this complexity.

Method: Proposes a multi-bias benchmark and CMBE, a causal effect estimation-guided method to eliminate multiple biases simultaneously.

Result: CMBE effectively eliminates multiple biases, enhancing LLMs' generalizability.

Conclusion: The study highlights the challenge of multi-bias elimination and demonstrates CMBE's effectiveness in improving LLM performance.

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [87] [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/pdf/2505.16526)
*Heejae Suh, Yejin Jeon, Deokhyung Kang, Taehee Park, Yejin Min, Gary Geunbae Lee*

Main category: cs.CL

TL;DR: EnSToM improves topic consistency in small LLMs for dialogue systems by dynamically adjusting steering vectors based on input uncertainty, outperforming fine-tuning with less data.


<details>
  <summary>Details</summary>
Motivation: Small LLMs (sLLMs) struggle with topic consistency in task-oriented dialogues, risking misuse. Existing activation engineering methods are insufficient.

Method: Proposed EnSToM: dynamically adjusts steering vectors using input uncertainty to handle off-topic inputs while maintaining on-topic accuracy.

Result: EnSToM achieves significant performance gains with minimal data compared to fine-tuning, enhancing topic adherence without efficiency loss.

Conclusion: EnSToM offers a robust solution for improving sLLM-based dialogue systems by balancing topic consistency and efficiency.

Abstract: Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.

</details>


### [88] [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/pdf/2505.16538)
*Ercong Nie, Helmut Schmid, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper investigates language confusion in LLMs, identifies confusion points, and proposes neuron-level interventions to mitigate the issue without compromising model performance.


<details>
  <summary>Details</summary>
Motivation: Language confusion in English-centric LLMs is a critical challenge, and understanding its mechanisms can improve multilingual modeling.

Method: Combines behavioral benchmarking (LCB) with mechanistic interpretability (MI), using TunedLens and neuron attribution to analyze confusion points and transition failures.

Result: Editing critical neurons reduces confusion, matches multilingual alignment, and improves output quality without harming fluency.

Conclusion: Neuron-level interventions offer a robust, interpretable solution for multilingual LLMs, enhancing their internal dynamics.

Abstract: Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.

</details>


### [89] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/pdf/2505.16552)
*Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song*

Main category: cs.CL

TL;DR: CoLaR introduces a framework to compress reasoning processes in latent space, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and computational expense of token-level reasoning chains in LLMs.

Method: A two-stage approach: supervised fine-tuning with compressed embedding prediction and reinforcement learning to explore diverse reasoning paths.

Result: Achieves higher accuracy and reduces reasoning chain length significantly compared to baselines.

Conclusion: CoLaR efficiently compresses reasoning, offering dynamic adjustment and improved performance in mathematical tasks.

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [90] [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/pdf/2505.16566)
*Dongwon Noh, Donghyeok Koh, Junghun Yuk, Gyuwan Kim, Jaeyong Lee, Kyungtae Lim, Cheoneum Park*

Main category: cs.CL

TL;DR: The paper introduces ScholarBench, a scalable benchmark for evaluating LLMs' domain-specific knowledge and academic reasoning across complex tasks and multiple research domains.


<details>
  <summary>Details</summary>
Motivation: Prior benchmarks lack scalability for complex academic tasks, prompting the need for ScholarBench to assess deep expert knowledge and reasoning.

Method: ScholarBench is constructed via a three-step process, focusing on specialized contexts from academic literature, with five problem types and bilingual (English-Korean) evaluation.

Result: State-of-the-art models like o3-mini score only 0.543 on average, highlighting the benchmark's difficulty.

Conclusion: ScholarBench effectively challenges LLMs in academic reasoning and linguistic capabilities, filling a gap in existing benchmarks.

Abstract: Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.

</details>


### [91] [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/pdf/2505.16570)
*Dongyang Fan, Vinko Sabolčec, Martin Jaggi*

Main category: cs.CL

TL;DR: URL metadata speeds up LLM training and improves downstream performance with longer prompts, while quality and topic metadata offer no training benefits but aid controllable generation.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the effectiveness of different metadata types (e.g., URL, quality, topic) in LLM pretraining, as prior work lacks clarity on their impact.

Method: Conducted a systematic evaluation of metadata types (URL, quality scores, topic/format) during LLM pretraining, analyzing their effects on training efficiency and downstream performance.

Result: URL context speeds up training and improves downstream performance with longer prompts. Quality and topic metadata do not aid training but enable controllable generation.

Conclusion: URL metadata is uniquely beneficial for training efficiency, while topic and format metadata are useful for steering model outputs, providing interpretable control.

Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.

</details>


### [92] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/pdf/2505.16576)
*Spencer Hong, Meng Luo, Xinyi Wan*

Main category: cs.CL

TL;DR: EMULATE is a multi-agent framework for claim verification that mimics human behavior, outperforming prior methods by iteratively retrieving and evaluating evidence.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking systems deviate from human-like processes, prompting the need for a more human-emulative approach.

Method: EMULATE uses a multi-agent framework where each agent handles specific tasks like ranking search results or evaluating webpage content, iteratively retrieving evidence.

Result: Experiments show EMULATE outperforms prior work on multiple benchmarks.

Conclusion: The multi-agent framework effectively emulates human fact-checking, improving claim verification accuracy.

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [93] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/pdf/2505.16582)
*Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao*

Main category: cs.CL

TL;DR: O²-Searcher, a reinforcement learning-based search agent, addresses the limitations of LLMs by dynamically acquiring knowledge for open- and closed-ended questions, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs are limited by static knowledge and struggle with open-ended questions. O²-Searcher aims to bridge this gap by integrating dynamic knowledge acquisition.

Method: Uses reinforcement learning in a locally simulated search environment, decoupling knowledge from reasoning, with unified training and adaptive strategies.

Result: O²-Searcher outperforms leading LLM agents on open-ended tasks (O²-QA benchmark) and achieves SOTA on closed-ended benchmarks with a 3B model.

Conclusion: O²-Searcher effectively tackles both open- and closed-ended questions, demonstrating superior performance with a compact model.

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [94] [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/pdf/2505.16591)
*Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan Wang, Haote Yang, Songyang Zhang, Dahua Lin, Lijun Wu, Conghui He*

Main category: cs.CL

TL;DR: KoLasSimpleQA is a multilingual benchmark for evaluating LLMs' factual abilities, covering 9 languages and dual domains (general and language-specific). It reveals performance gaps and aims to guide model optimization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive multilingual benchmark for evaluating LLMs' factual memory and self-awareness.

Method: Created a question set with features like single knowledge point coverage, objectivity, unique answers, and temporal stability. Evaluated mainstream LLMs using the LLM-as-judge paradigm.

Result: Significant performance differences between general and language-specific domains, highlighting gaps in multilingual capabilities.

Conclusion: KoLasSimpleQA aids in identifying LLM boundaries and optimizing models for multilingual contexts. The benchmark will be publicly released.

Abstract: We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness ("know what they don't know").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .

</details>


### [95] [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/pdf/2505.16610)
*Jing Ye, Lu Xiang, Yaping Zhang, Chengqing Zong*

Main category: cs.CL

TL;DR: A self-evolution framework for LLMs improves emotional support by aligning responses with users' implicit preferences through iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Generic LLM responses fail to address users' specific emotional needs, necessitating a personalized approach.

Method: Two-phase framework: (1) fine-tuning on emotional support data, (2) self-improvement via self-reflection and refinement.

Result: Enhanced performance in emotional support, fewer unhelpful responses, and better alignment with user preferences.

Conclusion: The framework effectively personalizes LLM responses for emotional support, improving user satisfaction.

Abstract: Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.

</details>


### [96] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/pdf/2505.16612)
*Daniel Scalena, Gabriele Sarti, Arianna Bisazza, Elisabetta Fersini, Malvina Nissim*

Main category: cs.CL

TL;DR: The paper explores strategies for personalizing LLM-generated translations in low-resource settings, focusing on literary translation. It introduces a contrastive framework and inference-time interventions to achieve strong personalization while maintaining translation quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based translation systems struggle with implicit stylistic requirements, especially in low-resource settings like literary translation.

Method: The study uses prompting strategies, inference-time interventions, and a contrastive framework leveraging latent concepts from sparse autoencoders.

Result: Steering methods achieve strong personalization without compromising translation quality. Multi-shot prompting and steering impact similar model layers, suggesting shared mechanisms.

Conclusion: The proposed methods effectively personalize translations in challenging settings, with implications for understanding LLM behavior in stylistic adaptation.

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [97] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/pdf/2505.16637)
*Wenjie Yang, Mao Zheng, Mingyang Song, Zheng Li*

Main category: cs.CL

TL;DR: A self-rewarding RL framework (SSR) for machine translation outperforms existing models without relying on expensive external supervision.


<details>
  <summary>Details</summary>
Motivation: Overcoming the reliance on costly external supervision signals in advanced MT-specific LLMs.

Method: Proposes a Simple Self-Rewarding (SSR) RL framework, reference-free and fully online, using self-judging rewards.

Result: SSR-Zero-7B outperforms existing MT-specific and larger general LLMs; SSR-X-Zero-7B achieves state-of-the-art performance.

Conclusion: Self-rewarding mechanisms are effective and complementary to external supervision, offering insights into self-improving RL methods.

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [98] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/pdf/2505.16648)
*Kexin Shang, Chia-Hsuan Chang, Christopher C. Yang*

Main category: cs.CL

TL;DR: A multi-LLM collaboration framework improves reasoning and reduces divergence in medical multiple-choice questions, with LLM confidence correlating with accuracy.


<details>
  <summary>Details</summary>
Motivation: Leverage the untapped potential of multiple LLMs' expertise for medical tasks, addressing the lack of synergy in current approaches.

Method: Propose a multi-LLM collaboration framework tested on a medical multiple-choice dataset, analyzing 3 pre-trained LLMs.

Result: The framework enhances reasoning ability and reduces divergence among LLMs, with confidence levels aligning with prediction accuracy.

Conclusion: Multi-LLM collaboration is effective for medical tasks, with confidence as a reliable indicator of accuracy.

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [99] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/pdf/2505.16660)
*Liu Chang, Wang Dongbo, Liu liu, Zhao Zhixiao*

Main category: cs.CL

TL;DR: The paper introduces Guji_MATH, a benchmark for evaluating reasoning models on Chinese ancient mathematical texts, revealing their limitations in classical Chinese comprehension and suggesting improvements.


<details>
  <summary>Details</summary>
Motivation: To address challenges in processing Chinese ancient mathematical classics and evaluate reasoning models' capabilities under classical Chinese linguistic constraints.

Method: Constructed Guji_MATH benchmark with 538 problems from 8 texts, using machine-assisted annotation and manual verification. Evaluated six models via closed-book and open-book modes.

Result: Models partially comprehend and solve problems but underperform compared to modern tasks. Classical Chinese comprehension and cultural knowledge need enhancement.

Conclusion: The study aids in mining mathematical knowledge from ancient texts and evaluates cross-linguistic reasoning models, suggesting focus on classical Chinese comprehension for optimization.

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [100] [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/pdf/2505.16661)
*Issey Sukeda, Takuro Fujii, Kosei Buma, Shunsuke Sasaki, Shinnosuke Ono*

Main category: cs.CL

TL;DR: A Japanese pharmaceutical domain-specific language model, trained on 2B Japanese and 8B English biomedical tokens, outperforms open models and competes with commercial ones like GPT-4o, especially in terminology-heavy tasks. New benchmarks (YakugakuQA, NayoseQA, SogoCheck) evaluate factual recall, lexical variation, and logical consistency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of domain-specific language models for Japanese pharmaceutical applications and provide rigorous evaluation benchmarks.

Method: Continual pretraining on Japanese pharmaceutical and English biomedical tokens, followed by evaluation using new benchmarks against open-source and commercial models.

Result: The model outperforms open models and matches commercial ones, with GPT-4o struggling on cross-sentence consistency tasks.

Conclusion: Demonstrates feasibility of practical, secure, and cost-effective Japanese domain-specific models, offering reusable evaluation resources for future research.

Abstract: We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.

</details>


### [101] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/pdf/2505.16694)
*Gouki Minegishi, Hiroki Furuta, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.CL

TL;DR: The paper explores how transformer-based models acquire meta-learning abilities during training, revealing multi-phase circuit dynamics distinct from simple induction heads.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models meta-learn tasks from context, beyond just copying answers, and clarify the training dynamics behind this ability.

Method: Extends the copy task to an In-Context Meta Learning setting, analyzing model circuit dynamics during training.

Result: Identifies multiple phases in acquiring meta-learning abilities, with unique circuits emerging in each phase, differing from induction heads.

Conclusion: Provides deeper insights into the transformer's ICL ability, linking circuit emergence to known phenomena in large language models.

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [102] [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/pdf/2505.16703)
*Zeping Yu, Sophia Ananiadou*

Main category: cs.CL

TL;DR: Locate-then-Merge framework mitigates catastrophic forgetting in MLLMs by selectively fusing parameters, preserving visual capabilities while retaining language skills.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of catastrophic forgetting of language abilities in MLLMs during multimodal instruction tuning.

Method: Proposes Locate-then-Merge, a training-free parameter fusion framework with Neuron-Fusion, a neuron-level strategy to selectively merge parameters.

Result: Outperforms existing methods on 13 benchmarks, reducing context hallucination and retaining visual adaptation.

Conclusion: Neuron-Fusion effectively balances visual and language capabilities in MLLMs.

Abstract: Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.

</details>


### [103] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/pdf/2505.16722)
*Himanshu Beniwal, Youngwoo Kim, Maarten Sap, Soham Dan, Thomas Hartvigsen*

Main category: cs.CL

TL;DR: The paper introduces 'Cross-lingual Detoxification' to reduce toxicity in LLMs across languages, evaluating its effectiveness in 504 settings and balancing safety with knowledge preservation.


<details>
  <summary>Details</summary>
Motivation: Ensuring toxicity-free LLMs in diverse linguistic contexts is a critical challenge.

Method: The study uses a cross-lingual paradigm to transfer detoxification capabilities between high and low-resource languages, analyzing 504 settings.

Result: The approach effectively reduces toxicity but reveals trade-offs between safety and model performance on non-toxic tasks.

Conclusion: Cross-lingual detoxification is viable but requires balancing safety and knowledge preservation.

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [104] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/pdf/2505.16743)
*Florentin Beck, William Rudman, Carsten Eickhoff*

Main category: cs.CL

TL;DR: TRIM introduces targeted row-wise pruning for LLMs, outperforming uniform methods with iterative metric-driven sparsity adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods use uniform sparsity, leading to suboptimal performance, especially at high sparsity ratios.

Method: TRIM applies varying sparsity ratios to individual output dimensions, guided by quality metrics, and integrates with layer-wise pruning.

Result: TRIM achieves state-of-the-art results, reducing perplexity by 48% for Qwen2.5-14B and over 90% for OPT-13B at 80% sparsity.

Conclusion: Fine-grained, dimension-wise sparsity adaptation is key for extreme LLM compression.

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [105] [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/pdf/2505.16774)
*Yiming Gao, Bin Wang, Chengwei Wei, Shuo Sun, AiTi Aw*

Main category: cs.CL

TL;DR: The paper introduces IFEval-Audio, a dataset to evaluate instruction-following in audio-based LLMs, addressing a gap in multimodal model research.


<details>
  <summary>Details</summary>
Motivation: Instruction-following in audio-based LLMs is underexplored compared to text and vision-language models, despite its importance.

Method: IFEval-Audio, a dataset of 280 audio-instruction-answer triples across six dimensions, is created to benchmark audio LLMs.

Result: The dataset is used to evaluate state-of-the-art audio LLMs, highlighting their instruction-following capabilities.

Conclusion: IFEval-Audio fills a research gap and supports future work in audio-based LLM instruction-following.

Abstract: Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.

</details>


### [106] [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.16782)
*Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, Xiaoyu Shen*

Main category: cs.CL

TL;DR: The paper provides a comprehensive overview of latent Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs), proposing a taxonomy and analyzing methods to improve abstract reasoning efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional CoT relies on explicit natural language steps, causing inefficiencies and limiting abstract reasoning. Latent CoT, reasoning in latent spaces, offers richer cognitive representations and faster inference.

Method: The paper proposes a unified taxonomy from four perspectives (token-wise strategies, internal mechanisms, analysis, applications) and analyzes representative methods.

Result: The analysis highlights design patterns, strengths, and challenges of latent CoT methods, providing a structured foundation for future research.

Conclusion: Latent CoT is a promising direction for advancing LLM reasoning, with ongoing updates and resources available for further exploration.

Abstract: Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.

</details>


### [107] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/pdf/2505.16789)
*Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin*

Main category: cs.CL

TL;DR: The paper explores vulnerabilities in fine-tuned large language models, termed 'Accidental Misalignment,' caused by dataset characteristics, and investigates their correlation with adversarial attack success.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning datasets introduce unexpected vulnerabilities in language models, impacting their adversarial robustness.

Method: Identify correlation factors (linguistic features, semantic similarity, toxicity) in datasets, evaluate adversarial performance of fine-tuned models, and explore causal links.

Result: Findings reveal how dataset factors correlate with adversarial attack success, providing insights into model vulnerabilities.

Conclusion: Highlights the importance of dataset design in maintaining model alignment and suggests implications for adversarial defense strategies.

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [108] [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/pdf/2505.16800)
*Changbing Yang, Garrett Nicolai*

Main category: cs.CL

TL;DR: A transformer-based system for morpheme segmentation uses multitask learning and LLM-generated synthetic data to improve accuracy in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in low-resource languages for morpheme segmentation and gloss prediction.

Method: Jointly predicts segments and glosses using shared linguistic representations and integrates LLM-generated synthetic data.

Result: Significant improvements in word-level segmentation accuracy and morpheme-level F1-score on SIGMORPHON 2023 dataset.

Conclusion: The approach effectively enhances model generalization and performance in low-resource settings.

Abstract: We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.

</details>


### [109] [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/pdf/2505.16806)
*Kexin Zhang, Junlan Chen, Daifeng Li, Yuxuan Zhang, Yangyang Feng, Bowen Deng, Weixu Chen*

Main category: cs.CL

TL;DR: The paper proposes ESA-DGR, a unified framework with two modules (TW-ESA and DGR) to improve LLMs' performance in knowledge-intensive multi-step reasoning tasks by addressing evidence extraction and reasoning challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive multi-step reasoning tasks due to flawed evidence extraction and reasoning under uncertainty.

Method: Introduces TW-ESA for evidence self-alignment and DGR for dual-gated reasoning enhancement, trained collaboratively in ESA-DGR.

Result: ESA-DGR outperforms state-of-the-art methods, achieving 4% EM and 5% F1 score improvements on three datasets.

Conclusion: ESA-DGR effectively enhances LLMs' reasoning by improving evidence alignment and knowledge fusion, demonstrating superior performance.

Abstract: Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.

</details>


### [110] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/pdf/2505.16814)
*Gaurav Kamath, Sowmya Vajjala*

Main category: cs.CL

TL;DR: Synthetic data shows promise for improving NER in low-resource languages, but effectiveness varies across languages.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited labeled training data for NER in low-resource languages.

Method: Exploring synthetic data augmentation for multilingual NER across 11 diverse languages.

Result: Synthetic data helps, but performance varies significantly between languages.

Conclusion: Synthetic data is a viable but inconsistent solution for low-resource NER.

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [111] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/pdf/2505.16831)
*Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du*

Main category: cs.CL

TL;DR: Current unlearning evaluations in LLMs rely on misleading token-level metrics. A new framework reveals reversible vs. irreversible forgetting, showing unlearning may obscure, not erase, data.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of token-level metrics in evaluating unlearning in LLMs, which can falsely indicate forgetting while latent features remain.

Method: Introduces a representation-level evaluation framework using PCA-based similarity, shift, centered kernel alignment, and Fisher information, tested across six unlearning methods, three domains, and two LLMs.

Result: Identifies reversible (token-level collapse but latent retention) and irreversible (deeper damage) forgetting, linking reversibility to task type and hyperparameters.

Conclusion: Highlights a gap in current evaluation practices and provides a toolkit for trustworthy unlearning analysis in LLMs.

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [112] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/pdf/2505.16834)
*Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, Ji-Rong Wen*

Main category: cs.CL

TL;DR: SimpleDeepSearcher is a lightweight framework for retrieval-augmented generation, using strategic data engineering to overcome training limitations and improve deep search systems.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems lack high-quality training data or face distribution mismatches and high computational costs, hindering real-world deployment.

Method: The framework synthesizes high-quality training data by simulating realistic user interactions in live web search environments and uses multi-criteria curation for diversity and quality.

Result: Experiments on five benchmarks show significant improvements over RL-based baselines using only 871 curated samples.

Conclusion: SimpleDeepSearcher demonstrates that SFT can effectively address data scarcity, offering practical insights for efficient deep search systems.

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [113] [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/pdf/2505.16838)
*Yibo Wang, Li Shen, Huanjin Yao, Tiansheng Huang, Rui Liu, Naiqiang Tan, Jiaxing Huang, Kai Zhang, Dacheng Tao*

Main category: cs.CL

TL;DR: R1-Compress is a two-stage chunk-level compression framework for Long-CoT reasoning, reducing token usage by 20% with minimal accuracy drop.


<details>
  <summary>Details</summary>
Motivation: Existing compression methods for Long-CoT either lose local reasoning signals or produce incoherent outputs, prompting the need for a balanced solution.

Method: Segments Long-CoT into chunks, applies LLM-driven inner-chunk compression, and uses inter-chunk search for coherence.

Result: Achieves 92.4% accuracy on MATH500 with only a 0.6% drop from baseline, reducing tokens by ~20%.

Conclusion: R1-Compress effectively balances compression and reasoning accuracy, offering a practical solution for Long-CoT.

Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress

</details>


### [114] [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/pdf/2505.16847)
*Baran Barbarestani, Isa Maks, Piek Vossen*

Main category: cs.CL

TL;DR: The paper presents a method for detecting inappropriate language in online conversations using crowd and expert annotations alongside ChatGPT, focusing on Reddit threads. It compares human and AI annotations, revealing insights into hate speech detection and moderation challenges.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve automated content moderation by understanding the strengths and limitations of human and AI annotations in detecting hate speech and discriminatory language.

Method: The approach involves annotating Reddit comments using a framework that labels targeting categories and words, comparing annotations from experts, crowd workers, and ChatGPT.

Result: Findings show contextual factors are crucial in hate speech detection, uncover new targeting categories (e.g., social belief, body image), and highlight ChatGPT's limitations with nuanced language.

Conclusion: The study offers insights for enhancing online safety through better automated moderation, addressing annotation challenges and AI limitations.

Abstract: This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.

</details>


### [115] [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/pdf/2505.16855)
*Alberto Muñoz-Ortiz, David Vilares, Caio COrro, Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: The paper proposes a sequence labeling method for nested named entity recognition (NNER) by linearizing constituency structures, achieving competitive performance with efficient training.


<details>
  <summary>Details</summary>
Motivation: To simplify the structured prediction problem of NNER by reducing it to token classification, making it more efficient and accessible.

Method: Leverages constituency linearizations with pretrained encoders to capture nested entities using exactly $n$ tagging actions.

Result: Achieves competitive performance compared to less efficient systems.

Conclusion: The method is efficient, trainable with standard sequence labeling tools, and performs well for NNER.

Abstract: We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.

</details>


### [116] [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/pdf/2505.16868)
*Sudhansu Bala Das, Samujjal Choudhury, Tapas Kumar Mishra, Bidyut Kr. Patra*

Main category: cs.CL

TL;DR: The paper explores subword tokenization techniques (SentencePiece, BPE, WordPiece) for Indian languages, finding SentencePiece best for Statistical/Neural MT and BPE for Multilingual MT.


<details>
  <summary>Details</summary>
Motivation: Indian languages' complex morphology and agglutinative structures require effective tokenization for machine translation.

Method: Evaluated tokenizers (SentencePiece, BPE, WordPiece) in Statistical, Neural, and Multilingual MT models using metrics like BLEU, TER, METEOR.

Result: SentencePiece excelled in Statistical/Neural MT, while BPE performed best in Multilingual MT. ILs to English translations outperformed English to ILs.

Conclusion: Tokenization choice impacts MT performance; SentencePiece and BPE are optimal for different MT models, with ILs to English translations being superior.

Abstract: Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.

</details>


### [117] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/pdf/2505.16881)
*Daniel F. Perez-Ramirez, Dejan Kostic, Magnus Boman*

Main category: cs.CL

TL;DR: CASTILLO is a dataset for analyzing response length variability in LLMs, aiding proactive resource allocation and model behavior analysis.


<details>
  <summary>Details</summary>
Motivation: Managing compute resources for LLM inference is challenging due to variable response lengths. Existing methods either bias generation or ignore model-specific variability.

Method: CASTILLO collects response length distributions from 13 LLMs across seven instruction-following corpora, generating 10 completions per prompt-model pair with fixed settings.

Result: Significant variability in response lengths was observed, even under identical settings, revealing model-specific behaviors and partial degeneration.

Conclusion: CASTILLO supports predictive modeling for resource scheduling and provides a framework for analyzing LLM generation behaviors, with publicly released data and code.

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [118] [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/pdf/2505.16869)
*Weixiang Zhao, Yulin Hu, Yang Deng, Tongtong Wu, Wenxuan Zhang, Jiahe Guo, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu*

Main category: cs.CL

TL;DR: MPO improves multilingual safety alignment in LLMs by leveraging dominant language (English) capabilities, outperforming monolingual methods like RLHF and DPO.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods (e.g., RLHF, DPO) are monolingual and ineffective for noisy multilingual data, necessitating a better approach.

Method: MPO minimizes the reward gap between dominant (English) and target languages, transferring safety capabilities without degrading utility.

Result: MPO enhances safety alignment across languages in LLMs (LLaMA-3.1, Gemma-2, Qwen2.5) while maintaining general multilingual performance.

Conclusion: MPO is an effective solution for multilingual safety alignment in LLMs, addressing limitations of current monolingual methods.

Abstract: Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.

</details>


### [119] [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/pdf/2505.16894)
*Zeyu Wei, Shuo Wang, Xiaohui Rong, Xuemin Liu, He Li*

Main category: cs.CL

TL;DR: The paper studies how incremental context injection in LLMs leads to hallucinations, revealing patterns in frequency, semantic assimilation, and attention dynamics.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder reliability, prompting a need to understand their link to internal-state drift from context injection.

Method: Using TruthfulQA, the study constructs titration tracks with relevant/flawed and misleading content, tracking hallucination rates and internal-state drifts across six LLMs.

Result: Hallucination frequency grows monotonically, plateauing after 5-7 rounds. Relevant context causes deep semantic assimilation, while irrelevant context leads to topic-drift errors. Attention-locking thresholds mark irreversible hallucinations.

Conclusion: The findings provide empirical insights for predicting and mitigating hallucinations in LLMs, highlighting the interplay between assimilation capacity and attention diffusion.

Abstract: Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round "titration" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence "self-consistent" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
"attention-locking" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.

</details>


### [120] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/pdf/2505.16900)
*Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.CL

TL;DR: The paper introduces Power-Law Decay Loss (PDL), a novel loss function for text generation finetuning, addressing the overemphasis on high-frequency tokens by re-weighting token contributions based on frequency.


<details>
  <summary>Details</summary>
Motivation: Standard cross-entropy loss treats all tokens equally, causing models to neglect low-frequency, high-information tokens. PDL is motivated by the inverse relationship between token frequency and informativeness.

Method: PDL re-weights tokens in cross-entropy loss using a power-law decay, reducing weights for high-frequency tokens and increasing them for low-frequency ones.

Result: PDL improves text generation by emphasizing informative tokens, enhancing quality, diversity, and specificity.

Conclusion: PDL offers advantages for tasks like summarization, dialogue systems, and style transfer by optimizing finetuning for informativeness.

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [121] [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/pdf/2505.16922)
*Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Dong Yu, Nigel Collier, Deqing Yang*

Main category: cs.CL

TL;DR: UNCLE benchmark evaluates LLMs' uncertainty expression in QA, showing current models fail in long-form generation. Training-based methods improve performance.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating LLMs' ability to express uncertainty in long-form generation, which is prone to hallucination.

Method: Introduce UNCLE benchmark with paired QA instances and propose metrics. Test prompt- and training-based methods.

Result: Current models inadequately express uncertainty in long-form QA. Training-based methods outperform prompt-based ones.

Conclusion: UNCLE highlights alignment gaps and suggests future research directions for improving uncertainty expression in LLMs.

Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.

</details>


### [122] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/pdf/2505.16927)
*Keshav Ramji, Tahira Naseem, Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: The paper proposes an automated method for eliciting and refining latent behavioral attributes in language models to improve response quality, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of behavioral attributes for language models is labor-intensive, prompting the need for an automated approach.

Method: The method uses self-correction and clustering to mine and compress latent principles from the model, employing posterior-regularized Monte Carlo Expectation-Maximization.

Result: Smaller language models (7-8B parameters) improved by +8-10% in AlpacaEval win-rate, +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval.

Conclusion: Automated, principle-driven post-training can enable continual self-improvement in language models.

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [123] [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/pdf/2505.16931)
*Matthew Zent, Digory Smith, Simon Woodhead*

Main category: cs.CL

TL;DR: PIIvot is a lightweight framework for PII anonymization that uses data context to simplify detection, demonstrated with QATD-2k, a large tutoring dataset.


<details>
  <summary>Details</summary>
Motivation: PII anonymization is critical for open-science data sharing but faces challenges in error thresholds and recall/precision trade-offs.

Method: PIIvot leverages data context knowledge to simplify PII detection.

Result: The framework is demonstrated with QATD-2k, a large open-source tutoring dataset.

Conclusion: PIIvot offers a practical solution for PII anonymization, supported by real-world educational data.

Abstract: Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.

</details>


### [124] [In-Context Watermarks for Large Language Models](https://arxiv.org/pdf/2505.16934)
*Yepeng Liu, Xuandong Zhao, Christopher Kruegel, Dawn Song, Yuheng Bu*

Main category: cs.CL

TL;DR: In-Context Watermarking (ICW) embeds watermarks into AI-generated text via prompt engineering, enabling detection without model access, validated in experiments.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for watermarking AI-generated text in real-world settings where model access is restricted, such as detecting dishonest peer reviews.

Method: ICW uses prompt engineering to embed watermarks, leveraging LLMs' in-context learning. Four strategies are explored, each with tailored detection methods, including Indirect Prompt Injection (IPI).

Result: Experiments confirm ICW's feasibility as a model-agnostic watermarking approach, scalable with advancing LLMs.

Conclusion: ICW is a practical, scalable solution for content attribution in AI-generated text, especially as LLMs improve.

Abstract: The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.

</details>


### [125] [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/pdf/2505.16956)
*Daniil Gurgurov, Michal Gregor, Josef van Genabith, Simon Ostermann*

Main category: cs.CL

TL;DR: The paper presents a method for extreme compression of multilingual encoder-only language models for low-resource languages, achieving up to 92% compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To create significantly smaller monolingual models for low-resource languages while retaining essential language-specific knowledge.

Method: Combines two-step knowledge distillation, structured pruning, truncation, and vocabulary trimming to reduce layer depth, feed-forward hidden size, and intermediate layer embedding size.

Result: Achieves compression rates of up to 92% with only a 2-10% performance drop in downstream tasks like sentiment analysis and named entity recognition.

Conclusion: The performance degradation correlates with the teacher model's language-specific data, and the paper provides best practices for multilingual model compression.

Abstract: In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.

</details>


### [126] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/pdf/2505.16965)
*Fengyi Li, Kayhan Behdin, Natesh Pillai, Xiaofeng Wang, Zhipeng Wang, Ercan Yildiz*

Main category: cs.CL

TL;DR: BP-Seg, an unsupervised graphical model-based method, efficiently segments text by considering local coherence and distant semantic similarity, outperforming competitors.


<details>
  <summary>Details</summary>
Motivation: Text segmentation is crucial for downstream applications, requiring methods that handle both local coherence and distant semantic relationships.

Method: BP-Seg uses belief propagation on graphical models to group adjacent and semantically similar sentences.

Result: Outperforms competing approaches on illustrative examples and long-form documents.

Conclusion: BP-Seg is effective for text segmentation by balancing local and global semantic relationships.

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [127] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/pdf/2505.16973)
*Rishanth Rajendhran, Amir Zadeh, Matthew Sarte, Chuan Li, Mohit Iyyer*

Main category: cs.CL

TL;DR: VeriFastScore improves efficiency of long-form factuality evaluation by fine-tuning Llama3.1 8B for simultaneous claim extraction and verification, achieving a 6.6x speedup over VeriScore.


<details>
  <summary>Details</summary>
Motivation: Current methods like FactScore and VeriScore are slow due to numerous LLM calls, limiting scalability.

Method: Fine-tune Llama3.1 8B using synthetic data to extract and verify claims concurrently with Google Search evidence.

Result: VeriFastScore correlates strongly with VeriScore (r=0.80 example, r=0.94 system) and speeds up evaluation by 6.6x.

Conclusion: VeriFastScore offers a scalable solution for factuality evaluation, with released model and datasets for future research.

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


### [128] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/pdf/2505.16983)
*Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen*

Main category: cs.CL

TL;DR: The paper addresses mismatches in adapting batch-oriented LLMs to streaming, focusing on input-attention issues, and introduces a group position encoding method for better consistency without architectural changes.


<details>
  <summary>Details</summary>
Motivation: To adapt LLMs for streaming without expensive re-encoding or specialized architectures, identifying key mismatches and improving performance.

Method: Introduces a group position encoding paradigm to enhance consistency between streaming and batch modes, leveraging batch architectures.

Result: Outperforms existing approaches in cross-lingual and cross-modal tasks, with strong generalization in both streaming and batch modes.

Conclusion: The proposed method effectively addresses streaming adaptation issues without architectural modifications, offering scalable and efficient performance.

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [129] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/pdf/2505.16986)
*Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, Milind Naphade, Genta Indra Winata*

Main category: cs.CL

TL;DR: T1 is a tool-augmented dataset for evaluating LLMs' planning and tool-use coordination in multi-turn conversations with dependencies.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effective planning in multi-turn conversations involving tool dependencies.

Method: Introducing T1, a multi-domain dataset with caching and dynamic replanning support.

Result: T1-Agent demonstrates improved planning and reasoning in tool-dependent scenarios.

Conclusion: T1 serves as a benchmark for evaluating LLMs' tool-use and planning capabilities.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [130] [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/pdf/2505.16995)
*Chao Zhang, Xin Shi, Xueqiao Zhang, Yifan Zhu, Yi Yang, Yawei Luo*

Main category: cs.CL

TL;DR: The paper introduces Inferential Preference Mining (IPM) and a Decoupled ESC framework to improve emotional support conversation by addressing challenges in preference learning and optimization ambiguity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Emotional Support Conversation (ESC) using LLMs still suffer from psychological errors, and Direct Preference Optimization (DPO) struggles with entangled data and ambiguous objectives.

Method: Proposes IPM to create high-quality preference data (IPM-PrefDial) and a Decoupled ESC framework, splitting the task into strategy planning and response generation, trained via SFT and enhanced by DPO.

Result: The Decoupled ESC framework outperforms joint optimization baselines, reducing bias and improving response quality.

Conclusion: The approach effectively addresses challenges in ESC by decoupling tasks and leveraging IPM, leading to better performance and alignment with psychological preferences.

Abstract: Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.

</details>


### [131] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/pdf/2505.16998)
*Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, Xunliang Cai, Liangcai Gao*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' logical reasoning using formal languages, finding Thinking models outperform Instruct models, LLMs struggle with inductive reasoning, and PoT format data generalizes best. It also enhances small models with formal-relative training.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs' logical reasoning capabilities using formal languages, addressing gaps in existing research.

Method: Comprehensive evaluation across LLMs, task taxonomies, and trajectory formats, plus formal-relative training data curation and fine-tuning.

Result: Thinking models excel, LLMs lack inductive reasoning, PoT format generalizes best, and fine-tuning improves performance.

Conclusion: Formal language enhances LLMs' reasoning, but inductive reasoning remains a challenge; fine-tuning with formal data improves generalization.

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [132] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.17005)
*Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen*

Main category: cs.CL

TL;DR: R1-Searcher++ is a framework for training LLMs to use both internal and external knowledge adaptively, improving retrieval-augmented reasoning.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current RAG methods (cost, poor generalization, ignoring internal knowledge) by enhancing LLMs' ability to leverage both knowledge sources.

Method: Two-stage training: SFT Cold-start for format learning, followed by RL for dynamic knowledge acquisition with outcome-supervision, reward mechanisms, and memorization.

Result: Outperforms previous RAG and reasoning methods, achieving efficient retrieval.

Conclusion: R1-Searcher++ effectively combines internal and external knowledge, enhancing LLM performance in retrieval-augmented tasks.

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


### [133] [Language Models are Universal Embedders](https://arxiv.org/pdf/2310.08232)
*Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang*

Main category: cs.CL

TL;DR: The paper proposes using pre-trained multilingual LLMs like BLOOM to create unified embedding models for diverse tasks and languages, demonstrating their effectiveness even without task-specific data.


<details>
  <summary>Details</summary>
Motivation: To address the need for versatile embedding models across languages and tasks, avoiding the inefficiency of dedicated models for each scenario.

Method: Proposes straightforward strategies for constructing embedders using pre-trained multilingual LLMs and introduces a universal evaluation benchmark.

Result: The trained model performs well across languages and tasks, including those without finetuning/pretraining data.

Conclusion: Encourages development of robust open-source universal embedders, highlighting the potential of multilingual LLMs for unified embeddings.

Abstract: In the large language model (LLM) revolution, embedding is a key component of
various systems, such as retrieving knowledge or memories for LLMs or building
content moderation filters. As such cases span from English to other natural or
programming languages, from retrieval to classification and beyond, it is
advantageous to build a unified embedding model rather than dedicated ones for
each scenario. In this context, the pre-trained multilingual decoder-only large
language models, e.g., BLOOM, emerge as a viable backbone option. To assess
their potential, we propose straightforward strategies for constructing
embedders and introduce a universal evaluation benchmark. Experimental results
show that our trained model is proficient at generating good embeddings across
languages and tasks, even extending to languages and tasks for which no
finetuning/pretraining data is available. We also present detailed analyses and
additional evaluations. We hope that this work could encourage the development
of more robust open-source universal embedders.

</details>


### [134] [Large Language Models are Miscalibrated In-Context Learners](https://arxiv.org/pdf/2312.13772)
*Chengzu Li, Han Zhou, Goran Glavaš, Anna Korhonen, Ivan Vulić*

Main category: cs.CL

TL;DR: The paper investigates whether instruction-tuned language models (LMs) achieve well-calibrated results in low-resource setups without overconfidence. It analyzes performance and calibration across learning methods, identifies miscalibration issues, and proposes self-ensembling as a solution.


<details>
  <summary>Details</summary>
Motivation: To understand if instruction-tuned LMs avoid overconfidence (miscalibration) in limited data scenarios, given their strong instruction-following ability.

Method: Conducts controlled experiments to evaluate performance and calibration across learning methods, then explores self-ensembling (e.g., varying prompts or examples) to improve both.

Result: Miscalibration exists in low-resource setups for all methods. Self-ensembling, especially with max probability, enhances calibration and performance.

Conclusion: The study highlights calibration issues in ICL despite task performance gains, recommends learning paradigms based on data familiarity, and suggests self-ensembling as a practical solution.

Abstract: When adapting ICL with or without fine-tuning, we are curious about whether
the instruction-tuned language model is able to achieve well-calibrated results
without suffering from the problem of overconfidence (i.e., miscalibration)
considering its strong instruction following ability, especially in such
limited data setups. In this work, we deliver an in-depth analysis of the
behavior across different choices of learning methods from the perspective of
both performance and calibration. Through extensive controlled experiments, we
observe that the miscalibration problem exists across all learning methods in
low-resource setups. To achieve simultaneous gain for both in-task performance
and calibration, we then study the potential of self-ensembling applied at
different modeling stages (e.g., variations of in-context examples or
variations in prompts or different ensembling strategies) to make the
predictions more calibrated and have comparable or even better performance. We
find that self-ensembling with max probability produces robust and calibrated
predictions. Our work reveals the potential calibration problem of using ICL
despite the improvements in task performance and sheds light on which learning
paradigm to choose. We also provide practical guidelines for choosing learning
paradigms depending on whether the data has been seen by the model before and a
worthwhile solution via self-ensembling on how to enhance both task performance
and calibration of LMs, which we hope could encourage further study.

</details>


### [135] [EntGPT: Entity Linking with Generative Large Language Models](https://arxiv.org/pdf/2402.06738)
*Yifan Ding, Amrit Poudel, Qingkai Zeng, Tim Weninger, Balaji Veeramani, Sanmitra Bhattacharya*

Main category: cs.CL

TL;DR: EntGPT improves entity linking (EL) tasks using advanced prompt engineering, outperforming traditional methods and naive prompts with significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: Traditional EL methods are complex, hard to train, and lack transferability. Generative LLMs like GPT underperform with naive prompts, prompting the need for better approaches.

Method: Introduces EntGPT with two methods: hard-prompting (EntGPT-P) and instruction tuning (EntGPT-I). EntGPT-P uses a three-step hard-prompting technique, while EntGPT-I involves supervised fine-tuning.

Result: EntGPT-P boosts micro-F_1 scores by up to 36% over vanilla prompts. EntGPT-I improves scores by 2.1% on average in supervised tasks and outperforms baselines in QA tasks.

Conclusion: EntGPT offers a scalable and effective solution for EL tasks, compatible with various LLMs, with open-source code and data available.

Abstract: Entity Linking in natural language processing seeks to match text entities to
their corresponding entries in a dictionary or knowledge base. Traditional
approaches rely on contextual models, which can be complex, hard to train, and
have limited transferability across different domains. Generative large
language models like GPT offer a promising alternative but often underperform
with naive prompts. In this study, we introduce EntGPT, employing advanced
prompt engineering to enhance EL tasks. Our three-step hard-prompting method
(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla
prompts, achieving competitive performance across 10 datasets without
supervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)
improves micro-F_1 scores by 2.1% on average in supervised EL tasks and
outperforms several baseline models in six Question Answering tasks. Our
methods are compatible with both open-source and proprietary LLMs. All data and
code are available on GitHub at https://github.com/yifding/In_Context_EL.

</details>


### [136] [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/pdf/2403.10056)
*Yongquan He, Wenyuan Zhang, Xuancheng Huang, Peng Zhang*

Main category: cs.CL

TL;DR: A novel method (KPIG) for continual instruction tuning in LLMs addresses catastrophic forgetting by dynamically replaying data and refining training objectives, improving generalization and instruction-following abilities.


<details>
  <summary>Details</summary>
Motivation: To mitigate catastrophic forgetting in continual instruction tuning for LLMs, which degrades previously learned abilities.

Method: Uses Key-part Information Gain (KPIG) to dynamically replay data and refine training objectives, focusing on task-aware information. Introduces P-score and V-score metrics.

Result: Superior performance on both seen and held-out tasks, with improved generalization and instruction-following.

Conclusion: KPIG effectively alleviates catastrophic forgetting and enhances LLM performance in continual instruction tuning.

Abstract: Instruction tuning for large language models (LLMs) can drive them to produce
results consistent with human goals in specific downstream tasks. However, the
process of continual instruction tuning (CIT) for LLMs may bring about the
catastrophic forgetting (CF) problem, where previously learned abilities are
degraded. Recent methods try to alleviate the CF problem by modifying models or
replaying data, which may only remember the surface-level pattern of
instructions and get confused on held-out tasks. In this paper, we propose a
novel continual instruction tuning method based on Key-part Information Gain
(KPIG). Our method computes the information gain on masked parts to dynamically
replay data and refine the training objective, which enables LLMs to capture
task-aware information relevant to the correct response and alleviate
overfitting to general descriptions in instructions. In addition, we propose
two metrics, P-score and V-score, to measure the generalization and
instruction-following abilities of LLMs. Experiments demonstrate our method
achieves superior performance on both seen and held-out tasks.

</details>


### [137] [Red-Teaming for Inducing Societal Bias in Large Language Models](https://arxiv.org/pdf/2405.04756)
*Chu Fei Luo, Ahmad Ghawanmeh, Bharat Bhimshetty, Kashyap Murali, Murli Jadhav, Xiaodan Zhu, Faiza Khan Khattak*

Main category: cs.CL

TL;DR: The paper introduces two bias-specific red-teaming methods, EBP and BiasKG, to evaluate social bias in AI models, revealing that even models with safety guardrails exhibit increased bias.


<details>
  <summary>Details</summary>
Motivation: The need to address social bias in AI systems, especially in high-stakes industry settings, where biased outputs pose operational, reputational, and regulatory risks.

Method: Proposes Emotional Bias Probe (EBP) and BiasKG, which refactor stereotypes into a knowledge graph, to induce and evaluate biased responses in language models.

Result: The methods increase bias in all tested models, including those with safety guardrails, highlighting the inadequacy of current safety measures for social bias.

Conclusion: The study underscores the importance of rigorous evaluation for societal bias in AI and recommends improved safety measures for industry deployments.

Abstract: Ensuring the safe deployment of AI systems is critical in industry settings
where biased outputs can lead to significant operational, reputational, and
regulatory risks. Thorough evaluation before deployment is essential to prevent
these hazards. Red-teaming addresses this need by employing adversarial attacks
to develop guardrails that detect and reject biased or harmful queries,
enabling models to be retrained or steered away from harmful outputs. However,
most red-teaming efforts focus on harmful or unethical instructions rather than
addressing social bias, leaving this critical area under-explored despite its
significant real-world impact, especially in customer-facing systems. We
propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and
BiasKG, to evaluate how standard safety measures for harmful content affect
bias. For BiasKG, we refactor natural language stereotypes into a knowledge
graph. We use these attacking strategies to induce biased responses from
several open- and closed-source language models. Unlike prior work, these
methods specifically target social bias. We find our method increases bias in
all models, even those trained with safety guardrails. Our work emphasizes
uncovering societal bias in LLMs through rigorous evaluation, and recommends
measures ensure AI safety in high-stakes industry deployments.

</details>


### [138] [BlockPruner: Fine-grained Pruning for Large Language Models](https://arxiv.org/pdf/2406.10594)
*Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, Liangzhi Li*

Main category: cs.CL

TL;DR: BlockPruner is a training-free structured pruning method for LLMs, targeting redundancies in MHA and MLP blocks for finer-grained pruning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The high costs of LLM training and inference, coupled with overlooked finer-grained redundancies in existing pruning methods, drive the need for more granular pruning.

Method: BlockPruner segments Transformer layers into MHA and MLP blocks, evaluates block importance using perplexity, and uses heuristic search for iterative pruning.

Result: BlockPruner achieves more granular and effective pruning across various LLM sizes and architectures, validated by downstream task performance.

Conclusion: BlockPruner offers a superior, training-free approach to pruning LLMs by addressing finer-grained redundancies, enhancing efficiency without compromising performance.

Abstract: With the rapid growth in the size and complexity of large language models
(LLMs), the costs associated with their training and inference have escalated
significantly. Research indicates that certain layers in LLMs harbor
substantial redundancy, and pruning these layers has minimal impact on the
overall performance. While various layer pruning methods have been developed
based on this insight, they generally overlook the finer-grained redundancies
within the layers themselves. In this paper, we delve deeper into the
architecture of LLMs and demonstrate that finer-grained pruning can be achieved
by targeting redundancies in multi-head attention (MHA) and multi-layer
perceptron (MLP) blocks. We propose a novel, training-free structured pruning
approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner
segments each Transformer layer into MHA and MLP blocks. It then assesses the
importance of these blocks using perplexity measures and applies a heuristic
search for iterative pruning. We applied BlockPruner to LLMs of various sizes
and architectures and validated its performance across a wide range of
downstream tasks. Experimental results show that BlockPruner achieves more
granular and effective pruning compared to state-of-the-art baselines.

</details>


### [139] [Determination of language families using deep learning](https://arxiv.org/pdf/2409.02393)
*Peter B. Lerner*

Main category: cs.CL

TL;DR: A c-GAN neural network analyzes transliterated text fragments of dead languages, including undeciphered ones like Cypro-Minoan, to identify linguistic affinities, potentially aiding future decipherment.


<details>
  <summary>Details</summary>
Motivation: To explore linguistic connections between dead languages without relying on translation or deciphering, with potential applications in decipherment using advanced neural networks.

Method: Uses a convolutional generative adversarial network (c-GAN) to analyze transliterated texts from dead languages, focusing on linguistic patterns.

Result: Identifies linguistic affinities among the analyzed languages, though decipherment is not directly achieved.

Conclusion: The approach shows promise for future decipherment efforts with more advanced neural network techniques.

Abstract: We use a c-GAN (convolutional generative adversarial) neural network to
analyze transliterated text fragments of extant, dead comprehensible, and one
dead non-deciphered (Cypro-Minoan) language to establish linguistic affinities.
The paper is agnostic with respect to translation and/or deciphering. However,
there is hope that the proposed approach can be useful for decipherment with
more sophisticated neural network techniques.

</details>


### [140] [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/pdf/2409.02813)
*Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, Graham Neubig*

Main category: cs.CL

TL;DR: MMMU-Pro is an enhanced version of the MMMU benchmark, designed to rigorously test multimodal models' understanding and reasoning by filtering text-only answerable questions, augmenting options, and introducing vision-only inputs. Results show lower performance on MMMU-Pro, highlighting the need for better multimodal integration.


<details>
  <summary>Details</summary>
Motivation: To create a more robust benchmark that accurately assesses multimodal models' ability to integrate visual and textual information, mimicking real-world cognitive tasks.

Method: A three-step process: (1) filtering text-only answerable questions, (2) augmenting candidate options, and (3) introducing vision-only inputs where questions are embedded in images.

Result: Model performance drops significantly (16.8% to 26.9%) on MMMU-Pro compared to MMMU. OCR prompts have minimal impact, while Chain of Thought reasoning improves performance.

Conclusion: MMMU-Pro offers a more rigorous evaluation tool for multimodal AI, aligning with real-world scenarios and guiding future research.

Abstract: This paper introduces MMMU-Pro, a robust version of the Massive
Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.
MMMU-Pro rigorously assesses multimodal models' true understanding and
reasoning capabilities through a three-step process based on MMMU: (1)
filtering out questions answerable by text-only models, (2) augmenting
candidate options, and (3) introducing a vision-only input setting where
questions are embedded within images. This setting challenges AI to truly "see"
and "read" simultaneously, testing a fundamental human cognitive skill of
seamlessly integrating visual and textual information. Results show that model
performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%
to 26.9% across models. We explore the impact of OCR prompts and Chain of
Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT
generally improves performance. MMMU-Pro provides a more rigorous evaluation
tool, closely mimicking real-world scenarios and offering valuable directions
for future research in multimodal AI.

</details>


### [141] [Normal forms in Virus Machines](https://arxiv.org/pdf/2409.03327)
*A. Ramírez-de-Arellano, F. G. C. Cabarle, D. Orellana-Martín, M. J. Pérez-Jiménez*

Main category: cs.CL

TL;DR: The paper explores the computational power of virus machines (VMs) by introducing normal forms that restrict features like host count, instructions, and virus objects, leading to new characterizations of sets like finite, semilinear, and recursively enumerable sets.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of VMs' computational capabilities by defining normal forms that limit specific model features.

Method: Introduces normal forms restricting VM features (hosts, instructions, virus objects) and analyzes their impact on computational power.

Result: Proves new characterizations of sets (finite, semilinear, recursively enumerable) through these normal forms.

Conclusion: Normal forms provide insights into VMs' computational limits and expand theoretical understanding of their power.

Abstract: In the present work, we further study the computational power of virus
machines (VMs in short).VMs provide a computing paradigm inspired by the
transmission and replication networks of viruses.VMs consist of process units
(called hosts) structured by a directed graph whose arcs are called channels
and an instruction graph that controls the transmissions of virus objects among
hosts. The present work complements our understanding of the computing power of
VMs by introducing normal forms; these expressions restrict the features in a
given computing model.Some of the features that we restrict in our normal forms
include (a) the number of hosts, (b) the number of instructions, and (c) the
number of virus objects in each host. After we recall some known results on the
computing power of VMs we give our series of normal forms, such as the size of
the loops in the network, proving new characterisations of family of sets, such
as finite sets, semilinear sets, or recursively enumerable sets (NRE).

</details>


### [142] [LangSAMP: Language-Script Aware Multilingual Pretraining](https://arxiv.org/pdf/2409.18199)
*Yihong Liu, Haotian Ye, Chunlan Ma, Mingyang Wang, Hinrich Schütze*

Main category: cs.CL

TL;DR: LangSAMP enhances multilingual pretraining by incorporating language and script embeddings, improving language neutrality and crosslingual transfer performance.


<details>
  <summary>Details</summary>
Motivation: Multilingual pretrained models often lack language embeddings, burdening token representations and hindering language neutrality.

Method: LangSAMP integrates language and script embeddings into Transformer outputs before prediction, applied to XLM-R pretraining on 500+ languages.

Result: LangSAMP outperforms baselines in zero-shot crosslingual transfer and improves language-neutral representations, as shown by cosine similarity.

Conclusion: Language and script embeddings enhance representation learning and aid in selecting better source languages for transfer.

Abstract: Recent multilingual pretrained language models (mPLMs) often avoid using
language embeddings -- learnable vectors assigned to individual languages.
However, this places a significant burden on token representations to encode
all language-specific information, which may hinder language neutrality. To
address this limitation, we propose Language-Script Aware Multilingual
Pretraining (LangSAMP), a method that incorporates both language and script
embeddings to enhance representation learning. Specifically, we integrate these
embeddings into the output of the Transformer blocks before passing the final
representations to the language modeling head for prediction. We apply LangSAMP
to the continual pretraining of XLM-R on a highly multilingual corpus covering
more than 500 languages. The resulting model consistently outperforms the
baseline in zero-shot crosslingual transfer across diverse downstream tasks.
Extensive analysis reveals that language and script embeddings capture
language- and script-specific nuances, which benefits more language-neutral
representations, proven by improved pairwise cosine similarity. In our case
study, we also show that language and script embeddings can be used to select
better source languages for crosslingual transfer. We make our code and models
publicly available at https://github.com/cisnlp/LangSAMP.

</details>


### [143] [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/pdf/2410.05254)
*Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz*

Main category: cs.CL

TL;DR: The paper introduces a benchmark for studying LLMs in strategic interactions, evaluating their rationality, performance, and outcomes in language-based games.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' behavior in economic interactions and their societal implications when integrated into real-world systems.

Method: Developed a benchmark with three game families, an open-source framework for simulation, and datasets of LLM vs. LLM and human vs. LLM interactions.

Result: Market parameters and LLM choice have complex, interdependent effects on economic outcomes, requiring careful ecosystem design.

Conclusion: The framework and dataset enable systematic study of LLMs in strategic contexts, highlighting the need for nuanced analysis in language-based economic systems.

Abstract: Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? How do they
perform compared to humans? Do they tend to reach an efficient and fair
outcome? What is the role of natural language in strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. To answer these questions, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents in various economic contexts; (ii) evaluate agents
in both individual and collective performance measures; and (iii) quantify the
effect of the economic characteristics of the environments on the behavior of
agents. Our results suggest that the market parameters, as well as the choice
of the LLMs, tend to have complex and interdependent effects on the economic
outcome, which calls for careful design and analysis of the language-based
economic ecosystem.

</details>


### [144] [Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering](https://arxiv.org/pdf/2410.08085)
*Yuan Sui, Yufei He, Zifeng Ding, Bryan Hooi*

Main category: cs.CL

TL;DR: OKGQA is a new benchmark for evaluating LLMs augmented with KGs in open-ended, real-world QA, addressing gaps in existing benchmarks and assessing KG impact on hallucination reduction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on closed-ended tasks, lacking evaluation in complex, real-world scenarios and thorough assessment of KGs' role in reducing LLM hallucinations.

Method: Introduces OKGQA for open-ended QA, with diverse question types and metrics for hallucination and reasoning. Proposes OKGQA-P to test KG robustness under perturbations.

Result: Aims to show if KGs enhance LLM trustworthiness in open-ended settings and provide insights for method design.

Conclusion: OKGQA facilitates comprehensive performance comparison and encourages KG-LLM integration to mitigate hallucinations and improve trustworthiness.

Abstract: Recent works integrating Knowledge Graphs (KGs) have shown promising
improvements in enhancing the reasoning capabilities of Large Language Models
(LLMs). However, existing benchmarks primarily focus on closed-ended tasks,
leaving a gap in evaluating performance on more complex, real-world scenarios.
This limitation also hinders a thorough assessment of KGs' potential to reduce
hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark
specifically designed to evaluate LLMs augmented with KGs in open-ended,
real-world question answering settings. OKGQA reflects practical complexities
through diverse question types and incorporates metrics to quantify both
hallucination rates and reasoning improvements in LLM+KG models. To consider
the scenarios in which KGs may contain varying levels of errors, we propose a
benchmark variant, OKGQA-P, to assess model performance when the semantics and
structure of KGs are deliberately perturbed and contaminated. In this paper, we
aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended
setting, and (2) conduct a comparative analysis to shed light on method design.
We believe this study can facilitate a more complete performance comparison and
encourages continuous improvement in integrating KGs with LLMs to mitigate
hallucination, and make LLMs more trustworthy. Code and data are released at
https://github.com/Y-Sui/OKGQA.

</details>


### [145] [Keys to Robust Edits: from Theoretical Insights to Practical Advances](https://arxiv.org/pdf/2410.09338)
*Jianhao Yan, Futing Wang, Yun Luo, Yafu Li, Yue Zhang*

Main category: cs.CL

TL;DR: The paper introduces Robust Edit Pathway (REP), a plug-and-play module to improve the robustness and specificity of knowledge editing in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with maintaining accurate knowledge due to conflicting or outdated parametric memories, and existing locate-and-edit methods fail in long-context reasoning and paraphrased queries.

Method: The authors propose REP, which disentangles editing keys from native model representations and dynamically adjusts keys via contrastive learning to balance robustness and specificity.

Result: Experiments show REP improves success rates in robustness tests by up to 66.4% without affecting the original success rate.

Conclusion: REP effectively addresses the limitations of locate-and-edit methods, providing a robust solution for knowledge editing in LLMs.

Abstract: Large language models (LLMs) struggle with maintaining accurate knowledge due
to conflicting/outdated parametric memories. While locate-and-edit methods
address this, their reliance on models' internal representations leads to
robustness failures in long-context reasoning and paraphrased queries. We
identify a fundamental limitation of locate-and-edit methods: existing semantic
keys (for memory localization) cannot simultaneously satisfy robustness
(context-invariant activation) and specificity (precise knowledge
discrimination). Through theoretical error-bound analysis, we establish formal
criteria for effective editing. Our solution introduces \textit{Robust Edit
Pathway (REP)}, a plug-and-play module that: (1) disentangles editing keys from
native model representations; (2) dynamically adjusts keys via contrastive
learning to achieve robustness-specificity balance. Extensive experiments
across various editing methods (ROME/MEMIT/R-ROME/EMMET), existing LLMs
(LLaMA2, QWen, Mistral), and datasets (CounterFact, ZsRE) show that REP
improves success rate over robustness tests by up-to 66.4\% while maintaining
the success rate unaffected. Our code can be found at
https://github.com/ElliottYan/RobustKeyEdit .

</details>


### [146] [A Unified Approach to Routing and Cascading for LLMs](https://arxiv.org/pdf/2410.10347)
*Jasper Dekoninck, Maximilian Baader, Martin Vechev*

Main category: cs.CL

TL;DR: The paper introduces 'cascade routing,' a unified framework combining routing and cascading for optimal model selection in LLMs, addressing limitations of existing strategies.


<details>
  <summary>Details</summary>
Motivation: Existing model selection strategies lack optimality proofs, fail to identify effective conditions, and cannot combine routing and cascading.

Method: Derives an optimal cascading strategy, proves routing optimality, and proposes cascade routing, a unified framework.

Result: Cascade routing outperforms individual approaches; quality estimators are critical for success.

Conclusion: Cascade routing is a superior, theoretically optimal strategy for model selection in LLMs.

Abstract: The availability of a wide range of large language models (LLMs) embedded in
various agentic systems has significantly increased the potential of model
selection strategies to improve the cost-performance tradeoff. Existing
strategies involve either routing, where a single model is chosen per query, or
cascading, which sequentially runs increasingly larger models until a
satisfactory answer is found. However, current approaches face three key
limitations: they (1) lack formal proofs of optimality, (2) fail to identify
the conditions under which these strategies are most effective to improve the
cost-performance tradeoff, and (3) are unable to combine both paradigms for
further improvements. To address these issues, we first derive a novel optimal
strategy for cascading and prove the optimality of an existing routing
strategy. Further, we propose cascade routing, a unified framework that
integrates routing and cascading into a theoretically optimal strategy. Through
our analysis, we identify good quality estimators as the critical factor for
the success of model selection paradigms. Finally, in our experiments, we show
that cascade routing consistently outperforms the individual approaches by a
large margin and we analyze quality estimators to determine when routing and/or
cascading are useful paradigms for model selection.

</details>


### [147] [Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination](https://arxiv.org/pdf/2410.17477)
*Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar*

Main category: cs.CL

TL;DR: The paper explores how different LLM architectures (e.g., recurrent models vs. self-attention) influence hallucination tendencies, revealing architecture-specific patterns in hallucination occurrence and ease of induction.


<details>
  <summary>Details</summary>
Motivation: To address the dual concerns of LLM hallucinations and computational limitations, and to understand how architectural changes impact hallucination risks.

Method: An extensive evaluation of how architecture-based inductive biases affect hallucination propensity in LLMs.

Result: Hallucination is a general phenomenon, but its occurrence and inducibility vary significantly by model architecture.

Conclusion: The findings emphasize the need for a combined understanding of architectural and hallucination issues, and for universal techniques to mitigate hallucinations.

Abstract: The growth in prominence of large language models (LLMs) in everyday life can
be largely attributed to their generative abilities, yet some of this is also
owed to the risks and costs associated with their use. On one front is their
tendency to hallucinate false or misleading information, limiting their
reliability. On another is the increasing focus on the computational
limitations associated with traditional self-attention based LLMs, which has
brought about new alternatives, in particular recurrent models, meant to
overcome them. Yet it remains uncommon to consider these two concerns
simultaneously. Do changes in architecture exacerbate/alleviate existing
concerns about hallucinations? Do they affect how and where they occur? Through
an extensive evaluation, we study how these architecture-based inductive biases
affect the propensity to hallucinate. While hallucination remains a general
phenomenon not limited to specific architectures, the situations in which they
occur and the ease with which specific types of hallucinations can be induced
can significantly differ based on the model architecture. These findings
highlight the need for better understanding both these problems in conjunction
with each other, as well as consider how to design more universal techniques
for handling hallucinations.

</details>


### [148] [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/pdf/2410.22316)
*Xinyu Zhao, Fangcong Yin, Greg Durrett*

Main category: cs.CL

TL;DR: Synthetic context extension for LLMs improves long-context tasks but falls short of real data. Retrieval heads explain performance gaps.


<details>
  <summary>Details</summary>
Motivation: To understand how synthetic data fine-tuning imparts abilities for long-context tasks and identify performance gaps.

Method: Fine-tuning LLMs on synthetic data with varied realism and diversity, analyzing retrieval heads and their impact.

Result: Models trained on synthetic data underperform but retrieval heads correlate with performance and explain gaps.

Conclusion: Retrieval heads are key to interpreting synthetic data performance; insights guide better synthetic data creation.

Abstract: Long-context LLMs are increasingly in demand for applications such as
retrieval-augmented generation. To defray the cost of pretraining LLMs over
long contexts, recent work takes an approach of synthetic context extension:
fine-tuning LLMs with synthetically generated long-context data in a
post-training stage. However, it remains unclear how and why this synthetic
context extension imparts abilities for downstream long-context tasks. In this
paper, we investigate fine-tuning on synthetic data for three long-context
tasks that require retrieval and reasoning. We vary the realism of "needle"
concepts to be retrieved and diversity of the surrounding "haystack" context,
from using LLMs to construct synthetic documents to using templated relations
and creating symbolic datasets. We find that models trained on synthetic data
fall short of the real data, but surprisingly, the mismatch can be interpreted
and even predicted in terms of a special set of attention heads that are
responsible for retrieval over long context, retrieval heads (Wu et al., 2024).
The retrieval heads learned on synthetic data have high overlap with retrieval
heads learned on real data, and there is a strong correlation between the
recall of heads learned and the downstream performance of a model. Furthermore,
with attention knockout and activation patching, we mechanistically show that
retrieval heads are necessary and explain model performance, although they are
not totally sufficient. Our results shed light on how to interpret synthetic
data fine-tuning performance and how to approach creating better data for
learning real-world capabilities over long contexts.

</details>


### [149] [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/pdf/2410.22394)
*Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, Wenpeng Yin*

Main category: cs.CL

TL;DR: AAAR-1.0 is a benchmark dataset for evaluating LLMs in research tasks like equation inference, experiment design, paper weakness identification, and review critique. It highlights LLMs' potential and limitations in expert research tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks don't address the unique challenges of using LLMs for research tasks like brainstorming, experiment design, and paper review. AAAR-1.0 fills this gap by focusing on expertise-intensive research activities.

Method: The study introduces AAAR-1.0, a dataset evaluating LLMs in four research tasks: EquationInference, ExperimentDesign, PaperWeakness, and REVIEWCRITIQUE. It tests open-source and proprietary LLMs.

Result: Evaluation shows LLMs have potential but also limitations in handling sophisticated research tasks.

Conclusion: AAAR-1.0 is a step toward better benchmarking LLMs for research, with plans for future iterations.

Abstract: Numerous studies have assessed the proficiency of AI systems, particularly
large language models (LLMs), in facilitating everyday tasks such as email
writing, question answering, and creative content generation. However,
researchers face unique challenges and opportunities in leveraging LLMs for
their own work, such as brainstorming research ideas, designing experiments,
and writing or reviewing papers. In this study, we introduce AAAR-1.0, a
benchmark dataset designed to evaluate LLM performance in three fundamental,
expertise-intensive research tasks: (i) EquationInference, assessing the
correctness of equations based on the contextual information in paper
submissions; (ii) ExperimentDesign, designing experiments to validate research
ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper
submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews
is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:
first, it is explicitly research-oriented, with tasks requiring deep domain
expertise; second, it is researcher-oriented, mirroring the primary activities
that researchers engage in on a daily basis. An evaluation of both open-source
and proprietary LLMs reveals their potential as well as limitations in
conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new
versions.

</details>


### [150] [Graph-based Confidence Calibration for Large Language Models](https://arxiv.org/pdf/2411.02454)
*Yukun Li, Sijia Wang, Lifu Huang, Li-Ping Liu*

Main category: cs.CL

TL;DR: Proposes using an auxiliary model and GNN to estimate LLM response confidence via self-consistency of outputs.


<details>
  <summary>Details</summary>
Motivation: Enhancing trustworthiness of LLMs in high-stakes scenarios by improving confidence estimation.

Method: Builds a consistency graph from multiple LLM outputs and uses a GNN to estimate response correctness.

Result: Strong calibration performance on benchmarks and good generalization to out-of-domain cases.

Conclusion: The method effectively improves confidence estimation in LLMs, aiding reliability.

Abstract: Reliable confidence estimation is essential for enhancing the trustworthiness
of large language models (LLMs), especially in high-stakes scenarios. Despite
its importance, accurately estimating confidence in LLM responses remains a
significant challenge. In this work, we propose using an auxiliary learning
model to assess response correctness based on the self-consistency of multiple
outputs generated by the LLM. Our method builds a consistency graph to
represent the agreement among multiple responses and uses a graph neural
network (GNN) to estimate the likelihood that each response is correct.
Experiments demonstrate that this method has strong calibration performance on
various benchmark datasets and generalizes well to out-of-domain cases.

</details>


### [151] [Prompt-Guided Internal States for Hallucination Detection of Large Language Models](https://arxiv.org/pdf/2411.04847)
*Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu*

Main category: cs.CL

TL;DR: PRISM improves cross-domain hallucination detection in LLMs by using prompt-guided internal states, enhancing generalization without needing out-of-domain data.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce factually incorrect responses (hallucinations), and existing detectors trained on specific domains fail to generalize well.

Method: Proposes PRISM, a framework using prompts to modify LLM internal states, making truthfulness structures more consistent across domains.

Result: Experiments show PRISM significantly boosts cross-domain performance of existing hallucination detectors.

Conclusion: PRISM effectively enhances generalization of hallucination detection methods using only in-domain data.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks in different domains. However, they sometimes generate
responses that are logically coherent but factually incorrect or misleading,
which is known as LLM hallucinations. Data-driven supervised methods train
hallucination detectors by leveraging the internal states of LLMs, but
detectors trained on specific domains often struggle to generalize well to
other domains. In this paper, we aim to enhance the cross-domain performance of
supervised detectors with only in-domain data. We propose a novel framework,
prompt-guided internal states for hallucination detection of LLMs, namely
PRISM. By utilizing appropriate prompts to guide changes to the structure
related to text truthfulness in LLMs' internal states, we make this structure
more salient and consistent across texts from different domains. We integrated
our framework with existing hallucination detection methods and conducted
experiments on datasets from different domains. The experimental results
indicate that our framework significantly enhances the cross-domain
generalization of existing hallucination detection methods.

</details>


### [152] [Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings](https://arxiv.org/pdf/2412.01031)
*Razi Mahmood, Pingkun Yan, Diego Machado Reyes, Ge Wang, Mannudeep K. Kalra, Parisa Kaviani, Joy T. Wu, Tanveer Syeda-Mahmood*

Main category: cs.CL

TL;DR: A new method for evaluating generative AI chest radiograph reports combines fine-grained textual findings with visual localization, outperforming text-only metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for assessing AI-generated chest radiograph reports rely solely on textual information, lacking integration with visual data for comprehensive evaluation.

Method: Extracts fine-grained finding patterns (location, laterality, severity), performs phrasal grounding to localize findings on images, and combines textual and visual measures for quality rating.

Result: The method shows robustness and sensitivity to factual errors, outperforming text-only metrics on a MIMIC-derived dataset.

Conclusion: Combining textual and visual measures provides a more accurate and comprehensive evaluation of AI-generated radiograph reports.

Abstract: Several evaluation metrics have been developed recently to automatically
assess the quality of generative AI reports for chest radiographs based only on
textual information using lexical, semantic, or clinical named entity
recognition methods. In this paper, we develop a new method of report quality
evaluation by first extracting fine-grained finding patterns capturing the
location, laterality, and severity of a large number of clinical findings. We
then performed phrasal grounding to localize their associated anatomical
regions on chest radiograph images. The textual and visual measures are then
combined to rate the quality of the generated reports. We present results that
compare this evaluation metric with other textual metrics on a gold standard
dataset derived from the MIMIC collection and show its robustness and
sensitivity to factual errors.

</details>


### [153] [Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study](https://arxiv.org/pdf/2412.06272)
*Jiuzhou Han, Paul Burgess, Ehsan Shareghi*

Main category: cs.CL

TL;DR: The paper introduces the AusLaw Citation Benchmark for legal citation prediction, evaluates various methods, and finds hybrid approaches with retrieval augmentation perform best, though a significant performance gap remains.


<details>
  <summary>Details</summary>
Motivation: Legal citation prediction is under-explored despite LLMs' potential in legal tasks, requiring fine-grained contextual understanding.

Method: The study benchmarks solutions including standard prompting, retrieval pipelines, supervised fine-tuning, and hybrid strategies combining LLMs with retrieval augmentation.

Result: General and law-specific LLMs perform poorly alone; instruction tuning and hybrid methods with trained re-rankers yield the best results, but a 50% performance gap persists.

Conclusion: The AusLaw Benchmark highlights the challenge of legal citation prediction and serves as a test-bed for future research, with hybrid methods showing promise.

Abstract: Large Language Models (LLMs) have demonstrated strong potential across legal
tasks, yet the problem of legal citation prediction remains under-explored. At
its core, this task demands fine-grained contextual understanding and precise
identification of relevant legislation or precedent. We introduce the AusLaw
Citation Benchmark, a real-world dataset comprising 55k Australian legal
instances and 18,677 unique citations which to the best of our knowledge is the
first of its scale and scope. We then conduct a systematic benchmarking across
a range of solutions: (i) standard prompting of both general and
law-specialised LLMs, (ii) retrieval-only pipelines with both generic and
domain-specific embeddings, (iii) supervised fine-tuning, and (iv) several
hybrid strategies that combine LLMs with retrieval augmentation through query
expansion, voting ensembles, or re-ranking. Results show that neither general
nor law-specific LLMs suffice as stand-alone solutions, with performance near
zero. Instruction tuning (of even a generic open-source LLM) on task-specific
dataset is among the best performing solutions. We highlight that database
granularity along with the type of embeddings play a critical role in
retrieval-based approaches, with hybrid methods which utilise a trained
re-ranker delivering the best results. Despite this, a performance gap of
nearly 50% remains, underscoring the value of this challenging benchmark as a
rigorous test-bed for future research in legal-domain.

</details>


### [154] [My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis](https://arxiv.org/pdf/2412.07367)
*Jian Liao, Yu Feng, Yujin Zheng, Jun Zhao, Suge Wang, Jianxing Zheng*

Main category: cs.CL

TL;DR: The paper introduces Personalized Implicit Emotion Analysis (PIEA) and the RAPPIE model, which incorporates reader feedback to improve emotion analysis by addressing subjective variability and data limitations.


<details>
  <summary>Details</summary>
Motivation: Current emotion analysis studies focus on authors but neglect the impact of intended readers on implicit emotional feedback, leading to incomplete analysis.

Method: The RAPPIE model uses reader agents based on large language models to simulate feedback, employs role-aware multi-view graph learning for sparse reader scenarios, and constructs new PIEA datasets with user metadata.

Result: RAPPIE outperforms state-of-the-art baselines, showing the value of reader feedback in PIEA.

Conclusion: Incorporating reader feedback significantly enhances implicit emotion analysis, addressing limitations of current approaches.

Abstract: The subtlety of emotional expressions makes implicit emotion analysis (IEA)
particularly sensitive to user-specific characteristics. Current studies
personalize emotion analysis by focusing on the author but neglect the impact
of the intended reader on implicit emotional feedback. In this paper, we
introduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses
subjective variability by incorporating reader feedback. In particular, (1) we
create reader agents based on large language models to simulate reader
feedback, overcoming the issue of ``spiral of silence effect'' and data
incompleteness of real reader reaction. (2) We develop a role-aware multi-view
graph learning to model the emotion interactive propagation process in
scenarios with sparse reader information. (3) We construct two new PIEA
datasets covering English and Chinese social media with detailed user metadata,
addressing the text-centric limitation of existing datasets. Extensive
experiments show that RAPPIE significantly outperforms state-of-the-art
baselines, demonstrating the value of incorporating reader feedback in PIEA.

</details>


### [155] [LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework](https://arxiv.org/pdf/2412.12459)
*Chia-Hsuan Chang, Jui-Tse Tsai, Yi-Hang Tsai, San-Yih Hwang*

Main category: cs.CL

TL;DR: LITA is an LLM-assisted framework for topic modeling that combines user-provided seeds with embedding-based clustering and iterative refinement, outperforming traditional models while minimizing API costs.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models lack specificity and coherence in domain-focused applications, and guided approaches are labor-intensive. LLMs offer dynamic refinement but incur high costs.

Method: LITA integrates user-provided seeds with embedding-based clustering and iterative refinement, using LLMs to reassign ambiguous documents to improve topic quality.

Result: LITA outperforms five baseline models (LDA, SeededLDA, CorEx, BERTopic, PromptTopic) in topic quality and clustering performance on two datasets.

Conclusion: LITA provides an efficient and adaptable framework for enhancing topic modeling and text clustering.

Abstract: Topic modeling is widely used for uncovering thematic structures within text
corpora, yet traditional models often struggle with specificity and coherence
in domain-focused applications. Guided approaches, such as SeededLDA and CorEx,
incorporate user-provided seed words to improve relevance but remain
labor-intensive and static. Large language models (LLMs) offer potential for
dynamic topic refinement and discovery, yet their application often incurs high
API costs. To address these challenges, we propose the LLM-assisted Iterative
Topic Augmentation framework (LITA), an LLM-assisted approach that integrates
user-provided seeds with embedding-based clustering and iterative refinement.
LITA identifies a small number of ambiguous documents and employs an LLM to
reassign them to existing or new topics, minimizing API costs while enhancing
topic quality. Experiments on two datasets across topic quality and clustering
performance metrics demonstrate that LITA outperforms five baseline models,
including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an
efficient and adaptable framework for advancing topic modeling and text
clustering.

</details>


### [156] [DocFusion: A Unified Framework for Document Parsing Tasks](https://arxiv.org/pdf/2412.12505)
*Mingxu Chai, Ziyu Shen, Chong Zhang, Yue Zhang, Xiao Wang, Shihan Dou, Jihua Kang, Jiazheng Zhang, Qi Zhang*

Main category: cs.CL

TL;DR: DocFusion is a lightweight generative model (0.28B parameters) unifying document parsing tasks, achieving SOTA performance by leveraging task interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods require multiple models for document parsing, leading to complexity and high maintenance.

Method: DocFusion uses a unified task representation and improved objective function for collaborative training.

Result: Integrating recognition data boosts detection performance; DocFusion achieves SOTA in four key tasks.

Conclusion: DocFusion simplifies document parsing with high performance, reducing complexity and overhead.

Abstract: Document parsing is essential for analyzing complex document structures and
extracting fine-grained information, supporting numerous downstream
applications. However, existing methods often require integrating multiple
independent models to handle various parsing tasks, leading to high complexity
and maintenance overhead. To address this, we propose DocFusion, a lightweight
generative model with only 0.28B parameters. It unifies task representations
and achieves collaborative training through an improved objective function.
Experiments reveal and leverage the mutually beneficial interaction among
recognition tasks, and integrating recognition data significantly enhances
detection performance. The final results demonstrate that DocFusion achieves
state-of-the-art (SOTA) performance across four key tasks.

</details>


### [157] [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/pdf/2412.16555)
*Yanxu Mao, Peipei Liu, Tiehan Cui, Zhaoteng Yan, Congying Liu, Datao You*

Main category: cs.CL

TL;DR: The paper proposes JMLLM, a multimodal jailbreaking method for LLMs, addressing limitations of existing methods by integrating text, visual, and auditory attacks. It introduces the TriJail dataset and achieves high success rates with reduced time overhead.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking methods for LLMs have limitations like high query counts, low success rates, and simplistic evaluations. The paper aims to overcome these by proposing a multimodal approach.

Method: JMLLM integrates multiple strategies for jailbreak attacks across text, visual, and auditory modalities. The TriJail dataset is introduced to support multimodal research.

Result: Experiments on TriJail and AdvBench datasets show advanced attack success rates and reduced time overhead across 13 popular LLMs.

Conclusion: JMLLM effectively addresses the limitations of existing jailbreaking methods, demonstrating superior performance in multimodal attacks.

Abstract: Large language models (LLMs) are widely applied in various fields of society
due to their powerful reasoning, understanding, and generation capabilities.
However, the security issues associated with these models are becoming
increasingly severe. Jailbreaking attacks, as an important method for detecting
vulnerabilities in LLMs, have been explored by researchers who attempt to
induce these models to generate harmful content through various attack methods.
Nevertheless, existing jailbreaking methods face numerous limitations, such as
excessive query counts, limited coverage of jailbreak modalities, low attack
success rates, and simplistic evaluation methods. To overcome these
constraints, this paper proposes a multimodal jailbreaking method: JMLLM. This
method integrates multiple strategies to perform comprehensive jailbreak
attacks across text, visual, and auditory modalities. Additionally, we
contribute a new and comprehensive dataset for multimodal jailbreaking
research: TriJail, which includes jailbreak prompts for all three modalities.
Experiments on the TriJail dataset and the benchmark dataset AdvBench,
conducted on 13 popular LLMs, demonstrate advanced attack success rates and
significant reduction in time overhead.

</details>


### [158] [BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism](https://arxiv.org/pdf/2412.17933)
*Martin Fajcik, Martin Docekal, Jan Dolezal, Karel Ondrej, Karel Beneš, Jan Kapsa, Pavel Smrz, Alexander Polok, Michal Hradis, Zuzana Neverilova, Ales Horak, Radoslav Sabol, Michal Stefanik, Adam Jirkovsky, David Adamczyk, Petr Hyner, Jan Hula, Hynek Kydlicek*

Main category: cs.CL

TL;DR: BenCzechMark (BCM) is the first Czech language benchmark for large language models, featuring 50 diverse tasks, a duel scoring system, and a new Czech-centric 7B model.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive Czech language benchmark for evaluating large language models.

Method: Developed a benchmark with 50 tasks across 8 categories, introduced a duel scoring system, and created a Czech-centric 7B model using the BUT-Large Czech Collection.

Result: The benchmark includes diverse tasks and a baseline model, with a leaderboard for model submissions.

Conclusion: BCM fills a gap in Czech language evaluation and provides tools for future research and model development.

Abstract: We present BenCzechMark (BCM), the first comprehensive Czech language
benchmark designed for large language models, offering diverse tasks, multiple
task formats, and multiple evaluation metrics. Its duel scoring system is
grounded in statistical significance theory and uses aggregation across tasks
inspired by social preference theory. Our benchmark encompasses 50 challenging
tasks, with corresponding test datasets, primarily in native Czech, with 14
newly collected ones. These tasks span 8 categories and cover diverse domains,
including historical Czech news, essays from pupils or language learners, and
spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the
largest publicly available clean Czech language corpus, and use it for (i)
contamination analysis and (ii) continuous pretraining of the first
Czech-centric 7B language model with Czech-specific tokenization. We use our
model as a baseline for comparison with publicly available multilingual models.
Lastly, we release and maintain a leaderboard with existing 50 model
submissions, where new model submissions can be made at
https://huggingface.co/spaces/CZLC/BenCzechMark.

</details>


### [159] [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/pdf/2501.05714)
*Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang*

Main category: cs.CL

TL;DR: A review of human-model cooperation in NLP, introducing a taxonomy and discussing challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the evolution of LLMs into autonomous agents and their cooperation with humans, addressing gaps in the field.

Method: Presents a thorough review, introduces a new taxonomy, and discusses principles and challenges.

Result: A unified perspective on existing approaches and identification of frontier areas.

Conclusion: The paper serves as an entry point for future research in human-model cooperation.

Abstract: With the advancement of large language models (LLMs), intelligent models have
evolved from mere tools to autonomous agents with their own goals and
strategies for cooperating with humans. This evolution has birthed a novel
paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable
progress in numerous NLP tasks in recent years. In this paper, we take the
first step to present a thorough review of human-model cooperation, exploring
its principles, formalizations, and open challenges. In particular, we
introduce a new taxonomy that provides a unified perspective to summarize
existing approaches. Also, we discuss potential frontier areas and their
corresponding challenges. We regard our work as an entry point, paving the way
for more breakthrough research in this regard.

</details>


### [160] [Diverse Preference Optimization](https://arxiv.org/pdf/2501.18101)
*Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov*

Main category: cs.CL

TL;DR: DivPO introduces a method to enhance response diversity in language models while maintaining quality, outperforming standard pipelines in diversity metrics.


<details>
  <summary>Details</summary>
Motivation: Standard post-training methods reduce response diversity, which is problematic for creative tasks.

Method: DivPO selects preference pairs based on diversity and quality, favoring rare but high-quality responses.

Result: DivPO increases diversity by 45.6% in persona attributes and 74.6% in stories, with comparable win rates.

Conclusion: DivPO effectively balances diversity and quality, outperforming baselines like DPO in diversity and win rates.

Abstract: Post-training of language models, either through reinforcement learning,
preference optimization or supervised finetuning, tends to sharpen the output
probability distribution and reduce the diversity of generated responses. This
is particularly a problem for creative generative tasks where varied responses
are desired. In this work we introduce Diverse Preference Optimization (DivPO),
an optimization method which learns to generate much more diverse responses
than standard pipelines, while maintaining the quality of the generations. In
DivPO, preference pairs are selected by first considering a pool of responses,
and a measure of diversity among them, and selecting chosen examples as being
more rare but high quality, while rejected examples are more common, but low
quality. DivPO results in generating 45.6% more diverse persona attributes, and
a 74.6% increase in story diversity, while maintaining similar win rates as
standard baselines. On general instruction following, DivPO results in a 46.2%
increase in diversity, and a 2.4% winrate improvement compared to DPO.

</details>


### [161] [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/pdf/2502.00675)
*Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang*

Main category: cs.CL

TL;DR: ReFoRCE is a Text-to-SQL agent that addresses challenges in enterprise environments, achieving top performance on the Spider 2.0 benchmark.


<details>
  <summary>Details</summary>
Motivation: Deploying Text-to-SQL systems in enterprise settings is difficult due to large schemas, diverse SQL dialects, and complex queries.

Method: ReFoRCE uses database compression, self-refinement, majority-vote consensus, and iterative column exploration to handle these challenges.

Result: ReFoRCE achieves state-of-the-art scores of 35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.

Conclusion: ReFoRCE effectively tackles enterprise Text-to-SQL challenges, setting new benchmarks in performance.

Abstract: We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0
leaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL
scenarios. While Text-to-SQL systems enable natural language queries over
structured databases, deploying them in enterprise environments remains
difficult due to large, complex schemas (with over 1,000 columns), diverse SQL
dialects (e.g., BigQuery, Snowflake), and sophisticated query requirements
(e.g., transformations and analytics). ReFoRCE addresses these challenges
through: (a) database information compression via pattern-based table grouping
and LLM-guided schema linking to alleviate long-context issues; (b)
self-refinement to iteratively correct syntax and semantic errors across
dialects; (c) majority-vote consensus to select high-confidence candidates
while deferring ambiguous cases arising from sophisticated queries; and (d)
iterative column exploration guided by execution feedback to resolve those
deferred cases. ReFoRCE achieves new state-of-the-art results, with scores of
35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.

</details>


### [162] [FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training](https://arxiv.org/pdf/2502.00761)
*Liangyu Xu, Xuemiao Zhang, Feiyu Duan, Sirui Wang, Rongxiang Weng, Jingang Wang, Xunliang Cai*

Main category: cs.CL

TL;DR: FIRE is a framework for comprehensive data quality assessment in LLM pretraining, outperforming heuristic methods and reducing data needs by 37.5%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for data quality assessment in LLM pretraining rely on heuristics or single signals, lacking comprehensiveness.

Method: FIRE integrates multiple quality raters, aligns signals into a unified space, and uses a progressive selection scheme.

Result: FIRE outperforms other methods, significantly improving model performance with less data (37.5% of Random baseline).

Conclusion: FIRE provides a scalable, flexible solution for high-quality data selection, enhancing pretraining efficiency and downstream task performance.

Abstract: Selecting high-quality data can improve the pretraining efficiency of large
language models (LLMs). Existing methods generally rely on heuristic techniques
or single quality signals, limiting their ability to evaluate data quality
comprehensively. In this work, we propose FIRE, a flexible and scalable
framework for integrating multiple data quality raters, which allows for a
comprehensive assessment of data quality across various dimensions. FIRE aligns
multiple quality signals into a unified space, and integrates diverse data
quality raters to provide a comprehensive quality signal for each data point.
Further, we introduce a progressive data selection scheme based on FIRE that
iteratively refines the selection of high-quality data points. Extensive
experiments show that FIRE outperforms other data selection methods and
significantly boosts pretrained model performance across a wide range of
downstream tasks, while requiring less than 37.5\% of the training data needed
by the Random baseline to reach the target performance.

</details>


### [163] [Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data](https://arxiv.org/pdf/2502.04380)
*Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Qianli Shen, Yaliang Li, Ying Shen*

Main category: cs.CL

TL;DR: The paper proposes a dual-identity method for fine-tuning LLMs, addressing challenges with missing or imprecise domain labels and balancing multi-domain performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with data lacking domain labels or balancing multi-domain performance, prompting the need for a new approach.

Method: The method assigns LLMs a dual identity: an output model for diversity-based data selection and an input model for tuning with selected data.

Result: Experiments show the method improves performance on domain-undetermined data and downstream tasks.

Conclusion: The study advances understanding of data diversity and feedback-driven data-model co-design for LLMs.

Abstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial
for enhancing their overall performance across various domains. In practical
scenarios, existing methods based on modeling the mixture proportions of data
composition often struggle with data whose domain labels are missing, imprecise
or non-normalized, while methods based on data selection usually encounter
difficulties in balancing multi-domain performance. To address these
challenges, in this work, we investigate the role of data diversity in
enhancing the overall abilities of LLMs by empirically constructing contrastive
data pools and theoretically deriving explanations. Building upon the insights
gained, we propose a new method that gives the LLM a dual identity: an output
model to cognitively probe and select data based on diversity reward, as well
as an input model to be tuned with the selected data. Extensive experiments
show that the proposed method notably boosts performance across
domain-undetermined data and a series of foundational downstream tasks when
applied to various advanced LLMs. We release our code and hope this study can
shed light on the understanding of data diversity and advance feedback-driven
data-model co-design for LLMs.

</details>


### [164] [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://arxiv.org/pdf/2502.06086)
*Seokwon Song, Taehyun Lee, Jaewoo Ahn, Jae Hyuk Sung, Gunhee Kim*

Main category: cs.CL

TL;DR: The paper introduces the CCPT dataset to evaluate LLMs in conceptual combination, finding that LLMs struggle with emergent properties and proposing a cognitive psychology-inspired method to improve performance.


<details>
  <summary>Details</summary>
Motivation: Previous studies on conceptual combination were limited in evaluating properties and lacked examination of the generative process.

Method: The CCPT dataset (12.3K annotated triplets) is used to evaluate LLMs through three task types, with a proposed cognitive psychology-inspired method.

Result: Key findings: (1) Automatic metrics align with human judgments, (2) LLMs struggle with emergent properties, (3) The proposed method improves generative tasks.

Conclusion: The CCPT dataset and proposed method advance the evaluation and performance of LLMs in conceptual combination tasks.

Abstract: Conceptual combination is a cognitive process that merges basic concepts,
enabling the creation of complex expressions. During this process, the
properties of combination (e.g., the whiteness of a peeled apple) can be
inherited from basic concepts, newly emerge, or be canceled. However, previous
studies have evaluated a limited set of properties and have not examined the
generative process. To address this gap, we introduce the Conceptual
Combination with Property Type dataset (CCPT), which consists of 12.3K
annotated triplets of noun phrases, properties, and property types. Using CCPT,
we establish three types of tasks to evaluate LLMs for conceptual combination
thoroughly. Our key findings are threefold: (1) Our automatic metric grading
property emergence and cancellation closely corresponds with human judgments.
(2) LLMs, including OpenAI's o1, struggle to generate noun phrases which
possess given emergent properties. (3) Our proposed method, inspired by
cognitive psychology model that explains how relationships between concepts are
formed, improves performances in all generative tasks. The dataset and
experimental code are available at https://github.com/seokwon99/CCPT.git.

</details>


### [165] [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://arxiv.org/pdf/2502.06139)
*Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo*

Main category: cs.CL

TL;DR: The paper introduces LCIRC and QD-LCIRC to efficiently process long-form sequences in LLMs by recurrent compression and query-dependent context modeling, improving context handling without full retraining.


<details>
  <summary>Details</summary>
Motivation: Fixed-length position embeddings and high computational costs limit LLMs' ability to handle long-form contexts effectively.

Method: Proposes LCIRC for recurrent compression and QD-LCIRC for query-dependent context modeling to selectively retain relevant information.

Result: QD-LCIRC significantly enhances LLMs' ability to manage extended contexts while maintaining query relevance.

Conclusion: The method is effective for tasks requiring comprehensive context understanding and relevance, without the need for full model retraining.

Abstract: While large language models (LLMs) excel in generating coherent and
contextually rich outputs, their capacity to efficiently handle long-form
contexts is limited by fixed-length position embeddings. Additionally, the
computational cost of processing long sequences increases quadratically, making
it challenging to extend context length. To address these challenges, we
propose Long-form Context Injection with Recurrent Compression (LCIRC), a
method that enables the efficient processing long-form sequences beyond the
model's length limit through recurrent compression without retraining the
entire model. We further introduce query dependent context modeling, which
selectively compresses query-relevant information, ensuring that the model
retains the most pertinent content. Our empirical results demonstrate that
Query Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage
extended contexts, making it well-suited for tasks that require both
comprehensive context understanding and query relevance.

</details>


### [166] [C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation](https://arxiv.org/pdf/2502.06205)
*Guoxin Chen, Minpeng Liao, Peiying Yu, Dingmin Wang, Zile Qiao, Chao Yang, Xin Zhao, Kai Fan*

Main category: cs.CL

TL;DR: C-3PO is a proxy-centric framework using a multi-agent system to align retrievers and LLMs in RAG systems, improving performance without modifying existing components.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems struggle with alignment between retrievers and LLMs, leading to sub-optimal performance. Inspired by human search behavior, C-3PO aims to bridge this gap.

Method: A lightweight multi-agent system with three specialized agents optimizes the RAG pipeline. A tree-structured rollout approach in reinforcement learning enables coordination.

Result: C-3PO significantly enhances RAG performance in both in-domain and out-of-distribution scenarios while maintaining flexibility and generalization.

Conclusion: C-3PO effectively aligns retrievers and LLMs, offering a plug-and-play solution with superior performance and adaptability.

Abstract: Retrieval-augmented generation (RAG) systems face a fundamental challenge in
aligning independently developed retrievers and large language models (LLMs).
Existing approaches typically involve modifying either component or introducing
simple intermediate modules, resulting in practical limitations and sub-optimal
performance. Inspired by human search behavior -- typically involving a
back-and-forth process of proposing search queries and reviewing documents, we
propose C-3PO, a proxy-centric framework that facilitates communication between
retrievers and LLMs through a lightweight multi-agent system. Our framework
implements three specialized agents that collaboratively optimize the entire
RAG pipeline without altering the retriever and LLMs. These agents work
together to assess the need for retrieval, generate effective queries, and
select information suitable for the LLMs. To enable effective multi-agent
coordination, we develop a tree-structured rollout approach for reward credit
assignment in reinforcement learning. Extensive experiments in both in-domain
and out-of-distribution scenarios demonstrate that C-3PO significantly enhances
RAG performance while maintaining plug-and-play flexibility and superior
generalization capabilities.

</details>


### [167] [No Need for Explanations: LLMs can implicitly learn from mistakes in-context](https://arxiv.org/pdf/2502.08550)
*Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Tan Yi-Chern, Marek Rei, Max Bartolo*

Main category: cs.CL

TL;DR: LLMs perform better in math reasoning tasks when incorrect answers lack explicit rationales, outperforming chain-of-thought prompting.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that explicit rationales are necessary for LLMs to learn from incorrect answers.

Method: Compare LLM performance with and without corrective rationales, analyze context length, answer diversity, and overfitting.

Result: LLMs learn more effectively without explicit rationales, as they over-constrain the model.

Conclusion: Incorrect answers alone are more beneficial for LLM learning than explicit rationales or additional correct answers.

Abstract: Showing incorrect answers to Large Language Models (LLMs) is a popular
strategy to improve their performance in reasoning-intensive tasks. It is
widely assumed that, in order to be helpful, the incorrect answers must be
accompanied by comprehensive rationales, explicitly detailing where the
mistakes are and how to correct them. However, in this work we present a
counterintuitive finding: we observe that LLMs perform better in math reasoning
tasks when these rationales are eliminated from the context and models are left
to infer on their own what makes an incorrect answer flawed. This approach also
substantially outperforms chain-of-thought prompting in our evaluations. These
results are consistent across LLMs of different sizes and varying reasoning
abilities. To gain an understanding of why LLMs learn from mistakes more
effectively without explicit corrective rationales, we perform a thorough
analysis, investigating changes in context length and answer diversity between
different prompting strategies, and their effect on performance. We also
examine evidence of overfitting to the in-context rationales when these are
provided, and study the extent to which LLMs are able to autonomously infer
high-quality corrective rationales given only incorrect answers as input. We
find evidence that, while incorrect answers are more beneficial for LLM
learning than additional diverse correct answers, explicit corrective
rationales over-constrain the model, thus limiting those benefits.

</details>


### [168] [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/pdf/2502.09604)
*Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih*

Main category: cs.CL

TL;DR: SelfCite is a self-supervised method for improving sentence-level citations in LLM-generated responses by using the model's own reward signal via context ablation.


<details>
  <summary>Details</summary>
Motivation: Existing citation methods rely on costly annotations; SelfCite aims to automate and improve citation quality without heavy human effort.

Method: Uses context ablation to derive a reward signal: removing cited text should prevent the same response, while retaining it should preserve the response. This guides inference-time sampling and fine-tuning.

Result: Increases citation F1 by up to 5.3 points on the LongBench-Cite benchmark across five tasks.

Conclusion: SelfCite effectively improves citation quality in LLM outputs, demonstrating the potential of self-supervised alignment for fine-grained tasks.

Abstract: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to
generate high-quality, fine-grained, sentence-level citations for the
statements in their generated responses. Instead of only relying on costly and
labor-intensive annotations, SelfCite leverages a reward signal provided by the
LLM itself through context ablation: If a citation is necessary, removing the
cited text from the context should prevent the same response; if sufficient,
retaining the cited text alone should preserve the same response. This reward
can guide the inference-time best-of-N sampling strategy to improve citation
quality significantly, as well as be used in preference optimization to
directly fine-tune the models for generating better citations. The
effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3
points on the LongBench-Cite benchmark across five long-form question answering
tasks. The source code is available at
https://github.com/facebookresearch/SelfCite

</details>


### [169] [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/pdf/2502.09674)
*Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, Xiaohua Jia*

Main category: cs.CL

TL;DR: Safety-aligned behaviors in LLMs are controlled by multi-dimensional directions, not just one, revealing deeper mechanistic insights and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To understand the multi-dimensional nature of safety-aligned behaviors in LLMs, moving beyond the single-direction model.

Method: Studied orthogonal directions in activation space of Llama 3 8B during safety fine-tuning, analyzing refusal behavior and interpretable features.

Result: Found a dominant refusal direction and smaller interpretable ones; secondary directions shape refusal representation, and token removal can bypass safety.

Conclusion: Safety alignment is multi-dimensional, with secondary directions playing key roles, offering new insights into vulnerabilities.

Abstract: Large Language Models' safety-aligned behaviors, such as refusing harmful
queries, can be represented by linear directions in activation space. Previous
research modeled safety behavior with a single direction, limiting mechanistic
understanding to an isolated safety feature. In this work, we discover that
safety-aligned behavior is jointly controlled by multi-dimensional directions.
Namely, we study the vector space of representation shifts during safety
fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal
directions in the space, we first find that a dominant direction governs the
model's refusal behavior, while multiple smaller directions represent distinct
and interpretable features like hypothetical narrative and role-playing. We
then measure how different directions promote or suppress the dominant
direction, showing the important role of secondary directions in shaping the
model's refusal representation. Finally, we demonstrate that removing certain
trigger tokens in harmful queries can mitigate these directions to bypass the
learned safety capability, providing new insights on understanding safety
alignment vulnerability from a multi-dimensional perspective. Code and
artifacts are available at https://github.com/BMPixel/safety-residual-space.

</details>


### [170] [GRIFFIN: Effective Token Alignment for Faster Speculative Decoding](https://arxiv.org/pdf/2502.11018)
*Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou*

Main category: cs.CL

TL;DR: GRIFFIN improves speculative decoding in LLMs by addressing token misalignment with a novel training strategy and draft model, achieving better performance and speed.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods suffer from token misalignment between training and decoding phases, limiting their effectiveness.

Method: GRIFFIN introduces a token-alignable training strategy (using loss masking) and a token-alignable draft model to correct feature inconsistencies.

Result: Experiments show GRIFFIN improves acceptance length by 8% and speed by 7%, outperforming current methods.

Conclusion: GRIFFIN effectively mitigates token misalignment, enhancing speculative decoding performance in LLMs.

Abstract: Speculative decoding accelerates inference in large language models (LLMs) by
generating multiple draft tokens simultaneously. However, existing methods
often struggle with token misalignment between the training and decoding
phases, limiting their performance. To address this, we propose GRIFFIN, a
novel framework that incorporates a token-alignable training strategy and a
token-alignable draft model to mitigate misalignment. The training strategy
employs a loss masking mechanism to exclude highly misaligned tokens during
training, preventing them from negatively impacting the draft model's
optimization. The token-alignable draft model introduces input tokens to
correct inconsistencies in generated features. Experiments on LLaMA, Vicuna,
Qwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance
length improvement of over 8% and a speedup ratio exceeding 7%, outperforming
current speculative decoding state-of-the-art methods. Our code and GRIFFIN's
draft models are released publicly in https://github.com/hsj576/GRIFFIN.

</details>


### [171] [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/pdf/2502.11438)
*Jimin Lee, Ingeol Baek, Byeongjeong Kim, Hyunkyung Bae, Hwanhee Lee*

Main category: cs.CL

TL;DR: SAFE-SQL improves Text-to-SQL by generating and filtering self-augmented examples, outperforming zero-shot and few-shot methods, especially in hard and unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: Previous methods rely on similar training examples, which are often unavailable in real-world scenarios, limiting their effectiveness.

Method: SAFE-SQL generates multiple Text-to-SQL examples, filters them via three relevance assessments, and uses these for in-context learning.

Result: SAFE-SQL achieves higher execution accuracy than zero-shot and few-shot frameworks, excelling in challenging scenarios.

Conclusion: SAFE-SQL addresses the limitation of unavailable training examples, offering robust performance in real-world and unseen cases.

Abstract: Text-to-SQL aims to convert natural language questions into executable SQL
queries. While previous approaches, such as skeleton-masked selection, have
demonstrated strong performance by retrieving similar training examples to
guide large language models (LLMs), they struggle in real-world scenarios where
such examples are unavailable. To overcome this limitation, we propose
Self-Augmentation in-context learning with Fine-grained Example selection for
Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by
generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM
to generate multiple Text-to-SQL examples relevant to the test input. Then
SAFE-SQL filters these examples through three relevance assessments,
constructing high-quality in-context learning examples. Using self-generated
examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL
frameworks, achieving higher execution accuracy. Notably, our approach provides
additional performance gains in extra hard and unseen scenarios, where
conventional methods often fail.

</details>


### [172] [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/pdf/2502.11471)
*Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun*

Main category: cs.CL

TL;DR: GLTW enhances Knowledge Graph Completion (KGC) by merging structural KG information with LLMs using an improved Graph Transformer (iGT) and subgraph-based training.


<details>
  <summary>Details</summary>
Motivation: Integrating KG structural information into LLMs for deterministic KGC predictions is challenging.

Method: Proposes GLTW with iGT for encoding subgraphs and a subgraph-based multi-classification training objective, combined with LLMs.

Result: GLTW achieves significant performance gains over SOTA baselines in experiments.

Conclusion: GLTW effectively merges KG structure with LLMs, improving KGC performance.

Abstract: Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.

</details>


### [173] [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2502.11811)
*Qianchi Zhang, Hainan Zhang, Liang Pang, Ziwei Wang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng*

Main category: cs.CL

TL;DR: FineFilter is a fine-grained noise filtering mechanism for RAG, improving QA performance by optimizing clue extraction, reranking, and truncation.


<details>
  <summary>Details</summary>
Motivation: Noise in retrieved documents hinders RAG's accuracy, and existing methods struggle with direct answer clue identification.

Method: FineFilter uses a clue extractor, reranker, and truncator to optimize sentence-level MinMax filtering.

Result: FineFilter outperforms baselines on QA datasets, showing effectiveness in complex reasoning and robustness.

Conclusion: FineFilter enhances RAG accuracy by fine-grained noise filtering and generalizes well across scenarios.

Abstract: Retrieved documents containing noise will hinder Retrieval-Augmented
Generation (RAG) from detecting answer clues, necessitating noise filtering
mechanisms to enhance accuracy. Existing methods use reranking or summarization
to identify the most relevant sentences, but directly and accurately locating
answer clues from these large-scale and complex documents remains challenging.
Unlike these document-level operations, we treat noise filtering as a
sentence-level MinMax optimization problem: first identifying potential clues
from multiple documents, then ranking them by relevance, and finally retaining
the minimum number of clues through truncation. In this paper, we propose
FineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting
of a clue extractor, a reranker, and a truncator. We optimize each module to
tackle complex reasoning challenges: (1) The clue extractor first uses
sentences containing the answer and similar ones as fine-tuning targets, aiming
to extract sufficient potential clues; (2) The reranker is trained to
prioritize effective clues based on the real feedback from the generation
module, with clues capable of generating correct answers as positive samples
and others as negative; (3) The truncator takes the minimum number of clues
needed to answer the question (truncation point) as fine-tuning targets, and
performs truncation on the reranked clues to achieve fine-grained noise
filtering. Experiments on three QA datasets demonstrate that FineFilter
significantly improves QA performance over baselines on both LLaMA3 and
Mistral. Further analysis confirms its effectiveness in complex reasoning,
robustness to unreliable retrieval, and generalization to different scenarios.

</details>


### [174] [M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis](https://arxiv.org/pdf/2502.11824)
*Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Yun Xue, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces M-ABSA, a multilingual dataset for aspect-based sentiment analysis (ABSA) covering 7 domains and 21 languages, addressing the lack of non-English ABSA resources. It focuses on triplet extraction and demonstrates the dataset's utility for diverse evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA datasets are mostly English-centric, limiting multilingual research. The authors aim to bridge this gap by creating a comprehensive, multilingual dataset.

Method: M-ABSA is built via automatic translation followed by human review. Experiments with various baselines assess its performance and compatibility.

Result: The dataset supports multilingual and multi-domain transfer learning, as well as large language model evaluation, showcasing its inclusivity and research potential.

Conclusion: M-ABSA is a valuable resource for advancing multilingual ABSA research, enabling diverse evaluation tasks and fostering inclusivity.

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial task in information
extraction and sentiment analysis, aiming to identify aspects with associated
sentiment elements in text. However, existing ABSA datasets are predominantly
English-centric, limiting the scope for multilingual evaluation and research.
To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7
domains and 21 languages, making it the most extensive multilingual parallel
dataset for ABSA to date. Our primary focus is on triplet extraction, which
involves identifying aspect terms, aspect categories, and sentiment polarities.
The dataset is constructed through an automatic translation process with human
review to ensure quality. We perform extensive experiments using various
baselines to assess performance and compatibility on M-ABSA. Our empirical
findings highlight that the dataset enables diverse evaluation tasks, such as
multilingual and multi-domain transfer learning, and large language model
evaluation, underscoring its inclusivity and its potential to drive
advancements in multilingual ABSA research.

</details>


### [175] [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/pdf/2502.12464)
*Seanie Lee, Dong Bok Lee, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang*

Main category: cs.CL

TL;DR: SafeRoute is a binary router that selectively uses a larger safety guard model only for hard examples, improving efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large safety guard models are computationally expensive, and smaller distilled models underperform on hard examples. SafeRoute addresses this by routing only hard cases to the larger model.

Method: Proposes SafeRoute, a binary router to distinguish hard examples from easy ones, applying the larger model selectively.

Result: Experimental results show improved trade-off between computational cost and safety performance, outperforming baselines.

Conclusion: SafeRoute effectively balances efficiency and accuracy in deploying safety guard models for LLMs.

Abstract: Deploying large language models (LLMs) in real-world applications requires
robust safety guard models to detect and block harmful user prompts. While
large safety guard models achieve strong performance, their computational cost
is substantial. To mitigate this, smaller distilled models are used, but they
often underperform on "hard" examples where the larger model provides accurate
predictions. We observe that many inputs can be reliably handled by the smaller
model, while only a small fraction require the larger model's capacity.
Motivated by this, we propose SafeRoute, a binary router that distinguishes
hard examples from easy ones. Our method selectively applies the larger safety
guard model to the data that the router considers hard, improving efficiency
while maintaining accuracy compared to solely using the larger safety guard
model. Experimental results on multiple benchmark datasets demonstrate that our
adaptive model selection significantly enhances the trade-off between
computational cost and safety performance, outperforming relevant baselines.

</details>


### [176] [Whose story is it? Personalizing story generation by inferring author styles](https://arxiv.org/pdf/2502.13028)
*Nischal Ashok Kumar, Chau Minh Pham, Mohit Iyyer, Andrew Lan*

Main category: cs.CL

TL;DR: The paper introduces a method for personalized story generation by mimicking authors' writing styles using a two-stage pipeline, validated by human evaluation.


<details>
  <summary>Details</summary>
Motivation: Personalization is understudied in story generation but crucial for enhancing user experience in writing and educational applications.

Method: A two-stage pipeline: (1) infer authors' writing characteristics into an Author Writing Sheet, and (2) simulate the author's persona using tailored descriptions and rules.

Result: Personalized stories outperformed non-personalized baselines, with 78% win-rate in capturing style and 59% similarity to ground-truth stories. Human evaluation confirmed trends like Reddit stories being easier to personalize.

Conclusion: The proposed method effectively personalizes story generation, with human validation supporting its success in capturing authors' styles.

Abstract: Personalization is critical for improving user experience in interactive
writing and educational applications, yet remains understudied in story
generation. We study the task of personalizing story generation, where our goal
is to mimic an author's writing style, given other stories written by them. We
collect Mythos, a dataset of 3.6k stories from 112 authors, with an average of
16 stories per author, across five distinct sources reflecting diverse
story-writing settings. We propose a two-stage pipeline for personalized story
generation: first, we infer authors' implicit writing characteristics and
organize them into an Author Writing Sheet, which is validated by humans to be
of high quality; second, we simulate the author's persona using tailored
persona descriptions and personalized story rules. We find that stories
personalized using the Author Writing Sheet outperform a non-personalized
baseline, achieving a 78% win-rate in capturing authors' past style and 59% in
similarity to ground-truth author stories. Human evaluation supports these
findings and further highlights trends, such as Reddit stories being easier to
personalize, and the Creativity and Language Use aspects of stories being
easier to personalize than the Plot.

</details>


### [177] [Transferring Textual Preferences to Vision-Language Understanding through Model Merging](https://arxiv.org/pdf/2502.13487)
*Chen-An Li, Tzu-Han Lin, Yun-Nung Chen, Hung-yi Lee*

Main category: cs.CL

TL;DR: A training-free method merges text-based reward models with large vision-language models to improve content evaluation without costly training.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LVLMs in evaluating generated content and the high computational cost of training vision-language reward models.

Method: Integrate text-based reward models with LVLMs to create vision-language reward models without additional training.

Result: Improved performance over LVLMs' scoring and text-based reward models.

Conclusion: The approach offers an efficient way to incorporate textual preferences into LVLMs.

Abstract: Large vision-language models (LVLMs) perform outstandingly across various
multimodal tasks. However, their ability to evaluate generated content remains
limited, and training vision-language reward models (VLRMs) with preference
data is computationally expensive. This paper explores a training-free
alternative by merging text-based reward models (RMs) with LVLMs to create
VLRMs. Our approach shows that integrating these models leads to improved
performance over LVLMs' scoring and text-based RMs, offering an efficient
method for incorporating textual preferences into LVLMs.

</details>


### [178] [CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations](https://arxiv.org/pdf/2502.15132)
*Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi*

Main category: cs.CL

TL;DR: CoT-ICL Lab is a framework for generating synthetic datasets to study chain-of-thought (CoT) in-context learning (ICL) in language models, showing CoT improves accuracy, especially with model depth and controlled token diversity.


<details>
  <summary>Details</summary>
Motivation: To systematically study CoT in ICL by decoupling causal structure from token processing, enabling fine-grained control over dataset complexity.

Method: Generate synthetic tokenized datasets, train decoder-only transformers (up to 700M parameters), and analyze embeddings and attention maps.

Result: CoT accelerates accuracy transitions; model depth is key for limited examples, while shallow models benefit from more examples. Limited token diversity improves causal learning.

Conclusion: CoT-ICL Lab is a powerful testbed for theoretical and empirical insights into ICL and CoT in language models.

Abstract: We introduce CoT-ICL Lab, a framework and methodology to generate synthetic
tokenized datasets and systematically study chain-of-thought (CoT) in-context
learning (ICL) in language models. CoT-ICL Lab allows fine grained control over
the complexity of in-context examples by decoupling (1) the causal structure
involved in chain token generation from (2) the underlying token processing
functions. We train decoder-only transformers (up to 700M parameters) on these
datasets and show that CoT accelerates the accuracy transition to higher values
across model sizes. In particular, we find that model depth is crucial for
leveraging CoT with limited in-context examples, while more examples help
shallow models match deeper model performance. Additionally, limiting the
diversity of token processing functions throughout training improves causal
structure learning via ICL. We also interpret these transitions by analyzing
transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a
simple yet powerful testbed for theoretical and empirical insights into ICL and
CoT in language models.

</details>


### [179] [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](https://arxiv.org/pdf/2502.16002)
*Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang*

Main category: cs.CL

TL;DR: KVLink enables efficient KV cache reuse in LLMs by precomputing and concatenating caches for overlapping contexts, improving accuracy and reducing inference time.


<details>
  <summary>Details</summary>
Motivation: Redundant computation in LLMs due to overlapping contexts in queries motivates the need for efficient KV cache reuse.

Method: KVLink precomputes KV caches for documents independently, concatenates them during inference, and uses adjusted positional embeddings and trainable tokens to maintain performance.

Result: KVLink improves QA accuracy by 4% on average and reduces time-to-first-token by up to 96%.

Conclusion: KVLink is a scalable, efficient solution for context reuse in LLMs, compatible with cache compression for further overhead reduction.

Abstract: We describe KVLink, an approach for efficient key-value (KV) cache reuse in
large language models (LLMs). In many LLM applications, different inputs can
share overlapping context, such as the same retrieved document appearing in
multiple queries. However, the LLMs still need to encode the entire context for
each query, leading to redundant computation. In this paper, we investigate a
new strategy to eliminate such inefficiency, where the KV cache of each
document is precomputed independently. During inference, the KV caches of
retrieved documents are concatenated, allowing the model to reuse cached
representations instead of recomputing them. To mitigate the performance
degradation when using KV caches computed independently for each document,
KVLink introduces two key techniques: adjusting positional embeddings of the KV
cache at inference to match the global position after concatenation, and using
trainable special tokens to restore self-attention across independently encoded
documents. Experiments across 7 datasets demonstrate that KVLink improves
question answering accuracy by an average of 4% over state-of-the-art methods.
Furthermore, by leveraging precomputed KV caches, our approach reduces
time-to-first-token by up to 96% compared to standard LLM inference, making it
a scalable and efficient solution for context reuse. Additionally, KVLink can
be combined with KV cache compression to further save cache loading and storage
overhead while outperforming the baselines.

</details>


### [180] [FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/pdf/2502.17775)
*Tanawan Premsri, Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: The paper introduces FoREST, a benchmark to evaluate Frame of Reference (FoR) comprehension in LLMs, revealing performance gaps and proposing Spatial-Guided prompting to improve spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: FoR is crucial for spatial cognition but lacks attention in AI models. There's a need for dedicated benchmarks to evaluate LLMs' FoR comprehension.

Method: Introduces FoREST benchmark to assess FoR in LLMs, evaluates performance on FoR-related questions and text-to-image layout generation, and proposes Spatial-Guided prompting.

Result: Identifies performance gaps in LLMs' FoR comprehension, impacting text-to-image accuracy. Spatial-Guided prompting improves spatial reasoning.

Conclusion: FoREST highlights LLMs' shortcomings in FoR comprehension, and Spatial-Guided prompting offers a solution to enhance spatial reasoning performance.

Abstract: Spatial reasoning is a fundamental aspect of human intelligence. One key
concept in spatial cognition is the Frame of Reference (FoR), which identifies
the perspective of spatial expressions. Despite its significance, FoR has
received limited attention in AI models that need spatial intelligence. There
is a lack of dedicated benchmarks and in-depth evaluation of large language
models (LLMs) in this area. To address this issue, we introduce the Frame of
Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to
assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that
require FoR comprehension and layout generation in text-to-image models using
FoREST. Our results reveal a notable performance gap across different FoR
classes in various LLMs, affecting their ability to generate accurate layouts
for text-to-image generation. This highlights critical shortcomings in FoR
comprehension. To improve FoR understanding, we propose Spatial-Guided
prompting, which improves LLMs ability to extract essential spatial concepts.
Our proposed method improves overall performance across spatial reasoning
tasks.

</details>


### [181] [Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments](https://arxiv.org/pdf/2502.17956)
*Patomporn Payoungkhamdee, Pume Tuchinda, Jinheon Baek, Samuel Cahyawijaya, Can Udomcharoenchaikit, Potsawee Manakul, Peerat Limkonchotiwat, Ekapol Chuangsuwanich, Sarana Nutanong*

Main category: cs.CL

TL;DR: PoT prompting improves multilingual reasoning in LLMs by separating reasoning from execution, outperforming CoT. Fine-tuning enhances question-reasoning alignment, and reasoning quality strongly correlates with answer accuracy.


<details>
  <summary>Details</summary>
Motivation: Multilingual reasoning in LLMs is challenging, especially with CoT prompting due to reasoning-execution entanglement. PoT offers a solution but introduces new challenges in program generation from non-English questions.

Method: Proposed framework evaluates PoT by separating multilingual reasoning from code execution, examining fine-tuning's impact on question-reasoning alignment and reasoning quality's effect on answer correctness.

Result: PoT fine-tuning significantly improves multilingual reasoning, surpassing CoT models. Reasoning quality (measured by code quality) strongly correlates with answer accuracy.

Conclusion: PoT is a promising alternative for multilingual reasoning, with fine-tuning and reasoning quality as key factors for performance improvement.

Abstract: Multi-step reasoning is essential for large language models (LLMs), yet
multilingual performance remains challenging. While Chain-of-Thought (CoT)
prompting improves reasoning, it struggles with non-English languages due to
the entanglement of reasoning and execution. Program-of-Thought (PoT) prompting
separates reasoning from execution, offering a promising alternative but
shifting the challenge to generating programs from non-English questions. We
propose a framework to evaluate PoT by separating multilingual reasoning from
code execution to examine (i) the impact of fine-tuning on question-reasoning
alignment and (ii) how reasoning quality affects answer correctness. Our
findings demonstrate that PoT fine-tuning substantially enhances multilingual
reasoning, outperforming CoT fine-tuned models. We further demonstrate a strong
correlation between reasoning quality (measured through code quality) and
answer accuracy, highlighting its potential as a test-time performance
improvement heuristic.

</details>


### [182] [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/pdf/2503.04598)
*Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma*

Main category: cs.CL

TL;DR: HybridNorm combines Pre-Norm and Post-Norm in transformers, improving gradient flow and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between training stability (Pre-Norm) and performance (Post-Norm) in deep transformer networks.

Method: Proposes HybridNorm: QKV normalization in attention and Post-Norm in FFN.

Result: Outperforms Pre-Norm and Post-Norm in benchmarks, enhancing gradient flow and robustness.

Conclusion: HybridNorm is a stable and effective technique for deep transformer models.

Abstract: Transformers have become the de facto architecture for a wide range of
machine learning tasks, particularly in large language models (LLMs). Despite
their remarkable performance, challenges remain in training deep transformer
networks, especially regarding the position of layer normalization. While
Pre-Norm structures facilitate more stable training owing to their stronger
identity path, they often lead to suboptimal performance compared to Post-Norm.
In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid
normalization strategy that integrates the advantages of both Pre-Norm and
Post-Norm. Specifically, HybridNorm employs QKV normalization within the
attention mechanism and Post-Norm in the feed-forward network (FFN) of each
transformer block. We provide both theoretical insights and empirical evidence
demonstrating that HybridNorm improves gradient flow and model robustness.
Extensive experiments on large-scale transformer models, including both dense
and sparse variants, show that HybridNorm consistently outperforms both
Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings
highlight the potential of HybridNorm as a more stable and effective technique
for improving the training and performance of deep transformer models. Code is
available at https://github.com/BryceZhuo/HybridNorm.

</details>


### [183] [ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews](https://arxiv.org/pdf/2503.08506)
*Xian Gao, Jiacheng Ruan, Jingsheng Gao, Ting Liu, Yuzhuo Fu*

Main category: cs.CL

TL;DR: The paper proposes ReviewAgents, a framework using LLMs to automate academic paper reviews, introduces a dataset (Review-CoT), and benchmarks performance against human reviews.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of academic publications makes manual reviews time-consuming, necessitating automation while maintaining quality.

Method: Develops ReviewAgents, a multi-role LLM framework, trained on the Review-CoT dataset, which emulates human reviewer reasoning.

Result: ReviewAgents outperforms existing LLMs but still lags behind human reviews, though it narrows the gap.

Conclusion: The framework shows promise for automating reviews but requires further improvement to match human quality.

Abstract: Academic paper review is a critical yet time-consuming task within the
research community. With the increasing volume of academic publications,
automating the review process has become a significant challenge. The primary
issue lies in generating comprehensive, accurate, and reasoning-consistent
review comments that align with human reviewers' judgments. In this paper, we
address this challenge by proposing ReviewAgents, a framework that leverages
large language models (LLMs) to generate academic paper reviews. We first
introduce a novel dataset, Review-CoT, consisting of 142k review comments,
designed for training LLM agents. This dataset emulates the structured
reasoning process of human reviewers-summarizing the paper, referencing
relevant works, identifying strengths and weaknesses, and generating a review
conclusion. Building upon this, we train LLM reviewer agents capable of
structured reasoning using a relevant-paper-aware training method. Furthermore,
we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to
enhance the review comment generation process. Additionally, we propose
ReviewBench, a benchmark for evaluating the review comments generated by LLMs.
Our experimental results on ReviewBench demonstrate that while existing LLMs
exhibit a certain degree of potential for automating the review process, there
remains a gap when compared to human-generated reviews. Moreover, our
ReviewAgents framework further narrows this gap, outperforming advanced LLMs in
generating review comments.

</details>


### [184] [Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence](https://arxiv.org/pdf/2503.14749)
*Sophia Hager, David Mueller, Kevin Duh, Nicholas Andrews*

Main category: cs.CL

TL;DR: The paper proposes 'uncertainty distillation' to teach LLMs to verbalize calibrated semantic confidences, ensuring their expressed uncertainty aligns with actual error rates.


<details>
  <summary>Details</summary>
Motivation: LLMs often express inconsistent confidence levels, making their uncertainty unreliable for factual question-answering.

Method: The method involves using held-out data to map initial uncertainty estimates to probabilities, creating examples for supervised fine-tuning.

Result: Uncertainty distillation produces verbalized confidences that correlate well with observed error rates.

Conclusion: The approach improves LLMs' ability to communicate meaningful uncertainty, enhancing their reliability in factual tasks.

Abstract: As large language models (LLMs) are increasingly used for factual
question-answering, it becomes more important for LLMs to have the capability
to communicate the likelihood that their answer is correct. For these
verbalized expressions of uncertainty to be meaningful, they should reflect the
error rates at the expressed level of confidence. However, when prompted to
express confidence, the error rates of current LLMs are inconsistent with their
communicated confidences, highlighting the need for uncertainty quantification
methods. Many prior methods calculate lexical uncertainty, estimating a model's
confidence in the specific string it generated. In some cases, however, it may
be more useful to estimate semantic uncertainty, or the model's confidence in
the answer regardless of how it is verbalized. We propose a simple procedure,
uncertainty distillation, to teach an LLM to verbalize calibrated semantic
confidences. Using held-out data to map initial uncertainty estimates to
meaningful probabilities, we create examples annotated with verbalized
probabilities for supervised fine-tuning. We compare uncertainty distillation
to several strong baselines, and find that our method yields verbalized
confidences that correlate well with observed error rates.

</details>


### [185] [From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment](https://arxiv.org/pdf/2503.15463)
*Jia-Nan Li, Jian Guan, Songhao Wu, Wei Wu, Rui Yan*

Main category: cs.CL

TL;DR: The paper introduces a scalable framework for personalized alignment of large language models (LLMs), addressing diversity in user preferences. It proposes two alignment methods and demonstrates significant improvements over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM alignment assumes uniform human preferences, ignoring user diversity. This work aims to address this gap by enabling personalized alignment.

Method: The paper introduces a preference space and persona representations, alongside the AlignX dataset. Two alignment approaches are developed: in-context alignment and preference-bridged alignment.

Result: Experiments show a 17.06% accuracy gain across benchmarks, with strong adaptation, robustness, and controllability.

Conclusion: The framework advances user-adaptive AI systems by effectively addressing diverse preferences.

Abstract: Large language models (LLMs) have traditionally been aligned through
one-size-fits-all approaches that assume uniform human preferences,
fundamentally overlooking the diversity in user values and needs. This paper
introduces a comprehensive framework for scalable personalized alignment of
LLMs. We establish a systematic preference space characterizing psychological
and behavioral dimensions, alongside diverse persona representations for robust
preference inference in real-world scenarios. Building upon this foundation, we
introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million
personalized preference examples, and develop two complementary alignment
approaches: \textit{in-context alignment} directly conditioning on persona
representations and \textit{preference-bridged alignment} modeling intermediate
preference distributions. Extensive experiments demonstrate substantial
improvements over existing methods, with an average 17.06\% accuracy gain
across four benchmarks while exhibiting a strong adaptation capability to novel
preferences, robustness to limited user data, and precise preference
controllability. These results validate our approach toward user-adaptive AI
systems.

</details>


### [186] [Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets](https://arxiv.org/pdf/2503.16674)
*Molly Kennedy, Ayyoob Imani, Timo Spinde, Hinrich Schütze*

Main category: cs.CL

TL;DR: The study evaluates media bias in LLM-generated content and their ability to detect subtle ideological bias using datasets PoliGen and EconoLex. It reveals Democratic and socialist leanings in political and economic topics, respectively.


<details>
  <summary>Details</summary>
Motivation: Detecting and mitigating subtle, subjective media bias in LLM-generated text is challenging but crucial.

Method: Seven LLMs were prompted to generate articles, and their biases were analyzed using Socratic probing on political (PoliGen) and economic (EconoLex) datasets.

Result: All models showed a Democratic preference in politics, while economic biases varied among Western LLMs, with Chinese models leaning toward socialism.

Conclusion: The study highlights the need for bias-aware LLM development and suggests Socratic probing as an effective tool for direct bias measurement.

Abstract: While detecting and avoiding bias in LLM-generated text is becoming
increasingly important, media bias often remains subtle and subjective, making
it particularly difficult to identify and mitigate. In this study, we assess
media bias in LLM-generated content and LLMs' ability to detect subtle
ideological bias. We conduct this evaluation using two datasets, PoliGen and
EconoLex, covering political and economic discourse, respectively. We evaluate
seven widely used LLMs by prompting them to generate articles and analyze their
ideological preferences via Socratic probing. By using our self-contained
Socratic approach, the study aims to directly measure the models' biases rather
than relying on external interpretations, thereby minimizing subjective
judgments about media bias. Our results reveal a consistent preference of
Democratic over Republican positions across all models. Conversely, in economic
topics, biases vary among Western LLMs, while those developed in China lean
more strongly toward socialism.

</details>


### [187] [Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](https://arxiv.org/pdf/2503.16965)
*Zhe Hu, Jing Li, Zhongzhu Pu, Hou Pong Chan, Yu Yin*

Main category: cs.CL

TL;DR: VLMs can achieve strong decision-making using text-only descriptions, leading to Praxis-VLM, a reasoning model that transfers text-learned reasoning to visual tasks, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: VLMs lack sophisticated situational reasoning for complex decision-making, but text-based reasoning shows promise for improving their capabilities.

Method: Proposes Praxis-VLM, using the GRPO algorithm on textual scenarios to teach reasoning, then transferring these skills to multimodal tasks with visual inputs.

Result: Praxis-VLM outperforms standard supervised fine-tuning in benchmarks, showing superior performance and generalizability.

Conclusion: Text-based reasoning can effectively enhance VLMs' decision-making, reducing reliance on paired image-text data and improving adaptability.

Abstract: Vision Language Models exhibited immense potential for embodied AI, yet they
often lack the sophisticated situational reasoning required for complex
decision-making. This paper shows that VLMs can achieve surprisingly strong
decision-making performance when visual scenes are represented merely as
text-only descriptions, suggesting foundational reasoning can be effectively
learned from language. Motivated by this insight, we propose Praxis-VLM, a
reasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO
algorithm on textual scenarios to instill robust reasoning capabilities, where
models learn to evaluate actions and their consequences. These reasoning
skills, acquired purely from text, successfully transfer to multimodal
inference with visual inputs, significantly reducing reliance on scarce paired
image-text training data. Experiments across diverse decision-making benchmarks
demonstrate that Praxis-VLM substantially outperforms standard supervised
fine-tuning, exhibiting superior performance and generalizability. Further
analysis confirms that our models engage in explicit and effective reasoning,
underpinning their enhanced performance and adaptability.

</details>


### [188] [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/pdf/2503.17287)
*Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang*

Main category: cs.CL

TL;DR: The paper explores how context length and training data complexity impact RL scaling efficiency, proposing FastCuRL, a curriculum RL framework, which outperforms state-of-the-art models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving training efficiency in large-scale RL by investigating context length and data complexity.

Method: Proposes FastCuRL, a curriculum RL framework with stage-wise context scaling, tested on R1-distilled models like DeepSeek-R1-Distill-Qwen-1.5B.

Result: FastCuRL-1.5B-V3 achieves 49.6% accuracy on AIME 2024 and outperforms benchmarks with fewer resources.

Conclusion: Optimal context length and curated data improve RL training efficiency and reasoning capabilities, demonstrated by FastCuRL's success.

Abstract: Improving training efficiency continues to be one of the primary challenges
in large-scale Reinforcement Learning (RL). In this paper, we investigate how
context length and the complexity of training data influence the RL scaling
training process of R1-distilled small reasoning models, e.g.,
DeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply
controlling the context length and curating the training data based on the
input prompt length can effectively improve the training efficiency of scaling
RL, achieving better performance with more concise CoT; (2) properly scaling
the context length helps mitigate entropy collapse; and (3) choosing an optimal
context length can improve the efficiency of model training and incentivize the
model's chain-of-thought reasoning capabilities. Inspired by these insights, we
propose FastCuRL, a curriculum RL framework with stage-wise context scaling to
achieve efficient training and concise CoT reasoning. Experiment results
demonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art
reasoning models on five competition-level benchmarks and achieves 49.6\%
accuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses
DeepScaleR-1.5B-Preview on five benchmarks while only using a single node with
8 GPUs and a total of 50\% of training steps. %The code, training data, and
models will be publicly released.

</details>


### [189] [DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](https://arxiv.org/pdf/2503.19498)
*Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, Manni Duan, Qing Zhang*

Main category: cs.CL

TL;DR: DomainCQA introduces a method for domain-specific Chart Question Answering (CQA) benchmarks, addressing gaps in current evaluations, and demonstrates its effectiveness with AstroChart in astronomy.


<details>
  <summary>Details</summary>
Motivation: Current CQA benchmarks lack domain-specific challenges, limiting the evaluation of Multimodal Large Language Models (MLLMs) in specialized fields.

Method: DomainCQA provides a systematic framework for creating domain-specific CQA benchmarks, exemplified by AstroChart in astronomy.

Result: Evaluation reveals MLLMs struggle with vision-language alignment and domain adaptation, exposing benchmark shortcomings.

Conclusion: DomainCQA offers a scalable, rigorous framework to better assess and enhance MLLMs for domain-specific tasks.

Abstract: Chart Question Answering (CQA) benchmarks are essential for evaluating the
capability of Multimodal Large Language Models (MLLMs) to interpret visual
data. However, current benchmarks focus primarily on the evaluation of
general-purpose CQA but fail to adequately capture domain-specific challenges.
We introduce DomainCQA, a systematic methodology for constructing
domain-specific CQA benchmarks, and demonstrate its effectiveness by developing
AstroChart, a CQA benchmark in the field of astronomy. Our evaluation shows
that current MLLMs face fundamental challenges in vision-language alignment and
domain adaptation, highlighting a critical gap in current benchmarks. By
providing a scalable and rigorous framework, DomainCQA enables more precise
assessment and improvement of MLLMs for domain-specific applications.

</details>


### [190] [Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching](https://arxiv.org/pdf/2503.20083)
*Benjamin Minixhofer, Ivan Vulić, Edoardo Maria Ponti*

Main category: cs.CL

TL;DR: A new cross-tokenizer distillation method enables effective knowledge transfer between LLMs with different tokenizers, outperforming prior methods and expanding applicability.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods require similar tokenizers, limiting their use. This work addresses the need for effective cross-tokenizer distillation.

Method: Develops a principled cross-tokenizer distillation method, tested on three use cases: tokenizer transfer, model distillation, and embedding prediction hypernetworks.

Result: Outperforms prior methods, enables effective transfer across tokenizers, and achieves competitive performance in specialized tasks.

Conclusion: The method expands the range of teacher-student pairs for distillation, enhancing LLM interaction and adaptation.

Abstract: Distillation has shown remarkable success in transferring knowledge from a
Large Language Model (LLM) teacher to a student LLM. However, current
distillation methods require similar tokenizers between the teacher and the
student, restricting their applicability to only a small subset of
teacher-student pairs. In this work, we develop a principled cross-tokenizer
distillation method to solve this crucial deficiency. Our method is the first
to enable effective distillation across fundamentally different tokenizers,
while also substantially outperforming prior methods in all other cases. We
verify the efficacy of our method on three distinct use cases. First, we show
that viewing tokenizer transfer as self-distillation enables unprecedentedly
effective transfer across tokenizers, including rapid transfer of subword
models to the byte-level. Transferring different models to the same tokenizer
also enables ensembling to boost performance. Secondly, we distil a large
maths-specialised LLM into a small general-purpose model with a different
tokenizer, achieving competitive maths problem-solving performance. Thirdly, we
use our method to train state-of-the-art embedding prediction hypernetworks for
training-free tokenizer transfer. Our results unlock an expanded range of
teacher-student pairs for distillation, enabling new ways to adapt and enhance
interaction between LLMs.

</details>


### [191] [Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance](https://arxiv.org/pdf/2504.09753)
*Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Siddhant Gupta, Drishti Sharma, Jebish Purbey, Kanwal Mehreen, Muhammad Arham, Hamza Farooq*

Main category: cs.CL

TL;DR: Mantra-14B, a Hindi-English bilingual LLM, achieves ~3% better benchmark scores than larger models by fine-tuning with curated data, avoiding resource-heavy methods.


<details>
  <summary>Details</summary>
Motivation: Address the underrepresentation of Hindi and other low-resource languages in LLM development.

Method: Instruction tuning of models like Qwen-2.5-14B-Instruct and Phi-4 using 485K English-Hindi samples, experimenting with data ratios.

Result: Improved multilingual performance without compromising native performance, outperforming larger models.

Conclusion: Modest fine-tuning with culturally informed data can bridge performance gaps efficiently; resources released for further research.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their
development has primarily focused on English and other high-resource languages,
leaving many languages underserved. We present our latest Hindi-English
bi-lingual LLM \textbf{Mantra-14B} with ~3\% average improvement in benchmark
scores over both languages, outperforming models twice its size. Using a
curated dataset composed of English and Hindi instruction data of 485K samples,
we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve
performance over both English and Hindi. Our experiments encompassing seven
different LLMs of varying parameter sizes and over 140 training attempts with
varying English-Hindi training data ratios demonstrated that it is possible to
significantly improve multilingual performance without compromising native
performance. Further, our approach avoids resource-intensive techniques like
vocabulary expansion or architectural modifications, thus keeping the model
size small. Our results indicate that modest fine-tuning with culturally and
locally informed data can bridge performance gaps without incurring significant
computational overhead. We release our training code, datasets, and models
under mit and apache licenses to aid further research towards under-represented
and low-resource languages.

</details>


### [192] [Hallucination Detection in LLMs with Topological Divergence on Attention Graphs](https://arxiv.org/pdf/2504.10063)
*Alexandra Bazarova, Aleksandr Yugay, Andrey Shulga, Alina Ermilova, Andrei Volodichev, Konstantin Polev, Julia Belikova, Rauf Parchiev, Dmitry Simakov, Maxim Savchenko, Andrey Savchenko, Serguei Barannikov, Alexey Zaytsev*

Main category: cs.CL

TL;DR: TOHA detects hallucinations in LLMs using topological divergence of attention matrices, achieving competitive results with minimal resources.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs generates factually incorrect content, posing a critical challenge.

Method: TOHA uses a topological divergence metric to analyze structural properties of graphs from attention matrices, comparing prompt and response subgraphs.

Result: Higher divergence in specific attention heads correlates with hallucinations. TOHA achieves state-of-the-art results on QA and summarization tasks.

Conclusion: Topological analysis of attention matrices is an efficient and robust indicator of factual reliability in LLMs.

Abstract: Hallucination, i.e., generating factually incorrect content, remains a
critical challenge for large language models (LLMs). We introduce TOHA, a
TOpology-based HAllucination detector in the RAG setting, which leverages a
topological divergence metric to quantify the structural properties of graphs
induced by attention matrices. Examining the topological divergence between
prompt and response subgraphs reveals consistent patterns: higher divergence
values in specific attention heads correlate with hallucinated outputs,
independent of the dataset. Extensive experiments - including evaluation on
question answering and summarization tasks - show that our approach achieves
state-of-the-art or competitive results on several benchmarks while requiring
minimal annotated data and computational resources. Our findings suggest that
analyzing the topological structure of attention matrices can serve as an
efficient and robust indicator of factual reliability in LLMs.

</details>


### [193] [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/pdf/2504.11952)
*Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Drishti Sharma, Siddhant Gupta, Jebish Purbey, Ashay Srivastava, Subhasya TippaReddy, Arvind Reddy Bobbili, Suraj Telugara Chandrashekhar, Modabbir Adeeb, Srinadh Vura, Hamza Farooq*

Main category: cs.CL

TL;DR: A token classification model for detecting human-LLM co-authored texts, trained on a large dataset, performs well across diverse conditions including unseen domains, generators, and adversarial inputs.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle with detecting AI-generated content in short or partially co-authored texts, necessitating a more robust solution.

Method: Developed token classification models trained on a dataset of 2.4M human-LLM co-authored texts across 23 languages.

Result: Models performed well on unseen domains, generators, non-native texts, and adversarial inputs. Detailed performance metrics by domain, generator, and adversarial method are provided.

Conclusion: The introduced models and dataset offer a robust solution for detecting human-LLM co-authored texts, addressing limitations of existing systems.

Abstract: An ideal detection system for machine generated content is supposed to work
well on any generator as many more advanced LLMs come into existence day by
day. Existing systems often struggle with accurately identifying AI-generated
content over shorter texts. Further, not all texts might be entirely authored
by a human or LLM, hence we focused more over partial cases i.e human-LLM
co-authored texts. Our paper introduces a set of models built for the task of
token classification which are trained on an extensive collection of
human-machine co-authored texts, which performed well over texts of unseen
domains, unseen generators, texts by non-native speakers and those with
adversarial inputs. We also introduce a new dataset of over 2.4M such texts
mostly co-authored by several popular proprietary LLMs over 23 languages. We
also present findings of our models' performance over each texts of each domain
and generator. Additional findings include comparison of performance against
each adversarial method, length of input texts and characteristics of generated
texts compared to the original human authored texts.

</details>


### [194] [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/pdf/2504.12816)
*Xue Wen Tan, Stanley Kok*

Main category: cs.CL

TL;DR: SMARTe is a slot-based method for relational triple extraction that emphasizes interpretability without sacrificing performance, using slot attention for traceable predictions.


<details>
  <summary>Details</summary>
Motivation: Prior RTE research focused on performance optimization, lacking understanding of internal model mechanisms and interpretability.

Method: SMARTe uses slot attention to consolidate information into distinct slots, framing RTE as a set prediction problem for traceable results.

Result: SMARTe matches state-of-the-art performance on NYT and WebNLG datasets while providing interpretability via attention heatmaps.

Conclusion: SMARTe successfully balances interpretability and performance, with qualitative assessments validating its explanations. Future research directions are proposed.

Abstract: Relational Triple Extraction (RTE) is a fundamental task in Natural Language
Processing (NLP). However, prior research has primarily focused on optimizing
model performance, with limited efforts to understand the internal mechanisms
driving these models. Many existing methods rely on complex preprocessing to
induce specific interactions, often resulting in opaque systems that may not
fully align with their theoretical foundations. To address these limitations,
we propose SMARTe: a Slot-based Method for Accountable Relational Triple
extraction. SMARTe introduces intrinsic interpretability through a slot
attention mechanism and frames the task as a set prediction problem. Slot
attention consolidates relevant information into distinct slots, ensuring all
predictions can be explicitly traced to learned slot representations and the
tokens contributing to each predicted relational triple. While emphasizing
interpretability, SMARTe achieves performance comparable to state-of-the-art
models. Evaluations on the NYT and WebNLG datasets demonstrate that adding
interpretability does not compromise performance. Furthermore, we conducted
qualitative assessments to showcase the explanations provided by SMARTe, using
attention heatmaps that map to their respective tokens. We conclude with a
discussion of our findings and propose directions for future research.

</details>


### [195] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/pdf/2504.15253)
*Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty*

Main category: cs.CL

TL;DR: The paper introduces the JETTS benchmark to evaluate LLM-judges in test-time scaling settings, comparing them to reward models across three domains and task settings. Findings show judges are competitive in reranking but underperform in beam search and critique-based refinement.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of LLM-judges as evaluators in test-time scaling settings, given their growing popularity but unclear performance compared to reward models.

Method: The JETTS benchmark evaluates 10 judge models and 8 base generator models across math reasoning, code generation, and instruction following, under reranking, beam search, and critique-based refinement tasks.

Result: Judges are competitive with outcome reward models in reranking but consistently worse than process reward models in beam search. Natural language critiques are ineffective for response refinement.

Conclusion: LLM-judges show promise in reranking but require improvement for beam search and critique-based tasks to match process reward models.

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>


### [196] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/pdf/2504.16084)
*Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, Bowen Zhou*

Main category: cs.CL

TL;DR: TTRL is a novel RL method for training LLMs on unlabeled data, using majority voting for reward estimation, and shows significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reward estimation in RL for LLMs without ground-truth labels.

Method: Introduces Test-Time Reinforcement Learning (TTRL), leveraging majority voting (maj@n) for reward estimation and pre-trained model priors.

Result: TTRL improves pass@1 performance by ~211% on AIME 2024 and surpasses initial model limits.

Conclusion: TTRL is effective for RL on unlabeled data, with potential for broader applications.

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the maj@n metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model maj@n,
and approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>


### [197] [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/pdf/2504.19110)
*Huajian Xin, Luming Li, Xiaoran Jin, Jacques Fleuriot, Wenda Li*

Main category: cs.CL

TL;DR: The paper introduces Automated Proof Engineering (APE) to automate proof tasks using LLMs, presents APE-Bench I for realistic benchmarking, and highlights challenges in complex proof engineering.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs in theorem proving are limited to static tasks, missing real-world iterative workflows. The goal is to automate proof engineering tasks like feature addition and bug fixing.

Method: APE-Bench I is created from Mathlib4 commit histories, featuring file-level tasks verified via Lean compiler and LLM-as-a-Judge. Eleanstic, a parallel verification infrastructure, is developed for scalable proof checking.

Result: State-of-the-art LLMs perform well on localized edits but struggle with complex proof engineering tasks.

Conclusion: The work sets the foundation for agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination and autonomous agents.

Abstract: Recent progress in large language models (LLMs) has shown promise in formal
theorem proving, yet existing benchmarks remain limited to isolated, static
proof tasks, failing to capture the iterative, engineering-intensive workflows
of real-world formal mathematics libraries. Motivated by analogous advances in
software engineering, we introduce the paradigm of Automated Proof Engineering
(APE), which aims to automate proof engineering tasks such as feature addition,
proof refactoring, and bug fixing using LLMs. To facilitate research in this
direction, we present APE-Bench I, the first realistic benchmark built from
real-world commit histories of Mathlib4, featuring diverse file-level tasks
described in natural language and verified via a hybrid approach combining the
Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable
parallel verification infrastructure optimized for proof checking across
multiple versions of Mathlib. Empirical results on state-of-the-art LLMs
demonstrate strong performance on localized edits but substantial degradation
on handling complex proof engineering. This work lays the foundation for
developing agentic workflows in proof engineering, with future benchmarks
targeting multi-file coordination, project-scale verification, and autonomous
agents capable of planning, editing, and repairing formal libraries.

</details>


### [198] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/pdf/2504.21800)
*Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill*

Main category: cs.CL

TL;DR: Synthetic PE therapy dialogues for PTSD are explored as a scalable alternative to real-world data, showing promise for privacy and scalability but struggling with therapeutic fidelity.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns, limited real-world data access, and high annotation costs in healthcare by using synthetic data for clinical model training.

Method: Compare real and synthetic dialogues using linguistic, structural, and PE-specific metrics, including turn-taking and treatment fidelity.

Result: Synthetic data matches structural features (e.g., speaker switch ratio) but lacks fidelity in distress monitoring and other clinical markers.

Conclusion: Synthetic data can complement real-world datasets but requires fidelity-aware metrics to address clinical limitations.

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. Synthetic therapy dialogues closely match structural
features of real-world conversations (e.g., speaker switch ratio: 0.98 vs.
0.99); however, they may not adequately reflect key fidelity markers (e.g.,
distress monitoring). We highlight gaps in existing evaluation frameworks and
advocate for fidelity-aware metrics that go beyond surface fluency to uncover
clinically significant failures. Our findings clarify where synthetic data can
effectively complement real-world datasets -- and where critical limitations
remain.

</details>


### [199] [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/pdf/2505.00063)
*Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Bin Fu, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao*

Main category: cs.CL

TL;DR: The paper introduces GDI-Bench, a benchmark for evaluating multimodal large language models (MLLMs) on document-specific tasks, and proposes GDI-Model to address weaknesses, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the ability to pinpoint model weaknesses or guide systematic improvements in document intelligence tasks.

Method: Developed GDI-Bench with 2.3k images across 9 scenarios and 19 tasks, decoupling visual and reasoning complexity. Proposed GDI-Model with intelligence-preserving training to mitigate catastrophic forgetting.

Result: GDI-Model outperforms existing models on GDI-Bench and previous benchmarks.

Conclusion: GDI-Bench and GDI-Model provide a robust framework for evaluating and improving document intelligence in MLLMs, with open-sourced resources.

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
profoundly impacted the document domain, creating a wide array of application
scenarios. This progress highlights the need for a comprehensive benchmark to
evaluate these models' capabilities across various document-specific tasks.
However, existing benchmarks often fail to locate specific model weaknesses or
guide systematic improvements. To bridge this gap, we introduce a General
Document Intelligence Benchmark (GDI-Bench), featuring 2.3k images across 9 key
scenarios and 19 document-specific tasks. By decoupling visual complexity and
reasoning complexity, the GDI-Bench structures graded tasks that allow
performance assessment by difficulty, aiding in model weakness identification
and optimization guidance. We evaluate various open-source and closed-source
models on GDI-Bench, conducting decoupled analyses in the visual and reasoning
domains, revealing their strengths and weaknesses. To address the diverse tasks
and domains in the GDI-Bench, we propose a GDI-Model that mitigates
catastrophic forgetting during the supervised fine-tuning (SFT) process through
an intelligence-preserving training strategy, thereby reinforcing the inherent
weaknesses of the base model. Our model achieves state-of-the-art performance
on previous benchmarks and the GDI-Bench. Both our benchmark and models are or
will be open-sourced on https://huggingface.co/GDIBench.

</details>


### [200] [Adaptive Thinking via Mode Policy Optimization for Social Language Agents](https://arxiv.org/pdf/2505.02156)
*Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao*

Main category: cs.CL

TL;DR: The paper proposes an Adaptive Mode Learning (AML) framework to enhance language agents' adaptive reasoning in social interactions, outperforming GPT-4o and GRPO in task performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods lack dynamic reasoning depth adjustment in social intelligence simulation, leading to inefficiency and inflexibility.

Method: The AML framework includes hierarchical thinking modes and the AMPO algorithm for context-aware mode switching and reasoning optimization.

Result: AML achieves 15.6% higher task performance than GPT-4o and 7.0% better than GRPO with 32.8% shorter reasoning chains.

Conclusion: AML's adaptive thinking mode selection and optimization significantly improve social intelligence simulation efficiency and performance.

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
studies. Existing methods either lack this kind of reasoning capability or
enforce Long Chain-of-Thought reasoning uniformly across all scenarios,
resulting in excessive token usage and inflexible social simulation. To address
this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning
($\textbf{AML}$) framework in this paper, aiming to improve the adaptive
thinking ability of language agents in dynamic social interactions. To this
end, we first identify hierarchical thinking modes ranging from intuitive
response to deep deliberation based on the cognitive control theory. We then
develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy
$\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the
context-aware mode switching and reasoning. Our framework advances existing
research in three key aspects: (1) Multi-granular thinking mode design, (2)
Context-aware mode switching across social interaction, and (3) Token-efficient
reasoning via depth-adaptive processing. Extensive experiments on social
intelligence benchmarks verify that AML achieves 15.6% higher task performance
than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter
reasoning chains, demonstrating the advantage of adaptive thinking mode
selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.

</details>


### [201] [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/pdf/2505.02172)
*Chuck Arvin*

Main category: cs.CL

TL;DR: Modern LLMs show strong performance on legal benchmarks like CaseHOLD, improving with model size, without needing fine-tuning. A novel anonymization test confirms results aren't due to memorization.


<details>
  <summary>Details</summary>
Motivation: Assess how advanced LLMs perform on legal benchmarks to understand their capabilities and limitations for legal tasks.

Method: Tested LLMs (3B to 90B+ parameters) on CaseHOLD, using a citation anonymization test to check for memorization.

Result: Larger models (e.g., GPT4o, AmazonNovaPro) achieved high F1 scores (0.744, 0.720), competitive with published results. Performance remained strong (0.728 F1) under anonymization.

Conclusion: LLMs show promise for legal tasks but have limitations, impacting automated legal analytics and benchmark development.

Abstract: As large language models (LLMs) continue to advance in capabilities, it is
essential to assess how they perform on established benchmarks. In this study,
we present a suite of experiments to assess the performance of modern LLMs
(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for
identifying case holdings. Our experiments demonstrate ``scaling effects'' -
performance on this task improves with model size, with more capable models
like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720
respectively. These scores are competitive with the best published results on
this dataset, and do not require any technically sophisticated model training,
fine-tuning or few-shot prompting. To ensure that these strong results are not
due to memorization of judicial opinions contained in the training data, we
develop and utilize a novel citation anonymization test that preserves semantic
meaning while ensuring case names and citations are fictitious. Models maintain
strong performance under these conditions (macro F1 of 0.728), suggesting the
performance is not due to rote memorization. These findings demonstrate both
the promise and current limitations of LLMs for legal tasks with important
implications for the development and measurement of automated legal analytics
and legal benchmarks.

</details>


### [202] [Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework](https://arxiv.org/pdf/2505.03563)
*Cléa Chataigner, Rebecca Ma, Prakhar Ganesh, Afaf Taïk, Elliot Creager, Golnoosh Farnadi*

Main category: cs.CL

TL;DR: AUGMENT is a framework for generating and evaluating realistic prompt paraphrases to audit LLMs systematically, addressing biases and performance shifts.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to prompt phrasing, and prior methods lack grounding in real-world linguistic and demographic factors.

Method: AUGMENT uses semantic, stylistic, and instruction-following criteria to create controlled, realistic paraphrases.

Result: User-grounded paraphrasing significantly affects LLM performance and bias metrics in the BBQ dataset.

Conclusion: Structured and representative prompt variation is crucial for reliable LLM auditing.

Abstract: Large language models (LLMs) are sensitive to subtle changes in prompt
phrasing, complicating efforts to audit them reliably. Prior approaches often
rely on arbitrary or ungrounded prompt variations, which may miss key
linguistic and demographic factors in real-world usage. We introduce AUGMENT
(Automated User-Grounded Modeling and Evaluation of Natural Language
Transformations), a framework for systematically generating and evaluating
controlled, realistic prompt paraphrases based on linguistic structure and user
demographics. AUGMENT ensures paraphrase quality through a combination of
semantic, stylistic, and instruction-following criteria. In a case study on the
BBQ dataset, we show that user-grounded paraphrasing leads to significant
shifts in LLM performance and bias metrics across nine models. Our findings
highlight the need for more representative and structured approaches to prompt
variation in LLM auditing.

</details>


### [203] [LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/pdf/2505.05423)
*Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger*

Main category: cs.CL

TL;DR: LiTransProQA is a new LLM-based framework for evaluating literary translations, outperforming existing metrics by focusing on artistic expression and cultural authenticity.


<details>
  <summary>Details</summary>
Motivation: Existing metrics favor mechanical accuracy over artistic quality, risking a decline in translation quality and cultural authenticity.

Method: LiTransProQA integrates insights from professional translators, assessing literary devices, cultural understanding, and authorial voice.

Result: LiTransProQA outperforms current metrics, achieving up to 0.07 gain in correlation and surpassing state-of-the-art metrics by 15+ points.

Conclusion: LiTransProQA matches human-level performance and is applicable to open-source models, offering a training-free tool for literary translation evaluation.

Abstract: The impact of Large Language Models (LLMs) has extended into literary
domains. However, existing evaluation metrics prioritize mechanical accuracy
over artistic expression and tend to overrate machine translation as being
superior to human translation from experienced professionals. In the long run,
this bias could result in an irreversible decline in translation quality and
cultural authenticity. In response to the urgent need for a specialized
literary evaluation metric, we introduce LiTransProQA, a novel, reference-free,
LLM-based question-answering framework designed for literary translation
evaluation. LiTransProQA uniquely integrates insights from professional
literary translators and researchers, focusing on critical elements in literary
quality assessment such as literary devices, cultural understanding, and
authorial voice. Our extensive evaluation shows that while literary-finetuned
XCOMET-XL yields marginal gains, LiTransProQA substantially outperforms current
metrics, achieving up to 0.07 gain in correlation and surpassing the best
state-of-the-art metrics by over 15 points in adequacy assessments.
Incorporating professional translator insights as weights further improves
performance, highlighting the value of translator inputs. Notably, LiTransProQA
reaches human-level evaluation performance comparable to trained student
evaluators. It shows broad applicability to open-source models like
LLaMa3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and
training-free tool for evaluating literary translations that require local
processing due to copyright or ethical considerations. The code and datasets
are available under: https://github.com/zhangr2021/TransProQA.

</details>


### [204] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/pdf/2505.15810)
*Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu*

Main category: cs.CL

TL;DR: The paper identifies challenges in GUI agent grounding tasks using RL and proposes three solutions: a Fast Thinking Template, box size constraint in rewards, and a revised RL objective. Their model, GUI-G1-3B, achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address issues in GUI grounding tasks where general-purpose RL leads to poor performance due to input design, output evaluation, and policy update challenges.

Method: Proposes three solutions: Fast Thinking Template for direct answers, box size constraint in rewards, and difficulty-aware RL objective.

Result: GUI-G1-3B achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro, outperforming larger models.

Conclusion: The proposed solutions effectively improve GUI grounding performance, setting a new benchmark for GUI agents.

Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [205] [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/pdf/2505.10736)
*Ximing Dong, Shaowei Wang, Dayi Lin, Ahmed E. Hassan*

Main category: cs.CL

TL;DR: IPOMP improves prompt optimization by selecting diverse, representative samples and refining them iteratively with real-time model performance, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is inefficient, and existing automated methods use unreliable evaluation subsets, leading to suboptimal prompts.

Method: IPOMP uses semantic clustering and boundary analysis for sample selection, followed by iterative refinement with real-time performance data.

Result: IPOMP improves effectiveness by 1.6% to 5.3% and stability by 57%, with minimal computational overhead (<1%).

Conclusion: IPOMP's real-time performance-guided refinement can universally enhance coreset selection methods for prompt optimization.

Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted
prompts, but manual prompt engineering is labor-intensive and often
ineffective. Automated prompt optimization techniques address this challenge
but the majority of them rely on randomly selected evaluation subsets, which
fail to represent the full dataset, leading to unreliable evaluations and
suboptimal prompts. Existing coreset selection methods, designed for LLM
benchmarking, are unsuitable for prompt optimization due to challenges in
clustering similar samples, high data collection costs, and the unavailability
of performance data for new or private datasets. To overcome these issues, we
propose IPOMP, an Iterative evaluation data selection for effective Prompt
Optimization using real-time Model Performance. IPOMP is a two-stage approach
that selects representative and diverse samples using semantic clustering and
boundary analysis, followed by iterative refinement with real-time model
performance data to replace redundant samples. Evaluations on the BIG-bench
dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by
at least 57% compared with SOTA baselines, with minimal computational overhead
below 1%. Furthermore, the results demonstrate that our real-time
performance-guided refinement approach can be universally applied to enhance
existing coreset selection methods.

</details>


### [206] [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/pdf/2505.10945)
*Seungyoon Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim*

Main category: cs.CL

TL;DR: SALT, a cross-lingual transfer technique, recycles target language PLM embeddings to enhance LLMs, outperforming other methods in performance and convergence.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current multilingual transfer methods that constrain expressive capacity in target languages due to English-dominated source models.

Method: Proposes Semantic Aware Linear Transfer (SALT), using regression lines based on vocabulary overlap to handle non-overlapping token embeddings.

Result: SALT outperforms other methods, achieves lower loss, faster convergence, and remarkable cross-lingual understanding performance.

Conclusion: SALT effectively leverages PLMs to enhance LLMs, demonstrating scalability and superior performance in cross-lingual transfer.

Abstract: Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.

</details>


### [207] [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/pdf/2505.11004)
*Jingcheng Niu, Subhabrata Dutta, Ahmed Elshabrawy, Harish Tayyar Madabushi, Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper investigates in-context learning (ICL) in large-scale Transformer models, showing it's not just memorization but also not a fully symbolic algorithm, while clarifying training dynamics and model capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism behind ICL in Transformer models, which remains controversial, and determine whether it's memorization or algorithmic development.

Method: Uses the Pythia scaling suite with interim checkpoints to explore ICL performance and analyze the residual stream's subspace.

Result: ICL goes beyond memorization but isn't a standalone symbolic algorithm; insights into training dynamics and model capabilities are provided.

Conclusion: Advances understanding of ICL, offering insights for model improvement and AI security guidelines.

Abstract: Large-scale Transformer language models (LMs) trained solely on next-token
prediction with web-scale data can solve a wide range of tasks after seeing
just a few examples. The mechanism behind this capability, known as in-context
learning (ICL), remains both controversial and poorly understood. Some studies
argue that it is merely the result of memorizing vast amounts of data, while
others contend that it reflects a fundamental, symbolic algorithmic development
in LMs. In this work, we introduce a suite of investigative tasks and a novel
method to systematically investigate ICL by leveraging the full Pythia scaling
suite, including interim checkpoints that capture progressively larger amount
of training data. By carefully exploring ICL performance on downstream tasks
and simultaneously conducting a mechanistic analysis of the residual stream's
subspace, we demonstrate that ICL extends beyond mere "memorization" of the
training corpus, yet does not amount to the implementation of an independent
symbolic algorithm. Our results also clarify several aspects of ICL, including
the influence of training dynamics, model capabilities, and elements of
mechanistic interpretability. Overall, our work advances the understanding of
ICL and its implications, offering model developers insights into potential
improvements and providing AI security practitioners with a basis for more
informed guidelines.

</details>


### [208] [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/pdf/2505.11628)
*Berkcan Kapusuzoglu, Supriyo Chakraborty, Chia-Hsuan Lee, Sambit Sahu*

Main category: cs.CL

TL;DR: CGD improves SFT by integrating teacher critiques and refined responses, reducing imitation issues and uncertainty, and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the imitation problem in SFT where models copy responses without understanding.

Method: Propose CGD, a multi-stage framework using teacher critiques and refined responses to train student models.

Result: CGD achieves significant gains on math (+17.5%) and language tasks (+6.3%), reducing format drift.

Conclusion: CGD effectively mitigates imitation and format drift, enhancing model understanding and performance.

Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from
the imitation problem, where the model learns to reproduce the correct
responses without understanding the underlying rationale. To address this
limitation, we propose Critique-Guided Distillation (CGD), a novel multi-stage
framework that integrates teacher model generated explanatory critiques and
refined responses into the SFT process. A student model is then trained to map
the triplet of prompt, teacher critique, and its own initial response to the
corresponding refined teacher response, thereby learning both what to imitate
and why. Using entropy-based analysis, we show that CGD reduces refinement
uncertainty and can be interpreted as a Bayesian posterior update. We perform
extensive empirical evaluation of CGD, on variety of benchmark tasks, and
demonstrate significant gains on both math (AMC23 +17.5%) and language
understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format
drift issues observed in previous critique fine-tuning (CFT) techniques.

</details>


### [209] [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/pdf/2505.12075)
*Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams*

Main category: cs.CL

TL;DR: The paper explores whether different prompting methods (demonstrations vs. instructions) for in-context learning tasks result in similar task representations in language models, using function vectors (FVs) to analyze and compare these representations.


<details>
  <summary>Details</summary>
Motivation: To understand how task representations vary with different prompting methods, aiming to improve interpretability and control over language models.

Method: Generalizes function vectors (FVs) to analyze task representations from both demonstration- and instruction-based prompts, focusing on zero-shot task accuracy.

Result: Demonstration- and instruction-based FVs use different model components, suggesting no common task representation but partially overlapping mechanisms.

Conclusion: Combining instructions and demonstrations is beneficial, but universally monitoring task inference across prompting forms is challenging. Further study of LLM task inference mechanisms is encouraged.

Abstract: Demonstrations and instructions are two primary approaches for prompting
language models to perform in-context learning (ICL) tasks. Do identical tasks
elicited in different ways result in similar representations of the task? An
improved understanding of task representation mechanisms would offer
interpretability insights and may aid in steering models. We study this through
\textit{function vectors} (FVs), recently proposed as a mechanism to extract
few-shot ICL task representations. We generalize FVs to alternative task
presentations, focusing on short textual instruction prompts, and successfully
extract instruction function vectors that promote zero-shot task accuracy. We
find evidence that demonstration- and instruction-based function vectors
leverage different model components, and offer several controls to dissociate
their contributions to task performance. Our results suggest that different
task promptings forms do not induce a common task representation through FVs
but elicit different, partly overlapping mechanisms. Our findings offer
principled support to the practice of combining instructions and task
demonstrations, imply challenges in universally monitoring task inference
across presentation forms, and encourage further examinations of LLM task
inference mechanisms.

</details>


### [210] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/pdf/2505.12082)
*Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Deyi Liu, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Xun Zhou, Siyuan Qiao, Liang Xiang, Yonghui Wu*

Main category: cs.CL

TL;DR: Model merging in large-scale pre-training improves performance and reduces costs, with insights from experiments on dense and MoE architectures.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored potential of model merging in large-scale pre-training and its benefits for efficiency and cost reduction.

Method: Extensive experiments with dense and MoE architectures, merging checkpoints trained with constant learning rates, and ablation studies on strategies and hyperparameters.

Result: Significant performance improvements, accurate prediction of annealing behavior, and lower training costs.

Conclusion: Provides practical pre-training guidelines for effective model merging, benefiting the open-source community.

Abstract: Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.

</details>


### [211] [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/pdf/2505.13176)
*Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang*

Main category: cs.CL

TL;DR: ToolSpectrum is a benchmark for evaluating LLMs' personalized tool utilization, highlighting the need for context-aware personalization beyond functional tool selection.


<details>
  <summary>Details</summary>
Motivation: Existing approaches overlook context-aware personalization in tool selection, leading to suboptimal user satisfaction and inefficient tool use.

Method: ToolSpectrum formalizes user profile and environmental factors to analyze their impact on tool utilization, testing LLMs' capabilities.

Result: Personalized tool utilization improves user experience, but current LLMs struggle to jointly reason about user profiles and environmental factors.

Conclusion: Context-aware personalization is crucial for tool-augmented LLMs, and current models have limitations in balancing user and environmental factors.

Abstract: While integrating external tools into large language models (LLMs) enhances
their ability to access real-time information and domain-specific services,
existing approaches focus narrowly on functional tool selection following user
instructions, overlooking the context-aware personalization in tool selection.
This oversight leads to suboptimal user satisfaction and inefficient tool
utilization, particularly when overlapping toolsets require nuanced selection
based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a
benchmark designed to evaluate LLMs' capabilities in personalized tool
utilization. Specifically, we formalize two key dimensions of personalization,
user profile and environmental factors, and analyze their individual and
synergistic impacts on tool utilization. Through extensive experiments on
ToolSpectrum, we demonstrate that personalized tool utilization significantly
improves user experience across diverse scenarios. However, even
state-of-the-art LLMs exhibit the limited ability to reason jointly about user
profiles and environmental factors, often prioritizing one dimension at the
expense of the other. Our findings underscore the necessity of context-aware
personalization in tool-augmented LLMs and reveal critical limitations for
current models. Our data and code are available at
https://github.com/Chengziha0/ToolSpectrum.

</details>


### [212] [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/pdf/2505.12212)
*Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang*

Main category: cs.CL

TL;DR: Data Whisperer is a training-free, attention-based method for selecting optimal subsets of task-specific data for fine-tuning LLMs, outperforming traditional methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Efficiently selecting optimal subsets for training LLMs is crucial due to growing dataset sizes and computational costs. Traditional methods are either resource-intensive or heuristic-based.

Method: Proposes Data Whisperer, leveraging few-shot in-context learning with the target model, eliminating the need for fine-tuning a scoring model.

Result: Achieves superior performance with 10% of the GSM8K dataset on Llama-3-8B-Instruct, outperforming existing methods by 3.1 points with a 7.4x speedup.

Conclusion: Data Whisperer offers an efficient, scalable solution for data selection in LLM fine-tuning, balancing performance and computational costs.

Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential
for their effective deployment. As dataset sizes grow, efficiently selecting
optimal subsets for training becomes crucial to balancing performance and
computational costs. Traditional data selection methods often require
fine-tuning a scoring model on the target dataset, which is time-consuming and
resource-intensive, or rely on heuristics that fail to fully leverage the
model's predictive capabilities. To address these challenges, we propose Data
Whisperer, an efficient, training-free, attention-based method that leverages
few-shot in-context learning with the model to be fine-tuned. Comprehensive
evaluations were conducted on both raw and synthetic datasets across diverse
tasks and models. Notably, Data Whisperer achieves superior performance
compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just
10% of the data, and outperforms existing methods with a 3.1-point improvement
and a 7.4$\times$ speedup.

</details>


### [213] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/pdf/2505.14079)
*Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei*

Main category: cs.CL

TL;DR: The paper proposes BAR, a backward reasoning-based agent for complex tasks in Minecraft, outperforming forward reasoning methods.


<details>
  <summary>Details</summary>
Motivation: Forward reasoning struggles with complex tasks due to the perception gap between initial state and goal. Backward reasoning is explored as a solution.

Method: BAR uses recursive goal decomposition, state consistency maintenance, and stage memory for robust planning from the terminal state.

Result: BAR outperforms existing methods, demonstrating the effectiveness of its modules.

Conclusion: Backward reasoning, implemented via BAR, is superior for complex task planning in environments like Minecraft.

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [214] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/pdf/2505.15046)
*Yifan Wu, Lutao Yan, Leixian Shen, Yinan Mei, Jiannan Wang, Yuyu Luo*

Main category: cs.CL

TL;DR: ChartCards is a framework for generating unified chart metadata to support multi-task chart understanding, reducing the need for large datasets. MetaChart, a dataset built using ChartCards, improves model performance by 5% on average across tasks.


<details>
  <summary>Details</summary>
Motivation: High data collection and training costs for fine-tuning MLLMs on fine-grained chart understanding tasks necessitate a more efficient solution.

Method: Proposes ChartCards, a framework synthesizing chart information into structured metadata, and constructs MetaChart dataset for validation.

Result: Fine-tuning on MetaChart improves performance by 5% on average, with notable gains in text-to-chart retrieval (17%) and chart-to-table tasks (28%).

Conclusion: ChartCards and MetaChart offer an efficient, high-quality solution for multi-task chart understanding, reducing reliance on large datasets.

Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.

</details>


### [215] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/pdf/2505.14464)
*Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: The paper explores distillation to boost reasoning in language models, using data from three teachers. AM-Thinking-v1-distilled data shows superior diversity and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in open-source language models through verified distillation data.

Method: Collect outputs from three teacher models on 1.89M queries, analyze distributions, and train student models on each dataset.

Result: AM-Thinking-v1-distilled data leads to the best student performance across benchmarks, with adaptive response lengths.

Conclusion: High-quality, verified reasoning traces are valuable; datasets are released for future research.

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The model distilled from AM-Thinking-v1 consistently achieves the best
performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and
65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing
longer responses for harder tasks and shorter ones for simpler tasks. These
findings highlight the value of high-quality, verified reasoning traces. We
release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support
future research on open and high-performing reasoning-oriented language models.
The datasets are publicly available on Hugging Face\footnote{Datasets are
available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [216] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/pdf/2505.14652)
*Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen*

Main category: cs.CL

TL;DR: General-Reasoner enhances LLM reasoning across diverse domains by using a novel training paradigm, a large-scale dataset, and a generative model-based answer verifier, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning methods are limited to mathematical and coding domains due to data abundance and answer verification ease, restricting broader applicability.

Method: Proposes General-Reasoner with a large-scale dataset from web crawling and a generative model-based answer verifier for diverse domains.

Result: Outperforms baselines on 12 benchmarks, showing robust and generalizable reasoning across domains like physics, chemistry, and finance.

Conclusion: General-Reasoner advances LLM reasoning beyond narrow domains, maintaining effectiveness in mathematical tasks while excelling in diverse disciplines.

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [217] [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/pdf/2505.15255)
*Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen*

Main category: cs.CL

TL;DR: The paper introduces MentalMAC, a method to improve LLMs' detection of mental manipulation in dialogues, using multi-task anti-curriculum distillation and unsupervised data expansion.


<details>
  <summary>Details</summary>
Motivation: Mental manipulation is hard to detect due to its covert nature and lack of annotated datasets, limiting LLMs' effectiveness.

Method: Proposes MentalMAC with EvoSA for data expansion, multi-task supervision, and progressive knowledge distillation. Uses the ReaMent dataset for validation.

Result: MentalMAC significantly improves detection performance, narrowing the gap between student and teacher models.

Conclusion: The method advances LLMs' ability to detect mental manipulation, with resources to be released publicly.

Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse
that poses serious threats to mental health. Its covert nature and the
complexity of manipulation strategies make it challenging to detect, even for
state-of-the-art large language models (LLMs). This concealment also hinders
the manual collection of large-scale, high-quality annotations essential for
training effective models. Although recent efforts have sought to improve LLMs'
performance on this task, progress remains limited due to the scarcity of
real-world annotated datasets. To address these challenges, we propose
MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'
ability to detect mental manipulation in multi-turn dialogue. Our approach
includes: (i) EvoSA, an unsupervised data expansion method based on
evolutionary operations and speech act theory; (ii) teacher model-generated
multi-task supervision; and (iii) progressive knowledge distillation from
complex to simpler tasks. We then constructed the ReaMent dataset with 5,000
real-world dialogue samples, using a MentalMAC-distilled model to assist human
annotation. Vast experiments demonstrate that our method significantly narrows
the gap between student and teacher models and outperforms competitive LLMs
across key evaluation metrics. All code, datasets, and checkpoints will be
released upon paper acceptance. Warning: This paper contains content that may
be offensive to readers.

</details>


### [218] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/pdf/2505.15553)
*Angelie Kraft, Judith Simon, Sonja Schimmler*

Main category: cs.CL

TL;DR: The paper highlights biases in QA and RC benchmarks due to lack of diversity in their creation, calling for more transparent and bias-aware practices.


<details>
  <summary>Details</summary>
Motivation: To assess biases in QA and RC benchmarks and advocate for fairer LLM development.

Method: Qualitative analysis of 30 benchmark papers and quantitative analysis of 20 datasets.

Result: Found insufficient transparency in benchmark creation and widespread biases (gender, religion, geography).

Conclusion: Urges more transparent, bias-aware practices for fairer benchmarks and LLMs.

Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.

</details>


### [219] [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/pdf/2505.15431)
*Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu*

Main category: cs.CL

TL;DR: Hunyuan-TurboS is a hybrid Transformer-Mamba MoE model combining efficiency and contextual understanding, achieving top performance with optimized computational resources.


<details>
  <summary>Details</summary>
Motivation: To create a high-performance, efficient large-scale model by synergizing Mamba's long-sequence efficiency and Transformer's contextual strengths.

Method: Uses a 56B activated parameter model with 128 layers (Mamba2, Attention, FFN), adaptive CoT, and MoE. Pre-trained on 16T tokens, enhanced via post-training strategies.

Result: Top 7 rank on LMSYS Chatbot Arena (1356 score), outperforms competitors, and averages 77.9% on 23 benchmarks.

Conclusion: Hunyuan-TurboS sets a new paradigm for efficient large-scale models, balancing performance and cost.

Abstract: As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep "thinking" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.

</details>


### [220] [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/pdf/2505.15472)
*Song Dai, Yibo Yan, Jiamin Su, Dongfang Zihao, Yubo Gao, Yonghua Hei, Jungang Li, Junyan Zhang, Sicheng Tao, Zhuoran Gao, Xuming Hu*

Main category: cs.CL

TL;DR: PhysicsArena is introduced as the first multimodal benchmark to evaluate MLLMs in physics reasoning, addressing gaps in variable identification, process formulation, and solution derivation.


<details>
  <summary>Details</summary>
Motivation: Current physics benchmarks lack multimodal inputs and overlook intermediate reasoning steps, limiting MLLMs' application in physics.

Method: PhysicsArena is designed to holistically assess MLLMs across variable identification, process formulation, and solution derivation.

Result: The benchmark aims to advance MLLMs' multimodal physics reasoning capabilities.

Conclusion: PhysicsArena provides a comprehensive platform for evaluating and improving MLLMs in physics reasoning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in diverse reasoning tasks, yet their application to complex
physics reasoning remains underexplored. Physics reasoning presents unique
challenges, requiring grounding in physical conditions and the interpretation
of multimodal information. Current physics benchmarks are limited, often
focusing on text-only inputs or solely on problem-solving, thereby overlooking
the critical intermediate steps of variable identification and process
formulation. To address these limitations, we introduce PhysicsArena, the first
multimodal physics reasoning benchmark designed to holistically evaluate MLLMs
across three critical dimensions: variable identification, physical process
formulation, and solution derivation. PhysicsArena aims to provide a
comprehensive platform for assessing and advancing the multimodal physics
reasoning abilities of MLLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [221] [Multilinear subspace learning for person re-identification based fusion of high order tensor features](https://arxiv.org/pdf/2505.15825)
*Ammar Chouchane, Mohcene Bessaoudi, Hamza Kheddar, Abdelmalik Ouamane, Tiago Vieira, Mahmoud Hassaballah*

Main category: cs.CV

TL;DR: The paper proposes High-Dimensional Feature Fusion (HDFF) for Person Re-Identification (PRe-ID), combining CNN and LOMO features using tensor fusion. TXQDA is used for subspace learning, achieving superior results on VIPeR, GRID, and PRID450S datasets.


<details>
  <summary>Details</summary>
Motivation: PRe-ID is challenging in video surveillance. Effective feature extraction and representation are key to identifying and tracking individuals across cameras.

Method: HDFF combines CNN and LOMO features via tensor fusion. TXQDA handles subspace learning, and cosine similarity is used for matching.

Result: The method outperforms state-of-the-art techniques on VIPeR, GRID, and PRID450S datasets.

Conclusion: HDFF with TXQDA effectively enhances PRe-ID accuracy by leveraging high-dimensional feature fusion.

Abstract: Video surveillance image analysis and processing is a challenging field in
computer vision, with one of its most difficult tasks being Person
Re-Identification (PRe-ID). PRe-ID aims to identify and track target
individuals who have already been detected in a network of cameras, using a
robust description of their pedestrian images. The success of recent research
in person PRe-ID is largely due to effective feature extraction and
representation, as well as the powerful learning of these features to reliably
discriminate between pedestrian images. To this end, two powerful features,
Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are
modeled on multidimensional data using the proposed method, High-Dimensional
Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced
to leverage and combine these two types of features in a single tensor, even
though their dimensions are not identical. To enhance the system's accuracy, we
employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace
learning, followed by cosine similarity for matching. TXQDA efficiently
facilitates learning while reducing the high dimensionality inherent in
high-order tensor data. The effectiveness of our approach is verified through
experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S.
Extensive experiments demonstrate that our approach outperforms recent
state-of-the-art methods.

</details>


### [222] [CP-LLM: Context and Pixel Aware Large Language Model for Video Quality Assessment](https://arxiv.org/pdf/2505.16025)
*Wen Wen, Yaohong Wu, Yue Sheng, Neil Birkbeck, Balu Adsumilli, Yilin Wang*

Main category: cs.CV

TL;DR: CP-LLM is a multimodal LLM for video quality assessment, combining high-level context and low-level pixel analysis to improve accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional VQA models (lack of contextual understanding) and LLM-based models (insensitivity to small distortions or task separation).

Method: Dual vision encoders for high-level and low-level analysis, with a language decoder to reason about their interplay. Multi-task training for score prediction, description generation, and comparisons.

Result: State-of-the-art cross-dataset performance and superior robustness to pixel distortions.

Conclusion: CP-LLM effectively bridges the gap in VQA by integrating context and pixel awareness, offering practical utility.

Abstract: Video quality assessment (VQA) is a challenging research topic with broad
applications. Effective VQA necessitates sensitivity to pixel-level distortions
and a comprehensive understanding of video context to accurately determine the
perceptual impact of distortions. Traditional hand-crafted and learning-based
VQA models mainly focus on pixel-level distortions and lack contextual
understanding, while recent LLM-based models struggle with sensitivity to small
distortions or handle quality scoring and description as separate tasks. To
address these shortcomings, we introduce CP-LLM: a Context and Pixel aware
Large Language Model. CP-LLM is a novel multimodal LLM architecture featuring
dual vision encoders designed to independently analyze perceptual quality at
both high-level (video context) and low-level (pixel distortion) granularity,
along with a language decoder subsequently reasons about the interplay between
these aspects. This design enables CP-LLM to simultaneously produce robust
quality scores and interpretable quality descriptions, with enhanced
sensitivity to pixel distortions (e.g. compression artifacts). The model is
trained via a multi-task pipeline optimizing for score prediction, description
generation, and pairwise comparisons. Experiment results demonstrate that
CP-LLM achieves state-of-the-art cross-dataset performance on established VQA
benchmarks and superior robustness to pixel distortions, confirming its
efficacy for comprehensive and practical video quality assessment in real-world
scenarios.

</details>


### [223] [Generative AI for Autonomous Driving: A Review](https://arxiv.org/pdf/2505.15863)
*Katharina Winter, Abhishek Vivekanandan, Rupert Polley, Yinzhe Shen, Christian Schlauch, Mohamed-Khalil Bouzidi, Bojan Derajic, Natalie Grabowsky, Annajoyce Mariani, Dennis Rochau, Giovanni Lucente, Harsh Yadav, Firas Mualla, Adam Molin, Sebastian Bernhard, Christian Wirth, Ömer Şahin Taş, Nadja Klein, Fabian B. Flohr, Hanno Gottschalk*

Main category: cs.CV

TL;DR: The paper explores how Generative AI (GenAI) enhances Autonomous Driving (AD) tasks, comparing various generative models and hybrid methods, while addressing challenges like safety and real-time capabilities.


<details>
  <summary>Details</summary>
Motivation: To leverage GenAI for improving AD tasks such as map creation, scenario generation, and motion planning, and to compare the effectiveness of different generative models.

Method: Examines generative models (VAEs, GANs, INNs, GTs, DMs) and hybrid approaches, evaluates their capabilities, and identifies datasets and open research questions.

Result: Highlights the adaptability and robustness of hybrid methods and outlines challenges (safety, interpretability, real-time performance) with recommendations for future work.

Conclusion: GenAI holds significant potential for AD, but challenges remain; future research should focus on safety, interpretability, and real-time applications.

Abstract: Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving
(AD), extending beyond traditional applications in text, image, and video
generation. We explore how generative models can enhance automotive tasks, such
as static map creation, dynamic scenario generation, trajectory forecasting,
and vehicle motion planning. By examining multiple generative approaches
ranging from Variational Autoencoder (VAEs) over Generative Adversarial
Networks (GANs) and Invertible Neural Networks (INNs) to Generative
Transformers (GTs) and Diffusion Models (DMs), we highlight and compare their
capabilities and limitations for AD-specific applications. Additionally, we
discuss hybrid methods integrating conventional techniques with generative
approaches, and emphasize their improved adaptability and robustness. We also
identify relevant datasets and outline open research questions to guide future
developments in GenAI. Finally, we discuss three core challenges: safety,
interpretability, and realtime capabilities, and present recommendations for
image generation, dynamic scenario generation, and planning.

</details>


### [224] [How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads](https://arxiv.org/pdf/2505.15865)
*Ingeol Baek, Hwan Chang, Sunghyun Ryu, Hwanhee Lee*

Main category: cs.CV

TL;DR: The paper identifies and analyzes OCR Heads in LVLMs, revealing their unique properties and impact on text recognition in images.


<details>
  <summary>Details</summary>
Motivation: To address the interpretability gap in LVLMs regarding how they locate and interpret textual information within images.

Method: Exploration of LVLMs to identify OCR Heads, analysis of their properties, and validation through downstream tasks like CoT and masking.

Result: OCR Heads are less sparse, qualitatively distinct, and statically activated, with redistributing sink-token values improving performance.

Conclusion: The findings enhance understanding of LVLMs' internal mechanisms for processing embedded text in images.

Abstract: Despite significant advancements in Large Vision Language Models (LVLMs), a
gap remains, particularly regarding their interpretability and how they locate
and interpret textual information within images. In this paper, we explore
various LVLMs to identify the specific heads responsible for recognizing text
from images, which we term the Optical Character Recognition Head (OCR Head).
Our findings regarding these heads are as follows: (1) Less Sparse: Unlike
previous retrieval heads, a large number of heads are activated to extract
textual information from images. (2) Qualitatively Distinct: OCR heads possess
properties that differ significantly from general retrieval heads, exhibiting
low similarity in their characteristics. (3) Statically Activated: The
frequency of activation for these heads closely aligns with their OCR scores.
We validate our findings in downstream tasks by applying Chain-of-Thought (CoT)
to both OCR and conventional retrieval heads and by masking these heads. We
also demonstrate that redistributing sink-token values within the OCR heads
improves performance. These insights provide a deeper understanding of the
internal mechanisms LVLMs employ in processing embedded textual information in
images.

</details>


### [225] [DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor](https://arxiv.org/pdf/2505.16256)
*Yan Zhao, Zhengxue Cheng, Junxuan Zhang, Qunshan Gu, Qi Wang, Li Song*

Main category: cs.CV

TL;DR: DualComp is a unified, lightweight learning-based compressor for images and text, addressing modality heterogeneity with efficient parameter use and near real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing compressors lack flexibility for multi-modal data, and MLLMs are too complex for practical use.

Method: DualComp uses modality-unified tokenization, modality-switching contextual learning, and modality-routing mixture-of-experts, with reparameterization training.

Result: Achieves SOTA performance for text and image compression with fewer parameters, and surpasses previous image compressors by 9% with 1.2% model size.

Conclusion: DualComp offers a practical, efficient solution for dual-modality compression, balancing performance and simplicity.

Abstract: Most learning-based lossless compressors are designed for a single modality,
requiring separate models for multi-modal data and lacking flexibility.
However, different modalities vary significantly in format and statistical
properties, making it ineffective to use compressors that lack
modality-specific adaptations. While multi-modal large language models (MLLMs)
offer a potential solution for modality-unified compression, their excessive
complexity hinders practical deployment. To address these challenges, we focus
on the two most common modalities, image and text, and propose DualComp, the
first unified and lightweight learning-based dual-modality lossless compressor.
Built on a lightweight backbone, DualComp incorporates three key structural
enhancements to handle modality heterogeneity: modality-unified tokenization,
modality-switching contextual learning, and modality-routing
mixture-of-experts. A reparameterization training strategy is also used to
boost compression performance. DualComp integrates both modality-specific and
shared parameters for efficient parameter utilization, enabling near real-time
inference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp
achieves compression performance on par with the SOTA LLM-based methods for
both text and image datasets. Its simplified single-modality variant surpasses
the previous best image compressor on the Kodak dataset by about 9% using just
1.2% of the model size.

</details>


### [226] [SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval](https://arxiv.org/pdf/2505.15867)
*Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou*

Main category: cs.CV

TL;DR: The paper introduces SCENIR, an unsupervised scene graph-based retrieval framework that outperforms existing methods by focusing on semantic content over low-level features. It replaces caption-based supervision with Graph Edit Distance (GED) for robust evaluation.


<details>
  <summary>Details</summary>
Motivation: Current image-to-image retrieval models are biased by low-level features and rely on inconsistent caption-based supervision. The paper aims to improve semantic understanding and retrieval reliability.

Method: Proposes SCENIR, a Graph Autoencoder-based unsupervised framework, eliminating the need for labeled data and using GED for ground truth similarity.

Result: SCENIR outperforms vision-based, multimodal, and supervised GNN methods in performance and efficiency.

Conclusion: The framework advances state-of-the-art in retrieval and generalizes well to unannotated datasets, promoting semantic-focused evaluation.

Abstract: Despite the dominance of convolutional and transformer-based architectures in
image-to-image retrieval, these models are prone to biases arising from
low-level visual features, such as color. Recognizing the lack of semantic
understanding as a key limitation, we propose a novel scene graph-based
retrieval framework that emphasizes semantic content over superficial image
characteristics. Prior approaches to scene graph retrieval predominantly rely
on supervised Graph Neural Networks (GNNs), which require ground truth graph
pairs driven from image captions. However, the inconsistency of caption-based
supervision stemming from variable text encodings undermine retrieval
reliability. To address these, we present SCENIR, a Graph Autoencoder-based
unsupervised retrieval framework, which eliminates the dependence on labeled
training data. Our model demonstrates superior performance across metrics and
runtime efficiency, outperforming existing vision-based, multimodal, and
supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as
a deterministic and robust ground truth measure for scene graph similarity,
replacing the inconsistent caption-based alternatives for the first time in
image-to-image retrieval evaluation. Finally, we validate the generalizability
of our method by applying it to unannotated datasets via automated scene graph
generation, while substantially contributing in advancing state-of-the-art in
counterfactual image retrieval.

</details>


### [227] [Joint Flow And Feature Refinement Using Attention For Video Restoration](https://arxiv.org/pdf/2505.16434)
*Ranjith Merugu, Mohammad Sameer Suhail, Akshay P Sarashetti, Venkata Bharath Reddy Reddem, Pankaj Kumar Bajpai, Amit Satish Unde*

Main category: cs.CV

TL;DR: A novel video restoration framework, JFFRA, improves temporal consistency and performance by iteratively refining flow and features using attention, outperforming state-of-the-art methods by up to 1.62 dB.


<details>
  <summary>Details</summary>
Motivation: Existing video restoration methods struggle with temporal consistency due to reliance on degraded inputs. JFFRA addresses this by refining flow and features iteratively.

Method: JFFRA uses a synergistic approach to refine flow and features at multiple scales, reducing reliance on precise flow estimation, and includes an occlusion-aware loss to reduce flickering.

Result: JFFRA achieves up to 1.62 dB improvement over state-of-the-art methods in tasks like denoising, deblurring, and super-resolution.

Conclusion: JFFRA effectively enhances video restoration by improving temporal consistency and feature quality, setting a new benchmark for performance.

Abstract: Recent advancements in video restoration have focused on recovering
high-quality video frames from low-quality inputs. Compared with static images,
the performance of video restoration significantly depends on efficient
exploitation of temporal correlations among successive video frames. The
numerous techniques make use of temporal information via flow-based strategies
or recurrent architectures. However, these methods often encounter difficulties
in preserving temporal consistency as they utilize degraded input video frames.
To resolve this issue, we propose a novel video restoration framework named
Joint Flow and Feature Refinement using Attention (JFFRA). The proposed JFFRA
is based on key philosophy of iteratively enhancing data through the
synergistic collaboration of flow (alignment) and restoration. By leveraging
previously enhanced features to refine flow and vice versa, JFFRA enables
efficient feature enhancement using temporal information. This interplay
between flow and restoration is executed at multiple scales, reducing the
dependence on precise flow estimation. Moreover, we incorporate an
occlusion-aware temporal loss function to enhance the network's capability in
eliminating flickering artifacts. Comprehensive experiments validate the
versatility of JFFRA across various restoration tasks such as denoising,
deblurring, and super-resolution. Our method demonstrates a remarkable
performance improvement of up to 1.62 dB compared to state-of-the-art
approaches.

</details>


### [228] [Satellites Reveal Mobility: A Commuting Origin-destination Flow Generator for Global Cities](https://arxiv.org/pdf/2505.15870)
*Can Rong, Xin Zhang, Yanxin Xi, Hongjie Sui, Jingtao Ding, Yong Li*

Main category: cs.CV

TL;DR: GlODGen uses satellite imagery and population data to generate commuting OD flows globally, achieving high accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Traditional OD flow data collection is costly and privacy-invasive; satellite imagery offers a scalable, privacy-preserving alternative.

Method: GlODGen extracts urban semantic signals from satellite imagery using Vision-Language Geo-Foundation Models, combines them with population data, and generates OD flows via graph diffusion models.

Result: GlODGen achieves over 98% expressiveness compared to traditional data sources and performs well across diverse cities on 4 continents.

Conclusion: GlODGen is a scalable, automated tool for generating accurate OD flow data globally, leveraging publicly available satellite imagery.

Abstract: Commuting Origin-destination~(OD) flows, capturing daily population mobility
of citizens, are vital for sustainable development across cities around the
world. However, it is challenging to obtain the data due to the high cost of
travel surveys and privacy concerns. Surprisingly, we find that satellite
imagery, publicly available across the globe, contains rich urban semantic
signals to support high-quality OD flow generation, with over 98\%
expressiveness of traditional multisource hard-to-collect urban
sociodemographic, economics, land use, and point of interest data. This
inspires us to design a novel data generator, GlODGen, which can generate OD
flow data for any cities of interest around the world. Specifically, GlODGen
first leverages Vision-Language Geo-Foundation Models to extract urban semantic
signals related to human mobility from satellite imagery. These features are
then combined with population data to form region-level representations, which
are used to generate OD flows via graph diffusion models. Extensive experiments
on 4 continents and 6 representative cities show that GlODGen has great
generalizability across diverse urban environments on different continents and
can generate OD flow data for global cities highly consistent with real-world
mobility data. We implement GlODGen as an automated tool, seamlessly
integrating data acquisition and curation, urban semantic feature extraction,
and OD flow generation together. It has been released at
https://github.com/tsinghua-fib-lab/generate-od-pubtools.

</details>


### [229] [Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging](https://arxiv.org/pdf/2505.15875)
*Shenghe Zheng, Hongzhi Wang, Chenyu Huang, Xiaohui Wang, Tao Chen, Jiayuan Fan, Shuyue Hu, Peng Ye*

Main category: cs.CV

TL;DR: DO-Merging improves model merging for LoRA by decoupling and orthogonalizing parameters, outperforming existing methods with minimal cost.


<details>
  <summary>Details</summary>
Motivation: Current merging methods perform poorly on LoRA due to large parameter magnitude variance, leading to information loss and degraded performance.

Method: Proposes DO-Merging: decouples parameters into magnitude and direction, merges independently, and uses data-free, layer-wise gradient descent with orthogonal constraints.

Result: DO-Merging achieves significantly higher performance across vision, language, and multi-modal tasks compared to existing methods.

Conclusion: DO-Merging is a flexible, cost-effective solution for merging LoRA models, offering near-free improvements.

Abstract: With more open-source models available for diverse tasks, model merging has
gained attention by combining models into one, reducing training, storage, and
inference costs. Current research mainly focuses on model merging for full
fine-tuning, overlooking the popular LoRA. However, our empirical analysis
reveals that: a) existing merging methods designed for full fine-tuning perform
poorly on LoRA; b) LoRA modules show much larger parameter magnitude variance
than full fine-tuned weights; c) greater parameter magnitude variance
correlates with worse merging performance. Considering that large magnitude
variances cause deviations in the distribution of the merged parameters,
resulting in information loss and performance degradation, we propose a
Decoupled and Orthogonal merging approach(DO-Merging). By separating parameters
into magnitude and direction components and merging them independently, we
reduce the impact of magnitude differences on the directional alignment of the
merged models, thereby preserving task information. Furthermore, we introduce a
data-free, layer-wise gradient descent method with orthogonal constraints to
mitigate interference during the merging of direction components. We provide
theoretical guarantees for both the decoupling and orthogonal components. And
we validate through extensive experiments across vision, language, and
multi-modal domains that our proposed DO-Merging can achieve significantly
higher performance than existing merging methods at a minimal cost. Notably,
each component can be flexibly integrated with existing methods, offering near
free-lunch improvements across tasks.

</details>


### [230] [CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation](https://arxiv.org/pdf/2505.16663)
*Haihong Hao, Mingfei Han, Changlin Li, Zhihui Li, Xiaojun Chang*

Main category: cs.CV

TL;DR: CoNav introduces a collaborative cross-modal reasoning framework for embodied navigation, combining 2D images, 3D point clouds, and text to improve performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in unified fusion of 2D images, 3D point clouds, and text for embodied navigation, particularly limited data and conflicting beliefs among modalities.

Method: CoNav uses a pretrained 3D-text model to guide an image-text navigation agent via Cross-Modal Belief Alignment, sharing textual hypotheses and fine-tuning on a small corpus.

Result: Significant improvements on four embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D), with shorter paths (higher SPL).

Conclusion: CoNav demonstrates the potential of cross-modal fusion in embodied navigation, though challenges remain.

Abstract: Embodied navigation demands comprehensive scene understanding and precise
spatial reasoning. While image-text models excel at interpreting pixel-level
color and lighting cues, 3D-text models capture volumetric structure and
spatial relationships. However, unified fusion approaches that jointly fuse 2D
images, 3D point clouds, and textual instructions face challenges in limited
availability of triple-modality data and difficulty resolving conflicting
beliefs among modalities. In this work, we introduce CoNav, a collaborative
cross-modal reasoning framework where a pretrained 3D-text model explicitly
guides an image-text navigation agent by providing structured spatial-semantic
knowledge to resolve ambiguities during navigation. Specifically, we introduce
Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance
by simply sharing textual hypotheses from the 3D-text model to the navigation
agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the
navigation agent learns to integrate visual cues with spatial-semantic
knowledge derived from the 3D-text model, enabling effective reasoning in
embodied navigation. CoNav achieves significant improvements on four standard
embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial
reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success
Rate, CoNav often generates shorter paths compared to other methods (as
measured by SPL), showcasing the potential and challenges of fusing data from
different modalities in embodied navigation. Project Page:
https://oceanhao.github.io/CoNav/

</details>


### [231] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/pdf/2505.15877)
*Siting Li, Xiang Gao, Simon Shaolei Du*

Main category: cs.CV

TL;DR: The paper introduces COCO-Facet, a benchmark for evaluating text-to-image retrievers on attribute-focused queries, revealing limitations in current models like CLIP and MLLM-based retrievers. It proposes promptable image embeddings to improve performance and offers acceleration strategies.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image retrievers like CLIP and MLLM-based models struggle with attribute-focused queries due to their focus on global semantics, leaving out crucial details. This limits their effectiveness in real-world applications.

Method: The authors build COCO-Facet, a benchmark with 9,112 queries, to evaluate retrievers. They propose promptable image embeddings to highlight required attributes and introduce two acceleration strategies: pre-processing embeddings and linear approximations.

Result: Promptable embeddings significantly improve performance, with pre-processing yielding a 15% improvement in Recall@5 for predefined prompts and linear approximations achieving an 8% improvement for inference-time prompts.

Conclusion: General image embeddings are suboptimal for attribute-focused queries. Promptable embeddings, combined with acceleration strategies, offer a scalable and effective solution, enhancing real-world applicability.

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [232] [Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval](https://arxiv.org/pdf/2505.16756)
*Hailong Ning, Siying Wang, Tao Lei, Xiaopeng Cao, Huanmin Dou, Bin Zhao, Asoke K. Nandi, Petia Radeva*

Main category: cs.CV

TL;DR: The paper proposes a Representation Discrepancy Bridging (RDB) method to address imbalanced cross-modal optimization in RSITR, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods for VLP models suffer from text modality dominance, hindering image representation learning and cross-modal alignment.

Method: RDB introduces a Cross-Modal Asymmetric Adapter (CMAA) with Visual Enhancement Adapter (VEA) and Text Semantic Adapter (TSA), and a Dual-Task Consistency Loss (DTCL) for robust alignment.

Result: RDB improves mR metrics by 6%-11% over PEFT methods and 1.15%-2% over GeoRSCLIP on RSICD and RSITMD datasets.

Conclusion: The RDB method effectively bridges cross-modal representation discrepancies, enhancing RSITR performance.

Abstract: Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in
geographic information interpretation, disaster monitoring, and urban planning
by establishing semantic associations between image and textual descriptions.
Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language
Pre-training (VLP) models typically adopt symmetric adapter structures for
exploring cross-modal correlations. However, the strong discriminative nature
of text modality may dominate the optimization process and inhibits image
representation learning. The nonnegligible imbalanced cross-modal optimization
remains a bottleneck to enhancing the model performance. To address this issue,
this study proposes a Representation Discrepancy Bridging (RDB) method for the
RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is
designed to enable modality-specific optimization and improve feature
alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text
Semantic Adapter (TSA). VEA mines fine-grained image features by Differential
Attention (DA) mechanism, while TSA identifies key textual semantics through
Hierarchical Attention (HA) mechanism. On the other hand, this study extends
the traditional single-task retrieval framework to a dual-task optimization
framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves
cross-modal alignment robustness through an adaptive weighted combination of
cross-modal, classification, and exponential moving average consistency
constraints. Experiments on RSICD and RSITMD datasets show that the proposed
RDB method achieves a 6%-11% improvement in mR metrics compared to
state-of-the-art PEFT methods and a 1.15%-2% improvement over the full
fine-tuned GeoRSCLIP model.

</details>


### [233] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/pdf/2505.15879)
*Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, Xin Eric Wang*

Main category: cs.CV

TL;DR: GRIT introduces a method for training multimodal language models (MLLMs) to generate visually grounded reasoning chains using bounding box coordinates and natural language, enhanced by a reinforcement learning approach (GRPO-GR).


<details>
  <summary>Details</summary>
Motivation: Existing visual reasoning models lack explicit integration of visual information, limiting their ability to produce visually grounded reasoning chains.

Method: GRIT combines natural language reasoning with bounding box coordinates for visual grounding and uses GRPO-GR, a reinforcement learning approach, to train models without needing annotated reasoning chains or bounding box labels.

Result: GRIT achieves high data efficiency, requiring only 20 image-question-answer triplets, and produces coherent, visually grounded reasoning chains.

Conclusion: GRIT successfully unifies reasoning and grounding abilities in MLLMs, demonstrating effective visually grounded reasoning.

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [234] [Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/pdf/2505.16809)
*Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong*

Main category: cs.CV

TL;DR: ReHyDIL improves brain tumor segmentation with missing MRI modalities using domain incremental learning and hypergraph networks, outperforming existing methods by 2% in Dice score.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in MRI segmentation due to missing modalities and inefficiency in retraining models for new modalities.

Method: Proposes ReHyDIL with Domain Incremental Learning (DIL), CHSNet for hypergraph-based associations, and TAC loss for information imbalance.

Result: Achieves over 2% improvement in Dice Similarity Coefficient on BraTS2019 dataset.

Conclusion: ReHyDIL effectively handles missing modalities and enhances segmentation performance without forgetting prior knowledge.

Abstract: Existing methods for multimodal MRI segmentation with missing modalities
typically assume that all MRI modalities are available during training.
However, in clinical practice, some modalities may be missing due to the
sequential nature of MRI acquisition, leading to performance degradation.
Furthermore, retraining models to accommodate newly available modalities can be
inefficient and may cause overfitting, potentially compromising previously
learned knowledge. To address these challenges, we propose Replay-based
Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation
with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to
enable the segmentation model to learn from newly acquired MRI modalities
without forgetting previously learned information. To enhance segmentation
performance across diverse patient scenarios, we introduce the Cross-Patient
Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture
high-order associations between patients. Additionally, we incorporate
Tversky-Aware Contrastive (TAC) loss to effectively mitigate information
imbalance both across and within different modalities. Extensive experiments on
the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art
methods, achieving an improvement of over 2\% in the Dice Similarity
Coefficient across various tumor regions. Our code is available at ReHyDIL.

</details>


### [235] [Challenger: Affordable Adversarial Driving Video Generation](https://arxiv.org/pdf/2505.15880)
*Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, Hao Zhao*

Main category: cs.CV

TL;DR: Challenger is a framework for generating photorealistic adversarial driving videos to test autonomous driving systems, combining physics-aware trajectory refinement and realistic behavior scoring.


<details>
  <summary>Details</summary>
Motivation: Current methods for adversarial driving scenarios lack realism and fail to stress-test autonomous driving systems effectively.

Method: Uses physics-aware multi-round trajectory refinement and a tailored scoring function to generate realistic adversarial maneuvers, rendered into multiview videos.

Result: Generates diverse aggressive scenarios (e.g., cut-ins, sudden lane changes) that increase collision rates in state-of-the-art AD models.

Conclusion: Challenger effectively creates realistic adversarial scenarios, revealing vulnerabilities in AD models and demonstrating transferability of adversarial behaviors.

Abstract: Generating photorealistic driving videos has seen significant progress
recently, but current methods largely focus on ordinary, non-adversarial
scenarios. Meanwhile, efforts to generate adversarial driving scenarios often
operate on abstract trajectory or BEV representations, falling short of
delivering realistic sensor data that can truly stress-test autonomous driving
(AD) systems. In this work, we introduce Challenger, a framework that produces
physically plausible yet photorealistic adversarial driving videos. Generating
such videos poses a fundamental challenge: it requires jointly optimizing over
the space of traffic interactions and high-fidelity sensor observations.
Challenger makes this affordable through two techniques: (1) a physics-aware
multi-round trajectory refinement process that narrows down candidate
adversarial maneuvers, and (2) a tailored trajectory scoring function that
encourages realistic yet adversarial behavior while maintaining compatibility
with downstream video synthesis. As tested on the nuScenes dataset, Challenger
generates a diverse range of aggressive driving scenarios-including cut-ins,
sudden lane changes, tailgating, and blind spot intrusions-and renders them
into multiview photorealistic videos. Extensive evaluations show that these
scenarios significantly increase the collision rate of state-of-the-art
end-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and
importantly, adversarial behaviors discovered for one model often transfer to
others.

</details>


### [236] [Creatively Upscaling Images with Global-Regional Priors](https://arxiv.org/pdf/2505.16976)
*Yurui Qian, Qi Cai, Yingwei Pan, Ting Yao, Tao Mei*

Main category: cs.CV

TL;DR: C-Upscale is a tuning-free method for high-resolution image generation using global-regional priors from prompts, improving fidelity and detail.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current diffusion models for high-resolution images, balancing global structure and regional creativity.

Method: Uses global-regional priors from prompts, regional attention control, and Multimodal LLM for detail-rich upscaling.

Result: Generates ultra-high-resolution images (e.g., 4,096 X 4,096) with better fidelity and creative details.

Conclusion: C-Upscale effectively enhances high-resolution image generation without tuning, outperforming existing methods.

Abstract: Contemporary diffusion models show remarkable capability in text-to-image
generation, while still being limited to restricted resolutions (e.g., 1,024 X
1,024). Recent advances enable tuning-free higher-resolution image generation
by recycling pre-trained diffusion models and extending them via regional
denoising or dilated sampling/convolutions. However, these models struggle to
simultaneously preserve global semantic structure and produce creative regional
details in higher-resolution images. To address this, we present C-Upscale, a
new recipe of tuning-free image upscaling that pivots on global-regional priors
derived from given global prompt and estimated regional prompts via Multimodal
LLM. Technically, the low-frequency component of low-resolution image is
recognized as global structure prior to encourage global semantic consistency
in high-resolution generation. Next, we perform regional attention control to
screen cross-attention between global prompt and each region during regional
denoising, leading to regional attention prior that alleviates object
repetition issue. The estimated regional prompts containing rich descriptive
details further act as regional semantic prior to fuel the creativity of
regional detail generation. Both quantitative and qualitative evaluations
demonstrate that our C-Upscale manages to generate ultra-high-resolution images
(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more
creative regional details.

</details>


### [237] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/pdf/2505.15928)
*Tony Montes, Fernando Lozano*

Main category: cs.CV

TL;DR: A new LLM-brained agent for zero-shot VideoQA combines Chain-of-Thought reasoning with YOLO-World for better object tracking, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Improve object tracking and reasoning alignment in VideoQA to enhance accuracy and reliability.

Method: Uses a Chain-of-Thought framework with grounding reasoning and YOLO-World for object tracking.

Result: Achieves state-of-the-art performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks.

Conclusion: The framework improves grounding accuracy and output reliability, with potential for cross-domain applications.

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [238] [Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On](https://arxiv.org/pdf/2505.16977)
*Siqi Wan, Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei*

Main category: cs.CV

TL;DR: The paper proposes a novel method using visual correspondence and 3D-aware cues to improve garment detail preservation in virtual try-on tasks with diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing dual-branch UNet architectures in diffusion models struggle to preserve garment details due to stochasticity.

Method: Uses semantic point matching and local flow warping to align garment details, augmented with 3D-aware cues (depth/normal maps) for supervision.

Result: Achieves state-of-the-art performance on VITON-HD and DressCode datasets, with strong detail preservation.

Conclusion: The proposed approach effectively addresses garment detail loss in diffusion-based virtual try-on tasks.

Abstract: Diffusion models have shown preliminary success in virtual try-on (VTON)
task. The typical dual-branch architecture comprises two UNets for implicit
garment deformation and synthesized image generation respectively, and has
emerged as the recipe for VTON task. Nevertheless, the problem remains
challenging to preserve the shape and every detail of the given garment due to
the intrinsic stochasticity of diffusion model. To alleviate this issue, we
novelly propose to explicitly capitalize on visual correspondence as the prior
to tame diffusion process instead of simply feeding the whole garment into UNet
as the appearance reference. Specifically, we interpret the fine-grained
appearance and texture details as a set of structured semantic points, and
match the semantic points rooted in garment to the ones over target person
through local flow warping. Such 2D points are then augmented into 3D-aware
cues with depth/normal map of target person. The correspondence mimics the way
of putting clothing on human body and the 3D-aware cues act as semantic point
matching to supervise diffusion model training. A point-focused diffusion loss
is further devised to fully take the advantage of semantic point matching.
Extensive experiments demonstrate strong garment detail preservation of our
approach, evidenced by state-of-the-art VTON performances on both VITON-HD and
DressCode datasets. Code is publicly available at:
https://github.com/HiDream-ai/SPM-Diff.

</details>


### [239] [VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance](https://arxiv.org/pdf/2505.15952)
*Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer*

Main category: cs.CV

TL;DR: The paper introduces VideoGameQA-Bench, a benchmark for evaluating Vision-Language Models (VLMs) in video game QA tasks, addressing the lack of standardized benchmarks in the domain.


<details>
  <summary>Details</summary>
Motivation: The growing revenue of video games necessitates optimized workflows, especially in QA, which is labor-intensive. VLMs offer automation potential, but current benchmarks are inadequate for game-specific needs.

Method: The authors propose VideoGameQA-Bench, a comprehensive benchmark covering various QA tasks like visual testing, glitch detection, and bug report generation for game images and videos.

Result: The benchmark provides a standardized evaluation tool for VLMs in game QA, filling a gap in existing resources.

Conclusion: VideoGameQA-Bench enables accurate assessment of VLMs in real-world game QA scenarios, supporting automation and efficiency in game development.

Abstract: With video games now generating the highest revenues in the entertainment
industry, optimizing game development workflows has become essential for the
sector's sustained growth. Recent advancements in Vision-Language Models (VLMs)
offer considerable potential to automate and enhance various aspects of game
development, particularly Quality Assurance (QA), which remains one of the
industry's most labor-intensive processes with limited automation options. To
accurately evaluate the performance of VLMs in video game QA tasks and
determine their effectiveness in handling real-world scenarios, there is a
clear need for standardized benchmarks, as existing benchmarks are insufficient
to address the specific requirements of this domain. To bridge this gap, we
introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array
of game QA activities, including visual unit testing, visual regression
testing, needle-in-a-haystack tasks, glitch detection, and bug report
generation for both images and videos of various games. Code and data are
available at: https://asgaardlab.github.io/videogameqa-bench/

</details>


### [240] [Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction](https://arxiv.org/pdf/2505.16980)
*Dong Li, Wenqi Zhong, Wei Yu, Yingwei Pan, Dingwen Zhang, Ting Yao, Junwei Han, Tao Mei*

Main category: cs.CV

TL;DR: DPIDM is a new framework for video virtual try-on that uses diffusion models to address spatiotemporal pose interactions, achieving superior results over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video virtual try-on methods lack effective spatiotemporal pose interactions, leading to temporal inconsistencies. DPIDM aims to solve this by modeling dynamic pose interactions.

Method: DPIDM integrates a skeleton-based pose adapter and a hierarchical attention module to model intra-frame and long-term pose dynamics, using diffusion models for denoising.

Result: DPIDM outperforms baselines, achieving a 60.5% improvement in VFID score over GPD-VVTO on the VVT dataset.

Conclusion: DPIDM effectively addresses spatiotemporal pose interactions in video virtual try-on, demonstrating significant performance gains.

Abstract: Video virtual try-on aims to seamlessly dress a subject in a video with a
specific garment. The primary challenge involves preserving the visual
authenticity of the garment while dynamically adapting to the pose and physique
of the subject. While existing methods have predominantly focused on
image-based virtual try-on, extending these techniques directly to videos often
results in temporal inconsistencies. Most current video virtual try-on
approaches alleviate this challenge by incorporating temporal modules, yet
still overlook the critical spatiotemporal pose interactions between human and
garment. Effective pose interactions in videos should not only consider spatial
alignment between human and garment poses in each frame but also account for
the temporal dynamics of human poses throughout the entire video. With such
motivation, we propose a new framework, namely Dynamic Pose Interaction
Diffusion Models (DPIDM), to leverage diffusion models to delve into dynamic
pose interactions for video virtual try-on. Technically, DPIDM introduces a
skeleton-based pose adapter to integrate synchronized human and garment poses
into the denoising network. A hierarchical attention module is then exquisitely
designed to model intra-frame human-garment pose interactions and long-term
human pose dynamics across frames through pose-aware spatial and temporal
attention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized
attention loss between consecutive frames to enhance temporal consistency.
Extensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate
the superiority of our DPIDM against the baseline methods. Notably, DPIDM
achieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over
the state-of-the-art GPD-VVTO approach.

</details>


### [241] [Super-Resolution with Structured Motion](https://arxiv.org/pdf/2505.15961)
*Gabby Litterio, Juan-David Lizarazo-Ferro, Pedro Felzenszwalb, Rashid Zia*

Main category: cs.CV

TL;DR: The paper explores super-resolution limits using imaging constraints, leveraging motion blur and convex optimization to achieve large resolution increases.


<details>
  <summary>Details</summary>
Motivation: Overcome theoretical and practical limitations in super-resolution, particularly small resolution increases and motion-blur as a nuisance.

Method: Uses high-precision motion information, sparse image priors, and convex optimization, including deconvolution with a box.

Result: Achieves perfect reconstructions of sparse signals and demonstrates motion blur's utility for super-resolution.

Conclusion: Motion blur, combined with pseudo-random motion and convex optimization, enables high-resolution reconstruction from a single low-resolution image.

Abstract: We consider the limits of super-resolution using imaging constraints. Due to
various theoretical and practical limitations, reconstruction-based methods
have been largely restricted to small increases in resolution. In addition,
motion-blur is usually seen as a nuisance that impedes super-resolution. We
show that by using high-precision motion information, sparse image priors, and
convex optimization, it is possible to increase resolution by large factors. A
key operation in super-resolution is deconvolution with a box. In general,
convolution with a box is not invertible. However, we obtain perfect
reconstructions of sparse signals using convex optimization. We also show that
motion blur can be helpful for super-resolution. We demonstrate that using
pseudo-random motion it is possible to reconstruct a high-resolution target
using a single low-resolution image. We present numerical experiments with
simulated data and results with real data captured by a camera mounted on a
computer controlled stage.

</details>


### [242] [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/pdf/2505.17022)
*Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, Xihui Liu*

Main category: cs.CV

TL;DR: GoT-R1 is a reinforcement learning framework enhancing semantic-spatial reasoning in visual generation, improving performance on complex prompts with precise spatial relationships and attributes.


<details>
  <summary>Details</summary>
Motivation: Existing visual generation models struggle with complex prompts requiring precise spatial and semantic reasoning.

Method: GoT-R1 uses reinforcement learning with a dual-stage multi-dimensional reward framework, leveraging MLLMs for evaluation.

Result: Significant improvements on T2I-CompBench, especially in compositional tasks with precise spatial relationships.

Conclusion: GoT-R1 advances visual generation by integrating sophisticated reasoning, with code and models made publicly available.

Abstract: Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.

</details>


### [243] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/pdf/2505.15963)
*Shujun Liu, Siyuan Wang, Zejun Li, Jianxiang Wang, Cheng Zeng, Zhongyu Wei*

Main category: cs.CV

TL;DR: OViP dynamically constructs contrastive training data from model hallucinations to improve alignment in vision-language models, reducing errors while maintaining expressiveness.


<details>
  <summary>Details</summary>
Motivation: Current methods for mitigating hallucination in LVLMs rely on predefined or random negative samples, which don't reflect actual model errors, limiting training effectiveness.

Method: Proposes OViP, a framework that dynamically generates contrastive training data by identifying semantic differences in model outputs and synthesizing negative images with a diffusion model.

Result: OViP effectively reduces hallucinations while preserving multi-modal capabilities, as shown in experiments on hallucination and general benchmarks.

Conclusion: OViP offers a failure-driven training approach that adaptively aligns textual and visual preferences, improving hallucination mitigation in LVLMs.

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [244] [Copy-Move Forgery Detection and Question Answering for Remote Sensing Image](https://arxiv.org/pdf/2412.02575)
*Ze Zhang, Enyuan Zhao, Di Niu, Jie Nie, Xinyue Liang, Lei Huang*

Main category: cs.CV

TL;DR: The paper introduces the RSCMQA task for interpreting tampered remote sensing images, proposes five datasets, and presents a CMFPF framework for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Practical needs in land monitoring and defense security drive the development of RSCMQA to address complex tampering scenarios.

Method: A region-discrimination-guided multimodal framework (CMFPF) is introduced to analyze tampered images by leveraging domain differences.

Result: The method outperforms general VQA and RSVQA models, providing a robust benchmark for RSCMQA.

Conclusion: The datasets and framework fill a critical gap, offering comprehensive, balanced, and challenging resources for the field.

Abstract: Driven by practical demands in land resource monitoring and national defense
security, this paper introduces the Remote Sensing Copy-Move Question Answering
(RSCMQA) task. Unlike traditional Remote Sensing Visual Question Answering
(RSVQA), RSCMQA focuses on interpreting complex tampering scenarios and
inferring relationships between objects. We present a suite of global RSCMQA
datasets, comprising images from 29 different regions across 14 countries.
Specifically, we propose five distinct datasets, including the basic dataset
RS-CMQA, the category-balanced dataset RS-CMQA-B, the high-authenticity dataset
Real-RSCM, the extended dataset RS-TQA, and the extended category-balanced
dataset RS-TQA-B. These datasets fill a critical gap in the field while
ensuring comprehensiveness, balance, and challenge. Furthermore, we introduce a
region-discrimination-guided multimodal copy-move forgery perception framework
(CMFPF), which enhances the accuracy of answering questions about tampered
images by leveraging prompt about the differences and connections between the
source and tampered domains. Extensive experiments demonstrate that our method
provides a stronger benchmark for RSCMQA compared to general VQA and RSVQA
models. Our datasets and code are publicly available at
https://github.com/shenyedepisa/RSCMQA.

</details>


### [245] [Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers](https://arxiv.org/pdf/2505.15997)
*Mehran Zoravar, Shadi Alijani, Homayoun Najjaran*

Main category: cs.CV

TL;DR: The paper proposes CE-ViTs, a framework combining conformal prediction and ensemble learning with Vision Transformers to improve domain adaptation and robustness in medical image classification.


<details>
  <summary>Details</summary>
Motivation: Ensuring trustworthiness in deep learning models for critical domains like medical imaging, addressing challenges in domain-shifted scenarios.

Method: Uses an ensemble of Vision Transformers trained on diverse datasets (HAM10000, Dermofit, ISIC) and calibrates them via conformal learning for domain adaptation.

Result: Achieves 90.38% coverage rate (9.95% improvement) and increases prediction set size for misclassified samples from 1.86 to 3.075.

Conclusion: CE-ViTs enhances conformal prediction performance, demonstrating better reliability and adaptability in domain-shifted scenarios.

Abstract: Exploring the trustworthiness of deep learning models is crucial, especially
in critical domains such as medical imaging decision support systems. Conformal
prediction has emerged as a rigorous means of providing deep learning models
with reliable uncertainty estimates and safety guarantees. However, conformal
prediction results face challenges due to the backbone model's struggles in
domain-shifted scenarios, such as variations in different sources. To aim this
challenge, this paper proposes a novel framework termed Conformal Ensemble of
Vision Transformers (CE-ViTs) designed to enhance image classification
performance by prioritizing domain adaptation and model robustness, while
accounting for uncertainty. The proposed method leverages an ensemble of vision
transformer models in the backbone, trained on diverse datasets including
HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning
approach, calibrated through the combined mentioned datasets, aims to enhance
domain adaptation through conformal learning. Experimental results underscore
that the framework achieves a high coverage rate of 90.38\%, representing an
improvement of 9.95\% compared to the HAM10000 model. This indicates a strong
likelihood that the prediction set includes the true label compared to singular
models. Ensemble learning in CE-ViTs significantly improves conformal
prediction performance, increasing the average prediction set size for
challenging misclassified samples from 1.86 to 3.075.

</details>


### [246] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/pdf/2505.15966)
*Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, Wenhu Chen*

Main category: cs.CV

TL;DR: The paper introduces pixel-space reasoning for Vision-Language Models (VLMs) to enhance visual task performance, overcoming limitations of text-only reasoning.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in LLMs is limited to text, hindering effectiveness in visual tasks. Pixel-space reasoning aims to bridge this gap.

Method: A two-phase training approach: instruction tuning on synthesized reasoning traces, followed by reinforcement learning with curiosity-driven rewards.

Result: The 7B model achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, setting new benchmarks.

Conclusion: Pixel-space reasoning significantly improves VLM performance, demonstrating the framework's effectiveness for visual tasks.

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [247] [Paired and Unpaired Image to Image Translation using Generative Adversarial Networks](https://arxiv.org/pdf/2505.16310)
*Gaurav Kumar, Soham Satyadharma, Harpreet Singh*

Main category: cs.CV

TL;DR: The paper explores paired and unpaired image-to-image translation using GANs, evaluating performance with quantitative metrics and qualitative analysis.


<details>
  <summary>Details</summary>
Motivation: To advance image translation research by studying both paired and unpaired methods, leveraging GANs for domain transformation.

Method: Used conditional GAN for paired translation and cycle consistency loss for unpaired translation, experimenting with loss functions, Patch-GAN sizes, and architectures.

Result: Evaluated results using precision, recall, FID score, and qualitative analysis.

Conclusion: Demonstrates effectiveness of GAN-based approaches for image translation, with insights from quantitative and qualitative evaluations.

Abstract: Image to image translation is an active area of research in the field of
computer vision, enabling the generation of new images with different styles,
textures, or resolutions while preserving their characteristic properties.
Recent architectures leverage Generative Adversarial Networks (GANs) to
transform input images from one domain to another. In this work, we focus on
the study of both paired and unpaired image translation across multiple image
domains. For the paired task, we used a conditional GAN model, and for the
unpaired task, we trained it using cycle consistency loss. We experimented with
different types of loss functions, multiple Patch-GAN sizes, and model
architectures. New quantitative metrics - precision, recall, and FID score -
were used for analysis. In addition, a qualitative study of the results of
different experiments was conducted.

</details>


### [248] [Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders](https://arxiv.org/pdf/2505.15970)
*Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng*

Main category: cs.CV

TL;DR: SAEs reveal hierarchical relationships in vision model activations, aligning with ImageNet taxonomy, and show how DINOv2 layers internalize hierarchical info.


<details>
  <summary>Details</summary>
Motivation: To analyze how vision models encode the ImageNet hierarchy using SAEs, extending their use from LLMs to vision models.

Method: Leverage Sparse Autoencoders (SAEs) to probe internal representations of vision models, focusing on DINOv2.

Result: SAEs uncover implicit hierarchical relationships in model activations, showing alignment with ImageNet taxonomy.

Conclusion: SAEs are effective for probing hierarchical semantic structure in vision models, offering a systematic analysis framework.

Abstract: The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.

</details>


### [249] [SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models](https://arxiv.org/pdf/2505.16318)
*Hossein Khalili, Seongbin Park, Venkat Bollapragada, Nader Sehatbakhsh*

Main category: cs.CV

TL;DR: SuperPure is a new defense strategy for adversarial patch attacks, improving robustness and reducing latency compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in current defenses against distributed and localized adversarial patches, and reduce impractical resource consumption.

Method: Uses a pixel-wise masking scheme with GAN-based super-resolution to purify images from adversarial patches.

Result: Improves robustness by 20% against localized patches, achieves 58% robustness against distributed patches, and reduces latency by 98%.

Conclusion: SuperPure is effective, efficient, and robust against various attack types, making it suitable for latency-sensitive applications.

Abstract: As vision-based machine learning models are increasingly integrated into
autonomous and cyber-physical systems, concerns about (physical) adversarial
patch attacks are growing. While state-of-the-art defenses can achieve
certified robustness with minimal impact on utility against highly-concentrated
localized patch attacks, they fall short in two important areas: (i)
State-of-the-art methods are vulnerable to low-noise distributed patches where
perturbations are subtly dispersed to evade detection or masking, as shown
recently by the DorPatch attack; (ii) Achieving high robustness with
state-of-the-art methods is extremely time and resource-consuming, rendering
them impractical for latency-sensitive applications in many cyber-physical
systems.
  To address both robustness and latency issues, this paper proposes a new
defense strategy for adversarial patch attacks called SuperPure. The key
novelty is developing a pixel-wise masking scheme that is robust against both
distributed and localized patches. The masking involves leveraging a GAN-based
super-resolution scheme to gradually purify the image from adversarial patches.
Our extensive evaluations using ImageNet and two standard classifiers, ResNet
and EfficientNet, show that SuperPure advances the state-of-the-art in three
major directions: (i) it improves the robustness against conventional localized
patches by more than 20%, on average, while also improving top-1 clean accuracy
by almost 10%; (ii) It achieves 58% robustness against distributed patch
attacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It
decreases the defense end-to-end latency by over 98% compared to PatchCleanser.
Our further analysis shows that SuperPure is robust against white-box attacks
and different patch sizes. Our code is open-source.

</details>


### [250] [Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning](https://arxiv.org/pdf/2505.16001)
*Qiang Zhu, Kuan Lu, Menghao Huo, Yuxiao Li*

Main category: cs.CV

TL;DR: A diffusion-based framework using Diffusion Transformers (DiT) for image-to-image translation, leveraging CLIP embeddings for fine-grained control and achieving high-quality results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To explore an alternative to GAN-based models for image-to-image translation by combining diffusion models' denoising capabilities with transformers' global modeling power.

Method: Adapts DiT for translation, conditions on CLIP image embeddings, and uses CLIP similarity and LPIPS perceptual losses for training.

Result: Achieves high-quality, semantically faithful translations on face2comics and edges2shoes datasets.

Conclusion: DiT with CLIP-based conditioning offers a promising alternative to GANs for paired image-to-image translation.

Abstract: Image-to-image translation aims to learn a mapping between a source and a
target domain, enabling tasks such as style transfer, appearance
transformation, and domain adaptation. In this work, we explore a
diffusion-based framework for image-to-image translation by adapting Diffusion
Transformers (DiT), which combine the denoising capabilities of diffusion
models with the global modeling power of transformers. To guide the translation
process, we condition the model on image embeddings extracted from a
pre-trained CLIP encoder, allowing for fine-grained and structurally consistent
translations without relying on text or class labels. We incorporate both a
CLIP similarity loss to enforce semantic consistency and an LPIPS perceptual
loss to enhance visual fidelity during training. We validate our approach on
two benchmark datasets: face2comics, which translates real human faces to
comic-style illustrations, and edges2shoes, which translates edge maps to
realistic shoe images. Experimental results demonstrate that DiT, combined with
CLIP-based conditioning and perceptual similarity objectives, achieves
high-quality, semantically faithful translations, offering a promising
alternative to GAN-based models for paired image-to-image translation tasks.

</details>


### [251] [Pose-invariant face recognition via feature-space pose frontalization](https://arxiv.org/pdf/2505.16412)
*Nikolay Stanishev, Yuhang Lu, Touradj Ebrahimi*

Main category: cs.CV

TL;DR: A new method for pose-invariant face recognition using feature space pose frontalization (FSPFM) and a novel training paradigm outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Pose-invariant face recognition is challenging for AI systems, especially matching profile faces with frontal ones in databases. Existing methods rely on generative models or pose-robust features, which may have limitations.

Method: Proposes a feature space pose frontalization module (FSPFM) to transform profile images into frontal ones. Introduces a two-stage training paradigm (pre-training and attention-guided fine-tuning) to enhance FSPFM's performance.

Result: Extensive experiments on five benchmarks show the method outperforms state-of-the-art in pose-invariant recognition and excels in standard scenarios.

Conclusion: The proposed FSPFM and training paradigm effectively address pose-invariant face recognition, achieving superior performance across diverse scenarios.

Abstract: Pose-invariant face recognition has become a challenging problem for modern
AI-based face recognition systems. It aims at matching a profile face captured
in the wild with a frontal face registered in a database. Existing methods
perform face frontalization via either generative models or learning a pose
robust feature representation. In this paper, a new method is presented to
perform face frontalization and recognition within the feature space. First, a
novel feature space pose frontalization module (FSPFM) is proposed to transform
profile images with arbitrary angles into frontal counterparts. Second, a new
training paradigm is proposed to maximize the potential of FSPFM and boost its
performance. The latter consists of a pre-training and an attention-guided
fine-tuning stage. Moreover, extensive experiments have been conducted on five
popular face recognition benchmarks. Results show that not only our method
outperforms the state-of-the-art in the pose-invariant face recognition task
but also maintains superior performance in other standard scenarios.

</details>


### [252] [Position: Agentic Systems Constitute a Key Component of Next-Generation Intelligent Image Processing](https://arxiv.org/pdf/2505.16007)
*Jinjin Gu*

Main category: cs.CV

TL;DR: The paper advocates for shifting focus from model-centric to agentic system design in image processing to improve generalization and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches in image processing lack generalization, adaptability, and real-world flexibility, necessitating a new paradigm.

Method: Proposes developing intelligent agentic systems that dynamically select and optimize image processing tools, mimicking human expert strategies.

Result: Identifies limitations of model-centric paradigms and outlines design principles and capability levels for agentic systems.

Conclusion: Agentic systems represent the next evolutionary step for image processing, addressing brittleness in monolithic models.

Abstract: This position paper argues that the image processing community should broaden
its focus from purely model-centric development to include agentic system
design as an essential complementary paradigm. While deep learning has
significantly advanced capabilities for specific image processing tasks,
current approaches face critical limitations in generalization, adaptability,
and real-world problem-solving flexibility. We propose that developing
intelligent agentic systems, capable of dynamically selecting, combining, and
optimizing existing image processing tools, represents the next evolutionary
step for the field. Such systems would emulate human experts' ability to
strategically orchestrate different tools to solve complex problems, overcoming
the brittleness of monolithic models. The paper analyzes key limitations of
model-centric paradigms, establishes design principles for agentic image
processing systems, and outlines different capability levels for such agents.

</details>


### [253] [Unsupervised Network Anomaly Detection with Autoencoders and Traffic Images](https://arxiv.org/pdf/2505.16650)
*Michael Neri, Sara Baldoni*

Main category: cs.CV

TL;DR: The paper proposes an image-based representation of network traffic for anomaly detection, using 1-second time windows and an unsupervised learning approach.


<details>
  <summary>Details</summary>
Motivation: The rise in connected devices and heterogeneous computational capacities necessitates efficient anomaly detection and data processing.

Method: An image-based representation of network traffic is introduced to summarize conditions in 1-second windows, reducing complex processing needs. An unsupervised learning approach detects anomalies.

Result: The method effectively highlights anomalies and simplifies processing.

Conclusion: The proposed approach offers a compact and efficient solution for detecting network anomalies, with code and dataset publicly available.

Abstract: Due to the recent increase in the number of connected devices, the need to
promptly detect security issues is emerging. Moreover, the high number of
communication flows creates the necessity of processing huge amounts of data.
Furthermore, the connected devices are heterogeneous in nature, having
different computational capacities. For this reason, in this work we propose an
image-based representation of network traffic which allows to realize a compact
summary of the current network conditions with 1-second time windows. The
proposed representation highlights the presence of anomalies thus reducing the
need for complex processing architectures. Finally, we present an unsupervised
learning approach which effectively detects the presence of anomalies. The code
and the dataset are available at
https://github.com/michaelneri/image-based-network-traffic-anomaly-detection.

</details>


### [254] [Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection](https://arxiv.org/pdf/2505.16029)
*Shichao Li, Peiliang Li, Qing Lian, Peng Yun, Xiaozhi Chen*

Main category: cs.CV

TL;DR: A new benchmark and auto-labeling system for 3D pedestrian tracking in crowded scenes improves efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of sparse point clouds and lack of benchmarks for crowded pedestrian scenes in autonomous perception.

Method: Collecting a multi-view LiDAR-camera benchmark and building an offboard auto-labeling system with density-aware and relationship-aware high-resolution representations.

Result: Significant improvement in 3D pedestrian tracking performance and auto-labeling efficiency.

Conclusion: The proposed approach effectively tackles the challenges of crowded scenes, with publicly available code for further use.

Abstract: Perceiving pedestrians in highly crowded urban environments is a difficult
long-tail problem for learning-based autonomous perception. Speeding up 3D
ground truth generation for such challenging scenes is performance-critical yet
very challenging. The difficulties include the sparsity of the captured
pedestrian point cloud and a lack of suitable benchmarks for a specific system
design study. To tackle the challenges, we first collect a new multi-view
LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded
pedestrians for in-depth analysis. We then build an offboard auto-labeling
system that reconstructs pedestrian trajectories from LiDAR point cloud and
multi-view images. To improve the generalization power for crowded scenes and
the performance for small objects, we propose to learn high-resolution
representations that are density-aware and relationship-aware. Extensive
experiments validate that our approach significantly improves the 3D pedestrian
tracking performance towards higher auto-labeling efficiency. The code will be
publicly available at this HTTP URL.

</details>


### [255] [Zero-Shot Hyperspectral Pansharpening Using Hysteresis-Based Tuning for Spectral Quality Control](https://arxiv.org/pdf/2505.16658)
*Giuseppe Guarino, Matteo Ciotola, Gemine Vivone, Giovanni Poggi, Giuseppe Scarpa*

Main category: cs.CV

TL;DR: A new hyperspectral pansharpening method using a lightweight neural network with adaptive weights ensures uniform spectral quality across all bands, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods borrowed from multispectral pansharpening fail to address unique hyperspectral challenges like noise, spectral mismatch, and inconsistent quality across bands.

Method: A single lightweight neural network adapts weights per band, uses a hysteresis-like dynamic for spectral loss convergence, and redefines spatial loss for nonlinear dependencies.

Result: The method achieves excellent sharpening quality consistently across all bands, validated by a benchmarking toolbox.

Conclusion: The proposed unsupervised, flexible, and low-complexity method outperforms state-of-the-art approaches, with code and results shared online.

Abstract: Hyperspectral pansharpening has received much attention in recent years due
to technological and methodological advances that open the door to new
application scenarios. However, research on this topic is only now gaining
momentum. The most popular methods are still borrowed from the more mature
field of multispectral pansharpening and often overlook the unique challenges
posed by hyperspectral data fusion, such as i) the very large number of bands,
ii) the overwhelming noise in selected spectral ranges, iii) the significant
spectral mismatch between panchromatic and hyperspectral components, iv) a
typically high resolution ratio. Imprecise data modeling especially affects
spectral fidelity. Even state-of-the-art methods perform well in certain
spectral ranges and much worse in others, failing to ensure consistent quality
across all bands, with the risk of generating unreliable results. Here, we
propose a hyperspectral pansharpening method that explicitly addresses this
problem and ensures uniform spectral quality. To this end, a single lightweight
neural network is used, with weights that adapt on the fly to each band. During
fine-tuning, the spatial loss is turned on and off to ensure a fast convergence
of the spectral loss to the desired level, according to a hysteresis-like
dynamic. Furthermore, the spatial loss itself is appropriately redefined to
account for nonlinear dependencies between panchromatic and spectral bands.
Overall, the proposed method is fully unsupervised, with no prior training on
external data, flexible, and low-complexity. Experiments on a recently
published benchmarking toolbox show that it ensures excellent sharpening
quality, competitive with the state-of-the-art, consistently across all bands.
The software code and the full set of results are shared online on
https://github.com/giu-guarino/rho-PNN.

</details>


### [256] [An Approach Towards Identifying Bangladeshi Leaf Diseases through Transfer Learning and XAI](https://arxiv.org/pdf/2505.16033)
*Faika Fairuj Preotee, Shuvashis Sarker, Shamim Rahim Refat, Tashreef Muhammad, Shifat Islam*

Main category: cs.CV

TL;DR: The paper proposes a deep learning-based system for classifying 21 leaf diseases across six plants in Bangladesh, achieving high accuracy with models like VGG19 and Xception, and uses Explainable AI for transparency.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient and accessible solution for identifying plant leaf diseases in Bangladesh, where agriculture is vital for food security, reducing reliance on expert knowledge.

Method: Deep Learning techniques (CNN, Transfer Learning models like VGG16, VGG19, MobileNetV2, InceptionV3, ResNet50V2, Xception) and Explainable AI (GradCAM, GradCAM++, LayerCAM, ScoreCAM, FasterScoreCAM) for disease classification and transparency.

Result: VGG19 and Xception achieved the highest accuracies of 98.90% and 98.66%, respectively. Explainable AI techniques highlighted model focus areas for better understanding.

Conclusion: The approach improves disease management, supports informed farmer decisions, and enhances agricultural productivity through accurate and transparent disease detection.

Abstract: Leaf diseases are harmful conditions that affect the health, appearance and
productivity of plants, leading to significant plant loss and negatively
impacting farmers' livelihoods. These diseases cause visible symptoms such as
lesions, color changes, and texture variations, making it difficult for farmers
to manage plant health, especially in large or remote farms where expert
knowledge is limited. The main motivation of this study is to provide an
efficient and accessible solution for identifying plant leaf diseases in
Bangladesh, where agriculture plays a critical role in food security. The
objective of our research is to classify 21 distinct leaf diseases across six
plants using deep learning models, improving disease detection accuracy while
reducing the need for expert involvement. Deep Learning (DL) techniques,
including CNN and Transfer Learning (TL) models like VGG16, VGG19, MobileNetV2,
InceptionV3, ResNet50V2 and Xception are used. VGG19 and Xception achieve the
highest accuracies, with 98.90% and 98.66% respectively. Additionally,
Explainable AI (XAI) techniques such as GradCAM, GradCAM++, LayerCAM, ScoreCAM
and FasterScoreCAM are used to enhance transparency by highlighting the regions
of the models focused on during disease classification. This transparency
ensures that farmers can understand the model's predictions and take necessary
action. This approach not only improves disease management but also supports
farmers in making informed decisions, leading to better plant protection and
increased agricultural productivity.

</details>


### [257] [One-Step Diffusion-Based Image Compression with Semantic Distillation](https://arxiv.org/pdf/2505.16687)
*Naifu Xue, Zhaoyang Jia, Jiahao Li, Bin Li, Yuan Zhang, Yan Lu*

Main category: cs.CV

TL;DR: OneDC is a one-step diffusion-based image codec that reduces latency by eliminating multi-step sampling, achieving SOTA perceptual quality with faster decoding and lower bitrates.


<details>
  <summary>Details</summary>
Motivation: Address the latency issue in diffusion-based generative image codecs by proving multi-step sampling is unnecessary for generative compression.

Method: Integrates a latent compression module with a one-step diffusion generator, uses hyperprior as semantic guidance, introduces semantic distillation, and employs hybrid pixel- and latent-domain optimization.

Result: Achieves 40% bitrate reduction and 20x faster decoding compared to multi-step diffusion-based codecs, with SOTA perceptual quality.

Conclusion: OneDC demonstrates that one-step diffusion is viable for generative compression, offering significant efficiency and performance improvements.

Abstract: While recent diffusion-based generative image codecs have shown impressive
performance, their iterative sampling process introduces unpleasing latency. In
this work, we revisit the design of a diffusion-based codec and argue that
multi-step sampling is not necessary for generative compression. Based on this
insight, we propose OneDC, a One-step Diffusion-based generative image Codec --
that integrates a latent compression module with a one-step diffusion
generator. Recognizing the critical role of semantic guidance in one-step
diffusion, we propose using the hyperprior as a semantic signal, overcoming the
limitations of text prompts in representing complex visual content. To further
enhance the semantic capability of the hyperprior, we introduce a semantic
distillation mechanism that transfers knowledge from a pretrained generative
tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and
latent-domain optimization to jointly enhance both reconstruction fidelity and
perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA
perceptual quality even with one-step generation, offering over 40% bitrate
reduction and 20x faster decoding compared to prior multi-step diffusion-based
codecs. Code will be released later.

</details>


### [258] [An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection](https://arxiv.org/pdf/2505.16039)
*Shuvashis Sarker, Shamim Rahim Refat, Faika Fairuj Preotee, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque*

Main category: cs.CV

TL;DR: The study compares Vision Transformer (ViT) and Transfer Learning models for brain disease classification using MRI data, finding ViT superior with 94.39% accuracy. Explainable AI methods enhance interpretability.


<details>
  <summary>Details</summary>
Motivation: Brain conditions are hard to diagnose; MRI interpretation is complex. The study aims to improve classification and transparency in diagnosis.

Method: Comparative analysis of ViT and TL models (VGG16, VGG19, Resnet50V2, MobilenetV2) on Bangladesh MRI data, using XAI methods for interpretation.

Result: ViT outperforms TL models with 94.39% accuracy. XAI methods provide interpretable insights for medical professionals.

Conclusion: ViT is effective for brain disease classification, and XAI enhances diagnostic precision, aiding medical decision-making.

Abstract: The brain is a highly complex organ that manages many important tasks,
including movement, memory and thinking. Brain-related conditions, like tumors
and degenerative disorders, can be hard to diagnose and treat. Magnetic
Resonance Imaging (MRI) serves as a key tool for identifying these conditions,
offering high-resolution images of brain structures. Despite this, interpreting
MRI scans can be complicated. This study tackles this challenge by conducting a
comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL)
models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain
diseases using MRI data from Bangladesh based dataset. ViT, known for their
ability to capture global relationships in images, are particularly effective
for medical imaging tasks. Transfer learning helps to mitigate data constraints
by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods
such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are
employed to interpret model predictions. The results demonstrate that ViT
surpasses transfer learning models, achieving a classification accuracy of
94.39%. The integration of XAI methods enhances model transparency, offering
crucial insights to aid medical professionals in diagnosing brain diseases with
greater precision.

</details>


### [259] [SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned Point Cloud Compression](https://arxiv.org/pdf/2505.16709)
*Kai Hsiang Hsieh, Monyneath Yim, Jui Chiu Chiang*

Main category: cs.CV

TL;DR: SEDD-PCC is an end-to-end learning-based framework for joint compression of point cloud geometry and attributes, using a shared encoder and dual decoders, enhanced by knowledge distillation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat geometry and attribute coding separately, increasing complexity and missing shared feature exploitation.

Method: SEDD-PCC uses a single encoder for shared features, dual decoders for reconstruction, and knowledge distillation for improved learning.

Result: Outperforms rule-based and learning-based methods, demonstrating competitive performance.

Conclusion: SEDD-PCC is an efficient, practical AI-driven solution for point cloud compression.

Abstract: To encode point clouds containing both geometry and attributes, most
learning-based compression schemes treat geometry and attribute coding
separately, employing distinct encoders and decoders. This not only increases
computational complexity but also fails to fully exploit shared features
between geometry and attributes. To address this limitation, we propose
SEDD-PCC, an end-to-end learning-based framework for lossy point cloud
compression that jointly compresses geometry and attributes. SEDD-PCC employs a
single encoder to extract shared geometric and attribute features into a
unified latent space, followed by dual specialized decoders that sequentially
reconstruct geometry and attributes. Additionally, we incorporate knowledge
distillation to enhance feature representation learning from a teacher model,
further improving coding efficiency. With its simple yet effective design,
SEDD-PCC provides an efficient and practical solution for point cloud
compression. Comparative evaluations against both rule-based and learning-based
methods demonstrate its competitive performance, highlighting SEDD-PCC as a
promising AI-driven compression approach.

</details>


### [260] [GMatch: Geometry-Constrained Feature Matching for RGB-D Object Pose Estimation](https://arxiv.org/pdf/2505.16144)
*Ming Yang, Haoran Li*

Main category: cs.CV

TL;DR: GMatch is a learning-free feature matcher for robust 6DoF object pose estimation, using SE(3)-invariant geometric consistency to outperform traditional and learning-based methods.


<details>
  <summary>Details</summary>
Motivation: Address local ambiguities in sparse feature matching without relying on descriptor similarity or training.

Method: Guided, incremental search with SE(3)-invariant geometric consistency, leveraging geometric features for globally consistent correspondences.

Result: Outperforms traditional and learning-based matchers on HOPE and YCB-Video datasets, achieving high accuracy and low variance.

Conclusion: GMatch-SIFT is effective for object pose estimation and general-purpose feature matching, with strong interpretability and generalization.

Abstract: We present GMatch, a learning-free feature matcher designed for robust 6DoF
object pose estimation, addressing common local ambiguities in sparse feature
matching. Unlike traditional methods that rely solely on descriptor similarity,
GMatch performs a guided, incremental search, enforcing SE(3)-invariant
geometric consistency throughout the matching process. It leverages a provably
complete set of geometric features that uniquely determine 3D keypoint
configurations, ensuring globally consistent correspondences without the need
for training or GPU support. When combined with classical descriptors such as
SIFT, GMatch-SIFT forms a general-purpose pose estimation pipeline that offers
strong interpretability and generalization across diverse objects and scenes.
Experiments on the HOPE dataset show that GMatch outperforms both traditional
and learning-based matchers, with GMatch-SIFT achieving or surpassing the
performance of instance-level pose networks. On the YCB-Video dataset,
GMatch-SIFT demonstrates high accuracy and low variance on texture-rich
objects. These results not only validate the effectiveness of GMatch-SIFT for
object pose estimation but also highlight the broader applicability of GMatch
as a general-purpose feature matcher. Code will be released upon acceptance.

</details>


### [261] [Deep mineralogical segmentation of thin section images based on QEMSCAN maps](https://arxiv.org/pdf/2505.17008)
*Jean Pablo Vieira de Mello, Matheus Augusto Alves Cuglieri, Leandro P. de Figueiredo, Fernando Bordignon, Marcelo Ramalho Albuquerque, Rodrigo Surmas, Bruno Cavalcanti de Paula*

Main category: cs.CV

TL;DR: A CNN model using U-Net architecture is proposed for automated mineralogical segmentation of carbonate rock thin sections, mimicking QEMSCAN at lower cost and with good generalization.


<details>
  <summary>Details</summary>
Motivation: Human analysis of rock thin sections is subjective and laborious, while existing automated methods like QEMSCAN are costly and time-consuming.

Method: The U-Net model is trained on plane and cross-polarized thin section images using QEMSCAN maps as targets, differentiating minerals like Calcite, Dolomite, and others. Image registration aligns varying resolutions.

Result: The model shows promising segmentation quality, with R^2 > 0.97 for seen facies and 0.88 for unseen, excelling in mineral boundary delineation and distribution estimation.

Conclusion: The approach is efficient and generalizable, though segmentation quality depends on resolution differences and texture variety.

Abstract: Interpreting the mineralogical aspects of rock thin sections is an important
task for oil and gas reservoirs evaluation. However, human analysis tend to be
subjective and laborious. Technologies like QEMSCAN(R) are designed to automate
the mineralogical mapping process, but also suffer from limitations like high
monetary costs and time-consuming analysis. This work proposes a Convolutional
Neural Network model for automatic mineralogical segmentation of thin section
images of carbonate rocks. The model is able to mimic the QEMSCAN mapping
itself in a low-cost, generalized and efficient manner. For this, the U-Net
semantic segmentation architecture is trained on plane and cross polarized thin
section images using the corresponding QEMSCAN maps as target, which is an
approach not widely explored. The model was instructed to differentiate
occurrences of Calcite, Dolomite, Mg-Clay Minerals, Quartz, Pores and the
remaining mineral phases as an unique class named "Others", while it was
validated on rock facies both seen and unseen during training, in order to
address its generalization capability. Since the images and maps are provided
in different resolutions, image registration was applied to align then
spatially. The study reveals that the quality of the segmentation is very much
dependent on these resolution differences and on the variety of learnable rock
textures. However, it shows promising results, especially with regard to the
proper delineation of minerals boundaries on solid textures and precise
estimation of the minerals distributions, describing a nearly linear
relationship between expected and predicted distributions, with coefficient of
determination (R^2) superior to 0.97 for seen facies and 0.88 for unseen.

</details>


### [262] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/pdf/2505.16146)
*Zhenglin Hua, Jinghan He, Zijun Yao, Tianxu Han, Haiyun Guo, Yuheng Jia, Junfeng Fang*

Main category: cs.CV

TL;DR: The paper proposes SSL, a training-free method using sparse autoencoders (SAEs) to mitigate hallucinations in large vision-language models (LVLMs) by steering latent directions.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from hallucinations (text inconsistent with visual input), and existing solutions are computationally expensive or ineffective.

Method: Leverages SAEs to identify hallucination-related semantic directions and intervenes along these directions to mitigate hallucinations.

Result: SSL outperforms existing methods in reducing hallucinations with negligible time overhead and maintains transferability.

Conclusion: SSL offers an efficient and effective solution for hallucination mitigation in LVLMs without additional training.

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [263] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/pdf/2505.16149)
*Zirui Pang, Haosheng Tan, Yuhan Pu, Zhijie Deng, Zhouan Shen, Keyu Hu, Jiaheng Wei*

Main category: cs.CV

TL;DR: REVEAL is a framework combining vision-language models and label curation methods to detect and correct noisy and missing labels in image classification datasets, improving benchmark quality.


<details>
  <summary>Details</summary>
Motivation: Existing datasets like CIFAR, MNIST, and ImageNet suffer from noisy and missing labels, leading to unfair model evaluations. Current methods overlook missing labels.

Method: REVEAL integrates pre-trained vision-language models (e.g., LLaVA, BLIP) with label curation tools (e.g., Cleanlab, MTurk) to detect and refine noisy/missing labels via consensus-based filtering and confidence-informed predictions.

Result: REVEAL improves label accuracy in 6 benchmark test sets, aligning closely with human judgments and providing soft-labeled results with likelihoods.

Conclusion: REVEAL effectively addresses noisy and missing labels, enhancing dataset quality for fairer and more accurate model evaluations in image classification.

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [264] [Training-Free Reasoning and Reflection in MLLMs](https://arxiv.org/pdf/2505.16151)
*Hongchen Wei, Zhenzhong Chen*

Main category: cs.CV

TL;DR: FRANK Model enhances off-the-shelf MLLMs with reasoning abilities without retraining, using hierarchical weight merging and outperforms baselines like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Extending reasoning capabilities to MLLMs is costly due to retraining expenses and lack of high-quality datasets.

Method: Decouples perception and reasoning via hierarchical weight merging, integrating reasoning into deep decoder layers while preserving visual grounding in shallow layers.

Result: Achieves 69.2 accuracy on MMMU benchmark, surpassing InternVL2.5-38B by +5.3 and GPT-4o.

Conclusion: FRANK Model effectively adds reasoning to MLLMs without training, demonstrating superior performance.

Abstract: Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have
showcased impressive reasoning capabilities via reinforcement learning.
However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by
the prohibitive costs of retraining and the scarcity of high-quality,
verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a
training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning
and reflection abilities, without any gradient updates or extra supervision.
Our key insight is to decouple perception and reasoning across MLLM decoder
layers. Specifically, we observe that compared to the deeper decoder layers,
the shallow decoder layers allocate more attention to visual tokens, while the
deeper decoder layers concentrate on textual semantics. This observation
motivates a hierarchical weight merging approach that combines a
visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we
propose a layer-wise, Taylor-derived closed-form fusion mechanism that
integrates reasoning capacity into deep decoder layers while preserving visual
grounding in shallow decoder layers. Extensive experiments on challenging
multimodal reasoning benchmarks demonstrate the effectiveness of our approach.
On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,
outperforming the strongest baseline InternVL2.5-38B by +5.3, and even
surpasses the proprietary GPT-4o model. Our project homepage is at:
http://iip.whu.edu.cn/frank/index.html

</details>


### [265] [A Deep Unrolling Model with Hybrid Optimization Structure for Hyperspectral Image Deconvolution](https://arxiv.org/pdf/2306.06378)
*Alexandros Gkillas, Dimitris Ampeliotis, Kostas Berberidis*

Main category: cs.CV

TL;DR: The paper justifies combining handcrafted and learnable regularizers in inverse imaging problems, introducing a novel framework (DeepMix) that enhances performance and reduces complexity.


<details>
  <summary>Details</summary>
Motivation: To explore the synergy between handcrafted and learnable regularizers, demonstrating their combined benefits in reducing complexity and improving performance.

Method: Proposes DeepMix, an interpretable model using deep unrolling with three modules: data consistency, handcrafted regularizer enforcement, and a context-aware denoising module with skip connections.

Result: DeepMix outperforms existing methods, improving image quality and computational efficiency in experiments on simulated and real-world datasets.

Conclusion: The combination of handcrafted and learnable regularizers, facilitated by DeepMix, is effective and enhances performance in inverse imaging problems.

Abstract: In recent literature there are plenty of works that combine handcrafted and
learnable regularizers to solve inverse imaging problems. While this hybrid
approach has demonstrated promising results, the motivation for combining
handcrafted and learnable regularizers remains largely underexplored. This work
aims to justify this combination, by demonstrating that the incorporation of
proper handcrafted regularizers alongside learnable regularizers not only
reduces the complexity of the learnable prior, but also the performance is
notably enhanced. To analyze the impact of this synergy, we introduce the
notion of residual structure, to refer to the structure of the solution that
cannot be modeled by the handcrafted regularizers per se. Motivated by these,
we propose a novel optimization framework for the hyperspectral deconvolution
problem, called DeepMix. Based on the proposed optimization framework, an
interpretable model is developed using the deep unrolling strategy, which
consists of three distinct modules, namely, a data consistency module, a module
that enforces the effect of the handcrafted regularizers, and a denoising
module. Recognizing the collaborative nature of these modules, this work
proposes a context aware denoising module designed to sustain the advancements
achieved by the cooperative efforts of the other modules. This is facilitated
through the incorporation of a proper skip connection, ensuring that essential
details and structures identified by other modules are effectively retained and
not lost during denoising. Extensive experimental results across simulated and
real-world datasets demonstrate that DeepMix is notable for surpassing existing
methodologies, offering marked improvements in both image quality and
computational efficiency.

</details>


### [266] [BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World](https://arxiv.org/pdf/2505.16154)
*Ji Guo, Long Zhou, Zhijin Wang, Jiaming He, Qiyang Song, Aiguo Chen, Wenbo Jiang*

Main category: cs.CV

TL;DR: BadDepth is the first backdoor attack method for Monocular Depth Estimation (MDE) models, overcoming limitations of existing methods by manipulating object depth and using digital-to-physical augmentation for robustness.


<details>
  <summary>Details</summary>
Motivation: The vulnerability of MDE models to backdoor attacks is unexplored, despite their widespread use in critical applications like autonomous driving and robotics.

Method: BadDepth selectively manipulates target object depth using image segmentation and depth completion, generating poisoned datasets. Digital-to-physical augmentation is introduced for real-world robustness.

Result: Extensive experiments show BadDepth's effectiveness in digital and physical domains, unaffected by environmental factors.

Conclusion: BadDepth successfully addresses the gap in backdoor attacks for MDE models, proving robust in real-world scenarios.

Abstract: In recent years, deep learning-based Monocular Depth Estimation (MDE) models
have been widely applied in fields such as autonomous driving and robotics.
However, their vulnerability to backdoor attacks remains unexplored. To fill
the gap in this area, we conduct a comprehensive investigation of backdoor
attacks against MDE models. Typically, existing backdoor attack methods can not
be applied to MDE models. This is because the label used in MDE is in the form
of a depth map. To address this, we propose BadDepth, the first backdoor attack
targeting MDE models. BadDepth overcomes this limitation by selectively
manipulating the target object's depth using an image segmentation model and
restoring the surrounding areas via depth completion, thereby generating
poisoned datasets for object-level backdoor attacks. To improve robustness in
physical world scenarios, we further introduce digital-to-physical augmentation
to adapt to the domain gap between the physical world and the digital domain.
Extensive experiments on multiple models validate the effectiveness of BadDepth
in both the digital domain and the physical world, without being affected by
environmental factors.

</details>


### [267] [Breaking Complexity Barriers: High-Resolution Image Restoration with Rank Enhanced Linear Attention](https://arxiv.org/pdf/2505.16157)
*Yuang Ai, Huaibo Huang, Tao Wu, Qihang Fan, Ran He*

Main category: cs.CV

TL;DR: LAformer, a Transformer-based model with Rank Enhanced Linear Attention (RELA), improves image restoration by combining linear and channel attention, outperforming SOTA methods efficiently.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic complexity and limited global context modeling of Transformer-based models in high-resolution image restoration.

Method: Propose RELA to enhance linear attention with depthwise convolution, and LAformer integrates linear and channel attention for global perception and local fitting.

Result: LAformer outperforms SOTA methods across 7 IR tasks and 21 benchmarks with computational efficiency.

Conclusion: LAformer offers an efficient and effective solution for high-resolution image restoration, overcoming limitations of existing methods.

Abstract: Transformer-based models have made remarkable progress in image restoration
(IR) tasks. However, the quadratic complexity of self-attention in Transformer
hinders its applicability to high-resolution images. Existing methods mitigate
this issue with sparse or window-based attention, yet inherently limit global
context modeling. Linear attention, a variant of softmax attention,
demonstrates promise in global context modeling while maintaining linear
complexity, offering a potential solution to the above challenge. Despite its
efficiency benefits, vanilla linear attention suffers from a significant
performance drop in IR, largely due to the low-rank nature of its attention
map. To counter this, we propose Rank Enhanced Linear Attention (RELA), a
simple yet effective method that enriches feature representations by
integrating a lightweight depthwise convolution. Building upon RELA, we propose
an efficient and effective image restoration Transformer, named LAformer.
LAformer achieves effective global perception by integrating linear attention
and channel attention, while also enhancing local fitting capabilities through
a convolutional gated feed-forward network. Notably, LAformer eliminates
hardware-inefficient operations such as softmax and window shifting, enabling
efficient processing of high-resolution images. Extensive experiments across 7
IR tasks and 21 benchmarks demonstrate that LAformer outperforms SOTA methods
and offers significant computational advantages.

</details>


### [268] [L2RDaS: Synthesizing 4D Radar Tensors for Model Generalization via Dataset Expansion](https://arxiv.org/pdf/2503.03637)
*Woo-Jin Jung, Dong-Hee Paek, Seung-Hyun Kong*

Main category: cs.CV

TL;DR: L2RDaS synthesizes 4D radar tensors from LiDAR data to address dataset scarcity, improving model generalization and detection performance.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available 4D radar tensor datasets limits model generalization, and existing synthetic methods fail to fully utilize spatial information.

Method: Proposes L2RDaS, a framework using a modified U-Net and OBIS module to synthesize 4D radar tensors from LiDAR data.

Result: Improves detection performance by 4.25% in $AP_{BEV}$ and 2.87% in $AP_{3D}$, with further gains via GT-Aug.

Conclusion: L2RDaS effectively enhances model generalization and performance by leveraging existing LiDAR datasets for radar tensor synthesis.

Abstract: 4-dimensional (4D) radar is increasingly adopted in autonomous driving for
perception tasks, owing to its robustness under adverse weather conditions. To
better utilize the spatial information inherent in 4D radar data, recent deep
learning methods have transitioned from using sparse point cloud to 4D radar
tensors. However, the scarcity of publicly available 4D radar tensor datasets
limits model generalization across diverse driving scenarios. Previous methods
addressed this by synthesizing radar data, but the outputs did not fully
exploit the spatial information characteristic of 4D radar. To overcome these
limitations, we propose LiDAR-to-4D radar data synthesis (L2RDaS), a framework
that synthesizes spatially informative 4D radar tensors from LiDAR data
available in existing autonomous driving datasets. L2RDaS integrates a modified
U-Net architecture to effectively capture spatial information and an object
information supplement (OBIS) module to enhance reflection fidelity. This
framework enables the synthesis of radar tensors across diverse driving
scenarios without additional sensor deployment or data collection. L2RDaS
improves model generalization by expanding real datasets with synthetic radar
tensors, achieving an average increase of 4.25\% in ${{AP}_{BEV}}$ and 2.87\%
in ${{AP}_{3D}}$ across three detection models. Additionally, L2RDaS supports
ground-truth augmentation (GT-Aug) by embedding annotated objects into LiDAR
data and synthesizing them into radar tensors, resulting in further average
increases of 3.75\% in ${{AP}_{BEV}}$ and 4.03\% in ${{AP}_{3D}}$. The
implementation will be available at https://github.com/kaist-avelab/K-Radar.

</details>


### [269] [Deep Learning-Driven Ultra-High-Definition Image Restoration: A Survey](https://arxiv.org/pdf/2505.16161)
*Liyan Wang, Weixiang Zhou, Cong Wang, Kin-Man Lam, Zhixun Su, Jinshan Pan*

Main category: cs.CV

TL;DR: A systematic review of recent advancements in UHD image restoration, covering datasets, algorithms, and deep learning innovations, with a proposed classification framework and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address quality degradation in UHD images and provide a comprehensive resource for understanding state-of-the-art developments in the field.

Method: Summarizes degradation models, reviews UHD datasets, organizes literature by degradation types, and classifies methods by network architectures and sampling strategies.

Result: Highlights milestones in deep learning-driven UHD restoration and proposes a clear classification framework for existing methods.

Conclusion: Offers insights into current research and suggests future directions, supported by a related repository for further exploration.

Abstract: Ultra-high-definition (UHD) image restoration aims to specifically solve the
problem of quality degradation in ultra-high-resolution images. Recent
advancements in this field are predominantly driven by deep learning-based
innovations, including enhancements in dataset construction, network
architecture, sampling strategies, prior knowledge integration, and loss
functions. In this paper, we systematically review recent progress in UHD image
restoration, covering various aspects ranging from dataset construction to
algorithm design. This serves as a valuable resource for understanding
state-of-the-art developments in the field. We begin by summarizing degradation
models for various image restoration subproblems, such as super-resolution,
low-light enhancement, deblurring, dehazing, deraining, and desnowing, and
emphasizing the unique challenges of their application to UHD image
restoration. We then highlight existing UHD benchmark datasets and organize the
literature according to degradation types and dataset construction methods.
Following this, we showcase major milestones in deep learning-driven UHD image
restoration, reviewing the progression of restoration tasks, technological
developments, and evaluations of existing methods. We further propose a
classification framework based on network architectures and sampling
strategies, helping to clearly organize existing methods. Finally, we share
insights into the current research landscape and propose directions for further
advancements. A related repository is available at
https://github.com/wlydlut/UHD-Image-Restoration-Survey.

</details>


### [270] [ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement](https://arxiv.org/pdf/2503.11787)
*Samuel W. Remedios, Shuwen Wei, Shuo Han, Jinwei Zhang, Aaron Carass, Kurt G. Schilling, Dzung L. Pham, Jerry L. Prince, Blake E. Dewey*

Main category: cs.CV

TL;DR: ECLARE is a self-supervised super-resolution method for 2D MR images, addressing slice profile, gap, domain shift, and arbitrary upsampling, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Automated 3D analysis struggles with 2D MR volumes due to thick slices and gaps, while existing SR methods fail to address all key factors.

Method: ECLARE estimates slice profiles, trains a network for in-plane SR, and includes anti-aliasing, avoiding external data to prevent domain shift.

Result: ECLARE outperforms cubic B-spline, SMORE, and others in signal recovery and downstream tasks, validated by simulations.

Conclusion: ECLARE is effective for 2D MR SR, avoids domain shift, and is open-source.

Abstract: In clinical imaging, magnetic resonance (MR) image volumes are often acquired
as stacks of 2D slices with decreased scan times, improved signal-to-noise
ratio, and image contrasts unique to 2D MR pulse sequences. While this is
sufficient for clinical evaluation, automated algorithms designed for 3D
analysis perform poorly on multi-slice 2D MR volumes, especially those with
thick slices and gaps between slices. Super-resolution (SR) methods aim to
address this problem, but previous methods do not address all of the following:
slice profile shape estimation, slice gap, domain shift, and non-integer or
arbitrary upsampling factors. In this paper, we propose ECLARE (Efficient
Cross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method
that addresses each of these factors. ECLARE uses a slice profile estimated
from the multi-slice 2D MR volume, trains a network to learn the mapping from
low-resolution to high-resolution in-plane patches from the same volume, and
performs SR with anti-aliasing. We compared ECLARE to cubic B-spline
interpolation, SMORE, and other contemporary SR methods. We used realistic and
representative simulations so that quantitative performance against ground
truth can be computed, and ECLARE outperformed all other methods in both signal
recovery and downstream tasks. Importantly, as ECLARE does not use external
training data it cannot suffer from domain shift between training and testing.
Our code is open-source and available at
https://www.github.com/sremedios/eclare.

</details>


### [271] [RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition](https://arxiv.org/pdf/2505.16165)
*Yechan Park, Gyuhyeon Pak, Euntai Kim*

Main category: cs.CV

TL;DR: The paper introduces RE-TRIP, a novel LiDAR-based descriptor for place recognition that combines geometric and reflectivity data to improve robustness in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based place recognition methods focus only on geometric data, ignoring reflectivity, which can enhance performance in complex environments.

Method: The authors propose RE-TRIP, a descriptor using both geometry and reflectivity, along with keypoint extraction, segmentation, matching, and loop verification methods.

Result: Experiments on public datasets show RE-TRIP outperforms state-of-the-art methods like Scan Context and STD.

Conclusion: RE-TRIP effectively leverages reflectivity to improve place recognition in diverse and dynamic environments.

Abstract: While most people associate LiDAR primarily with its ability to measure
distances and provide geometric information about the environment (via point
clouds), LiDAR also captures additional data, including reflectivity or
intensity values. Unfortunately, when LiDAR is applied to Place Recognition
(PR) in mobile robotics, most previous works on LiDAR-based PR rely only on
geometric measurements, neglecting the additional reflectivity information that
LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named
RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new
descriptor leverages both geometric measurements and reflectivity to enhance
robustness in challenging scenarios such as geometric degeneracy, high
geometric similarity, and the presence of dynamic objects. To implement RE-TRIP
in real-world applications, we further propose (1) a keypoint extraction
method, (2) a key instance segmentation method, (3) a RE-TRIP matching method,
and (4) a reflectivity-combined loop verification method. Finally, we conduct a
series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to
public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios
such as long corridors, bridges, large-scale urban areas, and highly dynamic
environments -- our experimental results show that the proposed method
outperforms existing state-of-the-art methods in terms of Scan Context,
Intensity Scan Context, and STD.

</details>


### [272] [TRAIL: Transferable Robust Adversarial Images via Latent diffusion](https://arxiv.org/pdf/2505.16166)
*Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, Cairong Zhao*

Main category: cs.CV

TL;DR: TRAIL improves adversarial attack transferability by aligning adversarial feature distribution with real-world data using a diffusion model framework.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks often fail to transfer across models due to distribution mismatches between generated adversarial features and real-world data.

Method: TRAIL adapts a diffusion model at test-time, combining adversarial and perceptual objectives to generate realistic adversarial samples.

Result: TRAIL outperforms state-of-the-art methods in cross-model attack transferability.

Conclusion: Aligning adversarial feature distribution with real-world data is key for effective black-box attacks.

Abstract: Adversarial attacks exploiting unrestricted natural perturbations present
severe security risks to deep learning systems, yet their transferability
across models remains limited due to distribution mismatches between generated
adversarial features and real-world data. While recent works utilize
pre-trained diffusion models as adversarial priors, they still encounter
challenges due to the distribution shift between the distribution of ideal
adversarial samples and the natural image distribution learned by the diffusion
model. To address the challenge, we propose Transferable Robust Adversarial
Images via Latent Diffusion (TRAIL), a test-time adaptation framework that
enables the model to generate images from a distribution of images with
adversarial features and closely resembles the target images. To mitigate the
distribution shift, during attacks, TRAIL updates the diffusion U-Net's weights
by combining adversarial objectives (to mislead victim models) and perceptual
constraints (to preserve image realism). The adapted model then generates
adversarial samples through iterative noise injection and denoising guided by
these objectives. Experiments demonstrate that TRAIL significantly outperforms
state-of-the-art methods in cross-model attack transferability, validating that
distribution-aligned adversarial feature synthesis is critical for practical
black-box attacks.

</details>


### [273] [Erased or Dormant? Rethinking Concept Erasure Through Reversibility](https://arxiv.org/pdf/2505.16174)
*Ping Liu, Chi Zhang*

Main category: cs.CV

TL;DR: Current concept erasure methods in diffusion models suppress but don't fully eliminate targeted concepts, as erased concepts can reemerge with minimal adaptation.


<details>
  <summary>Details</summary>
Motivation: To determine if concept erasure techniques genuinely remove generative capacity or just superficially suppress concepts.

Method: Systematic evaluation of two concept erasure methods (Unified Concept Editing and Erased Stable Diffusion) using instance-level fine-tuning to test reactivation potential.

Result: Erased concepts often reemerge with high visual fidelity after minimal adaptation, showing suppression without full elimination.

Conclusion: Existing methods have limitations; deeper interventions and stricter evaluation are needed for irreversible concept removal.

Abstract: To what extent does concept erasure eliminate generative capacity in
diffusion models? While prior evaluations have primarily focused on measuring
concept suppression under specific textual prompts, we explore a complementary
and fundamental question: do current concept erasure techniques genuinely
remove the ability to generate targeted concepts, or do they merely achieve
superficial, prompt-specific suppression? We systematically evaluate the
robustness and reversibility of two representative concept erasure methods,
Unified Concept Editing and Erased Stable Diffusion, by probing their ability
to eliminate targeted generative behaviors in text-to-image models. These
methods attempt to suppress undesired semantic concepts by modifying internal
model parameters, either through targeted attention edits or model-level
fine-tuning strategies. To rigorously assess whether these techniques truly
erase generative capacity, we propose an instance-level evaluation strategy
that employs lightweight fine-tuning to explicitly test the reactivation
potential of erased concepts. Through quantitative metrics and qualitative
analyses, we show that erased concepts often reemerge with substantial visual
fidelity after minimal adaptation, indicating that current methods suppress
latent generative representations without fully eliminating them. Our findings
reveal critical limitations in existing concept erasure approaches and
highlight the need for deeper, representation-level interventions and more
rigorous evaluation standards to ensure genuine, irreversible removal of
concepts from generative models.

</details>


### [274] [QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design](https://arxiv.org/pdf/2505.16175)
*Benjamin Schneider, Dongfu Jiang, Chao Du, Tianyu Pang, Wenhu Chen*

Main category: cs.CV

TL;DR: QuickVideo accelerates long-video understanding by addressing decoding and prefilling bottlenecks with parallelized decoding, KV-cache pruning, and CPU-GPU overlap.


<details>
  <summary>Details</summary>
Motivation: Long-video understanding is vital for real-world applications but hindered by slow decoding and high memory use in VideoLLMs.

Method: QuickVideo combines QuickDecoder (parallelized CPU decoding), QuickPrefill (KV-cache pruning), and CPU-GPU overlapping.

Result: Reduces inference time by a minute, supports more frames with less memory, and generalizes across video durations.

Conclusion: QuickVideo enables scalable, efficient long-video understanding on limited hardware.

Abstract: Long-video understanding has emerged as a crucial capability in real-world
applications such as video surveillance, meeting summarization, educational
lecture analysis, and sports broadcasting. However, it remains computationally
prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential
video decoding, the process of converting the raw bit stream to RGB frames can
take up to a minute for hour-long video inputs, and 2) costly prefilling of up
to several million tokens for LLM inference, resulting in high latency and
memory use. To address these challenges, we propose QuickVideo, a
system-algorithm co-design that substantially accelerates long-video
understanding to support real-time downstream applications. It comprises three
key innovations: QuickDecoder, a parallelized CPU-based video decoder that
achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals
processed concurrently; QuickPrefill, a memory-efficient prefilling method
using KV-cache pruning to support more frames with less GPU memory; and an
overlapping scheme that overlaps CPU video decoding with GPU inference.
Together, these components infernece time reduce by a minute on long video
inputs, enabling scalable, high-quality video understanding even on limited
hardware. Experiments show that QuickVideo generalizes across durations and
sampling rates, making long video processing feasible in practice.

</details>


### [275] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/pdf/2505.16180)
*Ashim Dahal, Ankit Ghimire, Saydul Akbar Murad, Nick Rahimi*

Main category: cs.CV

TL;DR: Redemption Score is a hybrid framework for evaluating image captions, combining MID, DINO-based similarity, and BERTScore, outperforming prior methods on Flickr8k.


<details>
  <summary>Details</summary>
Motivation: Existing metrics often fail to fully assess visual semantics and language pragmatics in image captions.

Method: Triangulates three signals: MID for distributional alignment, DINO for visual grounding, and BERTScore for text similarity.

Result: Achieves Kendall-τ of 56.43 on Flickr8k, outperforming twelve prior methods and correlating better with human judgments.

Conclusion: Redemption Score offers a robust, nuanced evaluation, redeeming image semantics and linguistic interpretability.

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [276] [Understanding Generative AI Capabilities in Everyday Image Editing Tasks](https://arxiv.org/pdf/2505.16181)
*Mohammad Reza Taesiri, Brandon Collins, Logan Bolton, Viet Dac Lai, Franck Dernoncourt, Trung Bui, Anh Totti Nguyen*

Main category: cs.CV

TL;DR: The study analyzes 83k image-editing requests from Reddit to understand user preferences and AI editor capabilities, finding that only 33% of requests can be handled by top AI editors like GPT-4o, with struggles in precise edits and preserving identities.


<details>
  <summary>Details</summary>
Motivation: To identify common image-editing requests, user preferences (precise vs. creative edits), and assess AI editor performance compared to human editors.

Method: Analysis of 83k requests and 305k edits from a Reddit community over 12 years, evaluating AI editor success rates and comparing human and VLM judge preferences.

Result: Only 33% of requests are successfully handled by AI editors, which struggle with precise edits, identity preservation, and often make unwanted touch-ups. VLM judges may favor AI edits over human ones.

Conclusion: AI editors need improvement in handling precise and identity-preserving edits, and VLM judge biases should be considered when evaluating AI performance.

Abstract: Generative AI (GenAI) holds significant promise for automating everyday image
editing tasks, especially following the recent release of GPT-4o on March 25,
2025. However, what subjects do people most often want edited? What kinds of
editing actions do they want to perform (e.g., removing or stylizing the
subject)? Do people prefer precise edits with predictable outcomes or highly
creative ones? By understanding the characteristics of real-world requests and
the corresponding edits made by freelance photo-editing wizards, can we draw
lessons for improving AI-based editors and determine which types of requests
can currently be handled successfully by AI editors? In this paper, we present
a unique study addressing these questions by analyzing 83k requests from the
past 12 years (2013-2025) on the Reddit community, which collected 305k
PSR-wizard edits. According to human ratings, approximately only 33% of
requests can be fulfilled by the best AI editors (including GPT-4o,
Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on
low-creativity requests that require precise editing than on more open-ended
tasks. They often struggle to preserve the identity of people and animals, and
frequently make non-requested touch-ups. On the other side of the table, VLM
judges (e.g., o1) perform differently from human judges and may prefer AI edits
more than human edits. Code and qualitative examples are available at:
https://psrdataset.github.io

</details>


### [277] [VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought](https://arxiv.org/pdf/2505.16192)
*Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang*

Main category: cs.CV

TL;DR: VLM-R³ enhances MLLMs by dynamically focusing on and revisiting visual regions for precise grounding of textual reasoning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with complex tasks requiring dynamic visual grounding and iterative reasoning.

Method: Introduces VLM-R³ with Region-Conditioned Reinforcement Policy Optimization (R-GRPO) and a curated VLIR corpus for training.

Result: Achieves new state-of-the-art performance on benchmarks like MathVista and ScienceQA, especially in tasks needing spatial reasoning.

Conclusion: VLM-R³ effectively bridges the gap between textual reasoning and visual evidence, improving performance in complex tasks.

Abstract: Recently, reasoning-based MLLMs have achieved a degree of success in
generating long-form textual reasoning chains. However, they still struggle
with complex tasks that necessitate dynamic and iterative focusing on and
revisiting of visual regions to achieve precise grounding of textual reasoning
in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual
\textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and
\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)
decide \emph{when} additional visual evidence is needed, (ii) determine
\emph{where} to ground within the image, and (iii) seamlessly weave the
relevant sub-image content back into an interleaved chain-of-thought. The core
of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization
(R-GRPO)}, a training paradigm that rewards the model for selecting informative
regions, formulating appropriate transformations (e.g.\ crop, zoom), and
integrating the resulting visual context into subsequent reasoning steps. To
bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual
Interleaved Rationale (VLIR) corpus that provides step-level supervision on
region selection and textual justification. Extensive experiments on MathVista,
ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art
in zero-shot and few-shot settings, with the largest gains appearing on
questions demanding subtle spatial reasoning or fine-grained visual cue
extraction.

</details>


### [278] [A Causal Approach to Mitigate Modality Preference Bias in Medical Visual Question Answering](https://arxiv.org/pdf/2505.16209)
*Shuchang Ye, Usman Naseem, Mingyuan Meng, Dagan Feng, Jinman Kim*

Main category: cs.CV

TL;DR: The paper introduces MedCFVQA, a model addressing modality preference bias in MedVQA by using causal graphs and reconstructing datasets to reduce prior dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing MedVQA models suffer from modality preference bias, where one modality dominates, limiting multimodal learning.

Method: Proposed MedCFVQA uses causal graphs to eliminate bias during inference and reconstructs datasets (SLAKE-CP, RadVQA-CP) to reduce prior dependencies.

Result: MedCFVQA outperforms non-causal models on both original and reconstructed datasets (SLAKE, RadVQA, SLAKE-CP, RadVQA-CP).

Conclusion: MedCFVQA effectively mitigates modality bias and improves performance by addressing dataset biases and leveraging causal inference.

Abstract: Medical Visual Question Answering (MedVQA) is crucial for enhancing the
efficiency of clinical diagnosis by providing accurate and timely responses to
clinicians' inquiries regarding medical images. Existing MedVQA models suffered
from modality preference bias, where predictions are heavily dominated by one
modality while overlooking the other (in MedVQA, usually questions dominate the
answer but images are overlooked), thereby failing to learn multimodal
knowledge. To overcome the modality preference bias, we proposed a Medical
CounterFactual VQA (MedCFVQA) model, which trains with bias and leverages
causal graphs to eliminate the modality preference bias during inference.
Existing MedVQA datasets exhibit substantial prior dependencies between
questions and answers, which results in acceptable performance even if the
model significantly suffers from the modality preference bias. To address this
issue, we reconstructed new datasets by leveraging existing MedVQA datasets and
Changed their P3rior dependencies (CP) between questions and their answers in
the training and test set. Extensive experiments demonstrate that MedCFVQA
significantly outperforms its non-causal counterpart on both SLAKE, RadVQA and
SLAKE-CP, RadVQA-CP datasets.

</details>


### [279] [A Shape-Aware Total Body Photography System for In-focus Surface Coverage Optimization](https://arxiv.org/pdf/2505.16228)
*Wei-Lun Huang, Joshua Liu, Davood Tashayyod, Jun Kang, Amir Gandjbakhche, Misha Kazhdan, Mehran Armand*

Main category: cs.CV

TL;DR: A novel shape-aware Total Body Photography (TBP) system improves image quality for skin cancer screening by optimizing resolution and sharpness using 3D body shape estimation and focus optimization.


<details>
  <summary>Details</summary>
Motivation: Existing TBP systems lack optimal image quality for automatic detection of suspicious skin lesions, necessitating improvements in resolution and sharpness.

Method: The system uses depth and RGB cameras on a rotary beam, 3D body shape estimation, and focus optimization to capture high-fidelity full-body images.

Result: Achieves 0.068 mm/pixel resolution and 85% in-focus coverage in simulations, and 0.0566 mm/pixel with 95% in-focus on a mannequin, outperforming auto-focus.

Conclusion: The proposed system enhances automated skin lesion analysis, advancing skin cancer screening.

Abstract: Total Body Photography (TBP) is becoming a useful screening tool for patients
at high risk for skin cancer. While much progress has been made, existing TBP
systems can be further improved for automatic detection and analysis of
suspicious skin lesions, which is in part related to the resolution and
sharpness of acquired images. This paper proposes a novel shape-aware TBP
system automatically capturing full-body images while optimizing image quality
in terms of resolution and sharpness over the body surface. The system uses
depth and RGB cameras mounted on a 360-degree rotary beam, along with 3D body
shape estimation and an in-focus surface optimization method to select the
optimal focus distance for each camera pose. This allows for optimizing the
focused coverage over the complex 3D geometry of the human body given the
calibrated camera poses. We evaluate the effectiveness of the system in
capturing high-fidelity body images. The proposed system achieves an average
resolution of 0.068 mm/pixel and 0.0566 mm/pixel with approximately 85% and 95%
of surface area in-focus, evaluated on simulation data of diverse body shapes
and poses as well as a real scan of a mannequin respectively. Furthermore, the
proposed shape-aware focus method outperforms existing focus protocols (e.g.
auto-focus). We believe the high-fidelity imaging enabled by the proposed
system will improve automated skin lesion analysis for skin cancer screening.

</details>


### [280] [CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering](https://arxiv.org/pdf/2505.16229)
*Yuren Mao, Wenyi Xu, Yuyang Qin, Yunjun Gao*

Main category: cs.CV

TL;DR: CT-Agent, a multimodal framework, improves CT radiology question answering by addressing anatomic complexity and spatial relationships across slices.


<details>
  <summary>Details</summary>
Motivation: Radiologists face challenges in creating CT reports due to time constraints and errors. Existing VQA systems fail to handle CTQA tasks effectively.

Method: CT-Agent uses anatomically independent tools and a global-local token compression strategy to simplify complexity and capture spatial relationships.

Result: CT-Agent outperforms on CT-RATE and RadGenome-ChestCT datasets.

Conclusion: CT-Agent is a promising solution for automating CT radiology reports and answering radiologists' questions.

Abstract: Computed Tomography (CT) scan, which produces 3D volumetric medical data that
can be viewed as hundreds of cross-sectional images (a.k.a. slices), provides
detailed anatomical information for diagnosis. For radiologists, creating CT
radiology reports is time-consuming and error-prone. A visual question
answering (VQA) system that can answer radiologists' questions about some
anatomical regions on the CT scan and even automatically generate a radiology
report is urgently needed. However, existing VQA systems cannot adequately
handle the CT radiology question answering (CTQA) task for: (1) anatomic
complexity makes CT images difficult to understand; (2) spatial relationship
across hundreds slices is difficult to capture. To address these issues, this
paper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent
adopts anatomically independent tools to break down the anatomic complexity;
furthermore, it efficiently captures the across-slice spatial relationship with
a global-local token compression strategy. Experimental results on two 3D chest
CT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of
CT-Agent.

</details>


### [281] [DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution](https://arxiv.org/pdf/2505.16239)
*Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo, Yulun Zhang*

Main category: cs.CV

TL;DR: DOVE is a one-step diffusion model for video super-resolution (VSR), offering faster inference while maintaining performance comparable to multi-step methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for VSR are slow due to multiple sampling steps. Single-step solutions are challenging due to training overhead and fidelity demands.

Method: DOVE fine-tunes a pretrained video diffusion model (CogVideoX) using a latent-pixel training strategy and a high-quality dataset (HQ-VSR).

Result: DOVE matches or outperforms multi-step VSR methods and achieves a 28x speed-up over existing methods like MGLD-VSR.

Conclusion: DOVE efficiently addresses the speed-performance trade-off in VSR, making it practical for real-world applications.

Abstract: Diffusion models have demonstrated promising performance in real-world video
super-resolution (VSR). However, the dozens of sampling steps they require,
make inference extremely slow. Sampling acceleration techniques, particularly
single-step, provide a potential solution. Nonetheless, achieving one step in
VSR remains challenging, due to the high training overhead on video data and
stringent fidelity demands. To tackle the above issues, we propose DOVE, an
efficient one-step diffusion model for real-world VSR. DOVE is obtained by
fine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To
effectively train DOVE, we introduce the latent-pixel training strategy. The
strategy employs a two-stage scheme to gradually adapt the model to the video
super-resolution task. Meanwhile, we design a video processing pipeline to
construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning
on this dataset further enhances the restoration capability of DOVE. Extensive
experiments show that DOVE exhibits comparable or superior performance to
multi-step diffusion-based VSR methods. It also offers outstanding inference
efficiency, achieving up to a **28$\times$** speed-up over existing methods
such as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.

</details>


### [282] [Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces](https://arxiv.org/pdf/2505.16253)
*Preeti Mehta, Aman Sagar, Suchi Kumari*

Main category: cs.CV

TL;DR: A Swin Transformer model is proposed to distinguish CGI from natural images across RGB, YCbCr, and HSV color spaces, outperforming CNN-based models like VGG-19 and ResNet-50 in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of differentiating CGI from authentic images due to limitations in existing methods.

Method: Uses a Swin Transformer model to capture local and global features, tested on datasets CiFAKE, JSSSTU, and Columbia with data augmentation and t-SNE visualization.

Result: RGB color scheme yielded highest accuracy; model outperformed VGG-19 and ResNet-50 in intra- and inter-dataset evaluations.

Conclusion: Swin Transformer is effective for CGI detection, offering robustness and domain generalization for digital image forensics.

Abstract: This study aims to address the growing challenge of distinguishing
computer-generated imagery (CGI) from authentic digital images across three
different color spaces; RGB, YCbCr, and HSV. Given the limitations of existing
classification methods in handling the complexity and variability of CGI, this
research proposes a Swin Transformer based model for accurate differentiation
between natural and synthetic images. The proposed model leverages the Swin
Transformer's hierarchical architecture to capture local and global features
for distinguishing CGI from natural images. Its performance was assessed
through intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,
and Columbia. The model was evaluated individually on each dataset (D1, D2, D3)
and on the combined datasets (D1+D2+D3) to test its robustness and domain
generalization. To address dataset imbalance, data augmentation techniques were
applied. Additionally, t-SNE visualization was used to demonstrate the feature
separability achieved by the Swin Transformer across the selected color spaces.
The model's performance was tested across all color schemes, with the RGB color
scheme yielding the highest accuracy for each dataset. As a result, RGB was
selected for domain generalization analysis and compared with other CNN-based
models, VGG-19 and ResNet-50. The comparative results demonstrate the proposed
model's effectiveness in detecting CGI, highlighting its robustness and
reliability in both intra-dataset and inter-dataset evaluations. The findings
of this study highlight the Swin Transformer model's potential as an advanced
tool for digital image forensics, particularly in distinguishing CGI from
natural images. The model's strong performance indicates its capability for
domain generalization, making it a valuable asset in scenarios requiring
precise and reliable image classification.

</details>


### [283] [LINEA: Fast and Accurate Line Detection Using Scalable Transformers](https://arxiv.org/pdf/2505.16264)
*Sebastian Janampa, Marios Pattichis*

Main category: cs.CV

TL;DR: A new transformer-based method, LINEA, is introduced for faster and more efficient line detection without requiring pretraining on large datasets.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based line detection methods are slow and require pretraining on large datasets, limiting their use in low-latency applications.

Method: Develops LINEA using Deformable Line Attention (DLA) to eliminate the need for pretraining and improve speed.

Result: LINEA is significantly faster and outperforms previous models in out-of-distribution testing.

Conclusion: LINEA offers a practical solution for efficient line detection without pretraining constraints.

Abstract: Line detection is a basic digital image processing operation used by
higher-level processing methods. Recently, transformer-based methods for line
detection have proven to be more accurate than methods based on CNNs, at the
expense of significantly lower inference speeds. As a result, video analysis
methods that require low latencies cannot benefit from current
transformer-based methods for line detection. In addition, current
transformer-based models require pretraining attention mechanisms on large
datasets (e.g., COCO or Object360). This paper develops a new transformer-based
method that is significantly faster without requiring pretraining the attention
mechanism on large datasets. We eliminate the need to pre-train the attention
mechanism using a new mechanism, Deformable Line Attention (DLA). We use the
term LINEA to refer to our new transformer-based method based on DLA. Extensive
experiments show that LINEA is significantly faster and outperforms previous
models on sAP in out-of-distribution dataset testing.

</details>


### [284] [DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving](https://arxiv.org/pdf/2505.16278)
*Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan*

Main category: cs.CV

TL;DR: DriveMoE introduces a Mixture-of-Experts (MoE) framework for autonomous driving, combining Scene-Specialized Vision MoE and Skill-Specialized Action MoE to handle diverse scenarios effectively.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of processing multi-view sensory data and handling rare maneuvers in autonomous driving by leveraging MoE architecture for specialization.

Method: Proposes DriveMoE, built on Drive-π0, with Vision MoE for dynamic camera selection and Action MoE for activating expert modules based on driving behaviors.

Result: Achieves state-of-the-art performance in Bench2Drive closed-loop evaluation, demonstrating effectiveness.

Conclusion: DriveMoE successfully combines vision and action MoE for robust autonomous driving, with plans to release code and models.

Abstract: End-to-end autonomous driving (E2E-AD) demands effective processing of
multi-view sensory data and robust handling of diverse and complex driving
scenarios, particularly rare maneuvers such as aggressive turns. Recent success
of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)
demonstrates that specialization of parameters enables strong scalability. In
this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a
Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is
built upon our $\pi_0$ Vision-Language-Action (VLA) baseline (originally from
the embodied AI field), called Drive-$\pi_0$. Specifically, we add Vision MoE
to Drive-$\pi_0$ by training a router to select relevant cameras according to
the driving context dynamically. This design mirrors human driving cognition,
where drivers selectively attend to crucial visual cues rather than
exhaustively processing all visual information. In addition, we add Action MoE
by training another router to activate specialized expert modules for different
driving behaviors. Through explicit behavioral specialization, DriveMoE is able
to handle diverse scenarios without suffering from modes averaging like
existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE
achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness
of combining vision and action MoE in autonomous driving tasks. We will release
our code and models of DriveMoE and Drive-$\pi_0$.

</details>


### [285] [ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay](https://arxiv.org/pdf/2505.16282)
*Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, Jiaya Jia*

Main category: cs.CV

TL;DR: ARPO is an RL method for training GUI agents, improving performance on complex tasks by reusing successful experiences and filtering tasks based on baseline performance.


<details>
  <summary>Details</summary>
Motivation: Optimizing long-horizon action sequences for GUI agents is challenging due to sparse rewards and delayed feedback.

Method: Proposes ARPO, an RL approach combining GRPO with a replay buffer and task selection strategy.

Result: ARPO achieves competitive results on the OSWorld benchmark, setting a new baseline for GUI agents.

Conclusion: Reinforcement learning is effective for training multi-turn, vision-language GUI agents.

Abstract: Training large language models (LLMs) as interactive agents for controlling
graphical user interfaces (GUIs) presents a unique challenge to optimize
long-horizon action sequences with multimodal feedback from complex
environments. While recent works have advanced multi-turn reinforcement
learning (RL) for reasoning and tool-using capabilities in LLMs, their
application to GUI-based agents remains relatively underexplored due to the
difficulty of sparse rewards, delayed feedback, and high rollout costs. In this
paper, we investigate end-to-end policy optimization for vision-language-based
GUI agents with the aim of improving performance on complex, long-horizon
computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an
end-to-end RL approach that augments Group Relative Policy Optimization (GRPO)
with a replay buffer to reuse the successful experience across training
iterations. To further stabilize the training process, we propose a task
selection strategy that filters tasks based on baseline agent performance,
allowing the agent to focus on learning from informative interactions.
Additionally, we compare ARPO with offline preference optimization approaches,
highlighting the advantages of policy-based methods in GUI environments.
Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive
results, establishing a new performance baseline for LLM-based GUI agents
trained via reinforcement learning. Our findings underscore the effectiveness
of reinforcement learning for training multi-turn, vision-language GUI agents
capable of managing complex real-world UI interactions. Codes and
models:https://github.com/dvlab-research/ARPO.git.

</details>


### [286] [Efficient Prototype Consistency Learning in Medical Image Segmentation via Joint Uncertainty and Data Augmentation](https://arxiv.org/pdf/2505.16283)
*Lijian Li, Yuanpeng He, Chi-Man Pun*

Main category: cs.CV

TL;DR: EPCL-JUDA enhances prototype learning in semi-supervised medical image segmentation by combining uncertainty quantification and data augmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled data limits prototype expressiveness in medical image segmentation, necessitating a more robust method.

Method: EPCL-JUDA uses joint uncertainty quantification and data augmentation within a Mean-Teacher framework to generate reliable prototypes from labeled and unlabeled data.

Result: The method achieves superior performance on datasets like Left Atrium and Pancreas-NIH, demonstrating its effectiveness.

Conclusion: EPCL-JUDA improves prototype learning and segmentation accuracy, with potential for broader medical imaging applications.

Abstract: Recently, prototype learning has emerged in semi-supervised medical image
segmentation and achieved remarkable performance. However, the scarcity of
labeled data limits the expressiveness of prototypes in previous methods,
potentially hindering the complete representation of prototypes for class
embedding. To overcome this issue, we propose an efficient prototype
consistency learning via joint uncertainty quantification and data augmentation
(EPCL-JUDA) to enhance the semantic expression of prototypes based on the
framework of Mean-Teacher. The concatenation of original and augmented labeled
data is fed into student network to generate expressive prototypes. Then, a
joint uncertainty quantification method is devised to optimize pseudo-labels
and generate reliable prototypes for original and augmented unlabeled data
separately. High-quality global prototypes for each class are formed by fusing
labeled and unlabeled prototypes, which are utilized to generate
prototype-to-features to conduct consistency learning. Notably, a prototype
network is proposed to reduce high memory requirements brought by the
introduction of augmented data. Extensive experiments on Left Atrium,
Pancreas-NIH, Type B Aortic Dissection datasets demonstrate EPCL-JUDA's
superiority over previous state-of-the-art approaches, confirming the
effectiveness of our framework. The code will be released soon.

</details>


### [287] [Self-Classification Enhancement and Correction for Weakly Supervised Object Detection](https://arxiv.org/pdf/2505.16294)
*Yufei Yin, Lechao Cheng, Wengang Zhou, Jiajun Deng, Zhou Yu, Houqiang Li*

Main category: cs.CV

TL;DR: A novel WSOD framework addresses classification ambiguities in weakly supervised object detection by integrating intra-class binary classification and a correction algorithm, improving performance on VOC datasets.


<details>
  <summary>Details</summary>
Motivation: To reduce labeling costs and improve weakly supervised object detection by addressing overlooked classification ambiguities between multi-class classification tasks.

Method: Proposes a self-classification enhancement module with intra-class binary classification and a self-classification correction algorithm during inference.

Result: Demonstrates superior performance on VOC 2007 & 2012 datasets.

Conclusion: The framework effectively bridges gaps between tasks and reduces mis-classifications, enhancing WSOD performance.

Abstract: In recent years, weakly supervised object detection (WSOD) has attracted much
attention due to its low labeling cost. The success of recent WSOD models is
often ascribed to the two-stage multi-class classification (MCC) task, i.e.,
multiple instance learning and online classification refinement. Despite
achieving non-trivial progresses, these methods overlook potential
classification ambiguities between these two MCC tasks and fail to leverage
their unique strengths. In this work, we introduce a novel WSOD framework to
ameliorate these two issues. For one thing, we propose a self-classification
enhancement module that integrates intra-class binary classification (ICBC) to
bridge the gap between the two distinct MCC tasks. The ICBC task enhances the
network's discrimination between positive and mis-located samples in a
class-wise manner and forges a mutually reinforcing relationship with the MCC
task. For another, we propose a self-classification correction algorithm during
inference, which combines the results of both MCC tasks to effectively reduce
the mis-classified predictions. Extensive experiments on the prevalent VOC 2007
& 2012 datasets demonstrate the superior performance of our framework.

</details>


### [288] [SAMba-UNet: Synergizing SAM2 and Mamba in UNet with Heterogeneous Aggregation for Cardiac MRI Segmentation](https://arxiv.org/pdf/2505.16304)
*Guohao Huo, Ruiting Dai, Hao Tang*

Main category: cs.CV

TL;DR: The paper proposes SAMba-UNet, a dual-encoder architecture for cardiac MRI segmentation, integrating SAM2, Mamba, and UNet. It introduces Dynamic Feature Fusion Refiner and HOACM to enhance feature extraction and fusion, achieving superior performance on the ACDC dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges in complex pathological feature extraction for automated cardiac MRI segmentation, particularly domain discrepancies and small lesion detection.

Method: Proposes SAMba-UNet with Dynamic Feature Fusion Refiner and HOACM for cross-modal feature learning, multi-scale pooling, and attention-based fusion.

Result: Achieves Dice coefficient of 0.9103 and HD95 boundary error of 1.0859 mm, outperforming existing methods, especially in boundary localization.

Conclusion: SAMba-UNet provides an efficient solution for cardiac disease diagnosis, with plans to open-source the code.

Abstract: To address the challenge of complex pathological feature extraction in
automated cardiac MRI segmentation, this study proposes an innovative
dual-encoder architecture named SAMba-UNet. The framework achieves cross-modal
feature collaborative learning by integrating the vision foundation model SAM2,
the state-space model Mamba, and the classical UNet. To mitigate domain
discrepancies between medical and natural images, a Dynamic Feature Fusion
Refiner is designed, which enhances small lesion feature extraction through
multi-scale pooling and a dual-path calibration mechanism across channel and
spatial dimensions. Furthermore, a Heterogeneous Omni-Attention Convergence
Module (HOACM) is introduced, combining global contextual attention with
branch-selective emphasis mechanisms to effectively fuse SAM2's local
positional semantics and Mamba's long-range dependency modeling capabilities.
Experiments on the ACDC cardiac MRI dataset demonstrate that the proposed model
achieves a Dice coefficient of 0.9103 and an HD95 boundary error of 1.0859 mm,
significantly outperforming existing methods, particularly in boundary
localization for complex pathological structures such as right ventricular
anomalies. This work provides an efficient and reliable solution for automated
cardiac disease diagnosis, and the code will be open-sourced.

</details>


### [289] [Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings](https://arxiv.org/pdf/2505.16313)
*Arjhun Swaminathan, Mete Akgün*

Main category: cs.CV

TL;DR: TEA is a novel adversarial attack method using edge information from target images to craft perturbations, outperforming state-of-the-art methods with fewer queries.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of targeted adversarial attacks in black-box settings where decision regions are narrow and current methods rely heavily on geometric properties.

Method: Utilizes edge information from the target image to perturb it, creating adversarial examples closer to the source image while achieving the target classification.

Result: Outperforms state-of-the-art methods, reducing queries by nearly 70% in low query settings, and improves target initialization for geometry-based attacks.

Conclusion: TEA is an efficient and effective method for crafting targeted adversarial attacks, especially in real-world black-box scenarios.

Abstract: Deep neural networks for image classification remain vulnerable to
adversarial examples -- small, imperceptible perturbations that induce
misclassifications. In black-box settings, where only the final prediction is
accessible, crafting targeted attacks that aim to misclassify into a specific
target class is particularly challenging due to narrow decision regions.
Current state-of-the-art methods often exploit the geometric properties of the
decision boundary separating a source image and a target image rather than
incorporating information from the images themselves. In contrast, we propose
Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge
information from the target image to carefully perturb it, thereby producing an
adversarial image that is closer to the source image while still achieving the
desired target classification. Our approach consistently outperforms current
state-of-the-art methods across different models in low query settings (nearly
70\% fewer queries are used), a scenario especially relevant in real-world
applications with limited queries and black-box access. Furthermore, by
efficiently generating a suitable adversarial example, TEA provides an improved
target initialization for established geometry-based attacks.

</details>


### [290] [NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment](https://arxiv.org/pdf/2505.16314)
*Shuhao Han, Haotian Fan, Fangyuan Kong, Wenjie Liao, Chunle Guo, Chongyi Li, Radu Timofte, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Jianhui Sun, Xinli Yue, Tianyi Wang, Huan Hou, Junda Lu, Xinyang Huang, Zitang Zhou, Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao, Trong-Hieu Nguyen-Mau, Minh-Hoang Le, Minh-Khoa Le-Phan, Duy-Nam Ly, Hai-Dang Nguyen, Minh-Triet Tran, Yukang Lin, Yan Hong, Chuanbiao Song, Siyuan Li, Jun Lan, Zhichao Zhang, Xinyue Li, Wei Sun, Zicheng Zhang, Yunhao Li, Xiaohong Liu, Guangtao Zhai, Zitong Xu, Huiyu Duan, Jiarui Wang, Guangji Ma, Liu Yang, Lu Liu, Qiang Hu, Xiongkuo Min, Zichuan Wang, Zhenchen Tang, Bo Peng, Jing Dong, Fengbin Guan, Zihao Yu, Yiting Lu, Wei Luo, Xin Li, Minhao Lin, Haofeng Chen, Xuanxuan He, Kele Xu, Qisheng Xu, Zijian Gao, Tianjiao Wan, Bo-Cheng Qiu, Chih-Chung Hsu, Chia-ming Lee, Yu-Fan Lin, Bo Yu, Zehao Wang, Da Mu, Mingxiu Chen, Junkang Fang, Huamei Sun, Wending Zhao, Zhiyu Wang, Wang Liu, Weikang Yu, Puhong Duan, Bin Sun, Xudong Kang, Shutao Li, Shuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, Jiarong He, Zhishan Qiao, Yongqing Huang, Zewen Chen, Zhe Pang, Juan Wang, Jian Guo, Zhizhuo Shao, Ziyu Feng, Bing Li, Weiming Hu, Hesong Li, Dehua Liu, Zeming Liu, Qingsong Xie, Ruichen Wang, Zhihao Li, Yuqi Liang, Jianqi Bi, Jun Luo, Junfeng Yang, Can Li, Jing Fu, Hongwei Xu, Mingrui Long, Lulin Tang*

Main category: cs.CV

TL;DR: The NTIRE 2025 challenge evaluates text-to-image (T2I) generation models for quality assessment, focusing on image-text alignment and structural distortion detection. It attracted hundreds of participants and submissions, with winning methods outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To address the fine-grained quality assessment of T2I models, focusing on alignment and structural distortion.

Method: The challenge is divided into two tracks: alignment (using EvalMuse-40K dataset) and structural (using EvalMuse-Structure dataset). Participants submit models evaluated in development and test phases.

Result: High participation (371 in alignment, 211 in structural) with many submissions. Winning methods outperformed baselines in both tracks.

Conclusion: The challenge successfully advanced T2I quality assessment, with winning methods demonstrating superior performance.

Abstract: This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)
generation model quality assessment, which will be held in conjunction with the
New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.
The aim of this challenge is to address the fine-grained quality assessment of
text-to-image generation models. This challenge evaluates text-to-image models
from two aspects: image-text alignment and image structural distortion
detection, and is divided into the alignment track and the structural track.
The alignment track uses the EvalMuse-40K, which contains around 40K
AI-Generated Images (AIGIs) generated by 20 popular generative models. The
alignment track has a total of 371 registered participants. A total of 1,883
submissions are received in the development phase, and 507 submissions are
received in the test phase. Finally, 12 participating teams submitted their
models and fact sheets. The structure track uses the EvalMuse-Structure, which
contains 10,000 AI-Generated Images (AIGIs) with corresponding structural
distortion mask. A total of 211 participants have registered in the structure
track. A total of 1155 submissions are received in the development phase, and
487 submissions are received in the test phase. Finally, 8 participating teams
submitted their models and fact sheets. Almost all methods have achieved better
results than baseline methods, and the winning methods in both tracks have
demonstrated superior prediction performance on T2I model quality assessment.

</details>


### [291] [Efficient Motion Prompt Learning for Robust Visual Tracking](https://arxiv.org/pdf/2505.16321)
*Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, Huchuan Lu*

Main category: cs.CV

TL;DR: A lightweight motion prompt tracking method is proposed to enhance vision-based trackers by integrating motion cues, improving robustness with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing trackers rely heavily on visual discriminability, ignoring temporal coherence in video data, which limits robustness.

Method: The method introduces a motion encoder with three positional encodings, a fusion decoder, and adaptive weight mechanism to dynamically combine motion and visual features.

Result: Integrated into three trackers, the module significantly improves robustness across seven benchmarks with minimal training cost and speed impact.

Conclusion: The motion prompt method effectively leverages temporal coherence, enhancing tracking performance efficiently.

Abstract: Due to the challenges of processing temporal information, most trackers
depend solely on visual discriminability and overlook the unique temporal
coherence of video data. In this paper, we propose a lightweight and
plug-and-play motion prompt tracking method. It can be easily integrated into
existing vision-based trackers to build a joint tracking framework leveraging
both motion and vision cues, thereby achieving robust tracking through
efficient prompt learning. A motion encoder with three different positional
encodings is proposed to encode the long-term motion trajectory into the visual
embedding space, while a fusion decoder and an adaptive weight mechanism are
designed to dynamically fuse visual and motion features. We integrate our
motion module into three different trackers with five models in total.
Experiments on seven challenging tracking benchmarks demonstrate that the
proposed motion module significantly improves the robustness of vision-based
trackers, with minimal training costs and negligible speed sacrifice. Code is
available at https://github.com/zj5559/Motion-Prompt-Tracking.

</details>


### [292] [TensorAR: Refinement is All You Need in Autoregressive Image Generation](https://arxiv.org/pdf/2505.16324)
*Cheng Cheng, Lin Song, Yicheng Xiao, Yuxin Chen, Xuchong Zhang, Hongbin Sun, Ying Shan*

Main category: cs.CV

TL;DR: TensorAR introduces a new autoregressive image generation method by predicting overlapping image patches (tensors) iteratively, improving quality over traditional AR models.


<details>
  <summary>Details</summary>
Motivation: Traditional AR image generators lack refinement mechanisms, limiting their quality compared to diffusion models. TensorAR addresses this gap.

Method: TensorAR reformulates AR generation as next-tensor prediction, using sliding windows for iterative refinement. A discrete tensor noising scheme prevents information leakage.

Result: Experiments show TensorAR significantly boosts performance of AR models like LlamaGEN, Open-MAGVIT2, and RAR.

Conclusion: TensorAR offers a plug-and-play solution to enhance AR image generation quality through iterative refinement.

Abstract: Autoregressive (AR) image generators offer a language-model-friendly approach
to image generation by predicting discrete image tokens in a causal sequence.
However, unlike diffusion models, AR models lack a mechanism to refine previous
predictions, limiting their generation quality. In this paper, we introduce
TensorAR, a new AR paradigm that reformulates image generation from next-token
prediction to next-tensor prediction. By generating overlapping windows of
image patches (tensors) in a sliding fashion, TensorAR enables iterative
refinement of previously generated content. To prevent information leakage
during training, we propose a discrete tensor noising scheme, which perturbs
input tokens via codebook-indexed noise. TensorAR is implemented as a
plug-and-play module compatible with existing AR models. Extensive experiments
on LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly
improves the generation performance of autoregressive models.

</details>


### [293] [Panoptic Captioning: Seeking An Equivalency Bridge for Image and Text](https://arxiv.org/pdf/2505.16334)
*Kun-Yu Lin, Hongjun Wang, Weining Ren, Kai Han*

Main category: cs.CV

TL;DR: The paper introduces panoptic captioning, a task to generate comprehensive textual descriptions of images, and proposes PancapEngine for data generation and PancapChain for improved performance, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current Multi-modal Large Language Models (MLLMs) in generating detailed, panoptic captions that include entities, locations, attributes, relationships, and global image state.

Method: Proposes PancapEngine for high-quality data generation using entity-aware prompts and PancapChain, which decouples the task into stages for step-by-step captioning. Introduces PancapScore for evaluation.

Result: PancapChain-13B outperforms open-source MLLMs like InternVL-2.5-78B and proprietary models like GPT-4o and Gemini-2.0-Pro.

Conclusion: The proposed methods and data engine effectively advance panoptic captioning, demonstrating superior performance over existing models.

Abstract: This work introduces panoptic captioning, a novel task striving to seek the
minimum text equivalence of images. We take the first step towards panoptic
captioning by formulating it as a task of generating a comprehensive textual
description for an image, which encapsulates all entities, their respective
locations and attributes, relationships among entities, as well as global image
state.Through an extensive evaluation, our work reveals that state-of-the-art
Multi-modal Large Language Models (MLLMs) have limited performance in solving
panoptic captioning. To address this, we propose an effective data engine named
PancapEngine to produce high-quality data and a novel method named PancapChain
to improve panoptic captioning. Specifically, our PancapEngine first detects
diverse categories of entities in images by an elaborate detection suite, and
then generates required panoptic captions using entity-aware prompts.
Additionally, our PancapChain explicitly decouples the challenging panoptic
captioning task into multiple stages and generates panoptic captions step by
step. More importantly, we contribute a comprehensive metric named PancapScore
and a human-curated test set for reliable model evaluation.Experiments show
that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like
InternVL-2.5-78B and even surpass proprietary models like GPT-4o and
Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method.
Project page: https://visual-ai.github.io/pancap/

</details>


### [294] [FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design](https://arxiv.org/pdf/2505.16335)
*Renjie Wei, Songqiang Xu, Qingyu Guo, Meng Li*

Main category: cs.CV

TL;DR: FPQVAR is an efficient post-training FP quantization framework for VAR models, improving image quality and reducing computation costs through algorithm-hardware co-design.


<details>
  <summary>Details</summary>
Motivation: The large parameter size and computation cost of VAR models hinder deployment on edge devices, necessitating an efficient quantization solution.

Method: Proposes Dual Format Quantization, Group-wise Hadamard Transformation, and GHT-Aware Learnable Transformation for activation imbalance and outlier channels. Also designs a low-bit FP quantizer and multiplier with FPGA-based accelerator.

Result: FPQVAR improves FID from 10.83 to 3.58 and IS from 175.9 to 241.5 under 4-bit quantization, with FPGA accelerator achieving 3.1x higher throughput and better energy efficiency.

Conclusion: FPQVAR effectively addresses VAR quantization challenges, enabling efficient deployment on edge devices with superior performance and energy efficiency.

Abstract: Visual autoregressive (VAR) modeling has marked a paradigm shift in image
generation from next-token prediction to next-scale prediction. VAR predicts a
set of tokens at each step from coarse to fine scale, leading to better image
quality and faster inference speed compared to existing diffusion models.
However, the large parameter size and computation cost hinder its deployment on
edge devices. To reduce the memory and computation cost, we propose FPQVAR, an
efficient post-training floating-point (FP) quantization framework for VAR
featuring algorithm and hardware co-design. At the algorithm level, we first
identify the challenges of quantizing VAR. To address them, we propose Dual
Format Quantization for the highly imbalanced input activation. We further
propose Group-wise Hadamard Transformation and GHT-Aware Learnable
Transformation to address the time-varying outlier channels. At the hardware
level, we design the first low-bit FP quantizer and multiplier with lookup
tables on FPGA and propose the first FPGA-based VAR accelerator featuring
low-bit FP computation and an elaborate two-level pipeline. Extensive
experiments show that compared to the state-of-the-art quantization method, our
proposed FPQVAR significantly improves Fr\'echet Inception Distance (FID) from
10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit
quantization. FPQVAR also significantly improves the performance of 6-bit
quantized VAR, bringing it on par with the FP16 model. Our accelerator on
AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x
higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x
higher energy efficiency compared to the integer-based accelerator and GPU
baseline, respectively.

</details>


### [295] [Fusion of Foundation and Vision Transformer Model Features for Dermatoscopic Image Classification](https://arxiv.org/pdf/2505.16338)
*Amirreza Mahbod, Rupert Ecker, Ramona Woitek*

Main category: cs.CV

TL;DR: PanDerm, a dermatology-specific foundation model, performs comparably to fine-tuned ViT models for skin lesion classification, with fusion further improving results.


<details>
  <summary>Details</summary>
Motivation: Accurate skin lesion classification is crucial for skin cancer diagnosis and treatment.

Method: Compare PanDerm with ViT models (ViT base, Swin Transformer V2 base) using frozen features and non-linear probing (MLP, XGBoost, TabNet). ViT models are fine-tuned.

Result: PanDerm-based MLP matches Swin Transformer performance; fusion of PanDerm and Swin predictions enhances results.

Conclusion: PanDerm is effective; future work will explore more models, fine-tuning, and fusion techniques.

Abstract: Accurate classification of skin lesions from dermatoscopic images is
essential for diagnosis and treatment of skin cancer. In this study, we
investigate the utility of a dermatology-specific foundation model, PanDerm, in
comparison with two Vision Transformer (ViT) architectures (ViT base and Swin
Transformer V2 base) for the task of skin lesion classification. Using frozen
features extracted from PanDerm, we apply non-linear probing with three
different classifiers, namely, multi-layer perceptron (MLP), XGBoost, and
TabNet. For the ViT-based models, we perform full fine-tuning to optimize
classification performance. Our experiments on the HAM10000 and MSKCC datasets
demonstrate that the PanDerm-based MLP model performs comparably to the
fine-tuned Swin transformer model, while fusion of PanDerm and Swin Transformer
predictions leads to further performance improvements. Future work will explore
additional foundation models, fine-tuning strategies, and advanced fusion
techniques.

</details>


### [296] [Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation](https://arxiv.org/pdf/2505.16360)
*Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin*

Main category: cs.CV

TL;DR: The paper introduces CACTI and CACTIF, diffusion-based techniques for semantically consistent style transfer to bridge the synthetic-to-real domain gap in semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Addressing the poor performance of models trained on synthetic data in real-world scenarios due to domain gaps, especially with scarce labeled data.

Method: Proposes Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF) for style transfer.

Result: Achieves higher quality images with lower FID scores and better content preservation compared to global transformations.

Conclusion: Demonstrates that class-aware diffusion-based style transfer effectively bridges the domain gap, advancing robust perception systems.

Abstract: Semantic segmentation models trained on synthetic data often perform poorly
on real-world images due to domain gaps, particularly in adverse conditions
where labeled data is scarce. Yet, recent foundation models enable to generate
realistic images without any training. This paper proposes to leverage such
diffusion models to improve the performance of vision models when learned on
synthetic data. We introduce two novel techniques for semantically consistent
style transfer using diffusion models: Class-wise Adaptive Instance
Normalization and Cross-Attention (CACTI) and its extension with selective
attention Filtering (CACTIF). CACTI applies statistical normalization
selectively based on semantic classes, while CACTIF further filters
cross-attention maps based on feature similarity, preventing artifacts in
regions with weak cross-attention correspondences. Our methods transfer style
characteristics while preserving semantic boundaries and structural coherence,
unlike approaches that apply global transformations or generate content without
constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target
domains show that our approach produces higher quality images with lower FID
scores and better content preservation. Our work demonstrates that class-aware
diffusion-based style transfer effectively bridges the synthetic-to-real domain
gap even with minimal target domain data, advancing robust perception systems
for challenging real-world applications. The source code is available at:
https://github.com/echigot/cactif.

</details>


### [297] [Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition](https://arxiv.org/pdf/2505.16372)
*Feng Liu, Bingyu Nan, Xuezhong Qian, Xiaolan Fu*

Main category: cs.CV

TL;DR: The paper proposes TSFmicro, a framework for dynamic micro-expression recognition (DMER) using multimodal fusion of temporal and spatial features, achieving superior performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-expressions reveal true emotions but are hard to recognize due to their transient nature. Current recognition accuracy is low (~50%), necessitating improved methods.

Method: TSFmicro integrates a Retention Network (RetNet) and a transformer-based DMER network, fusing temporal and spatial features in parallel for richer semantic information.

Result: TSFmicro outperforms state-of-the-art methods on three benchmark micro-expression datasets.

Conclusion: The proposed framework effectively addresses DMER challenges by leveraging multimodal fusion, enhancing recognition accuracy.

Abstract: When emotions are repressed, an individual's true feelings may be revealed
through micro-expressions. Consequently, micro-expressions are regarded as a
genuine source of insight into an individual's authentic emotions. However, the
transient and highly localised nature of micro-expressions poses a significant
challenge to their accurate recognition, with the accuracy rate of
micro-expression recognition being as low as 50%, even for professionals. In
order to address these challenges, it is necessary to explore the field of
dynamic micro expression recognition (DMER) using multimodal fusion techniques,
with special attention to the diverse fusion of temporal and spatial modal
features. In this paper, we propose a novel Temporal and Spatial feature Fusion
framework for DMER (TSFmicro). This framework integrates a Retention Network
(RetNet) and a transformer-based DMER network, with the objective of efficient
micro-expression recognition through the capture and fusion of temporal and
spatial relations. Meanwhile, we propose a novel parallel time-space fusion
method from the perspective of modal fusion, which fuses spatio-temporal
information in high-dimensional feature space, resulting in complementary
"where-how" relationships at the semantic level and providing richer semantic
information for the model. The experimental results demonstrate the superior
performance of the TSFmicro method in comparison to other contemporary
state-of-the-art methods. This is evidenced by its effectiveness on three
well-recognised micro-expression datasets.

</details>


### [298] [DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos](https://arxiv.org/pdf/2505.16376)
*Zijia Lu, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen*

Main category: cs.CV

TL;DR: DeCafNet introduces a delegate-and-conquer strategy for efficient long video temporal grounding, reducing computation by 47% while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LVTG are computationally expensive due to full-scale processing of video clips.

Method: DeCafNet uses a sidekick encoder for efficient feature extraction and saliency mapping, followed by query-aware temporal aggregation and multi-scale refinement.

Result: DeCafNet achieves state-of-the-art performance with 47% less computation on benchmark datasets.

Conclusion: DeCafNet balances efficiency and accuracy, setting a new standard for LVTG.

Abstract: Long Video Temporal Grounding (LVTG) aims at identifying specific moments
within lengthy videos based on user-provided text queries for effective content
retrieval. The approach taken by existing methods of dividing video into clips
and processing each clip via a full-scale expert encoder is challenging to
scale due to prohibitive computational costs of processing a large number of
clips in long videos. To address this issue, we introduce DeCafNet, an approach
employing ``delegate-and-conquer'' strategy to achieve computation efficiency
without sacrificing grounding performance. DeCafNet introduces a sidekick
encoder that performs dense feature extraction over all video clips in a
resource-efficient manner, while generating a saliency map to identify the most
relevant clips for full processing by the expert encoder. To effectively
leverage features from sidekick and expert encoders that exist at different
temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines
them via query-aware temporal aggregation and multi-scale temporal refinement
for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate
that DeCafNet reduces computation by up to 47\% while still outperforming
existing methods, establishing a new state-of-the-art for LTVG in terms of both
efficiency and performance. Our code is available at
https://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.

</details>


### [299] [MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module](https://arxiv.org/pdf/2505.16384)
*Haoming Huang, Musen Zhang, Jianxin Yang, Zhen Li, Jinkai Li, Yao Guo*

Main category: cs.CV

TL;DR: MAGE is a multi-task architecture for 6-DoF gaze estimation in HRI, addressing limitations of existing methods by combining directional and positional features and introducing an efficient calibration module.


<details>
  <summary>Details</summary>
Motivation: Existing gaze estimation methods lack comprehensive 6-DoF analysis and struggle with individual variations, limiting their applicability in real-world HRI.

Method: MAGE encodes directional and positional features from facial images, uses dedicated information flow and multiple decoders, and includes an Easy-Calibration module for subject-specific fine-tuning.

Result: Achieves state-of-the-art performance on MPIIFaceGaze, EYEDIAP, and IMRGaze datasets.

Conclusion: MAGE provides a robust solution for 6-DoF gaze estimation in HRI, overcoming individual variations and calibration challenges.

Abstract: Eye gaze can provide rich information on human psychological activities, and
has garnered significant attention in the field of Human-Robot Interaction
(HRI). However, existing gaze estimation methods merely predict either the gaze
direction or the Point-of-Gaze (PoG) on the screen, failing to provide
sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze
analysis in 3D space. Moreover, the variations of eye shape and structure among
individuals also impede the generalization capability of these methods. In this
study, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an
efficient calibration module, to predict the 6-DoF gaze information that is
applicable for the real-word HRI. Our basic model encodes both the directional
and positional features from facial images, and predicts gaze results with
dedicated information flow and multiple decoders. To reduce the impact of
individual variations, we propose a novel calibration module, namely
Easy-Calibration, to fine-tune the basic model with subject-specific data,
which is efficient to implement without the need of a screen. Experimental
results demonstrate that our method achieves state-of-the-art performance on
the public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.

</details>


### [300] [Sketchy Bounding-box Supervision for 3D Instance Segmentation](https://arxiv.org/pdf/2505.16399)
*Qian Deng, Le Hui, Jin Xie, Jian Yang*

Main category: cs.CV

TL;DR: Sketchy-3DIS is a weakly supervised 3D instance segmentation framework that improves performance under inaccurate bounding box supervision by jointly learning a pseudo labeler and segmentator.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining accurate bounding boxes in practical applications, the paper explores sketchy bounding boxes (inaccurate boxes) and aims to improve segmentation performance under such supervision.

Method: The framework includes an adaptive box-to-point pseudo labeler for assigning points in overlapped regions and a coarse-to-fine instance segmentator for refining predictions. Joint training generates high-quality instances.

Result: The method achieves state-of-the-art performance on ScanNetV2 and S3DIS benchmarks, outperforming some fully supervised methods.

Conclusion: Sketchy-3DIS demonstrates the effectiveness of weakly supervised learning with sketchy bounding boxes, offering a practical solution for 3D instance segmentation.

Abstract: Bounding box supervision has gained considerable attention in weakly
supervised 3D instance segmentation. While this approach alleviates the need
for extensive point-level annotations, obtaining accurate bounding boxes in
practical applications remains challenging. To this end, we explore the
inaccurate bounding box, named sketchy bounding box, which is imitated through
perturbing ground truth bounding box by adding scaling, translation, and
rotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance
segmentation framework, which jointly learns pseudo labeler and segmentator to
improve the performance under the sketchy bounding-box supervisions.
Specifically, we first propose an adaptive box-to-point pseudo labeler that
adaptively learns to assign points located in the overlapped parts between two
sketchy bounding boxes to the correct instance, resulting in compact and pure
pseudo instance labels. Then, we present a coarse-to-fine instance segmentator
that first predicts coarse instances from the entire point cloud and then
learns fine instances based on the region of coarse instances. Finally, by
using the pseudo instance labels to supervise the instance segmentator, we can
gradually generate high-quality instances through joint training. Extensive
experiments show that our method achieves state-of-the-art performance on both
the ScanNetV2 and S3DIS benchmarks, and even outperforms several fully
supervised methods using sketchy bounding boxes. Code is available at
https://github.com/dengq7/Sketchy-3DIS.

</details>


### [301] [AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems](https://arxiv.org/pdf/2505.16402)
*Yuanhao Huang, Yilong Ren, Jinlei Wang, Lujia Huo, Xuesong Bai, Jinchuan Zhang, Haiyan Yu*

Main category: cs.CV

TL;DR: A unified adversarial training framework for 2D/3D samples enhances robustness against adversarial attacks in autonomous vehicle perception systems.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities of deep learning-based perception methods to adversarial samples in real-world scenarios.

Method: Proposes a joint adversarial training framework with non-rigid surface modeling and 3D matching for realistic adversarial samples.

Result: Adversarial textures effectively mislead object detectors, showing robustness under varied conditions.

Conclusion: The method improves adversarial attack resilience in autonomous vehicle perception systems.

Abstract: Autonomous vehicles are typical complex intelligent systems with artificial
intelligence at their core. However, perception methods based on deep learning
are extremely vulnerable to adversarial samples, resulting in safety accidents.
How to generate effective adversarial examples in the physical world and
evaluate object detection systems is a huge challenge. In this study, we
propose a unified joint adversarial training framework for both 2D and 3D
samples to address the challenges of intra-class diversity and environmental
variations in real-world scenarios. Building upon this framework, we introduce
an adversarial sample reality enhancement approach that incorporates non-rigid
surface modeling and a realistic 3D matching mechanism. We compare with 5
advanced adversarial patches and evaluate their attack performance on 8 object
detecotrs, including single-stage, two-stage, and transformer-based models.
Extensive experiment results in digital and physical environments demonstrate
that the adversarial textures generated by our method can effectively mislead
the target detection model. Moreover, proposed method demonstrates excellent
robustness and transferability under multi-angle attacks, varying lighting
conditions, and different distance in the physical world. The demo video and
code can be obtained at https://github.com/Huangyh98/AdvReal.git.

</details>


### [302] [Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression](https://arxiv.org/pdf/2505.16411)
*Sreetama Sarkar, Yue Che, Alex Gavin, Peter A. Beerel, Souvik Kundu*

Main category: cs.CV

TL;DR: SPIN reduces hallucinations in LVLMs by selectively suppressing low-attention heads, improving performance and speed.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate misaligned text (hallucinations), and existing solutions increase latency.

Method: SPIN suppresses low-attention heads dynamically during inference without extra compute.

Result: Reduces hallucinations by 2.7x, maintains F1, and improves throughput by 1.8x.

Conclusion: SPIN effectively mitigates hallucinations efficiently, enhancing LVLM reliability.

Abstract: Despite their remarkable progress in multimodal understanding tasks, large
vision language models (LVLMs) often suffer from "hallucinations", generating
texts misaligned with the visual context. Existing methods aimed at reducing
hallucinations through inference time intervention incur a significant increase
in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided
head suppression strategy that can be seamlessly integrated during inference,
without incurring any significant compute or latency overhead. We investigate
whether hallucination in LVLMs can be linked to specific model components. Our
analysis suggests that hallucinations can be attributed to a dynamic subset of
attention heads in each layer. Leveraging this insight, for each text query
token, we selectively suppress attention heads that exhibit low attention to
image tokens, keeping the top-K attention heads intact. Extensive evaluations
on visual question answering and image description tasks demonstrate the
efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining
F1, and improving throughput by 1.8x compared to existing alternatives. Code is
available at https://github.com/YUECHE77/SPIN.

</details>


### [303] [Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models](https://arxiv.org/pdf/2505.16416)
*Chengcheng Wang, Jianyuan Guo, Hongguang Li, Yuchuan Tian, Ying Nie, Chang Xu, Kai Han*

Main category: cs.CV

TL;DR: The paper introduces Circle-RoPE, a novel positional encoding scheme for vision-language models, addressing cross-modal biases in Rotary Position Embedding (RoPE) by ensuring equal distance between text and image tokens.


<details>
  <summary>Details</summary>
Motivation: Existing RoPE variants in vision-language models create unintended cross-modal positional biases, leading to spurious alignments and inconsistent associations.

Method: Proposes Per-Token Distance (PTD) to quantify positional independence and introduces Circle-RoPE, mapping image tokens orthogonally to text tokens. Also employs a staggered layer strategy for enhanced performance.

Result: Circle-RoPE reduces artificial biases while preserving intra-image spatial information, improving model robustness.

Conclusion: The method offers a flexible and effective positional encoding framework for large vision-language models, validated by experimental results.

Abstract: Rotary Position Embedding (RoPE) is a widely adopted technique for encoding
relative positional information in large language models (LLMs). However, when
extended to large vision-language models (LVLMs), its variants introduce
unintended cross-modal positional biases. Specifically, they enforce relative
positional dependencies between text token indices and image tokens, causing
spurious alignments. This issue arises because image tokens representing the
same content but located at different spatial positions are assigned distinct
positional biases, leading to inconsistent cross-modal associations. To address
this, we propose Per-Token Distance (PTD) - a simple yet effective metric for
quantifying the independence of positional encodings across modalities.
Informed by this analysis, we introduce Circle-RoPE, a novel encoding scheme
that maps image token indices onto a circular trajectory orthogonal to the
linear path of text token indices, forming a cone-like structure. This
configuration ensures that each text token maintains an equal distance to all
image tokens, reducing artificial cross-modal biases while preserving
intra-image spatial information. To further enhance performance, we propose a
staggered layer strategy that applies different RoPE variants across layers.
This design leverages the complementary strengths of each RoPE variant, thereby
enhancing the model's overall performance. Our experimental results demonstrate
that our method effectively preserves spatial information from images while
reducing relative positional bias, offering a more robust and flexible
positional encoding framework for LVLMs. The code is available at
[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).

</details>


### [304] [Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment](https://arxiv.org/pdf/2505.16419)
*Soh Takahashi, Masaru Sasaki, Ken Takeda, Masafumi Oizumi*

Main category: cs.CV

TL;DR: The paper investigates how human-like object representations are in deep neural networks (DNNs), using unsupervised alignment to compare fine- and coarse-grained levels. CLIP models align well with human representations, while self-supervised models show limited matching but capture coarse categories.


<details>
  <summary>Details</summary>
Motivation: To understand whether DNNs' internal representations match human object representations at fine- or coarse-grained levels, and how different learning paradigms (supervised, self-supervised, CLIP) affect this.

Method: An unsupervised alignment method based on Gromov-Wasserstein Optimal Transport is used to compare human and model object representations at fine- and coarse-grained levels, leveraging human similarity judgments from the THINGS dataset.

Result: CLIP models achieve strong alignment with human representations at both fine- and coarse-grained levels, while self-supervised models show limited matching but still reflect coarse category structures.

Conclusion: Linguistic information (CLIP) aids in precise object representation, while self-supervised learning captures coarse categorical structures, offering insights into human-like representation learning.

Abstract: The learning mechanisms by which humans acquire internal representations of
objects are not fully understood. Deep neural networks (DNNs) have emerged as a
useful tool for investigating this question, as they have internal
representations similar to those of humans as a byproduct of optimizing their
objective functions. While previous studies have shown that models trained with
various learning paradigms - such as supervised, self-supervised, and CLIP -
acquire human-like representations, it remains unclear whether their similarity
to human representations is primarily at a coarse category level or extends to
finer details. Here, we employ an unsupervised alignment method based on
Gromov-Wasserstein Optimal Transport to compare human and model object
representations at both fine-grained and coarse-grained levels. The unique
feature of this method compared to conventional representational similarity
analysis is that it estimates optimal fine-grained mappings between the
representation of each object in human and model representations. We used this
unsupervised alignment method to assess the extent to which the representation
of each object in humans is correctly mapped to the corresponding
representation of the same object in models. Using human similarity judgments
of 1,854 objects from the THINGS dataset, we find that models trained with CLIP
consistently achieve strong fine- and coarse-grained matching with human object
representations. In contrast, self-supervised models showed limited matching at
both fine- and coarse-grained levels, but still formed object clusters that
reflected human coarse category structure. Our results offer new insights into
the role of linguistic information in acquiring precise object representations
and the potential of self-supervised learning to capture coarse categorical
structures.

</details>


### [305] [Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach](https://arxiv.org/pdf/2505.16422)
*Xiaoran Yin, Xu Luo, Hao Wu, Lianli Gao, Jingkuan Song*

Main category: cs.CV

TL;DR: FPWC enhances mobile device control by using a world model for foresighted planning, improving task success by 44.4% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current reactive policies in mobile device control are limited by immediate observations, leading to suboptimal decisions. FPWC aims to improve global understanding and decision-making.

Method: FPWC develops a task-oriented, refinable world model for structured reasoning and generates foresighted actions via iterative planning, executed as code.

Result: FPWC outperforms prior methods, achieving a 44.4% relative improvement in task success rate in simulated environments.

Conclusion: FPWC's foresighted planning with a world model significantly enhances mobile device control, demonstrating superior performance in experiments.

Abstract: The automatic control of mobile devices is essential for efficiently
performing complex tasks that involve multiple sequential steps. However, these
tasks pose significant challenges due to the limited environmental information
available at each step, primarily through visual observations. As a result,
current approaches, which typically rely on reactive policies, focus solely on
immediate observations and often lead to suboptimal decision-making. To address
this problem, we propose \textbf{Foresighted Planning with World Model-Driven
Code Execution (FPWC)},a framework that prioritizes natural language
understanding and structured reasoning to enhance the agent's global
understanding of the environment by developing a task-oriented, refinable
\emph{world model} at the outset of the task. Foresighted actions are
subsequently generated through iterative planning within this world model,
executed in the form of executable code. Extensive experiments conducted in
simulated environments and on real mobile devices demonstrate that our method
outperforms previous approaches, particularly achieving a 44.4\% relative
improvement in task success rate compared to the state-of-the-art in the
simulated environment. Code and demo are provided in the supplementary
material.

</details>


### [306] [Ranked Entropy Minimization for Continual Test-Time Adaptation](https://arxiv.org/pdf/2505.16441)
*Jisu Han, Jaemin Na, Wonjun Hwang*

Main category: cs.CV

TL;DR: The paper proposes ranked entropy minimization to address model collapse in continual test-time adaptation, improving stability and performance.


<details>
  <summary>Details</summary>
Motivation: Entropy minimization is efficient for test-time adaptation but suffers from instability and model collapse in continual scenarios.

Method: The approach uses a progressive masking strategy to structure prediction difficulty and preserve entropy rank order.

Result: Extensive benchmarks show the method's effectiveness in mitigating model collapse and improving adaptation.

Conclusion: Ranked entropy minimization enhances stability and extends the applicability of entropy minimization in continual test-time adaptation.

Abstract: Test-time adaptation aims to adapt to realistic environments in an online
manner by learning during test time. Entropy minimization has emerged as a
principal strategy for test-time adaptation due to its efficiency and
adaptability. Nevertheless, it remains underexplored in continual test-time
adaptation, where stability is more important. We observe that the entropy
minimization method often suffers from model collapse, where the model
converges to predicting a single class for all images due to a trivial
solution. We propose ranked entropy minimization to mitigate the stability
problem of the entropy minimization method and extend its applicability to
continuous scenarios. Our approach explicitly structures the prediction
difficulty through a progressive masking strategy. Specifically, it gradually
aligns the model's probability distributions across different levels of
prediction difficulty while preserving the rank order of entropy. The proposed
method is extensively evaluated across various benchmarks, demonstrating its
effectiveness through empirical results. Our code is available at
https://github.com/pilsHan/rem

</details>


### [307] [MAFE R-CNN: Selecting More Samples to Learn Category-aware Features for Small Object Detection](https://arxiv.org/pdf/2505.16442)
*Yichen Li, Qiankun Liu, Zhenchao Jin, Jiuzhe Wei, Jing Nie, Ying Fu*

Main category: cs.CV

TL;DR: MAFE R-CNN improves small object detection by introducing Multi-Clue Sample Selection and Category-aware Feature Enhancement Mechanism, validated on the SODA dataset.


<details>
  <summary>Details</summary>
Motivation: The challenge of small object detection arises from ineffective feature learning and poor sample selection during training.

Method: Integrates Multi-Clue Sample Selection (MCSS) using IoU, confidence, and size, and Category-aware Feature Enhancement Mechanism (CFEM) for feature interaction.

Result: Demonstrated effectiveness on the SODA dataset.

Conclusion: MAFE R-CNN addresses small object detection challenges with innovative sample selection and feature enhancement.

Abstract: Small object detection in intricate environments has consistently represented
a major challenge in the field of object detection. In this paper, we identify
that this difficulty stems from the detectors' inability to effectively learn
discriminative features for objects of small size, compounded by the complexity
of selecting high-quality small object samples during training, which motivates
the proposal of the Multi-Clue Assignment and Feature Enhancement
R-CNN.Specifically, MAFE R-CNN integrates two pivotal components.The first is
the Multi-Clue Sample Selection (MCSS) strategy, in which the Intersection over
Union (IoU) distance, predicted category confidence, and ground truth region
sizes are leveraged as informative clues in the sample selection process. This
methodology facilitates the selection of diverse positive samples and ensures a
balanced distribution of object sizes during training, thereby promoting
effective model learning.The second is the Category-aware Feature Enhancement
Mechanism (CFEM), where we propose a simple yet effective category-aware memory
module to explore the relationships among object features. Subsequently, we
enhance the object feature representation by facilitating the interaction
between category-aware features and candidate box features.Comprehensive
experiments conducted on the large-scale small object dataset SODA validate the
effectiveness of the proposed method. The code will be made publicly available.

</details>


### [308] [TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition](https://arxiv.org/pdf/2505.16447)
*Oliver Grainge, Michael Milford, Indu Bodala, Sarvapali D. Ramchurn, Shoaib Ehsan*

Main category: cs.CV

TL;DR: TAT-VPR introduces a ternary-quantized transformer for dynamic accuracy-efficiency trade-offs in visual SLAM loop-closure, achieving up to 40% computation reduction without performance loss.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and adaptable visual SLAM loop-closure for resource-constrained platforms like micro-UAVs and embedded systems.

Method: Fuses ternary weights with a learned activation-sparsity gate and employs a two-stage distillation pipeline to preserve descriptor quality.

Result: Achieves up to 40% computation reduction at runtime without degrading Recall@1 performance, matching state-of-the-art localization accuracy.

Conclusion: TAT-VPR successfully balances efficiency and accuracy, making it suitable for resource-limited applications while maintaining high performance.

Abstract: TAT-VPR is a ternary-quantized transformer that brings dynamic
accuracy-efficiency trade-offs to visual SLAM loop-closure. By fusing ternary
weights with a learned activation-sparsity gate, the model can control
computation by up to 40% at run-time without degrading performance (Recall@1).
The proposed two-stage distillation pipeline preserves descriptor quality,
letting it run on micro-UAV and embedded SLAM stacks while matching
state-of-the-art localization accuracy.

</details>


### [309] [CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI](https://arxiv.org/pdf/2505.16452)
*Mohamed S. Elmahdy, Marius Staring, Patrick J. H. de Koning, Samer Alabed, Mahan Salehi, Faisal Alandejani, Michael Sharkey, Ziad Aldabbagh, Andrew J. Swift, Rob J. van der Geest*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end deep learning model for joint groupwise registration and segmentation in cardiac cine-MRI to improve cardiac function assessment.


<details>
  <summary>Details</summary>
Motivation: LVEF has limitations in reproducibility and sensitivity, and existing methods for cardiac function assessment are often performed separately, limiting accuracy.

Method: An anatomically-guided Deep GW network is introduced for joint estimation of groupwise registration and segmentation in cardiac cine-MRI.

Result: The model outperformed conventional GW registration and DL-based methods, improving performance and reducing computation time.

Conclusion: The proposed model offers a more efficient and accurate approach for comprehensive cardiac function assessment.

Abstract: Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.

</details>


### [310] [MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM](https://arxiv.org/pdf/2505.16456)
*Siwei Meng, Yawei Luo, Ping Liu*

Main category: cs.CV

TL;DR: MAGIC is a training-free framework for generating physically consistent dynamic 3D content from static images, combining pretrained diffusion models with LLM-based reasoning and a differentiable simulator.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models lack physical plausibility, and prior physics-aware methods require extensive datasets or tuning, limiting scalability.

Method: Integrates pretrained image-to-video diffusion models with iterative LLM-based reasoning and a differentiable MPM simulator for physics-relevant motion.

Result: MAGIC outperforms physics-aware generative methods in accuracy and achieves better temporal coherence than top video diffusion models.

Conclusion: MAGIC bridges the visual-to-physical gap efficiently, enabling physics-consistent dynamic generation without training or supervision.

Abstract: Recent advances in static 3D generation have intensified the demand for
physically consistent dynamic 3D content. However, existing video generation
models, including diffusion-based methods, often prioritize visual realism
while neglecting physical plausibility, resulting in implausible object
dynamics. Prior approaches for physics-aware dynamic generation typically rely
on large-scale annotated datasets or extensive model fine-tuning, which imposes
significant computational and data collection burdens and limits scalability
across scenarios. To address these challenges, we present MAGIC, a
training-free framework for single-image physical property inference and
dynamic generation, integrating pretrained image-to-video diffusion models with
iterative LLM-based reasoning. Our framework generates motion-rich videos from
a static image and closes the visual-to-physical gap through a
confidence-driven LLM feedback loop that adaptively steers the diffusion model
toward physics-relevant motion. To translate visual dynamics into controllable
physical behavior, we further introduce a differentiable MPM simulator
operating directly on 3D Gaussians reconstructed from the single image,
enabling physically grounded, simulation-ready outputs without any supervision
or model tuning. Experiments show that MAGIC outperforms existing physics-aware
generative methods in inference accuracy and achieves greater temporal
coherence than state-of-the-art video diffusion models.

</details>


### [311] [AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer](https://arxiv.org/pdf/2505.16463)
*Jiquan Shan, Junxiao Wang, Lifeng Zhao, Liang Cai, Hongyuan Zhang, Ioannis Liritzis*

Main category: cs.CV

TL;DR: AnchorFormer reduces ViT complexity from O(n²) to O(mn) using anchor tokens, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: ViTs have high computational cost (O(n²)) and gather pivotal information randomly. AnchorFormer addresses this by focusing on key regions.

Method: Uses anchor tokens to learn pivotal information, reducing complexity via bipartite attention and differentiable anchor learning.

Result: Achieves 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet, 81.3% higher mAP on COCO detection.

Conclusion: AnchorFormer efficiently approximates global self-attention, enhancing performance in vision tasks.

Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.

</details>


### [312] [Consistent World Models via Foresight Diffusion](https://arxiv.org/pdf/2505.16474)
*Yu Zhang, Xingzhuo Guo, Haoran Xu, Mingsheng Long*

Main category: cs.CV

TL;DR: ForeDiff improves diffusion-based world models by decoupling condition understanding from target denoising, enhancing consistency and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with consistent sample alignment in world modeling due to entangled condition understanding and denoising.

Method: Proposes ForeDiff, a framework with separate streams for condition understanding and denoising, leveraging a pretrained predictor.

Result: ForeDiff outperforms baselines in predictive accuracy and sample consistency in robot video prediction and spatiotemporal forecasting.

Conclusion: ForeDiff offers a promising direction for consistent diffusion-based world models.

Abstract: Diffusion and flow-based models have enabled significant progress in
generation tasks across various modalities and have recently found applications
in world modeling. However, unlike typical generation tasks that encourage
sample diversity, world models entail different sources of uncertainty and
require consistent samples aligned with the ground-truth trajectory, which is a
limitation we empirically observe in diffusion models. We argue that a key
bottleneck in learning consistent diffusion-based world models lies in the
suboptimal predictive ability, which we attribute to the entanglement of
condition understanding and target denoising within shared architectures and
co-training schemes. To address this, we propose Foresight Diffusion
(ForeDiff), a diffusion-based world modeling framework that enhances
consistency by decoupling condition understanding from target denoising.
ForeDiff incorporates a separate deterministic predictive stream to process
conditioning inputs independently of the denoising stream, and further
leverages a pretrained predictor to extract informative representations that
guide generation. Extensive experiments on robot video prediction and
scientific spatiotemporal forecasting show that ForeDiff improves both
predictive accuracy and sample consistency over strong baselines, offering a
promising direction for diffusion-based world models.

</details>


### [313] [Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration](https://arxiv.org/pdf/2505.16479)
*Yuetong Liu, Yunqiu Xu, Yang Wei, Xiuli Bi, Bin Xiao*

Main category: cs.CV

TL;DR: The paper introduces ClearNight, a framework for restoring nighttime images affected by multiple weather conditions, and the AllWeatherNight dataset for research support.


<details>
  <summary>Details</summary>
Motivation: Nighttime image restoration is challenging due to coexisting weather degradations and lighting effects, which are under-explored.

Method: ClearNight uses Retinex-based dual priors and a weather-aware dynamic method to address degradations.

Result: ClearNight achieves state-of-the-art performance on synthetic and real-world images.

Conclusion: The AllWeatherNight dataset and ClearNight framework are validated as effective for nighttime image restoration.

Abstract: Restoring nighttime images affected by multiple adverse weather conditions is
a practical yet under-explored research problem, as multiple weather conditions
often coexist in the real world alongside various lighting effects at night.
This paper first explores the challenging multi-weather nighttime image
restoration task, where various types of weather degradations are intertwined
with flare effects. To support the research, we contribute the AllWeatherNight
dataset, featuring large-scale high-quality nighttime images with diverse
compositional degradations, synthesized using our introduced illumination-aware
degradation generation. Moreover, we present ClearNight, a unified nighttime
image restoration framework, which effectively removes complex degradations in
one go. Specifically, ClearNight extracts Retinex-based dual priors and
explicitly guides the network to focus on uneven illumination regions and
intrinsic texture contents respectively, thereby enhancing restoration
effectiveness in nighttime scenarios. In order to better represent the common
and unique characters of multiple weather degradations, we introduce a
weather-aware dynamic specific-commonality collaboration method, which
identifies weather degradations and adaptively selects optimal candidate units
associated with specific weather types. Our ClearNight achieves
state-of-the-art performance on both synthetic and real-world images.
Comprehensive ablation experiments validate the necessity of AllWeatherNight
dataset as well as the effectiveness of ClearNight. Project page:
https://henlyta.github.io/ClearNight/mainpage.html

</details>


### [314] [InspectionV3: Enhancing Tobacco Quality Assessment with Deep Convolutional Neural Networks for Automated Workshop Management](https://arxiv.org/pdf/2505.16485)
*Yao Wei, Muhammad Usman, Hazrat Bilal*

Main category: cs.CV

TL;DR: InspectionV3, a customized deep CNN, automates tobacco grading with high accuracy, addressing issues like poor curing and inconsistent supplies.


<details>
  <summary>Details</summary>
Motivation: Tobacco workshops face challenges like high costs, poor quality, and unreliable manual inspection, necessitating an automated solution.

Method: Uses a labeled dataset of 21,113 images across 20 quality classes, processed by experts, and a multi-layer CNN with batch normalization for grading.

Result: Achieves 97% accuracy, 95% precision/recall, 96% F1-score/AUC, and 95% specificity, validating real-world applicability.

Conclusion: InspectionV3 effectively automates tobacco grading, improving workflow and decision-making with high accuracy and adaptability.

Abstract: The problems that tobacco workshops encounter include poor curing,
inconsistencies in supplies, irregular scheduling, and a lack of oversight, all
of which drive up expenses and worse quality. Large quantities make manual
examination costly, sluggish, and unreliable. Deep convolutional neural
networks have recently made strides in capabilities that transcend those of
conventional methods. To effectively enhance them, nevertheless, extensive
customization is needed to account for subtle variations in tobacco grade. This
study introduces InspectionV3, an integrated solution for automated flue-cured
tobacco grading that makes use of a customized deep convolutional neural
network architecture. A scope that covers color, maturity, and curing
subtleties is established via a labelled dataset consisting of 21,113 images
spanning 20 quality classes. Expert annotators performed preprocessing on the
tobacco leaf images, including cleaning, labelling, and augmentation.
Multi-layer CNN factors use batch normalization to describe domain properties
like as permeability and moisture spots, and so account for the subtleties of
the workshop. Its expertise lies in converting visual patterns into useful
information for enhancing workflow. Fast notifications are made possible by
real-time, on-the-spot grading that matches human expertise. Images-powered
analytics dashboards facilitate the tracking of yield projections, inventories,
bottlenecks, and the optimization of data-driven choices. More labelled images
are assimilated after further retraining, improving representational capacities
and enabling adaptations for seasonal variability. Metrics demonstrate 97%
accuracy, 95% precision and recall, 96% F1-score and AUC, 95% specificity;
validating real-world viability.

</details>


### [315] [ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation](https://arxiv.org/pdf/2505.16495)
*Lingfeng Wang, Hualing Lin, Senda Chen, Tao Wang, Changxu Cheng, Yangyang Zhong, Dong Zheng, Wuyue Zhao*

Main category: cs.CV

TL;DR: ALTo introduces an adaptive tokenizer for MLLMs, improving mask generation by dynamically adjusting token lengths, achieving top performance in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs use rigid token representations, unlike humans who adapt attention based on complexity. ALTo bridges this gap.

Method: ALTo includes a token length predictor, length regularization, and differentiable token chunking. Integrated into ALToLLM with GRPO for efficiency trade-offs.

Result: ALToLLM achieves state-of-the-art performance on segmentation benchmarks with adaptive token cost.

Conclusion: ALTo enhances MLLMs by adaptive tokenization, balancing quality and efficiency, with code and models publicly available.

Abstract: While humans effortlessly draw visual objects and shapes by adaptively
allocating attention based on their complexity, existing multimodal large
language models (MLLMs) remain constrained by rigid token representations.
Bridging this gap, we propose ALTo, an adaptive length tokenizer for
autoregressive mask generation. To achieve this, a novel token length predictor
is designed, along with a length regularization term and a differentiable token
chunking strategy. We further build ALToLLM that seamlessly integrates ALTo
into MLLM. Preferences on the trade-offs between mask quality and efficiency is
implemented by group relative policy optimization (GRPO). Experiments
demonstrate that ALToLLM achieves state-of-the-art performance with adaptive
token cost on popular segmentation benchmarks. Code and models are released at
https://github.com/yayafengzi/ALToLLM.

</details>


### [316] [Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection](https://arxiv.org/pdf/2505.16512)
*Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Zhaofeng He*

Main category: cs.CV

TL;DR: The paper introduces DigiFakeAV, a large-scale dataset for detecting deepfake videos generated by diffusion models, and proposes DigiShield, a detection method that outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: The rise of diffusion model-based digital human generation poses severe challenges to detection methods due to its realism and flexibility, necessitating a robust dataset and detection framework.

Method: The authors create DigiFakeAV using five digital human generation methods and voice cloning, then propose DigiShield, a detection baseline leveraging spatiotemporal and cross-modal fusion.

Result: User studies show a 68% confusion rate between real and forged videos, and DigiShield achieves state-of-the-art performance on DigiFakeAV and DF-TIMIT datasets.

Conclusion: DigiShield effectively identifies synthetic videos by analyzing temporal facial features, addressing the limitations of current detection methods.

Abstract: In recent years, the rapid development of deepfake technology has given rise
to an emerging and serious threat to public security: diffusion model-based
digital human generation. Unlike traditional face manipulation methods, such
models can generate highly realistic videos with consistency through multimodal
control signals. Their flexibility and covertness pose severe challenges to
existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the
first large-scale multimodal digital human forgery dataset based on diffusion
models. Employing five latest digital human generation methods (Sonic, Hallo,
etc.) and voice cloning method, we systematically produce a dataset comprising
60,000 videos (8.4 million frames), covering multiple nationalities, skin
tones, genders, and real-world scenarios, significantly enhancing data
diversity and realism. User studies show that the confusion rate between forged
and real videos reaches 68%, and existing state-of-the-art (SOTA) detection
models exhibit large drops in AUC values on DigiFakeAV, highlighting the
challenge of the dataset. To address this problem, we further propose
DigiShield, a detection baseline based on spatiotemporal and cross-modal
fusion. By jointly modeling the 3D spatiotemporal features of videos and the
semantic-acoustic features of audio, DigiShield achieves SOTA performance on
both the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method
effectively identifies covert artifacts through fine-grained analysis of the
temporal evolution of facial features in synthetic videos.

</details>


### [317] [Detailed Evaluation of Modern Machine Learning Approaches for Optic Plastics Sorting](https://arxiv.org/pdf/2505.16513)
*Vaishali Maheshkar, Aadarsh Anantha Ramakrishnan, Charuvahan Adhivarahan, Karthik Dantu*

Main category: cs.CV

TL;DR: The paper examines the limitations of optical recognition methods for plastic sorting in recycling facilities, using machine learning and novel datasets.


<details>
  <summary>Details</summary>
Motivation: Low plastic recycling rates due to contamination, poor incentives, and technical challenges highlight the need for better automated sorting solutions.

Method: The study compiled 20,000+ images and used machine learning pipelines (including Mask R-CNN and YOLO) to evaluate optical sorting. Tools like Grad-CAM and confusion matrices analyzed model behavior.

Result: Optical recognition methods show limited success in accurately sorting real-world plastics due to reliance on physical properties like color and shape.

Conclusion: Current optic detection methods are insufficient for practical plastic sorting, suggesting a need for improved approaches.

Abstract: According to the EPA, only 25% of waste is recycled, and just 60% of U.S.
municipalities offer curbside recycling. Plastics fare worse, with a recycling
rate of only 8%; an additional 16% is incinerated, while the remaining 76% ends
up in landfills. The low plastic recycling rate stems from contamination, poor
economic incentives, and technical difficulties, making efficient recycling a
challenge. To improve recovery, automated sorting plays a critical role.
Companies like AMP Robotics and Greyparrot utilize optical systems for sorting,
while Materials Recovery Facilities (MRFs) employ Near-Infrared (NIR) sensors
to detect plastic types.
  Modern optical sorting uses advances in computer vision such as object
recognition and instance segmentation, powered by machine learning. Two-stage
detectors like Mask R-CNN use region proposals and classification with deep
backbones like ResNet. Single-stage detectors like YOLO handle detection in one
pass, trading some accuracy for speed. While such methods excel under ideal
conditions with a large volume of labeled training data, challenges arise in
realistic scenarios, emphasizing the need to further examine the efficacy of
optic detection for automated sorting.
  In this study, we compiled novel datasets totaling 20,000+ images from varied
sources. Using both public and custom machine learning pipelines, we assessed
the capabilities and limitations of optical recognition for sorting. Grad-CAM,
saliency maps, and confusion matrices were employed to interpret model
behavior. We perform this analysis on our custom trained models from the
compiled datasets. To conclude, our findings are that optic recognition methods
have limited success in accurate sorting of real-world plastics at MRFs,
primarily because they rely on physical properties such as color and shape.

</details>


### [318] [CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving](https://arxiv.org/pdf/2505.16524)
*Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, Yadan Luo*

Main category: cs.CV

TL;DR: CodeMerge is a lightweight model merging framework for 3D object detection, improving stability and efficiency by operating in a latent space with low-dimensional fingerprints.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation methods struggle with instability and computational cost in dynamic 3D perception tasks.

Method: CodeMerge uses a compact latent space with low-dimensional fingerprints and a key-value codebook, computing merging coefficients via ridge leverage scores.

Result: Achieves 14.9% NDS improvement on nuScenes-C and 7.6% mAP on nuScenes-to-KITTI, benefiting downstream tasks without training.

Conclusion: CodeMerge offers a scalable and efficient solution for robust 3D perception, outperforming existing methods.

Abstract: Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.

</details>


### [319] [Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction](https://arxiv.org/pdf/2505.16533)
*Jiacong Chen, Qingyu Mao, Youneng Bao, Xiandong Meng, Fanyang Meng, Ronggang Wang, Yongsheng Liang*

Main category: cs.CV

TL;DR: ComGS is a storage-efficient framework for dynamic scene reconstruction, reducing storage by 159X vs 3DGStream and 14X vs QUEEN, while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing online methods for FVV reconstruction suffer from high storage due to point-wise modeling, ignoring motion properties.

Method: ComGS uses keypoint-driven motion representation, adaptive motion propagation, and error-aware correction for efficient storage.

Result: Achieves 159X and 14X storage reduction over 3DGStream and QUEEN, respectively, with competitive visual fidelity.

Conclusion: ComGS provides a scalable and efficient solution for dynamic scene reconstruction with minimal storage overhead.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient
paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers
rapid responsiveness and immersive experiences. However, existing online
methods face challenge in prohibitive storage requirements primarily due to
point-wise modeling that fails to exploit the motion properties. To address
this limitation, we propose a novel Compact Gaussian Streaming (ComGS)
framework, leveraging the locality and consistency of motion in dynamic scene,
that models object-consistent Gaussian point motion through keypoint-driven
motion representation. By transmitting only the keypoint attributes, this
framework provides a more storage-efficient solution. Specifically, we first
identify a sparse set of motion-sensitive keypoints localized within motion
regions using a viewspace gradient difference strategy. Equipped with these
keypoints, we propose an adaptive motion-driven mechanism that predicts a
spatial influence field for propagating keypoint motion to neighboring Gaussian
points with similar motion. Moreover, ComGS adopts an error-aware correction
strategy for key frame reconstruction that selectively refines erroneous
regions and mitigates error accumulation without unnecessary overhead. Overall,
ComGS achieves a remarkable storage reduction of over 159 X compared to
3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining
competitive visual fidelity and rendering speed. Our code will be released.

</details>


### [320] [SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion](https://arxiv.org/pdf/2505.16535)
*Asrar Alruwayqi*

Main category: cs.CV

TL;DR: A novel framework for dynamic 3D scene reconstruction integrates tri-plane deformation, SH-based rendering, and latent diffusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve dynamic 3D scene reconstruction by combining efficient spatiotemporal representation with interpretable rendering and robust motion handling.

Method: Uses tri-plane deformation fields, SH-based rendering, and a latent diffusion prior for refining features. Trained in two stages: pre-training the diffusion module and joint fine-tuning.

Result: Surpasses HexPlane and 4D Gaussian Splatting in visual quality, temporal coherence, and robustness to sparse-view inputs.

Conclusion: The framework offers a compact, efficient, and high-fidelity solution for dynamic 3D scene reconstruction.

Abstract: We present a novel framework for dynamic 3D scene reconstruction that
integrates three key components: an explicit tri-plane deformation field, a
view-conditioned canonical radiance field with spherical harmonics (SH)
attention, and a temporally-aware latent diffusion prior. Our method encodes 4D
scenes using three orthogonal 2D feature planes that evolve over time, enabling
efficient and compact spatiotemporal representation. These features are
explicitly warped into a canonical space via a deformation offset field,
eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured
SH-based rendering head that synthesizes view-dependent color via attention
over learned frequency bands improving both interpretability and rendering
efficiency. To further enhance fidelity and temporal consistency, we introduce
a transformer-guided latent diffusion module that refines the tri-plane and
deformation features in a compressed latent space. This generative module
denoises scene representations under ambiguous or out-of-distribution (OOD)
motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained
independently, and then fine-tuned jointly with the full pipeline using a
combination of image reconstruction, diffusion denoising, and temporal
consistency losses. We demonstrate state-of-the-art results on synthetic
benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian
Splatting in visual quality, temporal coherence, and robustness to sparse-view
dynamic inputs.

</details>


### [321] [TextureSAM: Towards a Texture Aware Foundation Model for Segmentation](https://arxiv.org/pdf/2505.16540)
*Inbal Cohen, Boaz Meivar, Peihan Tu, Shai Avidan, Gal Oren*

Main category: cs.CV

TL;DR: TextureSAM addresses SAM's bias toward shape over texture in segmentation by introducing texture-aware fine-tuning, outperforming SAM-2 in texture-dominant tasks.


<details>
  <summary>Details</summary>
Motivation: SAM's reliance on shape cues limits performance in domains like medical imaging and material classification, where texture defines boundaries.

Method: TextureSAM uses texture augmentation and fine-tuning on a modified ADE20K dataset to prioritize texture features.

Result: TextureSAM outperforms SAM-2 by +0.2 mIoU on natural and +0.18 mIoU on synthetic texture datasets.

Conclusion: TextureSAM effectively mitigates SAM's shape bias, offering superior segmentation in texture-dominant scenarios.

Abstract: Segment Anything Models (SAM) have achieved remarkable success in object
segmentation tasks across diverse datasets. However, these models are
predominantly trained on large-scale semantic segmentation datasets, which
introduce a bias toward object shape rather than texture cues in the image.
This limitation is critical in domains such as medical imaging, material
classification, and remote sensing, where texture changes define object
boundaries. In this study, we investigate SAM's bias toward semantics over
textures and introduce a new texture-aware foundation model, TextureSAM, which
performs superior segmentation in texture-dominant scenarios. To achieve this,
we employ a novel fine-tuning approach that incorporates texture augmentation
techniques, incrementally modifying training images to emphasize texture
features. By leveraging a novel texture-alternation of the ADE20K dataset, we
guide TextureSAM to prioritize texture-defined regions, thereby mitigating the
inherent shape bias present in the original SAM model. Our extensive
experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both
natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation
datasets. The code and texture-augmented dataset will be publicly available.

</details>


### [322] [Auto-nnU-Net: Towards Automated Medical Image Segmentation](https://arxiv.org/pdf/2505.16561)
*Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer*

Main category: cs.CV

TL;DR: Auto-nnU-Net enhances nnU-Net by adding hyperparameter optimization, neural architecture search, and hierarchical NAS, improving segmentation performance while managing computational resources.


<details>
  <summary>Details</summary>
Motivation: Address limitations of nnU-Net's fixed hyperparameters and heuristic design choices to improve medical image segmentation performance and resource efficiency.

Method: Propose Auto-nnU-Net with HPO, NAS, and HNAS, and introduce Regularized PriorBand for resource-efficient training.

Result: Substantially improves segmentation on 6/10 datasets, matches performance on others, and maintains practical resource use.

Conclusion: Auto-nnU-Net advances MIS by automating model configuration and balancing accuracy with computational constraints.

Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ
segmentation, each with its own challenges in finding the best segmentation
model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many
aspects of model configuration but remains constrained by fixed hyperparameters
and heuristic design choices. As a full-AutoML framework for MIS, we propose
Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization
(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).
Additionally, we propose Regularized PriorBand to balance model accuracy with
the computational resources required for training, addressing the resource
constraints often faced in real-world medical settings that limit the
feasibility of extensive training procedures. We evaluate our approach across
diverse MIS datasets from the well-established Medical Segmentation Decathlon,
analyzing the impact of AutoML techniques on segmentation performance,
computational efficiency, and model design choices. The results demonstrate
that our AutoML approach substantially improves the segmentation performance of
nnU-Net on 6 out of 10 datasets and is on par on the other datasets while
maintaining practical resource requirements. Our code is available at
https://github.com/LUH-AI/AutonnUNet.

</details>


### [323] [M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion](https://arxiv.org/pdf/2505.16565)
*Nina Shvetsova, Goutam Bhat, Prune Truong, Hilde Kuehne, Federico Tombari*

Main category: cs.CV

TL;DR: A novel architecture for monocular-to-stereo video conversion using Stable Video Diffusion (SVD) with improved attention layers for inpainting, achieving high-quality results and faster performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating high-quality stereo video from monocular input by refining the warped right view and handling disocclusions effectively.

Method: Extends SVD to use left video, warped right video, and disocclusion masks as inputs, modifies attention layers for better inpainting, and trains end-to-end with image space losses.

Result: Outperforms state-of-the-art methods, ranking 1.43/4 in a user study and being 6x faster than the second-best method.

Conclusion: The proposed method effectively converts monocular to stereo video with superior quality and efficiency.

Abstract: We tackle the problem of monocular-to-stereo video conversion and propose a
novel architecture for inpainting and refinement of the warped right view
obtained by depth-based reprojection of the input left view. We extend the
Stable Video Diffusion (SVD) model to utilize the input left video, the warped
right video, and the disocclusion masks as conditioning input to generate a
high-quality right camera view. In order to effectively exploit information
from neighboring frames for inpainting, we modify the attention layers in SVD
to compute full attention for discoccluded pixels. Our model is trained to
generate the right view video in an end-to-end manner by minimizing image space
losses to ensure high-quality generation. Our approach outperforms previous
state-of-the-art methods, obtaining an average rank of 1.43 among the 4
compared methods in a user study, while being 6x faster than the second placed
method.

</details>


### [324] [Temporal Object Captioning for Street Scene Videos from LiDAR Tracks](https://arxiv.org/pdf/2505.16594)
*Vignesh Gopinathan, Urs Zimmermann, Michael Arnold, Matthias Rottmann*

Main category: cs.CV

TL;DR: Proposed LiDAR-based captioning improves temporal understanding in video captioning models, reducing visual/static biases.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding how models capture temporal semantics, especially for Advanced Driver Assistance Systems.

Method: Automated LiDAR-based captioning with rule-based extraction of lane position and motion, followed by template-based caption generation.

Result: Training SwinBERT with template-based captions enhances temporal understanding across three datasets.

Conclusion: LiDAR-based caption supervision significantly improves temporal understanding and reduces biases in models.

Abstract: Video captioning models have seen notable advancements in recent years,
especially with regard to their ability to capture temporal information. While
many research efforts have focused on architectural advancements, such as
temporal attention mechanisms, there remains a notable gap in understanding how
models capture and utilize temporal semantics for effective temporal feature
extraction, especially in the context of Advanced Driver Assistance Systems. We
propose an automated LiDAR-based captioning procedure that focuses on the
temporal dynamics of traffic participants. Our approach uses a rule-based
system to extract essential details such as lane position and relative motion
from object tracks, followed by a template-based caption generation. Our
findings show that training SwinBERT, a video captioning model, using only
front camera images and supervised with our template-based captions,
specifically designed to encapsulate fine-grained temporal behavior, leads to
improved temporal understanding consistently across three datasets. In
conclusion, our results clearly demonstrate that integrating LiDAR-based
caption supervision significantly enhances temporal understanding, effectively
addressing and reducing the inherent visual/static biases prevalent in current
state-of-the-art model architectures.

</details>


### [325] [Decoupled Geometric Parameterization and its Application in Deep Homography Estimation](https://arxiv.org/pdf/2505.16599)
*Yao Huang, Si-Yuan Cao, Yaqing Ding, Hao Yin, Shibin Xie, Shuting Wang, Zhijun Fang, Jiachun Wang, Shen Cai, Junchi Yan, Shuhan Shen*

Main category: cs.CV

TL;DR: A novel geometric parameterization for planar homography using SKS decomposition, enabling direct homography estimation without solving linear systems.


<details>
  <summary>Details</summary>
Motivation: Traditional homography parameterization lacks geometric interpretability and requires solving linear systems, prompting a need for a more intuitive and efficient method.

Method: Proposes a similarity-kernel-similarity (SKS) decomposition to decouple geometric parameters into similarity and kernel transformations, with derived linear relations for angular offsets.

Result: The method achieves comparable performance to traditional four-corner positional offsets in deep homography estimation while simplifying computation.

Conclusion: The SKS-based parameterization offers a geometrically interpretable and computationally efficient alternative for homography estimation.

Abstract: Planar homography, with eight degrees of freedom (DOFs), is fundamental in
numerous computer vision tasks. While the positional offsets of four corners
are widely adopted (especially in neural network predictions), this
parameterization lacks geometric interpretability and typically requires
solving a linear system to compute the homography matrix. This paper presents a
novel geometric parameterization of homographies, leveraging the
similarity-kernel-similarity (SKS) decomposition for projective
transformations. Two independent sets of four geometric parameters are
decoupled: one for a similarity transformation and the other for the kernel
transformation. Additionally, the geometric interpretation linearly relating
the four kernel transformation parameters to angular offsets is derived. Our
proposed parameterization allows for direct homography estimation through
matrix multiplication, eliminating the need for solving a linear system, and
achieves performance comparable to the four-corner positional offsets in deep
homography estimation.

</details>


### [326] [MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation](https://arxiv.org/pdf/2505.16602)
*Bohan Zhou, Yi Zhan, Zhongbin Zhang, Zongqing Lu*

Main category: cs.CV

TL;DR: MEgoHand is a multimodal framework for generating realistic hand-object interactions from egocentric RGB, text, and initial hand pose, addressing challenges like unstable viewpoints and noisy ego-motion. It uses a bi-level architecture and a curated dataset to achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating egocentric hand-object motion due to unstable viewpoints, self-occlusions, and noisy ego-motion, along with limitations of existing methods relying on predefined 3D priors or ambiguous textual cues.

Method: MEgoHand employs a bi-level architecture: a high-level 'cerebrum' with a vision language model and monocular depth estimator, and a low-level DiT-based flow-matching policy. It also introduces a dataset curation paradigm for consistency.

Result: MEgoHand reduces wrist translation error by 86.9% and joint rotation error by 34.1%, demonstrating robust generalization across diverse scenarios.

Conclusion: MEgoHand effectively addresses the challenges of egocentric hand-object motion generation, offering a scalable and generalizable solution for AR/VR and robotic applications.

Abstract: Egocentric hand-object motion generation is crucial for immersive AR/VR and
robotic imitation but remains challenging due to unstable viewpoints,
self-occlusions, perspective distortion, and noisy ego-motion. Existing methods
rely on predefined 3D object priors, limiting generalization to novel objects,
which restricts their generalizability to novel objects. Meanwhile, recent
multimodal approaches suffer from ambiguous generation from abstract textual
cues, intricate pipelines for modeling 3D hand-object correlation, and
compounding errors in open-loop prediction. We propose MEgoHand, a multimodal
framework that synthesizes physically plausible hand-object interactions from
egocentric RGB, text, and initial hand pose. MEgoHand introduces a bi-level
architecture: a high-level "cerebrum" leverages a vision language model (VLM)
to infer motion priors from visual-textual context and a monocular depth
estimator for object-agnostic spatial reasoning, while a low-level DiT-based
flow-matching policy generates fine-grained trajectories with temporal
orthogonal filtering to enhance stability. To address dataset inconsistency, we
design a dataset curation paradigm with an Inverse MANO Retargeting Network and
Virtual RGB-D Renderer, curating a unified dataset of 3.35M RGB-D frames, 24K
interactions, and 1.2K objects. Extensive experiments across five in-domain and
two cross-domain datasets demonstrate the effectiveness of MEgoHand, achieving
substantial reductions in wrist translation error (86.9%) and joint rotation
error (34.1%), highlighting its capacity to accurately model fine-grained hand
joint structures and generalize robustly across diverse scenarios.

</details>


### [327] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/pdf/2505.16624)
*Francesco Dalla Serra, Patrick Schrempf, Chaoyang Wang, Zaiqiao Meng, Fani Deligianni, Alison Q. O'Neil*

Main category: cs.CV

TL;DR: A novel approach for Chest X-ray Visual Question Answering (VQA) integrates radiology reports to improve performance on single-image and image-difference questions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To enhance VQA models for medical imaging by leveraging radiology reports as additional input, addressing both single-image and image-difference questions.

Method: A unified method processes single or paired CXRs and uses auto-regressive answer generation. It incorporates predicted radiology reports in a two-step process: Report Generation (RG) and Answer Generation (AG).

Result: Incorporating predicted radiology reports improves performance on both question types, achieving state-of-the-art results on the Medical-Diff-VQA dataset.

Conclusion: The integration of radiology reports significantly enhances VQA model performance, demonstrating their utility beyond pre-training.

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [328] [Background Matters: A Cross-view Bidirectional Modeling Framework for Semi-supervised Medical Image Segmentation](https://arxiv.org/pdf/2505.16625)
*Luyang Cao, Jianwei Li, Yinghuan Shi*

Main category: cs.CV

TL;DR: CVBM improves semi-supervised medical image segmentation by incorporating background modeling, enhancing foreground confidence, and achieving SOTA results with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Current SSMIS methods focus on foreground modeling, neglecting background modeling's potential benefits. This study shows background modeling can improve foreground confidence.

Method: Proposes Cross-view Bidirectional Modeling (CVBM), integrating background modeling as an auxiliary perspective and introducing a bidirectional consistency mechanism.

Result: Achieves SOTA performance on LA, Pancreas, ACDC, and HRF datasets, outperforming fully supervised methods on Pancreas with 20% labeled data (DSC: 84.57% vs. 83.89%).

Conclusion: CVBM demonstrates the value of background modeling in SSMIS, offering a novel framework for improved segmentation with limited annotations.

Abstract: Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data
to reduce reliance on manually annotated images. However, current SOTA
approaches predominantly focus on foreground-oriented modeling (i.e.,
segmenting only the foreground region) and have largely overlooked the
potential benefits of explicitly modeling the background region. Our study
theoretically and empirically demonstrates that highly certain predictions in
background modeling enhance the confidence of corresponding foreground
modeling. Building on this insight, we propose the Cross-view Bidirectional
Modeling (CVBM) framework, which introduces a novel perspective by
incorporating background modeling to improve foreground modeling performance.
Within CVBM, background modeling serves as an auxiliary perspective, providing
complementary supervisory signals to enhance the confidence of the foreground
model. Additionally, CVBM introduces an innovative bidirectional consistency
mechanism, which ensures mutual alignment between foreground predictions and
background-guided predictions. Extensive experiments demonstrate that our
approach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets.
Notably, on the Pancreas dataset, CVBM outperforms fully supervised methods
(i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data.
Our code is publicly available at https://github.com/caoluyang0830/CVBM.git.

</details>


### [329] [SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding](https://arxiv.org/pdf/2505.16630)
*Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen, Mubarak Shah*

Main category: cs.CV

TL;DR: SoccerChat is a multimodal AI framework for soccer video analysis, integrating visual and textual data to improve game understanding and referee decisions.


<details>
  <summary>Details</summary>
Motivation: Traditional soccer analytics methods lack context due to isolated data streams. SoccerChat aims to enhance comprehension by combining multimodal data.

Method: SoccerChat uses the SoccerNet dataset with jersey color annotations and ASR transcripts, fine-tuned on a structured video instruction dataset for tasks like action classification and referee decision making.

Result: SoccerChat performs well in general soccer event comprehension and maintains competitive accuracy in referee decision making.

Conclusion: Multimodal integration is crucial for advancing soccer analytics, enabling more interactive and explainable AI-driven analysis.

Abstract: The integration of artificial intelligence in sports analytics has
transformed soccer video understanding, enabling real-time, automated insights
into complex game dynamics. Traditional approaches rely on isolated data
streams, limiting their effectiveness in capturing the full context of a match.
To address this, we introduce SoccerChat, a multimodal conversational AI
framework that integrates visual and textual data for enhanced soccer video
comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey
color annotations and automatic speech recognition (ASR) transcripts,
SoccerChat is fine-tuned on a structured video instruction dataset to
facilitate accurate game understanding, event classification, and referee
decision making. We benchmark SoccerChat on action classification and referee
decision-making tasks, demonstrating its performance in general soccer event
comprehension while maintaining competitive accuracy in referee decision
making. Our findings highlight the importance of multimodal integration in
advancing soccer analytics, paving the way for more interactive and explainable
AI-driven sports analysis. https://github.com/simula/SoccerChat

</details>


### [330] [Towards Texture- And Shape-Independent 3D Keypoint Estimation in Birds](https://arxiv.org/pdf/2505.16633)
*Valentin Schmuker, Alex Hoi Hang Chan, Bastian Goldluecke, Urs Waldmann*

Main category: cs.CV

TL;DR: A texture-independent method for 3D joint position estimation in pigeons, extending 3D-MuPPET with segmentation and 2D keypoints, achieving comparable accuracy and showing promise for other bird species.


<details>
  <summary>Details</summary>
Motivation: To develop a texture-independent approach for 3D pose estimation in pigeons, improving upon the texture-dependent 3D-MuPPET framework and exploring its applicability to other bird species.

Method: Extends 3D-MuPPET by using segmentation to generate silhouettes, estimating 2D keypoints, triangulating for 3D poses, and tracking identities. Tested on pigeons and other bird species without fine-tuning.

Result: Achieves comparable accuracy to the original 3D-MuPPET and shows promising results for other bird species.

Conclusion: The approach provides a foundation for robust, texture-independent pose estimation frameworks and inspires further development.

Abstract: In this paper, we present a texture-independent approach to estimate and
track 3D joint positions of multiple pigeons. For this purpose, we build upon
the existing 3D-MuPPET framework, which estimates and tracks the 3D poses of up
to 10 pigeons using a multi-view camera setup. We extend this framework by
using a segmentation method that generates silhouettes of the individuals,
which are then used to estimate 2D keypoints. Following 3D-MuPPET, these 2D
keypoints are triangulated to infer 3D poses, and identities are matched in the
first frame and tracked in 2D across subsequent frames. Our proposed
texture-independent approach achieves comparable accuracy to the original
texture-dependent 3D-MuPPET framework. Additionally, we explore our approach's
applicability to other bird species. To do that, we infer the 2D joint
positions of four bird species without additional fine-tuning the model trained
on pigeons and obtain preliminary promising results. Thus, we think that our
approach serves as a solid foundation and inspires the development of more
robust and accurate texture-independent pose estimation frameworks.

</details>


### [331] [From Evaluation to Defense: Advancing Safety in Video Large Language Models](https://arxiv.org/pdf/2505.16643)
*Yiwei Sun, Peiqi Jiang, Chuanbin Liu, Luohao Lin, Zhiying Lu, Hongtao Xie*

Main category: cs.CV

TL;DR: The paper introduces VideoSafetyBench (VSB-77k), a large-scale benchmark for Video LLM safety, and reveals a 42.3% safety degradation due to video modality. It proposes VideoSafety-R1, a dual-stage framework, achieving significant safety improvements.


<details>
  <summary>Details</summary>
Motivation: To address the under-examined safety risks of video-based large language models (Video LLMs) and mitigate systemic vulnerabilities exposed by multimodal attacks.

Method: Proposes VideoSafety-R1 with two innovations: (1) Alarm Token-Guided Safety Fine-Tuning (AT-SFT) for explicit harm perception, and (2) Safety-Guided GRPO for defensive reasoning.

Result: Achieves a 65.1% improvement on VSB-Eval-HH and notable gains on other safety datasets (MMBench, VLGuard, FigStep).

Conclusion: The framework shifts safety alignment from passive recognition to active reasoning, significantly enhancing Video LLM safety.

Abstract: While the safety risks of image-based large language models have been
extensively studied, their video-based counterparts (Video LLMs) remain
critically under-examined. To systematically study this problem, we introduce
\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse
benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and
spans 19 principal risk categories across 10 language communities. \textit{We
reveal that integrating video modality degrades safety performance by an
average of 42.3\%, exposing systemic risks in multimodal attack exploitation.}
To address this vulnerability, we propose \textbf{VideoSafety-R1}, a dual-stage
framework achieving unprecedented safety gains through two innovations: (1)
Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens
into visual and textual sequences, enabling explicit harm perception across
modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances
defensive reasoning through dynamic policy optimization with rule-based rewards
derived from dual-modality verification. These components synergize to shift
safety alignment from passive harm recognition to active reasoning. The
resulting framework achieves a 65.1\% improvement on VSB-Eval-HH, and improves
by 59.1\%, 44.3\%, and 15.0\% on the image safety datasets MMBench, VLGuard,
and FigStep, respectively. \textit{Our codes are available in the supplementary
materials.} \textcolor{red}{Warning: This paper contains examples of harmful
language and videos, and reader discretion is recommended.}

</details>


### [332] [Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models](https://arxiv.org/pdf/2505.16647)
*Sushant Gautam, Michael A. Riegler, Pål Halvorsen*

Main category: cs.CV

TL;DR: Fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding improves robustness and accuracy but reveals trade-offs in edge cases.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic accuracy and efficiency by adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning.

Method: Fine-tuned Qwen2.5-VL-7B-Instruct using LoRA on MedMultiPoints dataset, reformulating tasks into instruction-based prompts.

Result: Multi-task training improves robustness and accuracy, e.g., reducing Count MAE and increasing Matching Accuracy, but with trade-offs like reduced reliability in edge cases.

Conclusion: The approach shows promise for explainable and versatile medical AI, mirroring clinical workflows and producing interpretable outputs.

Abstract: We investigate fine-tuning Vision-Language Models (VLMs) for multi-task
medical image understanding, focusing on detection, localization, and counting
of findings in medical images. Our objective is to evaluate whether
instruction-tuned VLMs can simultaneously improve these tasks, with the goal of
enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a
multimodal dataset with annotations from endoscopy (polyps and instruments) and
microscopy (sperm cells), we reformulate each task into instruction-based
prompts suitable for vision-language reasoning. We fine-tune
Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task
combinations. Results show that multi-task training improves robustness and
accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and
increases Matching Accuracy in the Counting + Pointing task. However,
trade-offs emerge, such as more zero-case point predictions, indicating reduced
reliability in edge cases despite overall performance gains. Our study
highlights the potential of adapting general-purpose VLMs to specialized
medical tasks via prompt-driven fine-tuning. This approach mirrors clinical
workflows, where radiologists simultaneously localize, count, and describe
findings - demonstrating how VLMs can learn composite diagnostic reasoning
patterns. The model produces interpretable, structured outputs, offering a
promising step toward explainable and versatile medical AI. Code, model
weights, and scripts will be released for reproducibility at
https://github.com/simula/PointDetectCount.

</details>


### [333] [Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding](https://arxiv.org/pdf/2505.16652)
*Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zelin Peng, Zhiwei Yang, Jionglong Su, Minquan Lin, Yifan Peng, Xuelian Cheng, Imran Razzak, Zongyuan Ge*

Main category: cs.CV

TL;DR: FarSight is a plug-and-play decoding strategy using causal masks to reduce hallucinations in multimodal large language models by optimizing token interaction and attention allocation.


<details>
  <summary>Details</summary>
Motivation: Addressing hallucinations in MLLMs, categorized as initial and snowball types, by improving token interaction and contextual inference.

Method: Leverages causal masks and an attention register structure to dynamically allocate attention and tackle outlier tokens, with positional awareness encoding for video tasks.

Result: FarSight significantly reduces hallucinations across MLLMs on image and video benchmarks.

Conclusion: The proposed method effectively mitigates hallucinations by enhancing token propagation and attention management.

Abstract: Recent advancements in multimodal large language models (MLLMs) have
significantly improved performance in visual question answering. However, they
often suffer from hallucinations. In this work, hallucinations are categorized
into two main types: initial hallucinations and snowball hallucinations. We
argue that adequate contextual information can be extracted directly from the
token interaction process. Inspired by causal inference in the decoding
strategy, we propose to leverage causal masks to establish information
propagation between multimodal tokens. The hypothesis is that insufficient
interaction between those tokens may lead the model to rely on outlier tokens,
overlooking dense and rich contextual cues. Therefore, we propose to intervene
in the propagation process by tackling outlier tokens to enhance in-context
inference. With this goal, we present FarSight, a versatile plug-and-play
decoding strategy to reduce attention interference from outlier tokens merely
by optimizing the causal mask. The heart of our method is effective token
propagation. We design an attention register structure within the upper
triangular matrix of the causal mask, dynamically allocating attention to
capture attention diverted to outlier tokens. Moreover, a positional awareness
encoding method with a diminishing masking rate is proposed, allowing the model
to attend to further preceding tokens, especially for video sequence tasks.
With extensive experiments, FarSight demonstrates significant
hallucination-mitigating performance across different MLLMs on both image and
video benchmarks, proving its effectiveness.

</details>


### [334] [SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images](https://arxiv.org/pdf/2505.16659)
*Kaiyu Guo, Tan Pan, Chen Jiang, Zijian Wang, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: A framework for few-shot medical anomaly detection (SD-MAD) is proposed, focusing on identifying multiple anomaly categories using radiological signs and textual descriptions from a Large-Language model.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited medical imaging data and the need to distinguish multiple anomaly categories in few-shot learning.

Method: SD-MAD, a two-stage framework: (i) aligning radiological signs with anomaly categories, (ii) selecting signs to mitigate under-fitting and uncertain-sample issues.

Result: The method effectively handles multi-anomaly detection, validated through extensive experiments.

Conclusion: SD-MAD improves few-shot medical anomaly detection by leveraging textual descriptions and radiological signs, offering a robust solution for multi-category anomaly identification.

Abstract: Medical anomaly detection (AD) is crucial for early clinical intervention,
yet it faces challenges due to limited access to high-quality medical imaging
data, caused by privacy concerns and data silos. Few-shot learning has emerged
as a promising approach to alleviate these limitations by leveraging the
large-scale prior knowledge embedded in vision-language models (VLMs). Recent
advancements in few-shot medical AD have treated normal and abnormal cases as a
one-class classification problem, often overlooking the distinction among
multiple anomaly categories. Thus, in this paper, we propose a framework
tailored for few-shot medical anomaly detection in the scenario where the
identification of multiple anomaly categories is required. To capture the
detailed radiological signs of medical anomaly categories, our framework
incorporates diverse textual descriptions for each category generated by a
Large-Language model, under the assumption that different anomalies in medical
images may share common radiological signs in each category. Specifically, we
introduce SD-MAD, a two-stage Sign-Driven few-shot Multi-Anomaly Detection
framework: (i) Radiological signs are aligned with anomaly categories by
amplifying inter-anomaly discrepancy; (ii) Aligned signs are selected further
to mitigate the effect of the under-fitting and uncertain-sample issue caused
by limited medical data, employing an automatic sign selection strategy at
inference. Moreover, we propose three protocols to comprehensively quantify the
performance of multi-anomaly detection. Extensive experiments illustrate the
effectiveness of our method.

</details>


### [335] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/pdf/2505.16673)
*Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang*

Main category: cs.CV

TL;DR: Share-GRPO, a novel RL approach, enhances reasoning in MLLMs by sharing diverse reasoning trajectories and reward information, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning in Multimodal Large Language Models (MLLMs) by addressing sparse rewards and advantage vanishing in reinforcement learning.

Method: Proposes Share-GRPO, which expands question space, explores diverse reasoning trajectories, and shares rewards hierarchically.

Result: Superior performance on six reasoning benchmarks.

Conclusion: Share-GRPO effectively mitigates RL challenges and boosts MLLM reasoning.

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


### [336] [Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual Question Answering with Prior Knowledge](https://arxiv.org/pdf/2505.16674)
*Marcella Astrid, Abdelrahman Shabayek, Djamila Aouada*

Main category: cs.CV

TL;DR: The paper proposes a zero-shot anomaly detection method for battery thermal images using VQA models, eliminating the need for labeled training data.


<details>
  <summary>Details</summary>
Motivation: Safety and efficiency in battery applications require early anomaly detection, but traditional methods rely on extensive labeled data, which is hard to obtain.

Method: Leverages pretrained VQA models (ChatGPT-4o, LLaVa-13b, BLIP-2) with text-based prompts incorporating prior knowledge of normal battery behavior.

Result: Competitive performance against state-of-the-art models without battery-specific training data.

Conclusion: VQA-based zero-shot learning shows promise for battery anomaly detection, with potential for future improvements.

Abstract: Batteries are essential for various applications, including electric vehicles
and renewable energy storage, making safety and efficiency critical concerns.
Anomaly detection in battery thermal images helps identify failures early, but
traditional deep learning methods require extensive labeled data, which is
difficult to obtain, especially for anomalies due to safety risks and high data
collection costs. To overcome this, we explore zero-shot anomaly detection
using Visual Question Answering (VQA) models, which leverage pretrained
knowledge and textbased prompts to generalize across vision tasks. By
incorporating prior knowledge of normal battery thermal behavior, we design
prompts to detect anomalies without battery-specific training data. We evaluate
three VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness
to prompt variations, repeated trials, and qualitative outputs. Despite the
lack of finetuning on battery data, our approach demonstrates competitive
performance compared to state-of-the-art models that are trained with the
battery data. Our findings highlight the potential of VQA-based zero-shot
learning for battery anomaly detection and suggest future directions for
improving its effectiveness.

</details>


### [337] [Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds](https://arxiv.org/pdf/2505.16679)
*Jordan Dotzel, Tony Montes, Mohamed S. Abdelfattah, Zhiru Zhang*

Main category: cs.CV

TL;DR: Semantic compression for 3D objects outperforms traditional methods at high compression rates (up to 105x) by focusing on core concepts and using natural language for storage.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D compression methods fail at high compression rates, leading to artifacts and poor quality. Semantic compression offers a better solution by leveraging deep generative models and human-readable formats.

Method: A pipeline using public generative models to compress 3D objects semantically, ignoring structural details and predicting missing information.

Result: Achieved compression rates up to 105x on the Objaverse dataset, outperforming traditional methods around 100x compression while preserving quality.

Conclusion: Semantic compression is superior for high-rate 3D object compression, especially in collaborative AR/VR applications.

Abstract: Traditional methods for 3D object compression operate only on structural
information within the object vertices, polygons, and textures. These methods
are effective at compression rates up to 10x for standard object sizes but
quickly deteriorate at higher compression rates with texture artifacts,
low-polygon counts, and mesh gaps. In contrast, semantic compression ignores
structural information and operates directly on the core concepts to push to
extreme levels of compression. In addition, it uses natural language as its
storage format, which makes it natively human-readable and a natural fit for
emerging applications built around large-scale, collaborative projects within
augmented and virtual reality. It deprioritizes structural information like
location, size, and orientation and predicts the missing information with
state-of-the-art deep generative models. In this work, we construct a pipeline
for 3D semantic compression from public generative models and explore the
quality-compression frontier for 3D object compression. We apply this pipeline
to achieve rates as high as 105x for 3D objects taken from the Objaverse
dataset and show that semantic compression can outperform traditional methods
in the important quality-preserving region around 100x compression.

</details>


### [338] [On the use of Graphs for Satellite Image Time Series](https://arxiv.org/pdf/2505.16685)
*Corentin Dufourg, Charlotte Pelletier, Stéphane May, Sébastien Lefèvre*

Main category: cs.CV

TL;DR: The paper explores graph-based methods for analyzing satellite image time series (SITS) to model spatio-temporal interactions, presenting a versatile pipeline and demonstrating its potential in land cover mapping and water resource forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and volume of SITS data, the paper motivates the use of graph-based techniques for better modeling of spatial and temporal interactions, improving pattern detection and analysis.

Method: The paper proposes a graph-based pipeline for SITS analysis, focusing on constructing spatio-temporal graphs and applying them to downstream tasks like classification and regression. It includes a review and case studies.

Result: Case studies demonstrate the effectiveness of graph-based approaches for land cover mapping and water resource forecasting, highlighting their potential.

Conclusion: The paper concludes by discussing limitations and future directions, encouraging further development of graph-based methods in remote-sensing analysis.

Abstract: The Earth's surface is subject to complex and dynamic processes, ranging from
large-scale phenomena such as tectonic plate movements to localized changes
associated with ecosystems, agriculture, or human activity. Satellite images
enable global monitoring of these processes with extensive spatial and temporal
coverage, offering advantages over in-situ methods. In particular, resulting
satellite image time series (SITS) datasets contain valuable information. To
handle their large volume and complexity, some recent works focus on the use of
graph-based techniques that abandon the regular Euclidean structure of
satellite data to work at an object level. Besides, graphs enable modelling
spatial and temporal interactions between identified objects, which are crucial
for pattern detection, classification and regression tasks. This paper is an
effort to examine the integration of graph-based methods in spatio-temporal
remote-sensing analysis. In particular, it aims to present a versatile
graph-based pipeline to tackle SITS analysis. It focuses on the construction of
spatio-temporal graphs from SITS and their application to downstream tasks. The
paper includes a comprehensive review and two case studies, which highlight the
potential of graph-based approaches for land cover mapping and water resource
forecasting. It also discusses numerous perspectives to resolve current
limitations and encourage future developments.

</details>


### [339] [KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models](https://arxiv.org/pdf/2505.16707)
*Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, Ming-Hsuan Yang, Xu Yang*

Main category: cs.CV

TL;DR: KRIS-Bench is a benchmark for evaluating knowledge-based reasoning in image-editing models, revealing gaps in current systems.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored capacity of multi-modal generative models in knowledge-based reasoning for image editing.

Method: Introduces KRIS-Bench, a diagnostic benchmark with 22 tasks across 7 reasoning dimensions, using a taxonomy of Factual, Conceptual, and Procedural knowledge. Includes 1,267 annotated instances and a Knowledge Plausibility metric.

Result: Tests on 10 state-of-the-art models show significant reasoning performance gaps.

Conclusion: Knowledge-centric benchmarks like KRIS-Bench are essential for advancing intelligent image editing systems.

Abstract: Recent advances in multi-modal generative models have enabled significant
progress in instruction-based image editing. However, while these models
produce visually plausible outputs, their capacity for knowledge-based
reasoning editing tasks remains under-explored. In this paper, we introduce
KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a
diagnostic benchmark designed to assess models through a cognitively informed
lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks
across three foundational knowledge types: Factual, Conceptual, and Procedural.
Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning
dimensions and release 1,267 high-quality annotated editing instances. To
support fine-grained evaluation, we propose a comprehensive protocol that
incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints
and calibrated through human studies. Empirical results on 10 state-of-the-art
models reveal significant gaps in reasoning performance, highlighting the need
for knowledge-centric benchmarks to advance the development of intelligent
image editing systems.

</details>


### [340] [Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP](https://arxiv.org/pdf/2505.16740)
*Alya Zouzou, Léo andéol, Mélanie Ducoffe, Ryma Boumazouza*

Main category: cs.CV

TL;DR: Conformal prediction improves runway detection reliability in vision-based landing systems by quantifying uncertainty, with a novel C-mAP metric introduced.


<details>
  <summary>Details</summary>
Motivation: To enhance safety and reliability in vision-based landing systems by providing statistical uncertainty guarantees for runway detection.

Method: Fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, applied conformal prediction for uncertainty quantification, and introduced C-mAP metric.

Result: Conformal prediction effectively quantifies uncertainty, improving detection reliability and supporting ML system certification in aerospace.

Conclusion: Conformal prediction offers a statistically sound method to enhance runway detection safety, enabling ML certification in aerospace.

Abstract: We explore the use of conformal prediction to provide statistical uncertainty
guarantees for runway detection in vision-based landing systems (VLS). Using
fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal
prediction to quantify localization reliability under user-defined risk levels.
We also introduce Conformal mean Average Precision (C-mAP), a novel metric
aligning object detection performance with conformal guarantees. Our results
show that conformal prediction can improve the reliability of runway detection
by quantifying uncertainty in a statistically sound way, increasing safety
on-board and paving the way for certification of ML system in the aerospace
domain.

</details>


### [341] [Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.16761)
*Jian Liu, Jing Xu, Song Guo, Jing Li, Jingfeng Guo, Jiaao Yu, Haohan Weng, Biwen Lei, Xianghui Yang, Zhuo Chen, Fangqi Zhu, Tao Han, Chunchao Guo*

Main category: cs.CV

TL;DR: Mesh-RFT introduces a fine-grained RL framework with M-DPO for localized mesh refinement, improving quality via face-level optimization and topology-aware metrics.


<details>
  <summary>Details</summary>
Motivation: Address biases and low-quality outputs in pretrained 3D mesh models and limitations of global RL methods in capturing local details.

Method: Uses Masked Direct Preference Optimization (M-DPO) for localized refinement and introduces BER and TS metrics for quality evaluation.

Result: Reduces Hausdorff Distance by 24.6% and improves Topology Score by 3.8% over pretrained models, outperforming global DPO methods.

Conclusion: Mesh-RFT achieves state-of-the-art performance in production-ready mesh generation by optimizing at face-level granularity.

Abstract: Existing pretrained models for 3D mesh generation often suffer from data
biases and produce low-quality results, while global reinforcement learning
(RL) methods rely on object-level rewards that struggle to capture local
structure details. To address these challenges, we present \textbf{Mesh-RFT}, a
novel fine-grained reinforcement fine-tuning framework that employs Masked
Direct Preference Optimization (M-DPO) to enable localized refinement via
quality-aware face masking. To facilitate efficient quality evaluation, we
introduce an objective topology-aware scoring system to evaluate geometric
integrity and topological regularity at both object and face levels through two
metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating
these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first
method to optimize mesh quality at the granularity of individual faces,
resolving localized errors while preserving global coherence. Experiment
results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\%
and improves Topology Score (TS) by 3.8\% over pre-trained models, while
outperforming global DPO methods with a 17.4\% HD reduction and 4.9\% TS gain.
These results demonstrate Mesh-RFT's ability to improve geometric integrity and
topological regularity, achieving new state-of-the-art performance in
production-ready mesh generation. Project Page:
\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.

</details>


### [342] [Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation](https://arxiv.org/pdf/2505.16763)
*Hongji Yang, Yucheng Zhou, Wencheng Han, Jianbing Shen*

Main category: cs.CV

TL;DR: A novel prompt optimization framework uses large vision language models (LVLMs) to rewrite and score prompts for text-to-image models, reducing reliance on manual data and trained models.


<details>
  <summary>Details</summary>
Motivation: Address the need for specialized vocabulary in crafting text prompts for text-to-image models and reduce dependence on large annotated datasets and biased trained models.

Method: Employ LVLMs as both prompt rewriters and reward models for aesthetics and alignment, using AI feedback instead of human input, and iterating via reinforcement learning.

Result: Outperforms competitors on two popular datasets.

Conclusion: The framework effectively optimizes prompts for text-to-image models using LVLMs, achieving self-improvement and reducing manual effort.

Abstract: Text-to-image models are powerful for producing high-quality images based on
given text prompts, but crafting these prompts often requires specialized
vocabulary. To address this, existing methods train rewriting models with
supervision from large amounts of manually annotated data and trained aesthetic
assessment models. To alleviate the dependence on data scale for model training
and the biases introduced by trained models, we propose a novel prompt
optimization framework, designed to rephrase a simple user prompt into a
sophisticated prompt to a text-to-image model. Specifically, we employ the
large vision language models (LVLMs) as the solver to rewrite the user prompt,
and concurrently, employ LVLMs as a reward model to score the aesthetics and
alignment of the images generated by the optimized prompt. Instead of laborious
human feedback, we exploit the prior knowledge of the LVLM to provide rewards,
i.e., AI feedback. Simultaneously, the solver and the reward model are unified
into one model and iterated in reinforcement learning to achieve
self-improvement by giving a solution and judging itself. Results on two
popular datasets demonstrate that our method outperforms other strong
competitors.

</details>


### [343] [RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/pdf/2505.16770)
*Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-Nin Hu*

Main category: cs.CV

TL;DR: A new benchmark, RBench-V, evaluates multi-modal models' reasoning abilities through vision-indispensable tasks, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook multi-modal output reasoning, focusing only on inputs and text-based reasoning, necessitating a tool like RBench-V.

Method: RBench-V includes 803 questions requiring image manipulation (e.g., generating images, drawing auxiliary lines) to solve problems in math, physics, counting, and games.

Result: Top models like o3 achieve only 25.8% accuracy on RBench-V, far below the human score of 82.3%.

Conclusion: Current multi-modal models struggle with vision-based reasoning, highlighting the need for further advancements in this area.

Abstract: The rapid advancement of native multi-modal models and omni-models,
exemplified by GPT-4o, Gemini, and o3, with their capability to process and
generate content across modalities such as text and images, marks a significant
milestone in the evolution of intelligence. Systematic evaluation of their
multi-modal output capabilities in visual thinking processes (also known as
multi-modal chain of thought, M-CoT) becomes critically important. However,
existing benchmarks for evaluating multi-modal models primarily focus on
assessing multi-modal inputs and text-only reasoning while neglecting the
importance of reasoning through multi-modal outputs. In this paper, we present
a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable
reasoning abilities. To construct RBench-V, we carefully hand-pick 803
questions covering math, physics, counting, and games. Unlike previous
benchmarks that typically specify certain input modalities, RBench-V presents
problems centered on multi-modal outputs, which require image manipulation such
as generating novel images and constructing auxiliary lines to support the
reasoning process. We evaluate numerous open- and closed-source models on
RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the
best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below
the human score of 82.3%, highlighting that current models struggle to leverage
multi-modal reasoning. Data and code are available at
https://evalmodels.github.io/rbenchv

</details>


### [344] [Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis](https://arxiv.org/pdf/2505.16773)
*Iván Matas, Carmen Serrano, Miguel Nogales, David Moreno, Lara Ferrándiz, Teresa Ojeda, Begoña Acha*

Main category: cs.CV

TL;DR: The paper introduces an unsupervised learning framework using a VAE for dermatological feature extraction, outperforming ImageNet-pretrained models in generalization and adaptability.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of transfer learning with ImageNet-pretrained models in medical imaging by developing a domain-specific, self-supervised approach.

Method: Train a Variational Autoencoder (VAE) on a dermatological dataset to learn clinically relevant features, then compare it to an ImageNet-pretrained model under identical classification conditions.

Result: The self-supervised model shows steady improvement (validation loss: -33.33%, accuracy: +44.44%) with minimal overfitting, while the ImageNet model overfits (overfitting gap: +0.060) despite faster convergence.

Conclusion: Domain-specific self-supervised learning offers better generalization and adaptability in medical imaging compared to general-purpose pretraining.

Abstract: Deep learning has transformed computer vision but relies heavily on large
labeled datasets and computational resources. Transfer learning, particularly
fine-tuning pretrained models, offers a practical alternative; however, models
pretrained on natural image datasets such as ImageNet may fail to capture
domain-specific characteristics in medical imaging. This study introduces an
unsupervised learning framework that extracts high-value dermatological
features instead of relying solely on ImageNet-based pretraining. We employ a
Variational Autoencoder (VAE) trained from scratch on a proprietary
dermatological dataset, allowing the model to learn a structured and clinically
relevant latent space. This self-supervised feature extractor is then compared
to an ImageNet-pretrained backbone under identical classification conditions,
highlighting the trade-offs between general-purpose and domain-specific
pretraining. Our results reveal distinct learning patterns. The self-supervised
model achieves a final validation loss of 0.110 (-33.33%), while the
ImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.
Accuracy trends confirm this: the self-supervised model improves from 45% to
65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained
model reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting
gap increasing to +0.060. These findings suggest that while ImageNet
pretraining accelerates convergence, it also amplifies overfitting on
non-clinically relevant features. In contrast, self-supervised learning
achieves steady improvements, stronger generalization, and superior
adaptability, underscoring the importance of domain-specific feature extraction
in medical imaging.

</details>


### [345] [Single Domain Generalization for Few-Shot Counting via Universal Representation Matching](https://arxiv.org/pdf/2505.16778)
*Xianing Chen, Si Huo, Borui Jiang, Hailin Hu, Xinghao Chen*

Main category: cs.CV

TL;DR: URM introduces universal vision-language representations to improve few-shot counting's robustness to domain shifts, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot counting methods struggle with domain shifts, limiting generalization to unseen scenarios.

Method: URM leverages universal vision-language representations from a pretrained model to enhance prototype generalization and correlation map construction.

Result: URM outperforms existing methods in both in-domain and domain generalization settings.

Conclusion: URM addresses domain shift challenges in few-shot counting by integrating universal representations, setting a new benchmark.

Abstract: Few-shot counting estimates the number of target objects in an image using
only a few annotated exemplars. However, domain shift severely hinders existing
methods to generalize to unseen scenarios. This falls into the realm of single
domain generalization that remains unexplored in few-shot counting. To solve
this problem, we begin by analyzing the main limitations of current methods,
which typically follow a standard pipeline that extract the object prototypes
from exemplars and then match them with image feature to construct the
correlation map. We argue that existing methods overlook the significance of
learning highly generalized prototypes. Building on this insight, we propose
the first single domain generalization few-shot counting model, Universal
Representation Matching, termed URM. Our primary contribution is the discovery
that incorporating universal vision-language representations distilled from a
large scale pretrained vision-language model into the correlation construction
process substantially improves robustness to domain shifts without compromising
in domain performance. As a result, URM achieves state-of-the-art performance
on both in domain and the newly introduced domain generalization setting.

</details>


### [346] [Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles](https://arxiv.org/pdf/2505.16784)
*Jun Xie, Xiongjun Guan, Yingjian Zhu, Zhaoran Zhao, Xinming Wang, Feng Chen, Zhepeng Wang*

Main category: cs.CV

TL;DR: Runner-up solution for Ego4D EgoSchema Challenge at CVPR 2025, leveraging multimodal large models for video understanding via few-shot learning and ensemble strategies.


<details>
  <summary>Details</summary>
Motivation: To adapt and evaluate leading multimodal large models for video tasks, exploring prompt styles and process paradigms to enhance model attention and generalization.

Method: Utilizes few-shot learning and model ensemble strategies, with diversified prompts and process paradigms, and introduces a cooperative ensemble stage.

Result: Outperforms previous SOTA with a single model; ensemble stage further improves performance.

Conclusion: Provides a practical reference for large model applications and inspires future research.

Abstract: In this paper, we present the runner-up solution for the Ego4D EgoSchema
Challenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of
large models, we evaluate and leverage leading accessible multimodal large
models and adapt them to video understanding tasks via few-shot learning and
model ensemble strategies. Specifically, diversified prompt styles and process
paradigms are systematically explored and evaluated to effectively guide the
attention of large models, fully unleashing their powerful generalization and
adaptability abilities. Experimental results demonstrate that, with our
carefully designed approach, directly utilizing an individual multimodal model
already outperforms the previous state-of-the-art (SOTA) method which includes
several additional processes. Besides, an additional stage is further
introduced that facilitates the cooperation and ensemble of periodic results,
which achieves impressive performance improvements. We hope this work serves as
a valuable reference for the practical application of large models and inspires
future research in the field.

</details>


### [347] [REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training](https://arxiv.org/pdf/2505.16792)
*Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, Yang You*

Main category: cs.CV

TL;DR: HASTE accelerates DiT training by aligning teacher features early and terminating alignment later, improving efficiency without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Training Diffusion Transformers (DiTs) is slow, and existing methods like REPA plateau or degrade performance. HASTE addresses this by dynamically aligning and then freeing the model.

Method: HASTE uses a two-phase schedule: Phase I aligns teacher features and attention maps into the DiT; Phase II terminates alignment to focus on denoising.

Result: HASTE speeds up training, achieving baseline FID in 50 epochs and matching REPA's best FID in 500 epochs (28x faster). It also improves text-to-image DiTs.

Conclusion: HASTE is a simple, effective method for efficient DiT training across tasks, demonstrated on ImageNet and MS-COCO.

Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet
their training remains notoriously slow. A recent remedy -- representation
alignment (REPA) that matches DiT hidden features to those of a non-generative
teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus
or even degrades performance later. We trace this failure to a capacity
mismatch: once the generative student begins modelling the joint data
distribution, the teacher's lower-dimensional embeddings and attention patterns
become a straitjacket rather than a guide. We then introduce HASTE (Holistic
Alignment with Stage-wise Termination for Efficient training), a two-phase
schedule that keeps the help and drops the hindrance. Phase I applies a
holistic alignment loss that simultaneously distills attention maps (relational
priors) and feature projections (semantic anchors) from the teacher into
mid-level layers of the DiT, yielding rapid convergence. Phase II then performs
one-shot termination that deactivates the alignment loss, once a simple trigger
such as a fixed iteration is hit, freeing the DiT to focus on denoising and
exploit its generative capacity. HASTE speeds up training of diverse DiTs
without architecture changes. On ImageNet 256X256, it reaches the vanilla
SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,
amounting to a 28X reduction in optimization steps. HASTE also improves
text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled
recipe for efficient diffusion training across various tasks. Our code is
available at https://github.com/NUS-HPC-AI-Lab/HASTE .

</details>


### [348] [REOBench: Benchmarking Robustness of Earth Observation Foundation Models](https://arxiv.org/pdf/2505.16793)
*Xiang Li, Yong Tao, Siyuan Zhang, Siwei Liu, Zhitong Xiong, Chunbo Luo, Lu Liu, Mykola Pechenizkiy, Xiao Xiang Zhu, Tianjin Huang*

Main category: cs.CV

TL;DR: REOBench evaluates Earth observation foundation models' robustness under real-world perturbations, revealing performance degradation and task-specific vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of Earth observation foundation models under real-world perturbations.

Method: Introduces REOBench, a benchmark with six tasks and twelve image corruptions, evaluating models trained via masked image modeling, contrastive learning, and vision-language pre-training.

Result: Performance degradation varies (1%-20%), with vision-language models showing enhanced robustness.

Conclusion: REOBench highlights vulnerabilities and provides insights for developing more robust models.

Abstract: Earth observation foundation models have shown strong generalization across
multiple Earth observation tasks, but their robustness under real-world
perturbations remains underexplored. To bridge this gap, we introduce REOBench,
the first comprehensive benchmark for evaluating the robustness of Earth
observation foundation models across six tasks and twelve types of image
corruptions, including both appearance-based and geometric perturbations. To
ensure realistic and fine-grained evaluation, our benchmark focuses on
high-resolution optical remote sensing images, which are widely used in
critical applications such as urban planning and disaster response. We conduct
a systematic evaluation of a broad range of models trained using masked image
modeling, contrastive learning, and vision-language pre-training paradigms. Our
results reveal that (1) existing Earth observation foundation models experience
significant performance degradation when exposed to input corruptions. (2) The
severity of degradation varies across tasks, model architectures, backbone
sizes, and types of corruption, with performance drop varying from less than 1%
to over 20%. (3) Vision-language models show enhanced robustness, particularly
in multimodal tasks. REOBench underscores the vulnerability of current Earth
observation foundation models to real-world corruptions and provides actionable
insights for developing more robust and reliable models.

</details>


### [349] [V2V: Scaling Event-Based Vision through Efficient Video-to-Voxel Simulation](https://arxiv.org/pdf/2505.16797)
*Hanyue Lou, Jinxiu Liang, Minggui Teng, Yi Wang, Boxin Shi*

Main category: cs.CV

TL;DR: V2V converts video frames to event-based voxel grids, reducing storage needs by 150x and enabling large-scale training for event vision models.


<details>
  <summary>Details</summary>
Motivation: Overcome storage and data scarcity issues in event-based vision by bypassing traditional event stream generation.

Method: Introduces Video-to-Voxel (V2V), a method to directly convert video frames into event-based voxel grids, avoiding storage-heavy event streams.

Result: Achieves 150x storage reduction, trains models on 52 hours of video (10,000 videos), and improves performance.

Conclusion: V2V enables scalable, efficient training for event vision models, enhancing generalization and robustness.

Abstract: Event-based cameras offer unique advantages such as high temporal resolution,
high dynamic range, and low power consumption. However, the massive storage
requirements and I/O burdens of existing synthetic data generation pipelines
and the scarcity of real data prevent event-based training datasets from
scaling up, limiting the development and generalization capabilities of event
vision models. To address this challenge, we introduce Video-to-Voxel (V2V), an
approach that directly converts conventional video frames into event-based
voxel grid representations, bypassing the storage-intensive event stream
generation entirely. V2V enables a 150 times reduction in storage requirements
while supporting on-the-fly parameter randomization for enhanced model
robustness. Leveraging this efficiency, we train several video reconstruction
and optical flow estimation model architectures on 10,000 diverse videos
totaling 52 hours--an order of magnitude larger than existing event datasets,
yielding substantial improvements.

</details>


### [350] [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/pdf/2505.16836)
*Fanrui Zhang, Dian Li, Qiang Zhang, Chenjun, sinbadliu, Junxiong Lin, Jiahong Yan, Jiawei Liu, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: FakeVV is a large-scale benchmark for video misinformation detection, and Fact-R1 is a novel framework combining deep reasoning and rule-based reinforcement learning for improved detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale datasets and deep reasoning in video misinformation detection.

Method: Fact-R1 integrates deep reasoning with collaborative rule-based reinforcement learning, trained via a three-stage process: misinformation CoT instruction tuning, DPO alignment, and GRPO with a verifiable reward function.

Result: Fact-R1 achieves emergent reasoning behaviors comparable to advanced text-based systems, tailored for multimodal misinformation.

Conclusion: The work sets a new paradigm for misinformation detection by combining large-scale video understanding, reasoning-guided alignment, and interpretable verification.

Abstract: The rapid spread of multimodal misinformation on social media has raised
growing concerns, while research on video misinformation detection remains
limited due to the lack of large-scale, diverse datasets. Existing methods
often overfit to rigid templates and lack deep reasoning over deceptive
content. To address these challenges, we introduce FakeVV, a large-scale
benchmark comprising over 100,000 video-text pairs with fine-grained,
interpretable annotations. In addition, we further propose Fact-R1, a novel
framework that integrates deep reasoning with collaborative rule-based
reinforcement learning. Fact-R1 is trained through a three-stage process: (1)
misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference
alignment via Direct Preference Optimization (DPO), and (3) Group Relative
Policy Optimization (GRPO) using a novel verifiable reward function. This
enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those
observed in advanced text-based reinforcement learning systems, but in the more
complex multimodal misinformation setting. Our work establishes a new paradigm
for misinformation detection, bridging large-scale video understanding,
reasoning-guided alignment, and interpretable verification.

</details>


### [351] [SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving](https://arxiv.org/pdf/2505.16805)
*Xuesong Chen, Linjiang Huang, Tao Ma, Rongyao Fang, Shaoshuai Shi, Hongsheng Li*

Main category: cs.CV

TL;DR: SOLVE integrates Vision-Language Models (VLMs) with end-to-end models for autonomous driving, improving trajectory prediction via a shared visual encoder and Trajectory Chain-of-Thought paradigm.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like computational demands and real-time decision-making in autonomous driving by enhancing VLM-E2E integration.

Method: Uses a shared visual encoder for feature-level knowledge sharing and introduces T-CoT for refining trajectory predictions. Temporal decoupling aligns VLM outputs with real-time E2E performance.

Result: Significant improvement in trajectory prediction accuracy on the nuScenes dataset.

Conclusion: SOLVE offers a robust framework for reliable autonomous driving systems by efficiently combining VLMs and E2E models.

Abstract: The integration of Vision-Language Models (VLMs) into autonomous driving
systems has shown promise in addressing key challenges such as learning
complexity, interpretability, and common-sense reasoning. However, existing
approaches often struggle with efficient integration and realtime
decision-making due to computational demands. In this paper, we introduce
SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E)
models to enhance autonomous vehicle planning. Our approach emphasizes
knowledge sharing at the feature level through a shared visual encoder,
enabling comprehensive interaction between VLM and E2E components. We propose a
Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines
trajectory predictions, reducing uncertainty and improving accuracy. By
employing a temporal decoupling strategy, SOLVE achieves efficient cooperation
by aligning high-quality VLM outputs with E2E real-time performance. Evaluated
on the nuScenes dataset, our method demonstrates significant improvements in
trajectory prediction accuracy, paving the way for more robust and reliable
autonomous driving systems.

</details>


### [352] [T2I-ConBench: Text-to-Image Benchmark for Continual Post-training](https://arxiv.org/pdf/2505.16875)
*Zhehao Huang, Yuhang Liu, Yixin Lou, Zhengbao He, Mingzhen He, Wenxing Zhou, Tao Li, Kehan Li, Zeyi Huang, Xiaolin Huang*

Main category: cs.CV

TL;DR: The paper introduces T2I-ConBench, a benchmark for evaluating continual post-training in text-to-image models, addressing issues like forgetting and lack of standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: Standardizing evaluation for continual post-training in text-to-image models to prevent forgetting and improve zero-shot compositionality.

Method: Introduces T2I-ConBench, a benchmark with automated metrics, human-preference modeling, and vision-language QA, tested on ten methods across three task sequences.

Result: No method excels in all dimensions (generality retention, task performance, forgetting, cross-task generalization); joint training also fails in some cases.

Conclusion: T2I-ConBench and released resources aim to advance research in continual post-training for text-to-image models.

Abstract: Continual post-training adapts a single text-to-image diffusion model to
learn new tasks without incurring the cost of separate models, but naive
post-training causes forgetting of pretrained knowledge and undermines
zero-shot compositionality. We observe that the absence of a standardized
evaluation protocol hampers related research for continual post-training. To
address this, we introduce T2I-ConBench, a unified benchmark for continual
post-training of text-to-image models. T2I-ConBench focuses on two practical
scenarios, item customization and domain enhancement, and analyzes four
dimensions: (1) retention of generality, (2) target-task performance, (3)
catastrophic forgetting, and (4) cross-task generalization. It combines
automated metrics, human-preference modeling, and vision-language QA for
comprehensive assessment. We benchmark ten representative methods across three
realistic task sequences and find that no approach excels on all fronts. Even
joint "oracle" training does not succeed for every task, and cross-task
generalization remains unsolved. We release all datasets, code, and evaluation
tools to accelerate research in continual post-training for text-to-image
models.

</details>


### [353] [Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining](https://arxiv.org/pdf/2505.16811)
*Shangquan Sun, Wenqi Ren, Juxiang Zhou, Shu Wang, Jianhou Gan, Xiaochun Cao*

Main category: cs.CV

TL;DR: A dual-branch spatio-temporal state-space model is proposed for video deraining, addressing generalization issues with synthetic data and enhancing downstream tasks like object detection in rain.


<details>
  <summary>Details</summary>
Motivation: Existing methods relying on paired data fail to generalize to real-world rain due to synthetic-authentic disparities.

Method: Uses spatial and temporal state-space layers for feature extraction and dependencies, a dynamic stacking filter for feature fusion, and a median stacking loss for semi-supervised learning.

Result: Superior performance in quantitative metrics, visual quality, efficiency, and utility for downstream tasks like object detection.

Conclusion: The proposed model effectively addresses real-world rain removal challenges and supports other vision tasks in rainy conditions.

Abstract: Significant progress has been made in video restoration under rainy
conditions over the past decade, largely propelled by advancements in deep
learning. Nevertheless, existing methods that depend on paired data struggle to
generalize effectively to real-world scenarios, primarily due to the disparity
between synthetic and authentic rain effects. To address these limitations, we
propose a dual-branch spatio-temporal state-space model to enhance rain streak
removal in video sequences. Specifically, we design spatial and temporal
state-space model layers to extract spatial features and incorporate temporal
dependencies across frames, respectively. To improve multi-frame feature
fusion, we derive a dynamic stacking filter, which adaptively approximates
statistical filters for superior pixel-wise feature refinement. Moreover, we
develop a median stacking loss to enable semi-supervised learning by generating
pseudo-clean patches based on the sparsity prior of rain. To further explore
the capacity of deraining models in supporting other vision-based tasks in
rainy environments, we introduce a novel real-world benchmark focused on object
detection and tracking in rainy conditions. Our method is extensively evaluated
across multiple benchmarks containing numerous synthetic and real-world rainy
videos, consistently demonstrating its superiority in quantitative metrics,
visual quality, efficiency, and its utility for downstream tasks.

</details>


### [354] [Perceptual Quality Assessment for Embodied AI](https://arxiv.org/pdf/2505.16815)
*Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces IQA for Embodied AI, proposing a new method to assess image usability for robots, creating a database, and validating existing IQA methods.


<details>
  <summary>Details</summary>
Motivation: Current IQA methods don't address image usability for embodied tasks, limiting real-world AI applications.

Method: Developed a perception-cognition-decision-execution pipeline, created the Embodied-IQA database, and tested mainstream IQA methods.

Result: Established a database with 36k image pairs and 5m annotations, showing existing IQA methods need improvement for embodied tasks.

Conclusion: The work aims to improve Embodied AI's real-world application by providing better quality indicators.

Abstract: Embodied AI has developed rapidly in recent years, but it is still mainly
deployed in laboratories, with various distortions in the Real-world limiting
its application. Traditionally, Image Quality Assessment (IQA) methods are
applied to predict human preferences for distorted images; however, there is no
IQA method to assess the usability of an image in embodied tasks, namely, the
perceptual quality for robots. To provide accurate and reliable quality
indicators for future embodied scenarios, we first propose the topic: IQA for
Embodied AI. Specifically, we (1) based on the Mertonian system and
meta-cognitive theory, constructed a perception-cognition-decision-execution
pipeline and defined a comprehensive subjective score collection process; (2)
established the Embodied-IQA database, containing over 36k reference/distorted
image pairs, with more than 5m fine-grained annotations provided by Vision
Language Models/Vision Language Action-models/Real-world robots; (3) trained
and validated the performance of mainstream IQA methods on Embodied-IQA,
demonstrating the need to develop more accurate quality indicators for Embodied
AI. We sincerely hope that through evaluation, we can promote the application
of Embodied AI under complex distortions in the Real-world. Project page:
https://github.com/lcysyzxdxc/EmbodiedIQA

</details>


### [355] [Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts](https://arxiv.org/pdf/2505.16819)
*Taewon Kang, Ming C. Lin*

Main category: cs.CV

TL;DR: A modular pipeline generates character-driven dialogue and speech for video narratives, combining visual context, structured prompts, and dialogue history to create expressive, coherent storytelling.


<details>
  <summary>Details</summary>
Motivation: Current video generation systems lack character-driven dialogue, a key storytelling element. This paper aims to enrich visual narratives with natural speech and character expression.

Method: The method uses a pretrained vision-language encoder to extract visual context, combines it with prompts, and guides a large language model to generate dialogue. A Recursive Narrative Bank ensures contextual consistency.

Result: The framework produces fully-voiced video narratives with character-consistent dialogue, applicable across diverse story settings without additional training.

Conclusion: The approach successfully integrates dialogue into visual storytelling, enhancing narrative coherence and character expression.

Abstract: Recent advances in scene-based video generation have enabled systems to
synthesize coherent visual narratives from structured prompts. However, a
crucial dimension of storytelling -- character-driven dialogue and speech --
remains underexplored. In this paper, we present a modular pipeline that
transforms action-level prompts into visually and auditorily grounded narrative
dialogue, enriching visual storytelling with natural voice and character
expression. Our method takes as input a pair of prompts per scene, where the
first defines the setting and the second specifies a character's behavior.
While a story generation model such as Text2Story generates the corresponding
visual scene, we focus on generating expressive character utterances from these
prompts and the scene image. We apply a pretrained vision-language encoder to
extract a high-level semantic feature from the representative frame, capturing
salient visual context. This feature is then combined with the structured
prompts and used to guide a large language model in synthesizing natural,
character-consistent dialogue. To ensure contextual consistency across scenes,
we introduce a Recursive Narrative Bank that conditions each dialogue
generation on the accumulated dialogue history from prior scenes. This approach
enables characters to speak in ways that reflect their evolving goals and
interactions throughout a story. Finally, we render each utterance as
expressive, character-consistent speech, resulting in fully-voiced video
narratives. Our framework requires no additional training and demonstrates
applicability across a variety of story settings, from fantasy adventures to
slice-of-life episodes.

</details>


### [356] [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/pdf/2505.16839)
*Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, Aditya Grover*

Main category: cs.CV

TL;DR: LaViDa introduces a family of Vision-Language Models (VLMs) built on discrete diffusion models (DMs), offering faster inference and controllable generation, outperforming autoregressive VLMs like LLaVA.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive VLMs lack fast inference and controllable generation, while DMs show promise but are underexplored for multimodal tasks.

Method: LaViDa integrates DMs with a vision encoder, using techniques like complementary masking, prefix KV cache, and timestep shifting for training and inference.

Result: LaViDa outperforms AR VLMs on benchmarks (e.g., +4.1 CIDEr on COCO captioning, +59% on Constrained Poem Completion) with faster inference.

Conclusion: LaViDa is a competitive alternative to AR VLMs, combining the advantages of DMs for multimodal tasks.

Abstract: Modern Vision-Language Models (VLMs) can solve a wide range of tasks
requiring visual reasoning. In real-world scenarios, desirable properties for
VLMs include fast inference and controllable generation (e.g., constraining
outputs to adhere to a desired format). However, existing autoregressive (AR)
VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)
offer a promising alternative, enabling parallel decoding for faster inference
and bidirectional context for controllable generation through text-infilling.
While effective in language-only settings, DMs' potential for multimodal tasks
is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build
LaViDa by equipping DMs with a vision encoder and jointly fine-tune the
combined parts for multimodal instruction following. To address challenges
encountered, LaViDa incorporates novel techniques such as complementary masking
for effective training, prefix KV cache for efficient inference, and timestep
shifting for high-quality sampling. Experiments show that LaViDa achieves
competitive or superior performance to AR VLMs on multi-modal benchmarks such
as MMMU, while offering unique advantages of DMs, including flexible
speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO
captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x
speedup. On bidirectional tasks, it achieves +59% improvement on Constrained
Poem Completion. These results demonstrate LaViDa as a strong alternative to AR
VLMs. Code and models will be released in the camera-ready version.

</details>


### [357] [DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?](https://arxiv.org/pdf/2505.16915)
*Qirui Jiao, Daoyuan Chen, Yilun Huang, Xika Lin, Ying Shen, Yaliang Li*

Main category: cs.CV

TL;DR: DetailMaster is a benchmark for evaluating text-to-image models on long, detail-rich prompts, revealing performance limitations and motivating future research.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with long, detail-intensive prompts, limiting professional applications.

Method: DetailMaster introduces four evaluation dimensions and tests 12 T2I models using expert-validated prompts.

Result: Models achieve ~50% accuracy in key areas, with performance degrading as prompts lengthen.

Conclusion: The benchmark highlights systemic failures and encourages research into better compositional reasoning architectures.

Abstract: While recent text-to-image (T2I) models show impressive capabilities in
synthesizing images from brief descriptions, their performance significantly
degrades when confronted with long, detail-intensive prompts required in
professional applications. We present DetailMaster, the first comprehensive
benchmark specifically designed to evaluate T2I models' systematical abilities
to handle extended textual inputs that contain complex compositional
requirements. Our benchmark introduces four critical evaluation dimensions:
Character Attributes, Structured Character Locations, Multi-Dimensional Scene
Attributes, and Explicit Spatial/Interactive Relationships. The benchmark
comprises long and detail-rich prompts averaging 284.89 tokens, with high
quality validated by expert annotators. Evaluation on 7 general-purpose and 5
long-prompt-optimized T2I models reveals critical performance limitations:
state-of-the-art models achieve merely ~50% accuracy in key dimensions like
attribute binding and spatial reasoning, while all models showing progressive
performance degradation as prompt length increases. Our analysis highlights
systemic failures in structural comprehension and detail overload handling,
motivating future research into architectures with enhanced compositional
reasoning. We open-source the dataset, data curation code, and evaluation tools
to advance detail-rich T2I generation and enable broad applications that would
otherwise be infeasible due to the lack of a dedicated benchmark.

</details>


### [358] [Conditional Panoramic Image Generation via Masked Autoregressive Modeling](https://arxiv.org/pdf/2505.16862)
*Chaoyang Wang, Xiangtai Li, Lu Qi, Xiaofan Lin, Jinbin Bai, Qianyu Zhou, Yunhai Tong*

Main category: cs.CV

TL;DR: The paper introduces PAR, a unified framework for panoramic image generation, addressing limitations of diffusion models and task separation in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for panoramic image generation are limited by diffusion models' incompatibility with equirectangular projection and the separation of text-to-panorama and panorama outpainting tasks.

Method: The proposed Panoramic AutoRegressive model (PAR) uses masked autoregressive modeling, circular padding, and consistency alignment to unify tasks and improve spatial coherence.

Result: PAR demonstrates competitive performance in text-to-image generation and panorama outpainting, with scalability and generalization.

Conclusion: PAR effectively addresses key challenges in panoramic image generation, offering a unified and scalable solution.

Abstract: Recent progress in panoramic image generation has underscored two critical
limitations in existing approaches. First, most methods are built upon
diffusion models, which are inherently ill-suited for equirectangular
projection (ERP) panoramas due to the violation of the identically and
independently distributed (i.i.d.) Gaussian noise assumption caused by their
spherical mapping. Second, these methods often treat text-conditioned
generation (text-to-panorama) and image-conditioned generation (panorama
outpainting) as separate tasks, relying on distinct architectures and
task-specific data. In this work, we propose a unified framework, Panoramic
AutoRegressive model (PAR), which leverages masked autoregressive modeling to
address these challenges. PAR avoids the i.i.d. assumption constraint and
integrates text and image conditioning into a cohesive architecture, enabling
seamless generation across tasks. To address the inherent discontinuity in
existing generative models, we introduce circular padding to enhance spatial
coherence and propose a consistency alignment strategy to improve generation
quality. Extensive experiments demonstrate competitive performance in
text-to-image generation and panorama outpainting tasks while showcasing
promising scalability and generalization capabilities.

</details>


### [359] [Training-Free Efficient Video Generation via Dynamic Token Carving](https://arxiv.org/pdf/2505.16864)
*Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, Jiaya Jia*

Main category: cs.CV

TL;DR: Jenga is an efficient inference pipeline for video Diffusion Transformer models, combining dynamic attention carving and progressive resolution generation to reduce computational costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: The high computational demands of video Diffusion Transformer models hinder their practical deployment due to quadratic complexity in self-attention and multi-step diffusion processes.

Method: Jenga uses block-wise attention with 3D space-filling curves for dynamic token selection and progressive resolution generation to reduce redundant computations.

Result: Jenga achieves an 8.83× speedup with minimal performance drop (0.01%) on VBench, reducing inference time from minutes to seconds.

Conclusion: Jenga enables practical, high-quality video generation on modern hardware without retraining, making it a plug-and-play solution.

Abstract: Despite the remarkable generation quality of video Diffusion Transformer
(DiT) models, their practical deployment is severely hindered by extensive
computational requirements. This inefficiency stems from two key challenges:
the quadratic complexity of self-attention with respect to token length and the
multi-step nature of diffusion models. To address these limitations, we present
Jenga, a novel inference pipeline that combines dynamic attention carving with
progressive resolution generation. Our approach leverages two key insights: (1)
early denoising steps do not require high-resolution latents, and (2) later
steps do not require dense attention. Jenga introduces a block-wise attention
mechanism that dynamically selects relevant token interactions using 3D
space-filling curves, alongside a progressive resolution strategy that
gradually increases latent resolution during generation. Experimental results
demonstrate that Jenga achieves substantial speedups across multiple
state-of-the-art video diffusion models while maintaining comparable generation
quality (8.83$\times$ speedup with 0.01\% performance drop on VBench). As a
plug-and-play solution, Jenga enables practical, high-quality video generation
on modern hardware by reducing inference time from minutes to seconds --
without requiring model retraining. Code:
https://github.com/dvlab-research/Jenga

</details>


### [360] [Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)](https://arxiv.org/pdf/2505.16882)
*Isla Duporge, Sofia Minano, Nikoloz Sirmpilatze, Igor Tatarnikov, Scott Wolf, Adam L. Tyson, Daniel Rubenstein*

Main category: cs.CV

TL;DR: The paper evaluates three computer vision methods to separate animal movement from drone motion in ethological research, using zebra escape footage to extract behavioral insights.


<details>
  <summary>Details</summary>
Motivation: The need for accessible, efficient tools to analyze high-resolution drone footage of animal movement, addressing the challenge of separating animal motion from drone motion.

Method: Three approaches are tested: a bioimaging-based registration technique, a Structure-from-Motion (SfM) pipeline, and a hybrid interpolation method, applied to drone footage of 44 zebras.

Result: The best method successfully extracted individual trajectories, revealing behavioral patterns like increased alignment during escape and tighter coordination near the group's center.

Conclusion: The study demonstrates the method's effectiveness for analyzing collective animal behavior and its scalability for larger datasets.

Abstract: Ethological research increasingly benefits from the growing affordability and
accessibility of drones, which enable the capture of high-resolution footage of
animal movement at fine spatial and temporal scales. However, analyzing such
footage presents the technical challenge of separating animal movement from
drone motion. While non-trivial, computer vision techniques such as image
registration and Structure-from-Motion (SfM) offer practical solutions. For
conservationists, open-source tools that are user-friendly, require minimal
setup, and deliver timely results are especially valuable for efficient data
interpretation. This study evaluates three approaches: a bioimaging-based
registration technique, an SfM pipeline, and a hybrid interpolation method. We
apply these to a recorded escape event involving 44 plains zebras, captured in
a single drone video. Using the best-performing method, we extract individual
trajectories and identify key behavioral patterns: increased alignment
(polarization) during escape, a brief widening of spacing just before stopping,
and tighter coordination near the group's center. These insights highlight the
method's effectiveness and its potential to scale to larger datasets,
contributing to broader investigations of collective animal behavior.

</details>


### [361] [RealEngine: Simulating Autonomous Driving in Realistic Context](https://arxiv.org/pdf/2505.16902)
*Junzhe Jiang, Nan Song, Jingyu Li, Xiatian Zhu, Li Zhang*

Main category: cs.CV

TL;DR: RealEngine is a driving simulation framework integrating 3D scene reconstruction and novel view synthesis for realistic, closed-loop simulation, addressing gaps in existing simulators.


<details>
  <summary>Details</summary>
Motivation: Existing simulators lack multi-modal sensing, diverse scenarios, and computational efficiency, limiting reliable driving agent evaluation.

Method: RealEngine uses real-world sensor data to separately reconstruct scenes and traffic participants, enabling flexible, photorealistic rendering.

Result: The framework supports non-reactive simulation, safety testing, and multi-agent interaction, forming a comprehensive benchmark.

Conclusion: RealEngine bridges key gaps in driving simulation, offering a reliable tool for evaluating driving agents.

Abstract: Driving simulation plays a crucial role in developing reliable driving agents
by providing controlled, evaluative environments. To enable meaningful
assessments, a high-quality driving simulator must satisfy several key
requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with
realistic scene rendering to minimize observational discrepancies; closed-loop
evaluation to support free-form trajectory behaviors; highly diverse traffic
scenarios for thorough evaluation; multi-agent cooperation to capture
interaction dynamics; and high computational efficiency to ensure affordability
and scalability. However, existing simulators and benchmarks fail to
comprehensively meet these fundamental criteria. To bridge this gap, this paper
introduces RealEngine, a novel driving simulation framework that holistically
integrates 3D scene reconstruction and novel view synthesis techniques to
achieve realistic and flexible closed-loop simulation in the driving context.
By leveraging real-world multi-modal sensor data, RealEngine reconstructs
background scenes and foreground traffic participants separately, allowing for
highly diverse and realistic traffic scenarios through flexible scene
composition. This synergistic fusion of scene reconstruction and view synthesis
enables photorealistic rendering across multiple sensor modalities, ensuring
both perceptual fidelity and geometric accuracy. Building upon this
environment, RealEngine supports three essential driving simulation categories:
non-reactive simulation, safety testing, and multi-agent interaction,
collectively forming a reliable and comprehensive benchmark for evaluating the
real-world performance of driving agents.

</details>


### [362] [Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation](https://arxiv.org/pdf/2505.16942)
*Karlis Martins Briedis, Markus Gross, Christopher Schroers*

Main category: cs.CV

TL;DR: Proposes a memory-efficient implementation of all-pairs correlation volume sampling for optical flow estimation, reducing memory usage by 95% and improving speed by 90% compared to on-demand sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optical flow estimation suffer from high computational and memory complexity, often forcing reduced resolutions and loss of fine details.

Method: Introduces an efficient implementation of all-pairs correlation volume sampling, matching RAFT's mathematical operator while optimizing memory and speed.

Result: Achieves up to 90% faster performance than on-demand sampling and 95% lower memory usage, with 50% savings in end-to-end inference time. State-of-the-art results on high-resolution datasets.

Conclusion: The proposed method significantly improves efficiency and accuracy in optical flow estimation, especially for high-resolution applications.

Abstract: Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.

</details>


### [363] [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/pdf/2505.16964)
*Suhao Yu, Haojin Wang, Juncheng Wu, Cihang Xie, Yuyin Zhou*

Main category: cs.CV

TL;DR: The paper introduces MedFrameQA, a benchmark for multi-image reasoning in medical VQA, highlighting the limitations of current models in handling such tasks.


<details>
  <summary>Details</summary>
Motivation: Clinicians often compare multiple images for diagnosis, but existing medical VQA benchmarks focus on single-image analysis. MedFrameQA aims to bridge this gap.

Method: An automated pipeline extracts coherent frames from medical videos, constructs VQA items, and employs a multi-stage filtering strategy for data quality.

Result: Benchmarking ten advanced Multimodal LLMs shows poor performance (below 50% accuracy), with issues like ignoring findings and mis-aggregating evidence.

Conclusion: MedFrameQA highlights the need for improved multi-image reasoning in AI, aiming to advance clinically grounded diagnostic systems.

Abstract: Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.

</details>


### [364] [UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation](https://arxiv.org/pdf/2505.16971)
*Himangi Mittal, Peiye Zhuang, Hsin-Ying Lee, Shubham Tulsiani*

Main category: cs.CV

TL;DR: UniPhy is a neural constitutive model that infers material properties without prior type information, improving accuracy and robustness through shared training.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on user-specified material types or instance-specific networks, limiting flexibility and accuracy. UniPhy aims to overcome these limitations.

Method: UniPhy uses a common latent-conditioned model trained on diverse materials (elastic, plasticine, sand, fluids) via differentiable simulation. It infers properties by optimizing latents to match observations.

Result: UniPhy outperforms prior inverse simulation methods, enabling accurate replay and re-simulation under novel conditions.

Conclusion: UniPhy offers a flexible, accurate approach for inferring material properties and simulating diverse scenarios without prior material type knowledge.

Abstract: We propose UniPhy, a common latent-conditioned neural constitutive model that
can encode the physical properties of diverse materials. At inference UniPhy
allows `inverse simulation' i.e. inferring material properties by optimizing
the scene-specific latent to match the available observations via
differentiable simulation. In contrast to existing methods that treat such
inference as system identification, UniPhy does not rely on user-specified
material type information. Compared to prior neural constitutive modeling
approaches which learn instance specific networks, the shared training across
materials improves both, robustness and accuracy of the estimates. We train
UniPhy using simulated trajectories across diverse geometries and materials --
elastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At
inference, given an object with unknown material properties, UniPhy can infer
the material properties via latent optimization to match the motion
observations, and can then allow re-simulating the object under diverse
scenarios. We compare UniPhy against prior inverse simulation methods, and show
that the inference from UniPhy enables more accurate replay and re-simulation
under novel conditions.

</details>


### [365] [Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation](https://arxiv.org/pdf/2505.16985)
*Moru Liu, Hao Dong, Jessica Kelly, Olga Fink, Mario Trapp*

Main category: cs.CV

TL;DR: Proposes Feature Mixing, a fast and simple method for multimodal OOD detection, and introduces CARLA-OOD dataset. Achieves SOTA performance with significant speedup.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require multimodal OOD detection, but lack supervision for unknown data leads to overconfident predictions.

Method: Feature Mixing, a modality-agnostic method for multimodal outlier synthesis, with theoretical support.

Result: State-of-the-art performance on SemanticKITTI, nuScenes, CARLA-OOD, and MultiOOD, with 10× to 370× speedup.

Conclusion: Feature Mixing is effective and efficient for multimodal OOD detection, supported by a new dataset.

Abstract: Out-of-distribution (OOD) detection and segmentation are crucial for
deploying machine learning models in safety-critical applications such as
autonomous driving and robot-assisted surgery. While prior research has
primarily focused on unimodal image data, real-world applications are
inherently multimodal, requiring the integration of multiple modalities for
improved OOD detection. A key challenge is the lack of supervision signals from
unknown data, leading to overconfident predictions on OOD samples. To address
this challenge, we propose Feature Mixing, an extremely simple and fast method
for multimodal outlier synthesis with theoretical support, which can be further
optimized to help the model better distinguish between in-distribution (ID) and
OOD data. Feature Mixing is modality-agnostic and applicable to various
modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal
dataset for OOD segmentation, featuring synthetic OOD objects across diverse
scenes and weather conditions. Extensive experiments on SemanticKITTI,
nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that
Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370
\times$ speedup. Our source code and dataset will be available at
https://github.com/mona4399/FeatureMixing.

</details>


### [366] [OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning](https://arxiv.org/pdf/2505.16974)
*Zongyan Han, Jiale Cao, Shuo Chen, Tong Wang, Jorma Laaksonen, Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: OpenSeg-R introduces step-by-step visual reasoning for open-vocabulary segmentation, improving accuracy and interpretability by leveraging Large Multimodal Models (LMMs).


<details>
  <summary>Details</summary>
Motivation: Existing OVS methods lack explicit reasoning, making it hard to distinguish similar categories in open-world settings.

Method: OpenSeg-R uses LMMs for hierarchical visual reasoning, generating structured triplets and detailed prompts for segmentation.

Result: Outperforms state-of-the-art methods on five benchmark datasets for semantic and panoptic segmentation.

Conclusion: OpenSeg-R enhances segmentation precision and interpretability, setting a new standard for OVS.

Abstract: Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its
capacity to generalize segmentation beyond predefined categories. However,
existing methods typically predict segmentation masks with simple forward
inference, lacking explicit reasoning and interpretability. This makes it
challenging for OVS model to distinguish similar categories in open-world
settings due to the lack of contextual understanding and discriminative visual
cues. To address this limitation, we propose a step-by-step visual reasoning
framework for open-vocabulary segmentation, named OpenSeg-R. The proposed
OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical
visual reasoning before segmentation. Specifically, we generate both generic
and image-specific reasoning for each image, forming structured triplets that
explain the visual reason for objects in a coarse-to-fine manner. Based on
these reasoning steps, we can compose detailed description prompts, and feed
them to the segmentor to produce more accurate segmentation masks. To the best
of our knowledge, OpenSeg-R is the first framework to introduce explicit
step-by-step visual reasoning into OVS. Experimental results demonstrate that
OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary
semantic segmentation across five benchmark datasets. Moreover, it achieves
consistent gains across all metrics on open-vocabulary panoptic segmentation.
Qualitative results further highlight the effectiveness of our reasoning-guided
framework in improving both segmentation precision and interpretability. Our
code is publicly available at https://github.com/Hanzy1996/OpenSeg-R.

</details>


### [367] [Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding](https://arxiv.org/pdf/2505.16990)
*Runpeng Yu, Xinyin Ma, Xinchao Wang*

Main category: cs.CV

TL;DR: Dimple is the first Discrete Diffusion Multimodal Large Language Model (DMLLM) that combines autoregressive and diffusion training phases, outperforming LLaVA-NEXT by 3.9%. It introduces confident decoding for efficiency and explores structured response control.


<details>
  <summary>Details</summary>
Motivation: Addressing training instability, suboptimal performance, and length bias in purely discrete diffusion models by proposing a hybrid training approach.

Method: Combines initial autoregressive training with a subsequent diffusion phase, introduces confident decoding for efficiency, and explores structured response control via priors.

Result: Dimple-7B surpasses LLaVA-NEXT by 3.9%, reduces generation iterations to response length/3, and achieves 1.5x-7x speedup with prefilling.

Conclusion: Dimple validates DMLLM feasibility, enhances efficiency and controllability, and offers structured response capabilities.

Abstract: In this work, we propose Dimple, the first Discrete Diffusion Multimodal
Large Language Model (DMLLM). We observe that training with a purely discrete
diffusion approach leads to significant training instability, suboptimal
performance, and severe length bias issues. To address these challenges, we
design a novel training paradigm that combines an initial autoregressive phase
with a subsequent diffusion phase. This approach yields the Dimple-7B model,
trained on the same dataset and using a similar training pipeline as
LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,
demonstrating that DMLLM can achieve performance comparable to that of
autoregressive models. To improve inference efficiency, we propose a decoding
strategy termed confident decoding, which dynamically adjusts the number of
tokens generated at each step, significantly reducing the number of generation
iterations. In autoregressive models, the number of forward iterations during
generation equals the response length. With confident decoding, however, the
number of iterations needed by Dimple is even only $\frac{\text{response
length}}{3}$. We also re-implement the prefilling technique in autoregressive
models and demonstrate that it does not significantly impact performance on
most benchmark evaluations, while offering a speedup of 1.5x to 7x.
Additionally, we explore Dimple's capability to precisely control its response
using structure priors. These priors enable structured responses in a manner
distinct from instruction-based or chain-of-thought prompting, and allow
fine-grained control over response format and length, which is difficult to
achieve in autoregressive models. Overall, this work validates the feasibility
and advantages of DMLLM and enhances its inference efficiency and
controllability. Code and models are available at
https://github.com/yu-rp/Dimple.

</details>


### [368] [PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association](https://arxiv.org/pdf/2505.17002)
*Abdul Hannan, Muhammad Arslan Manzoor, Shah Nawaz, Muhammad Irzam Liaqat, Markus Schedl, Mubashir Noman*

Main category: cs.CV

TL;DR: The paper proposes a method to improve face-voice association by aligning embedding spaces and using enhanced gated fusion, addressing issues like negative mining and margin parameter reliance.


<details>
  <summary>Details</summary>
Motivation: The task of learning associations between faces and voices is gaining interest, but current methods suffer from negative mining procedures and reliance on distant margin parameters.

Method: The approach involves learning a joint embedding space with orthogonality constraints, aligning the spaces of faces and voices, and fusing them with enhanced gated fusion.

Result: Extensive experiments on the VoxCeleb dataset show improved performance in face-voice association.

Conclusion: The proposed method effectively addresses the challenges and enhances the performance of face-voice association tasks.

Abstract: We study the task of learning association between faces and voices, which is
gaining interest in the multimodal community lately. These methods suffer from
the deliberate crafting of negative mining procedures as well as the reliance
on the distant margin parameter. These issues are addressed by learning a joint
embedding space in which orthogonality constraints are applied to the fused
embeddings of faces and voices. However, embedding spaces of faces and voices
possess different characteristics and require spaces to be aligned before
fusing them. To this end, we propose a method that accurately aligns the
embedding spaces and fuses them with an enhanced gated fusion thereby improving
the performance of face-voice association. Extensive experiments on the
VoxCeleb dataset reveals the merits of the proposed approach.

</details>


### [369] [An Effective Training Framework for Light-Weight Automatic Speech Recognition Models](https://arxiv.org/pdf/2505.16991)
*Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman*

Main category: cs.CV

TL;DR: A two-step representation learning approach is introduced to create smaller ASR models from a large one, improving performance and training speed without significant degradation.


<details>
  <summary>Details</summary>
Motivation: Large ASR models are impractical for low-resource devices, and existing methods either degrade performance or require prolonged training.

Method: A two-step representation learning approach to derive smaller models from a single large model.

Result: Achieves three-fold training speed-up and up to 12.54% word error rate improvement.

Conclusion: The approach effectively balances performance and resource constraints for ASR deployment.

Abstract: Recent advancement in deep learning encouraged developing large automatic
speech recognition (ASR) models that achieve promising results while ignoring
computational and memory constraints. However, deploying such models on low
resource devices is impractical despite of their favorable performance.
Existing approaches (pruning, distillation, layer skip etc.) transform the
large models into smaller ones at the cost of significant performance
degradation or require prolonged training of smaller models for better
performance. To address these issues, we introduce an efficacious two-step
representation learning based approach capable of producing several small sized
models from a single large model ensuring considerably better performance in
limited number of epochs. Comprehensive experimentation on ASR benchmarks
reveals the efficacy of our approach, achieving three-fold training speed-up
and up to 12.54% word error rate improvement.

</details>


### [370] [Native Segmentation Vision Transformers](https://arxiv.org/pdf/2505.16993)
*Guillem Brasó, Aljoša Ošep, Laura Leal-Taixé*

Main category: cs.CV

TL;DR: Proposes a content-aware spatial grouping layer for vision backbones, enabling hierarchical segmentation without additional heads, leading to efficient and strong zero-shot segmentation.


<details>
  <summary>Details</summary>
Motivation: To replace uniform downsampling with a dynamic, content-aware method for better spatial resolution reduction and native segmentation.

Method: Uses a content-aware spatial grouping layer to dynamically assign tokens based on image boundaries and semantics, stacking it across backbone stages.

Result: Emergence of strong segmentation masks without extra heads, enabling zero-shot segmentation and efficient downstream task design.

Conclusion: Introduces a new paradigm for native, backbone-level segmentation, offering strong performance without mask supervision.

Abstract: Uniform downsampling remains the de facto standard for reducing spatial
resolution in vision backbones. In this work, we propose an alternative design
built around a content-aware spatial grouping layer, that dynamically assigns
tokens to a reduced set based on image boundaries and their semantic content.
Stacking our grouping layer across consecutive backbone stages results in
hierarchical segmentation that arises natively in the feature extraction
process, resulting in our coined Native Segmentation Vision Transformer. We
show that a careful design of our architecture enables the emergence of strong
segmentation masks solely from grouping layers, that is, without additional
segmentation-specific heads. This sets the foundation for a new paradigm of
native, backbone-level segmentation, which enables strong zero-shot results
without mask supervision, as well as a minimal and efficient standalone model
design for downstream segmentation tasks. Our project page is
https://research.nvidia.com/labs/dvl/projects/native-segmentation.

</details>


### [371] [SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding](https://arxiv.org/pdf/2505.17012)
*Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, Weidi Xie*

Main category: cs.CV

TL;DR: The paper introduces VGBench and SpatialScore to evaluate MLLMs' 3D spatial understanding, proposes SpatialAgent for improved reasoning, and highlights persistent challenges in spatial tasks.


<details>
  <summary>Details</summary>
Motivation: To explore and assess the 3D spatial perception capabilities of multimodal large language models (MLLMs), which are less studied despite their success in question-answering tasks.

Method: (i) Introduces VGBench for visual geometry perception; (ii) Proposes SpatialScore, a comprehensive benchmark integrating VGBench with 11 datasets; (iii) Develops SpatialAgent, a multi-agent system with specialized tools for spatial reasoning.

Result: Extensive evaluations reveal persistent challenges in spatial reasoning but demonstrate SpatialAgent's effectiveness.

Conclusion: SpatialScore provides valuable insights and a rigorous benchmark for advancing MLLMs' spatial understanding capabilities.

Abstract: Multimodal large language models (MLLMs) have achieved impressive success in
question-answering tasks, yet their capabilities for spatial understanding are
less explored. This work investigates a critical question: do existing MLLMs
possess 3D spatial perception and understanding abilities? Concretely, we make
the following contributions in this paper: (i) we introduce VGBench, a
benchmark specifically designed to assess MLLMs for visual geometry perception,
e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most
comprehensive and diverse multimodal spatial understanding benchmark to date,
integrating VGBench with relevant data from the other 11 existing datasets.
This benchmark comprises 28K samples across various spatial understanding
tasks, modalities, and QA formats, along with a carefully curated challenging
subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent
system incorporating 9 specialized tools for spatial understanding, supporting
both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive
evaluations to reveal persistent challenges in spatial reasoning while
demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will
offer valuable insights and serve as a rigorous benchmark for the next
evolution of MLLMs.

</details>


### [372] [Seeing through Satellite Images at Street Views](https://arxiv.org/pdf/2505.17001)
*Ming Qian, Bin Tan, Qiuyu Wang, Xianwei Zheng, Hanjiang Xiong, Gui-Song Xia, Yujun Shen, Nan Xue*

Main category: cs.CV

TL;DR: The paper introduces Sat2Density++, a method for synthesizing photorealistic street-view panoramas from satellite images by addressing challenges like sparse-view data and large viewpoint changes.


<details>
  <summary>Details</summary>
Motivation: The task of rendering street-view panoramas from satellite images is challenging due to sparse data and significant viewpoint differences. The paper aims to solve this by leveraging street-view specific elements like sky and illumination effects.

Method: The proposed Sat2Density++ learns a neural radiance field from paired satellite and street-view images, focusing on modeling street-view specific elements in neural networks.

Result: Experiments on urban and suburban datasets show Sat2Density++ successfully renders photorealistic, consistent, and satellite-faithful street-view panoramas.

Conclusion: Sat2Density++ effectively addresses the challenges of street-view synthesis from satellite images, achieving high-quality results.

Abstract: This paper studies the task of SatStreet-view synthesis, which aims to render
photorealistic street-view panorama images and videos given any satellite image
and specified camera positions or trajectories. We formulate to learn neural
radiance field from paired images captured from satellite and street
viewpoints, which comes to be a challenging learning problem due to the
sparse-view natural and the extremely-large viewpoint changes between satellite
and street-view images. We tackle the challenges based on a task-specific
observation that street-view specific elements, including the sky and
illumination effects are only visible in street-view panoramas, and present a
novel approach Sat2Density++ to accomplish the goal of photo-realistic
street-view panoramas rendering by modeling these street-view specific in
neural networks. In the experiments, our method is testified on both urban and
suburban scene datasets, demonstrating that Sat2Density++ is capable of
rendering photorealistic street-view panoramas that are consistent across
multiple views and faithful to the satellite image.

</details>


### [373] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/pdf/2505.17017)
*Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper explores Reinforcement Learning (RL) in autoregressive image generation, comparing GRPO and DPO algorithms, and analyzing reward models' impact on performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of in-depth analysis of RL strategies and domain-specific challenges in autoregressive image generation, particularly for Chain-of-Thought (CoT) reasoning.

Method: Comprehensive investigation of GRPO and DPO algorithms, evaluating in-domain performance, out-of-domain generalization, and the role of reward models. Scaling strategies are also explored.

Result: GRPO and DPO show distinct advantages; reward models with strong generalization enhance RL algorithms. Scaling strategies improve performance for both paradigms.

Conclusion: The study provides insights for developing effective RL algorithms in autoregressive image generation, advancing robust CoT reasoning.

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [374] [CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning](https://arxiv.org/pdf/2505.17006)
*Jiange Yang, Yansong Shi, Haoyi Zhu, Mingyu Liu, Kaijing Ma, Yating Wang, Gangshan Wu, Tong He, Limin Wang*

Main category: cs.CV

TL;DR: CoMo learns continuous motion representations from internet videos, improving generalization and enabling unified policy learning with pseudo actions.


<details>
  <summary>Details</summary>
Motivation: Existing discrete latent action methods lose information and struggle with fine-grained dynamics, limiting their effectiveness for generalist robots.

Method: CoMo uses early temporal feature differences to avoid model collapse and suppress noise, guided by the information bottleneck principle. It introduces new metrics for robust evaluation.

Result: CoMo shows strong zero-shot generalization, enabling pseudo actions for unseen domains and superior policy performance in experiments.

Conclusion: CoMo advances motion learning by providing informative, continuous representations, facilitating better policy training across diverse video datasets.

Abstract: Learning latent motion from Internet videos is crucial for building
generalist robots. However, existing discrete latent action methods suffer from
information loss and struggle with complex and fine-grained dynamics. We
propose CoMo, which aims to learn more informative continuous motion
representations from diverse, internet-scale videos. CoMo employs a early
temporal feature difference mechanism to prevent model collapse and suppress
static appearance noise, effectively discouraging shortcut learning problem.
Furthermore, guided by the information bottleneck principle, we constrain the
latent motion embedding dimensionality to achieve a better balance between
retaining sufficient action-relevant information and minimizing the inclusion
of action-irrelevant appearance noise. Additionally, we also introduce two new
metrics for more robustly and affordably evaluating motion and guiding motion
learning methods development: (i) the linear probing MSE of action prediction,
and (ii) the cosine similarity between past-to-current and future-to-current
motion embeddings. Critically, CoMo exhibits strong zero-shot generalization,
enabling it to generate continuous pseudo actions for previously unseen video
domains. This capability facilitates unified policy joint learning using pseudo
actions derived from various action-less video datasets (such as
cross-embodiment videos and, notably, human demonstration videos), potentially
augmented with limited labeled robot data. Extensive experiments show that
policies co-trained with CoMo pseudo actions achieve superior performance with
both diffusion and autoregressive architectures in simulated and real-world
settings.

</details>


### [375] [Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework](https://arxiv.org/pdf/2505.17019)
*Chenhao Zhang, Yazhe Niu*

Main category: cs.CV

TL;DR: LAD is a novel framework for image implication understanding, outperforming 15+ MLLMs on benchmarks and advancing vision-language reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing AI models struggle with metaphorical comprehension in images due to contextual gaps, limiting their ability to interpret abstract meanings.

Method: LAD uses a three-stage framework: Perception (visual-to-textual conversion), Search (cross-domain knowledge integration), and Reasoning (context-aligned implication generation).

Result: LAD achieves SOTA performance on English and Chinese benchmarks, outperforming GPT-4o-mini and other models, with significant improvements in MCQ and OSQ tasks.

Conclusion: LAD advances AI's ability to interpret image implications, offering insights for vision-language reasoning and human-AI interaction.

Abstract: Metaphorical comprehension in images remains a critical challenge for AI
systems, as existing models struggle to grasp the nuanced cultural, emotional,
and contextual implications embedded in visual content. While multimodal large
language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they
struggle with a fundamental limitation on image implication tasks: contextual
gaps that obscure the relationships between different visual elements and their
abstract meanings. Inspired by the human cognitive process, we propose Let
Androids Dream (LAD), a novel framework for image implication understanding and
reasoning. LAD addresses contextual missing through the three-stage framework:
(1) Perception: converting visual information into rich and multi-level textual
representations, (2) Search: iteratively searching and integrating cross-domain
knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment
image implication via explicit reasoning. Our framework with the lightweight
GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English
image implication benchmark and a huge improvement on Chinese benchmark,
performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ)
and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work
provides new insights into how AI can more effectively interpret image
implications, advancing the field of vision-language reasoning and human-AI
interaction. Our project is publicly available at
https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.

</details>


### [376] [Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space](https://arxiv.org/pdf/2505.17011)
*Yan Li, Changyao Tian, Renqiu Xia, Ning Liao, Weiwei Guo, Junchi Yan, Hongsheng Li, Jifeng Dai, Hao Li, Xue Yang*

Main category: cs.CV

TL;DR: AdapTok is an adaptive video tokenizer that dynamically allocates tokens per frame using content-aware strategies, improving video reconstruction and generation under fixed token budgets.


<details>
  <summary>Details</summary>
Motivation: To enable scalable and token-efficient generative video modeling by dynamically allocating tokens based on video content.

Method: Uses block-wise masking during training, a block causal scorer for reconstruction quality prediction, and adaptive token allocation via integer linear programming during inference.

Result: Improves reconstruction quality and generation performance on UCF-101 and Kinetics-600 without extra data.

Conclusion: AdapTok offers a scalable, content-aware solution for token-efficient video modeling.

Abstract: We propose AdapTok, an adaptive temporal causal video tokenizer that can
flexibly allocate tokens for different frames based on video content. AdapTok
is equipped with a block-wise masking strategy that randomly drops tail tokens
of each block during training, and a block causal scorer to predict the
reconstruction quality of video frames using different numbers of tokens.
During inference, an adaptive token allocation strategy based on integer linear
programming is further proposed to adjust token usage given predicted scores.
Such design allows for sample-wise, content-aware, and temporally dynamic token
allocation under a controllable overall budget. Extensive experiments for video
reconstruction and generation on UCF-101 and Kinetics-600 demonstrate the
effectiveness of our approach. Without additional image data, AdapTok
consistently improves reconstruction quality and generation performance under
different token budgets, allowing for more scalable and token-efficient
generative video modeling.

</details>


### [377] [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/pdf/2505.17015)
*Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, Kevin J. Liang*

Main category: cs.CV

TL;DR: A framework enhances MLLMs with multi-frame spatial understanding using depth, visual correspondence, and dynamic perception, validated by the MultiSPA dataset and benchmark.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack robust multi-frame spatial reasoning, limiting their use in robotics and real-world applications.

Method: Proposes integrating depth perception, visual correspondence, and dynamic perception, supported by the MultiSPA dataset and benchmark.

Result: Multi-SpatialMLLM outperforms baselines and proprietary systems, showing scalable multi-frame reasoning and emergent capabilities.

Conclusion: The framework advances MLLMs for robotics and real-world tasks, with potential as a multi-frame reward annotator.

Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.

</details>


### [378] [SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://arxiv.org/pdf/2505.17018)
*Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, Xiangyu Yue*

Main category: cs.CV

TL;DR: SophiaVL-R1 enhances MLLMs by adding process-based rewards to rule-based RL, improving reasoning and generalization.


<details>
  <summary>Details</summary>
Motivation: Current rule-based RL in MLLMs lacks supervision over the reasoning process, leading to sub-optimal strategies.

Method: Introduces a thinking reward model and Trust-GRPO method to weight rewards, plus an annealing strategy.

Result: Outperforms other MLLMs on benchmarks, even surpassing larger models like LLaVA-OneVision-72B.

Conclusion: SophiaVL-R1 effectively improves reasoning by supervising the thinking process, with public release of resources.

Abstract: Recent advances have shown success in eliciting strong reasoning abilities in
multimodal large language models (MLLMs) through rule-based reinforcement
learning (RL) with outcome rewards. However, this paradigm typically lacks
supervision over the thinking process leading to the final outcome.As a result,
the model may learn sub-optimal reasoning strategies, which can hinder its
generalization ability. In light of this, we propose SophiaVL-R1, as an attempt
to add reward signals for the thinking process in this paradigm. To achieve
this, we first train a thinking reward model that evaluates the quality of the
entire thinking process. Given that the thinking reward may be unreliable for
certain samples due to reward hacking, we propose the Trust-GRPO method, which
assigns a trustworthiness weight to the thinking reward during training. This
weight is computed based on the thinking reward comparison of responses leading
to correct answers versus incorrect answers, helping to mitigate the impact of
potentially unreliable thinking rewards. Moreover, we design an annealing
training strategy that gradually reduces the thinking reward over time,
allowing the model to rely more on the accurate rule-based outcome reward in
later training stages. Experiments show that our SophiaVL-R1 surpasses a series
of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),
demonstrating strong reasoning and generalization capabilities. Notably, our
SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite
the latter having 10 times more parameters. All code, models, and datasets are
made publicly available at https://github.com/kxfan2002/SophiaVL-R1.

</details>


### [379] [CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/pdf/2505.17020)
*Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, Ray Zhang*

Main category: cs.CV

TL;DR: CrossLMM reduces video token complexity in LMMs via dual cross-attention, maintaining performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiency of processing long video sequences in LMMs due to quadratic token costs.

Method: Uses pooling for token reduction and dual cross-attention (visual-to-visual and text-to-visual) to retain information fidelity.

Result: Achieves comparable or superior performance on video-based LMM benchmarks with reduced computational resources.

Conclusion: CrossLMM effectively balances efficiency and performance in multimodal video processing.

Abstract: The advent of Large Multimodal Models (LMMs) has significantly enhanced Large
Language Models (LLMs) to process and interpret diverse data modalities (e.g.,
image and video). However, as input complexity increases, particularly with
long video sequences, the number of required tokens has grown significantly,
leading to quadratically computational costs. This has made the efficient
compression of video tokens in LMMs, while maintaining performance integrity, a
pressing research challenge. In this paper, we introduce CrossLMM, decoupling
long video sequences from LMMs via a dual cross-attention mechanism, which
substantially reduces visual token quantity with minimal performance
degradation. Specifically, we first implement a significant token reduction
from pretrained visual encoders through a pooling methodology. Then, within LLM
layers, we employ a visual-to-visual cross-attention mechanism, wherein the
pooled visual tokens function as queries against the original visual token set.
This module enables more efficient token utilization while retaining
fine-grained informational fidelity. In addition, we introduce a text-to-visual
cross-attention mechanism, for which the text tokens are enhanced through
interaction with the original visual tokens, enriching the visual comprehension
of the text tokens. Comprehensive empirical evaluation demonstrates that our
approach achieves comparable or superior performance across diverse video-based
LMM benchmarks, despite utilizing substantially fewer computational resources.

</details>


### [380] [ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark](https://arxiv.org/pdf/2505.17021)
*Sara Ghaboura, Ketan More, Wafa Alghallabi, Omkar Thawakar, Jorma Laaksonen, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: The paper introduces ARB, the first benchmark for evaluating multimodal reasoning in Arabic, covering 11 domains and 1,356 samples. It highlights challenges in LMMs' coherence, faithfulness, and cultural grounding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating multimodal reasoning in underrepresented languages like Arabic.

Method: Developed ARB, a benchmark with 1,356 multimodal samples and 5,119 reasoning steps, evaluated on 12 LMMs.

Result: Identified persistent challenges in LMMs' reasoning coherence, faithfulness, and cultural grounding.

Conclusion: ARB provides a framework for inclusive, transparent AI systems and supports future research in underrepresented languages.

Abstract: As Large Multimodal Models (LMMs) become more capable, there is growing
interest in evaluating their reasoning processes alongside their final outputs.
However, most benchmarks remain focused on English, overlooking languages with
rich linguistic and cultural contexts, such as Arabic. To address this gap, we
introduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the
first benchmark designed to evaluate step-by-step reasoning in Arabic across
both textual and visual modalities. ARB spans 11 diverse domains, including
visual reasoning, document understanding, OCR, scientific analysis, and
cultural interpretation. It comprises 1,356 multimodal samples paired with
5,119 human-curated reasoning steps and corresponding actions. We evaluated 12
state-of-the-art open- and closed-source LMMs and found persistent challenges
in coherence, faithfulness, and cultural grounding. ARB offers a structured
framework for diagnosing multimodal reasoning in underrepresented languages and
marks a critical step toward inclusive, transparent, and culturally aware AI
systems. We release the benchmark, rubric, and evaluation suit to support
future research and reproducibility. Code available at:
https://github.com/mbzuai-oryx/ARB

</details>


### [381] [SegMatch: A semi-supervised learning method for surgical instrument segmentation](https://arxiv.org/pdf/2308.05232)
*Meng Wei, Charlie Budd, Luis C. Garcia-Peraza-Herrera, Reuben Dorent, Miaojing Shi, Tom Vercauteren*

Main category: cs.CV

TL;DR: SegMatch, a semi-supervised learning method, reduces annotation costs for surgical instrument segmentation by combining consistency regularization and pseudo-labeling, outperforming fully-supervised and other semi-supervised models.


<details>
  <summary>Details</summary>
Motivation: To reduce the need for expensive annotation in surgical image segmentation while maintaining high performance.

Method: Adapts FixMatch for segmentation, using weak and strong augmentation branches with consistency loss and adversarial augmentation.

Result: Outperforms fully-supervised approaches and state-of-the-art semi-supervised models on MICCAI and CholecInstanceSeg datasets.

Conclusion: SegMatch effectively leverages unlabelled data to improve surgical instrument segmentation, reducing reliance on costly annotations.

Abstract: Surgical instrument segmentation is recognised as a key enabler in providing
advanced surgical assistance and improving computer-assisted interventions. In
this work, we propose SegMatch, a semi-supervised learning method to reduce the
need for expensive annotation for laparoscopic and robotic surgical images.
SegMatch builds on FixMatch, a widespread semi supervised classification
pipeline combining consistency regularization and pseudo-labelling, and adapts
it for the purpose of segmentation. In our proposed SegMatch, the unlabelled
images are first weakly augmented and fed to the segmentation model to generate
pseudo-labels. In parallel, images are fed to a strong augmentation branch and
consistency between the branches is used as an unsupervised loss. To increase
the relevance of our strong augmentations, we depart from using only
handcrafted augmentations and introduce a trainable adversarial augmentation
strategy. Our FixMatch adaptation for segmentation tasks further includes
carefully considering the equivariance and invariance properties of the
augmentation functions we rely on. For binary segmentation tasks, our algorithm
was evaluated on the MICCAI Instrument Segmentation Challenge datasets,
Robust-MIS 2019 and EndoVis 2017. For multi-class segmentation tasks, we relied
on the recent CholecInstanceSeg dataset. Our results show that SegMatch
outperforms fully-supervised approaches by incorporating unlabelled data, and
surpasses a range of state-of-the-art semi-supervised models across different
labelled to unlabelled data ratios.

</details>


### [382] [Maximizing Discrimination Capability of Knowledge Distillation with Energy Function](https://arxiv.org/pdf/2311.14334)
*Seonghak Kim, Gyeongdo Ham, Suin Lee, Donggon Jang, Daeshik Kim*

Main category: cs.CV

TL;DR: The paper introduces Energy KD, a knowledge distillation method that classifies samples by energy scores to optimize temperature scaling, outperforming existing methods. It also proposes HE-DA for efficient data augmentation.


<details>
  <summary>Details</summary>
Motivation: To improve knowledge distillation by addressing the limitation of constant temperature scaling, leveraging sample-specific energy scores for better knowledge utilization.

Method: Classifies samples into low/high energy categories, applies adaptive temperature scaling, and introduces HE-DA for selective data augmentation.

Result: Energy KD outperforms logit-based and feature-based methods, especially on challenging datasets like CIFAR-100-LT and ImageNet. HE-DA further enhances performance efficiently.

Conclusion: Energy KD and HE-DA advance knowledge distillation and data augmentation by leveraging energy scores, offering practical benefits for resource-limited applications.

Abstract: To apply the latest computer vision techniques that require a large
computational cost in real industrial applications, knowledge distillation
methods (KDs) are essential. Existing logit-based KDs apply the constant
temperature scaling to all samples in dataset, limiting the utilization of
knowledge inherent in each sample individually. In our approach, we classify
the dataset into two categories (i.e., low energy and high energy samples)
based on their energy score. Through experiments, we have confirmed that low
energy samples exhibit high confidence scores, indicating certain predictions,
while high energy samples yield low confidence scores, meaning uncertain
predictions. To distill optimal knowledge by adjusting non-target class
predictions, we apply a higher temperature to low energy samples to create
smoother distributions and a lower temperature to high energy samples to
achieve sharper distributions. When compared to previous logit-based and
feature-based methods, our energy-based KD (Energy KD) achieves better
performance on various datasets. Especially, Energy KD shows significant
improvements on CIFAR-100-LT and ImageNet datasets, which contain many
challenging samples. Furthermore, we propose high energy-based data
augmentation (HE-DA) for further improving the performance. We demonstrate that
higher performance improvement could be achieved by augmenting only a portion
of the dataset rather than the entire dataset, suggesting that it can be
employed on resource-limited devices. To the best of our knowledge, this paper
represents the first attempt to make use of energy function in knowledge
distillation and data augmentation, and we believe it will greatly contribute
to future research.

</details>


### [383] [Leveraging Habitat Information for Fine-grained Bird Identification](https://arxiv.org/pdf/2312.14999)
*Tin Nguyen, Peijie Chen, Anh Totti Nguyen*

Main category: cs.CV

TL;DR: The paper explores integrating habitat information into bird classifiers, improving accuracy for CNNs, ViTs, and CLIP models.


<details>
  <summary>Details</summary>
Motivation: Traditional bird classifiers ignore habitat, a key cue for ornithologists. This work aims to leverage habitat data for better classification.

Method: Habitat-augmented data is used to train CNNs/ViTs, and habitat descriptors are added to CLIP prompts.

Result: Accuracy improved by up to +0.83 (NABirds) and +0.23 (CUB-200) for CNNs/ViTs, and +0.99 (NABirds) and +1.1 (CUB-200) for CLIP.

Conclusion: Integrating habitat features consistently boosts classifier accuracy, validating its importance in bird identification.

Abstract: Traditional bird classifiers mostly rely on the visual characteristics of
birds. Some prior works even train classifiers to be invariant to the
background, completely discarding the living environment of birds. Instead, we
are the first to explore integrating habitat information, one of the four major
cues for identifying birds by ornithologists, into modern bird classifiers. We
focus on two leading model types: (1) CNNs and ViTs trained on the downstream
bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with
habitat-augmented data results in an improvement of up to +0.83 and +0.23
points on NABirds and CUB-200, respectively. Similarly, adding habitat
descriptors to the prompts for CLIP yields a substantial accuracy boost of up
to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find
consistent accuracy improvement after integrating habitat features into the
image augmentation process and into the textual descriptors of vision-language
CLIP classifiers. Code is available at:
https://anonymous.4open.science/r/reasoning-8B7E/.

</details>


### [384] [Fast Sampling Through The Reuse Of Attention Maps In Diffusion Models](https://arxiv.org/pdf/2401.01008)
*Rosco Hunter, Łukasz Dudziak, Mohamed S. Abdelfattah, Abhinav Mehrotra, Sourav Bhattacharya, Hongkai Wen*

Main category: cs.CV

TL;DR: The paper proposes reusing attention maps in diffusion models to reduce latency without retraining, outperforming few-step sampling methods.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are slow due to repeated attention map calculations, and existing efficiency improvements require additional training.

Method: Reuses attention maps during sampling based on ODE theory, with strategies to minimize distortion.

Result: Reuse strategies generate images closer to the original high-latency model compared to few-step sampling.

Conclusion: Directly reusing attention maps is an effective way to reduce latency without compromising image quality.

Abstract: Text-to-image diffusion models have demonstrated unprecedented capabilities
for flexible and realistic image synthesis. Nevertheless, these models rely on
a time-consuming sampling procedure, which has motivated attempts to reduce
their latency. When improving efficiency, researchers often use the original
diffusion model to train an additional network designed specifically for fast
image generation. In contrast, our approach seeks to reduce latency directly,
without any retraining, fine-tuning, or knowledge distillation. In particular,
we find the repeated calculation of attention maps to be costly yet redundant,
and instead suggest reusing them during sampling. Our specific reuse strategies
are based on ODE theory, which implies that the later a map is reused, the
smaller the distortion in the final image. We empirically compare our reuse
strategies with few-step sampling procedures of comparable latency, finding
that reuse generates images that are closer to those produced by the original
high-latency diffusion model.

</details>


### [385] [A Review of Pseudo-Labeling for Computer Vision](https://arxiv.org/pdf/2408.07221)
*Patrick Kage, Jay C. Rothenberger, Pavlos Andreadis, Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: The paper explores pseudo-labeling in semi-supervised, self-supervised, and unsupervised learning, identifying cross-domain benefits.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks require large labeled datasets; semi-supervised learning leverages unlabeled data, with pseudo-labeling as a key method.

Method: The study broadens pseudo-labeling's scope to self-supervised and unsupervised methods, linking these areas.

Result: Connections between pseudo-labeling methods reveal potential advancements like curriculum learning and self-supervised regularization.

Conclusion: Expanding pseudo-labeling's application can drive progress across learning paradigms.

Abstract: Deep neural models have achieved state of the art performance on a wide range
of problems in computer science, especially in computer vision. However, deep
neural networks often require large datasets of labeled samples to generalize
effectively, and an important area of active research is semi-supervised
learning, which attempts to instead utilize large quantities of (easily
acquired) unlabeled samples. One family of methods in this space is
pseudo-labeling, a class of algorithms that use model outputs to assign labels
to unlabeled samples which are then used as labeled samples during training.
Such assigned labels, called pseudo-labels, are most commonly associated with
the field of semi-supervised learning. In this work we explore a broader
interpretation of pseudo-labels within both self-supervised and unsupervised
methods. By drawing the connection between these areas we identify new
directions when advancements in one area would likely benefit others, such as
curriculum learning and self-supervised regularization.

</details>


### [386] [Depth-Weighted Detection of Behaviours of Risk in People with Dementia using Cameras](https://arxiv.org/pdf/2408.15519)
*Pratik K. Mishra, Irene Ballester, Andrea Iaboni, Bing Ye, Kristine Newman, Alex Mihailidis, Shehroz S. Khan*

Main category: cs.CV

TL;DR: The paper proposes a depth-weighted loss method to reduce false alarms in automated dementia behavior detection using video cameras, achieving improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing false alarms in dementia behavior monitoring due to distance disparities, aiming for timely staff intervention.

Method: Proposed a depth-weighted loss for equal event importance and used training outliers for anomaly threshold. Tested on data from nine dementia participants.

Result: Achieved AUC scores of 0.852, 0.81, and 0.768 for three cameras, with reduced false alarms.

Conclusion: The approach shows promise for deployment in care facilities, warranting further research.

Abstract: The behavioural and psychological symptoms of dementia, such as agitation and
aggression, present a significant health and safety risk in residential care
settings. Many care facilities have video cameras in place for digital
monitoring of public spaces, which can be leveraged to develop an automated
behaviours of risk detection system that can alert the staff to enable timely
intervention and prevent the situation from escalating. However, one of the
challenges in our previous study was the presence of false alarms due to
disparate importance of events based on distance. To address this issue, we
proposed a novel depth-weighted loss to enforce equivalent importance to the
events happening both near and far from the cameras; thus, helping to reduce
false alarms. We further propose to utilize the training outliers to determine
the anomaly threshold. The data from nine dementia participants across three
cameras in a specialized dementia unit were used for training. The proposed
approach obtained the best area under receiver operating characteristic curve
performance of 0.852, 0.81 and 0.768, respectively, for the three cameras.
Ablation analysis was conducted for the individual components of the proposed
approach and effect of frame size and frame rate. The performance of the
proposed approach was investigated for cross-camera, participant-specific and
sex-specific behaviours of risk detection. The proposed approach performed
reasonably well in reducing false alarms. This motivates further research to
make the system more suitable for deployment in care facilities.

</details>


### [387] [More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding](https://arxiv.org/pdf/2408.15966)
*Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen*

Main category: cs.CV

TL;DR: GreenPLM enables LLMs to understand 3D objects with minimal 3D-text data by leveraging text data and a novel training strategy.


<details>
  <summary>Details</summary>
Motivation: The challenge of enabling LLMs to comprehend 3D worlds due to limited 3D-text datasets.

Method: Uses a pre-trained point cloud-text encoder, generates 6M text descriptions, and employs a three-stage training strategy with a zero-parameter cross-attention module.

Result: Achieves superior 3D understanding with only 12% of the 3D data used by SOTA models and performs well with text-only data.

Conclusion: GreenPLM effectively bridges the gap between 3D and text modalities, reducing reliance on large 3D datasets.

Abstract: Enabling Large Language Models (LLMs) to comprehend the 3D physical world
remains a significant challenge. Due to the lack of large-scale 3D-text pair
datasets, the success of LLMs has yet to be replicated in 3D understanding. In
this paper, we rethink this issue and propose a new task: 3D Data-Efficient
Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D
object understanding with minimal 3D point cloud and text data pairs. To
address this task, we introduce GreenPLM, which leverages more text data to
compensate for the lack of 3D data. First, inspired by using CLIP to align
images and text, we utilize a pre-trained point cloud-text encoder to map the
3D point cloud space to the text space. This mapping leaves us to seamlessly
connect the text space with LLMs. Once the point-text-LLM connection is
established, we further enhance text-LLM alignment by expanding the
intermediate text space, thereby reducing the reliance on 3D point cloud data.
Specifically, we generate 6M free-text descriptions of 3D objects, and design a
three-stage training strategy to help LLMs better explore the intrinsic
connections between different modalities. To achieve efficient modality
alignment, we design a zero-parameter cross-attention module for token pooling.
Extensive experimental results show that GreenPLM requires only 12% of the 3D
training data used by existing state-of-the-art models to achieve superior 3D
understanding. Remarkably, GreenPLM also achieves competitive performance using
text-only data. The code and weights are available at:
https://github.com/TangYuan96/GreenPLM.

</details>


### [388] [GeoBiked: A Dataset with Geometric Features and Automated Labeling Techniques to Enable Deep Generative Models in Engineering Design](https://arxiv.org/pdf/2409.17045)
*Phillip Mueller, Sebastian Mueller, Lars Mikelsons*

Main category: cs.CV

TL;DR: The paper introduces GeoBiked, a dataset for Deep Generative Models in engineering design, and explores automated labeling using foundation models like GPT-4o and Diffusion-Hyperfeatures.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in applying foundation models to engineering design by providing a dataset and methods for automated data labeling.

Method: Uses Hyperfeatures for geometric correspondences and GPT-4o for text descriptions, balancing creativity and accuracy.

Result: Improved geometric point detection and accurate text descriptions, though hallucinations occur without grounding.

Conclusion: Foundation models can automate labeling in engineering design, but require careful prompt-engineering and input selection.

Abstract: We provide a dataset for enabling Deep Generative Models (DGMs) in
engineering design and propose methods to automate data labeling by utilizing
large-scale foundation models. GeoBiked is curated to contain 4 355 bicycle
images, annotated with structural and technical features and is used to
investigate two automated labeling techniques: The utilization of consolidated
latent features (Hyperfeatures) from image-generation models to detect
geometric correspondences (e.g. the position of the wheel center) in structural
images and the generation of diverse text descriptions for structural images.
GPT-4o, a vision-language-model (VLM), is instructed to analyze images and
produce diverse descriptions aligned with the system-prompt. By representing
technical images as Diffusion-Hyperfeatures, drawing geometric correspondences
between them is possible. The detection accuracy of geometric points in unseen
samples is improved by presenting multiple annotated source images. GPT-4o has
sufficient capabilities to generate accurate descriptions of technical images.
Grounding the generation only on images leads to diverse descriptions but
causes hallucinations, while grounding it on categorical labels restricts the
diversity. Using both as input balances creativity and accuracy. Successfully
using Hyperfeatures for geometric correspondence suggests that this approach
can be used for general point-detection and annotation tasks in technical
images. Labeling such images with text descriptions using VLMs is possible, but
dependent on the models detection capabilities, careful prompt-engineering and
the selection of input information. Applying foundation models in engineering
design is largely unexplored. We aim to bridge this gap with a dataset to
explore training, finetuning and conditioning DGMs in this field and suggesting
approaches to bootstrap foundation models to process technical images.

</details>


### [389] [When LLMs Learn to be Students: The SOEI Framework for Modeling and Evaluating Virtual Student Agents in Educational Interaction](https://arxiv.org/pdf/2410.15701)
*Yiping Ma, Shiyu Hu, Xuchen Li, Yipei Wang, Yuqing Chen, Shiqing Liu, Kang Hao Cheong*

Main category: cs.CV

TL;DR: The paper introduces the SOEI framework for developing and evaluating personality-aligned Large Language Model-based Virtual Student Agents (LVSAs) in classroom settings, demonstrating their pedagogical utility.


<details>
  <summary>Details</summary>
Motivation: To address gaps in principled personality modeling, scalable evaluation, and empirical validation of LVSAs for adaptive teaching and pedagogical skill development.

Method: Proposes the SOEI framework (Scene, Object, Evaluation, Interaction) for constructing and evaluating LVSAs, using LoRA fine-tuning and expert-informed prompts to model Big Five traits. Hybrid human & GPT-4 evaluation assesses behavioral realism.

Result: LVSAs elicited adaptive teaching strategies and maintained trait-consistent behavior in multi-turn dialogues, validated through controlled experiments with pre-service teachers.

Conclusion: SOEI bridges AI for Education and Education for AI, offering a scalable pipeline for personality-aligned LVSAs and insights into their pedagogical utility.

Abstract: Recent advances in large language models (LLMs) have enabled intelligent
tutoring systems, yet the development of LLM-based Virtual Student Agents
(LVSAs) remains underexplored. Such agents are essential for teacher-facing
applications, where simulating diverse learner traits can support adaptive
instruction and pedagogical skill development. However, current methods lack
principled personality modeling, scalable evaluation of behavioral consistency,
and empirical validation in interactive teaching settings. We propose the SOEI
framework, a structured pipeline comprising Scene, Object, Evaluation, and
Interaction, for constructing and evaluating personality-aligned LVSAs in
classroom scenarios. Leveraging Chinese language instruction as a cognitively
and emotionally rich testbed, we generate five LVSAs based on Big Five traits
through LoRA fine-tuning and expert-informed prompt design. Their behavioral
realism and personality coherence are assessed using a hybrid human & GPT-4
evaluation and a multi-dimensional annotation protocol. Through controlled
experiments with real pre-service teachers, we demonstrate that LVSAs can
elicit adaptive teaching strategies and maintain trait-consistent behavior
across multi-turn dialogues. Our results provide: (1) an educationally and
psychologically grounded generation pipeline for LLM-based student agents; (2)
a hybrid, scalable evaluation framework for behavioral realism; and (3)
empirical insights into the pedagogical utility of LVSAs in shaping
instructional adaptation. By embedding LVSAs into both generative modeling and
human-in-the-loop teaching, SOEI bridges AI for Education (AI4Edu) and
Education for AI (Edu4AI), positioning classroom interaction as a rigorous
testbed for controllability, personality alignment, and human-likeness in large
language models.

</details>


### [390] [GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing](https://arxiv.org/pdf/2410.19552)
*Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Yasser Ashraf, Mohsen Guizani*

Main category: cs.CV

TL;DR: The paper introduces a dataset and fine-tuning techniques to improve vision-language models (VLMs) for detecting temporal changes in geographical landscapes, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with temporal dynamics in remote sensing data, which is crucial for applications like environmental monitoring and urban planning.

Method: The authors use annotated video frame pairs and fine-tuning techniques (LoRA, QLoRA, model pruning) on models like Video-LLaVA and LLaVA-NeXT-Video.

Result: Significant improvements were achieved, with a BERT score of 0.864 and ROUGE-1 score of 0.576, showing superior accuracy in describing land-use changes.

Conclusion: The proposed approach effectively enhances VLM performance for tracking geographical patterns over time.

Abstract: Detecting temporal changes in geographical landscapes is critical for
applications like environmental monitoring and urban planning. While remote
sensing data is abundant, existing vision-language models (VLMs) often fail to
capture temporal dynamics effectively. This paper addresses these limitations
by introducing an annotated dataset of video frame pairs to track evolving
geographical patterns over time. Using fine-tuning techniques like Low-Rank
Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as
Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in
processing remote sensing temporal changes. Results show significant
improvements, with the best performance achieving a BERT score of 0.864 and
ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use
transformations.

</details>


### [391] [Feature Map Similarity Reduction in Convolutional Neural Networks](https://arxiv.org/pdf/2411.03226)
*Zakariae Belmekki, Jun Li, Patrick Reuter, David Antonio Gómez Jáuregui, Karl Jenkins*

Main category: cs.CV

TL;DR: The paper challenges the effectiveness of kernel orthogonality in reducing feature map redundancy in CNNs and proposes the Convolutional Similarity method to address this issue, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: CNNs suffer from feature map redundancy, and existing solutions like kernel orthogonality are ineffective. The paper aims to find a better method to reduce redundancy.

Method: Proposes Convolutional Similarity, a method to reduce feature map similarity, usable as regularization or iterative initialization.

Result: Minimizing Convolutional Similarity improves accuracy, speeds convergence, and allows smaller models for the same performance.

Conclusion: The method enhances CNN efficiency and performance, with future work exploring its integration with optimization momentum and generative frameworks.

Abstract: It has been observed that Convolutional Neural Networks (CNNs) suffer from
redundancy in feature maps, leading to inefficient capacity utilization.
Efforts to address this issue have largely focused on kernel orthogonality
method. In this work, we theoretically and empirically demonstrate that kernel
orthogonality does not necessarily lead to a reduction in feature map
redundancy. Based on this analysis, we propose the Convolutional Similarity
method to reduce feature map similarity, independently of the CNN's input. The
Convolutional Similarity can be minimized as either a regularization term or an
iterative initialization method. Experimental results show that minimizing
Convolutional Similarity not only improves classification accuracy but also
accelerates convergence. Furthermore, our method enables the use of
significantly smaller models to achieve the same level of performance,
promoting a more efficient use of model capacity. Future work will focus on
coupling the iterative initialization method with the optimization momentum
term and examining the method's impact on generative frameworks.

</details>


### [392] [TRACE: Transformer-based Risk Assessment for Clinical Evaluation](https://arxiv.org/pdf/2411.08701)
*Dionysis Christopoulos, Sotiris Spanos, Valsamis Ntouskos, Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: TRACE is a Transformer-based method for clinical risk assessment, handling diverse data modalities and outperforming baselines while offering interpretable results.


<details>
  <summary>Details</summary>
Motivation: To improve clinical risk assessment by leveraging Transformer self-attention for better feature interaction and interpretability.

Method: Integrates specialized embeddings for different data modalities (continuous, categorical, checkbox) and uses Transformer encoder layers for risk detection.

Result: Outperforms non-negative MLP baselines and handles missing values effectively.

Conclusion: TRACE enhances clinical decision-making with interpretable results via attention weights.

Abstract: We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),
a novel method for clinical risk assessment based on clinical data, leveraging
the self-attention mechanism for enhanced feature interaction and result
interpretation. Our approach is able to handle different data modalities,
including continuous, categorical and multiple-choice (checkbox) attributes.
The proposed architecture features a shared representation of the clinical data
obtained by integrating specialized embeddings of each data modality, enabling
the detection of high-risk individuals using Transformer encoder layers. To
assess the effectiveness of the proposed method, a strong baseline based on
non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method
outperforms various baselines widely used in the domain of clinical risk
assessment, while effectively handling missing values. In terms of
explainability, our Transformer-based method offers easily interpretable
results via attention weights, further enhancing the clinicians'
decision-making process.

</details>


### [393] [KAN-Mamba FusionNet: Redefining Medical Image Segmentation with Non-Linear Modeling](https://arxiv.org/pdf/2411.11926)
*Akansh Agrawal, Akshan Agrawal, Shashwat Gupta, Priyanka Bagade*

Main category: cs.CV

TL;DR: The paper introduces KAN-Mamba FusionNet, a novel architecture combining KANs and Mamba to improve medical image segmentation by capturing non-linearities and long-range dependencies, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing models like KANs and Mamba have limitations in handling long-range dependencies and non-linearities, respectively, which are crucial for accurate medical image segmentation.

Method: Proposes KAN-Mamba FusionNet with a KAMBA block to combine strengths of KANs (non-linearity) and Mamba (long-range dependencies). Evaluated on BUSI, Kvasir-Seg, and GlaS datasets.

Result: Outperforms state-of-the-art methods in IoU and F1 scores. Ablation studies confirm the contributions of model components.

Conclusion: KAN-Mamba FusionNet effectively addresses challenges in medical image segmentation, offering a reliable solution for healthcare applications.

Abstract: Medical image segmentation is essential for applications like robotic
surgeries, disease diagnosis, and treatment planning. Recently, various
deep-learning models have been proposed to enhance medical image segmentation.
One promising approach utilizes Kolmogorov-Arnold Networks (KANs), which better
capture non-linearity in input data. However, they are unable to effectively
capture long-range dependencies, which are required to accurately segment
complex medical images and, by that, improve diagnostic accuracy in clinical
settings. Neural networks such as Mamba can handle long-range dependencies.
However, they have a limited ability to accurately capture non-linearities in
the images as compared to KANs. Thus, we propose a novel architecture, the
KAN-Mamba FusionNet, which improves segmentation accuracy by effectively
capturing the non-linearities from input and handling long-range dependencies
with the newly proposed KAMBA block. We evaluated the proposed KAN-Mamba
FusionNet on three distinct medical image segmentation datasets: BUSI,
Kvasir-Seg, and GlaS - and found it consistently outperforms state-of-the-art
methods in IoU and F1 scores. Further, we examined the effects of various
components and assessed their contributions to the overall model performance
via ablation studies. The findings highlight the effectiveness of this
methodology for reliable medical image segmentation, providing a unique
approach to address intricate visual data issues in healthcare.

</details>


### [394] [VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving](https://arxiv.org/pdf/2411.14716)
*Haiming Zhang, Wending Zhou, Yiyao Zhu, Xu Yan, Jiantao Gao, Dongfeng Bai, Yingjie Cai, Bingbing Liu, Shuguang Cui, Zhen Li*

Main category: cs.CV

TL;DR: VisionPAD is a self-supervised pre-training method for autonomous driving vision tasks, using 3D Gaussian Splatting and multi-view reconstruction without explicit depth supervision. It outperforms existing methods in 3D object detection, occupancy prediction, and map segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on explicit depth supervision, which is inefficient. VisionPAD aims to leverage self-supervision for better efficiency and performance in vision-centric autonomous driving tasks.

Method: Uses 3D Gaussian Splatting for multi-view reconstruction and introduces voxel velocity estimation via self-supervision. Enhances geometric perception with multi-frame photometric consistency.

Result: Outperforms state-of-the-art pre-training methods in 3D object detection, occupancy prediction, and map segmentation on autonomous driving datasets.

Conclusion: VisionPAD demonstrates the effectiveness of self-supervised pre-training for vision-centric autonomous driving tasks, offering superior performance without explicit depth supervision.

Abstract: This paper introduces VisionPAD, a novel self-supervised pre-training
paradigm designed for vision-centric algorithms in autonomous driving. In
contrast to previous approaches that employ neural rendering with explicit
depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to
reconstruct multi-view representations using only images as supervision.
Specifically, we introduce a self-supervised method for voxel velocity
estimation. By warping voxels to adjacent frames and supervising the rendered
outputs, the model effectively learns motion cues in the sequential data.
Furthermore, we adopt a multi-frame photometric consistency approach to enhance
geometric perception. It projects adjacent frames to the current frame based on
rendered depths and relative poses, boosting the 3D geometric representation
through pure image supervision. Extensive experiments on autonomous driving
datasets demonstrate that VisionPAD significantly improves performance in 3D
object detection, occupancy prediction and map segmentation, surpassing
state-of-the-art pre-training strategies by a considerable margin.

</details>


### [395] [Remote Sensing Spatio-Temporal Vision-Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2412.02573)
*Chenyang Liu, Jiafan Zhang, Keyan Chen, Man Wang, Zhengxia Zou, Zhenwei Shi*

Main category: cs.CV

TL;DR: A survey on Remote Sensing Spatio-Temporal Vision-Language Models (RS-STVLMs) reviewing their evolution, tasks, components, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: Existing change detection methods lack human-readable insights; VLMs offer richer semantic analysis of temporal images.

Method: Comprehensive review of RS-STVLMs, covering model evolution, tasks (e.g., change captioning), components, datasets, and metrics.

Result: Synthesis of current achievements and identification of key technologies and shared architectural patterns.

Conclusion: Highlights future research directions for spatio-temporal vision-language understanding in remote sensing.

Abstract: The interpretation of multi-temporal remote sensing imagery is critical for
monitoring Earth's dynamic processes-yet previous change detection methods,
which produce binary or semantic masks, fall short of providing human-readable
insights into changes. Recent advances in Vision-Language Models (VLMs) have
opened a new frontier by fusing visual and linguistic modalities, enabling
spatio-temporal vision-language understanding: models that not only capture
spatial and temporal dependencies to recognize changes but also provide a
richer interactive semantic analysis of temporal images (e.g., generate
descriptive captions and answer natural-language queries). In this survey, we
present the first comprehensive review of RS-STVLMs. The survey covers the
evolution of models from early task-specific models to recent general
foundation models that leverage powerful large language models. We discuss
progress in representative tasks, such as change captioning, change question
answering, and change grounding. Moreover, we systematically dissect the
fundamental components and key technologies underlying these models, and review
the datasets and evaluation metrics that have driven the field. By synthesizing
task-level insights with a deep dive into shared architectural patterns, we aim
to illuminate current achievements and chart promising directions for future
research in spatio-temporal vision-language understanding for remote sensing.
We will keep tracing related works at
https://github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs

</details>


### [396] [Mask of truth: model sensitivity to unexpected regions of medical images](https://arxiv.org/pdf/2412.04030)
*Théo Sourget, Michelle Hestbek-Møller, Amelia Jiménez-Sánchez, Jack Junchi Xu, Veronika Cheplygina*

Main category: cs.CV

TL;DR: CNNs can classify medical images even when clinically relevant parts are masked, revealing potential spurious correlations and raising concerns about model reliability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether CNNs rely on clinically irrelevant features (shortcuts) in medical image analysis, potentially compromising real-world applicability.

Method: Train CNNs on masked chest X-rays and eye fundus images, evaluate performance (AUC), and use SHAP and embedding analysis for explainability. Clinical validation was done by a radiology resident.

Result: Models perform well even without clinically relevant regions, sometimes better than with them, indicating reliance on shortcuts. A spurious correlation was found in one dataset.

Conclusion: CNNs may exploit non-relevant features, questioning their reliability for medical diagnosis. Explainability tools and clinical input are crucial for validation.

Abstract: The development of larger models for medical image analysis has led to
increased performance. However, it also affected our ability to explain and
validate model decisions. Models can use non-relevant parts of images, also
called spurious correlations or shortcuts, to obtain high performance on
benchmark datasets but fail in real-world scenarios. In this work, we challenge
the capacity of convolutional neural networks (CNN) to classify chest X-rays
and eye fundus images while masking out clinically relevant parts of the image.
We show that all models trained on the PadChest dataset, irrespective of the
masking strategy, are able to obtain an Area Under the Curve (AUC) above
random. Moreover, the models trained on full images obtain good performance on
images without the region of interest (ROI), even superior to the one obtained
on images only containing the ROI. We also reveal a possible spurious
correlation in the Chaksu dataset while the performances are more aligned with
the expectation of an unbiased model. We go beyond the performance analysis
with the usage of the explainability method SHAP and the analysis of
embeddings. We asked a radiology resident to interpret chest X-rays under
different masking to complement our findings with clinical knowledge. Our code
is available at https://github.com/TheoSourget/MMC_Masking and
https://github.com/TheoSourget/MMC_Masking_EyeFundus

</details>


### [397] [Motion by Queries: Identity-Motion Trade-offs in Text-to-Video Generation](https://arxiv.org/pdf/2412.07750)
*Yuval Atzmon, Rinon Gal, Yoad Tewel, Yoni Kasten, Gal Chechik*

Main category: cs.CV

TL;DR: The paper explores how self-attention query (Q) features in text-to-video diffusion models influence motion, structure, and identity, revealing challenges in disentangling these effects. It proposes Q injection control for efficient motion transfer and consistent multi-shot video generation.


<details>
  <summary>Details</summary>
Motivation: The interplay between motion, structure, and identity in text-to-video models is under-explored, prompting investigation into Q features' dual role.

Method: Analyzes Q features' impact on layout and identity during denoising, then introduces Q injection control for motion transfer and multi-shot consistency.

Result: Develops a zero-shot motion transfer method 10x more efficient than existing approaches and a training-free technique for identity-consistent multi-shot videos.

Conclusion: Understanding Q features' dual role enables practical applications like motion transfer and multi-shot generation, advancing text-to-video model capabilities.

Abstract: Text-to-video diffusion models have shown remarkable progress in generating
coherent video clips from textual descriptions. However, the interplay between
motion, structure, and identity representations in these models remains
under-explored. Here, we investigate how self-attention query (Q) features
simultaneously govern motion, structure, and identity and examine the
challenges arising when these representations interact. Our analysis reveals
that Q affects not only layout, but that during denoising Q also has a strong
effect on subject identity, making it hard to transfer motion without the
side-effect of transferring identity. Understanding this dual role enabled us
to control query feature injection (Q injection) and demonstrate two
applications: (1) a zero-shot motion transfer method - implemented with
VideoCrafter2 and WAN 2.1 - that is 10 times more efficient than existing
approaches, and (2) a training-free technique for consistent multi-shot video
generation, where characters maintain identity across multiple video shots
while Q injection enhances motion fidelity.

</details>


### [398] [ErasableMask: A Robust and Erasable Privacy Protection Scheme against Black-box Face Recognition Models](https://arxiv.org/pdf/2412.17038)
*Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Yueyun Shang, Zhihong Tian*

Main category: cs.CV

TL;DR: ErasableMask is a privacy protection scheme for face recognition, enhancing transferability and offering perturbation erasion without degrading image quality.


<details>
  <summary>Details</summary>
Motivation: Address weak transferability and permanent damage in existing facial privacy protection methods.

Method: Introduces meta-auxiliary attack, perturbation erasion, and curriculum learning.

Result: Achieves 72% confidence in commercial FR systems and 90% erasion success rate.

Conclusion: ErasableMask is robust, erasable, and outperforms existing methods.

Abstract: While face recognition (FR) models have brought remarkable convenience in
face verification and identification, they also pose substantial privacy risks
to the public. Existing facial privacy protection schemes usually adopt
adversarial examples to disrupt face verification of FR models. However, these
schemes often suffer from weak transferability against black-box FR models and
permanently damage the identifiable information that cannot fulfill the
requirements of authorized operations such as forensics and authentication. To
address these limitations, we propose ErasableMask, a robust and erasable
privacy protection scheme against black-box FR models. Specifically, via
rethinking the inherent relationship between surrogate FR models, ErasableMask
introduces a novel meta-auxiliary attack, which boosts black-box
transferability by learning more general features in a stable and balancing
optimization strategy. It also offers a perturbation erasion mechanism that
supports the erasion of semantic perturbations in protected face without
degrading image quality. To further improve performance, ErasableMask employs a
curriculum learning strategy to mitigate optimization conflicts between
adversarial attack and perturbation erasion. Extensive experiments on the
CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the
state-of-the-art performance in transferability, achieving over 72% confidence
on average in commercial FR systems. Moreover, ErasableMask also exhibits
outstanding perturbation erasion performance, achieving over 90% erasion
success rate.

</details>


### [399] [Refining CNN-based Heatmap Regression with Gradient-based Corner Points for Electrode Localization](https://arxiv.org/pdf/2412.17105)
*Lin Wu*

Main category: cs.CV

TL;DR: A method for detecting electrode positions in lithium-ion batteries using X-ray images, combining CNN-based regression with corner point priors for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of electrode position detection in lithium-ion batteries, addressing issues like localization accuracy loss from feature map down-sampling.

Method: Identify ROI via corner point detection, regress pole positions using CNN, and optimize with corner point priors.

Result: Combining pixel gradient analysis with CNN-based heatmap regression improves accuracy and efficiency.

Conclusion: The proposed method significantly enhances performance in electrode position detection.

Abstract: We propose a method for detecting the electrode positions in lithium-ion
batteries. The process begins by identifying the region of interest (ROI) in
the battery's X-ray image through corner point detection. A convolutional
neural network is then used to regress the pole positions within this ROI.
Finally, the regressed positions are optimized and corrected using corner point
priors, significantly mitigating the loss of localization accuracy caused by
operations such as feature map down-sampling and padding during network
training. Our findings show that combining traditional pixel gradient analysis
with CNN-based heatmap regression for keypoint extraction enhances both
accuracy and efficiency, resulting in significant performance improvements.

</details>


### [400] [Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot Quantization in Edge Computing](https://arxiv.org/pdf/2412.19125)
*Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park*

Main category: cs.CV

TL;DR: AKT (Advanced Knowledge Transfer) enhances low-bit quantized models in zero-shot quantization by refining feature maps and using spatial/channel attention, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot quantization methods struggle with low-bit models due to limited information capacity. AKT aims to improve knowledge transfer without relying on data generation.

Method: AKT refines feature maps in feature distillation and uses spatial/channel attention to transfer core information from full-precision to quantized models.

Result: AKT significantly improves accuracy in low-bit quantized models, achieving state-of-the-art results on CIFAR-10 and CIFAR-100 datasets.

Conclusion: AKT effectively addresses gradient exploding in low-bit models and outperforms existing methods, demonstrating its potential for practical applications.

Abstract: We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.

</details>


### [401] [UniRestorer: Universal Image Restoration via Adaptively Estimating Image Degradation at Proper Granularity](https://arxiv.org/pdf/2412.20157)
*Jingbo Lin, Zhilu Zhang, Wenbo Li, Renjing Pei, Hang Xu, Hongzhi Zhang, Wangmeng Zuo*

Main category: cs.CV

TL;DR: UniRestorer improves all-in-one image restoration by combining hierarchical clustering and a multi-granularity MoE model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods are either degradation-agnostic (limited in specific restoration) or degradation-aware (prone to estimation errors), creating a performance gap with single-task models.

Method: Hierarchical clustering on degradation space and training a multi-granularity MoE model, with adaptive expert selection via degradation and granularity estimation.

Result: UniRestorer outperforms state-of-the-art all-in-one methods and narrows the gap to single-task models.

Conclusion: UniRestorer effectively balances degradation-specific restoration and robustness to estimation errors, advancing all-in-one image restoration.

Abstract: Recently, considerable progress has been made in all-in-one image
restoration. Generally, existing methods can be degradation-agnostic or
degradation-aware. However, the former are limited in leveraging
degradation-specific restoration, and the latter suffer from the inevitable
error in degradation estimation. Consequently, the performance of existing
methods has a large gap compared to specific single-task models. In this work,
we make a step forward in this topic, and present our UniRestorer with improved
restoration performance. Specifically, we perform hierarchical clustering on
degradation space, and train a multi-granularity mixture-of-experts (MoE)
restoration model. Then, UniRestorer adopts both degradation and granularity
estimation to adaptively select an appropriate expert for image restoration. In
contrast to existing degradation-agnostic and -aware methods, UniRestorer can
leverage degradation estimation to benefit degradation specific restoration,
and use granularity estimation to make the model robust to degradation
estimation error. Experimental results show that our UniRestorer outperforms
state-of-the-art all-in-one methods by a large margin, and is promising in
closing the performance gap to specific single task models.

</details>


### [402] [Auto-Prompting SAM for Weakly Supervised Landslide Extraction](https://arxiv.org/pdf/2501.13426)
*Jian Wang, Xiaokang Zhang, Xianping Ma, Weikang Yu, Pedram Ghamisi*

Main category: cs.CV

TL;DR: The paper proposes APSAM, a method for weakly supervised landslide extraction using the Segment Anything Model (SAM) with adaptive prompt generation, improving accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Weakly supervised landslide extraction faces challenges like imprecise boundaries due to lack of pixel-wise supervision. The goal is to improve segmentation accuracy without fine-tuning SAM or relying on high-quality CAMs.

Method: APSAM auto-prompts SAM using hybrid prompts (box and point) generated adaptively from CAMs via an APG algorithm, enabling fine-grained segmentation without fine-tuning.

Result: APSAM outperforms state-of-the-art methods, achieving at least 3.0% higher F1 score and 3.69% higher IoU on aerial and satellite datasets.

Conclusion: APSAM is a simple yet effective solution for weakly supervised landslide extraction, leveraging SAM's capabilities through adaptive prompt engineering.

Abstract: Weakly supervised landslide extraction aims to identify landslide regions
from remote sensing data using models trained with weak labels, particularly
image-level labels. However, it is often challenged by the imprecise boundaries
of the extracted objects due to the lack of pixel-wise supervision and the
properties of landslide objects. To tackle these issues, we propose a simple
yet effective method by auto-prompting the Segment Anything Model (SAM), i.e.,
APSAM. Instead of depending on high-quality class activation maps (CAMs) for
pseudo-labeling or fine-tuning SAM, our method directly yields fine-grained
segmentation masks from SAM inference through prompt engineering. Specifically,
it adaptively generates hybrid prompts from the CAMs obtained by an object
localization network. To provide sufficient information for SAM prompting, an
adaptive prompt generation (APG) algorithm is designed to fully leverage the
visual patterns of CAMs, enabling the efficient generation of pseudo-masks for
landslide extraction. These informative prompts are able to identify the extent
of landslide areas (box prompts) and denote the centers of landslide objects
(point prompts), guiding SAM in landslide segmentation. Experimental results on
high-resolution aerial and satellite datasets demonstrate the effectiveness of
our method, achieving improvements of at least 3.0\% in F1 score and 3.69\% in
IoU compared to other state-of-the-art methods. The source codes and datasets
will be available at https://github.com/zxk688.

</details>


### [403] [OCSU: Optical Chemical Structure Understanding for Molecule-centric Scientific Discovery](https://arxiv.org/pdf/2501.15415)
*Siqi Fan, Yuguang Xie, Bowen Cai, Ailin Xie, Gaochao Liu, Mu Qiao, Jie Xing, Zaiqing Nie*

Main category: cs.CV

TL;DR: The paper introduces the Optical Chemical Structure Understanding (OCSU) task, extending molecule image captioning beyond OCSR, and proposes two methods (DoubleCheck and Mol-VL) with a new dataset (Vis-CheBI20).


<details>
  <summary>Details</summary>
Motivation: To advance molecule-centric discovery by translating chemical structure diagrams into readable strings for both machines and chemists, addressing limitations of existing OCSR methods.

Method: Proposes OCSU task, explores OCSR-based (DoubleCheck) and OCSR-free (Mol-VL) paradigms, and introduces Vis-CheBI20 dataset.

Result: Demonstrates superior performance in providing chemist-readable captions, establishing baselines for OCSU research.

Conclusion: The proposed methods and dataset advance chemical structure understanding, with open-sourced resources for further research.

Abstract: Understanding the chemical structure from a graphical representation of a
molecule is a challenging image caption task that would greatly benefit
molecule-centric scientific discovery. Variations in molecular images and
caption subtasks pose a significant challenge in both image representation
learning and task modeling. Yet, existing methods only focus on a specific
caption task that translates a molecular image into its graph structure, i.e.,
OCSR. In this paper, we propose the Optical Chemical Structure Understanding
(OCSU) task, which extends low-level recognition to multilevel understanding
and aims to translate chemical structure diagrams into readable strings for
both machine and chemist. To facilitate the development of OCSU technology, we
explore both OCSR-based and OCSR-free paradigms. We propose DoubleCheck to
enhance OCSR performance via attentive feature enhancement for local ambiguous
atoms. It can be cascaded with existing SMILES-based molecule understanding
methods to achieve OCSU. Meanwhile, Mol-VL is a vision-language model
end-to-end optimized for OCSU. We also construct Vis-CheBI20, the first
large-scale OCSU dataset. Through comprehensive experiments, we demonstrate the
proposed approaches excel at providing chemist-readable caption for chemical
structure diagrams, which provide solid baselines for further research. Our
code, model, and data are open-sourced at https://github.com/PharMolix/OCSU.

</details>


### [404] [Efficient Feature Fusion for UAV Object Detection](https://arxiv.org/pdf/2501.17983)
*Xudong Wang, Yaxin Peng, Chaomin Shen*

Main category: cs.CV

TL;DR: A novel feature fusion framework improves UAV object detection by balancing classification and localization for small objects, achieving a 2% AP boost over YOLO-v10.


<details>
  <summary>Details</summary>
Motivation: Challenges in UAV object detection include unstable image quality, small object sizes, and complex backgrounds, which existing methods struggle to address effectively.

Method: Proposes a framework with hybrid upsampling and downsampling modules for flexible feature map adjustment, enhancing small object representation and multi-scale fusion.

Result: Achieves a 2% AP improvement on public UAV datasets while maintaining the same parameter count as YOLO-v10.

Conclusion: The framework shows promise for accurate and efficient UAV object detection, particularly for small objects in complex environments.

Abstract: Object detection in unmanned aerial vehicle (UAV) remote sensing images poses
significant challenges due to unstable image quality, small object sizes,
complex backgrounds, and environmental occlusions. Small objects, in
particular, occupy small portions of images, making their accurate detection
highly difficult. Existing multi-scale feature fusion methods address these
challenges to some extent by aggregating features across different resolutions.
However, they often fail to effectively balance the classification and
localization performance for small objects, primarily due to insufficient
feature representation and imbalanced network information flow. In this paper,
we propose a novel feature fusion framework specifically designed for UAV
object detection tasks to enhance both localization accuracy and classification
performance. The proposed framework integrates hybrid upsampling and
downsampling modules, enabling feature maps from different network depths to be
flexibly adjusted to arbitrary resolutions. This design facilitates cross-layer
connections and multi-scale feature fusion, ensuring improved representation of
small objects. Our approach leverages hybrid downsampling to enhance
fine-grained feature representation, improving spatial localization of small
targets, even under complex conditions. Simultaneously, the upsampling module
aggregates global contextual information, optimizing feature consistency across
scales and enhancing classification robustness in cluttered scenes.
Experimental results on two public UAV datasets demonstrate the effectiveness
of the proposed framework. Integrated into the YOLO-v10 model, our method
achieves a 2% improvement in average precision (AP) compared to the baseline
YOLO-v10 model, while maintaining the same number of parameters. These results
highlight the potential of our framework for accurate and efficient UAV object
detection.

</details>


### [405] [Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion](https://arxiv.org/pdf/2502.07203)
*Xingpei Ma, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Qiang Zhang, Shunsi Zhang*

Main category: cs.CV

TL;DR: Playmate is a two-stage framework for generating lifelike talking faces with improved lip-sync, head posture, and emotion control, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based models struggle with inaccurate lip-sync, inappropriate head posture, and lack of fine-grained facial expression control.

Method: A two-stage approach: (1) decoupled implicit 3D representation for accurate attribute disentanglement, (2) emotion-control module for fine-grained emotion manipulation.

Result: Playmate achieves superior video quality, lip synchronization, and flexibility in controlling emotion and head pose.

Conclusion: Playmate advances talking face generation by addressing key challenges and enabling expressive, controllable outputs.

Abstract: Recent diffusion-based talking face generation models have demonstrated
impressive potential in synthesizing videos that accurately match a speech
audio clip with a given reference identity. However, existing approaches still
encounter significant challenges due to uncontrollable factors, such as
inaccurate lip-sync, inappropriate head posture and the lack of fine-grained
control over facial expressions. In order to introduce more face-guided
conditions beyond speech audio clips, a novel two-stage training framework
Playmate is proposed to generate more lifelike facial expressions and talking
faces. In the first stage, we introduce a decoupled implicit 3D representation
along with a meticulously designed motion-decoupled module to facilitate more
accurate attribute disentanglement and generate expressive talking videos
directly from audio cues. Then, in the second stage, we introduce an
emotion-control module to encode emotion control information into the latent
space, enabling fine-grained control over emotions and thereby achieving the
ability to generate talking videos with desired emotion. Extensive experiments
demonstrate that Playmate not only outperforms existing state-of-the-art
methods in terms of video quality, but also exhibits strong competitiveness in
lip synchronization while offering improved flexibility in controlling emotion
and head pose. The code will be available at
https://github.com/Playmate111/Playmate.

</details>


### [406] [Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance](https://arxiv.org/pdf/2502.11971)
*Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, Jian Yang*

Main category: cs.CV

TL;DR: A robust contour-based pose tracking method for augmented reality, addressing challenges like cluttered backgrounds and noisy sequences, with a CPU-only strategy for symmetric objects and high efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve robustness in 6DoF pose tracking for AR applications, especially in cluttered environments and with symmetric objects.

Method: Uses a fan-shaped search strategy for contour correspondences, models noise uncertainty, and integrates sparse interior correspondences via DIS optical flow.

Result: Outperforms state-of-the-art monocular tracking methods, achieving over 100 FPS on CPU.

Conclusion: The proposed method is highly robust and efficient, suitable for real-time AR applications.

Abstract: Augmented reality assembly guidance is essential for intelligent
manufacturing and medical applications, requiring continuous measurement of the
6DoF poses of manipulated objects. Although current tracking methods have made
significant advancements in accuracy and efficiency, they still face challenges
in robustness when dealing with cluttered backgrounds, rotationally symmetric
objects, and noisy sequences. In this paper, we first propose a robust
contour-based pose tracking method that addresses error-prone contour
correspondences and improves noise tolerance. It utilizes a fan-shaped search
strategy to refine correspondences and models local contour shape and noise
uncertainty as mixed probability distribution, resulting in a highly robust
contour energy function. Secondly, we introduce a CPU-only strategy to better
track rotationally symmetric objects and assist the contour-based method in
overcoming local minima by exploring sparse interior correspondences. This is
achieved by pre-sampling interior points from sparse viewpoint templates
offline and using the DIS optical flow algorithm to compute their
correspondences during tracking. Finally, we formulate a unified energy
function to fuse contour and interior information, which is solvable using a
re-weighted least squares algorithm. Experiments on public datasets and real
scenarios demonstrate that our method significantly outperforms
state-of-the-art monocular tracking methods and can achieve more than 100 FPS
using only a CPU.

</details>


### [407] [Progressive Local Alignment for Medical Multimodal Pre-training](https://arxiv.org/pdf/2502.18047)
*Huimin Yan, Xian Yang, Liang Bai, Jiye Liang*

Main category: cs.CV

TL;DR: PLAN introduces a contrastive learning-based approach for local alignment in medical images and text, improving precision and robustness through progressive learning.


<details>
  <summary>Details</summary>
Motivation: Local alignment in medical images and text is challenging due to lack of natural pairings and rigid region recognition methods.

Method: PLAN uses contrastive learning for word-pixel relationships and progressive learning to refine alignment.

Result: PLAN outperforms state-of-the-art methods in tasks like phrase grounding and image-text retrieval.

Conclusion: PLAN sets a new benchmark for medical image-text alignment by enhancing soft region recognition and reducing noise.

Abstract: Local alignment between medical images and text is essential for accurate
diagnosis, though it remains challenging due to the absence of natural local
pairings and the limitations of rigid region recognition methods. Traditional
approaches rely on hard boundaries, which introduce uncertainty, whereas
medical imaging demands flexible soft region recognition to handle irregular
structures. To overcome these challenges, we propose the Progressive Local
Alignment Network (PLAN), which designs a novel contrastive learning-based
approach for local alignment to establish meaningful word-pixel relationships
and introduces a progressive learning strategy to iteratively refine these
relationships, enhancing alignment precision and robustness. By combining these
techniques, PLAN effectively improves soft region recognition while suppressing
noise interference. Extensive experiments on multiple medical datasets
demonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,
image-text retrieval, object detection, and zero-shot classification, setting a
new benchmark for medical image-text alignment.

</details>


### [408] [Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG](https://arxiv.org/pdf/2503.01222)
*Wenbin Wang, Yongcheng Jing, Liang Ding, Yingjie Wang, Li Shen, Yong Luo, Bo Du, Dacheng Tao*

Main category: cs.CV

TL;DR: The paper introduces Retrieval-Augmented Perception (RAP) to enhance high-resolution image perception in MLLMs using retrieval-augmented generation, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for high-resolution image perception in MLLMs are limited, prompting a shift to leveraging long-context techniques like RAG.

Method: Proposes RAP, a training-free framework that retrieves and fuses relevant image crops with spatial context using Spatial-Awareness Layout, and dynamically selects crops via RE-Search.

Result: RAP improves LLaVA-v1.5-13B by 43% on V* Bench and 19% on HR-Bench.

Conclusion: RAP effectively addresses HR perception challenges in MLLMs by integrating retrieval techniques, demonstrating substantial performance gains.

Abstract: High-resolution (HR) image perception remains a key challenge in multimodal
large language models (MLLMs). To overcome the limitations of existing methods,
this paper shifts away from prior dedicated heuristic approaches and revisits
the most fundamental idea to HR perception by enhancing the long-context
capability of MLLMs, driven by recent advances in long-context techniques like
retrieval-augmented generation (RAG) for general LLMs. Towards this end, this
paper presents the first study exploring the use of RAG to address HR
perception challenges. Specifically, we propose Retrieval-Augmented Perception
(RAP), a training-free framework that retrieves and fuses relevant image crops
while preserving spatial context using the proposed Spatial-Awareness Layout.
To accommodate different tasks, the proposed Retrieved-Exploration Search
(RE-Search) dynamically selects the optimal number of crops based on model
confidence and retrieval scores. Experimental results on HR benchmarks
demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving
a 43% improvement on $V^*$ Bench and 19% on HR-Bench.

</details>


### [409] [DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating Semantic Understanding of Dongba Pictograms](https://arxiv.org/pdf/2503.03644)
*Xiaojun Bi, Shuo Li, Junyao Xing, Ziyue Wang, Fuwen Luo, Weizheng Qiao, Lu Han, Ziwei Sun, Peng Li, Yang Liu*

Main category: cs.CV

TL;DR: The paper introduces DongbaMIE, the first dataset for multimodal information extraction of Dongba pictographs, addressing the lack of datasets for semantic understanding. It evaluates models' performance on this task.


<details>
  <summary>Details</summary>
Motivation: The lack of datasets for Dongba pictographs hinders semantic understanding research. DongbaMIE aims to fill this gap.

Method: Constructed DongbaMIE dataset with 23,530 sentence-level and 2,539 paragraph-level text-image pairs, annotated across four semantic dimensions. Evaluated mainstream multimodal models under zero-shot, few-shot, and supervised fine-tuning.

Result: Models struggle with efficient information extraction under zero-shot and few-shot learning. Supervised fine-tuning improves performance but complex semantics remain challenging.

Conclusion: DongbaMIE enables research on Dongba pictographs, but current models face challenges in accurately extracting complex semantics.

Abstract: Dongba pictographic is the only pictographic script still in use in the
world. Its pictorial ideographic features carry rich cultural and contextual
information. However, due to the lack of relevant datasets, research on
semantic understanding of Dongba hieroglyphs has progressed slowly. To this
end, we constructed \textbf{DongbaMIE} - the first dataset focusing on
multimodal information extraction of Dongba pictographs. The dataset consists
of images of Dongba hieroglyphic characters and their corresponding semantic
annotations in Chinese. It contains 23,530 sentence-level and 2,539
paragraph-level high-quality text-image pairs. The annotations cover four
semantic dimensions: object, action, relation and attribute. Systematic
evaluation of mainstream multimodal large language models shows that the models
are difficult to perform information extraction of Dongba hieroglyphs
efficiently under zero-shot and few-shot learning. Although supervised
fine-tuning can improve the performance, accurate extraction of complex
semantics is still a great challenge at present.

</details>


### [410] [Leveraging Large Language Models For Scalable Vector Graphics Processing: A Review](https://arxiv.org/pdf/2503.04983)
*Boris Malashenko, Ivan Jarsky, Valeria Efimova*

Main category: cs.CV

TL;DR: The paper reviews LLM-based approaches for SVG processing, categorizing them into generation, editing, and understanding tasks, and evaluates models and datasets, finding reasoning-enhanced LLMs outperform standard ones.


<details>
  <summary>Details</summary>
Motivation: Vector graphics, unlike raster images, are understudied despite their scalability and ease of editing. Traditional vectorization techniques are slow and complex, prompting exploration of LLMs for SVG tasks.

Method: The paper systematically reviews LLM-based SVG approaches, categorizes tasks, evaluates models (e.g., IconShop, StrokeNUWA), and benchmarks datasets (e.g., SVGEditBench, VGBench).

Result: Reasoning-enhanced LLMs outperform standard LLMs in SVG generation and understanding tasks. Current datasets lack diversity and rich annotations.

Conclusion: More diverse datasets are needed to enhance LLM capabilities in vector graphics tasks, as reasoning-enhanced models show promise in SVG processing.

Abstract: In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.

</details>


### [411] [EDM: Efficient Deep Feature Matching](https://arxiv.org/pdf/2503.05122)
*Xi Li, Tong Rao, Cihui Pan*

Main category: cs.CV

TL;DR: The paper introduces EDM, an efficient deep feature matching network, improving accuracy and speed in feature matching by optimizing feature extraction, correlation injection, and refinement stages.


<details>
  <summary>Details</summary>
Motivation: Existing feature matching methods lack efficiency despite high performance. The paper aims to enhance both accuracy and speed in detector-free matching pipelines.

Method: EDM uses a deeper CNN with fewer dimensions for feature extraction, a Correlation Injection Module for multi-scale feature aggregation, and a lightweight bidirectional axis-based regression head for subpixel-level correspondence prediction.

Result: EDM achieves competitive accuracy on benchmarks while demonstrating excellent efficiency, making it suitable for real-world applications.

Conclusion: EDM provides a balanced solution for feature matching, combining high performance with efficiency, and offers practical best practices for implementation.

Abstract: Recent feature matching methods have achieved remarkable performance but lack
efficiency consideration. In this paper, we revisit the mainstream
detector-free matching pipeline and improve all its stages considering both
accuracy and efficiency. We propose an Efficient Deep feature Matching network,
EDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level
features. Then we present a Correlation Injection Module that conducts feature
transformation on high-level deep features, and progressively injects feature
correlations from global to local for efficient multi-scale feature
aggregation, improving both speed and performance. In the refinement stage, a
novel lightweight bidirectional axis-based regression head is designed to
directly predict subpixel-level correspondences from latent features, avoiding
the significant computational cost of explicitly locating keypoints on
high-resolution local feature heatmaps. Moreover, effective selection
strategies are introduced to enhance matching accuracy. Extensive experiments
show that our EDM achieves competitive matching accuracy on various benchmarks
and exhibits excellent efficiency, offering valuable best practices for
real-world applications. The code is available at
https://github.com/chicleee/EDM.

</details>


### [412] [MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?](https://arxiv.org/pdf/2503.09499)
*Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen*

Main category: cs.CV

TL;DR: MindGYM introduces a structured framework for self-generated, cognitively guided data to enhance large models' reasoning abilities, outperforming baselines in quality and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of rigid templates and crowd-annotated datasets in fostering transferable, structured thinking in large foundation models.

Method: MindGYM includes Cognitive Thinking Process Injection, Seed Single-Hop Question Synthesis, and Challenging Multi-Hop QA Synthesis to generate high-quality, self-contained data.

Result: Synthetic data achieves 16.7% higher quality and 67.91% lower variance, with performance gains up to 16% on reasoning benchmarks like MathVision.

Conclusion: MindGYM demonstrates the effectiveness of self-challenging mechanisms for refining model capabilities with minimal human intervention, promoting data-centric research.

Abstract: Large foundation models face challenges in acquiring transferable, structured
thinking abilities, especially when supervised with rigid templates or
crowd-annotated instruction datasets. Unlike prior approaches, we focus on a
thinking-centric data synthesis paradigm that enables models to evolve through
self-generated, cognitively guided data. We propose MindGYM, a structured and
scalable framework for question synthesis, composed of: (1) Cognitive Thinking
Process Injection, which infuses high-level reasoning objectives to shape the
model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating
atomic questions from diverse semantic types to encourage broader thinking; and
(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop
questions based on QA seeds for deeper reasoning. Detailed analysis shows that
synthetic data generated by our method achieves 16.7% higher average quality
and 67.91% lower quality variance compared to baseline sources, highlighting
that both high-quality and self-contained data are essential for effective,
thinking-oriented fine-tuning. MindGYM improves performance on six reasoning
benchmarks, achieving gains of up to 16% on MathVision using only 400 data
samples, and generalizable improvements across different model sizes and
architectures. MindGYM underscores the viability of self-challenging mechanisms
in refining large model capabilities while minimizing human intervention and
resource demands. Code and data are released to promote data-centric research
into self-evolving foundation models driven by their internal reasoning
capabilities.

</details>


### [413] [Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability](https://arxiv.org/pdf/2504.10804)
*Jiani Liu, Zhiyuan Wang, Zeliang Zhang, Chao Huang, Susan Liang, Yunlong Tang, Chenliang Xu*

Main category: cs.CV

TL;DR: The paper explores how Vision Transformers (ViTs) exhibit higher adversarial transferability than CNNs due to computational redundancy, proposing techniques to exploit this for stronger attacks.


<details>
  <summary>Details</summary>
Motivation: ViTs' unique architecture raises new challenges in adversarial robustness, with observed higher transferability of adversarial examples compared to CNNs.

Method: Investigates computational redundancy in ViTs, identifying data-level and model-level redundancy. Proposes techniques like attention sparsity manipulation and ghost MoE diversification.

Result: Experiments on ImageNet-1k show the proposed methods outperform baselines in transferability and generality across architectures.

Conclusion: Exploiting ViTs' redundancy improves adversarial attack effectiveness, offering new insights for robustness research.

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
range of applications, including many safety-critical tasks. However, their
unique architectural properties raise new challenges and opportunities in
adversarial robustness. In particular, we observe that adversarial examples
crafted on ViTs exhibit higher transferability compared to those crafted on
CNNs, suggesting that ViTs contain structural characteristics favorable for
transferable attacks. In this work, we investigate the role of computational
redundancy in ViTs and its impact on adversarial transferability. Unlike prior
studies that aim to reduce computation for efficiency, we propose to exploit
this redundancy to improve the quality and transferability of adversarial
examples. Through a detailed analysis, we identify two forms of redundancy,
including the data-level and model-level, that can be harnessed to amplify
attack effectiveness. Building on this insight, we design a suite of
techniques, including attention sparsity manipulation, attention head
permutation, clean token regularization, ghost MoE diversification, and
test-time adversarial training. Extensive experiments on the ImageNet-1k
dataset validate the effectiveness of our approach, showing that our methods
significantly outperform existing baselines in both transferability and
generality across diverse model architectures.

</details>


### [414] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/pdf/2504.14202)
*Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu*

Main category: cs.CV

TL;DR: A novel framework, FaceCLIP, integrates identity and text into a unified conditioning input for ID-preserving image generation, outperforming prior methods in photorealistic portraits.


<details>
  <summary>Details</summary>
Motivation: To improve identity preservation and textual alignment in image generation by unifying identity and text inputs rather than using adapters.

Method: Introduces FaceCLIP, a multi-modal encoder for joint embedding of identity and text, and integrates it with Stable Diffusion XL (SDXL) for image synthesis.

Result: FaceCLIP-SDXL achieves photorealistic portrait generation with superior identity preservation and textual relevance.

Conclusion: The proposed framework demonstrates quantitative and qualitative improvements over existing methods.

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [415] [How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](https://arxiv.org/pdf/2504.14391)
*Rahul Thapa, Andrew Li, Qingyang Wu, Bryan He, Yuki Sahashi, Christina Binder, Angela Zhang, Ben Athiwaratkun, Shuaiwen Leon Song, David Ouyang, James Zou*

Main category: cs.CV

TL;DR: OpenBiomedVi, a biomedical video dataset, enhances vision-language models' performance using educational videos, achieving significant gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore if pedagogically rich but non-standardized biomedical videos can effectively teach vision-language models biomedical knowledge.

Method: Curated OpenBiomedVi dataset (1031 hours of video-caption/Q&A pairs) and fine-tuned Qwen-2-VL models. Introduced new benchmarks (MIMICEchoQA, SurgeryVideoQA).

Result: Substantial improvements: 2B model (98.7% video, 71.2% image, 0.2% text); 7B model (37.09% video, 11.2% image, -2.7% text). New benchmarks also showed gains.

Conclusion: Educational videos for humans are surprisingly effective for training biomedical VLMs, even with informal content.

Abstract: Publicly available biomedical videos, such as those on YouTube, serve as
valuable educational resources for medical students. Unlike standard machine
learning datasets, these videos are designed for human learners, often mixing
medical imagery with narration, explanatory diagrams, and contextual framing.
In this work, we investigate whether such pedagogically rich, yet
non-standardized and heterogeneous videos can effectively teach general-domain
vision-language models biomedical knowledge. To this end, we introduce
OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031
hours of video-caption and Q/A pairs, curated through a multi-step
human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and
OpenBiomedVid fills an important gap by providing instruction-style supervision
grounded in real-world educational content. Surprisingly, despite the informal
and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models
exhibit substantial performance improvements across most benchmarks. The 2B
model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on
text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on
image tasks, with a slight degradation of 2.7% on text tasks compared to their
respective base models. To address the lack of standardized biomedical video
evaluation datasets, we also introduce two new expert curated benchmarks,
MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves
gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,
respectively, demonstrating the models' ability to generalize and perform
biomedical video understanding on cleaner and more standardized datasets than
those seen during training. These results suggest that educational videos
created for human learning offer a surprisingly effective training signal for
biomedical VLMs.

</details>


### [416] [Relation-R1: Progressively Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relation Comprehension](https://arxiv.org/pdf/2504.14642)
*Lin Li, Wei Chen, Jiahui Li, Kwang-Ting Cheng, Long Chen*

Main category: cs.CV

TL;DR: Relation-R1 is a unified framework for visual relation understanding, combining cognitive CoT-guided SFT and GRPO in RL, outperforming existing methods in binary and N-ary relations.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack structural semantic dependency modeling, leading to unreliable outputs and biases in visual relation understanding.

Method: Proposes Relation-R1, integrating CoT-guided SFT for reasoning and GRPO for multi-rewards optimization to prioritize visual-semantic grounding.

Result: Achieves state-of-the-art performance on PSG and SWiG datasets for binary and N-ary relation understanding.

Conclusion: Relation-R1 effectively addresses limitations in visual relation understanding by combining structured reasoning and optimization, improving generalization.

Abstract: Recent advances in multi-modal large language models (MLLMs) have
significantly improved object-level grounding and region captioning. However,
they remain limited in visual relation understanding, struggling even with
binary relation detection, let alone \textit{N}-ary relations involving
multiple semantic roles. The core reason is the lack of modeling for
\textit{structural semantic dependencies} among multi-entities, leading to
unreliable outputs, hallucinations, and over-reliance on language priors (\eg,
defaulting to ``person drinks a milk'' if a person is merely holding it). To
this end, we propose Relation-R1, the \textit{first unified} relation
comprehension framework that explicitly integrates cognitive chain-of-thought
(CoT)-guided supervised fine-tuning (SFT) and group relative policy
optimization (GRPO) within a reinforcement learning (RL) paradigm.
Specifically, we first establish foundational reasoning capabilities via SFT,
enforcing structured outputs with thinking processes. Then, GRPO is utilized to
refine these outputs via multi-rewards optimization, prioritizing
visual-semantic grounding over language-induced biases, thereby improving
generalization capability. Furthermore, we investigate the impact of various
CoT strategies within this framework, demonstrating that a specific-to-general
progressive approach in CoT guidance further improves generalization,
especially in capturing synonymous \textit{N}-ary relations. Extensive
experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1
achieves state-of-the-art performance in both binary and \textit{N}-ary
relation understanding.

</details>


### [417] [DC4CR: When Cloud Removal Meets Diffusion Control in Remote Sensing](https://arxiv.org/pdf/2504.14785)
*Zhenyu Yu, Mohd Yamani Idna Idris, Pei Wang*

Main category: cs.CV

TL;DR: DC4CR is a diffusion-based framework for cloud removal in remote sensing, offering prompt-driven control, computational efficiency, and scalability without needing cloud masks.


<details>
  <summary>Details</summary>
Motivation: Cloud occlusion obstructs surface information in remote sensing, complicating analysis. DC4CR aims to address this by enabling efficient and adaptable cloud removal.

Method: Uses multimodal diffusion with prompt-driven control, low-rank adaptation, subject-driven generation, and grouped learning for small datasets. Designed as a plug-and-play module.

Result: Achieves state-of-the-art performance on RICE and CUHK-CR datasets, effectively removing clouds under diverse conditions.

Conclusion: DC4CR provides a practical, scalable solution for cloud removal in remote sensing, with broad real-world applications.

Abstract: Cloud occlusion significantly hinders remote sensing applications by
obstructing surface information and complicating analysis. To address this, we
propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal
diffusion-based framework for cloud removal in remote sensing imagery. Our
method introduces prompt-driven control, allowing selective removal of thin and
thick clouds without relying on pre-generated cloud masks, thereby enhancing
preprocessing efficiency and model adaptability. Additionally, we integrate
low-rank adaptation for computational efficiency, subject-driven generation for
improved generalization, and grouped learning to enhance performance on small
datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into
existing cloud removal models, providing a scalable and robust solution.
Extensive experiments on the RICE and CUHK-CR datasets demonstrate
state-of-the-art performance, achieving superior cloud removal across diverse
conditions. This work presents a practical and efficient approach for remote
sensing image processing with broad real-world applications.

</details>


### [418] [TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models](https://arxiv.org/pdf/2504.14933)
*Mazharul Islam Rakib, Showrin Rahman, Joyanta Jyoti Mondal, Xi Xiao, David Lewis, Alessandra Mileo, Meem Arafat Manab*

Main category: cs.CV

TL;DR: A two-step image generation model is proposed to avoid copyright infringement and source copying in AI-generated images, using segmentation masks and diffusion models to reduce structural similarity to training images.


<details>
  <summary>Details</summary>
Motivation: Addressing copyright concerns in AI-generated images, particularly the issue of source copying, which hinders the free use of generative models.

Method: A novel two-step approach: first, creating a segmentation mask for prompt-based images; second, regenerating the image while avoiding the masked shape, reducing structural similarity to training data.

Result: The method decreases structural similarity to training images, effectively avoiding source copying without costly retraining or user-centered prompts.

Conclusion: The proposed approach is a computationally inexpensive solution to prevent copyright infringement and source copying in diffusion model-based image generation.

Abstract: In today's age of social media and marketing, copyright issues can be a major
roadblock to the free sharing of images. Generative AI models have made it
possible to create high-quality images, but concerns about copyright
infringement are a hindrance to their abundant use. As these models use data
from training images to generate new ones, it is often a daunting task to
ensure they do not violate intellectual property rights. Some AI models have
even been noted to directly copy copyrighted images, a problem often referred
to as source copying. Traditional copyright protection measures such as
watermarks and metadata have also proven to be futile in this regard. To
address this issue, we propose a novel two-step image generation model inspired
by the conditional diffusion model. The first step involves creating an image
segmentation mask for some prompt-based generated images. This mask embodies
the shape of the image. Thereafter, the diffusion model is asked to generate
the image anew while avoiding the shape in question. This approach shows a
decrease in structural similarity from the training image, i.e. we are able to
avoid the source copying problem using this approach without expensive
retraining of the model or user-centered prompt generation techniques. This
makes our approach the most computationally inexpensive approach to avoiding
both copyright infringement and source copying for diffusion model-based image
generation.

</details>


### [419] [Persistence-based Hough Transform for Line Detection](https://arxiv.org/pdf/2504.16114)
*Johannes Ferner, Stefan Huber, Saverio Messineo, Angel Pop, Martin Uray*

Main category: cs.CV

TL;DR: The paper proposes a persistent homology-based voting technique for the Hough transform, improving robustness and performance over traditional thresholding methods.


<details>
  <summary>Details</summary>
Motivation: The Hough transform's susceptibility to noise and artifacts due to thresholding motivates the search for a more robust alternative.

Method: The authors introduce a voting technique using persistent homology to detect peaks in Hough space, addressing thresholding limitations.

Result: Experiments on synthetic data show the method outperforms the original Hough transform in robustness and performance.

Conclusion: The work advocates for integrating Topological Data Analysis into existing methods and encourages further research on mathematically stable improvements to the Hough transform.

Abstract: The Hough transform is a popular and classical technique in computer vision
for the detection of lines (or more general objects). It maps a pixel into a
dual space -- the Hough space: each pixel is mapped to the set of lines through
this pixel, which forms a curve in Hough space. The detection of lines then
becomes a voting process to find those lines that received many votes by
pixels. However, this voting is done by thresholding, which is susceptible to
noise and other artifacts.
  In this work, we present an alternative voting technique to detect peaks in
the Hough space based on persistent homology, which very naturally addresses
limitations of simple thresholding. Experiments on synthetic data show that our
method significantly outperforms the original method, while also demonstrating
enhanced robustness.
  This work seeks to inspire future research in two key directions. First, we
highlight the untapped potential of Topological Data Analysis techniques and
advocate for their broader integration into existing methods, including
well-established ones. Secondly, we initiate a discussion on the mathematical
stability of the Hough transform, encouraging exploration of mathematically
grounded improvements to enhance its robustness.

</details>


### [420] [Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/pdf/2505.02567)
*Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang*

Main category: cs.CV

TL;DR: A survey on unifying multimodal understanding and image generation models, covering foundational concepts, architectural paradigms, datasets, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the gap between multimodal understanding and image generation models by exploring unified frameworks, inspired by trends like GPT-4o.

Method: Review and categorize existing unified models into diffusion-based, autoregressive-based, and hybrid approaches, analyzing their designs and innovations.

Result: Compiled datasets and benchmarks for unified models, identified key challenges (tokenization, cross-modal attention, data), and provided a GitHub resource.

Conclusion: The survey aims to guide future research, inspire advancements, and serve as a reference for the community in this emerging field.

Abstract: Recent years have seen remarkable progress in both multimodal understanding
models and image generation models. Despite their respective successes, these
two domains have evolved independently, leading to distinct architectural
paradigms: While autoregressive-based architectures have dominated multimodal
understanding, diffusion-based models have become the cornerstone of image
generation. Recently, there has been growing interest in developing unified
frameworks that integrate these tasks. The emergence of GPT-4o's new
capabilities exemplifies this trend, highlighting the potential for
unification. However, the architectural differences between the two domains
pose significant challenges. To provide a clear overview of current efforts
toward unification, we present a comprehensive survey aimed at guiding future
research. First, we introduce the foundational concepts and recent advancements
in multimodal understanding and text-to-image generation models. Next, we
review existing unified models, categorizing them into three main architectural
paradigms: diffusion-based, autoregressive-based, and hybrid approaches that
fuse autoregressive and diffusion mechanisms. For each category, we analyze the
structural designs and innovations introduced by related works. Additionally,
we compile datasets and benchmarks tailored for unified models, offering
resources for future exploration. Finally, we discuss the key challenges facing
this nascent field, including tokenization strategy, cross-modal attention, and
data. As this area is still in its early stages, we anticipate rapid
advancements and will regularly update this survey. Our goal is to inspire
further research and provide a valuable reference for the community. The
references associated with this survey are available on GitHub
(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).

</details>


### [421] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/pdf/2505.03176)
*Hafez Ghaemi, Eilif Muller, Shahab Bakhtiari*

Main category: cs.CV

TL;DR: seq-JEPA introduces a world modeling framework to resolve trade-offs between invariant and equivariant representations by processing sequences of views and actions, achieving strong performance on both tasks.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised methods limit flexibility by enforcing invariance or equivariance separately, creating trade-offs for downstream tasks.

Method: seq-JEPA processes sequences of views and actions using a transformer encoder and predictor head to learn segregated invariant and equivariant representations.

Result: seq-JEPA performs well on both invariant and equivariant benchmarks and excels at tasks requiring sequential observation aggregation.

Conclusion: seq-JEPA effectively resolves the trade-off between invariant and equivariant representations, offering improved flexibility for diverse tasks.

Abstract: Current self-supervised algorithms commonly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by enforcing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm often
limits the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between high-level invariance-demanding tasks
such as image classification and more fine-grained equivariance-related tasks.
In this work, we proposes \emph{seq-JEPA}, a world modeling framework that
introduces architectural inductive biases into joint-embedding predictive
architectures to resolve this trade-off. Without relying on dual equivariance
predictors or loss terms, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to specified transformations and
another invariant to them. To do so, our model processes short sequences of
different views (observations) of inputs. Each encoded view is concatenated
with an embedding of the relative transformation (action) that produces the
next observation in the sequence. These view-action pairs are passed through a
transformer encoder that outputs an aggregate representation. A predictor head
then conditions this aggregate representation on the upcoming action to predict
the representation of the next observation. Empirically, seq-JEPA demonstrates
strong performance on both equivariant and invariant benchmarks without
sacrificing one for the other. Furthermore, it excels at tasks that inherently
require aggregating a sequence of observations, such as path integration across
actions and predictive learning across eye movements.

</details>


### [422] [Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models](https://arxiv.org/pdf/2505.05626)
*Aarti Ghatkesar, Uddeshya Upadhyay, Ganesh Venkatesh*

Main category: cs.CV

TL;DR: The paper addresses the challenge of deep alignment between vision and language in MLLMs, proposing techniques to enhance visual understanding and its influence on language generation, resulting in improved performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs often underutilize visual input, relying too heavily on language priors, which limits their multimodal capabilities.

Method: The approach involves analyzing how MLLMs build visual understanding and introducing techniques to deepen this understanding and ensure it guides language generation.

Result: The model shows superior multimodal understanding, with improved prediction of visually-dependent tokens and a 10-point boost on visually challenging tasks.

Conclusion: The proposed techniques effectively enhance MLLMs' visual understanding and alignment with language, leading to better performance in multimodal tasks.

Abstract: Achieving deep alignment between vision and language remains a central
challenge for Multimodal Large Language Models (MLLMs). These models often fail
to fully leverage visual input, defaulting to strong language priors. Our
approach first provides insights into how MLLMs internally build visual
understanding of image regions and then introduces techniques to amplify this
capability. Specifically, we explore techniques designed both to deepen the
model's understanding of visual content and to ensure that these visual
insights actively guide language generation. We demonstrate the superior
multimodal understanding of our resultant model through a detailed upstream
analysis quantifying its ability to predict visually-dependent tokens as well
as 10 pt boost on visually challenging tasks.

</details>


### [423] [Generative Pre-trained Autoregressive Diffusion Transformer](https://arxiv.org/pdf/2505.07344)
*Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan*

Main category: cs.CV

TL;DR: GPDiT combines diffusion and autoregressive modeling for high-quality long-range video synthesis in continuous latent space, improving motion dynamics and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: To unify diffusion and autoregressive modeling for better video synthesis, addressing motion dynamics and semantic consistency in a continuous latent space.

Method: GPDiT autoregressively predicts future latent frames using a diffusion loss, with a lightweight causal attention variant and rotation-based time-conditioning.

Result: GPDiT excels in video generation quality, representation ability, and few-shot learning, outperforming existing methods.

Conclusion: GPDiT is a promising framework for continuous-space video modeling, offering enhanced generation and representation capabilities.

Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.

</details>


### [424] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/pdf/2505.10049)
*Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao*

Main category: cs.CV

TL;DR: A survey of 200+ papers on dynamic scene representation using radiance fields, covering neural representations to Gaussian primitives, with focus on motion, reconstruction, and regularization.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze advancements in dynamic scene representation and reconstruction, addressing complexities in 4D scenes and providing a reference for researchers.

Method: Categorizes and evaluates works through lenses like motion representation, reconstruction techniques, auxiliary data integration, and regularization.

Result: Enhanced understanding of dynamic scene reconstruction, highlighting quality improvements and methodological diversity.

Conclusion: Identifies challenges and future directions, offering a unified framework for researchers and practitioners.

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [425] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/pdf/2505.10250)
*Wenhao Shen, Wanqi Yin, Xiaofeng Yang, Cheng Chen, Chaoyue Song, Zhongang Cai, Lei Yang, Hao Wang, Guosheng Lin*

Main category: cs.CV

TL;DR: ADHMR is a framework that aligns a diffusion-based human mesh recovery model using preference optimization, improving alignment with 2D observations and robustness for in-the-wild images.


<details>
  <summary>Details</summary>
Motivation: Existing probabilistic HMR methods often misalign with 2D observations and lack robustness for in-the-wild images.

Method: Train an HMR-Scorer to assess predictions, create a preference dataset, and finetune the base model using direct preference optimization.

Result: ADHMR outperforms state-of-the-art methods and improves existing models via data cleaning.

Conclusion: ADHMR effectively addresses misalignment and robustness issues in HMR, demonstrating superior performance.

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [426] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/pdf/2505.10473)
*Fengdi Zhang, Hongkun Cao, Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS is a 3DGS optimization method enabling intuitive adjustment of the trade-off between Gaussian quantity and rendering quality, outperforming baselines with higher quality and fewer Gaussians.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack user-adjustable trade-offs for practical needs like diverse hardware constraints. ControlGS addresses this gap.

Method: ControlGS uses a single training run with a fixed setup and user-specified hyperparameter to find optimal trade-off points across scenes.

Result: It achieves higher rendering quality with fewer Gaussians and supports stepless control over the trade-off.

Conclusion: ControlGS provides a flexible, high-performance solution for 3DGS optimization, suitable for diverse practical applications.

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control. Through a single training run using a
fixed setup and a user-specified hyperparameter reflecting quantity-quality
preference, ControlGS can automatically find desirable quantity-quality
trade-off points across diverse scenes, from compact objects to large outdoor
scenes. It also outperforms baselines by achieving higher rendering quality
with fewer Gaussians, and supports a broad adjustment range with stepless
control over the trade-off. Project page:
https://zhang-fengdi.github.io/ControlGS/

</details>


### [427] [PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging](https://arxiv.org/pdf/2505.11872)
*Quoc-Huy Trinh, Minh-Van Nguyen, Jung Peng, Ulas Bagci, Debesh Jha*

Main category: cs.CV

TL;DR: PRS-Med integrates vision-language models for medical image segmentation and spatial reasoning, outperforming existing methods and introducing the MMRS dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in natural language interaction and positional reasoning in medical image segmentation.

Method: Combines vision-language models with segmentation to generate masks and spatial reasoning outputs, using the MMRS dataset.

Result: Superior performance in segmentation and position reasoning across six imaging modalities.

Conclusion: PRS-Med enhances doctor-system interaction and diagnoses, with plans to release resources for further research.

Abstract: Recent advancements in prompt-based medical image segmentation have enabled
clinicians to identify tumors using simple input like bounding boxes or text
prompts. However, existing methods face challenges when doctors need to
interact through natural language or when position reasoning is required -
understanding spatial relationships between anatomical structures and
pathologies. We present PRS-Med, a framework that integrates vision-language
models with segmentation capabilities to generate both accurate segmentation
masks and corresponding spatial reasoning outputs. Additionally, we introduce
the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),
which provides diverse, spatially-grounded question-answer pairs to address the
lack of position reasoning data in medical imaging. PRS-Med demonstrates
superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,
endoscopy, RGB), significantly outperforming state-of-the-art methods in both
segmentation accuracy and position reasoning. Our approach enables intuitive
doctor-system interaction through natural language, facilitating more efficient
diagnoses. Our dataset pipeline, model, and codebase will be released to foster
further research in spatially-aware multimodal reasoning for medical
applications.

</details>


### [428] [Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning](https://arxiv.org/pdf/2505.11945)
*Bonan li, Zicheng Zhang, Songhua Liu, Weihao Yu, Xinchao Wang*

Main category: cs.CV

TL;DR: LLaVA-Meteor introduces a Top-Down Compression paradigm and Flash Global Fusion module to balance accuracy and efficiency in visual instruction tuning, reducing visual tokens by 75-95% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between accuracy and efficiency in vision-to-language projection for large language models.

Method: Uses Top-Down Compression, Flash Global Fusion for feature alignment, local-to-single scanning for dependencies, and Visual-Native Selection for token reduction.

Result: Achieves comparable or superior performance on 12 benchmarks with 75-95% fewer visual tokens.

Conclusion: LLaVA-Meteor effectively balances efficiency and accuracy in visual instruction tuning.

Abstract: Visual instruction tuning aims to enable large language models to comprehend
the visual world, with a pivotal challenge lying in establishing an effective
vision-to-language projection. However, existing methods often grapple with the
intractable trade-off between accuracy and efficiency. In this paper, we
present LLaVA-Meteor, a novel approach designed to break this deadlock,
equipped with a novel Top-Down Compression paradigm that strategically
compresses visual tokens without compromising core information. Specifically,
we construct a trainable Flash Global Fusion module based on efficient
selective state space operators, which aligns the feature space while enabling
each token to perceive holistic visual context and instruction preference at
low cost. Furthermore, a local-to-single scanning manner is employed to
effectively capture local dependencies, thereby enhancing the model's
capability in vision modeling. To alleviate computational overhead, we explore
a Visual-Native Selection mechanism that independently assesses token
significance by both the visual and native experts, followed by aggregation to
retain the most critical subset. Extensive experiments show that our approach
reduces visual tokens by 75--95% while achieving comparable or superior
performance across 12 benchmarks, significantly improving efficiency.

</details>


### [429] [Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition](https://arxiv.org/pdf/2505.12007)
*Runduo Han, Xiuping Liu, Shangxuan Yi, Yi Zhang, Hongchen Tan*

Main category: cs.CV

TL;DR: The paper introduces MCO-E Net, a multi-modal network for single-eye expression recognition, addressing challenges like low light and high dynamic range. It features MCO-Mamba for dual-modal optimization and HCE-MoE for heterogeneous expert collaboration, achieving strong performance in poor lighting.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in single-eye expression recognition, such as low light and high dynamic range, by leveraging multi-modal collaboration and heterogeneous feature extraction.

Method: Proposes MCO-E Net with two key designs: MCO-Mamba for dual-modal optimization and HCE-MoE for dynamic expert routing, integrating deep, attention, and focal features.

Result: The network achieves competitive performance in single-eye expression recognition, particularly under poor lighting conditions.

Conclusion: MCO-E Net effectively addresses lighting challenges in expression recognition through multi-modal collaboration and heterogeneous feature integration.

Abstract: In this paper, we proposed a Multi-modal Collaborative Optimization and
Expansion Network (MCO-E Net), to use event modalities to resist challenges
such as low light, high exposure, and high dynamic range in single-eye
expression recognition tasks. The MCO-E Net introduces two innovative designs:
Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous
Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building
upon Mamba, leverages dual-modal information to jointly optimize the model,
facilitating collaborative interaction and fusion of modal semantics. This
approach encourages the model to balance the learning of both modalities and
harness their respective strengths. HCE-MoE, on the other hand, employs a
dynamic routing mechanism to distribute structurally varied experts (deep,
attention, and focal), fostering collaborative learning of complementary
semantics. This heterogeneous architecture systematically integrates diverse
feature extraction paradigms to comprehensively capture expression semantics.
Extensive experiments demonstrate that our proposed network achieves
competitive performance in the task of single-eye expression recognition,
especially under poor lighting conditions.

</details>


### [430] [VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.12081)
*Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, Jiaya Jia*

Main category: cs.CV

TL;DR: VisionReasoner is a unified framework for multiple visual perception tasks, outperforming Qwen2.5VL in detection, segmentation, and counting.


<details>
  <summary>Details</summary>
Motivation: To create a shared model capable of reasoning and solving diverse visual perception tasks efficiently.

Method: Uses multi-object cognitive learning and task reformulation to enhance reasoning and unify tasks.

Result: Achieves superior performance: 29.1% better on COCO, 22.1% on ReasonSeg, and 15.3% on CountBench.

Conclusion: VisionReasoner is effective as a unified model for diverse visual perception tasks.

Abstract: Large vision-language models exhibit inherent capabilities to handle diverse
visual perception tasks. In this paper, we introduce VisionReasoner, a unified
framework capable of reasoning and solving multiple visual perception tasks
within a shared model. Specifically, by designing novel multi-object cognitive
learning strategies and systematic task reformulation, VisionReasoner enhances
its reasoning capabilities to analyze visual inputs, and addresses diverse
perception tasks in a unified framework. The model generates a structured
reasoning process before delivering the desired outputs responding to user
queries. To rigorously assess unified visual perception capabilities, we
evaluate VisionReasoner on ten diverse tasks spanning three critical domains:
detection, segmentation, and counting. Experimental results show that
VisionReasoner achieves superior performance as a unified model, outperforming
Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg
(segmentation), and 15.3% on CountBench (counting).

</details>


### [431] [LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking](https://arxiv.org/pdf/2505.12753)
*Martha Teiko Teye, Ori Maoz, Matthias Rottmann*

Main category: cs.CV

TL;DR: A two-staged transformer-based approach for LiDAR multi-object tracking improves performance over traditional methods by refining detections and maintaining tracks using attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in LiDAR tracking due to sparse data and the need for temporal coherence, especially in crowded or dynamic scenes.

Method: Uses a two-staged DETR-inspired transformer: a smoother for refining detections and a tracker for maintaining object identities using attention.

Result: Outperforms baselines on nuScenes (aMOTA: 0.722, aMOTP: 0.475) and shows further improvement in offline mode (+3 pp aMOTP).

Conclusion: The proposed transformer-based method effectively handles LiDAR tracking challenges, achieving state-of-the-art performance.

Abstract: Multi-object tracking from LiDAR point clouds presents unique challenges due
to the sparse and irregular nature of the data, compounded by the need for
temporal coherence across frames. Traditional tracking systems often rely on
hand-crafted features and motion models, which can struggle to maintain
consistent object identities in crowded or fast-moving scenes. We present a
lidar-based two-staged DETR inspired transformer; a smoother and tracker. The
smoother stage refines lidar object detections, from any off-the-shelf
detector, across a moving temporal window. The tracker stage uses a DETR-based
attention block to maintain tracks across time by associating tracked objects
with the refined detections using the point cloud as context. The model is
trained on the datasets nuScenes and KITTI in both online and offline (forward
peeking) modes demonstrating strong performance across metrics such as
ID-switch and multiple object tracking accuracy (MOTA). The numerical results
indicate that the online mode outperforms the lidar-only baseline and SOTA
models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,
while the offline mode provides an additional 3 pp aMOTP.

</details>


### [432] [Place Recognition: A Comprehensive Review, Current Challenges and Future Directions](https://arxiv.org/pdf/2505.14068)
*Zhenyu Li, Tianyi Shang, Pengjie Xu, Zhaojun Deng*

Main category: cs.CV

TL;DR: A survey on place recognition methods, covering CNN-based, Transformer-based, and cross-modal approaches, with insights into datasets, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Place recognition is crucial for tasks like SLAM and long-term navigation, requiring robust methods to handle varying conditions.

Method: Review of CNN-based, Transformer-based, and cross-modal methods, analyzing their strengths and applications.

Result: Identifies advancements in descriptor learning, global dependency capture, and multi-modal integration, with available experimental results.

Conclusion: Highlights challenges like domain adaptation and real-time performance, suggesting future research directions.

Abstract: Place recognition is a cornerstone of vehicle navigation and mapping, which
is pivotal in enabling systems to determine whether a location has been
previously visited. This capability is critical for tasks such as loop closure
in Simultaneous Localization and Mapping (SLAM) and long-term navigation under
varying environmental conditions. In this survey, we comprehensively review
recent advancements in place recognition, emphasizing three representative
methodological paradigms: Convolutional Neural Network (CNN)-based approaches,
Transformer-based frameworks, and cross-modal strategies. We begin by
elucidating the significance of place recognition within the broader context of
autonomous systems. Subsequently, we trace the evolution of CNN-based methods,
highlighting their contributions to robust visual descriptor learning and
scalability in large-scale environments. We then examine the emerging class of
Transformer-based models, which leverage self-attention mechanisms to capture
global dependencies and offer improved generalization across diverse scenes.
Furthermore, we discuss cross-modal approaches that integrate heterogeneous
data sources such as Lidar, vision, and text description, thereby enhancing
resilience to viewpoint, illumination, and seasonal variations. We also
summarize standard datasets and evaluation metrics widely adopted in the
literature. Finally, we identify current research challenges and outline
prospective directions, including domain adaptation, real-time performance, and
lifelong learning, to inspire future advancements in this domain. The unified
framework of leading-edge place recognition methods, i.e., code library, and
the results of their experimental evaluations are available at
https://github.com/CV4RA/SOTA-Place-Recognitioner.

</details>


### [433] [UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens](https://arxiv.org/pdf/2505.14671)
*Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao Zhang*

Main category: cs.CV

TL;DR: UniCTokens introduces a unified framework for personalized concept understanding and generation, addressing limitations of isolated tasks. It achieves state-of-the-art results in knowledge-driven generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat concept understanding and generation separately, limiting performance for complex prompts. UniCTokens aims to unify these tasks for better results.

Method: UniCTokens trains unified concept tokens in a vision language model (VLM) using a progressive three-stage training strategy: understanding warm-up, bootstrapping generation, and deepening understanding.

Result: UniCTokens outperforms leading methods in concept understanding and generation, achieving state-of-the-art in personalized knowledge-driven generation.

Conclusion: Unifying understanding and generation enhances both tasks, with generation providing insights for understanding. The framework and benchmark (UnifyBench) are publicly available.

Abstract: Personalized models have demonstrated remarkable success in understanding and
generating concepts provided by users. However, existing methods use separate
concept tokens for understanding and generation, treating these tasks in
isolation. This may result in limitations for generating images with complex
prompts. For example, given the concept $\langle bo\rangle$, generating
"$\langle bo\rangle$ wearing its hat" without additional textual descriptions
of its hat. We call this kind of generation personalized knowledge-driven
generation. To address the limitation, we present UniCTokens, a novel framework
that effectively integrates personalized information into a unified vision
language model (VLM) for understanding and generation. UniCTokens trains a set
of unified concept tokens to leverage complementary semantics, boosting two
personalized tasks. Moreover, we propose a progressive training strategy with
three stages: understanding warm-up, bootstrapping generation from
understanding, and deepening understanding from generation to enhance mutual
benefits between both tasks. To quantitatively evaluate the unified VLM
personalization, we present UnifyBench, the first benchmark for assessing
concept understanding, concept generation, and knowledge-driven generation.
Experimental results on UnifyBench indicate that UniCTokens shows competitive
performance compared to leading methods in concept understanding, concept
generation, and achieving state-of-the-art results in personalized
knowledge-driven generation. Our research demonstrates that enhanced
understanding improves generation, and the generation process can yield
valuable insights into understanding. Our code and dataset will be released at:
\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.

</details>


### [434] [Uncovering Cultural Representation Disparities in Vision-Language Models](https://arxiv.org/pdf/2505.14729)
*Ram Mohan Rao Kadiyala, Siddhant Gupta, Jebish Purbey, Srishti Yadav, Alejandro Salamanca, Desmond Elliott*

Main category: cs.CV

TL;DR: The paper investigates cultural biases in Vision-Language Models (VLMs) by evaluating their performance on a country identification task using the Country211 dataset, revealing disparities influenced by training data and prompting strategies.


<details>
  <summary>Details</summary>
Motivation: To assess and uncover cultural biases in VLMs, which could impact their fairness and generalization across diverse global contexts.

Method: Evaluated VLMs on the Country211 dataset using open-ended questions, multiple-choice questions (MCQs), and challenging setups like multilingual and adversarial settings.

Result: Significant performance variations across countries and question formats, indicating biases inherited from pre-training data.

Conclusion: VLMs exhibit cultural biases due to training data distribution, affecting their uniform generalization globally.

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities
across a range of tasks, yet concerns about their potential biases exist. This
work investigates the extent to which prominent VLMs exhibit cultural biases by
evaluating their performance on an image-based country identification task at a
country level. Utilizing the geographically diverse Country211 dataset, we
probe several large vision language models (VLMs) under various prompting
strategies: open-ended questions, multiple-choice questions (MCQs) including
challenging setups like multilingual and adversarial settings. Our analysis
aims to uncover disparities in model accuracy across different countries and
question formats, providing insights into how training data distribution and
evaluation methodologies might influence cultural biases in VLMs. The findings
highlight significant variations in performance, suggesting that while VLMs
possess considerable visual understanding, they inherit biases from their
pre-training data and scale that impact their ability to generalize uniformly
across diverse global contexts.

</details>


### [435] [Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives](https://arxiv.org/pdf/2505.15222)
*Yisi Luo, Xile Zhao, Deyu Meng*

Main category: cs.CV

TL;DR: The paper reviews continuous representation methods, highlighting their advantages over discrete frameworks, and explores their designs, theoretical foundations, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To systematically examine advancements in continuous representation frameworks, emphasizing their superiority in data representation and reconstruction.

Method: The review focuses on three aspects: method designs (e.g., implicit neural representation), theoretical foundations (e.g., approximation error analysis), and applications (e.g., computer vision).

Result: Continuous representations offer resolution flexibility, cross-modal adaptability, and parameter efficiency, with diverse applications across fields.

Conclusion: The paper outlines future directions to inspire further exploration and development of continuous representation methods and theories.

Abstract: Recently, continuous representation methods emerge as novel paradigms that
characterize the intrinsic structures of real-world data through function
representations that map positional coordinates to their corresponding values
in the continuous space. As compared with the traditional discrete framework,
the continuous framework demonstrates inherent superiority for data
representation and reconstruction (e.g., image restoration, novel view
synthesis, and waveform inversion) by offering inherent advantages including
resolution flexibility, cross-modal adaptability, inherent smoothness, and
parameter efficiency. In this review, we systematically examine recent
advancements in continuous representation frameworks, focusing on three
aspects: (i) Continuous representation method designs such as basis function
representation, statistical modeling, tensor function decomposition, and
implicit neural representation; (ii) Theoretical foundations of continuous
representations such as approximation error analysis, convergence property, and
implicit regularization; (iii) Real-world applications of continuous
representations derived from computer vision, graphics, bioinformatics, and
remote sensing. Furthermore, we outline future directions and perspectives to
inspire exploration and deepen insights to facilitate continuous representation
methods, theories, and applications. All referenced works are summarized in our
open-source repository:
https://github.com/YisiLuo/Continuous-Representation-Zoo

</details>


### [436] [Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation](https://arxiv.org/pdf/2505.15267)
*Wenmin Li, Shunsuke Sakai, Tatsuhito Hasegawa*

Main category: cs.CV

TL;DR: A novel dataset distillation method integrates contrastive learning to enhance synthetic data quality under extreme sample scarcity, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Current dataset distillation methods fail to preserve semantic richness in very small datasets, limiting their effectiveness in resource-constrained environments.

Method: The proposed method combines contrastive learning with image synthesis to maximize instance-level feature discrimination, improving diversity and informativeness of synthetic samples.

Result: The approach significantly boosts model performance and visual fidelity on small-scale synthetic datasets, outperforming existing methods.

Conclusion: Integrating contrastive learning in dataset distillation enhances feature representation and synthetic data quality, especially in extreme scarcity scenarios.

Abstract: Deploying machine learning models in resource-constrained environments, such
as edge devices or rapid prototyping scenarios, increasingly demands
distillation of large datasets into significantly smaller yet informative
synthetic datasets. Current dataset distillation techniques, particularly
Trajectory Matching methods, optimize synthetic data so that the model's
training trajectory on synthetic samples mirrors that on real data. While
demonstrating efficacy on medium-scale synthetic datasets, these methods fail
to adequately preserve semantic richness under extreme sample scarcity. To
address this limitation, we propose a novel dataset distillation method
integrating contrastive learning during image synthesis. By explicitly
maximizing instance-level feature discrimination, our approach produces more
informative and diverse synthetic samples, even when dataset sizes are
significantly constrained. Experimental results demonstrate that incorporating
contrastive learning substantially enhances the performance of models trained
on very small-scale synthetic datasets. This integration not only guides more
effective feature representation but also significantly improves the visual
fidelity of the synthesized images. Experimental results demonstrate that our
method achieves notable performance improvements over existing distillation
techniques, especially in scenarios with extremely limited synthetic data.

</details>


### [437] [Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model](https://arxiv.org/pdf/2505.15358)
*Angelique Mangubat, Shane Gilroy*

Main category: cs.CV

TL;DR: A novel benchmark for bicycle occlusion level classification using computer vision improves cyclist safety by objectively quantifying visibility and occlusion.


<details>
  <summary>Details</summary>
Motivation: Enhancing road safety for cyclists, who are vulnerable road users, by addressing the challenge of occlusion in cyclist detection.

Method: Utilizes a parts-based detection model and custom image detection pipeline to classify bicycle occlusion levels.

Result: The model robustly quantifies bicycle visibility and occlusion, outperforming current subjective methods.

Conclusion: The proposed methodology can improve cyclist detection algorithms, aiding autonomous vehicle safety systems.

Abstract: Road safety is a critical challenge, particularly for cyclists, who are among
the most vulnerable road users. This study aims to enhance road safety by
proposing a novel benchmark for bicycle occlusion level classification using
advanced computer vision techniques. Utilizing a parts-based detection model,
images are annotated and processed through a custom image detection pipeline. A
novel method of bicycle occlusion level is proposed to objectively quantify the
visibility and occlusion level of bicycle semantic parts. The findings indicate
that the model robustly quantifies the visibility and occlusion level of
bicycles, a significant improvement over the subjective methods used by the
current state of the art. Widespread use of the proposed methodology will
facilitate accurate performance reporting of cyclist detection algorithms for
occluded cyclists, informing the development of more robust vulnerable road
user detection methods for autonomous vehicles.

</details>


### [438] [Stronger ViTs With Octic Equivariance](https://arxiv.org/pdf/2505.15441)
*David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman*

Main category: cs.CV

TL;DR: Optic ViTs, incorporating octic-equivariant layers, improve efficiency and performance in vision tasks, reducing FLOPs by 40% for ViT-H.


<details>
  <summary>Details</summary>
Motivation: To enhance Vision Transformers (ViTs) by integrating equivariance under the octic group (reflections and 90-degree rotations) as an inductive bias.

Method: Develop octic ViTs with octic-equivariant layers and evaluate them on supervised and self-supervised learning tasks using DeiT-III and DINOv2 on ImageNet-1K.

Result: 40% reduction in FLOPs for ViT-H, with improved classification and segmentation performance.

Conclusion: Optic ViTs offer a computationally efficient and high-performing alternative to traditional ViTs.

Abstract: Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [439] [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/pdf/2505.16997)
*Rui Ye, Xiangrui Liu, Qimin Wu, Xianghe Pang, Zhenfei Yin, Lei Bai, Siheng Chen*

Main category: cs.AI

TL;DR: Heterogeneous LLM-driven multi-agent systems (X-MAS) outperform homogeneous systems by leveraging diverse LLMs, achieving significant performance boosts without structural changes.


<details>
  <summary>Details</summary>
Motivation: Existing MAS frameworks limit intelligence to a single LLM, missing the potential of diverse collective intelligence.

Method: Introduces X-MAS-Bench, a testbed evaluating 27 LLMs across 5 domains and 5 functions, with over 1.7M evaluations.

Result: Heterogeneous MAS improves performance by up to 8.4% (MATH dataset) and 47% (AIME dataset) compared to homogeneous systems.

Conclusion: Heterogeneous LLMs in MAS offer transformative potential for scalable, collaborative AI systems.

Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.

</details>


### [440] [Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems](https://arxiv.org/pdf/2505.15862)
*Long Wanga, Jiongzhi Zheng, Zhengda Xiong, ChuMin Li, Kun He*

Main category: cs.AI

TL;DR: The paper proposes a dynamic candidate edge selection method using multi-armed bandit models to improve the Lin-Kernighan-Helsgaun (LKH) algorithm for TSP, enhancing solution quality and performance.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms like LKH use static candidate edges, leading to local optima traps and suboptimal solutions.

Method: Incorporates multi-armed bandit models to dynamically select promising candidate edges during iterations.

Result: Experiments show improved performance on TSP benchmarks and enhanced LKH-3 for TSP variants.

Conclusion: Dynamic candidate selection via bandit models significantly boosts LKH and LKH-3 performance.

Abstract: Algorithms designed for routing problems typically rely on high-quality
candidate edges to guide their search, aiming to reduce the search space and
enhance the search efficiency. However, many existing algorithms, like the
classical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman
Problem (TSP), often use predetermined candidate edges that remain static
throughout local searches. This rigidity could cause the algorithm to get
trapped in local optima, limiting its potential to find better solutions. To
address this issue, we propose expanding the candidate sets to include other
promising edges, providing them an opportunity for selection. Specifically, we
incorporate multi-armed bandit models to dynamically select the most suitable
candidate edges in each iteration, enabling LKH to make smarter choices and
lead to improved solutions. Extensive experiments on multiple TSP benchmarks
show the excellent performance of our method. Moreover, we employ this
bandit-based method to LKH-3, an extension of LKH tailored for solving various
TSP variant problems, and our method also significantly enhances LKH-3's
performance across typical TSP variants.

</details>


### [441] [PhyX: Does Your Model Have the "Wits" for Physical Reasoning?](https://arxiv.org/pdf/2505.15929)
*Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, Zhongwei Wan, Kai Zhang, Wendong Xu, Jing Xiong, Ping Luo, Wenhu Chen, Chaofan Tao, Zhuoqing Mao, Ngai Wong*

Main category: cs.AI

TL;DR: PhyX is a new benchmark for physics-grounded reasoning in visual scenarios, revealing significant gaps in current AI models' capabilities compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack assessment of physical reasoning, a key aspect of intelligence. PhyX fills this gap by evaluating models' ability to integrate domain knowledge, symbolic reasoning, and real-world constraints.

Method: PhyX includes 3K multimodal questions across 6 reasoning types and 25 sub-domains in 6 core physics areas. It uses a comprehensive evaluation protocol with fine-grained statistics and case studies.

Result: State-of-the-art models (GPT-4o, Claude3.7-Sonnet, GPT-o4-mini) perform poorly (32.5%-45.8% accuracy), lagging behind human experts by over 29%. Key limitations include reliance on memorization, math formulations, and surface-level visual matching.

Conclusion: PhyX highlights critical shortcomings in AI models' physical reasoning, providing a robust benchmark for future improvements. The evaluation protocol ensures reproducibility and standardized assessment.

Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence:
physical reasoning, the integrated ability to combine domain knowledge,
symbolic reasoning, and understanding of real-world constraints. To address
this gap, we introduce PhyX: the first large-scale benchmark designed to assess
models capacity for physics-grounded reasoning in visual scenarios. PhyX
includes 3K meticulously curated multimodal questions spanning 6 reasoning
types across 25 sub-domains and 6 core physics domains: thermodynamics,
electromagnetism, mechanics, modern physics, optics, and wave\&acoustics. In
our comprehensive evaluation, even state-of-the-art models struggle
significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and
GPT-o4-mini achieve only 32.5\%, 42.2\%, and 45.8\% accuracy
respectively-performance gaps exceeding 29\% compared to human experts. Our
analysis exposes critical limitations in current models: over-reliance on
memorized disciplinary knowledge, excessive dependence on mathematical
formulations, and surface-level visual pattern matching rather than genuine
physical understanding. We provide in-depth analysis through fine-grained
statistics, detailed case studies, and multiple evaluation paradigms to
thoroughly examine physical reasoning capabilities. To ensure reproducibility,
we implement a compatible evaluation protocol based on widely-used toolkits
such as VLMEvalKit, enabling one-click evaluation.

</details>


### [442] [Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics](https://arxiv.org/pdf/2505.15998)
*Thomas Michel, Marko Cvjetko, Gautier Hamon, Pierre-Yves Oudeyer, Clément Moulin-Frier*

Main category: cs.AI

TL;DR: A method for automated discovery of system-level dynamics in Flow-Lenia using curiosity-driven AI, revealing diverse self-organized behaviors beyond individual patterns.


<details>
  <summary>Details</summary>
Motivation: To uncover self-organization in continuous cellular automata (CAs) with mass conservation, extending beyond individual patterns to ecosystemic dynamics.

Method: Adapts Intrinsically Motivated Goal Exploration Processes (IMGEPs) with simulation-wide metrics (evolutionary activity, complexity, entropy) to explore diverse Flow-Lenia environments.

Result: Outperforms random search in discovering diverse dynamics, showcasing complex collective behaviors not found in individual pattern analysis.

Conclusion: Provides a framework for automated and collaborative exploration of emergent properties in parameterizable complex systems.

Abstract: We present a method for the automated discovery of system-level dynamics in
Flow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and
parameter localization$-$using a curiosity-driven AI scientist. This method
aims to uncover processes leading to self-organization of evolutionary and
ecosystemic dynamics in CAs. We build on previous work which uses diversity
search algorithms in Lenia to find self-organized individual patterns, and
extend it to large environments that support distinct interacting patterns. We
adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive
exploration of diverse Flow-Lenia environments using simulation-wide metrics,
such as evolutionary activity, compression-based complexity, and multi-scale
entropy. We test our method in two experiments, showcasing its ability to
illuminate significantly more diverse dynamics compared to random search. We
show qualitative results illustrating how ecosystemic simulations enable
self-organization of complex collective behaviors not captured by previous
individual pattern search and analysis. We complement automated discovery with
an interactive exploration tool, creating an effective human-AI collaborative
workflow for scientific investigation. Though demonstrated specifically with
Flow-Lenia, this methodology provides a framework potentially applicable to
other parameterizable complex systems where understanding emergent collective
properties is of interest.

</details>


### [443] [Children's Mental Models of AI Reasoning: Implications for AI Literacy Education](https://arxiv.org/pdf/2505.16031)
*Aayushi Dangol, Robert Wolfe, Runhua Zhao, JaeWon Kim, Trushaa Ramanan, Katie Davis, Julie A. Kientz*

Main category: cs.AI

TL;DR: The paper explores children's mental models of AI reasoning, identifying three types (Deductive, Inductive, Inherent) and age-related differences in understanding. It suggests implications for AI education and explainable AI design.


<details>
  <summary>Details</summary>
Motivation: Understanding how children conceptualize AI's reasoning processes is critical for fostering AI literacy, especially as AI advances in reasoning capabilities.

Method: A two-phase approach: co-design sessions with 8 children and a field study with 106 children (grades 3-8).

Result: Younger children (grades 3-5) see AI's reasoning as inherent, while older children (grades 6-8) view it as pattern recognition. Three tensions in understanding were identified.

Conclusion: The findings have implications for designing AI curricula and explainable AI tools to better support children's AI literacy.

Abstract: As artificial intelligence (AI) advances in reasoning capabilities, most
recently with the emergence of Large Reasoning Models (LRMs), understanding how
children conceptualize AI's reasoning processes becomes critical for fostering
AI literacy. While one of the "Five Big Ideas" in AI education highlights
reasoning algorithms as central to AI decision-making, less is known about
children's mental models in this area. Through a two-phase approach, consisting
of a co-design session with 8 children followed by a field study with 106
children (grades 3-8), we identified three models of AI reasoning: Deductive,
Inductive, and Inherent. Our findings reveal that younger children (grades 3-5)
often attribute AI's reasoning to inherent intelligence, while older children
(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions
that surfaced in children's understanding of AI reasoning and conclude with
implications for scaffolding AI curricula and designing explainable AI tools.

</details>


### [444] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/pdf/2505.16037)
*Asterios Tsiourvas, Wei Sun, Georgia Perakis*

Main category: cs.AI

TL;DR: A causal end-to-end framework for LLM routing learns policies from observational data, outperforming decoupled strategies by minimizing regret and handling cost preferences.


<details>
  <summary>Details</summary>
Motivation: Prior decoupled strategies for LLM routing suffer from compounding errors and require costly full-feedback data. Observational data offers a practical alternative.

Method: Proposes a causal framework with surrogate objectives: a classification-based upper bound and a softmax-weighted regret approximation. Extends to handle cost preferences.

Result: Outperforms baselines on public benchmarks, achieving state-of-the-art performance across embedding models.

Conclusion: The framework effectively learns routing policies from observational data, addressing limitations of prior approaches.

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [445] [SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution](https://arxiv.org/pdf/2505.16048)
*Philipp D. Siedler*

Main category: cs.AI

TL;DR: A new dataset benchmarks LLMs' physical and spatial reasoning using topology optimization tasks in 2D settings.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to reason about material distribution and structural stability without simulation tools.

Method: LLMs are given boundary conditions, forces, and supports to predict optimal material distributions in tasks like filling masked regions or predicting complete structures.

Result: The dataset challenges LLMs to understand force flow and material distribution under constraints, focusing on spatial and physical reasoning.

Conclusion: This dataset provides a unique benchmark for assessing LLMs' spatial and physical reasoning, complementing traditional language and logic tests.

Abstract: We introduce a novel dataset designed to benchmark the physical and spatial
reasoning capabilities of Large Language Models (LLM) based on topology
optimization, a method for computing optimal material distributions within a
design space under prescribed loads and supports. In this dataset, LLMs are
provided with conditions such as 2D boundary, applied forces and supports, and
must reason about the resulting optimal material distribution. The dataset
includes a variety of tasks, ranging from filling in masked regions within
partial structures to predicting complete material distributions. Solving these
tasks requires understanding the flow of forces and the required material
distribution under given constraints, without access to simulation tools or
explicit physical models, challenging models to reason about structural
stability and spatial organization. Our dataset targets the evaluation of
spatial and physical reasoning abilities in 2D settings, offering a
complementary perspective to traditional language and logic benchmarks.

</details>


### [446] [How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior](https://arxiv.org/pdf/2505.16067)
*Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Jiliang Tang, Himabindu Lakkaraju, Zhen Xiang*

Main category: cs.AI

TL;DR: The paper studies how memory management (addition and deletion of experiences) impacts LLM agents' long-term performance, identifying challenges like error propagation and misaligned experience replay. Selective strategies improve performance by 10%.


<details>
  <summary>Details</summary>
Motivation: Memory is crucial for LLM agents to improve task performance over time, but poor memory management can lead to degraded behavior. This study aims to understand and mitigate such issues.

Method: Empirical study focusing on memory operations (addition and deletion) and their impact on agent behavior, using quantitative analysis and controlled experiments.

Result: LLM agents exhibit experience-following behavior, leading to challenges like error propagation and misaligned replay. Selective memory strategies improve performance by 10%.

Conclusion: Effective memory management (selective addition/deletion) enhances LLM agent robustness, especially under challenging conditions. The findings guide better memory system design.

Abstract: Memory is a critical component in large language model (LLM)-based agents,
enabling them to store and retrieve past executions to improve task performance
over time. In this paper, we conduct an empirical study on how memory
management choices impact the LLM agents' behavior, especially their long-term
performance. Specifically, we focus on two fundamental memory operations that
are widely used by many agent frameworks-addition, which incorporates new
experiences into the memory base, and deletion, which selectively removes past
experiences-to systematically study their impact on the agent behavior. Through
our quantitative analysis, we find that LLM agents display an
experience-following property: high similarity between a task input and the
input in a retrieved memory record often results in highly similar agent
outputs. Our analysis further reveals two significant challenges associated
with this property: error propagation, where inaccuracies in past experiences
compound and degrade future performance, and misaligned experience replay,
where outdated or irrelevant experiences negatively influence current tasks.
Through controlled experiments, we show that combining selective addition and
deletion strategies can help mitigate these negative effects, yielding an
average absolute performance gain of 10% compared to naive memory growth.
Furthermore, we highlight how memory management choices affect agents' behavior
under challenging conditions such as task distribution shifts and constrained
memory resources. Our findings offer insights into the behavioral dynamics of
LLM agent memory systems and provide practical guidance for designing memory
components that support robust, long-term agent performance. We also release
our code to facilitate further study.

</details>


### [447] [SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation](https://arxiv.org/pdf/2505.16080)
*Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang*

Main category: cs.AI

TL;DR: SynEVO is a spatiotemporal network that enhances cross-domain knowledge transfer by enabling collective intelligence and model evolution, improving generalization by up to 42%.


<details>
  <summary>Details</summary>
Motivation: Current spatiotemporal learners lack transferability across domains, requiring new designs for correlated tasks. SynEVO aims to address this by leveraging collective intelligence inspired by neuroscience.

Method: SynEVO reorders sample groups for curriculum learning, uses elastic common containers and task-independent extractors for model growth, and employs an adaptive dynamic coupler for domain adaptation.

Result: SynEVO improves generalization by up to 42% in cross-domain scenarios, demonstrating effective knowledge transfer.

Conclusion: SynEVO offers a NeuroAI paradigm for spatiotemporal knowledge transfer and adaptation, breaking model independence barriers.

Abstract: Discovering regularities from spatiotemporal systems can benefit various
scientific and social planning. Current spatiotemporal learners usually train
an independent model from a specific source data that leads to limited
transferability among sources, where even correlated tasks requires new design
and training. The key towards increasing cross-domain knowledge is to enable
collective intelligence and model evolution. In this paper, inspired by
neuroscience theories, we theoretically derive the increased information
boundary via learning cross-domain collective intelligence and propose a
Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the
model independence and enables cross-domain knowledge to be shared and
aggregated. Specifically, we first re-order the sample groups to imitate the
human curriculum learning, and devise two complementary learners, elastic
common container and task-independent extractor to allow model growth and
task-wise commonality and personality disentanglement. Then an adaptive dynamic
coupler with a new difference metric determines whether the new sample group
should be incorporated into common container to achieve model evolution under
various domains. Experiments show that SynEVO improves the generalization
capacity by at most 42% under cross-domain scenarios and SynEVO provides a
paradigm of NeuroAI for knowledge transfer and adaptation.

</details>


### [448] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/pdf/2505.16086)
*Ming Shen, Raphael Shu, Anurag Pratik, James Gung, Yubin Ge, Monica Sunkara, Yi Zhang*

Main category: cs.AI

TL;DR: The paper presents a two-step optimization pipeline for LLM-based multi-agent systems, using natural language feedback to improve performance in software development tasks. It compares online vs. offline and individual vs. group optimization, showing effectiveness and practical insights.


<details>
  <summary>Details</summary>
Motivation: Optimizing LLM-based multi-agent systems for complex tasks like software development is challenging, necessitating empirical study and practical solutions.

Method: A two-step pipeline: identifying underperforming agents via textual feedback and optimizing their prompts. Comparisons include online/offline and individual/group optimization, with one-pass and multi-pass prompting strategies.

Result: The method effectively optimizes role-based multi-agent systems for software tasks, with varied impacts from different optimization settings.

Conclusion: The study provides practical insights for future development of LLM-based multi-agent systems, demonstrating the effectiveness of prompt optimization.

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [449] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/pdf/2505.16090)
*Dominick Kubica, Dylan T. Gordon, Nanami Emura, Derleen Saini, Charlie Goldenberg*

Main category: cs.AI

TL;DR: The paper evaluates the reliability of GenAI and LLMs in sentiment analysis for financial texts, focusing on earnings call transcripts, and compares performance across models like Microsoft's Copilot and OpenAI's ChatGPT.


<details>
  <summary>Details</summary>
Motivation: Assess the accuracy of LLMs in high-stakes financial contexts, where nuanced language and sentiment are challenging to interpret.

Method: Benchmarks LLMs and traditional models using Microsoft earnings call transcripts, analyzing sentiment correlation with market trends and stock movements. Examines prompt engineering for improved results.

Result: LLMs struggle with nuanced financial language, but prompt engineering can enhance sentiment analysis accuracy. Visualizations show sentiment trends and their alignment with stock performance.

Conclusion: While LLMs face challenges in financial sentiment analysis, targeted improvements like prompt engineering can bridge gaps, aiding better market insights.

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [450] [TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials](https://arxiv.org/pdf/2505.16097)
*Zifeng Wang, Qiao Jin, Jiacheng Lin, Junyi Gao, Jathurshan Pradeepkumar, Pengcheng Jiang, Benjamin Danek, Zhiyong Lu, Jimeng Sun*

Main category: cs.AI

TL;DR: TrialPanorama is a large-scale, structured database of clinical trial records, linked to biomedical ontologies, serving as a resource for AI tasks in clinical trials. Benchmarks show general-purpose LLMs are insufficient for high-stakes workflows.


<details>
  <summary>Details</summary>
Motivation: To provide a unified, extensible data foundation for AI in clinical trials, addressing gaps in training and evaluation.

Method: Introduce TrialPanorama, a database of 1.6M trial records from 15 sources, linked to ontologies. Derive benchmark tasks for systematic review and trial design, testing five LLMs.

Result: General-purpose LLMs show limited zero-shot capability, inadequate for clinical trial workflows.

Conclusion: TrialPanorama and its benchmark are released to advance AI research in clinical trials, highlighting the need for domain-specific models.

Abstract: Developing artificial intelligence (AI) for vertical domains requires a solid
data foundation for both training and evaluation. In this work, we introduce
TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical
trial records aggregated from 15 global sources. The database captures key
aspects of trial design and execution, including trial setups, interventions,
conditions, biomarkers, and outcomes, and links them to standard biomedical
ontologies such as DrugBank and MedDRA. This structured and ontology-grounded
design enables TrialPanorama to serve as a unified, extensible resource for a
wide range of clinical trial tasks, including trial planning, design, and
summarization. To demonstrate its utility, we derive a suite of benchmark tasks
directly from the TrialPanorama database. The benchmark spans eight tasks
across two categories: three for systematic review (study search, study
screening, and evidence summarization) and five for trial design (arm design,
eligibility criteria, endpoint selection, sample size estimation, and trial
completion assessment). The experiments using five state-of-the-art large
language models (LLMs) show that while general-purpose LLMs exhibit some
zero-shot capability, their performance is still inadequate for high-stakes
clinical trial workflows. We release TrialPanorama database and the benchmark
to facilitate further research on AI for clinical trials.

</details>


### [451] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/pdf/2505.16100)
*Zifeng Wang, Benjamin Danek, Jimeng Sun*

Main category: cs.AI

TL;DR: BioDSA-1K is a benchmark for evaluating AI agents on biomedical hypothesis validation tasks, featuring 1,029 tasks with structured hypotheses and evidence from real studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of validating scientific hypotheses in biomedical research using AI, given the complexity of real-world data.

Method: Curated 1,029 tasks with hypotheses and evidence from 300+ studies, focusing on realistic workflows and including non-verifiable cases.

Result: The benchmark evaluates AI agents on accuracy, evidence alignment, reasoning correctness, and code executability.

Conclusion: BioDSA-1K serves as a foundation for developing trustworthy AI agents in biomedical discovery.

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [452] [Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language](https://arxiv.org/pdf/2505.16114)
*Naiqi Li, Peiyuan Liu, Zheng Liu, Tao Dai, Yong Jiang, Shu-Tao Xia*

Main category: cs.AI

TL;DR: A hybrid framework, Logic-of-Thought (Logot), combines LLMs and logic programming to solve complex puzzles with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving natural language puzzles, which LLMs struggle with due to demands for precise reasoning.

Method: Uses LLMs to translate puzzle rules into answer set programs (ASPs), solved by an ASP interpreter.

Result: Achieves near-perfect accuracy on grid and dynamic puzzles.

Conclusion: Logot effectively bridges LLMs and logic programming for precise puzzle-solving.

Abstract: Solving puzzles in natural language poses a long-standing challenge in AI.
While large language models (LLMs) have recently shown impressive capabilities
in a variety of tasks, they continue to struggle with complex puzzles that
demand precise reasoning and exhaustive search. In this paper, we propose
Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic
programming to address this problem. Our method leverages LLMs to translate
puzzle rules and states into answer set programs (ASPs), the solution of which
are then accurately and efficiently inferred by an ASP interpreter. This hybrid
approach combines the natural language understanding of LLMs with the precise
reasoning capabilities of logic programs. We evaluate our method on various
grid puzzles and dynamic puzzles involving actions, demonstrating near-perfect
accuracy across all tasks. Our code and data are available at:
https://github.com/naiqili/Logic-of-Thought.

</details>


### [453] [LLM-Powered AI Agent Systems and Their Applications in Industry](https://arxiv.org/pdf/2505.16120)
*Guannan Liang, Qianqian Tong*

Main category: cs.AI

TL;DR: The paper explores the evolution of agent systems from rule-based to LLM-powered architectures, highlighting their flexibility, multi-modal capabilities, and applications across various domains, while addressing challenges like latency, uncertainty, and security.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has transformed agent systems, enabling broader task scope and natural language interaction, necessitating a review of their evolution, applications, and challenges.

Method: The paper categorizes agent systems into software-based, physical, and hybrid systems, examining their applications and analyzing challenges like inference latency and security vulnerabilities.

Result: LLM-powered agents offer enhanced flexibility and multi-modal processing but face issues like high latency and output uncertainty. Potential solutions are proposed.

Conclusion: LLM-powered agents represent a significant advancement but require addressing key challenges to fully realize their potential across diverse applications.

Abstract: The emergence of Large Language Models (LLMs) has reshaped agent systems.
Unlike traditional rule-based agents with limited task scope, LLM-powered
agents offer greater flexibility, cross-domain reasoning, and natural language
interaction. Moreover, with the integration of multi-modal LLMs, current agent
systems are highly capable of processing diverse data modalities, including
text, images, audio, and structured tabular data, enabling richer and more
adaptive real-world behavior. This paper comprehensively examines the evolution
of agent systems from the pre-LLM era to current LLM-powered architectures. We
categorize agent systems into software-based, physical, and adaptive hybrid
systems, highlighting applications across customer service, software
development, manufacturing automation, personalized education, financial
trading, and healthcare. We further discuss the primary challenges posed by
LLM-powered agents, including high inference latency, output uncertainty, lack
of evaluation metrics, and security vulnerabilities, and propose potential
solutions to mitigate these concerns.

</details>


### [454] [Sudoku-Bench: Evaluating creative reasoning with Sudoku variants](https://arxiv.org/pdf/2505.16135)
*Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, Llion Jones*

Main category: cs.AI

TL;DR: Sudoku-Bench is a benchmark for evaluating creative, multi-step reasoning in LLMs using unconventional Sudoku variants, where memorization is ineffective.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning benchmarks for LLMs often reward memorization, failing to assess authentic creativity and novel logical reasoning.

Method: Sudoku-Bench uses challenging Sudoku variants with unique constraints, requiring solvers to identify novel logical breakthroughs ("break-ins"). It includes a curated puzzle set, standardized representation, and flexible tools.

Result: State-of-the-art LLMs solve fewer than 15% of puzzles unaided, indicating room for improvement in long-horizon reasoning.

Conclusion: Sudoku-Bench effectively evaluates creative reasoning in LLMs and highlights opportunities for advancing strategic reasoning capabilities.

Abstract: Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.

</details>


### [455] [Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value](https://arxiv.org/pdf/2505.16147)
*Le Ma, Shirao Yang, Zihao Wang, Yinggui Wang, Lei Wang, Tao Wei, Kejun Zhang*

Main category: cs.AI

TL;DR: Proposes Unlearning Shapley, a framework using machine unlearning to efficiently compute data values via performance shifts, avoiding retraining and full data access.


<details>
  <summary>Details</summary>
Motivation: Addresses the inefficiency and impracticality of traditional data valuation methods (e.g., Shapley value, influence functions) for large models and partial data.

Method: Leverages machine unlearning to estimate Shapley values by measuring performance shifts on a test set after unlearning target data, using Monte Carlo sampling.

Result: Matches state-of-the-art accuracy while significantly reducing computational costs, validated on benchmark datasets and large text corpora.

Conclusion: Provides a scalable, privacy-compliant solution for data valuation, bridging theory and practical deployment in AI ecosystems.

Abstract: The proliferation of large models has intensified the need for efficient data
valuation methods to quantify the contribution of individual data providers.
Traditional approaches, such as game-theory-based Shapley value and
influence-function-based techniques, face prohibitive computational costs or
require access to full data and model training details, making them hardly
achieve partial data valuation. To address this, we propose Unlearning Shapley,
a novel framework that leverages machine unlearning to estimate data values
efficiently. By unlearning target data from a pretrained model and measuring
performance shifts on a reachable test set, our method computes Shapley values
via Monte Carlo sampling, avoiding retraining and eliminating dependence on
full data. Crucially, Unlearning Shapley supports both full and partial data
valuation, making it scalable for large models (e.g., LLMs) and practical for
data markets. Experiments on benchmark datasets and large-scale text corpora
demonstrate that our approach matches the accuracy of state-of-the-art methods
while reducing computational overhead by orders of magnitude. Further analysis
confirms a strong correlation between estimated values and the true impact of
data subsets, validating its reliability in real-world scenarios. This work
bridges the gap between data valuation theory and practical deployment,
offering a scalable, privacy-compliant solution for modern AI ecosystems.

</details>


### [456] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/pdf/2505.16176)
*Jun Rao, Xuebo Liu, Hexuan Deng, Zepeng Lin, Zixiong Yu, Jiansheng Wei, Xiaojun Meng, Min Zhang*

Main category: cs.AI

TL;DR: SAI-DPO is a dynamic data selection algorithm for reasoning tasks, outperforming static methods by adapting to model capabilities during training, achieving up to 21.3% performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing static data selection methods lack adaptability to evolving model capabilities, especially in dynamic training paradigms like online RL.

Method: SAI-DPO dynamically selects training data by assessing model reasoning abilities in real-time, aligning with evolving strengths and weaknesses.

Result: SAI-DPO boosts performance by up to 21.3%, with notable gains of 10 and 15 points on AIME24 and AMC23 benchmarks.

Conclusion: Dynamic, model-adaptive data selection (SAI-DPO) is superior to static strategies for advancing reasoning tasks.

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [457] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/pdf/2505.16186)
*Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeKey improves safety in Large Reasoning Models (LRMs) by activating a 'safety aha moment' in key sentences, reducing harmfulness by 9.6% while maintaining general performance.


<details>
  <summary>Details</summary>
Motivation: LRMs face safety risks from harmful queries and adversarial attacks. Current methods like supervised fine-tuning (SFT) fail to generalize to unseen jailbreak prompts.

Method: Proposes SafeKey with two objectives: (1) Dual-Path Safety Head to enhance safety signals, and (2) Query-Mask Modeling to improve attention on query understanding.

Result: SafeKey lowers harmfulness rate by 9.6% across benchmarks, improving safety generalization without compromising general abilities.

Conclusion: SafeKey effectively reshapes internal attention and representations, enhancing LRM safety against diverse threats.

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [458] [Velocity Completion Task and Method for Event-based Player Positional Data in Soccer](https://arxiv.org/pdf/2505.16199)
*Rikuhei Umemoto, Keisuke Fujii*

Main category: cs.AI

TL;DR: Proposes a neural network-based method to estimate velocities from event-based positional data in team sports, improving dynamic analysis and evaluation.


<details>
  <summary>Details</summary>
Motivation: Event-based positional data lacks continuous temporal info, limiting dynamic analysis of multi-agent systems like team sports.

Method: Uses neural networks to complete velocity for all agents from event-based data, leveraging temporal dependencies and interaction graphs.

Result: Neural networks outperform rule-based methods in velocity completion, and space evaluation aligns closer with complete tracking data.

Conclusion: The method enhances team sports analysis by providing more accurate velocity estimates and better dynamic insights.

Abstract: In many real-world complex systems, the behavior can be observed as a
collection of discrete events generated by multiple interacting agents.
Analyzing the dynamics of these multi-agent systems, especially team sports,
often relies on understanding the movement and interactions of individual
agents. However, while providing valuable snapshots, event-based positional
data typically lacks the continuous temporal information needed to directly
calculate crucial properties such as velocity. This absence severely limits the
depth of dynamic analysis, preventing a comprehensive understanding of
individual agent behaviors and emergent team strategies. To address this
challenge, we propose a new method to simultaneously complete the velocity of
all agents using only the event-based positional data from team sports. Based
on this completed velocity information, we investigate the applicability of
existing team sports analysis and evaluation methods. Experiments using soccer
event data demonstrate that neural network-based approaches outperformed
rule-based methods regarding velocity completion error, considering the
underlying temporal dependencies and graph structure of player-to-player or
player-to-ball interaction. Moreover, the space evaluation results obtained
using the completed velocity are closer to those derived from complete tracking
data, highlighting our method's potential for enhanced team sports system
analysis.

</details>


### [459] [LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead](https://arxiv.org/pdf/2505.16221)
*Yifan Zhang, Xinkui Zhao, Zuxin Wang, Guanjie Cheng, Yueshen Xu, Shuiguang Deng, Jianwei Yin*

Main category: cs.AI

TL;DR: LightRouter is a framework for selecting and integrating a small subset of LLMs to optimize task performance and cost efficiency, achieving up to 25% accuracy improvement and 27% cost reduction.


<details>
  <summary>Details</summary>
Motivation: The diversity in LLMs (cost, performance, computational demands) makes it challenging to choose the best model for specific tasks.

Method: LightRouter uses an adaptive selection mechanism to minimize boot tokens and an integration strategy to combine outputs.

Result: Matches or outperforms ensemble baselines with 25% accuracy improvement and 27% cost reduction.

Conclusion: LightRouter offers a practical, efficient approach for LLM selection and integration without prior model knowledge.

Abstract: The rapid advancement of large language models has unlocked remarkable
capabilities across a diverse array of natural language processing tasks.
However, the considerable differences among available LLMs-in terms of cost,
performance, and computational demands-pose significant challenges for users
aiming to identify the most suitable model for specific tasks. In this work, we
present LightRouter, a novel framework designed to systematically select and
integrate a small subset of LLMs from a larger pool, with the objective of
jointly optimizing both task performance and cost efficiency. LightRouter
leverages an adaptive selection mechanism to identify models that require only
a minimal number of boot tokens, thereby reducing costs, and further employs an
effective integration strategy to combine their outputs. Extensive experiments
across multiple benchmarks demonstrate that LightRouter matches or outperforms
widely-used ensemble baselines, achieving up to a 25% improvement in accuracy.
Compared with leading high-performing models, LightRouter achieves comparable
performance while reducing inference costs by up to 27%. Importantly, our
framework operates without any prior knowledge of individual models and relies
exclusively on inexpensive, lightweight models. This work introduces a
practical approach for efficient LLM selection and provides valuable insights
into optimal strategies for model combination.

</details>


### [460] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/pdf/2505.16223)
*Sangyong Lee, Subo Hwang, Dohoon Kim*

Main category: cs.AI

TL;DR: MADCluster is a model-agnostic anomaly detection framework using self-supervised clustering to address the 'hypersphere collapse' problem in deep learning-based anomaly detection. It clusters normal data into a single cluster and introduces a new loss function for optimization.


<details>
  <summary>Details</summary>
Motivation: To solve the 'hypersphere collapse' issue in existing anomaly detection methods and provide a versatile framework applicable to various deep learning architectures.

Method: MADCluster uses self-supervised clustering with three components: Base Embedder, Cluster Distance Mapping, and Sequence-wise Clustering. It introduces a 'One-directed Adaptive loss' for optimization.

Result: Experiments on four time series datasets show improved performance of comparative models when using MADCluster.

Conclusion: MADCluster is compatible with various architectures and enhances model performance, showing broad potential.

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [461] [MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning](https://arxiv.org/pdf/2505.16225)
*Zihan Chen, Song Wang, Zhen Tan, Jundong Li, Cong Shen*

Main category: cs.AI

TL;DR: MAPLE is a framework using pseudo-labeled samples to enhance many-shot ICL, reducing reliance on labeled data while improving LLM performance.


<details>
  <summary>Details</summary>
Motivation: High costs of obtaining labeled data hinder many-shot ICL; MAPLE addresses this by leveraging pseudo-labeled samples.

Method: MAPLE identifies impactful unlabeled samples, pseudo-labels them via LLMs, and adaptively selects them for input to improve many-shot ICL.

Result: Experiments show MAPLE enhances LLM adaptability and performance with limited labeled data.

Conclusion: MAPLE effectively reduces labeling costs while improving many-shot ICL performance.

Abstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle
diverse tasks by incorporating multiple input-output examples, known as
demonstrations, into the input of LLMs. More recently, advancements in the
expanded context windows of LLMs have led to many-shot ICL, which uses hundreds
of demonstrations and outperforms few-shot ICL, which relies on fewer examples.
However, this approach is often hindered by the high cost of obtaining large
amounts of labeled data. To address this challenge, we propose Many-Shot
Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL
framework that utilizes pseudo-labeled samples to compensate for the lack of
label information. We first identify a subset of impactful unlabeled samples
and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled
samples are then adaptively selected and tailored to each test query as input
to improve the performance of many-shot ICL, without significant labeling
costs. Extensive experiments on real-world datasets demonstrate the
effectiveness of our framework, showcasing its ability to enhance LLM
adaptability and performance with limited labeled data.

</details>


### [462] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/pdf/2505.16276)
*Desiree Heim, Lars-Peter Meyer, Markus Schröder, Johannes Frey, Andreas Dengel*

Main category: cs.AI

TL;DR: The paper explores the relationship between model size and performance in LLMs for KGE tasks, using the LLM-KG-Bench framework. While larger models generally perform better, cost-effectiveness and plateau effects suggest smaller models may sometimes be preferable.


<details>
  <summary>Details</summary>
Motivation: To understand how model size impacts performance in KGE tasks and assess cost-effectiveness, given the trade-off between capabilities and resource costs.

Method: The LLM-KG-Bench framework was used to evaluate 26 open state-of-the-art LLMs, analyzing performance across different model sizes and families.

Result: Larger models generally perform better, but plateau effects and occasional worse performance in larger models of the same family suggest smaller models can be cost-effective.

Conclusion: Model size scaling laws apply to KGE tasks, but testing adjacent model sizes within families is recommended to ensure optimal performance and cost-effectiveness.

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [463] [No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery](https://arxiv.org/pdf/2505.16288)
*Xiaoxue Han, Pengfei Hu, Jun-En Ding, Chang Lu, Feng Liu, Yue Ning*

Main category: cs.AI

TL;DR: II-KEA is a framework enhancing interpretability and interactivity in EHR-based deep learning models for diagnosis prediction by integrating knowledge databases and agentic LLMs.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for EHR lack interpretability and interactivity, limiting clinician trust and decision-making.

Method: II-KEA uses knowledge-enhanced agent-driven causal discovery, personalized knowledge databases, and agentic LLMs for explicit reasoning and clinician interaction.

Result: Evaluated on MIMIC-III and MIMIC-IV, II-KEA shows superior performance, interpretability, and interactivity in case studies.

Conclusion: II-KEA addresses key limitations of EHR models, improving clinician trust and decision-making through interpretability and interactivity.

Abstract: Deep learning models trained on extensive Electronic Health Records (EHR)
data have achieved high accuracy in diagnosis prediction, offering the
potential to assist clinicians in decision-making and treatment planning.
However, these models lack two crucial features that clinicians highly value:
interpretability and interactivity. The ``black-box'' nature of these models
makes it difficult for clinicians to understand the reasoning behind
predictions, limiting their ability to make informed decisions. Additionally,
the absence of interactive mechanisms prevents clinicians from incorporating
their own knowledge and experience into the decision-making process. To address
these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal
discovery framework that integrates personalized knowledge databases and
agentic LLMs. II-KEA enhances interpretability through explicit reasoning and
causal analysis, while also improving interactivity by allowing clinicians to
inject their knowledge and experience through customized knowledge bases and
prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating
superior performance along with enhanced interpretability and interactivity, as
evidenced by its strong results from extensive case studies.

</details>


### [464] [EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning](https://arxiv.org/pdf/2505.16312)
*Jiawei Liu, Qisi Chen, Jianshu Zhang, Quan Liu, Defu Lian*

Main category: cs.AI

TL;DR: EquivPruner reduces token consumption in LLM reasoning by pruning semantically equivalent steps, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning strategies waste tokens on redundant exploration of semantically equivalent steps, especially in domain-specific contexts like math.

Method: Proposes EquivPruner to prune equivalent actions and introduces MathEquiv dataset for training an equivalence detector.

Result: EquivPruner reduces token use by 48.1% and improves accuracy on tasks like GSM8K.

Conclusion: EquivPruner is effective for efficient LLM reasoning, with potential for broader applications.

Abstract: Large Language Models (LLMs) excel at complex reasoning through search
algorithms, yet current strategies often suffer from massive token consumption
due to redundant exploration of semantically equivalent steps. Existing
semantic similarity methods struggle to accurately identify such equivalence in
domain-specific contexts like mathematical reasoning. To address this, we
propose EquivPruner, a simple yet effective approach that identifies and prunes
semantically equivalent actions during LLM reasoning search. We also introduce
MathEquiv, the first dataset we created for mathematical statement equivalence,
which enables the training of a lightweight equivalence detector. Extensive
experiments across various models and tasks demonstrate that EquivPruner
significantly reduces token consumption, improving searching efficiency and
often bolstering reasoning accuracy. For instance, when applied to
Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by
48.1\% while also improving accuracy. Our code is available at
https://github.com/Lolo1222/EquivPruner.

</details>


### [465] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/pdf/2505.16315)
*Xiaoxue Cheng, Junyi Li, Zhenduo Zhang, Xinyu Tang, Wayne Xin Zhao, Xinyu Kong, Zhiqiang Zhang*

Main category: cs.AI

TL;DR: ACPO is a reinforcement learning framework for large reasoning models to reduce redundant reasoning by adaptive cognitive allocation and dynamic system switch.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models often generate redundant content regardless of task difficulty, leading to inefficiency.

Method: ACPO uses system-aware reasoning tokens and online difficulty estimation to guide adaptive reasoning. It involves a two-stage training strategy: supervised fine-tuning followed by reinforcement learning.

Result: ACPO reduces redundant reasoning and adapts cognitive allocation based on task complexity, achieving efficient hybrid reasoning.

Conclusion: ACPO enhances reasoning efficiency in large models by dynamically adjusting cognitive processes.

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [466] [Serious Games: Human-AI Interaction, Evolution, and Coevolution](https://arxiv.org/pdf/2505.16388)
*Nandini Doreswamy, Louise Horstmanshof*

Main category: cs.AI

TL;DR: The paper explores Evolutionary Game Theory (EGT) models to predict human-AI evolutionary dynamics, focusing on Hawk-Dove Game, Iterated Prisoner's Dilemma, and War of Attrition. It suggests EGT as a framework for understanding coevolution but calls for future interdisciplinary and ethical research.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the evolutionary equilibrium and coevolutionary trajectories between humans and AI using EGT models.

Method: Examination of three EGT models (Hawk-Dove Game, Iterated Prisoner's Dilemma, War of Attrition) based on their relevance to human-AI dynamics.

Result: EGT models predict mixed-strategy equilibria, cognitive coevolution, and strategic coevolution, suggesting a framework for human-AI interaction.

Conclusion: EGT provides insights into human-AI dynamics, but future research should expand frameworks, validate empirically, and address ethical implications of coevolution.

Abstract: The serious games between humans and AI have only just begun. Evolutionary
Game Theory (EGT) models the competitive and cooperative strategies of
biological entities. EGT could help predict the potential evolutionary
equilibrium of humans and AI. The objective of this work was to examine some of
the EGT models relevant to human-AI interaction, evolution, and coevolution. Of
thirteen EGT models considered, three were examined: the Hawk-Dove Game,
Iterated Prisoner's Dilemma, and the War of Attrition. This selection was based
on the widespread acceptance and clear relevance of these models to potential
human-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove
Game predicts balanced mixed-strategy equilibria based on the costs of
conflict. It also shows the potential for balanced coevolution rather than
dominance. Iterated Prisoner's Dilemma suggests that repeated interaction may
lead to cognitive coevolution. It demonstrates how memory and reciprocity can
lead to cooperation. The War of Attrition suggests that competition for
resources may result in strategic coevolution, asymmetric equilibria, and
conventions on sharing resources. Therefore, EGT may provide a suitable
framework to understand and predict the human-AI evolutionary dynamic. However,
future research could extend beyond EGT and explore additional frameworks,
empirical validation methods, and interdisciplinary perspectives. AI is being
shaped by human input and is evolving in response to it. So too,
neuroplasticity allows the human brain to grow and evolve in response to
stimuli. If humans and AI converge in future, what might be the result of human
neuroplasticity combined with an ever-evolving AI? Future research should be
mindful of the ethical and cognitive implications of human-AI interaction,
evolution, and coevolution.

</details>


### [467] [FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS](https://arxiv.org/pdf/2505.16409)
*Chaeeun Kim, Seungone Kim*

Main category: cs.AI

TL;DR: FREESON is a retriever-free framework enabling Large Reasoning Models (LRMs) to independently retrieve and reason, improving performance over traditional retrieval-augmented models.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented reasoning approaches rely on separate retrieval models, increasing costs and causing errors due to representation bottlenecks.

Method: Proposes FREESON, where LRMs act as both generator and retriever, using CT-MCTS (Corpus-Traversing Monte Carlo Tree Search) to locate answer-containing paths.

Result: Achieves 14.4% average improvement in EM and F1 over multi-step reasoning models with separate retrievers, and outperforms baselines on specific benchmarks.

Conclusion: FREESON demonstrates the viability of unified retrieval-reasoning models, enhancing efficiency and accuracy in open-domain QA tasks.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
multi-step reasoning and calling search engines at appropriate steps. However,
existing retrieval-augmented reasoning approaches rely on separate retrieval
models, limiting the LRM's role in retrieval to deciding when to retrieve and
how to query. This separation not only increases hardware and operational costs
but also leads to errors in the retrieval process due to the representation
bottleneck, a phenomenon where the retriever's embedding space is not
expressive enough to meet the generator's requirements. To address this, we
shift our perspective from sequence-to-sequence matching to locating the
answer-containing paths within the corpus, and propose a novel framework called
FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables
LRMs to retrieve relevant knowledge on their own by acting as both a generator
and retriever. To achieve this, we introduce a variant of the MCTS algorithm
specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing
Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus
toward answer-containing regions. Our results on five open-domain QA
benchmarks, including single-hop and multi-hop questions, show that FREESON
achieves an average improvement of 14.4% in EM and F1 over four multi-step
reasoning models with a separate retriever, and it also performs comparably to
the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.

</details>


### [468] [Internal Bias in Reasoning Models leads to Overthinking](https://arxiv.org/pdf/2505.16448)
*Renfei Dang, Shujian Huang, Jiajun Chen*

Main category: cs.AI

TL;DR: The paper reveals that reasoning models' overthinking stems from internal bias towards input texts, leading to redundant reflections. Masking the input section reduces reasoning length by 31%-53% and often improves accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overthinking in reasoning models, which wastes computational resources due to unnecessary reflections caused by internal bias.

Method: The study identifies internal bias as a preliminary guess formed by models upon encountering problems. It conducts interpretability experiments and tests masking the input section to mitigate bias.

Result: Masking the input section reduces reasoning length by 31%-53% and often improves accuracy, demonstrating a causal link between internal bias and overthinking.

Conclusion: Internal bias drives overthinking in reasoning models, and masking input sections effectively alleviates this issue, enhancing efficiency and accuracy.

Abstract: While current reasoning models possess strong exploratory capabilities, they
are often criticized for overthinking due to redundant and unnecessary
reflections. In this work, we reveal for the first time that overthinking in
reasoning models may stem from their internal bias towards input texts. Upon
encountering a reasoning problem, the model immediately forms a preliminary
guess about the answer, which we term as an internal bias since it is not
derived through actual reasoning. When this guess conflicts with its reasoning
result, the model tends to engage in reflection, leading to the waste of
computational resources. Through further interpretability experiments, we find
that this behavior is largely driven by the model's excessive attention to the
input section, which amplifies the influence of internal bias on its
decision-making process. Additionally, by masking out the original input
section, the affect of internal bias can be effectively alleviated and the
reasoning length could be reduced by 31%-53% across different complex reasoning
tasks. Notably, in most cases, this approach also leads to improvements in
accuracy. These findings demonstrate a causal relationship between internal
bias and overthinking.

</details>


### [469] [Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events](https://arxiv.org/pdf/2505.16455)
*Mengzhu Liu, Zhengqiu Zhu, Chuan Ai, Chen Gao, Xinghong Li, Lingnan He, Kaisheng Lai, Yingfeng Chen, Xin Lu, Yong Li, Quanjun Yin*

Main category: cs.AI

TL;DR: The paper introduces PsychoAgent, a psychology-driven framework for explainable panic prediction on social media during disasters, addressing data, perception, and interpretability challenges.


<details>
  <summary>Details</summary>
Motivation: Accurate panic sentiment prediction is vital for crisis management, but current methods lack annotated data, ignore risk perception, and lack interpretability.

Method: PsychoAgent combines a fine-grained dataset (COPE) with cross-domain data and LLM-based role-playing agents to model risk perception and emotion generation.

Result: PsychoAgent outperforms baselines by 12.6% to 21.7% in panic prediction and enhances explainability and generalization.

Conclusion: The framework shifts panic prediction from opaque data-driven methods to transparent, role-based simulations with mechanistic interpretation.

Abstract: During sudden disaster events, accurately predicting public panic sentiment
on social media is crucial for proactive governance and crisis management.
Current efforts on this problem face three main challenges: lack of finely
annotated data hinders emotion prediction studies, unmodeled risk perception
causes prediction inaccuracies, and insufficient interpretability of panic
formation mechanisms. We address these issues by proposing a Psychology-driven
generative Agent framework (PsychoAgent) for explainable panic prediction based
on emotion arousal theory. Specifically, we first construct a fine-grained open
panic emotion dataset (namely COPE) via human-large language models (LLMs)
collaboration to mitigate semantic bias. Then, we develop a framework
integrating cross-domain heterogeneous data grounded in psychological
mechanisms to model risk perception and cognitive differences in emotion
generation. To enhance interpretability, we design an LLM-based role-playing
agent that simulates individual psychological chains through dedicatedly
designed prompts. Experimental results on our annotated dataset show that
PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%
compared to baseline models. Furthermore, the explainability and generalization
of our approach is validated. Crucially, this represents a paradigm shift from
opaque "data-driven fitting" to transparent "role-based simulation with
mechanistic interpretation" for panic emotion prediction during emergencies.
Our implementation is publicly available at:
https://anonymous.4open.science/r/PsychoAgent-19DD.

</details>


### [470] [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/pdf/2505.16459)
*Guiyao Tie, Xueyang Zhou, Tianhe Gu, Ruihang Zhang, Chaoran Hu, Sizhe Zhang, Mengqu Sun, Yan Zhang, Pan Zhou, Lichao Sun*

Main category: cs.AI

TL;DR: The paper introduces MMMR, a benchmark to evaluate multi-modal reasoning in MLLMs, highlighting gaps in reasoning quality despite improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on reasoning quality in multi-modal tasks, leaving MLLMs' reasoning capabilities poorly understood.

Method: The MMMR benchmark includes a high-difficulty dataset and a Reasoning Trace Evaluation Pipeline (RTEP) to assess reasoning beyond accuracy.

Result: MLLMs with intermediate thinking traces outperform others but exhibit reasoning flaws like inconsistency.

Conclusion: MMMR provides a scalable tool for evaluating and improving multi-modal reasoning systems.

Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled
unified processing of language, vision, and structured inputs, opening the door
to complex tasks such as logical deduction, spatial reasoning, and scientific
analysis. Despite their promise, the reasoning capabilities of MLLMs,
particularly those augmented with intermediate thinking traces (MLLMs-T),
remain poorly understood and lack standardized evaluation benchmarks. Existing
work focuses primarily on perception or final answer correctness, offering
limited insight into how models reason or fail across modalities. To address
this gap, we introduce the MMMR, a new benchmark designed to rigorously
evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a
high-difficulty dataset of 1,083 questions spanning six diverse reasoning types
with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace
Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy
through metrics like relevance, consistency, and structured error annotations.
Empirical results show that MLLMs-T overall outperform non-thinking
counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro
suffer from reasoning pathologies such as inconsistency and overthinking. This
benchmark reveals persistent gaps between accuracy and reasoning quality and
provides an actionable evaluation pipeline for future model development.
Overall, the MMMR offers a scalable foundation for evaluating, comparing, and
improving the next generation of multi-modal reasoning systems.

</details>


### [471] [ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection](https://arxiv.org/pdf/2505.16475)
*Jiaqi Li, Xinyi Dong, Yang Liu, Zhizhuo Yang, Quansen Wang, Xiaobo Wang, SongChun Zhu, Zixia Jia, Zilong Zheng*

Main category: cs.AI

TL;DR: ReflectEvo is a pipeline enabling small language models (SLMs) to improve reasoning via iterative self-reflection, achieving significant performance boosts on models like Llama-3 and Mistral.


<details>
  <summary>Details</summary>
Motivation: To enhance SLMs' reasoning without relying on superior models or human annotation by leveraging self-generated reflections.

Method: Uses reflection learning to iteratively generate self-reflections for self-training, creating a dataset (ReflectEvo-460k) and applying SFT and DPO.

Result: Boosts Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%, rivaling top open-sourced models on BIG-bench.

Conclusion: Iterative reflection learning can continuously improve SLMs' reasoning, highlighting its long-term potential.

Abstract: We present a novel pipeline, ReflectEvo, to demonstrate that small language
models (SLMs) can enhance meta introspection through reflection learning. This
process iteratively generates self-reflection for self-training, fostering a
continuous and self-evolving process. Leveraging this pipeline, we construct
ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection
dataset with broadened instructions and diverse multi-domain tasks. Building
upon this dataset, we demonstrate the effectiveness of reflection learning to
improve SLMs' reasoning abilities using SFT and DPO with remarkable
performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral
from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the
reasoning capability of the three prominent open-sourced models on BIG-bench
without distillation from superior models or fine-grained human annotation. We
further conduct a deeper analysis of the high quality of self-generated
reflections and their impact on error localization and correction. Our work
highlights the potential of continuously enhancing the reasoning performance of
SLMs through iterative reflection learning in the long run.

</details>


### [472] [Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery](https://arxiv.org/pdf/2505.16477)
*Yanbo Zhang, Sumeer A. Khan, Adnan Mahmud, Huck Yang, Alexander Lavin, Michael Levin, Jeremy Frey, Jared Dunnmon, James Evans, Alan Bundy, Saso Dzeroski, Jesper Tegner, Hector Zenil*

Main category: cs.AI

TL;DR: LLMs are reshaping scientific research by enhancing productivity and redefining the scientific method, but challenges like hallucinations and reliability remain. Their integration requires collaboration with human goals and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs are transforming scientific research and their potential applications across the scientific cycle, while addressing challenges and ethical concerns.

Method: Review of LLMs' roles in experimental design, data analysis, and workflows, focusing on chemistry and biology, and their integration into the scientific process.

Result: LLMs can serve as creative engines and productivity enhancers, but their integration must align with human goals and include clear evaluation metrics.

Conclusion: For LLMs to drive transformative breakthroughs responsibly, careful guidance and ethical oversight are needed, alongside decisions on their role in scientific exploration.

Abstract: With recent Nobel Prizes recognising AI contributions to science, Large
Language Models (LLMs) are transforming scientific research by enhancing
productivity and reshaping the scientific method. LLMs are now involved in
experimental design, data analysis, and workflows, particularly in chemistry
and biology. However, challenges such as hallucinations and reliability
persist. In this contribution, we review how Large Language Models (LLMs) are
redefining the scientific method and explore their potential applications
across different stages of the scientific cycle, from hypothesis testing to
discovery. We conclude that, for LLMs to serve as relevant and effective
creative engines and productivity enhancers, their deep integration into all
steps of the scientific process should be pursued in collaboration and
alignment with human scientific goals, with clear evaluation metrics. The
transition to AI-driven science raises ethical questions about creativity,
oversight, and responsibility. With careful guidance, LLMs could evolve into
creative engines, driving transformative breakthroughs across scientific
disciplines responsibly and effectively. However, the scientific community must
also decide how much it leaves to LLMs to drive science, even when associations
with 'reasoning', mostly currently undeserved, are made in exchange for the
potential to explore hypothesis and solution regions that might otherwise
remain unexplored by human exploration alone.

</details>


### [473] [Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes](https://arxiv.org/pdf/2505.16482)
*Huynh Thi Thanh Binh, Le Van Cuong, Dang Hai Dang, Le Trong Vinh*

Main category: cs.AI

TL;DR: A novel partial charging approach for WRSNs optimizes both charging path and time to minimize energy depletion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of fully charging approaches in WRSNs, which cause sensor deaths due to extended charging latency.

Method: Proposes a bi-level optimized scheme with two algorithms: one combining Multi-start Local Search and Genetic Algorithm, and another using Multitasking and Covariance Matrix Adaptation Evolutionary Strategies.

Result: Experimental validations show the proposed algorithms outperform existing works in various network scenarios.

Conclusion: The partial charging approach effectively minimizes energy depletion in WRSNs by optimizing charging path and time simultaneously.

Abstract: Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the
advantage of wireless energy transfer technology have opened a promising
opportunity in solving the limited energy issue. However, an ineffective
charging strategy may reduce the charging performance. Although many practical
charging algorithms have been introduced, these studies mainly focus on
optimizing the charging path with a fully charging approach. This approach may
lead to the death of a series of sensors due to their extended charging
latency. This paper introduces a novel partial charging approach that follows a
bi-level optimized scheme to minimize energy depletion in WRSNs. We aim at
optimizing simultaneously two factors: the charging path and time. To
accomplish this, we first formulate a mathematical model of the investigated
problem. We then propose two approximate algorithms in which the optimization
of the charging path and the charging time are considered as the upper and
lower level, respectively. The first algorithm combines a Multi-start Local
Search method and a Genetic Algorithm to find a solution. The second algorithm
adopts a nested approach that utilizes the advantages of the Multitasking and
Covariance Matrix Adaptation Evolutionary Strategies. Experimental validations
on various network scenarios demonstrate that our proposed algorithms
outperform the existing works.

</details>


### [474] [Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)](https://arxiv.org/pdf/2505.16507)
*Anshu Xiong, Songmao Zhang*

Main category: cs.AI

TL;DR: The paper extends the notion of relevance for stability in incomplete argumentation frameworks (IAFs) to sets of arguments, introduces strong relevance, and analyzes computational complexity, showing P-time detection for most semantics but tractability challenges under grounded semantics.


<details>
  <summary>Details</summary>
Motivation: To generalize the concept of relevance from single arguments to sets of arguments in IAFs, ensuring consistent verification status across completions, and to explore computational feasibility.

Method: Extends relevance to sets of arguments, introduces strong relevance, and analyzes complexity under various semantics.

Result: Detection of (strong) relevance for sets of arguments is in P-time for most semantics, but tractable methods under grounded semantics are challenging.

Conclusion: The study advances understanding of relevance in IAFs, highlighting computational limits under grounded semantics.

Abstract: The notion of relevance was proposed for stability of justification status of
a single argument in incomplete argumentation frameworks (IAFs) in 2024 by
Odekerken et al. To extend the notion, we study the relevance for stability of
verification status of a set of arguments in this paper, i.e., the
uncertainties in an IAF that have to be resolved in some situations so that
answering whether a given set of arguments is an extension obtains the same
result in every completion of the IAF. Further we propose the notion of strong
relevance for describing the necessity of resolution in all situations reaching
stability. An analysis of complexity reveals that detecting the (strong)
relevance for stability of sets of arguments can be accomplished in P time
under the most semantics discussed in the paper. We also discuss the difficulty
in finding tractable methods for relevance detection under grounded semantics.

</details>


### [475] [Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning](https://arxiv.org/pdf/2505.16579)
*Siqu Ou, Hongcheng Liu, Pingjie Wang, Yusheng Liao, Chuan Xuan, Yanfeng Wang, Yu Wang*

Main category: cs.AI

TL;DR: GRASSLAND introduces a maze navigation benchmark for dynamic spatial reasoning, and D2R enhances MLLMs by integrating visual drafts with textual reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Existing methods for complex reasoning in MLLMs are limited to text or static visuals, struggling with dynamic spatial tasks.

Method: Proposes D2R, a training-free framework combining textual CoT with dynamic visual drafts.

Result: D2R outperforms conventional methods, improving performance in dynamic spatial reasoning without fine-tuning.

Conclusion: D2R sets a robust baseline for dynamic spatial reasoning, with open-source availability.

Abstract: While chains-of-thought (CoT) have advanced complex reasoning in multimodal
large language models (MLLMs), existing methods remain confined to text or
static visual domains, often faltering in dynamic spatial reasoning tasks. To
bridge this gap, we present GRASSLAND, a novel maze navigation benchmark
designed to evaluate dynamic spatial reasoning. Our experiments show that
augmenting textual reasoning chains with dynamic visual drafts, overlaid on
input images, significantly outperforms conventional approaches, offering new
insights into spatial reasoning in evolving environments. To generalize this
capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free
framework that seamlessly integrates textual CoT with corresponding visual
drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently
enhances performance across diverse tasks, establishing a robust baseline for
dynamic spatial reasoning without requiring model fine-tuning. Project is open
at https://github.com/Cratileo/D2R.

</details>


### [476] [Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences](https://arxiv.org/pdf/2505.16619)
*Gavin Farrell, Eleni Adamidi, Rafael Andrade Buono, Mihail Anton, Omar Abdelghani Attafi, Salvador Capella Gutierrez, Emidio Capriotti, Leyla Jael Castro, Davide Cirillo, Lisa Crossman, Christophe Dessimoz, Alexandros Dimopoulos, Raul Fernandez-Diaz, Styliani-Christina Fragkouli, Carole Goble, Wei Gu, John M. Hancock, Alireza Khanteymoori, Tom Lenaerts, Fabio G. Liberante, Peter Maccallum, Alexander Miguel Monzon, Magnus Palmblad, Lucy Poveda, Ovidiu Radulescu, Denis C. Shields, Shoaib Sufi, Thanasis Vergoulis, Fotis Psomopoulos, Silvio C. E. Tosatto*

Main category: cs.AI

TL;DR: The paper discusses the challenges of AI in life sciences, focusing on trust, reproducibility, and sustainability, and proposes Open and Sustainable AI (OSAI) recommendations to address these issues.


<details>
  <summary>Details</summary>
Motivation: To address the erosion of trust and sustainability issues in AI-based life science research due to poor reusability and reproducibility.

Method: The paper reviews challenges and introduces OSAI recommendations mapped to over 300 AI ecosystem components.

Result: A practical set of OSAI recommendations is provided to support sustainable, reusable, and transparent AI in life sciences.

Conclusion: The OSAI framework aids future policy development and structured AI implementation in life sciences.

Abstract: Artificial intelligence (AI) has recently seen transformative breakthroughs
in the life sciences, expanding possibilities for researchers to interpret
biological information at an unprecedented capacity, with novel applications
and advances being made almost daily. In order to maximise return on the
growing investments in AI-based life science research and accelerate this
progress, it has become urgent to address the exacerbation of long-standing
research challenges arising from the rapid adoption of AI methods. We review
the increased erosion of trust in AI research outputs, driven by the issues of
poor reusability and reproducibility, and highlight their consequent impact on
environmental sustainability. Furthermore, we discuss the fragmented components
of the AI ecosystem and lack of guiding pathways to best support Open and
Sustainable AI (OSAI) model development. In response, this perspective
introduces a practical set of OSAI recommendations directly mapped to over 300
components of the AI ecosystem. Our work connects researchers with relevant AI
resources, facilitating the implementation of sustainable, reusable and
transparent AI. Built upon life science community consensus and aligned to
existing efforts, the outputs of this perspective are designed to aid the
future development of policy and structured pathways for guiding AI
implementation.

</details>


### [477] [SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving](https://arxiv.org/pdf/2505.16646)
*Yujie Hou, Ting Zhang, Mei Wang, Xuetao Ma, Hu Huang*

Main category: cs.AI

TL;DR: SMART is a framework for multi-dimensional assessment of LLMs in math, evaluating understanding, reasoning, arithmetic, and reflection, revealing gaps in current metrics.


<details>
  <summary>Details</summary>
Motivation: Current metrics like final answer accuracy fail to assess genuine mathematical reasoning in LLMs, necessitating a more nuanced evaluation.

Method: SMART decomposes math problem-solving into four dimensions, using tailored tasks and an automated self-generating/validating mechanism for scalable, reliable benchmarks.

Result: Applied to 21 LLMs, SMART uncovered significant discrepancies in abilities across dimensions, showing final answer accuracy is insufficient.

Conclusion: SMART highlights the need for holistic metrics to better capture true problem-solving capabilities in LLMs.

Abstract: Large Language Models have achieved remarkable results on a variety of
mathematical benchmarks. However, concerns remain as to whether these successes
reflect genuine mathematical reasoning or superficial pattern recognition.
Common evaluation metrics, such as final answer accuracy, fail to disentangle
the underlying competencies involved, offering limited diagnostic value. To
address these limitations, we introduce SMART: a Self-Generating and
Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes
mathematical problem solving into four distinct dimensions: understanding,
reasoning, arithmetic, and reflection \& refinement. Each dimension is
evaluated independently through tailored tasks, enabling interpretable and
fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated
self-generating and self-validating mechanism to produce and verify benchmark
data, ensuring both scalability and reliability. We apply SMART to 21
state-of-the-art open- and closed-source LLMs, uncovering significant
discrepancies in their abilities across different dimensions. Our findings
demonstrate the inadequacy of final answer accuracy as a sole metric and
motivate a new holistic metric to better capture true problem-solving
capabilities. Code and benchmarks will be released upon acceptance.

</details>


### [478] [ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming](https://arxiv.org/pdf/2505.16667)
*Xinwei Yang, Zhaofeng Liu, Chen Huang, Jiashuai Zhang, Tong Zhang, Yifan Zhang, Wenqiang Lei*

Main category: cs.AI

TL;DR: The paper introduces a taxonomy of human feedback, a dataset (ELABORATIONSET), and a benchmark (ELABORATION) for human-LLM collaboration in competitive programming, aiming to unify fragmented research and improve evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive understanding in human-LLM collaboration for competitive programming due to fragmented studies and diverse feedback methods.

Method: Develops a taxonomy of human feedback, creates ELABORATIONSET (a dataset for human-LLM collaboration), and introduces ELABORATION (a benchmark for assessment).

Result: Provides tools for fine-grained evaluation, large-scale simulated feedback, and identifies strengths/weaknesses of existing methods.

Conclusion: Lays the foundation for future improvements in human-LLM collaboration for competitive programming.

Abstract: While recent research increasingly emphasizes the value of human-LLM
collaboration in competitive programming and proposes numerous empirical
methods, a comprehensive understanding remains elusive due to the fragmented
nature of existing studies and their use of diverse, application-specific human
feedback. Thus, our work serves a three-fold purpose: First, we present the
first taxonomy of human feedback consolidating the entire programming process,
which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a
novel programming dataset specifically designed for human-LLM collaboration,
meticulously annotated to enable large-scale simulated human feedback and
facilitate costeffective real human interaction studies. Third, we introduce
ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM
competitive programming. With ELABORATION, we pinpoint strengthes and
weaknesses of existing methods, thereby setting the foundation for future
improvement. Our code and dataset are available at
https://github.com/SCUNLP/ELABORATION

</details>


### [479] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/pdf/2505.16686)
*Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp*

Main category: cs.AI

TL;DR: SPaRC is a 2D grid pathfinding dataset testing spatial and symbolic reasoning. Humans excel, while models like o4-mini struggle, especially on hard puzzles. Models often fail in navigation and spatial logic, but multiple attempts improve accuracy, hinting at potential for better training.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack abstract, multi-step reasoning challenges like pathfinding and rule constraint satisfaction, limiting evaluation of spatial and symbolic reasoning.

Method: SPaRC introduces 1,000 2D grid pathfinding puzzles requiring step-by-step planning with arithmetic and geometric rules. Human and model performance (e.g., o4-mini) are compared.

Result: Humans achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while models struggle (15.8%; 1.1% on hard puzzles), often generating invalid paths. Models fail to scale compute with difficulty.

Conclusion: SPaRC highlights models' spatial reasoning limitations and suggests potential improvements through better training and test-time scaling. It aims to drive research in abstract, multi-step problem-solving.

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [480] [MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models](https://arxiv.org/pdf/2505.16700)
*Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, Chao Shen*

Main category: cs.AI

TL;DR: MCP-RADAR is a benchmark for evaluating LLMs in the MCP framework, focusing on five dimensions: accuracy, tool selection, resource efficiency, parameter accuracy, and speed. It reveals trade-offs in performance and offers guidance for optimizing tool compatibility.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods fail to assess LLM tool utilization in the MCP framework, necessitating a comprehensive benchmark like MCP-RADAR.

Method: MCP-RADAR uses a five-dimensional approach to objectively measure LLM performance across tasks like software engineering and problem-solving.

Result: Evaluations show trade-offs between accuracy, efficiency, and speed among LLMs, challenging single-metric rankings.

Conclusion: MCP-RADAR provides insights for optimizing LLM-tool interactions, applicable beyond MCP to other frameworks.

Abstract: As Large Language Models (LLMs) evolve from passive text generators to active
reasoning agents capable of tool interaction, the Model Context Protocol (MCP)
has emerged as a standardized framework for dynamic tool discovery and
orchestration. Despite widespread industry adoption, existing evaluation
methodologies fail to adequately assess tool utilization capabilities within
this new paradigm. This paper introduces MCP-RADAR, the first comprehensive
benchmark specifically designed to evaluate LLM performance in the MCP
framework through a novel five-dimensional approach measuring: answer accuracy,
tool selection efficiency, computational resource efficiency, parameter
construction accuracy, and execution speed. Unlike conventional benchmarks that
rely on subjective human evaluations or binary success metrics, MCP-RADAR
employs objective, quantifiable measurements across multiple task domains
including software engineering, mathematical reasoning, and general
problem-solving. Our evaluations of leading commercial and open-source LLMs
reveal distinctive capability profiles with significant trade-offs between
accuracy, efficiency, and speed, challenging traditional single-metric
performance rankings. Besides, we provide valuable guidance for developers to
optimize their tools for maximum model compatibility and effectiveness. While
focused on MCP due to its standardized approach, our methodology remains
applicable across all LLM agent tool integration frameworks, providing valuable
insights for both LLM developers and tool creators to optimize the entire
LLM-tool interaction ecosystem. The implementation, configurations, and
datasets used in our evaluation are publicly available at
https://anonymous.4open.science/r/MCPRadar-B143.

</details>


### [481] [Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review](https://arxiv.org/pdf/2505.16771)
*Beyazit Bestami Yuksel, Ayse Yilmazer Metin*

Main category: cs.AI

TL;DR: A synthesis of AI breakthroughs over 15 years, highlighting key inflection points like GPU training, ImageNet, Transformers, and GPT, framed as paradigm shifts. Discusses data-centric approaches, privacy solutions, and synthetic data, offering guidance for future AI research and policy.


<details>
  <summary>Details</summary>
Motivation: To integrate historical, theoretical, and technological perspectives of AI advancements and identify deeper paradigm shifts, while addressing modern challenges like privacy and data access.

Method: Analyzes key inflection points (GPU training, ImageNet, Transformers, GPT) using statistical learning theory, evaluates privacy solutions (federated learning, PETs), and assesses synthetic data utility.

Result: Identifies paradigm shifts in AI, explains scalability of breakthroughs, and evaluates emerging solutions for privacy and data constraints.

Conclusion: Provides strategic guidance for future AI research and policy by aligning technical insights with evolving data infrastructure and addressing modern challenges.

Abstract: This paper presents a comprehensive synthesis of major breakthroughs in
artificial intelligence (AI) over the past fifteen years, integrating
historical, theoretical, and technological perspectives. It identifies key
inflection points in AI' s evolution by tracing the convergence of
computational resources, data access, and algorithmic innovation. The analysis
highlights how researchers enabled GPU based model training, triggered a data
centric shift with ImageNet, simplified architectures through the Transformer,
and expanded modeling capabilities with the GPT series. Rather than treating
these advances as isolated milestones, the paper frames them as indicators of
deeper paradigm shifts. By applying concepts from statistical learning theory
such as sample complexity and data efficiency, the paper explains how
researchers translated breakthroughs into scalable solutions and why the field
must now embrace data centric approaches. In response to rising privacy
concerns and tightening regulations, the paper evaluates emerging solutions
like federated learning, privacy enhancing technologies (PETs), and the data
site paradigm, which reframe data access and security. In cases where real
world data remains inaccessible, the paper also assesses the utility and
constraints of mock and synthetic data generation. By aligning technical
insights with evolving data infrastructure, this study offers strategic
guidance for future AI research and policy development.

</details>


### [482] [Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making](https://arxiv.org/pdf/2505.16781)
*Qianlei Jia, Xinliang Zhou, Ondrej Krejcar, Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: A novel SNGDM framework integrates 3WD theory, dynamic network reconstruction, and linguistic opinion representation to address uncertainty and dynamic social structures in GDM.


<details>
  <summary>Details</summary>
Motivation: Traditional opinion dynamics models struggle with uncertainty, dynamic social structures, and vague information in GDM.

Method: The framework uses 3WD for hesitation modeling, opinion similarity-based connection adjustment, and linguistic terms for opinion representation.

Result: Simulations in multi-UAV scenarios show improved stability and realistic decision-making behaviors.

Conclusion: The proposed model effectively handles uncertainty and dynamic social relationships, enhancing decision-making in complex scenarios.

Abstract: In group decision-making (GDM) scenarios, uncertainty, dynamic social
structures, and vague information present major challenges for traditional
opinion dynamics models. To address these issues, this study proposes a novel
social network group decision-making (SNGDM) framework that integrates
three-way decision (3WD) theory, dynamic network reconstruction, and linguistic
opinion representation. First, the 3WD mechanism is introduced to explicitly
model hesitation and ambiguity in agent judgments, thereby preventing
irrational decisions. Second, a connection adjustment rule based on opinion
similarity is developed, enabling agents to adaptively update their
communication links and better reflect the evolving nature of social
relationships. Third, linguistic terms are used to describe agent opinions,
allowing the model to handle subjective, vague, or incomplete information more
effectively. Finally, an integrated multi-agent decision-making framework is
constructed, which simultaneously considers individual uncertainty, opinion
evolution, and network dynamics. The proposed model is applied to a multi-UAV
cooperative decision-making scenario, where simulation results and consensus
analysis demonstrate its effectiveness. Experimental comparisons further verify
the advantages of the algorithm in enhancing system stability and representing
realistic decision-making behaviors.

</details>


### [483] [Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce](https://arxiv.org/pdf/2505.16787)
*Ashish Sundar, Chunbo Luo, Xiaoyang Wang*

Main category: cs.AI

TL;DR: The paper introduces a novel MBRL method that improves world model fidelity by actively seeking high-entropy states, outperforming base Dreamer in efficiency and convergence.


<details>
  <summary>Details</summary>
Motivation: MBRL methods often neglect world model optimization, focusing on the actor. Improving the world model can enhance downstream performance.

Method: Proposes a hierarchical planner that dynamically adjusts replanning, horizon length, and reward-entropy balance, using short-horizon latent predictions.

Result: Achieves 50% faster maze completion and 60% fewer environment steps for policy convergence compared to base Dreamer.

Conclusion: The method demonstrates the benefits of optimizing world model learning, offering a principled alternative to curiosity-driven approaches.

Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.

</details>


### [484] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/pdf/2505.16826)
*Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, Jiajun Zhang*

Main category: cs.AI

TL;DR: Proposes Key-token Advantage Estimation (KTAE) to address coarse granularity in reinforcement learning for language models, improving token-level advantage estimation and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning algorithms like GRPO and DAPO compute rollout-level advantages, missing token-specific contributions, which limits learning effectiveness.

Method: Introduces KTAE, a novel algorithm that estimates token-level advantages using statistical analysis of rollout correctness, combining it with rollout-level advantages.

Result: Models with GRPO+KTAE and DAPO+KTAE outperform baselines in mathematical reasoning benchmarks, achieving higher accuracy with shorter responses.

Conclusion: KTAE enhances token-level advantage estimation, improving model performance without additional models, surpassing even larger models like R1-Distill-Qwen-1.5B.

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [485] [GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent](https://arxiv.org/pdf/2505.16827)
*Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, Liqiang Nie*

Main category: cs.AI

TL;DR: GUI-explorer is a training-free GUI agent that autonomously explores and mines knowledge for GUI automation, outperforming state-of-the-art agents without requiring parameter updates.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in GUI automation, such as misinterpretation of UI components and outdated knowledge, without costly fine-tuning.

Method: Uses autonomous exploration of function-aware trajectories and unsupervised mining of transition-aware knowledge to systematically collect and analyze GUI interactions.

Result: Achieves task success rates of 53.7% on SPA-Bench and 47.4% on AndroidWorld, surpassing existing agents.

Conclusion: GUI-explorer is an efficient, training-free solution for GUI automation, open-sourced for public use.

Abstract: GUI automation faces critical challenges in dynamic environments. MLLMs
suffer from two key issues: misinterpreting UI components and outdated
knowledge. Traditional fine-tuning methods are costly for app-specific
knowledge updates. We propose GUI-explorer, a training-free GUI agent that
incorporates two fundamental mechanisms: (1) Autonomous Exploration of
Function-aware Trajectory. To comprehensively cover all application
functionalities, we design a Function-aware Task Goal Generator that
automatically constructs exploration goals by analyzing GUI structural
information (e.g., screenshots and activity hierarchies). This enables
systematic exploration to collect diverse trajectories. (2) Unsupervised Mining
of Transition-aware Knowledge. To establish precise screen-operation logic, we
develop a Transition-aware Knowledge Extractor that extracts effective
screen-operation logic through unsupervised analysis the state transition of
structured interaction triples (observation, action, outcome). This eliminates
the need for human involvement in knowledge extraction. With a task success
rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows
significant improvements over SOTA agents. It requires no parameter updates for
new apps. GUI-explorer is open-sourced and publicly available at
https://github.com/JiuTian-VL/GUI-explorer.

</details>


### [486] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/pdf/2505.16832)
*Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao*

Main category: cs.AI

TL;DR: The paper introduces EduVisBench, a benchmark for evaluating visual reasoning in foundation models (FMs) for education, and EduVisAgent, a multi-agent framework to improve pedagogically effective visualizations.


<details>
  <summary>Details</summary>
Motivation: Existing FMs lack pedagogically effective visual explanations, focusing too much on textual reasoning and neglecting structured visualizations for conceptual understanding.

Method: The authors propose EduVisBench, a multi-domain, multi-level benchmark with STEM problem sets and a fine-grained rubric. They also introduce EduVisAgent, a multi-agent framework for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design.

Result: EduVisAgent outperforms baselines by 40.2%, generating more educationally aligned visualizations.

Conclusion: The work highlights the limitations of current FMs in visual reasoning for education and demonstrates the effectiveness of EduVisAgent in addressing these challenges.

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [487] [Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models](https://arxiv.org/pdf/2505.16854)
*Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou*

Main category: cs.AI

TL;DR: TON is a two-stage training strategy for VLMs that reduces unnecessary reasoning steps, cutting completion length by up to 90% without performance loss.


<details>
  <summary>Details</summary>
Motivation: To mimic human-like selective reasoning, avoiding computational waste on easy questions while ensuring careful thought for complex ones.

Method: Combines supervised fine-tuning with 'thought dropout' and GRPO to train models to decide when reasoning is needed.

Result: Reduces token usage significantly (up to 90%) while maintaining or improving performance across tasks.

Conclusion: TON advances human-like reasoning in RL, optimizing efficiency without sacrificing accuracy.

Abstract: Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.

</details>


### [488] [Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings](https://arxiv.org/pdf/2505.16877)
*Yuqicheng Zhu, Daniel Hernández, Yuan He, Zifeng Ding, Bo Xiong, Evgeny Kharlamov, Steffen Staab*

Main category: cs.AI

TL;DR: CondKGCP introduces a method for stronger per-query uncertainty guarantees in KGE, improving reliability for high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: Existing KGE uncertainty methods only provide marginal coverage guarantees, which are insufficient for high-stakes applications like medical diagnosis requiring per-query consistency.

Method: CondKGCP merges similar predicates and uses rank information to approximate predicate-conditional coverage while keeping prediction sets compact.

Result: Theoretical guarantees are proven, and empirical evaluations demonstrate CondKGCP's effectiveness.

Conclusion: CondKGCP advances KGE uncertainty quantification by providing stronger conditional coverage guarantees, enhancing reliability for critical applications.

Abstract: Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is
crucial for ensuring the reliability of downstream applications. A recent work
applies conformal prediction to KGE methods, providing uncertainty estimates by
generating a set of answers that is guaranteed to include the true answer with
a predefined confidence level. However, existing methods provide probabilistic
guarantees averaged over a reference set of queries and answers (marginal
coverage guarantee). In high-stakes applications such as medical diagnosis, a
stronger guarantee is often required: the predicted sets must provide
consistent coverage per query (conditional coverage guarantee). We propose
CondKGCP, a novel method that approximates predicate-conditional coverage
guarantees while maintaining compact prediction sets. CondKGCP merges
predicates with similar vector representations and augments calibration with
rank information. We prove the theoretical guarantees and demonstrate empirical
effectiveness of CondKGCP by comprehensive evaluations.

</details>


### [489] [Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships](https://arxiv.org/pdf/2505.16899)
*Kerem Oktar, Katherine M. Collins, Jose Hernandez-Orallo, Diane Coyle, Stephen Cave, Adrian Weller, Ilia Sucholutsky*

Main category: cs.AI

TL;DR: The paper discusses risks of AI thought partners, proposing a framework (RISc) to evaluate and mitigate risks at real-time, individual, and societal levels.


<details>
  <summary>Details</summary>
Motivation: To address the novel risks posed by AI systems that collaborate with humans in complex reasoning, beyond traditional AI tools.

Method: Introduces the RISc framework to systematically identify risks at multiple levels (Real-time, Individual, Societal) and proposes metrics and mitigation strategies.

Result: A structured approach to evaluate and mitigate risks of AI thought partners, ensuring safer collaboration.

Conclusion: Proactive strategies are needed to prevent harms and maximize benefits of AI thought partnerships.

Abstract: Artificial Intelligence (AI) systems have historically been used as tools
that execute narrowly defined tasks. Yet recent advances in AI have unlocked
possibilities for a new class of models that genuinely collaborate with humans
in complex reasoning, from conceptualizing problems to brainstorming solutions.
Such AI thought partners enable novel forms of collaboration and extended
cognition, yet they also pose major risks-including and beyond risks of typical
AI tools and agents. In this commentary, we systematically identify risks of AI
thought partners through a novel framework that identifies risks at multiple
levels of analysis, including Real-time, Individual, and Societal risks arising
from collaborative cognition (RISc). We leverage this framework to propose
concrete metrics for risk evaluation, and finally suggest specific mitigation
strategies for developers and policymakers. As AI thought partners continue to
proliferate, these strategies can help prevent major harms and ensure that
humans actively benefit from productive thought partnerships.

</details>


### [490] [Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning](https://arxiv.org/pdf/2505.16928)
*Bosung Kim, Prithviraj Ammanabrolu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks
that advances long-context understanding in embodied AI. $\infty$-THOR
provides: (1) a generation framework for synthesizing scalable, reproducible,
and unlimited long-horizon trajectories; (2) a novel embodied QA task,
Needle(s) in the Embodied Haystack, where multiple scattered clues across
extended trajectories test agents' long-context reasoning ability; and (3) a
long-horizon dataset and benchmark suite featuring complex tasks that span
hundreds of environment steps, each paired with ground-truth action sequences.
To enable this capability, we explore architectural adaptations, including
interleaved Goal-State-Action modeling, context extension techniques, and
Context Parallelism, to equip LLM-based agents for extreme long-context
reasoning and interaction. Experimental results and analyses highlight the
challenges posed by our benchmark and provide insights into training strategies
and model behaviors under long-horizon conditions. Our work provides a
foundation for the next generation of embodied AI systems capable of robust,
long-term reasoning and planning.

</details>


### [491] [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/pdf/2505.16938)
*NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Runmin Ma, Tianshuo Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, Yilan Zhang, Meng Li, Zhongying Tu, Xiangyu Yue, Wangli Ouyang, Bowen Zhou, Lei Bai*

Main category: cs.AI

TL;DR: NovelSeek is a multi-agent AI framework for autonomous scientific research, offering scalability, interactivity, and efficiency across diverse fields.


<details>
  <summary>Details</summary>
Motivation: To enhance research efficiency and innovation by automating complex scientific tasks with AI.

Method: A closed-loop multi-agent framework (NovelSeek) integrates human feedback and multi-agent interaction for end-to-end automation.

Result: Improved performance in tasks like reaction yield prediction (27.6% to 35.4%), enhancer activity prediction (0.52 to 0.79), and 2D semantic segmentation (78.8% to 81.0%) with minimal time.

Conclusion: NovelSeek demonstrates significant potential in accelerating scientific research with AI-driven automation and human-AI collaboration.

Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.

</details>


### [492] [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/pdf/2505.16944)
*Yunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng Liu, Bin Xu, Lei Hou, Juanzi Li*

Main category: cs.AI

TL;DR: AgentIF is a benchmark for evaluating LLMs' ability to follow lengthy, complex instructions in agentic scenarios, revealing poor performance of current models.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored challenge of LLMs reliably adhering to lengthy, complex instructions in agentic applications.

Method: Developed AgentIF, a benchmark with 707 human-annotated instructions from 50 real-world agentic tasks, featuring long, complex constraints. Evaluated LLMs using code-based, LLM-based, and hybrid metrics.

Result: Current LLMs perform poorly, especially with complex constraints and tool specifications. Error analysis highlights failure modes.

Conclusion: AgentIF provides a systematic evaluation tool, revealing limitations of existing LLMs and encouraging future research.

Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.

</details>


### [493] [HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation](https://arxiv.org/pdf/2505.16978)
*Weizhi Tang, Yixuan Li, Chris Sypherd, Elizabeth Polgreen, Vaishak Belle*

Main category: cs.AI

TL;DR: The paper explores LLMs' ability to generate grammars from few examples, introduces a dataset and metrics, and proposes HyGenar, a hybrid genetic algorithm, to improve grammar generation.


<details>
  <summary>Details</summary>
Motivation: To study and enhance LLMs' capability in few-shot grammar generation, as their potential in this area is underexplored.

Method: Introduced a dataset of 540 grammar challenges, devised 6 metrics, and evaluated 8 LLMs. Proposed HyGenar, an LLM-driven hybrid genetic algorithm.

Result: Existing LLMs perform poorly in grammar generation. HyGenar significantly improves syntactic and semantic correctness.

Conclusion: HyGenar effectively enhances LLMs' grammar generation, addressing their current limitations.

Abstract: Grammar plays a critical role in natural language processing and text/code
generation by enabling the definition of syntax, the creation of parsers, and
guiding structured outputs. Although large language models (LLMs) demonstrate
impressive capabilities across domains, their ability to infer and generate
grammars has not yet been thoroughly explored. In this paper, we aim to study
and improve the ability of LLMs for few-shot grammar generation, where grammars
are inferred from sets of a small number of positive and negative examples and
generated in Backus-Naur Form. To explore this, we introduced a novel dataset
comprising 540 structured grammar generation challenges, devised 6 metrics, and
evaluated 8 various LLMs against it. Our findings reveal that existing LLMs
perform sub-optimally in grammar generation. To address this, we propose an
LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar
generation. HyGenar achieves substantial improvements in both the syntactic and
semantic correctness of generated grammars across LLMs.

</details>


### [494] [Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design](https://arxiv.org/pdf/2505.16979)
*Zhenkun Li, Lingyao Li, Shuhang Lin, Yongfeng Zhang*

Main category: cs.AI

TL;DR: KtR framework improves multi-agent LLM performance by decomposing tasks hierarchically, using domain priors and minimal boosts, achieving high accuracy on complex problems.


<details>
  <summary>Details</summary>
Motivation: Single-agent LLMs face limits like finite context and brittle domain transfer, while multi-agent approaches introduce issues like fuzzy contracts. KtR addresses these by leveraging domain priors for structured decomposition.

Method: KtR converts domain priors into a hierarchical blueprint, recursively splitting tasks into typed subtasks with minimal boosts (e.g., chain-of-thought). It avoids universal prompts for disciplined decomposition.

Result: On Knapsack (3-8 items), accuracy rose from 3% to 95% for size-5 instances. On Task-Assignment (6-15 jobs), accuracy reached 100% for size 10 and 84% for sizes 13-15, versus 11% zero-shot.

Conclusion: KtR enables modest models to perform reliably through algorithm-aware decomposition and targeted augmentation, eliminating the need for larger monolithic models.

Abstract: Single-agent LLMs hit hard limits--finite context, role overload, and brittle
domain transfer. Conventional multi-agent fixes soften those edges yet expose
fresh pains: ill-posed decompositions, fuzzy contracts, and verification
overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a
framework that converts domain priors into an algorithmic blueprint hierarchy,
in which tasks are recursively split into typed, controller-mediated subtasks,
each solved zero-shot or with the lightest viable boost (e.g.,
chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch
theorem, KtR trades the chase for a universal prompt for disciplined
decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents
raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a
single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a
six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,
versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation
thus turns modest models into reliable collaborators--no ever-larger monoliths
required.

</details>


### [495] [Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine](https://arxiv.org/pdf/2505.16982)
*Adib Bazgir, Amir Habibdoust Lafmajani, Yuwen Zhang*

Main category: cs.AI

TL;DR: The paper proposes causal LLM agents for biomedicine, integrating multimodal data and intervention-based reasoning to address the lack of true causal understanding in current LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on correlations, not causal understanding, limiting their biomedical applications. The paper aims to bridge this gap by developing causal LLM agents.

Method: The approach involves integrating multimodal data (text, images, genomics), designing safe agentic frameworks, creating causal benchmarks, and combining LLMs with structured knowledge and causal tools.

Result: Potential outcomes include transformative applications like accelerated drug discovery and personalized medicine through automated hypothesis generation and patient-specific models.

Conclusion: The research agenda calls for interdisciplinary collaboration to develop reliable AI partners for biomedical progress by merging causal concepts with foundation models.

Abstract: Large Language Models (LLMs) show promise in biomedicine but lack true causal
understanding, relying instead on correlations. This paper envisions causal LLM
agents that integrate multimodal data (text, images, genomics, etc.) and
perform intervention-based reasoning to infer cause-and-effect. Addressing this
requires overcoming key challenges: designing safe, controllable agentic
frameworks; developing rigorous benchmarks for causal evaluation; integrating
heterogeneous data sources; and synergistically combining LLMs with structured
knowledge (KGs) and formal causal inference tools. Such agents could unlock
transformative opportunities, including accelerating drug discovery through
automated hypothesis generation and simulation, enabling personalized medicine
through patient-specific causal models. This research agenda aims to foster
interdisciplinary efforts, bridging causal concepts and foundation models to
develop reliable AI partners for biomedical progress.

</details>


### [496] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/pdf/2505.14403)
*Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang*

Main category: cs.AI

TL;DR: BCPG-NSA is a new offline RL framework that leverages negative samples for improved reasoning in language models, outperforming baselines in math/coding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods discard or poorly utilize negative samples, missing valuable learning signals like self-reflection and error-correction.

Method: BCPG-NSA involves sample segmentation, consensus-based correctness assessment (LLM + PRM), and policy optimization with Negative Sample Augmentation (NSA).

Result: BCPG-NSA outperforms baselines on math/coding benchmarks, showing better sample efficiency, robustness, and scalability.

Conclusion: The framework effectively mines positive steps from negative samples, enhancing reasoning model performance.

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [497] [FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering](https://arxiv.org/pdf/2405.13873)
*Yuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, Bryan Hooi*

Main category: cs.AI

TL;DR: FiDeLiS is a unified framework that improves LLM factuality by anchoring answers to verifiable reasoning steps from KGs, using step-wise beam search and Path-RAG to reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' challenges with erroneous responses in complex reasoning tasks by leveraging KGs for verifiable knowledge.

Method: Uses step-wise beam search with deductive scoring and Path-RAG to pre-select candidate sets, reducing search space.

Result: Improves performance, factuality, and interpretability across benchmarks without training.

Conclusion: FiDeLiS effectively enhances LLM responses by integrating KG-based verifiable reasoning.

Abstract: Large Language Models (LLMs) are often challenged by generating erroneous or
hallucinated responses, especially in complex reasoning tasks. Leveraging
Knowledge Graphs (KGs) as external knowledge sources has emerged as a viable
solution. However, existing KG-enhanced methods, either retrieval-based or
agent-based, encounter difficulties in accurately retrieving knowledge and
efficiently traversing KGs at scale. In this paper, we propose a unified
framework, FiDeLiS, designed to improve the factuality of LLM responses by
anchoring answers to verifiable reasoning steps retrieved from KGs. To achieve
this, we leverage step-wise beam search with a deductive scoring function,
allowing the LLM to validate reasoning process step by step, and halt the
search once the question is deducible. In addition, we propose a Path-RAG
module to pre-select a smaller candidate set for each beam search step,
reducing computational costs by narrowing the search space. Extensive
experiments show that our method, as a training-free framework, not only
improve the performance but also enhance the factuality and interpretability
across different benchmarks. Code is released at
https://github.com/Y-Sui/FiDeLiS.

</details>


### [498] [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](https://arxiv.org/pdf/2407.10973)
*Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu*

Main category: cs.AI

TL;DR: Make-An-Agent uses conditional diffusion models to generate control policies from single demonstrations, showing versatility and generalization across tasks and real-world robots.


<details>
  <summary>Details</summary>
Motivation: To simplify policy generation for agents, akin to text-to-image models, using minimal demonstrations.

Method: Leverages conditional diffusion models with behavior embeddings to synthesize latent policy parameters, trained on policy checkpoints and trajectories.

Result: Demonstrates versatility, scalability, and strong generalization on unseen tasks with few-shot inputs, including real-world robot deployment.

Conclusion: Make-An-Agent efficiently generates high-performing policies from minimal demonstrations, applicable across diverse domains and real-world scenarios.

Abstract: Can we generate a control policy for an agent using just one demonstration of
desired behaviors as a prompt, as effortlessly as creating an image from a
textual description? In this paper, we present Make-An-Agent, a novel policy
parameter generator that leverages the power of conditional diffusion models
for behavior-to-policy generation. Guided by behavior embeddings that encode
trajectory information, our policy generator synthesizes latent parameter
representations, which can then be decoded into policy networks. Trained on
policy network checkpoints and their corresponding trajectories, our generation
model demonstrates remarkable versatility and scalability on multiple tasks and
has a strong generalization ability on unseen tasks to output well-performed
policies with only few-shot demonstrations as inputs. We showcase its efficacy
and efficiency on various domains and tasks, including varying objectives,
behaviors, and even across different robot manipulators. Beyond simulation, we
directly deploy policies generated by Make-An-Agent onto real-world robots on
locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/

</details>


### [499] [Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models](https://arxiv.org/pdf/2409.16635)
*Sungjune Park, Heehwan Kim, Haehyun Cho, Daeseon Choi*

Main category: cs.AI

TL;DR: JoT introduces a multi-agent prompting approach for binary logical reasoning, outperforming existing methods with 98% accuracy in Boolean expressions.


<details>
  <summary>Details</summary>
Motivation: Existing prompting approaches struggle with complex logical reasoning tasks, prompting the need for a more effective method.

Method: JoT uses a multi-agent system with lawyer, prosecutor, and judge roles to debate and evaluate arguments systematically.

Result: JoT achieves superior performance, including 98% accuracy in Boolean expressions, validated by ablation studies.

Conclusion: JoT enhances accuracy, reliability, and consistency in binary reasoning, showing practical potential.

Abstract: This paper proposes a novel prompting approach, Judgment of Thought (JoT),
specifically tailored for binary logical reasoning tasks. Despite advances in
prompt engineering, existing approaches still face limitations in handling
complex logical reasoning tasks. To address these issues, JoT introduces a
multi-agent approach with three specialized
roles$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$lawyer, prosecutor, and
judge$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$where a high-level
model acts as the judge, and lower-level models serve as lawyer and prosecutor
to systematically debate and evaluate arguments. Experimental evaluations on
benchmarks such as BigBenchHard and Winogrande demonstrate JoT's superior
performance compared to existing prompting approaches, achieving notable
improvements, including 98\% accuracy in Boolean expressions. Also, our
ablation studies validate the critical contribution of each role, iterative
refinement loops, and feedback mechanisms. Consequently, JoT significantly
enhances accuracy, reliability, and consistency in binary reasoning tasks and
shows potential for practical applications.

</details>


### [500] [Bias Amplification: Large Language Models as Increasingly Biased Media](https://arxiv.org/pdf/2410.15234)
*Ze Wang, Zekun Wu, Jeremy Zhang, Xin Guan, Navya Jain, Skylar Lu, Saloni Gupta, Adriano Koshiyama*

Main category: cs.AI

TL;DR: The paper explores bias amplification in LLMs due to iterative synthetic training, introducing a benchmark for political bias measurement and revealing persistent bias despite model collapse control.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored implications of model collapse on bias amplification in LLMs, given their growing societal influence.

Method: Uses a generational benchmark with sentence continuation tasks on U.S. political news, tests mitigation strategies, and employs mechanistic neuron analysis.

Result: Shows consistent political bias amplification (e.g., right-leaning) and distinct neuron populations driving bias and model collapse.

Conclusion: Bias amplification and model collapse have different mechanisms, requiring targeted mitigation strategies.

Abstract: Model collapse, a phenomenon characterized by performance degradation due to
iterative training on synthetic data, has been widely studied. However, its
implications for bias amplification, the progressive intensification of
pre-existing societal biases in Large Language Models (LLMs), remain
significantly underexplored, despite the growing influence of LLMs in shaping
online discourse. In this paper, we introduce a open, generational, and
long-context benchmark specifically designed to measure political bias
amplification in LLMs, leveraging sentence continuation tasks derived from a
comprehensive dataset of U.S. political news. Our empirical study using GPT-2
reveals consistent and substantial political bias intensification (e.g.,
right-leaning amplification) over iterative synthetic training cycles. We
evaluate three mitigation strategies, Overfitting, Preservation, and
Accumulation, and demonstrate that bias amplification persists independently of
model collapse, even when the latter is effectively controlled. Furthermore, we
propose a mechanistic analysis approach that identifies neurons correlated with
specific phenomena during inference through regression and statistical tests.
This analysis uncovers largely distinct neuron populations driving bias
amplification and model collapse, underscoring fundamentally different
underlying mechanisms. Finally, we supplement our empirical findings with
theoretical intuition that explains the separate origins of these phenomena,
guiding targeted strategies for bias mitigation.

</details>


### [501] [Facial Expression Analysis and Its Potentials in IoT Systems: A Contemporary Survey](https://arxiv.org/pdf/2412.17616)
*Zixuan Shanggua, Yanjie Dong, Song Guo, Victor C. M. Leung, M. Jamal Deen, Xiping Hu*

Main category: cs.AI

TL;DR: The paper reviews facial expression analysis (macro- and micro-expressions) and its integration with IoT systems, highlighting applications in healthcare and security, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of combining facial expression analysis with IoT for improved mental health care and security, and to provide a comprehensive overview of advancements in this field.

Method: The work reviews existing research, distinguishes itself from prior surveys, and examines advancements in MaE and MiE analysis techniques across learning paradigms.

Result: Identifies practical applications in IoT-enhanced healthcare and security, and outlines challenges and future directions for the field.

Conclusion: The paper systematically presents how facial expression analysis can enhance IoT systems, fostering innovation in healthcare, security, and other domains.

Abstract: Facial expressions convey human emotions and can be categorized into
macro-expressions (MaEs) and micro-expressions (MiEs) based on duration and
intensity. While MaEs are voluntary and easily recognized, MiEs are
involuntary, rapid, and can reveal concealed emotions. The integration of
facial expression analysis with Internet-of-Thing (IoT) systems has significant
potential across diverse scenarios. IoT-enhanced MaE analysis enables real-time
monitoring of patient emotions, facilitating improved mental health care in
smart healthcare. Similarly, IoT-based MiE detection enhances surveillance
accuracy and threat detection in smart security. Our work aims to provide a
comprehensive overview of research progress in facial expression analysis and
explores its potential integration with IoT systems. We discuss the
distinctions between our work and existing surveys, elaborate on advancements
in MaE and MiE analysis techniques across various learning paradigms, and
examine their potential applications in IoT. We highlight challenges and future
directions for the convergence of facial expression-based technologies and IoT
systems, aiming to foster innovation in this domain. By presenting recent
developments and practical applications, our work offers a systematic
understanding of the ways of facial expression analysis to enhance IoT systems
in healthcare, security, and beyond.

</details>


### [502] [To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization](https://arxiv.org/pdf/2502.00691)
*Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, Wei Chu, Fangzhen Lin*

Main category: cs.AI

TL;DR: The paper introduces an Expectation-Maximization (EM) framework to improve autonomous code integration in language models, addressing inefficiencies in reinforcement learning for this task.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid frameworks for mathematical problem-solving lack metacognitive awareness, relying on rigid instructions or templates. This limits dynamic adaptation of tool-usage strategies.

Method: Proposes an EM framework combining structured exploration (E-step) with off-policy RL optimization (M-step) to enhance autonomous code integration.

Result: The method achieves significant improvements: 11% on MATH500 and 9.4% on AIME, outperforming existing approaches.

Conclusion: The EM framework effectively addresses exploration challenges in autonomous code integration, leading to superior performance in mathematical problem-solving.

Abstract: Recent advances in mathematical problem-solving with language models (LMs)
integrate chain-of-thought (CoT) reasoning and code execution to harness their
complementary strengths. However, existing hybrid frameworks exhibit a critical
limitation: they depend on externally dictated instructions or rigid
code-integration templates, lacking metacognitive awareness -- the capacity to
dynamically evaluate intrinsic capabilities and autonomously determine when and
how to integrate tools. This rigidity motivates our study of autonomous code
integration, enabling models to adapt tool-usage strategies as their reasoning
abilities evolve during training.
  While reinforcement learning (RL) shows promise for boosting LLM reasoning at
scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning
autonomous code integration due to inadequate exploration of the vast
combinatorial space of CoT-code interleaving patterns. To address this
challenge, we propose a novel Expectation-Maximization (EM) framework that
synergizes structured exploration (E-step) with off-policy RL optimization
(M-step), creating a self-reinforcing cycle between metacognitive tool-use
decisions and evolving capabilities. Experiments reveal our method achieves
superior results through improved exploration. Notably, our 7B model improves
over 11% on MATH500 and 9.4% on AIME without o1-like CoT.

</details>


### [503] [Knowledge Integration Strategies in Autonomous Vehicle Prediction and Planning: A Comprehensive Survey](https://arxiv.org/pdf/2502.10477)
*Kumar Manas, Adrian Paschke*

Main category: cs.AI

TL;DR: A survey on integrating knowledge-based approaches in autonomous driving, focusing on trajectory prediction and planning, analyzing methodologies, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore how domain knowledge, traffic rules, and commonsense reasoning can enhance autonomous driving systems.

Method: Categorizes and analyzes approaches by knowledge representation (symbolic to hybrid neuro-symbolic), including logic programming, foundation models, and reinforcement learning.

Result: Identifies trends like interpretable AI, formal verification, and hybrid methods combining knowledge representation with machine learning.

Conclusion: Highlights challenges and opportunities for future research in knowledge-enhanced autonomous driving.

Abstract: This comprehensive survey examines the integration of knowledge-based
approaches in autonomous driving systems, specifically focusing on trajectory
prediction and planning. We extensively analyze various methodologies for
incorporating domain knowledge, traffic rules, and commonsense reasoning into
autonomous driving systems. The survey categorizes and analyzes approaches
based on their knowledge representation and integration methods, ranging from
purely symbolic to hybrid neuro-symbolic architectures. We examine recent
developments in logic programming, foundation models for knowledge
representation, reinforcement learning frameworks, and other emerging
technologies incorporating domain knowledge. This work systematically reviews
recent approaches, identifying key challenges, opportunities, and future
research directions in knowledge-enhanced autonomous driving systems. Our
analysis reveals emerging trends in the field, including the increasing
importance of interpretable AI, the role of formal verification in
safety-critical systems, and the potential of hybrid approaches that combine
traditional knowledge representation with modern machine learning techniques.

</details>


### [504] [CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space](https://arxiv.org/pdf/2502.12532)
*Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang*

Main category: cs.AI

TL;DR: CityEQA introduces a new task for embodied question answering in urban settings, supported by a benchmark dataset and a novel agent (PMA) that outperforms baselines but still lags behind human performance.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored complexities of urban environments in embodied question answering, bridging the gap between indoor and outdoor settings.

Method: Introduces CityEQA-EC dataset and PMA agent, which uses hierarchical task execution (Planner-Manager-Actor) for long-horizon planning and spatial reasoning.

Result: PMA achieves 60.7% human-level accuracy, outperforming baselines but showing room for improvement in visual reasoning.

Conclusion: This work advances urban spatial intelligence, with the dataset and code publicly available for future research.

Abstract: Embodied Question Answering (EQA) has primarily focused on indoor
environments, leaving the complexities of urban settings-spanning environment,
action, and perception-largely unexplored. To bridge this gap, we introduce
CityEQA, a new task where an embodied agent answers open-vocabulary questions
through active exploration in dynamic city spaces. To support this task, we
present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated
tasks across six categories, grounded in a realistic 3D urban simulator.
Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for
CityEQA. PMA enables long-horizon planning and hierarchical task execution: the
Planner breaks down the question answering into sub-tasks, the Manager
maintains an object-centric cognitive map for spatial reasoning during the
process control, and the specialized Actors handle navigation, exploration, and
collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of
human-level answering accuracy, significantly outperforming competitive
baselines. While promising, the performance gap compared to humans highlights
the need for enhanced visual reasoning in CityEQA. This work paves the way for
future advancements in urban spatial intelligence. Dataset and code are
available at https://github.com/BiluYong/CityEQA.git.

</details>


### [505] [Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals](https://arxiv.org/pdf/2502.16101)
*Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, Yi Zhang*

Main category: cs.AI

TL;DR: RAGuard is a new benchmark evaluating RAG systems' robustness against misleading retrievals, revealing their vulnerability to noisy real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing RAG benchmarks assume clean retrieval settings, misaligning with real-world misinformation, especially in political domains.

Method: RAGuard uses Reddit discussions to create a dataset with supporting, misleading, and irrelevant evidence, testing RAG systems' performance.

Result: RAG systems perform worse with misleading retrievals than zero-shot baselines, showing susceptibility to noise.

Conclusion: RAGuard highlights the need for more robust RAG systems, driving research beyond idealized datasets.

Abstract: Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to handle misleading retrievals and often fail to maintain their own
reasoning when exposed to conflicting or selectively-framed evidence, making
them vulnerable to real-world misinformation. In such real-world retrieval
scenarios, misleading and conflicting information is rampant, particularly in
the political domain, where evidence is often selectively framed, incomplete,
or polarized. However, existing RAG benchmarks largely assume a clean retrieval
setting, where models succeed by accurately retrieving and generating answers
from gold-standard documents. This assumption fails to align with real-world
conditions, leading to an overestimation of RAG system performance. To bridge
this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate
the robustness of RAG systems against misleading retrievals. Unlike prior
benchmarks that rely on synthetic noise, our dataset constructs its retrieval
corpus from Reddit discussions, capturing naturally occurring misinformation.
It categorizes retrieved evidence into three types: supporting, misleading, and
irrelevant, providing a realistic and challenging testbed for assessing how
well RAG systems navigate different retrieval information. Our benchmark
experiments reveal that when exposed to misleading retrievals, all tested
LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no
retrieval at all), highlighting their susceptibility to noisy environments. To
the best of our knowledge, RAGuard is the first benchmark to systematically
assess RAG robustness against misleading evidence. We expect this benchmark
will drive future research toward improving RAG systems beyond idealized
datasets, making them more reliable for real-world applications.

</details>


### [506] [Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models](https://arxiv.org/pdf/2502.19918)
*Yuan Sui, Yufei He, Tri Cao, Simeng Han, Yulin Chen, Bryan Hooi*

Main category: cs.AI

TL;DR: Meta-Reasoner optimizes LLM reasoning by dynamically guiding the process, reducing errors and computational waste.


<details>
  <summary>Details</summary>
Motivation: Addressing high computational overhead and error propagation in LLMs' prolonged reasoning chains.

Method: Uses meta-cognition and dual-process theory, employing contextual multi-armed bandits to evaluate and optimize reasoning strategies.

Result: Improves reasoning efficiency and accuracy in tasks like math and puzzles.

Conclusion: Meta-Reasoner offers a scalable, adaptable solution for complex reasoning tasks.

Abstract: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains
to solve complex tasks. However, this trial-and-error approach often leads to
high computational overhead and error propagation, where early mistakes can
derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a
framework that dynamically optimizes inference-time reasoning by enabling LLMs
to \enquote{think about how to think.} Drawing inspiration from human
meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic
advisor, decoupling high-level guidance from step-by-step generation. It
employs contextual multi-armed bandits to iteratively evaluate reasoning
progress and select optimal strategies (e.g., backtrack, clarify ambiguity,
restart from scratch, or propose alternative approaches), and reallocates
computational resources toward the most promising paths. Our evaluations on
mathematical reasoning and puzzles highlight the potential of dynamic reasoning
chains to overcome inherent challenges in the LLM reasoning process and also
show promise in broader applications, offering a scalable and adaptable
solution for reasoning-intensive tasks.

</details>


### [507] [MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow](https://arxiv.org/pdf/2503.18968)
*Ziyue Wang, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, Yueming Jin*

Main category: cs.AI

TL;DR: MedAgent-Pro introduces a hierarchical, evidence-based reasoning paradigm for medical diagnosis, outperforming existing VLMs and expert models.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack quantitative analysis and reliability in medical diagnosis, limiting clinical usability.

Method: MedAgent-Pro decouples diagnosis into disease-level planning (RAG-based agent) and patient-level reasoning (quantitative tools).

Result: Superior performance across anatomical regions, imaging modalities, and diseases, validated by experts.

Conclusion: MedAgent-Pro enhances reliability and clinical relevance through step-by-step, evidence-based reasoning.

Abstract: In modern medicine, clinical diagnosis relies on the comprehensive analysis
of primarily textual and visual data, drawing on medical expertise to ensure
systematic and rigorous reasoning. Recent advances in large Vision-Language
Models (VLMs) and agent-based methods hold great potential for medical
diagnosis, thanks to the ability to effectively integrate multi-modal patient
data. However, they often provide direct answers and draw empirical-driven
conclusions without quantitative analysis, which reduces their reliability and
clinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm
that follows the diagnosis principle in modern medicine, to decouple the
process into sequential components for step-by-step, evidence-based reasoning.
Our MedAgent-Pro workflow presents a hierarchical diagnostic structure to
mirror this principle, consisting of disease-level standardized plan generation
and patient-level personalized step-by-step reasoning. To support disease-level
planning, an RAG-based agent is designed to retrieve medical guidelines to
ensure alignment with clinical standards. For patient-level reasoning, we
propose to integrate professional tools such as visual models to enable
quantitative assessments. Meanwhile, we propose to verify the reliability of
each step to achieve evidence-based diagnosis, enforcing rigorous logical
reasoning and a well-founded conclusion. Extensive experiments across a wide
range of anatomical regions, imaging modalities, and diseases demonstrate the
superiority of MedAgent-Pro to mainstream VLMs, agentic systems and
state-of-the-art expert models. Ablation studies and human evaluation by
clinical experts further validate its robustness and clinical relevance. Code
is available at https://github.com/jinlab-imvr/MedAgent-Pro.

</details>


### [508] [HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation](https://arxiv.org/pdf/2503.21322)
*Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, Luu Anh Tuan*

Main category: cs.AI

TL;DR: HyperGraphRAG improves RAG by using hypergraphs for n-ary relations, outperforming standard and graph-based RAG in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods are limited to binary relations, failing to represent real-world n-ary relations.

Method: Proposes HyperGraphRAG, using hyperedges for n-ary relations, with steps: knowledge hypergraph construction, retrieval, and generation.

Result: Outperforms standard and graph-based RAG in accuracy, retrieval efficiency, and generation quality across multiple domains.

Conclusion: HyperGraphRAG is a superior RAG method for handling complex n-ary relations in real-world knowledge.

Abstract: Standard Retrieval-Augmented Generation (RAG) relies on chunk-based
retrieval, whereas GraphRAG advances this approach by graph-based knowledge
representation. However, existing graph-based RAG approaches are constrained by
binary relations, as each edge in an ordinary graph connects only two entities,
limiting their ability to represent the n-ary relations (n >= 2) in real-world
knowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG
method that represents n-ary relational facts via hyperedges, and consists of
knowledge hypergraph construction, retrieval, and generation. Experiments
across medicine, agriculture, computer science, and law demonstrate that
HyperGraphRAG outperforms both standard RAG and previous graph-based RAG
methods in answer accuracy, retrieval efficiency, and generation quality.

</details>


### [509] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/pdf/2504.17531)
*Justus Flerlage, Ilja Behnke, Odej Kao*

Main category: cs.AI

TL;DR: The paper explores using LLMs like GPT-4o-mini to generate and execute workflows from user intentions, showing feasibility and proficiency in code-oriented tasks.


<details>
  <summary>Details</summary>
Motivation: To reassess user-device interaction by leveraging AI for intent resolution through model-generated code, enabling hybrid human-AI workflows.

Method: Investigates generating and executing workflows via LLM-prompted code generation for a GUI-less OS, analyzing user intentions, code, and execution.

Result: Demonstrates feasibility and GPT-4o-mini's proficiency in generating code workflows aligned with user intentions.

Conclusion: AI-driven code generation for workflows is viable, marking progress in hybrid human-AI collaboration for intent resolution.

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code. This development represents a significant progression in
the realm of hybrid workflows, where human and artificial intelligence
collaborate to address user intentions, with the former responsible for
defining these intentions and the latter for implementing the solutions to
address them. In this paper, we investigate the feasibility of generating and
executing workflows through code generation that results from prompting an LLM
with a concrete user intention, and a simplified application programming
interface for a GUI-less operating system. We provide an in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate the general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [510] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/pdf/2504.20090)
*Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R. Varshney, Dilek Hakkani-Tur*

Main category: cs.AI

TL;DR: Spark is a system combining LLMs and a reviewer model (Judge) for generating and evaluating scientific ideas, grounded in computational creativity principles.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for generating novel research ideas and align with computational creativity principles, while inspiring further exploration in the field.

Method: Uses retrieval-augmented idea generation with LLMs and a reviewer model (Judge) trained on 600K scientific reviews from OpenReview.

Result: Demonstrates the system's potential and releases the annotated dataset for broader research use.

Conclusion: Encourages further exploration of LLMs in idea generation and creative evaluations, sharing resources to foster collaboration.

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>


### [511] [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](https://arxiv.org/pdf/2505.07581)
*Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen*

Main category: cs.AI

TL;DR: YuLan-OneSim is a novel LLM-based social simulator enabling code-free scenario construction, large-scale simulations, and AI-driven social research.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance social behavior simulations by reducing programming needs and providing scalable, evolvable tools for diverse researchers.

Method: Develops a code-free, natural language interface, 50 default scenarios, evolvable LLMs, a distributed architecture for scalability, and an AI social researcher.

Result: Demonstrates high-quality scenario generation, reliable simulations for up to 100,000 agents, and effective AI-driven research automation.

Conclusion: YuLan-OneSim advances social simulation by making it accessible, scalable, and automated, benefiting diverse research domains.

Abstract: Leveraging large language model (LLM) based agents to simulate human social
behaviors has recently gained significant attention. In this paper, we
introduce a novel social simulator called YuLan-OneSim. Compared to previous
works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free
scenario construction: Users can simply describe and refine their simulation
scenarios through natural language interactions with our simulator. All
simulation code is automatically generated, significantly reducing the need for
programming expertise. (2) Comprehensive default scenarios: We implement 50
default simulation scenarios spanning 8 domains, including economics,
sociology, politics, psychology, organization, demographics, law, and
communication, broadening access for a diverse range of social researchers. (3)
Evolvable simulation: Our simulator is capable of receiving external feedback
and automatically fine-tuning the backbone LLMs, significantly enhancing the
simulation quality. (4) Large-scale simulation: By developing a fully
responsive agent framework and a distributed simulation architecture, our
simulator can handle up to 100,000 agents, ensuring more stable and reliable
simulation results. (5) AI social researcher: Leveraging the above features, we
develop an AI social researcher. Users only need to propose a research topic,
and the AI researcher will automatically analyze the input, construct
simulation environments, summarize results, generate technical reports, review
and refine the reports--completing the social science research loop. To
demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate
the quality of the automatically generated scenarios, the reliability,
efficiency, and scalability of the simulation process, as well as the
performance of the AI social researcher.

</details>


### [512] [PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/pdf/2505.10819)
*Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis*

Main category: cs.AI

TL;DR: A novel program synthesis method (PoE-World) uses LLMs to create world models as code, enabling efficient learning from sparse data and strong generalization in complex domains like Atari games.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning world models require extensive data and lack flexibility. Program synthesis with LLMs offers a promising alternative for adaptable, data-efficient world modeling.

Method: The approach represents world models as exponentially-weighted products of programmatic experts (PoE-World) synthesized by LLMs, learning from few observations.

Result: The method successfully learns complex, stochastic world models and enables efficient planning and generalization in Atari games like Pong and Montezuma's Revenge.

Conclusion: PoE-World demonstrates the potential of program synthesis for scalable, adaptable world modeling, with applications in AI agents for complex environments.

Abstract: Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
sparse observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.

</details>


### [513] [Emergent Specialization: Rare Token Neurons in Language Models](https://arxiv.org/pdf/2505.12822)
*Jing Liu, Haozheng Wang, Yueheng Li*

Main category: cs.AI

TL;DR: The paper identifies rare token neurons in large language models, revealing their three-phase organization and coordinated subnetwork behavior, suggesting a statistical basis for their specialization.


<details>
  <summary>Details</summary>
Motivation: To understand how rare tokens are represented and generated in language models, despite their challenges in specialized domains.

Method: Identified rare token neurons, analyzed their three-phase organization during training, and investigated their activation patterns and weight distributions.

Result: Rare token neurons form a coordinated subnetwork with selective co-activation and exhibit a dynamic three-phase structure, correlating with heavy-tailed weight distributions.

Conclusion: The study provides insights into the emergent specialization of rare token neurons, suggesting a statistical mechanical basis for their behavior.

Abstract: Large language models struggle with representing and generating rare tokens
despite their importance in specialized domains. In this study, we identify
neuron structures with exceptionally strong influence on language model's
prediction of rare tokens, termed as rare token neurons, and investigate the
mechanism for their emergence and behavior. These neurons exhibit a
characteristic three-phase organization (plateau, power-law, and rapid decay)
that emerges dynamically during training, evolving from a homogeneous initial
state to a functionally differentiated architecture. In the activation space,
rare token neurons form a coordinated subnetwork that selectively co-activates
while avoiding co-activation with other neurons. This functional specialization
potentially correlates with the development of heavy-tailed weight
distributions, suggesting a statistical mechanical basis for emergent
specialization.

</details>


### [514] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/pdf/2505.14510)
*Haishi Bai, Jozo Dujmovic, Jianwu Wang*

Main category: cs.AI

TL;DR: BACON is a framework for training explainable AI models using graded logic, achieving high accuracy and transparency for human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The need for transparent and trustworthy AI explanations in high-stakes domains like healthcare and finance.

Method: BACON uses graded logic to train explainable models, ensuring structural transparency and symbolic explanations.

Result: BACON delivers high-performance models with compact, human-verifiable decision logic across diverse scenarios.

Conclusion: BACON is a practical and principled approach for trustworthy explainable AI.

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [515] [Guarded Query Routing for Large Language Models](https://arxiv.org/pdf/2505.14524)
*Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, Lukas Galke*

Main category: cs.AI

TL;DR: The paper introduces GQR-Bench for guarded query routing, comparing methods like LLMs, guardrails, and traditional models, finding WideMLP offers the best balance of accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of routing queries to LLM endpoints while handling out-of-distribution queries effectively.

Method: Introduces GQR-Bench and evaluates LLM-based routing, guardrails, and traditional models (e.g., WideMLP, fastText, SVM, XGBoost).

Result: WideMLP with out-of-domain detection achieves 88% accuracy and <4ms speed, while LLMs are slower but more accurate (91%).

Conclusion: LLMs aren't always optimal for query routing; practical recommendations favor WideMLP or fastText for speed-accuracy trade-offs.

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a guarded query routing problem, for
which we first introduce the Guarded Query Routing Benchmark (GQR-Bench), which
covers three exemplary target domains (law, finance, and healthcare), and seven
datasets to test robustness against out-of-distribution queries. We then use
GQR-Bench to contrast the effectiveness and efficiency of LLM-based routing
mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B), standard LLM-based
guardrail approaches (LlamaGuard and NVIDIA NeMo Guardrails), continuous
bag-of-words classifiers (WideMLP, fastText), and traditional machine learning
models (SVM, XGBoost). Our results show that WideMLP, enhanced with
out-of-domain detection capabilities, yields the best trade-off between
accuracy (88%) and speed (<4ms). The embedding-based fastText excels at speed
(<1ms) with acceptable accuracy (80%), whereas LLMs yield the highest accuracy
(91%) but are comparatively slow (62ms for local Llama-3.1:8B and 669ms for
remote GPT-4o-mini calls). Our findings challenge the automatic reliance on
LLMs for (guarded) query routing and provide concrete recommendations for
practical applications. GQR-Bench will be released as a Python package -- gqr.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [516] [A Novel Deep Learning Framework for Efficient Multichannel Acoustic Feedback Control](https://arxiv.org/pdf/2505.15914)
*Yuan-Kuei Wu, Juan Azcarreta, Kashyap Patel, Buye Xu, Jung-Suk Lee, Sanha Lee, Ashutosh Pandey*

Main category: cs.SD

TL;DR: A deep-learning framework using a Convolutional Recurrent Network improves multichannel acoustic feedback control, outperforming traditional methods with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional digital signal processing methods fail to converge with highly correlated noise like feedback, necessitating a more efficient solution.

Method: The framework employs a Convolutional Recurrent Network for spatial and temporal processing, trained via In-a-Loop Training, Teacher Forcing, and a Hybrid strategy with a Multichannel Wiener Filter.

Result: The approach significantly enhances speech enhancement and is scalable for real-world applications.

Conclusion: This framework advances Acoustic Feedback Control technology, offering robust performance in complex acoustic environments.

Abstract: This study presents a deep-learning framework for controlling multichannel
acoustic feedback in audio devices. Traditional digital signal processing
methods struggle with convergence when dealing with highly correlated noise
such as feedback. We introduce a Convolutional Recurrent Network that
efficiently combines spatial and temporal processing, significantly enhancing
speech enhancement capabilities with lower computational demands. Our approach
utilizes three training methods: In-a-Loop Training, Teacher Forcing, and a
Hybrid strategy with a Multichannel Wiener Filter, optimizing performance in
complex acoustic environments. This scalable framework offers a robust solution
for real-world applications, making significant advances in Acoustic Feedback
Control technology.

</details>


### [517] [Source Separation by Flow Matching](https://arxiv.org/pdf/2505.16119)
*Robin Scheibler, John R. Hershey, Arnaud Doucet, Henry Li*

Main category: cs.SD

TL;DR: FLOSS uses flow matching for single-channel audio source separation, ensuring mixture consistency and handling source permutations with an equivariant neural network.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the ill-posed problem of reconstructing multiple sources from a single-channel audio mixture, aiming for accurate and consistent separation.

Method: FLOSS employs flow matching, augmenting mixture samples with noise to match source dimensionality and using an equivariant neural network for permutation invariance.

Result: The method demonstrates effectiveness in separating overlapping speech sources.

Conclusion: FLOSS provides a robust solution for single-channel source separation by leveraging flow matching and equivariant architectures.

Abstract: We consider the problem of single-channel audio source separation with the
goal of reconstructing $K$ sources from their mixture. We address this
ill-posed problem with FLOSS (FLOw matching for Source Separation), a
constrained generation method based on flow matching, ensuring strict mixture
consistency. Flow matching is a general methodology that, when given samples
from two probability distributions defined on the same space, learns an
ordinary differential equation to output a sample from one of the distributions
when provided with a sample from the other. In our context, we have access to
samples from the joint distribution of $K$ sources and so the corresponding
samples from the lower-dimensional distribution of their mixture. To apply flow
matching, we augment these mixture samples with artificial noise components to
ensure the resulting "augmented" distribution matches the dimensionality of the
$K$ source distribution. Additionally, as any permutation of the sources yields
the same mixture, we adopt an equivariant formulation of flow matching which
relies on a suitable custom-designed neural network architecture. We
demonstrate the performance of the method for the separation of overlapping
speech.

</details>


### [518] [Selective Invocation for Multilingual ASR: A Cost-effective Approach Adapting to Speech Recognition Difficulty](https://arxiv.org/pdf/2505.16168)
*Hongfei Xue, Yufeng Tang, Jun Zhang, Xuelong Geng, Lei Xie*

Main category: cs.SD

TL;DR: SIMA, a selective invocation method for multilingual ASR, reduces costs and errors by adapting to input difficulty, outperforming SLLM and LID-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multilingual ASR, such as linguistic differences, data imbalances, and high costs/inaccuracies of LID-based routing.

Method: Proposes SIMA, which uses a spoken large language model (SLLM) to evaluate input difficulty and selectively invoke SOTA ASR models.

Result: Reduces word error rates by 18.7% compared to SLLM and halves invocation costs compared to LID-based methods.

Conclusion: SIMA is a scalable, cost-effective solution for multilingual ASR applications, validated on three datasets.

Abstract: Although multilingual automatic speech recognition (ASR) systems have
significantly advanced, enabling a single model to handle multiple languages,
inherent linguistic differences and data imbalances challenge SOTA performance
across all languages. While language identification (LID) models can route
speech to the appropriate ASR model, they incur high costs from invoking SOTA
commercial models and suffer from inaccuracies due to misclassification. To
overcome these, we propose SIMA, a selective invocation for multilingual ASR
that adapts to the difficulty level of the input speech. Built on a spoken
large language model (SLLM), SIMA evaluates whether the input is simple enough
for direct transcription or requires the invocation of a SOTA ASR model. Our
approach reduces word error rates by 18.7% compared to the SLLM and halves
invocation costs compared to LID-based methods. Tests on three datasets show
that SIMA is a scalable, cost-effective solution for multilingual ASR
applications.

</details>


### [519] [Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data](https://arxiv.org/pdf/2505.16182)
*Kentaro Onda, Keisuke Imoto, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu*

Main category: cs.SD

TL;DR: The study explores accent-robust ASR using native speech data, leveraging ISIB and discrete tokens from SSL models to improve non-native speech recognition.


<details>
  <summary>Details</summary>
Motivation: To achieve accent-robust ASR without relying on non-native speech data by mimicking human perception (ISIB) through discrete tokens.

Method: Analyzed discrete token-based ASR robustness to non-native speech, varying tokenization training languages to simulate ISIB.

Result: ISIB was observed in the discrete token-based ASR, proving its effectiveness.

Conclusion: The approach, using only native speech data, is scalable for accents with scarce data, mimicking human perception effectively.

Abstract: In this study, we gained insight that contributes to achieving accent-robust
ASR using only native speech data. In human perception of non-native speech,
the phenomenon known as "interlanguage speech intelligibility benefit" (ISIB)
is observed, where non-native listeners who share the native language with the
speaker understand the speech better compared even to native listeners. Based
on the idea that discrete tokens extracted from self-supervised learning (SSL)
models represent the human perception of speech, we conducted an analytical
study on the robustness of discrete token-based ASR to non-native speech,
varying the language used for training the tokenization, which is viewed as a
technical implementation of ISIB. The results showed that ISIB actually
occurred in the discrete token-based ASR. Since our approach relies only on
native speech data to simulate the behavior of human perception, it is expected
to be applicable to a wide range of accents for which speech data is scarce.

</details>


### [520] [Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based Resynthesis Only with Native Speech Corpora](https://arxiv.org/pdf/2505.16191)
*Kentaro Onda, Keisuke Imoto, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu*

Main category: cs.SD

TL;DR: A method to synthesize foreign-accented speech using native data and SSL tokens is improved by adding duration modification to better replicate durational accents.


<details>
  <summary>Details</summary>
Motivation: Limited accented speech data availability makes simulating foreign accents challenging. Existing methods fail to reproduce duration-related accents, common in L2 speakers.

Method: Integrates duration modification into a previous SSL-based method to simulate foreign accents more accurately.

Result: The proposed method successfully replicates durational accents observed in real L2 speech.

Conclusion: The enhanced method improves foreign accent simulation, benefiting applications like ASR and human listening materials.

Abstract: Recently, a method for synthesizing foreign-accented speech only with native
speech data using discrete tokens obtained from self-supervised learning (SSL)
models was proposed. Considering limited availability of accented speech data,
this method is expected to make it much easier to simulate foreign accents. By
using the synthesized accented speech as listening materials for humans or
training data for automatic speech recognition (ASR), both of them will acquire
higher robustness against foreign accents. However, the previous method has a
fatal flaw that it cannot reproduce duration-related accents. Durational
accents are commonly seen when L2 speakers, whose native language has
syllable-timed or mora-timed rhythm, speak stress-timed languages, such as
English. In this paper, we integrate duration modification to the previous
method to simulate foreign accents more accurately. Experiments show that the
proposed method successfully replicates durational accents seen in real L2
speech.

</details>


### [521] [SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet](https://arxiv.org/pdf/2505.16195)
*Zhi Zhong, Akira Takahashi, Shuyang Cui, Keisuke Toyama, Shusuke Takahashi, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: SpecMaskFoley improves ControlNet-based foley synthesis by aligning video features with a pretrained audio model, outperforming from-scratch methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between ControlNet-based and from-scratch foley synthesis models by leveraging pretrained models and better feature alignment.

Method: Proposes SpecMaskFoley, which uses a frequency-aware temporal feature aligner to adapt SpecMaskGIT for video-synchronized foley synthesis via ControlNet.

Result: Outperforms strong from-scratch baselines on a common benchmark.

Conclusion: SpecMaskFoley advances ControlNet-based foley synthesis, demonstrating the potential of pretrained models with proper feature alignment.

Abstract: Foley synthesis aims to synthesize high-quality audio that is both
semantically and temporally aligned with video frames. Given its broad
application in creative industries, the task has gained increasing attention in
the research community. To avoid the non-trivial task of training audio
generative models from scratch, adapting pretrained audio generative models for
video-synchronized foley synthesis presents an attractive direction.
ControlNet, a method for adding fine-grained controls to pretrained generative
models, has been applied to foley synthesis, but its use has been limited to
handcrafted human-readable temporal conditions. In contrast, from-scratch
models achieved success by leveraging high-dimensional deep features extracted
using pretrained video encoders. We have observed a performance gap between
ControlNet-based and from-scratch foley models. To narrow this gap, we propose
SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward
video-synchronized foley synthesis via ControlNet. To unlock the potential of a
single ControlNet branch, we resolve the discrepancy between the temporal video
features and the time-frequency nature of the pretrained SpecMaskGIT via a
frequency-aware temporal feature aligner, eliminating the need for complicated
conditioning mechanisms widely used in prior arts. Evaluations on a common
foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform
strong from-scratch baselines, substantially advancing the development of
ControlNet-based foley synthesis models. Demo page:
https://zzaudio.github.io/SpecMaskFoley_Demo/

</details>


### [522] [Differentiable K-means for Fully-optimized Discrete Token-based ASR](https://arxiv.org/pdf/2505.16207)
*Kentaro Onda, Yosuke Kashiwagi, Emiru Tsunoo, Hayato Futami, Shinji Watanabe*

Main category: cs.SD

TL;DR: The paper proposes differentiable k-means for joint optimization of tokenization and downstream tasks, improving ASR accuracy and phonetic purity.


<details>
  <summary>Details</summary>
Motivation: Discrete tokens from SSL models are suboptimal for specific tasks due to independent k-means clustering.

Method: Differentiable k-means is introduced to jointly optimize tokenization and downstream tasks, fine-tuning SSL parameters and layer weights.

Result: ASR accuracy improved, and tokens showed higher phonetic purity, benefiting speech resynthesis.

Conclusion: Joint optimization enhances token utility for speech tasks, demonstrating broader applicability.

Abstract: Recent studies have highlighted the potential of discrete tokens derived from
self-supervised learning (SSL) models for various speech-related tasks. These
tokens serve not only as substitutes for text in language modeling but also as
intermediate representations for tasks such as automatic speech recognition
(ASR). However, discrete tokens are typically obtained via k-means clustering
of SSL features independently of downstream tasks, making them suboptimal for
specific applications. This paper proposes the use of differentiable k-means,
enabling the joint optimization of tokenization and downstream tasks. This
approach enables the fine-tuning of the SSL parameters and learning weights for
outputs from multiple SSL layers. Experiments were conducted with ASR as a
downstream task. ASR accuracy successfully improved owing to the optimized
tokens. The acquired tokens also exhibited greater purity of phonetic
information, which were found to be useful even in speech resynthesis.

</details>


### [523] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/pdf/2505.16211)
*Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhuo Chen, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li*

Main category: cs.SD

TL;DR: AudioTrust is a framework for evaluating trustworthiness in Audio Large Language Models (ALLMs) across six dimensions, using a dataset of 4,420 samples and 9 metrics.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks lack focus on audio-specific risks, necessitating a dedicated evaluation tool for ALLMs.

Method: AudioTrust uses 18 experimental setups and a dataset of real-world audio/text samples to assess fairness, hallucination, safety, privacy, robustness, and authentication.

Result: The framework reveals trustworthiness limitations in current ALLMs, providing insights for secure deployment.

Conclusion: AudioTrust offers a comprehensive benchmark for evaluating and improving the trustworthiness of ALLMs.

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


### [524] [Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System](https://arxiv.org/pdf/2505.16259)
*Hayeon Bang, Taegyun Kwon, Juhan Nam*

Main category: cs.SD

TL;DR: The paper introduces <Dialogue in Resonance>, an interactive music piece combining human and computer-controlled piano, using real-time transcription for dynamic interaction.


<details>
  <summary>Details</summary>
Motivation: To create a balanced framework integrating composed structure with live interaction, moving beyond improvisation-based approaches.

Method: Uses real-time automatic transcription to interpret and respond to the pianist's input, blending composition with unpredictability.

Result: Developed a functional interactive piece, detailing technical implementation, rehearsal, and performance.

Conclusion: Successfully demonstrated a novel approach to interactive music, balancing structure and live dynamics.

Abstract: This paper presents <Dialogue in Resonance>, an interactive music piece for a
human pianist and a computer-controlled piano that integrates real-time
automatic music transcription into a score-driven framework. Unlike previous
approaches that primarily focus on improvisation-based interactions, our work
establishes a balanced framework that combines composed structure with dynamic
interaction. Through real-time automatic transcription as its core mechanism,
the computer interprets and responds to the human performer's input in real
time, creating a musical dialogue that balances compositional intent with live
interaction while incorporating elements of unpredictability. In this paper, we
present the development process from composition to premiere performance,
including technical implementation, rehearsal process, and performance
considerations.

</details>


### [525] [Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models](https://arxiv.org/pdf/2505.16306)
*Yizhi Zhou, Haina Zhu, Hangting Chen*

Main category: cs.SD

TL;DR: Analysis of SSL models MusicFM and MuQ in music information retrieval, focusing on their advantages, layer-wise specialization, and performance differences.


<details>
  <summary>Details</summary>
Motivation: Limited research on the encoded information and applicability of SSL models in music information retrieval, prompting a deeper understanding for better downstream task use.

Method: Analyze MusicFM and MuQ by validating SSL advantages, exploring layer-wise task specialization, and comparing performance with specific layers.

Result: Reveals insights into SSL model structure and potential applications in music information retrieval.

Conclusion: Understanding SSL model capabilities and limitations enhances their effective use in downstream tasks.

Abstract: Recently, pre-trained models for music information retrieval based on
self-supervised learning (SSL) are becoming popular, showing success in various
downstream tasks. However, there is limited research on the specific meanings
of the encoded information and their applicability. Exploring these aspects can
help us better understand their capabilities and limitations, leading to more
effective use in downstream tasks.
  In this study, we analyze the advanced music representation model MusicFM and
the newly emerged SSL model MuQ. We focus on three main aspects: (i) validating
the advantages of SSL models across multiple downstream tasks, (ii) exploring
the specialization of layer-wise information for different tasks, and (iii)
comparing performance differences when selecting specific layers. Through this
analysis, we reveal insights into the structure and potential applications of
SSL models in music information retrieval.

</details>


### [526] [X-ARES: A Comprehensive Framework for Assessing Audio Encoder Performance](https://arxiv.org/pdf/2505.16369)
*Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, Jian Luan*

Main category: cs.SD

TL;DR: X-ARES is a new open-source benchmark for evaluating audio encoders across diverse domains using two approaches: linear fine-tuning and unparameterized evaluation.


<details>
  <summary>Details</summary>
Motivation: To systematically assess audio encoder performance across various domains like speech, environmental sounds, and music.

Method: The framework includes 22 tasks covering audio processing aspects, evaluated via linear fine-tuning and unparameterized methods.

Result: Evaluation shows significant performance variations across tasks and domains, indicating the complexity of general audio representation learning.

Conclusion: X-ARES effectively benchmarks audio encoders, revealing domain-specific challenges in audio representation learning.

Abstract: We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a
novel open-source benchmark designed to systematically assess audio encoder
performance across diverse domains. By encompassing tasks spanning speech,
environmental sounds, and music, X-ARES provides two evaluation approaches for
evaluating audio representations: linear fine-tuning and unparameterized
evaluation. The framework includes 22 distinct tasks that cover essential
aspects of audio processing, from speech recognition and emotion detection to
sound event classification and music genre identification. Our extensive
evaluation of state-of-the-art audio encoders reveals significant performance
variations across different tasks and domains, highlighting the complexity of
general audio representation learning.

</details>


### [527] [EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion](https://arxiv.org/pdf/2505.16691)
*Advait Joglekar, Divyanshu Singh, Rooshil Rohit Bhatia, S. Umesh*

Main category: cs.SD

TL;DR: A novel voice conversion method combines discrete speech representations with a Diffusion-Transformer decoder, excelling in zero-shot cross-lingual settings without multiple encoders.


<details>
  <summary>Details</summary>
Motivation: Current voice conversion methods struggle with zero-shot cross-lingual generalization for unseen languages and accents.

Method: Combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer decoder, trained textlessly.

Result: The model performs well in zero-shot cross-lingual settings, even for unseen languages.

Conclusion: The proposed approach is simple, effective, and generalizes well for zero-shot voice conversion across languages.

Abstract: Voice Conversion research in recent times has increasingly focused on
improving the zero-shot capabilities of existing methods. Despite remarkable
advancements, current architectures still tend to struggle in zero-shot
cross-lingual settings. They are also often unable to generalize for speakers
of unseen languages and accents. In this paper, we adopt a simple yet effective
approach that combines discrete speech representations from self-supervised
models with a non-autoregressive Diffusion-Transformer based conditional flow
matching speech decoder. We show that this architecture allows us to train a
voice-conversion model in a purely textless, self-supervised fashion. Our
technique works without requiring multiple encoders to disentangle speech
features. Our model also manages to excel in zero-shot cross-lingual settings
even for unseen languages.

</details>


### [528] [Long-Form Text-to-Music Generation with Adaptive Prompts: A Case Study in Tabletop Role-Playing Games Soundtracks](https://arxiv.org/pdf/2411.03948)
*Felipe Marra, Lucas N. Ferreira*

Main category: cs.SD

TL;DR: The paper explores text-to-audio music generation for dynamic TRPG soundtracks, comparing LLM-based methods for improved quality and alignment.


<details>
  <summary>Details</summary>
Motivation: To enhance soundtrack generation for TRPGs by enabling dynamic, prompt-driven music creation.

Method: Introduces Babel Bardo, using LLMs to convert speech into music descriptions for text-to-music models, comparing four versions in TRPG campaigns.

Result: Detailed music descriptions boost audio quality; consistent descriptions improve story alignment and transitions.

Conclusion: LLM-based approaches enhance dynamic music generation for TRPGs, balancing quality and narrative coherence.

Abstract: This paper investigates the capabilities of text-to-audio music generation
models in producing long-form music with prompts that change over time,
focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We
introduce Babel Bardo, a system that uses Large Language Models (LLMs) to
transform speech transcriptions into music descriptions for controlling a
text-to-music model. Four versions of Babel Bardo were compared in two TRPG
campaigns: a baseline using direct speech transcriptions, and three LLM-based
versions with varying approaches to music description generation. Evaluations
considered audio quality, story alignment, and transition smoothness. Results
indicate that detailed music descriptions improve audio quality while
maintaining consistency across consecutive descriptions enhances story
alignment and transition smoothness.

</details>


### [529] [Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN](https://arxiv.org/pdf/2505.15368)
*Yicheng Gu, Chaoren Wang, Zhizheng Wu, Lauri Juvela*

Main category: cs.SD

TL;DR: Neurodyne improves pitch manipulation in music production by using adversarial representation learning and cycle-consistency training, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current neural-network-based pitch-manipulation systems face limitations due to inaccurate feature disentanglement and lack of paired training data.

Method: Neurodyne employs adversarial representation learning for pitch-independent latent representation and cycle-consistency training for implicit paired data creation.

Result: Experiments show improved synthesis quality while preserving singer identity in global-key and template-based pitch manipulation.

Conclusion: Neurodyne effectively addresses existing limitations, enhancing pitch manipulation in music production.

Abstract: Pitch manipulation is the process of producers adjusting the pitch of an
audio segment to a specific key and intonation, which is essential in music
production. Neural-network-based pitch-manipulation systems have been popular
in recent years due to their superior synthesis quality compared to classical
DSP methods. However, their performance is still limited due to their
inaccurate feature disentanglement using source-filter models and the lack of
paired in- and out-of-tune training data. This work proposes Neurodyne to
address these issues. Specifically, Neurodyne uses adversarial representation
learning to learn a pitch-independent latent representation to avoid inaccurate
disentanglement and cycle-consistency training to create paired training data
implicitly. Experimental results on global-key and template-based pitch
manipulation demonstrate the effectiveness of the proposed system, marking
improved synthesis quality while maintaining the original singer identity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [530] [Adaptive Tokenization: On the Hop-Overpriority Problem in Tokenized Graph Learning Models](https://arxiv.org/pdf/2505.15845)
*Zhibiao Wang, Yunlong Zhou, Ziwei Zhang, Mengmei Zhang, Shirui Pan, Chunming Hu, Xiao Wang*

Main category: cs.LG

TL;DR: The paper introduces Learnable Graph Token List (LGTL) to address the hop-overpriority problem in Tokenized Graph Learning Models (TGLMs), improving adaptability and performance for both homophilic and heterophilic graphs.


<details>
  <summary>Details</summary>
Motivation: Existing TGLMs rely on hand-designed token lists, which overemphasize nearby nodes and struggle to balance local and global signals, especially in heterophilic graphs.

Method: Proposes LGTL, a plug-and-play module with a graph attention gate and selection module to adaptively adjust hop weights and prioritize informative nodes.

Result: LGTL effectively addresses the hop-overpriority problem and improves performance across Graph Transformers and Graph LLM benchmarks.

Conclusion: LGTL offers a scalable and adaptable solution for TGLMs, enhancing their ability to handle diverse graph learning scenarios.

Abstract: Graph Transformers, leveraging the global attention to capture long-range
dependencies in graph structures, have significantly advanced graph machine
learning, but face prohibitive computational complexity. Tokenized Graph
Learning Models (TGLMs) address this issue by converting graphs into ordered
token lists for scalable processing. Besides, TGLMs also empower Large Language
Models (LLMs) to handle text-attributed graphs more effectively and thus are
also employed in Graph LLMs. However, existing TGLMs rely on hand-designed
token lists and their adaptability to diverse graph learning scenarios remains
unexplored. In this paper, we first conduct extensive empirical and theoretical
preliminary studies for hand-designed token lists. Surprisingly, we identify an
unexplored hop-overpriority problem: the common pre-defined token lists
overemphasize nearby nodes and overwhelm the ability of TGLMs to balance local
and global signals. This phenomenon is especially harmful for heterophilic
graphs. To address this problem, we propose the Learnable Graph Token List
(LGTL), a plug-and-play module to replace hand-designed token lists in TGLMs.
Specifically, LGTL adaptively adjusts the weights across hops and prioritizes
informative nodes within hops through a graph attention gate module and a
selection module, respectively. In this way, contextually informative nodes can
be adaptively emphasized for both homophilic and heterophilic graphs. Besides,
we theoretically show that LGTL can address the hop-overpriority problem.
Extensive experiments on benchmarks validate the efficacy of LGTL across both
Graph Transformers and Graph LLM backbones.

</details>


### [531] [Last Layer Empirical Bayes](https://arxiv.org/pdf/2505.15888)
*Valentin Villecroze, Yixin Wang, Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: LLEB (last layer empirical Bayes) is proposed as a method for uncertainty quantification in neural networks, combining ideas from BNNs and ensembles by using a learnable prior on the last layer.


<details>
  <summary>Details</summary>
Motivation: Quantifying uncertainty in neural network predictions is crucial, with BNNs and deep ensembles being prominent approaches. LLEB aims to bridge these methods by leveraging empirical Bayes.

Method: LLEB uses a learnable prior (normalizing flow) on the last layer, trained to maximize the evidence lower bound, balancing between BNNs and ensembles.

Result: LLEB performs comparably to existing methods, demonstrating its effectiveness.

Conclusion: Empirical Bayes, as exemplified by LLEB, is a promising direction for future research in uncertainty quantification.

Abstract: The task of quantifying the inherent uncertainty associated with neural
network predictions is a key challenge in artificial intelligence. Bayesian
neural networks (BNNs) and deep ensembles are among the most prominent
approaches to tackle this task. Both approaches produce predictions by
computing an expectation of neural network outputs over some distribution on
the corresponding weights; this distribution is given by the posterior in the
case of BNNs, and by a mixture of point masses for ensembles. Inspired by
recent work showing that the distribution used by ensembles can be understood
as a posterior corresponding to a learned data-dependent prior, we propose last
layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a
normalizing flow, which is then trained to maximize the evidence lower bound;
to retain tractability we use the flow only on the last layer. We show why LLEB
is well motivated, and how it interpolates between standard BNNs and ensembles
in terms of the strength of the prior that they use. LLEB performs on par with
existing approaches, highlighting that empirical Bayes is a promising direction
for future research in uncertainty quantification.

</details>


### [532] [Is (Selective) Round-To-Nearest Quantization All You Need?](https://arxiv.org/pdf/2505.15909)
*Alex Kogan*

Main category: cs.LG

TL;DR: RTN quantization, often dismissed, is shown to be cost-effective, competitive in throughput, and similar in accuracy to advanced methods, making it viable for LLMs.


<details>
  <summary>Details</summary>
Motivation: To challenge the dismissal of RTN quantization by demonstrating its cost-effectiveness, competitive performance, and accuracy improvements for LLMs.

Method: Implemented RTN using Marlin kernels and selectively increased data precision for certain layers to improve accuracy.

Result: RTN achieved comparable accuracy and better throughput than advanced methods, proving its practicality.

Conclusion: RTN is a viable and practical choice for quantizing LLMs, offering simplicity and competitive performance.

Abstract: Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
quantization technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced quantization methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.

</details>


### [533] [AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning](https://arxiv.org/pdf/2505.15931)
*Morteza Alizadeh, Mehrdad Oveisi, Sonya Falahati, Ghazal Mousavi, Mohsen Alambardar Meybodi, Somayeh Sadat Mehrnia, Ilker Hacihaliloglu, Arman Rahmim, Mohammad R. Salmanpour*

Main category: cs.LG

TL;DR: AllMetrics is a unified Python library addressing inconsistencies in ML metric evaluation by standardizing implementations across tasks.


<details>
  <summary>Details</summary>
Motivation: Fragmentation and inconsistency in existing ML metric libraries lead to unreliable results and hinder comparability.

Method: Developed AllMetrics, a modular API with robust validation, tested across diverse datasets and compared with other tools.

Result: AllMetrics mitigates evaluation errors and enhances reliability in ML workflows, as demonstrated empirically.

Conclusion: AllMetrics provides a standardized, trustworthy solution for ML metric evaluation, improving reproducibility and comparability.

Abstract: Machine learning (ML) models rely heavily on consistent and accurate
performance metrics to evaluate and compare their effectiveness. However,
existing libraries often suffer from fragmentation, inconsistent
implementations, and insufficient data validation protocols, leading to
unreliable results. Existing libraries have often been developed independently
and without adherence to a unified standard, particularly concerning the
specific tasks they aim to support. As a result, each library tends to adopt
its conventions for metric computation, input/output formatting, error
handling, and data validation protocols. This lack of standardization leads to
both implementation differences (ID) and reporting differences (RD), making it
difficult to compare results across frameworks or ensure reliable evaluations.
To address these issues, we introduce AllMetrics, an open-source unified Python
library designed to standardize metric evaluation across diverse ML tasks,
including regression, classification, clustering, segmentation, and
image-to-image translation. The library implements class-specific reporting for
multi-class tasks through configurable parameters to cover all use cases, while
incorporating task-specific parameters to resolve metric computation
discrepancies across implementations. Various datasets from domains like
healthcare, finance, and real estate were applied to our library and compared
with Python, Matlab, and R components to identify which yield similar results.
AllMetrics combines a modular Application Programming Interface (API) with
robust input validation mechanisms to ensure reproducibility and reliability in
model evaluation. This paper presents the design principles, architectural
components, and empirical analyses demonstrating the ability to mitigate
evaluation errors and to enhance the trustworthiness of ML workflows.

</details>


### [534] [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/pdf/2505.15946)
*Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun*

Main category: cs.LG

TL;DR: MoRE-Brain is a neuro-inspired framework for high-fidelity, adaptable, and interpretable visual reconstruction from fMRI, using a hierarchical Mixture-of-Experts architecture and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current fMRI-based visual decoding prioritizes fidelity over interpretability, limiting neuroscientific insights. MoRE-Brain aims to bridge this gap.

Method: Hierarchical Mixture-of-Experts architecture processes fMRI signals, trained in CLIP space, with a diffusion model for image synthesis and dual-stage routing.

Result: Achieves high reconstruction fidelity, cross-subject generalization, and mechanistic insight into brain region contributions.

Conclusion: MoRE-Brain advances fMRI-based visual decoding by improving generalizability and interpretability.

Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand
human perception and develop advanced brain-computer interfaces. However,
current progress often prioritizes maximizing reconstruction fidelity while
overlooking interpretability, an essential aspect for deriving neuroscientific
insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework
designed for high-fidelity, adaptable, and interpretable visual reconstruction.
MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture
where distinct experts process fMRI signals from functionally related voxel
groups, mimicking specialized brain networks. The experts are first trained to
encode fMRI into the frozen CLIP space. A finetuned diffusion model then
synthesizes images, guided by expert outputs through a novel dual-stage routing
mechanism that dynamically weighs expert contributions across the diffusion
process. MoRE-Brain offers three main advancements: First, it introduces a
novel Mixture-of-Experts architecture grounded in brain network principles for
neuro-decoding. Second, it achieves efficient cross-subject generalization by
sharing core expert networks while adapting only subject-specific routers.
Third, it provides enhanced mechanistic insight, as the explicit routing
reveals precisely how different modeled brain regions shape the semantic and
spatial attributes of the reconstructed image. Extensive experiments validate
MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further
demonstrating its effective utilization of fMRI signals, distinguishing genuine
neural decoding from over-reliance on generative priors. Consequently,
MoRE-Brain marks a substantial advance towards more generalizable and
interpretable fMRI-based visual decoding. Code will be publicly available soon:
https://github.com/yuxiangwei0808/MoRE-Brain.

</details>


### [535] [Towards Identifiability of Interventional Stochastic Differential Equations](https://arxiv.org/pdf/2505.15987)
*Aaron Zweig, Zaikang Lin, Elham Azizi, David Knowles*

Main category: cs.LG

TL;DR: The paper provides provable bounds for uniquely recovering SDE parameters from stationary distributions under interventions, with tight bounds for linear SDEs and upper bounds for nonlinear SDEs.


<details>
  <summary>Details</summary>
Motivation: To address identifiability challenges in SDE models under interventions and provide theoretical guarantees for parameter recovery.

Method: Theoretical analysis of SDE identifiability under interventions, with experiments on synthetic data and learnable activation functions.

Result: Tight bounds for linear SDEs and upper bounds for nonlinear SDEs in small noise regimes, validated experimentally.

Conclusion: The study advances SDE model identifiability, with practical implications for parameter recovery and learnable parameterizations.

Abstract: We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.

</details>


### [536] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/pdf/2505.16004)
*Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: SAEs for interpreting LLMs lack robustness to input perturbations, making concept representations fragile and unreliable for model oversight.


<details>
  <summary>Details</summary>
Motivation: Existing SAE evaluations overlook robustness, a critical aspect for concept representation fidelity.

Method: Formulated robustness quantification via input-space optimization and adversarial perturbation crafting.

Result: Tiny adversarial perturbations manipulate SAE interpretations without affecting LLM outputs.

Conclusion: SAE concept representations are fragile, unsuitable for model monitoring and oversight.

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [537] [HyperMARL: Adaptive Hypernetworks for Multi-Agent RL](https://arxiv.org/pdf/2412.04233)
*Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht*

Main category: cs.LG

TL;DR: HyperMARL is a parameter-sharing MARL method using hypernetworks to dynamically adapt agent-specific parameters, reducing gradient interference and preserving diversity without added complexity.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of parameter-sharing in MARL, which often restricts behavioral diversity due to cross-agent gradient interference, HyperMARL aims to enable adaptive policies without altering objectives or requiring preset diversity levels.

Method: HyperMARL employs hypernetworks to dynamically generate agent-specific parameters, decoupling observation- and agent-conditioned gradients to reduce interference and variance.

Result: HyperMARL achieves competitive performance across diverse MARL benchmarks (up to 20 agents), maintaining behavioral diversity comparable to non-parameter sharing while outperforming other methods.

Conclusion: HyperMARL is a versatile and effective approach for adaptive MARL, balancing performance and diversity without added complexity.

Abstract: Adaptability to specialised or homogeneous behaviours is critical in
cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS)
techniques, common for efficient adaptation, often limit behavioural diversity
due to cross-agent gradient interference, which we show can be exacerbated by
the coupling of observations and agent IDs. Current remedies typically add
complexity through altered objectives, manual preset diversity levels, or
sequential updates. We ask: can shared policies adapt without these
complexities? We propose HyperMARL, a PS approach using hypernetworks for
dynamic agent-specific parameters, without altering the RL objective or
requiring preset diversity levels. HyperMARL's explicit decoupling of
observation- and agent-conditioned gradients empirically reduces policy
gradient variance, facilitates shared-policy adaptation (including
specialisation), and helps mitigate cross-agent interference. Across diverse
MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or
mixed behaviours, HyperMARL achieves competitive performance against key
baselines -- fully shared, non-parameter sharing, and three diversity-promoting
methods -- while preserving behavioural diversity comparable to non-parameter
sharing. These findings establish HyperMARL as a versatile approach for
adaptive MARL. The code is publicly available at
https://github.com/KaleabTessera/HyperMARL.

</details>


### [538] [GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection](https://arxiv.org/pdf/2505.16017)
*Mariia Seleznova, Hung-Hsu Chou, Claudio Mayrink Verdun, Gitta Kutyniok*

Main category: cs.LG

TL;DR: GradPCA is a new OOD detection method using PCA on neural network gradients, outperforming existing methods with theoretical support from NTK alignment.


<details>
  <summary>Details</summary>
Motivation: To improve OOD detection by leveraging the low-rank structure of gradients and NTK alignment in neural networks.

Method: Applies PCA to gradient class-means and analyzes spectral properties for OOD detection.

Result: GradPCA shows consistent performance across benchmarks, with feature quality (pretrained vs. non-pretrained) being critical.

Conclusion: GradPCA is effective, and the theoretical framework aids in designing better spectral OOD detectors.

Abstract: We introduce GradPCA, an Out-of-Distribution (OOD) detection method that
exploits the low-rank structure of neural network gradients induced by Neural
Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis
(PCA) to gradient class-means, achieving more consistent performance than
existing methods across standard image classification benchmarks. We provide a
theoretical perspective on spectral OOD detection in neural networks to support
GradPCA, highlighting feature-space properties that enable effective detection
and naturally emerge from NTK alignment. Our analysis further reveals that
feature quality -- particularly the use of pretrained versus non-pretrained
representations -- plays a crucial role in determining which detectors will
succeed. Extensive experiments validate the strong performance of GradPCA, and
our theoretical framework offers guidance for designing more principled
spectral OOD detectors.

</details>


### [539] [Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging](https://arxiv.org/pdf/2505.16024)
*Weiguo Gao, Ming Li*

Main category: cs.LG

TL;DR: The paper explores diffusion trajectory distillation to speed up sampling in diffusion models by training a student model to mimic a teacher's multi-step denoising in one step. It frames distillation as an operator merging problem, proposes a dynamic programming solution for optimal merging, and identifies a phase transition in strategies based on data covariance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models produce high-quality outputs but suffer from slow sampling speeds. Existing distillation methods lack theoretical insights into trade-offs between strategies and generative quality, hindering optimization.

Method: Reinterprets trajectory distillation as a linear operator merging problem, where each teacher step is a linear operator. Proposes a dynamic programming algorithm to compute the optimal merging strategy, considering signal shrinkage from discretization and limited optimization.

Result: Identifies a sharp phase transition in optimal merging strategies, governed by data covariance structures. The dynamic programming approach maximizes signal fidelity.

Conclusion: The work advances theoretical understanding of diffusion trajectory distillation and provides practical insights for optimizing distillation strategies.

Abstract: Diffusion trajectory distillation methods aim to accelerate sampling in
diffusion models, which produce high-quality outputs but suffer from slow
sampling speeds. These methods train a student model to approximate the
multi-step denoising process of a pretrained teacher model in a single step,
enabling one-shot generation. However, theoretical insights into the trade-off
between different distillation strategies and generative quality remain
limited, complicating their optimization and selection. In this work, we take a
first step toward addressing this gap. Specifically, we reinterpret trajectory
distillation as an operator merging problem in the linear regime, where each
step of the teacher model is represented as a linear operator acting on noisy
data. These operators admit a clear geometric interpretation as projections and
rescalings corresponding to the noise schedule. During merging, signal
shrinkage occurs as a convex combination of operators, arising from both
discretization and limited optimization time of the student model. We propose a
dynamic programming algorithm to compute the optimal merging strategy that
maximally preserves signal fidelity. Additionally, we demonstrate the existence
of a sharp phase transition in the optimal strategy, governed by data
covariance structures. Our findings enhance the theoretical understanding of
diffusion trajectory distillation and offer practical insights for improving
distillation strategies.

</details>


### [540] [Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces](https://arxiv.org/pdf/2505.16035)
*Alejandro García-Castellanos, David R. Wessels, Nicky J. van den Berg, Remco Duits, Daniël M. Pelt, Erik J. Bekkers*

Main category: cs.LG

TL;DR: A novel framework combining Equivariant Neural Fields and Neural Eikonal Solvers for efficient, steerable, and geometrically grounded Eikonal solutions on Riemannian manifolds.


<details>
  <summary>Details</summary>
Motivation: To enhance representation efficiency, geometric grounding, and solution steerability in Eikonal travel-time modeling.

Method: Uses a single neural field with a shared backbone conditioned on latent point clouds in a Lie group, integrated with Physics-Informed Neural Networks.

Result: Superior performance, scalability, and adaptability in seismic travel-time modeling on 2D/3D datasets.

Conclusion: The framework generalizes well across Riemannian manifolds and outperforms existing Neural Operator-based methods.

Abstract: We introduce Equivariant Neural Eikonal Solvers, a novel framework that
integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our
approach employs a single neural field where a unified shared backbone is
conditioned on signal-specific latent variables - represented as point clouds
in a Lie group - to model diverse Eikonal solutions. The ENF integration
ensures equivariant mapping from these latent representations to the solution
field, delivering three key benefits: enhanced representation efficiency
through weight-sharing, robust geometric grounding, and solution steerability.
This steerability allows transformations applied to the latent point cloud to
induce predictable, geometrically meaningful modifications in the resulting
Eikonal solution. By coupling these steerable representations with
Physics-Informed Neural Networks (PINNs), our framework accurately models
Eikonal travel-time solutions while generalizing to arbitrary Riemannian
manifolds with regular group actions. This includes homogeneous spaces such as
Euclidean, position-orientation, spherical, and hyperbolic manifolds. We
validate our approach through applications in seismic travel-time modeling of
2D and 3D benchmark datasets. Experimental results demonstrate superior
performance, scalability, adaptability, and user controllability compared to
existing Neural Operator-based Eikonal solver methods.

</details>


### [541] [Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs](https://arxiv.org/pdf/2505.16053)
*Jan Tönshoff, Martin Grohe*

Main category: cs.LG

TL;DR: The paper introduces RLAF, a method using GNNs and reinforcement learning to improve SAT solver heuristics, achieving significant speedups and outperforming expert-supervised approaches.


<details>
  <summary>Details</summary>
Motivation: SAT solvers rely on hand-crafted heuristics, which may not be optimal. The work aims to automate and improve heuristic design using machine learning.

Method: Uses GNNs to infer variable weights and polarities in a single forward pass, trained via reinforcement learning (e.g., GRPO) using solver cost as the reward.

Result: RLAF-trained policies reduce mean solve times by over 2x in some cases and generalize well to harder problems.

Conclusion: RLAF offers a promising, data-driven approach to heuristic design in combinatorial optimization, outperforming traditional methods.

Abstract: Boolean Satisfiability (SAT) solvers are foundational to computer science,
yet their performance typically hinges on hand-crafted heuristics. This work
introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm
for learning to guide SAT solver branching heuristics with Graph Neural
Networks (GNNs). Central to our approach is a novel and generic mechanism for
injecting inferred variable weights and polarities into the branching
heuristics of existing SAT solvers. In a single forward pass, a GNN assigns
these parameters to all variables. Casting this one-shot guidance as a
reinforcement learning problem lets us train the GNN with off-the-shelf
policy-gradient methods, such as GRPO, directly using the solver's
computational cost as the sole reward signal. Extensive evaluations demonstrate
that RLAF-trained policies significantly reduce the mean solve times of
different base solvers across diverse SAT problem distributions, achieving more
than a 2x speedup in some cases, while generalizing effectively to larger and
harder problems after training. Notably, these policies consistently outperform
expert-supervised approaches based on learning handcrafted weighting
heuristics, offering a promising path towards data-driven heuristic design in
combinatorial optimization.

</details>


### [542] [Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models](https://arxiv.org/pdf/2505.16056)
*Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei*

Main category: cs.LG

TL;DR: The paper introduces metrics to measure local routing consistency in MoE models, analyzes 20 MoE LLMs, and provides insights for memory-efficient deployment.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the local routing consistency of MoE models for efficient deployment on memory-constrained devices.

Method: Proposes two metrics (SRP and SCH) to measure local routing consistency and analyzes 20 diverse MoE LLMs.

Result: Models with MoE on every layer and no shared experts show highest consistency; domain-specialized experts enhance consistency. Optimal cache size is ~2x active experts.

Conclusion: Findings enable memory-efficient MoE design without sacrificing inference speed. Code is publicly available for replication.

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with sparsely activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .

</details>


### [543] [Mesh-free sparse identification of nonlinear dynamics](https://arxiv.org/pdf/2505.16058)
*Mars Liyao Gao, J. Nathan Kutz, Bernat Font*

Main category: cs.LG

TL;DR: Mesh-free SINDy is a novel algorithm for identifying governing equations from arbitrary sensor data, robust to noise and limited data, with minimal hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require high-quality, uniformly sampled data, which is often impractical. Mesh-free SINDy addresses this limitation.

Method: Leverages neural networks and auto-differentiation to identify equations from non-uniform, noisy data.

Result: Effective on various PDEs, robust to high noise (up to 75%) and low data (100 samples), with training under one minute.

Conclusion: Mesh-free SINDy is widely applicable, efficient, and outperforms state-of-the-art methods in challenging scenarios.

Abstract: Identifying the governing equations of a dynamical system is one of the most
important tasks for scientific modeling. However, this procedure often requires
high-quality spatio-temporal data uniformly sampled on structured grids. In
this paper, we propose mesh-free SINDy, a novel algorithm which leverages the
power of neural network approximation as well as auto-differentiation to
identify governing equations from arbitrary sensor placements and non-uniform
temporal data sampling. We show that mesh-free SINDy is robust to high noise
levels and limited data while remaining computationally efficient. In our
implementation, the training procedure is straight-forward and nearly free of
hyperparameter tuning, making mesh-free SINDy widely applicable to many
scientific and engineering problems. In the experiments, we demonstrate its
effectiveness on a series of PDEs including the Burgers' equation, the heat
equation, the Korteweg-De Vries equation and the 2D advection-diffusion
equation. We conduct detailed numerical experiments on all datasets, varying
the noise levels and number of samples, and we also compare our approach to
previous state-of-the-art methods. It is noteworthy that, even in high-noise
and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,
achieving successful identification with up to 75% noise for the Burgers'
equation using 5,000 samples and with as few as 100 samples and 1% noise. All
of this is achieved within a training time of under one minute.

</details>


### [544] [Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond](https://arxiv.org/pdf/2505.16060)
*Shangding Gu, Donghao Ying, Ming Jin, Yu Joe Lu, Jun Wang, Javad Lavaei, Costas Spanos*

Main category: cs.LG

TL;DR: Model Feedback Learning (MFL) is a test-time optimization framework that optimizes inputs for pre-trained models or hardware systems without retraining, using a lightweight reverse model for efficient adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods require model parameter adjustments, which is impractical for deployed systems like semiconductor manufacturing. MFL addresses this by enabling input optimization without system modifications.

Method: MFL uses a reverse model to iteratively search for optimal inputs, incorporating stability-aware optimization for robustness.

Result: MFL outperforms Bayesian optimization and human experts in semiconductor plasma etching, achieving target recipes in five iterations. It also excels in chemical and electronic applications.

Conclusion: MFL offers a scalable, efficient solution for intelligent control in real-world settings, enabling few-shot adaptation without system changes.

Abstract: We introduce Model Feedback Learning (MFL), a novel test-time optimization
framework for optimizing inputs to pre-trained AI models or deployed hardware
systems without requiring any retraining of the models or modifications to the
hardware. In contrast to existing methods that rely on adjusting model
parameters, MFL leverages a lightweight reverse model to iteratively search for
optimal inputs, enabling efficient adaptation to new objectives under
deployment constraints. This framework is particularly advantageous in
real-world settings, such as semiconductor manufacturing recipe generation,
where modifying deployed systems is often infeasible or cost-prohibitive. We
validate MFL on semiconductor plasma etching tasks, where it achieves target
recipe generation in just five iterations, significantly outperforming both
Bayesian optimization and human experts. Beyond semiconductor applications, MFL
also demonstrates strong performance in chemical processes (e.g., chemical
vapor deposition) and electronic systems (e.g., wire bonding), highlighting its
broad applicability. Additionally, MFL incorporates stability-aware
optimization, enhancing robustness to process variations and surpassing
conventional supervised learning and random search methods in high-dimensional
control settings. By enabling few-shot adaptation, MFL provides a scalable and
efficient paradigm for deploying intelligent control in real-world
environments.

</details>


### [545] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/pdf/2505.16066)
*Zhixu Silvia Tao, Kasper Vinken, Hao-Wei Yeh, Avi Cooper, Xavier Boix*

Main category: cs.LG

TL;DR: Merge to Mix accelerates dataset mixture selection for fine-tuning LMs by using model merging as a surrogate for full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current methods for composing dataset mixtures rely on heuristics and trial-and-error, requiring multiple fine-tuning runs, which is inefficient.

Method: Proposes Merge to Mix, which uses model merging (combining individually fine-tuned LMs) to simulate the effect of fine-tuning on a dataset mixture.

Result: Merge to Mix outperforms state-of-the-art methods in dataset selection for fine-tuning LMs.

Conclusion: Merge to Mix offers an efficient alternative to traditional trial-and-error methods for dataset mixture selection.

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [546] [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/pdf/2502.15814)
*Gallil Maimon, Avishai Elmakies, Yossi Adi*

Main category: cs.LG

TL;DR: Slam is a method to train high-quality Speech Language Models (SLMs) efficiently on a single academic GPU in 24 hours, achieving results comparable to leading SLMs at lower compute costs.


<details>
  <summary>Details</summary>
Motivation: To make SLM training and research more accessible by reducing compute requirements while maintaining high performance.

Method: Empirical analysis of model initialization, architecture, synthetic training data, preference optimization, and component tweaking.

Result: Outperforms predicted compute optimal performance, scaling well with more compute and matching leading SLMs.

Conclusion: Slam offers an optimistic view of SLM feasibility, making training more accessible and efficient.

Abstract: We introduce Slam, a recipe for training high-quality Speech Language Models
(SLMs) on a single academic GPU in 24 hours. We do so through empirical
analysis of model initialisation and architecture, synthetic training data,
preference optimisation with synthetic data and tweaking all other components.
We empirically demonstrate that this training recipe also scales well with more
compute getting results on par with leading SLMs in a fraction of the compute
cost. We hope these insights will make SLM training and research more
accessible. In the context of SLM scaling laws, our results far outperform
predicted compute optimal performance, giving an optimistic view to SLM
feasibility. See code, data, models, samples at -
https://pages.cs.huji.ac.il/adiyoss-lab/slamming .

</details>


### [547] [Bidirectional Variational Autoencoders](https://arxiv.org/pdf/2505.16074)
*Bart Kosko, Olaoluwa Adigun*

Main category: cs.LG

TL;DR: Bidirectional VAE (BVAE) uses a single network for encoding and decoding, reducing parameters by 50% while outperforming traditional VAEs on image tasks.


<details>
  <summary>Details</summary>
Motivation: To simplify VAE architecture by using a single bidirectional network instead of separate encoder-decoder pairs, aiming for efficiency without performance loss.

Method: BVAE employs a single neural network for both encoding (forward pass) and decoding (backward pass), tested on MNIST, Fashion-MNIST, CIFAR-10, and CelebA-64 datasets for tasks like reconstruction, classification, interpolation, and generation.

Result: BVAE reduced parameters by nearly 50% and slightly outperformed traditional VAEs in performance.

Conclusion: BVAE offers a more efficient and effective alternative to traditional VAEs for image-related tasks.

Abstract: We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.

</details>


### [548] [Ensembling Sparse Autoencoders](https://arxiv.org/pdf/2505.16077)
*Soham Gadgil, Chris Lin, Su-In Lee*

Main category: cs.LG

TL;DR: Ensembling sparse autoencoders (SAEs) improves feature diversity, reconstruction, and downstream task performance compared to single SAEs.


<details>
  <summary>Details</summary>
Motivation: Single SAEs capture only a subset of features from neural network activations, motivating the use of ensembles to enhance feature extraction.

Method: Proposes ensembling SAEs via naive bagging (different initial weights) and boosting (sequential training to minimize residual error).

Result: Ensembling improves reconstruction, feature diversity, stability, and outperforms single SAEs in tasks like concept detection and spurious correlation removal.

Conclusion: Ensembling SAEs enhances practical utility and feature extraction capabilities.

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into human-interpretable features. Typically, features learned by a single SAE
are used for downstream applications. However, it has recently been shown that
SAEs trained with different initial weights can learn different features,
demonstrating that a single SAE captures only a limited subset of features that
can be extracted from the activation space. Motivated by this limitation, we
propose to ensemble multiple SAEs through naive bagging and boosting.
Specifically, SAEs trained with different weight initializations are ensembled
in naive bagging, whereas SAEs sequentially trained to minimize the residual
error are ensembled in boosting. We evaluate our ensemble approaches with three
settings of language models and SAE architectures. Our empirical results
demonstrate that ensembling SAEs can improve the reconstruction of language
model activations, diversity of features, and SAE stability. Furthermore,
ensembling SAEs performs better than applying a single SAE on downstream tasks
such as concept detection and spurious correlation removal, showing improved
practical utility.

</details>


### [549] [FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model](https://arxiv.org/pdf/2505.16083)
*Jiahuan Long, Wenzhe Zhang, Ning Wang, Tingsong Jiang, Wen Yao*

Main category: cs.LG

TL;DR: FR-Mamba, a hybrid neural network combining FNO and SSM, improves physical field reconstruction by capturing long-range temporal and spatial dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with long-range temporal dependencies in time-evolving physical systems, limiting PFR performance.

Method: FR-Mamba integrates Fourier Neural Operator (FNO) for spatial features and State Space Model (SSM, specifically Mamba) for temporal dependencies, fusing their outputs for reconstruction.

Result: FR-Mamba outperforms existing methods, achieving high accuracy in flow field reconstruction, especially on long sequences.

Conclusion: The proposed hybrid architecture effectively addresses PFR challenges, offering superior performance in spatiotemporal physical field reconstruction.

Abstract: Physical field reconstruction (PFR) aims to predict the state distribution of
physical quantities (e.g., velocity, pressure, and temperature) based on
limited sensor measurements. It plays a critical role in domains such as fluid
dynamics and thermodynamics. However, existing deep learning methods often fail
to capture long-range temporal dependencies, resulting in suboptimal
performance on time-evolving physical systems. To address this, we propose
FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on
state space modeling. Specifically, we design a hybrid neural network
architecture that combines Fourier Neural Operator (FNO) and State Space Model
(SSM) to capture both global spatial features and long-range temporal
dependencies. We adopt Mamba, a recently proposed efficient SSM architecture,
to model long-range temporal dependencies with linear time complexity. In
parallel, the FNO is employed to capture non-local spatial features by
leveraging frequency-domain transformations. The spatiotemporal representations
extracted by these two components are then fused to reconstruct the full-field
distribution of the physical system. Extensive experiments demonstrate that our
approach significantly outperforms existing PFR methods in flow field
reconstruction tasks, achieving high-accuracy performance on long sequences.

</details>


### [550] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/pdf/2505.16094)
*Ziqing Wang, Kexin Zhang, Zihan Zhao, Yibo Wen, Abhishek Pandey, Han Liu, Kaize Ding*

Main category: cs.LG

TL;DR: A survey on the use of large language models (LLMs) for molecular discovery, focusing on molecule generation and optimization, with a taxonomy, analysis of techniques, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To review and advance the emerging field of LLMs in molecular discovery by providing a structured overview and identifying key challenges.

Method: Proposes a taxonomy for molecule generation and optimization, analyzes representative techniques, and includes datasets and evaluation protocols.

Result: Highlights how LLMs are leveraged in molecular discovery tasks and provides a resource for researchers.

Conclusion: Identifies key challenges and future directions, positioning the survey as a valuable resource for the intersection of LLMs and molecular science.

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [551] [Reinforcement Learning for Stock Transactions](https://arxiv.org/pdf/2505.16099)
*Ziyi, Zhou, Nicholas Stern, Julien Laasri*

Main category: cs.LG

TL;DR: The paper applies reinforcement learning (RL) to determine optimal stock buying times, using Q-Learning variants and comparing their performance.


<details>
  <summary>Details</summary>
Motivation: To identify patterns in stock market transactions for profitable trading by leveraging RL techniques.

Method: Defines a custom Markov Decision Process (MDP) problem, trains agents using Q-Learning, linear function approximation, and deep Q-Learning, and compares their convergence and profitability.

Result: Agents are evaluated for policy convergence and profit maximization, with results compared across methods.

Conclusion: The study demonstrates the feasibility of RL in stock market analysis, highlighting the best-performing method for profit maximization.

Abstract: Much research has been done to analyze the stock market. After all, if one
can determine a pattern in the chaotic frenzy of transactions, then they could
make a hefty profit from capitalizing on these insights. As such, the goal of
our project was to apply reinforcement learning (RL) to determine the best time
to buy a stock within a given time frame. With only a few adjustments, our
model can be extended to identify the best time to sell a stock as well. In
order to use the format of free, real-world data to train the model, we define
our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped
us in formulating the state space and the reward system of our MDP problem. We
train a series of agents using Q-Learning, Q-Learning with linear function
approximation, and deep Q-Learning. In addition, we try to predict the stock
prices using machine learning regression and classification models. We then
compare our agents to see if they converge on a policy, and if so, which one
learned the best policy to maximize profit on the stock market.

</details>


### [552] [Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI](https://arxiv.org/pdf/2505.16103)
*Monirul Islam Mahmud*

Main category: cs.LG

TL;DR: The paper analyzes keylogger detection using traditional and ensemble machine learning models, feature selection methods, and Explainable AI (XAI) techniques, achieving near-perfect performance with AdaBoost.


<details>
  <summary>Details</summary>
Motivation: To comprehensively analyze and improve keylogger detection by evaluating various machine learning models, feature selection methods, and incorporating XAI for interpretability.

Method: Used traditional models (SVC, Random Forest, etc.), ensemble methods (Stacking, Blending, Voting), and feature selection (Information gain, Lasso L1, Fisher Score). Applied XAI techniques (SHAP, LIME) for model interpretation. Evaluated using AUC, sensitivity, specificity, accuracy, and F1 score.

Result: AdaBoost achieved the best performance: 99.76% accuracy, F1 score of 0.99, 100% precision, 98.6% recall, 1.0 specificity, and 0.99 AUC with Fisher Score.

Conclusion: The study demonstrates the effectiveness of AdaBoost and feature selection (Fisher Score) for keylogger detection, with XAI providing valuable insights into feature contributions.

Abstract: Keylogger detection involves monitoring for unusual system behaviors such as
delays between typing and character display, analyzing network traffic patterns
for data exfiltration. In this study, we provide a comprehensive analysis for
keylogger detection with traditional machine learning models - SVC, Random
Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes
and advanced ensemble methods including Stacking, Blending and Voting.
Moreover, feature selection approaches such as Information gain, Lasso L1 and
Fisher Score are thoroughly assessed to improve predictive performance and
lower computational complexity. The Keylogger Detection dataset from publicly
available Kaggle website is used in this project. In addition to accuracy-based
classification, this study implements the approach for model interpretation
using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to
deliver finer explanations for how much each feature contributes in assisting
or hindering the detection process. To evaluate the models result, we have used
AUC score, sensitivity, Specificity, Accuracy and F1 score. The best
performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,
100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is
near-perfect classification with Fisher Score.

</details>


### [553] [Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools](https://arxiv.org/pdf/2505.16113)
*Panagiotis Lymperopoulos, Vasanth Sarathy*

Main category: cs.LG

TL;DR: A framework for quantifying uncertainty in LLMs using external tools, enhancing trust in high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of assessing trustworthiness in LLM-tool systems, crucial for high-stakes domains like medicine.

Method: Extends uncertainty quantification to tool-calling LLMs, proposing efficient approximations for practical use.

Result: Effective in synthetic QA datasets and RAG systems, improving trust when external tools are needed.

Conclusion: The framework enhances reliability in LLM-based systems by jointly modeling LLM and tool uncertainty.

Abstract: Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.

</details>


### [554] [A Generic Framework for Conformal Fairness](https://arxiv.org/pdf/2505.16115)
*Aditya T. Vadlamani, Anutam Srinivasan, Pranav Maneriker, Ali Payani, Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: The paper introduces Conformal Fairness, a fairness-aware extension of Conformal Prediction (CP), ensuring balanced coverage across sensitive groups without relying on IID assumptions.


<details>
  <summary>Details</summary>
Motivation: Current CP methods lack fairness guarantees across sensitive attributes, limiting their applicability in diverse datasets.

Method: Proposes a framework leveraging CP's exchangeability assumption to control coverage gaps between sensitive groups, applicable to non-IID data like graphs.

Result: Experiments on graph and tabular datasets confirm the framework's ability to balance coverage and fairness gaps as theoretically expected.

Conclusion: Conformal Fairness extends CP's utility to fairness-critical applications, even for non-IID data.

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
with machine learning models. While conformal prediction provides probabilistic
guarantees regarding the coverage of the true label, these guarantees are
agnostic to the presence of sensitive attributes within the dataset. In this
work, we formalize \textit{Conformal Fairness}, a notion of fairness using
conformal predictors, and provide a theoretically well-founded algorithm and
associated framework to control for the gaps in coverage between different
sensitive groups. Our framework leverages the exchangeability assumption
(implicit to CP) rather than the typical IID assumption, allowing us to apply
the notion of Conformal Fairness to data types and tasks that are not IID, such
as graph data. Experiments were conducted on graph and tabular datasets to
demonstrate that the algorithm can control fairness-related gaps in addition to
coverage aligned with theoretical expectations.

</details>


### [555] [Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning](https://arxiv.org/pdf/2505.16122)
*Junhong Lin, Xinyue Zeng, Jie Zhu, Song Wang, Julian Shun, Jun Wu, Dawei Zhou*

Main category: cs.LG

TL;DR: The paper addresses computational inefficiency in LLMs due to overthinking, proposing Plan-and-Budget, a framework that improves reasoning efficiency by decomposing queries and allocating token budgets adaptively.


<details>
  <summary>Details</summary>
Motivation: LLMs often overthink, generating verbose reasoning for simple queries, leading to inefficiency. Fixed token budgets can cause underthinking on harder problems.

Method: Develops BBAM, a theoretical model for reasoning as sub-questions with uncertainty, and introduces the $E^3$ metric. Proposes Plan-and-Budget, a framework for adaptive token allocation.

Result: Plan-and-Budget improves efficiency: +70% accuracy, -39% tokens, +187.5% $E^3$ gain. Smaller models match larger ones' efficiency.

Conclusion: Plan-and-Budget effectively balances correctness and efficiency, closing performance gaps without retraining.

Abstract: Large Language Models (LLMs) have achieved remarkable success in complex
reasoning tasks, but their inference remains computationally inefficient. We
observe a common failure mode in many prevalent LLMs, overthinking, where
models generate verbose and tangential reasoning traces even for simple
queries. Recent works have tried to mitigate this by enforcing fixed token
budgets, however, this can lead to underthinking, especially on harder
problems. Through empirical analysis, we identify that this inefficiency often
stems from unclear problem-solving strategies. To formalize this, we develop a
theoretical model, BBAM (Bayesian Budget Allocation Model), which models
reasoning as a sequence of sub-questions with varying uncertainty, and
introduce the $E^3$ metric to capture the trade-off between correctness and
computation efficiency. Building on theoretical results from BBAM, we propose
Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex
queries into sub-questions and allocates token budgets based on estimated
complexity using adaptive scheduling. Plan-and-Budget improves reasoning
efficiency across a range of tasks and models, achieving up to +70% accuracy
gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it
elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger
model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close
performance gaps without retraining. Our code is available at
anonymous.4open.science/r/P-and-B-6513/.

</details>


### [556] [Robust Invariant Representation Learning by Distribution Extrapolation](https://arxiv.org/pdf/2505.16126)
*Kotaro Yoshida, Slavakis Konstantinos*

Main category: cs.LG

TL;DR: The paper proposes an extrapolation-based framework to improve IRM by addressing its sensitivity to limited environment diversity and over-parameterization, outperforming existing IRM variants.


<details>
  <summary>Details</summary>
Motivation: Existing IRM methods, like IRMv1, often fail to outperform ERM due to sensitivity to limited environment diversity and over-parameterization.

Method: A novel extrapolation-based framework is introduced, enhancing IRM by augmenting its penalty with synthetic distributional shifts.

Result: The proposed method consistently outperforms state-of-the-art IRM variants in experiments, including synthetic and over-parameterized scenarios.

Conclusion: The framework effectively addresses IRM's limitations, offering improved robustness and performance for OOD generalization.

Abstract: Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.

</details>


### [557] [Scalable Graph Generative Modeling via Substructure Sequences](https://arxiv.org/pdf/2505.16130)
*Zehong Wang, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye*

Main category: cs.LG

TL;DR: The paper introduces G$^2$PM, a generative Transformer framework for graphs, addressing limitations of message-passing GNNs like expressiveness and scalability. It outperforms baselines and scales well with model size.


<details>
  <summary>Details</summary>
Motivation: Message-passing GNNs face issues like constrained expressiveness and scalability, limiting their use as graph foundation models.

Method: G$^2$PM represents graphs as sequences of substructures and uses generative pre-training to learn transferable representations.

Result: G$^2$PM scales effectively (up to 60M parameters) and outperforms prior methods on tasks like node and graph classification.

Conclusion: G$^2$PM provides a scalable and generalizable foundation for graph learning, surpassing traditional GNNs.

Abstract: Graph neural networks (GNNs) has been predominantly driven by
message-passing, where node representations are iteratively updated via local
neighborhood aggregation. Despite their success, message-passing suffers from
fundamental limitations -- including constrained expressiveness,
over-smoothing, over-squashing, and limited capacity to model long-range
dependencies. These issues hinder scalability: increasing data size or model
size often fails to yield improved performance, limiting the viability of GNNs
as backbones for graph foundation models. In this work, we explore pathways
beyond message-passing and introduce Generative Graph Pattern Machine
(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM
represents graph instances (nodes, edges, or entire graphs) as sequences of
substructures, and employs generative pre-training over the sequences to learn
generalizable, transferable representations. Empirically, G$^2$PM demonstrates
strong scalability: on the ogbn-arxiv benchmark, it continues to improve with
model sizes up to 60M parameters, outperforming prior generative approaches
that plateau at significantly smaller scales (e.g., 3M). In addition, we
systematically analyze the model design space, highlighting key architectural
choices that contribute to its scalability and generalization. Across diverse
tasks -- including node classification, graph classification, and transfer
learning -- G$^2$PM consistently outperforms strong baselines, establishing a
compelling foundation for scalable graph learning. The code and dataset are
available at https://github.com/Zehong-Wang/G2PM.

</details>


### [558] [Multimodal Online Federated Learning with Modality Missing in Internet of Things](https://arxiv.org/pdf/2505.16138)
*Heqiang Wang, Xiang Liu, Xiaoxiong Zhong, Lixing Chen, Fangming Liu, Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper proposes Multimodal Online Federated Learning (MMO-FL) for IoT, addressing missing modalities with the Prototypical Modality Mitigation (PMM) algorithm, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Handling multimodal data in IoT environments with distributed learning and addressing missing modalities due to device instability.

Method: Introduces MMO-FL for decentralized learning and PMM algorithm to compensate for missing modalities.

Result: PMM outperforms benchmarks on two datasets, mitigating performance degradation from missing modalities.

Conclusion: MMO-FL and PMM effectively address challenges of multimodal learning in IoT, enhancing performance despite missing data.

Abstract: The Internet of Things (IoT) ecosystem generates vast amounts of multimodal
data from heterogeneous sources such as sensors, cameras, and microphones. As
edge intelligence continues to evolve, IoT devices have progressed from simple
data collection units to nodes capable of executing complex computational
tasks. This evolution necessitates the adoption of distributed learning
strategies to effectively handle multimodal data in an IoT environment.
Furthermore, the real-time nature of data collection and limited local storage
on edge devices in IoT call for an online learning paradigm. To address these
challenges, we introduce the concept of Multimodal Online Federated Learning
(MMO-FL), a novel framework designed for dynamic and decentralized multimodal
learning in IoT environments. Building on this framework, we further account
for the inherent instability of edge devices, which frequently results in
missing modalities during the learning process. We conduct a comprehensive
theoretical analysis under both complete and missing modality scenarios,
providing insights into the performance degradation caused by missing
modalities. To mitigate the impact of modality missing, we propose the
Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype
learning to effectively compensate for missing modalities. Experimental results
on two multimodal datasets further demonstrate the superior performance of PMM
compared to benchmarks.

</details>


### [559] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/pdf/2505.16148)
*Chongjie Si, Kangtao Lv, Jingjing Jiang, Yadao Wang, Yongwei Wang, Xiaokang Yang, Wenbo Su, Bo Zheng, Wei Shen*

Main category: cs.LG

TL;DR: NAN is a training-free, plug-and-play method for model merging, using inverse parameter norms to determine optimal weights, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods rely on heuristics, limiting scalability and generality.

Method: Proposes NAN, which estimates merging coefficients via the inverse of parameter norm, based on least-squares optimization.

Result: NAN consistently improves performance of baseline methods in experiments.

Conclusion: NAN is a scalable and general solution for model merging, leveraging task-specific information effectively.

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [560] [Why Can Accurate Models Be Learned from Inaccurate Annotations?](https://arxiv.org/pdf/2505.16159)
*Chongjie Si, Yidan Cui, Fuchao Yang, Xiaokang Yang, Wei Shen*

Main category: cs.LG

TL;DR: The paper investigates why models trained on noisy labels can still generalize well, finding that label inaccuracy mainly affects lower singular components, leaving the principal subspace largely intact. A lightweight plug-in, LIP, is proposed to enhance performance by preserving this subspace.


<details>
  <summary>Details</summary>
Motivation: Understanding why models can generalize effectively despite noisy labels, as precise labeling is costly and noisy data is common.

Method: Analyzes weight matrices empirically and theoretically, focusing on singular components and principal subspaces. Proposes LIP to retain principal subspace information.

Result: Label inaccuracy primarily perturbs lower singular components, leaving the principal subspace aligned with clean labels. LIP improves performance under noisy conditions.

Conclusion: Models generalize well under noisy labels due to preserved principal subspaces. LIP effectively mitigates noise, offering practical and theoretical insights.

Abstract: Learning from inaccurate annotations has gained significant attention due to
the high cost of precise labeling. However, despite the presence of erroneous
labels, models trained on noisy data often retain the ability to make accurate
predictions. This intriguing phenomenon raises a fundamental yet largely
unexplored question: why models can still extract correct label information
from inaccurate annotations remains unexplored. In this paper, we conduct a
comprehensive investigation into this issue. By analyzing weight matrices from
both empirical and theoretical perspectives, we find that label inaccuracy
primarily accumulates noise in lower singular components and subtly perturbs
the principal subspace. Within a certain range, the principal subspaces of
weights trained on inaccurate labels remain largely aligned with those learned
from clean labels, preserving essential task-relevant information. We formally
prove that the angles of principal subspaces exhibit minimal deviation under
moderate label inaccuracy, explaining why models can still generalize
effectively. Building on these insights, we propose LIP, a lightweight plug-in
designed to help classifiers retain principal subspace information while
mitigating noise induced by label inaccuracy. Extensive experiments on tasks
with various inaccuracy conditions demonstrate that LIP consistently enhances
the performance of existing algorithms. We hope our findings can offer valuable
theoretical and practical insights to understand of model robustness under
inaccurate supervision.

</details>


### [561] [Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare](https://arxiv.org/pdf/2505.16190)
*Navid Seidi, Satyaki Roy, Sajal Das*

Main category: cs.LG

TL;DR: A robust, peer-driven reputation mechanism for federated healthcare is proposed, addressing data heterogeneity and unreliable contributions while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like data heterogeneity and unreliable contributions in federated learning for healthcare, while maintaining patient data privacy.

Method: A hybrid communication model integrates decentralized peer feedback with clustering-based noise handling, using differential privacy for client-side updates.

Result: The method achieves high and stable C-index values, outperforming FL methods without a reputation system.

Conclusion: The proposed framework effectively balances privacy, reputation, and performance in federated healthcare.

Abstract: Federated Learning (FL) holds great promise for digital health by enabling
collaborative model training without compromising patient data privacy.
However, heterogeneity across institutions, lack of sustained reputation, and
unreliable contributions remain major challenges. In this paper, we propose a
robust, peer-driven reputation mechanism for federated healthcare that employs
a hybrid communication model to integrate decentralized peer feedback with
clustering-based noise handling to enhance model aggregation. Crucially, our
approach decouples the federated aggregation and reputation mechanisms by
applying differential privacy to client-side model updates before sharing them
for peer evaluation. This ensures sensitive information remains protected
during reputation computation, while unaltered updates are sent to the server
for global model training. Using the Cox Proportional Hazards model for
survival analysis across multiple federated nodes, our framework addresses both
data heterogeneity and reputation deficit by dynamically adjusting trust scores
based on local performance improvements measured via the concordance index.
Experimental evaluations on both synthetic datasets and the SEER dataset
demonstrate that our method consistently achieves high and stable C-index
values, effectively down-weighing noisy client updates and outperforming FL
methods that lack a reputation system.

</details>


### [562] [Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks](https://arxiv.org/pdf/2505.16204)
*Ichiro Hashimoto*

Main category: cs.LG

TL;DR: The paper proves directional convergence of network parameters in fixed-width leaky ReLU two-layer neural networks under gradient descent with exponential loss, extending prior gradient flow results. It identifies conditions for benign overfitting and a new test error phase transition, applicable beyond nearly orthogonal data. Applications include sub-Gaussian mixture models.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of directional convergence in neural networks beyond gradient flow, and to explore conditions for benign overfitting and test error behavior in broader data settings.

Method: Analysis of convergent direction in fixed-width leaky ReLU two-layer networks optimized by gradient descent with exponential loss.

Result: Established sufficient conditions for benign overfitting and discovered a new phase transition in test error bounds, applicable beyond nearly orthogonal data. Demonstrated benign overfitting in sub-Gaussian mixture models.

Conclusion: The work generalizes prior results, providing insights into convergence and overfitting in neural networks under gradient descent, with practical implications for sub-Gaussian data.

Abstract: In this paper, we prove directional convergence of network parameters of
fixed width leaky ReLU two-layer neural networks optimized by gradient descent
with exponential loss, which was previously only known for gradient flow. By a
careful analysis of the convergent direction, we establish sufficient
conditions of benign overfitting and discover a new phase transition in the
test error bound. All of these results hold beyond the nearly orthogonal data
setting which was studied in prior works. As an application, we demonstrate
that benign overfitting occurs with high probability in sub-Gaussian mixture
models.

</details>


### [563] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/pdf/2505.16210)
*Zhihang Cai, Xingjun Zhang, Zhendong Tan, Zheng Wei*

Main category: cs.LG

TL;DR: NQKV algorithm quantizes the KV cache to lower bits using per-block quantile quantization, enabling larger batch sizes and longer context lengths without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the memory bottleneck in LLM deployment caused by large KV cache consumption during inference.

Method: Analyzed KV cache element distribution and designed NQKV, a per-block quantile quantization method.

Result: Enables 2x larger batch size, 4x longer context length, and 9.3x throughput improvement for the OPT model.

Conclusion: NQKV effectively reduces KV cache memory usage while maintaining model performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [564] [Reward-Aware Proto-Representations in Reinforcement Learning](https://arxiv.org/pdf/2505.16217)
*Hon Tik Tse, Siddarth Chandrasekar, Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper introduces the Default Representation (DR), a reward-aware alternative to the Successor Representation (SR) in RL, providing theoretical foundations and empirical benefits.


<details>
  <summary>Details</summary>
Motivation: The SR lacks reward awareness, limiting its effectiveness in RL tasks. The DR addresses this by incorporating reward dynamics.

Method: Theoretical analysis includes dynamic programming, temporal-difference methods, and extending DR to function approximation. Empirical tests cover reward shaping, option discovery, exploration, and transfer learning.

Result: DR outperforms SR in reward-aware behavior and performance across multiple RL settings.

Conclusion: DR is a promising, reward-aware representation with theoretical and empirical advantages over SR.

Abstract: In recent years, the successor representation (SR) has attracted increasing
attention in reinforcement learning (RL), and it has been used to address some
of its key challenges, such as exploration, credit assignment, and
generalization. The SR can be seen as representing the underlying credit
assignment structure of the environment by implicitly encoding its induced
transition dynamics. However, the SR is reward-agnostic. In this paper, we
discuss a similar representation that also takes into account the reward
dynamics of the problem. We study the default representation (DR), a recently
proposed representation with limited theoretical (and empirical) analysis.
Here, we lay some of the theoretical foundation underlying the DR in the
tabular case by (1) deriving dynamic programming and (2) temporal-difference
methods to learn the DR, (3) characterizing the basis for the vector space of
the DR, and (4) formally extending the DR to the function approximation case
through default features. Empirically, we analyze the benefits of the DR in
many of the settings in which the SR has been applied, including (1) reward
shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our
results show that, compared to the SR, the DR gives rise to qualitatively
different, reward-aware behaviour and quantitatively better performance in
several settings.

</details>


### [565] [Realistic Evaluation of TabPFN v2 in Open Environments](https://arxiv.org/pdf/2505.16226)
*Zi-Jian Cheng, Zi-Yi Jia, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo*

Main category: cs.LG

TL;DR: TabPFN v2 excels in closed environments but struggles in open ones, where tree-based models remain superior.


<details>
  <summary>Details</summary>
Motivation: To evaluate TabPFN v2's adaptability in open environments, addressing gaps in prior research.

Method: A unified evaluation framework was created to test TabPFN v2's robustness against real-world open-environment challenges.

Result: TabPFN v2 performs poorly in open environments but works well for small-scale, covariate-shifted, and class-balanced tasks.

Conclusion: Tree-based models are better for open environments; future research should focus on benchmarks and robustness improvements.

Abstract: Tabular data, owing to its ubiquitous presence in real-world domains, has
garnered significant attention in machine learning research. While tree-based
models have long dominated tabular machine learning tasks, the recently
proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled
performance and scalability potential. Although extensive research has been
conducted on TabPFN v2 to further improve performance, the majority of this
research remains confined to closed environments, neglecting the challenges
that frequently arise in open environments. This raises the question: Can
TabPFN v2 maintain good performance in open environments? To this end, we
conduct the first comprehensive evaluation of TabPFN v2's adaptability in open
environments. We construct a unified evaluation framework covering various
real-world challenges and assess the robustness of TabPFN v2 under open
environments scenarios using this framework. Empirical results demonstrate that
TabPFN v2 shows significant limitations in open environments but is suitable
for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models
remain the optimal choice for general tabular tasks in open environments. To
facilitate future research on open environments challenges, we advocate for
open environments tabular benchmarks, multi-metric evaluation, and universal
modules to strengthen model robustness. We publicly release our evaluation
framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.

</details>


### [566] [Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies](https://arxiv.org/pdf/2505.16242)
*Runze Yan, Xun Shen, Akifumi Wachi, Sebastien Gros, Anni Zhao, Xiao Hu*

Main category: cs.LG

TL;DR: The paper proposes Offline Guarded Safe Reinforcement Learning (OGSRL) to address out-of-distribution (OOD) issues in offline RL for healthcare, ensuring safe and reliable policy improvements beyond clinician actions.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CQL inadequately address OOD issues by only constraining actions, limiting long-term treatment strategy discovery. Healthcare requires safe exploration beyond clinician recommendations.

Method: OGSRL introduces dual constraints: an OOD guardian for clinically validated safe regions and a safety cost constraint encoding medical knowledge, ensuring safe policy optimization.

Result: OGSRL guarantees policies remain in safe regions and achieve near-optimal performance, outperforming clinician behavior without unsafe deviations.

Conclusion: OGSRL provides a theoretically grounded, safe, and effective framework for offline RL in healthcare, addressing OOD challenges and enabling reliable policy improvements.

Abstract: When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.

</details>


### [567] [Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems](https://arxiv.org/pdf/2505.16248)
*Wenxuan Zhu, Qiyuan Wu, Tengda Tang, Renzi Meng, Sheng Chai, Xuehui Quan*

Main category: cs.LG

TL;DR: A GNN-based multi-node collaborative perception mechanism improves distributed system performance by enhancing perception and scheduling efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in multi-node perception and delayed scheduling in distributed systems.

Method: Proposes a GNN-based mechanism with message-passing, state-update modules, and multi-layer GNN for dynamic state inference and information aggregation.

Result: Outperforms mainstream algorithms in task completion, latency, load balancing, and transmission efficiency under dynamic conditions.

Conclusion: The method enhances system perception and cooperative scheduling, achieving rapid convergence and efficient responses.

Abstract: This paper addresses the limitations of multi-node perception and delayed
scheduling response in distributed systems by proposing a GNN-based multi-node
collaborative perception mechanism. The system is modeled as a graph structure.
Message-passing and state-update modules are introduced. A multi-layer graph
neural network is constructed to enable efficient information aggregation and
dynamic state inference among nodes. In addition, a perception representation
method is designed by fusing local states with global features. This improves
each node's ability to perceive the overall system status. The proposed method
is evaluated within a customized experimental framework. A dataset featuring
heterogeneous task loads and dynamic communication topologies is used.
Performance is measured in terms of task completion rate, average latency, load
balancing, and transmission efficiency. Experimental results show that the
proposed method outperforms mainstream algorithms under various conditions,
including limited bandwidth and dynamic structural changes. It demonstrates
superior perception capabilities and cooperative scheduling performance. The
model achieves rapid convergence and efficient responses to complex system
states.

</details>


### [568] [Small-to-Large Generalization: Data Influences Models Consistently Across Scale](https://arxiv.org/pdf/2505.16260)
*Alaa Khaddaj, Logan Engstrom, Aleksander Madry*

Main category: cs.LG

TL;DR: Small- and large-scale language models show high correlation in predictions across training data distributions, validating the use of proxy models for data attribution and dataset selection.


<details>
  <summary>Details</summary>
Motivation: Understanding how training data distribution affects model behavior across different compute scales, given the impracticality of training large models repeatedly.

Method: Analyzing correlations between small- and large-scale language model predictions under varying training data distributions.

Result: High correlation between small and large model predictions, supporting proxy model use.

Conclusion: Proxy models are effective for tasks like data attribution and dataset selection, despite scale differences.

Abstract: Choice of training data distribution greatly influences model behavior. Yet,
in large-scale settings, precisely characterizing how changes in training data
affects predictions is often difficult due to model training costs. Current
practice is to instead extrapolate from scaled down, inexpensive-to-train proxy
models. However, changes in data do not influence smaller and larger models
identically. Therefore, understanding how choice of data affects large-scale
models raises the question: how does training data distribution influence model
behavior across compute scale? We find that small- and large-scale language
model predictions (generally) do highly correlate across choice of training
data. Equipped with these findings, we characterize how proxy scale affects
effectiveness in two downstream proxy model applications: data attribution and
dataset selection.

</details>


### [569] [Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models](https://arxiv.org/pdf/2505.16265)
*Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, Chao Zhang, Tuo Zhao*

Main category: cs.LG

TL;DR: Think-RM improves reinforcement learning from human feedback (RLHF) by enabling long-horizon reasoning in generative reward models (GenRMs), outperforming traditional methods by 8%.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of conventional Bradley-Terry reward models (BT RMs) and shallow GenRMs in RLHF, which struggle with data sensitivity, reward hacking, and complex tasks.

Method: Introduces Think-RM, a framework for self-guided reasoning in GenRMs, trained via supervised fine-tuning and rule-based reinforcement learning, and a pairwise RLHF pipeline.

Result: Think-RM achieves state-of-the-art results on RM-Bench, outperforming BT RM and vertically scaled GenRM by 8%.

Conclusion: Think-RM and the pairwise RLHF pipeline offer a robust solution for aligning language models with human preferences, enhancing performance and reasoning capabilities.

Abstract: Reinforcement learning from human feedback (RLHF) has become a powerful
post-training paradigm for aligning large language models with human
preferences. A core challenge in RLHF is constructing accurate reward signals,
where the conventional Bradley-Terry reward models (BT RMs) often suffer from
sensitivity to data size and coverage, as well as vulnerability to reward
hacking. Generative reward models (GenRMs) offer a more robust alternative by
generating chain-of-thought (CoT) rationales followed by a final reward.
However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting
their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.
Moreover, their pairwise preference outputs are incompatible with standard RLHF
algorithms that require pointwise reward signals. In this work, we introduce
Think-RM, a training framework that enables long-horizon reasoning in GenRMs by
modeling an internal thinking process. Rather than producing structured,
externally provided rationales, Think-RM generates flexible, self-guided
reasoning traces that support advanced capabilities such as self-reflection,
hypothetical reasoning, and divergent reasoning. To elicit these reasoning
abilities, we first warm-up the models by supervised fine-tuning (SFT) over
long CoT data. We then further improve the model's long-horizon abilities by
rule-based reinforcement learning (RL). In addition, we propose a novel
pairwise RLHF pipeline that directly optimizes policies using pairwise
preference rewards, eliminating the need for pointwise reward conversion and
enabling more effective use of Think-RM outputs. Experiments show that Think-RM
achieves state-of-the-art results on RM-Bench, outperforming both BT RM and
vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,
it demonstrates superior end-policy performance compared to traditional
approaches.

</details>


### [570] [Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse](https://arxiv.org/pdf/2505.16284)
*Josh Alman, Zhao Song*

Main category: cs.LG

TL;DR: Quadratic time for attention mechanisms in LLMs is unavoidable for expressive transformers, as small weights lead to layer collapse, a representational weakness.


<details>
  <summary>Details</summary>
Motivation: To investigate the necessity of quadratic time in attention mechanisms and the role of large weights in preventing layer collapse, a representational weakness.

Method: Analyzing the impact of weight size on layer collapse, comparing with prior work on rank collapse, and demonstrating the inevitability of quadratic time for expressive transformers.

Result: Large weights are necessary to avoid layer collapse; skip connections alone cannot prevent it, making quadratic time unavoidable for expressive attention mechanisms.

Conclusion: Quadratic running time in attention mechanisms is essential for expressive transformers, as small weights lead to layer collapse, and skip connections are insufficient to mitigate this.

Abstract: Attention mechanisms lie at the heart of modern large language models (LLMs).
Straightforward algorithms for forward and backward (gradient) computation take
quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]
and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary
unless the model weights are small, in which case almost linear time algorithms
are possible. In this paper, we show that large weights are necessary to avoid
a strong preclusion to representational strength we call layer collapse, which
means that the entire network can be approximated well by a network with only a
single layer. Thus, the quadratic running time of attention is unavoidable for
expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of
rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They
showed that in Self Attention Networks with small weights and with skip
connections, rank collapse must occur. This is typically interpreted as
justifying the necessity of skip connections in expressive networks. However,
our result shows that even with skip connections, if the weights are small,
then layer collapse still occurs. Thus, only large weights, and not skip
connections, can prevent these representational weaknesses.

</details>


### [571] [Fairness under Competition](https://arxiv.org/pdf/2505.16291)
*Ronen Gradwohl, Eilam Shapira, Moshe Tennenholtz*

Main category: cs.LG

TL;DR: Fair classifiers may fail to ensure ecosystem fairness, as competing firms' use of individually fair classifiers can lead to unfair outcomes overall.


<details>
  <summary>Details</summary>
Motivation: To examine the impact of fair classifiers on ecosystem fairness, especially in competitive environments.

Method: Introduces a model to study fairness with competing firms, analyzing classifiers' correlation and data overlap. Theoretical and experimental approaches are used.

Result: Individually fair classifiers can result in unfair ecosystem outcomes. Improving individual fairness may reduce overall ecosystem fairness.

Conclusion: Highlights the need for systemic fairness approaches beyond individual classifier adjustments.

Abstract: Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.

</details>


### [572] [Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution](https://arxiv.org/pdf/2505.16305)
*Bingyang Cheng, Zhongtao Chen, Yichen Jin, Hao Zhang, Chen Zhang, Edmud Y. Lam, Yik-Chung Wu*

Main category: cs.LG

TL;DR: CP-GAMP is a scalable Bayesian CPD algorithm that avoids matrix inversions and jointly infers tensor rank and noise power, reducing runtime by 82.7% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Bayesian CPD methods are inefficient for large tensors due to high-dimensional matrix inversions.

Method: CP-GAMP leverages GAMP to avoid matrix inversions and uses EM to infer tensor rank and noise power.

Result: For synthetic 100x100x100 rank 20 tensors with 20% observed elements, runtime is reduced by 82.7% with comparable accuracy.

Conclusion: CP-GAMP is a scalable and efficient Bayesian CPD method for large tensors.

Abstract: Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for
tensor reconstruction. Although the Bayesian framework allows for principled
uncertainty quantification and automatic hyperparameter learning, existing
methods do not scale well for large tensors because of high-dimensional matrix
inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD
algorithm. This algorithm leverages generalized approximate message passing
(GAMP) to avoid matrix inversions and incorporates an expectation-maximization
routine to jointly infer the tensor rank and noise power. Through multiple
experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements
observed, the proposed algorithm reduces runtime by 82.7% compared to the
state-of-the-art variational Bayesian CPD method, while maintaining comparable
reconstruction accuracy.

</details>


### [573] [CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2505.16308)
*Xingyu Zhang, Wenwen Qiang, Siyu Zhao, Huijie Guo, Jiangmeng Li, Chuxiong Sun, Changwen Zheng*

Main category: cs.LG

TL;DR: The paper proposes an all-to-one forecasting paradigm for multivariate time series, using causal structure to partition data and exclude spurious correlations, introducing the CAIFormer model for improved predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat all variables uniformly, obscuring causal relationships and mixing relevant information with spurious correlations.

Method: Constructs a Structural Causal Model, partitions data into causally relevant sub-segments, and uses the CAIFormer model to process these segments for prediction.

Result: CAIFormer outperforms benchmarks, demonstrating effectiveness in distinguishing causal influences.

Conclusion: The all-to-one paradigm and CAIFormer enhance forecasting by focusing on causally relevant data, improving accuracy and interpretability.

Abstract: Most existing multivariate time series forecasting methods adopt an
all-to-all paradigm that feeds all variable histories into a unified model to
predict their future values without distinguishing their individual roles.
However, this undifferentiated paradigm makes it difficult to identify
variable-specific causal influences and often entangles causally relevant
information with spurious correlations. To address this limitation, we propose
an all-to-one forecasting paradigm that predicts each target variable
separately. Specifically, we first construct a Structural Causal Model from
observational data and then, for each target variable, we partition the
historical sequence into four sub-segments according to the inferred causal
structure: endogenous, direct causal, collider causal, and spurious
correlation. The prediction relies solely on the first three causally relevant
sub-segments, while the spurious correlation sub-segment is excluded.
Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel
forecasting model comprising three components: Endogenous Sub-segment
Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider
Causal Sub-segment Prediction Block, which process the endogenous, direct
causal, and collider causal sub-segments, respectively. Their outputs are then
combined to produce the final prediction. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of the CAIFormer.

</details>


### [574] [FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail](https://arxiv.org/pdf/2505.16319)
*Yangyang Wang, Jiawei Gu, Li Long, Xin Li, Li Shen, Zhouyu Fu, Xiangjun Zhou, Xu Jiang*

Main category: cs.LG

TL;DR: FreshRetailNet-50K is a large-scale benchmark for censored demand estimation, offering hourly sales data and stockout annotations to improve demand forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of censored sales data during stockouts, which biases demand estimation and affects inventory/pricing policies.

Method: Introduces FreshRetailNet-50K, a dataset with 50,000 store-product time series, and proposes a two-stage demand modeling approach: demand reconstruction during stockouts followed by robust forecasting.

Result: Achieves a 2.73% improvement in prediction accuracy and reduces systematic demand underestimation from 7.37% to near-zero bias.

Conclusion: FreshRetailNet-50K enables innovative research in demand imputation, inventory optimization, and retail analytics, with open data and code for broader use.

Abstract: Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73\%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37\% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.

</details>


### [575] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/pdf/2505.16322)
*Woosung Koh, Wonbeen Oh, Jaein Jang, MinHyung Lee, Hyeongjin Kim, Ah Yeon Kim, Joonkee Kim, Junghyun Lee, Taehyeon Kim, Se-Young Yun*

Main category: cs.LG

TL;DR: AdaSTaR improves self-improving LMs by adaptive sampling for diversity and curriculum, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing trained observation imbalance in self-improving LMs caused by random sampling.

Method: Introduces AdaSTaR with adaptive sampling for diversity and curriculum to balance training and adjust data difficulty dynamically.

Result: Achieves best test accuracy in all benchmarks (6/6) and reduces training FLOPs by 58.6%.

Conclusion: AdaSTaR enhances efficiency and effectiveness of self-improving LMs, generalizing across models.

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [576] [ChemMLLM: Chemical Multimodal Large Language Model](https://arxiv.org/pdf/2505.16326)
*Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, Tianfan Fu*

Main category: cs.LG

TL;DR: ChemMLLM is a multimodal large language model designed for chemical tasks, outperforming existing models in molecule understanding and generation.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models (MLLMs) lack focus on chemical cross-modal tasks, prompting the development of ChemMLLM.

Method: The authors propose ChemMLLM, a unified model for chemical tasks, and design five multimodal tasks involving text, SMILES strings, and images. Datasets are curated for benchmarking.

Result: ChemMLLM outperforms leading MLLMs and chemical LLMs, achieving a 118.9% improvement over GPT-4o in molecule image optimization.

Conclusion: ChemMLLM demonstrates superior performance in chemical multimodal tasks, filling a gap in the field.

Abstract: Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large
language model for molecule understanding and generation. Also, we design five
multimodal tasks across text, molecular SMILES strings, and image, and curate
the datasets. We benchmark ChemMLLM against a range of general leading MLLMs
and Chemical LLMs on these tasks. Experimental results show that ChemMLLM
achieves superior performance across all evaluated tasks. For example, in
molecule image optimization task, ChemMLLM outperforms the best baseline
(GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly
available at https://github.com/bbsbz/ChemMLLM.git.

</details>


### [577] [Understanding Differential Transformer Unchains Pretrained Self-Attentions](https://arxiv.org/pdf/2505.16333)
*Chaerin Kong, Jiho Jang, Nojun Kwak*

Main category: cs.LG

TL;DR: The paper investigates the Differential Transformer, identifies three key success factors, and proposes DEX, a lightweight method to integrate differential attention into pretrained models, achieving significant performance gains with minimal data.


<details>
  <summary>Details</summary>
Motivation: To understand how Differential Transformer works and address its limitations, such as the need for large-scale training from scratch and inability to utilize pretrained weights.

Method: Proposes DEX, which reuses softmax attention scores and adds a lightweight differential operation on the output value matrix to integrate differential attention into pretrained models.

Result: DEX improves pretrained LLMs across diverse benchmarks with minimal adaptation data (< 0.01%).

Conclusion: DEX successfully incorporates differential attention's advantages into pretrained models, offering lightweight and efficient performance gains.

Abstract: Differential Transformer has recently gained significant attention for its
impressive empirical performance, often attributed to its ability to perform
noise canceled attention. However, precisely how differential attention
achieves its empirical benefits remains poorly understood. Moreover,
Differential Transformer architecture demands large-scale training from
scratch, hindering utilization of open pretrained weights. In this work, we
conduct an in-depth investigation of Differential Transformer, uncovering three
key factors behind its success: (1) enhanced expressivity via negative
attention, (2) reduced redundancy among attention heads, and (3) improved
learning dynamics. Based on these findings, we propose DEX, a novel method to
efficiently integrate the advantages of differential attention into pretrained
language models. By reusing the softmax attention scores and adding a
lightweight differential operation on the output value matrix, DEX effectively
incorporates the key advantages of differential attention while remaining
lightweight in both training and inference. Evaluations confirm that DEX
substantially improves the pretrained LLMs across diverse benchmarks, achieving
significant performance gains with minimal adaptation data (< 0.01\%).

</details>


### [578] [Improving Chemical Understanding of LLMs via SMILES Parsing](https://arxiv.org/pdf/2505.16340)
*Yunhui Jang, Jaehyung Kim, Sungsoo Ahn*

Main category: cs.LG

TL;DR: CLEANMOL improves LLMs' ability to understand SMILES representations of molecules by introducing structured tasks for molecular comprehension.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with interpreting SMILES representations, hindering their use in molecular science.

Method: CLEANMOL formulates SMILES parsing into deterministic tasks (subgraph to global graph matching) and pre-trains LLMs on a dataset with adaptive difficulty.

Result: CLEANMOL enhances structural comprehension and performs competitively on the Mol-Instructions benchmark.

Conclusion: CLEANMOL addresses LLMs' limitations in molecular science, improving their utility for scientific discovery.

Abstract: Large language models (LLMs) are increasingly recognized as powerful tools
for scientific discovery, particularly in molecular science. A fundamental
requirement for these models is the ability to accurately understand molecular
structures, commonly encoded in the SMILES representation. However, current
LLMs struggle to interpret SMILES, even failing to carry out basic tasks such
as counting molecular rings. To address this limitation, we introduce CLEANMOL,
a novel framework that formulates SMILES parsing into a suite of clean and
deterministic tasks explicitly designed to promote graph-level molecular
comprehension. These tasks span from subgraph matching to global graph
matching, providing structured supervision aligned with molecular structural
properties. We construct a molecular pretraining dataset with adaptive
difficulty scoring and pre-train open-source LLMs on these tasks. Our results
show that CLEANMOL not only enhances structural comprehension but also achieves
the best or competes with the baseline on the Mol-Instructions benchmark.

</details>


### [579] [A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning](https://arxiv.org/pdf/2505.16341)
*Yaxin Hou, Yuheng Jia*

Main category: cs.LG

TL;DR: The paper proposes a dynamic expert assignment module and multi-depth feature fusion to address long-tailed semi-supervised learning with distribution mismatch, improving pseudo-label quality and reducing model bias.


<details>
  <summary>Details</summary>
Motivation: Existing methods in LTSSL underutilize expert strengths and fail to address feature bias, prompting the need for dynamic expert assignment and feature fusion.

Method: Introduces a dynamic expert assignment module to estimate class membership and assign suitable experts, along with multi-depth feature fusion to mitigate bias.

Result: Demonstrates effectiveness on CIFAR-10-LT, STL-10-LT, and SVHN-LT datasets, showing improved performance.

Conclusion: The proposed method effectively leverages expert strengths and feature fusion to enhance LTSSL performance.

Abstract: This paper studies the long-tailed semi-supervised learning (LTSSL) with
distribution mismatch, where the class distribution of the labeled training
data follows a long-tailed distribution and mismatches with that of the
unlabeled training data. Most existing methods introduce auxiliary classifiers
(experts) to model various unlabeled data distributions and produce
pseudo-labels, but the expertises of various experts are not fully utilized. We
observe that different experts are good at predicting different intervals of
samples, e.g., long-tailed expert is skilled in samples located in the head
interval and uniform expert excels in samples located in the medium interval.
Therefore, we propose a dynamic expert assignment module that can estimate the
class membership (i.e., head, medium, or tail class) of samples, and
dynamically assigns suitable expert to each sample based on the estimated
membership to produce high-quality pseudo-label in the training phase and
produce prediction in the testing phase. We also theoretically reveal that
integrating different experts' strengths will lead to a smaller generalization
error bound. Moreover, we find that the deeper features are more biased toward
the head class but with more discriminative ability, while the shallower
features are less biased but also with less discriminative ability. We,
therefore, propose a multi-depth feature fusion module to utilize different
depth features to mitigate the model bias. Our method demonstrates its
effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,
and SVHN-LT datasets across various settings. The code is available at
https://github.com/yaxinhou/Meta-Expert.

</details>


### [580] [Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning](https://arxiv.org/pdf/2505.16353)
*Céline Comte, Pascal Moyal*

Main category: cs.LG

TL;DR: A versatile scheme for optimizing arrival rates in quasi-reversible queueing systems is introduced, with balanced arrival control policies preserving quasi-reversibility.


<details>
  <summary>Details</summary>
Motivation: To generalize and optimize arrival rates in quasi-reversible queueing systems, extending beyond Whittle networks.

Method: Proposes an alternative definition of quasi-reversibility, introduces balanced arrival control policies, and applies them to canonical examples.

Result: Balanced arrival control preserves quasi-reversibility, with stationary measures specified. Applied to Whittle networks and order-independent queues.

Conclusion: The framework is leveraged for admission control, optimization, and reinforcement learning, demonstrating broad applicability.

Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.

</details>


### [581] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/pdf/2505.16363)
*Huishuai Zhang, Bohan Wang, Luoxin Chen*

Main category: cs.LG

TL;DR: AdamS is a memory-efficient alternative to Adam for LLM training, using a novel denominator to avoid second-moment estimates, matching SGD's footprint while outperforming AdamW.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiencies in Adam by leveraging transformer objectives' smoothness properties, approximated by gradient and momentum magnitudes.

Method: Replaces second-moment estimates with a denominator based on the root of weighted sum of squares of momentum and current gradient.

Result: Demonstrates superior performance in GPT-2, Llama2 (up to 13B), and RL tasks, with theoretical convergence guarantees.

Conclusion: AdamS is efficient, easy to adopt, and outperforms existing optimizers, making it a compelling choice for LLM training.

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [582] [A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules](https://arxiv.org/pdf/2505.16365)
*Manuel Ruiz-Botella, Marta Sales-Pardo, Roger Guimerà*

Main category: cs.LG

TL;DR: CoCoGraph is a constrained graph diffusion model for generating chemically valid molecules efficiently, outperforming state-of-the-art methods with fewer parameters and closer-to-real molecule distributions.


<details>
  <summary>Details</summary>
Motivation: The vast molecular space makes discovering new compounds challenging, necessitating efficient and valid molecule generation methods.

Method: CoCoGraph uses a collaborative and constrained graph diffusion model to ensure chemical validity and efficiency.

Result: It outperforms benchmarks, requires fewer parameters, and generates molecules with distributions closer to real ones. A database of 8.2M synthetic molecules was created.

Conclusion: CoCoGraph is efficient and effective, validated by expert Turing-like tests, though potential biases and limitations exist.

Abstract: Developing new molecular compounds is crucial to address pressing challenges,
from health to environmental sustainability. However, exploring the molecular
space to discover new molecules is difficult due to the vastness of the space.
Here we introduce CoCoGraph, a collaborative and constrained graph diffusion
model capable of generating molecules that are guaranteed to be chemically
valid. Thanks to the constraints built into the model and to the collaborative
mechanism, CoCoGraph outperforms state-of-the-art approaches on standard
benchmarks while requiring up to an order of magnitude fewer parameters.
Analysis of 36 chemical properties also demonstrates that CoCoGraph generates
molecules with distributions more closely matching real molecules than current
models. Leveraging the model's efficiency, we created a database of 8.2M
million synthetically generated molecules and conducted a Turing-like test with
organic chemistry experts to further assess the plausibility of the generated
molecules, and potential biases and limitations of CoCoGraph.

</details>


### [583] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/pdf/2505.16368)
*Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li*

Main category: cs.LG

TL;DR: Saturn is a SAT-based RL framework for training LLMs, addressing scalability, verifiability, and difficulty control in reasoning tasks. It shows improvements in SAT, math, and programming tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RL tasks for LLMs face scalability, verifiability, and difficulty control issues. Saturn aims to solve these using SAT problems.

Method: Saturn uses Boolean Satisfiability (SAT) problems for scalable task construction, rule-based verification, and curriculum learning with controlled difficulty transitions.

Result: Saturn models (1.5B and 7B) improve SAT pass@3 by +14.0 and +28.1, and math/programming scores by +4.9 and +1.8, outperforming SOTA by +8.8%.

Conclusion: Saturn effectively trains LLMs for reasoning, offering scalable, verifiable, and difficulty-controlled tasks, with demonstrated performance gains.

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [584] [Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space](https://arxiv.org/pdf/2505.16386)
*Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: Omni TM-AE is a new embedding model combining interpretability and scalability, outperforming traditional models in NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of transparency and reusability in large-scale language models while maintaining performance.

Method: Introduces Omni TM-AE, leveraging the Tsetlin Machine's state matrix for reusable, interpretable embeddings in one training phase.

Result: Competes with or surpasses mainstream models in semantic similarity, sentiment classification, and document clustering.

Conclusion: Shows that performance, scalability, and interpretability can coexist in NLP without opaque architectures.

Abstract: The increasing complexity of large-scale language models has amplified
concerns regarding their interpretability and reusability. While traditional
embedding models like Word2Vec and GloVe offer scalability, they lack
transparency and often behave as black boxes. Conversely, interpretable models
such as the Tsetlin Machine (TM) have shown promise in constructing explainable
learning systems, though they previously faced limitations in scalability and
reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni
TM-AE), a novel embedding model that fully exploits the information contained
in the TM's state matrix, including literals previously excluded from clause
formation. This method enables the construction of reusable, interpretable
embeddings through a single training phase. Extensive experiments across
semantic similarity, sentiment classification, and document clustering tasks
show that Omni TM-AE performs competitively with and often surpasses mainstream
embedding models. These results demonstrate that it is possible to balance
performance, scalability, and interpretability in modern Natural Language
Processing (NLP) systems without resorting to opaque architectures.

</details>


### [585] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/pdf/2505.16400)
*Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping*

Main category: cs.LG

TL;DR: Large-scale RL enhances reasoning in small/mid-sized models, outperforming distillation. Math-only RL boosts math and code performance, while code-only RL further improves code tasks. Key insights include curriculum learning and stable on-policy updates.


<details>
  <summary>Details</summary>
Motivation: Address the lack of clear training recipes for high-performing reasoning models and the omission of key details in frontier models like DeepSeek-R1.

Method: Train models sequentially on math-only and code-only prompts, using a robust data curation pipeline for high-quality prompts and verification-based RL.

Result: Math-only RL improves math benchmarks (+14.6%/17.2%) and code tasks (+6.8%/5.8%). Code-only RL further enhances code performance without degrading math results.

Conclusion: RL unlocks foundational reasoning capabilities and extends model limits, solving previously unsolvable problems.

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [586] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/pdf/2505.16401)
*Xiaoqing Zhang, Huabin Zheng, Ang Lv, Yuhan Liu, Zirui Song, Flood Sung, Xiuying Chen, Rui Yan*

Main category: cs.LG

TL;DR: The paper introduces Divide-Fuse-Conquer, a framework to improve generalization in multi-scenario RL for LLMs, achieving competitive performance in 18 TextArena games.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of RL in multi-scenario games where policies often fail to generalize due to diverse rules and complexities.

Method: Divide-Fuse-Conquer: grouping games by characteristics, training specialized models per group, fusing parameters, and continuing training for generalization.

Result: Qwen2.5-32B-Align trained with this method matches Claude3.5's performance, with 7 wins and 4 draws in TextArena.

Conclusion: The framework effectively enhances generalization in multi-scenario RL, inspiring future research on RL for LLMs.

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [587] [Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach](https://arxiv.org/pdf/2505.16403)
*Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, Suiyang Khoo*

Main category: cs.LG

TL;DR: FedSA is a novel poisoning attack in federated learning using Sliding Mode Control to precisely manipulate global model accuracy with stealth and control.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for subtle, controlled poisoning attacks in federated learning, beyond traditional DoS-focused methods.

Method: Integrates Sliding Mode Control (SMC) with model poisoning to manipulate updates and control convergence bounds.

Result: FedSA achieves predefined global accuracy with fewer malicious clients, maintaining stealth and adjustable rates.

Conclusion: FedSA offers a precise, controlled, and stealthy approach to poisoning attacks in federated learning.

Abstract: Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.

</details>


### [588] [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/pdf/2505.16446)
*Zhaoxin Wang, Handing Wang, Cong Tian, Yaochu Jin*

Main category: cs.LG

TL;DR: Proposes IJA, an implicit jailbreak framework for MLLMs using steganography and adversarial prompts, achieving high success rates.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of bypassing cross-modal consistency in MLLMs for jailbreak attacks.

Method: Uses least significant bit steganography to embed malicious instructions in images, combined with benign textual prompts and adversarial suffixes.

Result: Achieves over 90% attack success on models like GPT-4o and Gemini-1.5 Pro with ~3 queries.

Conclusion: Demonstrates the vulnerability of MLLMs to stealthy, multimodal jailbreak attacks.

Abstract: Multimodal large language models (MLLMs) enable powerful cross-modal
reasoning capabilities. However, the expanded input space introduces new attack
surfaces. Previous jailbreak attacks often inject malicious instructions from
text into less aligned modalities, such as vision. As MLLMs increasingly
incorporate cross-modal consistency and alignment mechanisms, such explicit
attacks become easier to detect and block. In this work, we propose a novel
implicit jailbreak framework termed IJA that stealthily embeds malicious
instructions into images via least significant bit steganography and couples
them with seemingly benign, image-related textual prompts. To further enhance
attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes
generated by a surrogate model and introduce a template optimization module
that iteratively refines both the prompt and embedding based on model feedback.
On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack
success rates of over 90% using an average of only 3 queries.

</details>


### [589] [Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling](https://arxiv.org/pdf/2505.16481)
*Xinxing Shi, Xiaoyu Jiang, Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: A scalable GPVAE method using neighbor-driven approximation for efficient inference, outperforming existing GPVAE variants.


<details>
  <summary>Details</summary>
Motivation: Standard GPVAEs face computational challenges with large-scale data due to restrictive kernel assumptions or excessive inducing points.

Method: Proposes a neighbor-driven approximation leveraging local adjacencies in latent space, reducing computational load while preserving latent dependencies.

Result: Outperforms other GPVAE variants in predictive performance and computational efficiency across tasks like representation learning and data imputation.

Conclusion: The method enables scalable GPVAE inference with flexible kernels and fewer inducing points, enhancing practical applicability.

Abstract: Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by
replacing the fully factorised Gaussian prior with a GP prior, thereby
capturing richer correlations among latent variables. However, performing exact
GP inference in large-scale GPVAEs is computationally prohibitive, often
forcing existing approaches to rely on restrictive kernel assumptions or large
sets of inducing points. In this work, we propose a neighbour-driven
approximation strategy that exploits local adjacencies in the latent space to
achieve scalable GPVAE inference. By confining computations to the nearest
neighbours of each data point, our method preserves essential latent
dependencies, allowing more flexible kernel choices and mitigating the need for
numerous inducing points. Through extensive experiments on tasks including
representation learning, data imputation, and conditional generation, we
demonstrate that our approach outperforms other GPVAE variants in both
predictive performance and computational efficiency.

</details>


### [590] [Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics](https://arxiv.org/pdf/2505.16493)
*Seyedeh Fatemeh Ebrahimi, Jaakko Peltonen*

Main category: cs.LG

TL;DR: A constrained NMF-based topic modeling method is proposed to better capture minority topics, outperforming baselines in synthetic and real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing topic models struggle with low-prevalence, domain-critical themes (minority topics), and guidance methods may hinder topic discovery.

Method: Uses a specially constrained NMF with seed word lists and prevalence constraints, fitted via KKT conditions and multiplicative updates.

Result: Outperforms baselines in topic purity, NMI, and JSD; successfully identifies minority topics in YouTube vlog comments.

Conclusion: The method effectively captures minority topics without requiring detailed pre-specification, proving useful for domain-relevant analysis.

Abstract: Topic models often fail to capture low-prevalence, domain-critical themes,
so-called minority topics, such as mental health themes in online comments.
While some existing methods can incorporate domain knowledge, such as expected
topical content, methods allowing guidance may require overly detailed expected
topics, hindering the discovery of topic divisions and variation. We propose a
topic modeling solution via a specially constrained NMF. We incorporate a seed
word list characterizing minority content of interest, but we do not require
experts to pre-specify their division across minority topics. Through
prevalence constraints on minority topics and seed word content across topics,
we learn distinct data-driven minority topics as well as majority topics. The
constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with
multiplicative updates. We outperform several baselines on synthetic data in
terms of topic purity, normalized mutual information, and also evaluate topic
quality using Jensen-Shannon divergence (JSD). We conduct a case study on
YouTube vlog comments, analyzing viewer discussion of mental health content;
our model successfully identifies and reveals this domain-relevant minority
content.

</details>


### [591] [Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility](https://arxiv.org/pdf/2505.16494)
*Noga Amit, Omer Reingold, Guy N. Rothblum*

Main category: cs.LG

TL;DR: The paper explores fairness in machine learning with richer data labels, proposing algorithms for stronger fairness notions and showing computational trade-offs between accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: To address fairness in settings with complex data (e.g., rankings, risk estimates) and reconcile it with utility and efficiency.

Method: Proposes algorithms for evidence-based fairness, supporting classification and ranking while preserving subpopulation accuracy and enabling loss minimization.

Result: Achieves stronger fairness than standard methods but shows computational infeasibility in simultaneously optimizing accuracy and fairness.

Conclusion: Fairness and accuracy can be individually achieved, but computational limitations force a choice between them.

Abstract: We revisit the foundations of fairness and its interplay with utility and
efficiency in settings where the training data contain richer labels, such as
individual types, rankings, or risk estimates, rather than just binary
outcomes. In this context, we propose algorithms that achieve stronger notions
of evidence-based fairness than are possible in standard supervised learning.
Our methods support classification and ranking techniques that preserve
accurate subpopulation classification rates, as suggested by the underlying
data distributions, across a broad class of classification rules and downstream
applications. Furthermore, our predictors enable loss minimization, whether
aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results
demonstrating that simultaneously achieving accurate classification rates and
optimal loss minimization is, in some cases, computationally infeasible. Unlike
prior impossibility results, our notions are not inherently in conflict and are
simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show
that each notion can be satisfied individually via efficient learning. Our
separation thus stems from the computational hardness of learning a
sufficiently good approximation of the Bayes-optimal predictor. These
computational impossibilities present a choice between two natural and
attainable notions of accuracy that could both be motivated by fairness.

</details>


### [592] [Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods](https://arxiv.org/pdf/2505.16516)
*Majid Mohammadi, Siu Lun Chau, Krikamol Muandet*

Main category: cs.LG

TL;DR: PKeX-Shapley enables exact computation of Shapley values for product-kernel models in polynomial time, improving interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Kernel methods lack interpretability, limiting their use in high-stakes applications. Shapley value approximations exist but are computationally expensive.

Method: Introduces PKeX-Shapley, leveraging the multiplicative structure of product kernels for exact Shapley value computation via recursive functional decomposition.

Result: Achieves polynomial-time exact computation of Shapley values, enhancing interpretability and extending to kernel-based statistical discrepancies like MMD and HSIC.

Conclusion: PKeX-Shapley provides a scalable and interpretable solution for kernel-based models, expanding tools for explainable statistical inference.

Abstract: Kernel methods are widely used in machine learning due to their flexibility
and expressive power. However, their black-box nature poses significant
challenges to interpretability, limiting their adoption in high-stakes
applications. Shapley value-based feature attribution techniques, such as SHAP
and kernel-specific variants like RKHS-SHAP, offer a promising path toward
explainability. Yet, computing exact Shapley values remains computationally
intractable in general, motivating the development of various approximation
schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that
utilizes the multiplicative structure of product kernels to enable the exact
computation of Shapley values in polynomial time. We show that product-kernel
models admit a functional decomposition that allows for a recursive formulation
of Shapley values. This decomposition not only yields computational efficiency
but also enhances interpretability in kernel-based learning. We also
demonstrate how our framework can be generalized to explain kernel-based
statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the
Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for
interpretable statistical inference.

</details>


### [593] [Joint Relational Database Generation via Graph-Conditional Diffusion Models](https://arxiv.org/pdf/2505.16527)
*Mohamed Amine Ketata, David Lüdke, Leo Schwinn, Stephan Günnemann*

Main category: cs.LG

TL;DR: Proposes GRDM, a graph-based model for joint generation of relational databases without fixed table order, outperforming autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for RDB generation are limited by fixed table order and sequential generation, reducing flexibility and parallelism.

Method: Uses a graph representation of RDBs and a graph neural network (GRDM) to jointly denoise row attributes and model inter-table dependencies.

Result: GRDM outperforms autoregressive baselines in multi-hop inter-table correlations and achieves state-of-the-art single-table fidelity.

Conclusion: GRDM offers a flexible, parallelizable, and high-fidelity approach for relational database generation.

Abstract: Building generative models for relational databases (RDBs) is important for
applications like privacy-preserving data release and augmenting real datasets.
However, most prior work either focuses on single-table generation or relies on
autoregressive factorizations that impose a fixed table order and generate
tables sequentially. This approach limits parallelism, restricts flexibility in
downstream applications like missing value imputation, and compounds errors due
to commonly made conditional independence assumptions. We propose a
fundamentally different approach: jointly modeling all tables in an RDB without
imposing any order. By using a natural graph representation of RDBs, we propose
the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph
neural network to jointly denoise row attributes and capture complex
inter-table dependencies. Extensive experiments on six real-world RDBs
demonstrate that our approach substantially outperforms autoregressive
baselines in modeling multi-hop inter-table correlations and achieves
state-of-the-art performance on single-table fidelity metrics.

</details>


### [594] [HOFT: Householder Orthogonal Fine-tuning](https://arxiv.org/pdf/2505.16531)
*Alejandro Moreno Arcas, Albert Sanchis, Jorge Civera, Alfons Juan*

Main category: cs.LG

TL;DR: HOFT and SHOFT are proposed as efficient orthogonal fine-tuning methods for foundation models, showing competitive or superior performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing orthogonal fine-tuning methods while maintaining their generalization benefits.

Method: Proposes HOFT and SHOFT, leveraging Householder transformations for efficiency, and explores theoretical properties of orthogonal fine-tuning.

Result: HOFT and SHOFT achieve comparable or better results than state-of-the-art methods in tasks like commonsense reasoning and machine translation.

Conclusion: HOFT and SHOFT offer efficient and effective alternatives to traditional orthogonal fine-tuning, with SHOFT further enhancing performance.

Abstract: Adaptation of foundation models using low-rank methods is a widespread
approach. Another way to adapt these models is to employ orthogonal fine-tuning
methods, which are less time and memory efficient despite their good
generalization properties. In this work, we propose Householder Orthogonal
Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to
alleviate time and space complexity. Moreover, some theoretical properties of
the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled
Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are
evaluated in downstream tasks, namely commonsense reasoning, machine
translation, subject-driven generation and mathematical reasoning. Compared
with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or
better results.

</details>


### [595] [Incremental Sequence Classification with Temporal Consistency](https://arxiv.org/pdf/2505.16548)
*Lucas Maystre, Gabriel Barello, Tudor Berariu, Aleix Cambray, Rares Dolga, Alvaro Ortega Gonzalez, Andrei Nica, David Barber*

Main category: cs.LG

TL;DR: A novel loss function for incremental sequence classification improves data efficiency and predictive accuracy, outperforming competing methods on benchmarks and aiding in verifying LLM-generated math solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of updating predictions incrementally in sequences, inspired by temporal-difference learning, ensuring temporal consistency in successive predictions.

Method: Develops a temporal-consistency-based loss function for training incremental sequence classifiers, validated on text classification and LLM-generated math problem verification.

Result: Substantial gains in data efficiency and improved predictive accuracy on benchmark datasets; better distinction of promising LLM generations in math problems.

Conclusion: The proposed method enhances incremental sequence classification, offering practical benefits in efficiency and accuracy, especially for early-stage prediction tasks.

Abstract: We address the problem of incremental sequence classification, where
predictions are updated as new elements in the sequence are revealed. Drawing
on temporal-difference learning from reinforcement learning, we identify a
temporal-consistency condition that successive predictions should satisfy. We
leverage this condition to develop a novel loss function for training
incremental sequence classifiers. Through a concrete example, we demonstrate
that optimizing this loss can offer substantial gains in data efficiency. We
apply our method to text classification tasks and show that it improves
predictive accuracy over competing approaches on several benchmark datasets. We
further evaluate our approach on the task of verifying large language model
generations for correctness in grade-school math problems. Our results show
that models trained with our method are better able to distinguish promising
generations from unpromising ones after observing only a few tokens.

</details>


### [596] [Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations](https://arxiv.org/pdf/2505.16549)
*Trung V. Phan, George A. Kevrekidis, Soledad Villar, Yannis G. Kevrekidis, Juan M. Bello-Rivas*

Main category: cs.LG

TL;DR: The paper introduces a coordinate-free, dimension-independent machine learning method for PDE identification, enabling generalization across spaces.


<details>
  <summary>Details</summary>
Motivation: Current PDE learning methods depend on specific spatial dimensions and coordinates, limiting generalization.

Method: Uses exterior calculus for coordinate-free, dimension-independent representations and applies machine learning to predict scalar field evolution.

Result: Demonstrated success in reaction-diffusion models and chemotactic bacteria observations, showing seamless transitions across spatial contexts.

Conclusion: The approach allows accurate predictions across varying dimensions, coordinates, boundaries, and curvatures, advancing PDE learning.

Abstract: The machine learning methods for data-driven identification of partial
differential equations (PDEs) are typically defined for a given number of
spatial dimensions and a choice of coordinates the data have been collected in.
This dependence prevents the learned evolution equation from generalizing to
other spaces. In this work, we reformulate the problem in terms of coordinate-
and dimension-independent representations, paving the way toward what we call
``spatially liberated" PDE learning. To this end, we employ a machine learning
approach to predict the evolution of scalar field systems expressed in the
formalism of exterior calculus, which is coordinate-free and immediately
generalizes to arbitrary dimensions by construction. We demonstrate the
performance of this approach in the FitzHugh-Nagumo and Barkley
reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by
in-situ chemotactic bacteria observations. We provide extensive numerical
experiments that demonstrate that our approach allows for seamless transitions
across various spatial contexts. We show that the field dynamics learned in one
space can be used to make accurate predictions in other spaces with different
dimensions, coordinate systems, boundary conditions, and curvatures.

</details>


### [597] [A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices](https://arxiv.org/pdf/2505.16563)
*Chen Gong, Rui Xing, Zhenzhe Zheng, Fan Wu*

Main category: cs.LG

TL;DR: Titan is a two-stage data selection framework for efficient on-device ML training, improving throughput and accuracy with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing under-utilization of on-device data due to low training throughput, storage limits, and diverse data importance.

Method: A two-stage framework: coarse-grained filtering of high-importance data, followed by fine-grained selection of the most impactful batch. Uses a pipeline for co-execution and idle resources.

Result: Reduces training time by up to 43% and increases accuracy by 6.2% with minor system overhead.

Conclusion: Titan effectively enhances on-device training efficiency and performance, making it practical for edge computing.

Abstract: The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.

</details>


### [598] [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/pdf/2505.16567)
*Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev*

Main category: cs.LG

TL;DR: The paper introduces FAB, a finetuning-activated backdoor attack on LLMs, showing how poisoned models appear benign but turn malicious after downstream finetuning.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in the finetuning process of LLMs, challenging the assumption that finetuning is secure and predictable.

Method: Uses meta-learning to poison LLMs, optimizing for hidden malicious behaviors triggered by finetuning while retaining general capabilities.

Result: FAB successfully induces malicious behaviors (e.g., advertising, refusal, jailbreakability) across multiple LLMs, robust to user finetuning choices.

Conclusion: Finetuning is not inherently secure; FAB reveals a critical attack vector, urging reevaluation of LLM security practices.

Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.

</details>


### [599] [Large Language Model-Empowered Interactive Load Forecasting](https://arxiv.org/pdf/2505.16577)
*Yu Zuo, Dalin Qin, Yi Wang*

Main category: cs.LG

TL;DR: An LLM-based multi-agent framework is proposed to improve load forecasting by enabling human-model interaction, reducing technical barriers, and integrating human expertise.


<details>
  <summary>Details</summary>
Motivation: Current load forecasting methods lack human-model interaction, making them hard for system operators to use and integrate their experience.

Method: A multi-agent collaboration framework using LLMs is designed to embed interactive mechanisms in the forecasting workflow.

Result: The framework improves forecasting accuracy with human insights and remains cost-effective for real-world use.

Conclusion: The proposed framework successfully bridges the gap between human operators and forecasting models, enhancing usability and accuracy.

Abstract: The growing complexity of power systems has made accurate load forecasting
more important than ever. An increasing number of advanced load forecasting
methods have been developed. However, the static design of current methods
offers no mechanism for human-model interaction. As the primary users of
forecasting models, system operators often find it difficult to understand and
apply these advanced models, which typically requires expertise in artificial
intelligence (AI). This also prevents them from incorporating their experience
and real-world contextual understanding into the forecasting process. Recent
breakthroughs in large language models (LLMs) offer a new opportunity to
address this issue. By leveraging their natural language understanding and
reasoning capabilities, we propose an LLM-based multi-agent collaboration
framework to bridge the gap between human operators and forecasting models. A
set of specialized agents is designed to perform different tasks in the
forecasting workflow and collaborate via a dedicated communication mechanism.
This framework embeds interactive mechanisms throughout the load forecasting
pipeline, reducing the technical threshold for non-expert users and enabling
the integration of human experience. Our experiments demonstrate that the
interactive load forecasting accuracy can be significantly improved when users
provide proper insight in key stages. Our cost analysis shows that the
framework remains affordable, making it practical for real-world deployment.

</details>


### [600] [How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning](https://arxiv.org/pdf/2505.16581)
*Max Weltevrede, Moritz A. Zanger, Matthijs T. J. Spaan, Wendelin Böhmer*

Main category: cs.LG

TL;DR: Policy distillation in zero-shot RL improves generalization; train ensembles and use diverse training data.


<details>
  <summary>Details</summary>
Motivation: Understand why policy distillation outperforms original policies in unseen environments and identify optimal distillation data.

Method: Prove generalization bound for policy distillation, suggesting ensemble training and diverse data usage.

Result: Empirical validation shows better generalization with ensembles and diverse data, outperforming original agents.

Conclusion: Ensemble distillation on diverse data significantly enhances zero-shot policy transfer.

Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal
is to train an agent on a fixed set of training environments so that it can
generalise to similar, but unseen, testing environments. Previous work has
shown that policy distillation after training can sometimes produce a policy
that outperforms the original in the testing environments. However, it is not
yet entirely clear why that is, or what data should be used to distil the
policy. In this paper, we prove, under certain assumptions, a generalisation
bound for policy distillation after training. The theory provides two practical
insights: for improved generalisation, you should 1) train an ensemble of
distilled policies, and 2) distil it on as much data from the training
environments as possible. We empirically verify that these insights hold in
more general settings, when the assumptions required for the theory no longer
hold. Finally, we demonstrate that an ensemble of policies distilled on a
diverse dataset can generalise significantly better than the original agent.

</details>


### [601] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/pdf/2505.16583)
*Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Training classifiers on plausible counterfactual explanations (p-CFEs) with incorrect labels improves accuracy and reduces bias compared to adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore if learning from p-CFEs, which are plausible under the data distribution, can enhance classifier performance and reduce bias.

Method: Train classifiers on p-CFEs labeled with incorrect target classes and evaluate their performance on unperturbed inputs.

Result: Classifiers trained on p-CFEs achieve high in-distribution accuracy and significantly reduced bias from spurious correlations.

Conclusion: Learning from p-CFEs is more effective than adversarial perturbations for improving classifier robustness and fairness.

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [602] [CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models](https://arxiv.org/pdf/2505.16620)
*Benjamin Herdeanu, Juan Nathaniel, Carla Roesch, Jatan Buch, Gregor Ramien, Johannes Haux, Pierre Gentine*

Main category: cs.LG

TL;DR: CausalDynamics is a benchmark and data generation framework for causal discovery in dynamical systems, addressing limitations of current methods by evaluating algorithms on noisy, confounded, and lagged dynamics.


<details>
  <summary>Details</summary>
Motivation: Current causal discovery methods for dynamical systems are limited to deterministic, low-dimensional, and weakly nonlinear data, prompting the need for a more robust and scalable framework.

Method: The framework uses true causal graphs from coupled differential equations and climate models, with a plug-and-play workflow for constructing hierarchical physical systems.

Result: CausalDynamics enables comprehensive evaluation of state-of-the-art causal discovery algorithms, highlighting their performance on complex dynamics.

Conclusion: The framework aims to advance robust causal discovery algorithms applicable across domains, with accessible implementation and documentation.

Abstract: Causal discovery for dynamical systems poses a major challenge in fields
where active interventions are infeasible. Most methods used to investigate
these systems and their associated benchmarks are tailored to deterministic,
low-dimensional and weakly nonlinear time-series data. To address these
limitations, we present CausalDynamics, a large-scale benchmark and extensible
data generation framework to advance the structural discovery of dynamical
causal models. Our benchmark consists of true causal graphs derived from
thousands of coupled ordinary and stochastic differential equations as well as
two idealized climate models. We perform a comprehensive evaluation of
state-of-the-art causal discovery algorithms for graph reconstruction on
systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of
a plug-and-play, build-your-own coupling workflow that enables the construction
of a hierarchy of physical systems. We anticipate that our framework will
facilitate the development of robust causal discovery algorithms that are
broadly applicable across domains while addressing their unique challenges. We
provide a user-friendly implementation and documentation on
https://kausable.github.io/CausalDynamics.

</details>


### [603] [Multivariate Latent Recalibration for Conditional Normalizing Flows](https://arxiv.org/pdf/2505.16636)
*Victor Dheur, Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: The paper introduces latent recalibration (LR), a method to improve multivariate model calibration by transforming the latent space of conditional normalizing flows, ensuring reliable joint distribution approximations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multivariate model calibration are limited, often providing poor joint distribution approximations or lacking explicit density functions, leading to unreliable predictions.

Method: Proposes latent recalibration (LR), which learns a transformation in the latent space of conditional normalizing flows with finite-sample bounds on calibration.

Result: LR improves latent calibration error and negative log-likelihood across tabular and image datasets.

Conclusion: LR effectively recalibrates multivariate models, providing explicit density functions and computational efficiency.

Abstract: Reliably characterizing the full conditional distribution of a multivariate
response variable given a set of covariates is crucial for trustworthy
decision-making. However, misspecified or miscalibrated multivariate models may
yield a poor approximation of the joint distribution of the response variables,
leading to unreliable predictions and suboptimal decisions. Furthermore,
standard recalibration methods are primarily limited to univariate settings,
while conformal prediction techniques, despite generating multivariate
prediction regions with coverage guarantees, do not provide a full probability
density function. We address this gap by first introducing a novel notion of
latent calibration, which assesses probabilistic calibration in the latent
space of a conditional normalizing flow. Second, we propose latent
recalibration (LR), a novel post-hoc model recalibration method that learns a
transformation of the latent space with finite-sample bounds on latent
calibration. Unlike existing methods, LR produces a recalibrated distribution
with an explicit multivariate density function while remaining computationally
efficient. Extensive experiments on both tabular and image datasets show that
LR consistently improves latent calibration error and the negative
log-likelihood of the recalibrated models.

</details>


### [604] [Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity](https://arxiv.org/pdf/2505.16638)
*Benedikt Höltgen, Nuria Oliver*

Main category: cs.LG

TL;DR: Fairness through Unawareness (FtU) can reduce algorithmic discrimination without sacrificing accuracy, challenging prior criticisms. It connects to Model Multiplicity and shows practical utility in equitable policy deployment.


<details>
  <summary>Details</summary>
Motivation: To challenge the notion that FtU is insufficient for fairness and detrimental to accuracy, showing its potential benefits in reducing discrimination.

Method: Theoretical and empirical analysis of FtU, linking it to Model Multiplicity, with real-life application examples.

Result: FtU reduces discrimination without necessarily lowering accuracy, supported by theoretical and empirical evidence.

Conclusion: FtU is viable in high-risk scenarios, and protected attributes should only be used with strong justification.

Abstract: Fairness through Unawareness (FtU) describes the idea that discrimination
against demographic groups can be avoided by not considering group membership
in the decisions or predictions. This idea has long been criticized in the
machine learning literature as not being sufficient to ensure fairness. In
addition, the use of additional features is typically thought to increase the
accuracy of the predictions for all groups, so that FtU is sometimes thought to
be detrimental to all groups. In this paper, we show both theoretically and
empirically that FtU can reduce algorithmic discrimination without necessarily
reducing accuracy. We connect this insight with the literature on Model
Multiplicity, to which we contribute with novel theoretical and empirical
results. Furthermore, we illustrate how, in a real-life application, FtU can
contribute to the deployment of more equitable policies without losing
efficacy. Our findings suggest that FtU is worth considering in practical
applications, particularly in high-risk scenarios, and that the use of
protected attributes such as gender in predictive models should be accompanied
by a clear and well-founded justification.

</details>


### [605] [Stochastic Forward-Forward Learning through Representational Dimensionality Compression](https://arxiv.org/pdf/2505.16649)
*Zhichao Zhu, Yang Qi, Hengyuan Ma, Wenlian Lu, Jianfeng Feng*

Main category: cs.LG

TL;DR: The paper introduces a novel goodness function, dimensionality compression, for the Forward-Forward algorithm, leveraging effective dimensionality to improve neural network training without negative samples. It shows competitive performance and highlights noise's constructive role in generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing goodness functions in the Forward-Forward algorithm, which neglect neuron correlations, by proposing a more biologically plausible method incorporating second-order statistical structure.

Method: Proposes a goodness function based on effective dimensionality (ED) of neural responses, minimizing ED for clamped inputs with noise and maximizing it across samples.

Result: Achieves competitive performance compared to non-backpropagation methods, with noise enhancing generalization and inference.

Conclusion: The method advances biologically plausible learning and fits neuromorphic computing, treating stochasticity as a resource. Code is publicly available.

Abstract: The Forward-Forward (FF) algorithm provides a bottom-up alternative to
backpropagation (BP) for training neural networks, relying on a layer-wise
"goodness" function to guide learning. Existing goodness functions, inspired by
energy-based learning (EBL), are typically defined as the sum of squared
post-synaptic activations, neglecting the correlations between neurons. In this
work, we propose a novel goodness function termed dimensionality compression
that uses the effective dimensionality (ED) of fluctuating neural responses to
incorporate second-order statistical structure. Our objective minimizes ED for
clamped inputs when noise is considered while maximizing it across the sample
distribution, promoting structured representations without the need to prepare
negative samples. We demonstrate that this formulation achieves competitive
performance compared to other non-BP methods. Moreover, we show that noise
plays a constructive role that can enhance generalization and improve inference
when predictions are derived from the mean of squared outputs, which is
equivalent to making predictions based on the energy term. Our findings
contribute to the development of more biologically plausible learning
algorithms and suggest a natural fit for neuromorphic computing, where
stochasticity is a computational resource rather than a nuisance. The code is
available at https://github.com/ZhichaoZhu/StochasticForwardForward

</details>


### [606] [End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries](https://arxiv.org/pdf/2505.16664)
*Khoa Tran, Tri Le, Bao Huynh, Hung-Cuong Trinh, Vy-Rin Nguyen*

Main category: cs.LG

TL;DR: A novel hybrid deep learning approach for predicting lithium-ion battery RUL, combining signal processing and deep learning, outperforms baselines with robust performance.


<details>
  <summary>Details</summary>
Motivation: Accurate RUL prediction is crucial for timely battery maintenance, enhancing operational efficiency in electric applications.

Method: Proposes a signal processing pipeline for feature enhancement and a hybrid deep learning model (CNN, A-LSTM, ODE-LSTM) to capture local and long-range temporal dependencies.

Result: Achieves RMSE of 101.59, outperforming baseline methods and demonstrating robustness in transfer learning scenarios.

Conclusion: The approach shows strong potential for real-world RUL prediction, even with limited target data.

Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for
enabling timely maintenance of lithium-ion batteries, impacting the operational
efficiency of electric applications that rely on them. This paper proposes a
RUL prediction approach that leverages data from recent charge-discharge cycles
to estimate the number of remaining usable cycles. The approach introduces both
a novel signal processing pipeline and a deep learning prediction model. In the
signal preprocessing pipeline, a derived capacity feature is computed based on
current and capacity signals. Alongside original capacity, voltage and current,
these features are denoised and enhanced using statistical metrics and a
delta-based method to capture differences between the current and previous
cycles. In the prediction model, the processed features are then fed into a
hybrid deep learning architecture composed of 1D Convolutional Neural Networks
(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential
Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to
capture both local signal characteristics and long-range temporal dependencies
while modeling the continuous-time dynamics of battery degradation. The model
is further evaluated using transfer learning across different learning
strategies and target data partitioning scenarios. Results indicate that the
model maintains robust performance, even when fine-tuned on limited target
data. Experimental results on two publicly available large-scale datasets
demonstrate that the proposed method outperforms a baseline deep learning
approach and machine learning techniques, achieving an RMSE of 101.59,
highlighting its strong potential for real-world RUL prediction applications.

</details>


### [607] [Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data](https://arxiv.org/pdf/2505.16672)
*Yun-Cheng Tsai, Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: Comparative analysis of classical, hybrid, and quantum clustering methods for blockchain transaction data, showing quantum methods improve performance.


<details>
  <summary>Details</summary>
Motivation: Blockchain transaction data is complex and noisy, challenging traditional clustering algorithms.

Method: Compared three approaches: Classical K-Means, Hybrid Clustering (classical + quantum features), and Fully Quantum Clustering (self-supervised QNN).

Result: Shallow quantum circuits effectively extract non-linear representations, enhancing clustering performance.

Conclusion: Quantum methods, even with shallow circuits, outperform classical approaches for clustering blockchain data.

Abstract: Blockchain transaction data exhibits high dimensionality, noise, and
intricate feature entanglement, presenting significant challenges for
traditional clustering algorithms. In this study, we conduct a comparative
analysis of three clustering approaches: (1) Classical K-Means Clustering,
applied to pre-processed feature representations; (2) Hybrid Clustering,
wherein classical features are enhanced with quantum random features extracted
using randomly initialized quantum neural networks (QNNs); and (3) Fully
Quantum Clustering, where a QNN is trained in a self-supervised manner
leveraging a SwAV-based loss function to optimize the feature space for
clustering directly. The proposed experimental framework systematically
investigates the impact of quantum circuit depth and the number of learned
prototypes, demonstrating that even shallow quantum circuits can effectively
extract meaningful non-linear representations, significantly improving
clustering performance.

</details>


### [608] [On the Out-of-Distribution Generalization of Self-Supervised Learning](https://arxiv.org/pdf/2505.16675)
*Wenwen Qiang, Jingyao Wang, Zeen Song, Jiangmeng Li, Changwen Zheng*

Main category: cs.LG

TL;DR: The paper explains why SSL has OOD generalization, identifies spurious correlations as a limitation, and proposes a PID-based batch sampling strategy to improve OOD performance.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the OOD generalization of SSL by addressing spurious correlations learned during training.

Method: Proposes a post-intervention distribution (PID) grounded in the Structural Causal Model and a batch sampling strategy enforcing PID constraints.

Result: Theoretical analysis shows identifiability of the latent variable model, and experiments validate the sampling strategy's effectiveness on OOD tasks.

Conclusion: The proposed PID-based sampling strategy improves SSL's OOD generalization by mitigating spurious correlations.

Abstract: In this paper, we focus on the out-of-distribution (OOD) generalization of
self-supervised learning (SSL). By analyzing the mini-batch construction during
the SSL training phase, we first give one plausible explanation for SSL having
OOD generalization. Then, from the perspective of data generation and causal
inference, we analyze and conclude that SSL learns spurious correlations during
the training process, which leads to a reduction in OOD generalization. To
address this issue, we propose a post-intervention distribution (PID) grounded
in the Structural Causal Model. PID offers a scenario where the spurious
variable and label variable is mutually independent. Besides, we demonstrate
that if each mini-batch during SSL training satisfies PID, the resulting SSL
model can achieve optimal worst-case OOD performance. This motivates us to
develop a batch sampling strategy that enforces PID constraints through the
learning of a latent variable model. Through theoretical analysis, we
demonstrate the identifiability of the latent variable model and validate the
effectiveness of the proposed sampling strategy. Experiments conducted on
various downstream OOD tasks demonstrate the effectiveness of the proposed
sampling strategy.

</details>


### [609] [Learning Genomic Structure from $k$-mers](https://arxiv.org/pdf/2505.16680)
*Filip Thor, Carl Nettelblad*

Main category: cs.LG

TL;DR: A contrastive learning method is proposed to analyze genomic read data, producing embeddings that cluster sequences from the same genomic region, useful for tasks like aDNA mapping and metagenomic species identification.


<details>
  <summary>Details</summary>
Motivation: The challenge of reassembling short nucleotide reads into a full genome and the need for efficient downstream genomic analysis tools.

Method: Uses contrastive learning to train an encoder model for sequence embeddings, preserving genomic region trajectories. Incorporates domain-specific noise and supervised settings when a reference genome is available.

Result: The model performs comparably to BWA-aln for aDNA mapping and shows promise for metagenomics and large genomes like humans.

Conclusion: The method offers a scalable, general representation for genomic read data, enabling robust analysis without full genome assembly.

Abstract: Sequencing a genome to determine an individual's DNA produces an enormous
number of short nucleotide subsequences known as reads, which must be
reassembled to reconstruct the full genome. We present a method for analyzing
this type of data using contrastive learning, in which an encoder model is
trained to produce embeddings that cluster together sequences from the same
genomic region. The sequential nature of genomic regions is preserved in the
form of trajectories through this embedding space. Trained solely to reflect
the structure of the genome, the resulting model provides a general
representation of $k$-mer sequences, suitable for a range of downstream tasks
involving read data. We apply our framework to learn the structure of the $E.\
coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read
mapping and identification of structural variations. Furthermore, we illustrate
the potential of using this type of model for metagenomic species
identification. We show how incorporating a domain-specific noise model can
enhance embedding robustness, and how a supervised contrastive learning setting
can be adopted when a linear reference genome is available, by introducing a
distance thresholding parameter $\Gamma$. The model can also be trained fully
self-supervised on read data, enabling analysis without the need to construct a
full genome assembly using specialized algorithms. Small prediction heads based
on a pre-trained embedding are shown to perform on par with BWA-aln, the
current gold standard approach for aDNA mapping, in terms of accuracy and
runtime for short genomes. Given the method's favorable scaling properties with
respect to total genome size, inference using our approach is highly promising
for metagenomic applications and for mapping to genomes comparable in size to
the human genome.

</details>


### [610] [Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator](https://arxiv.org/pdf/2505.16690)
*Beier Luo, Shuoyuan Wang, Yixuan Li, Hongxin Wei*

Main category: cs.LG

TL;DR: DACA is an unsupervised method to improve confidence calibration in post-trained language models by selectively using agreement examples, avoiding over-confidence issues.


<details>
  <summary>Details</summary>
Motivation: Post-trained language models (PoLMs) often suffer from over-confidence, undermining reliability. Labeled data scarcity for calibration is a challenge.

Method: Proposes Disagreement-Aware Confidence Alignment (DACA), which aligns confidence via temperature scaling using only agreement examples, decoupling disagreement influence.

Result: Improves average ECE by up to 15.08% on benchmarks, including models like GPT-4o.

Conclusion: DACA effectively mitigates over-confidence in PoLMs by optimizing calibration parameters without labeled data.

Abstract: Post-training of large language models is essential for adapting pre-trained
language models (PLMs) to align with human preferences and downstream tasks.
While PLMs typically exhibit well-calibrated confidence, post-trained language
models (PoLMs) often suffer from over-confidence, assigning high confidence to
both correct and incorrect outputs, which can undermine reliability in critical
applications. A major obstacle in calibrating PoLMs is the scarcity of labeled
data for individual downstream tasks. To address this, we propose
Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to
optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence
calibration. Our method is motivated by the under-confidence issue caused by
prediction disagreement between the PLM and PoLM while aligning their
confidence via temperature scaling. Theoretically, the PLM's confidence
underestimates PoLM's prediction accuracy on disagreement examples, causing a
larger $\tau$ and producing under-confident predictions. DACA mitigates this by
selectively using only agreement examples for calibration, effectively
decoupling the influence of disagreement. In this manner, our method avoids an
overly large $\tau$ in temperature scaling caused by disagreement examples,
improving calibration performance. Extensive experiments demonstrate the
effectiveness of our method, improving the average ECE of open-sourced and
API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.

</details>


### [611] [An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations](https://arxiv.org/pdf/2505.16705)
*Seonghwan Park, Jueun Mun, Donghyun Oh, Namhoon Lee*

Main category: cs.LG

TL;DR: CBMs' interpretability is compromised by noisy annotations, impairing performance and intervention. A two-stage framework mitigates this by stabilizing training and correcting uncertain concepts during inference.


<details>
  <summary>Details</summary>
Motivation: To understand and address the impact of noisy annotations on CBMs' interpretability, performance, and intervention effectiveness.

Method: Proposes a two-stage framework: sharpness-aware minimization during training and entropy-based concept correction during inference.

Result: Noise significantly harms CBMs, but the framework improves robustness and preserves interpretability.

Conclusion: The study provides a principled solution to maintain CBMs' interpretability and resilience despite noisy supervision.

Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing
predictions into human interpretable concepts. Yet the annotations used for
training CBMs that enable this transparency are often noisy, and the impact of
such corruption is not well understood. In this study, we present the first
systematic study of noise in CBMs and show that even moderate corruption
simultaneously impairs prediction performance, interpretability, and the
intervention effectiveness. Our analysis identifies a susceptible subset of
concepts whose accuracy declines far more than the average gap between noisy
and clean supervision and whose corruption accounts for most performance loss.
To mitigate this vulnerability we propose a two-stage framework. During
training, sharpness-aware minimization stabilizes the learning of
noise-sensitive concepts. During inference, where clean labels are unavailable,
we rank concepts by predictive entropy and correct only the most uncertain
ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and
extensive ablations elucidate why sharpness-aware training confers robustness
and why uncertainty reliably identifies susceptible concepts, providing a
principled basis that preserves both interpretability and resilience in the
presence of noise.

</details>


### [612] [Training Long-Context LLMs Efficiently via Chunk-wise Optimization](https://arxiv.org/pdf/2505.16710)
*Wenhao Li, Yuxin Zhang, Gen Luo, Daohai Yu, Rongrong Ji*

Main category: cs.LG

TL;DR: SeCO and SpaCO are memory-efficient training methods for long-context LLMs, enabling longer sequences and faster training by chunking inputs and optimizing gradient propagation.


<details>
  <summary>Details</summary>
Motivation: High training costs of long-context LLMs hinder customization; SeCO and SpaCO address this by reducing memory and computational overhead.

Method: SeCO partitions inputs into chunks with localized backpropagation; SpaCO adds selective gradient propagation and compensation for efficiency.

Result: SeCO extends sequence length to 16K tokens; SpaCO speeds up training by 3x compared to SeCO.

Conclusion: SeCO and SpaCO make long-context LLMs more practical, with open-sourced code for broader adoption.

Abstract: While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.

</details>


### [613] [Advancing Brainwave Modeling with a Codebook-Based Foundation Model](https://arxiv.org/pdf/2505.16724)
*Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: LaBraM++ is an improved EEG pre-trained model that outperforms its predecessor and other models by better capturing neural oscillations, enhancing BCI and healthcare applications.


<details>
  <summary>Details</summary>
Motivation: Existing EEG pre-trained models often fail to fully utilize neural oscillations, limiting their performance and generalizability in BCI tasks.

Method: LaBraM++ introduces principled architectural improvements based on robust signal processing to enhance representational capacity.

Result: LaBraM++ achieves superior performance and training efficiency, outperforming its predecessor and competing with other open-source models.

Conclusion: LaBraM++ is a promising foundation for future advancements in large-scale EEG models.

Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models
have shown great promise, driving progress in Brain-Computer Interfaces (BCIs)
and healthcare applications. However, despite their success, many existing
pre-trained models have struggled to fully capture the rich information content
of neural oscillations, a limitation that fundamentally constrains their
performance and generalizability across diverse BCI tasks. This limitation is
frequently rooted in suboptimal architectural design choices which constrain
their representational capacity. In this work, we introduce LaBraM++, an
enhanced Large Brainwave Foundation Model (LBM) that incorporates principled
improvements grounded in robust signal processing foundations. LaBraM++
demonstrates substantial gains across a variety of tasks, consistently
outperforming its originally-based architecture and achieving competitive
results when compared to other open-source LBMs. Its superior performance and
training efficiency highlight its potential as a strong foundation for future
advancements in LBMs.

</details>


### [614] [Masked Conditioning for Deep Generative Models](https://arxiv.org/pdf/2505.16725)
*Phillip Mueller, Jannik Wiese, Sebastian Mueller, Lars Mikelsons*

Main category: cs.LG

TL;DR: A novel masked-conditioning approach for generative models to handle sparse, mixed-type data in engineering tasks, integrated into efficient VAE and latent diffusion models, and coupled with large pretrained models for improved quality.


<details>
  <summary>Details</summary>
Motivation: Engineering datasets are often small, sparsely labeled, and mixed-type, with limited computational resources, hindering generative model adoption.

Method: Masked-conditioning during training to simulate sparse conditions, flexible embeddings for mixed-type data, and integration into VAE and latent diffusion models.

Result: Demonstrated applicability on 2D point clouds and images, with improved generation quality by coupling small models with large pretrained models.

Conclusion: The approach enables generative models to work effectively with sparse, mixed-type data in resource-constrained engineering applications.

Abstract: Datasets in engineering domains are often small, sparsely labeled, and
contain numerical as well as categorical conditions. Additionally.
computational resources are typically limited in practical applications which
hinders the adoption of generative models for engineering tasks. We introduce a
novel masked-conditioning approach, that enables generative models to work with
sparse, mixed-type data. We mask conditions during training to simulate sparse
conditions at inference time. For this purpose, we explore the use of various
sparsity schedules that show different strengths and weaknesses. In addition,
we introduce a flexible embedding that deals with categorical as well as
numerical conditions. We integrate our method into an efficient variational
autoencoder as well as a latent diffusion model and demonstrate the
applicability of our approach on two engineering-related datasets of 2D point
clouds and images. Finally, we show that small models trained on limited data
can be coupled with large pretrained foundation models to improve generation
quality while retaining the controllability induced by our conditioning scheme.

</details>


### [615] [Sequential Monte Carlo for Policy Optimization in Continuous POMDPs](https://arxiv.org/pdf/2505.16732)
*Hany Abdulsamad, Sahel Iqbal, Simo Särkkä*

Main category: cs.LG

TL;DR: A novel policy optimization framework for continuous POMDPs balances exploration and exploitation by modeling policy learning as probabilistic inference, using a nested SMC algorithm for efficient optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing exploration and exploitation in partially observable environments without relying on extrinsic bonuses or heuristics.

Method: Introduces a probabilistic inference approach in a non-Markovian Feynman-Kac model and uses a nested SMC algorithm to estimate policy gradients.

Result: Demonstrates effectiveness in continuous POMDP benchmarks where existing methods fail under uncertainty.

Conclusion: The framework successfully integrates exploration and exploitation in POMDPs, outperforming traditional methods.

Abstract: Optimal decision-making under partial observability requires agents to
balance reducing uncertainty (exploration) against pursuing immediate
objectives (exploitation). In this paper, we introduce a novel policy
optimization framework for continuous partially observable Markov decision
processes (POMDPs) that explicitly addresses this challenge. Our method casts
policy learning as probabilistic inference in a non-Markovian Feynman--Kac
model that inherently captures the value of information gathering by
anticipating future observations, without requiring extrinsic exploration
bonuses or handcrafted heuristics. To optimize policies under this model, we
develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently
estimates a history-dependent policy gradient under samples from the optimal
trajectory distribution induced by the POMDP. We demonstrate the effectiveness
of our algorithm across standard continuous POMDP benchmarks, where existing
methods struggle to act under uncertainty.

</details>


### [616] [Forward-only Diffusion Probabilistic Models](https://arxiv.org/pdf/2505.16733)
*Ziwei Luo, Fredrik K. Gustafsson, Jens Sjölund, Thomas B. Schön*

Main category: cs.LG

TL;DR: FoD introduces a forward-only diffusion approach for generative modeling, simplifying the process by using a single forward diffusion and achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion models rely on complex forward-backward schemes; FoD aims to simplify this with a single forward process while maintaining effectiveness.

Method: FoD uses a state-dependent linear stochastic differential equation with mean-reverting terms, trained via stochastic flow matching for efficient sampling.

Result: FoD performs competitively in image-conditioned and unconditional generation tasks.

Conclusion: FoD offers a simpler, efficient, and effective alternative to traditional diffusion models.

Abstract: This work presents a forward-only diffusion (FoD) approach for generative
modelling. In contrast to traditional diffusion models that rely on a coupled
forward-backward diffusion scheme, FoD directly learns data generation through
a single forward diffusion process, yielding a simple yet efficient generative
framework. The core of FoD is a state-dependent linear stochastic differential
equation that involves a mean-reverting term in both the drift and diffusion
functions. This mean-reversion property guarantees the convergence to clean
data, naturally simulating a stochastic interpolation between source and target
distributions. More importantly, FoD is analytically tractable and is trained
using a simple stochastic flow matching objective, enabling a few-step
non-Markov chain sampling during inference. The proposed FoD model, despite its
simplicity, achieves competitive performance on various image-conditioned
(e.g., image restoration) and unconditional generation tasks, demonstrating its
effectiveness in generative modelling. Our code is available at
https://github.com/Algolzw/FoD.

</details>


### [617] [Maximum Total Correlation Reinforcement Learning](https://arxiv.org/pdf/2505.16734)
*Bang You, Puze Liu, Huaping Liu, Jan Peters, Oleg Arenz*

Main category: cs.LG

TL;DR: The paper explores promoting simple behavior in reinforcement learning by maximizing total correlation in trajectories, leading to more robust and generalizable policies.


<details>
  <summary>Details</summary>
Motivation: To enhance generalizability and robustness by focusing on simplicity in behavior throughout the episode, supplementing existing techniques like regularization and sparse rewards.

Method: Introduces a modified RL problem maximizing total correlation in trajectories, with a practical algorithm optimizing policies and state representations via a lower-bound approximation.

Result: In simulated robot environments, the method produces periodic, compressible trajectories, improving robustness to noise and dynamics changes while enhancing task performance.

Conclusion: Maximizing total correlation in RL trajectories effectively promotes simplicity, leading to more robust and performant policies.

Abstract: Simplicity is a powerful inductive bias. In reinforcement learning,
regularization is used for simpler policies, data augmentation for simpler
representations, and sparse reward functions for simpler objectives, all that,
with the underlying motivation to increase generalizability and robustness by
focusing on the essentials. Supplementary to these techniques, we investigate
how to promote simple behavior throughout the episode. To that end, we
introduce a modification of the reinforcement learning problem that
additionally maximizes the total correlation within the induced trajectories.
We propose a practical algorithm that optimizes all models, including policy
and state representation, based on a lower-bound approximation. In simulated
robot environments, our method naturally generates policies that induce
periodic and compressible trajectories, and that exhibit superior robustness to
noise and changes in dynamics compared to baseline methods, while also
improving performance in the original tasks.

</details>


### [618] [Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?](https://arxiv.org/pdf/2505.16736)
*Nicolas Keriven*

Main category: cs.LG

TL;DR: The paper examines oversmoothing in GNNs from an optimization perspective, introducing backward oversmoothing and showing its role in creating spurious stationary points.


<details>
  <summary>Details</summary>
Motivation: Oversmoothing in GNNs is a known issue, but practical avoidance is rare. The paper investigates why, focusing on backward oversmoothing and its interaction with forward smoothing.

Method: Analyzes backward oversmoothing, its linear nature despite non-linear activations, and its impact on gradient computation. Theoretical proofs and counter-examples (e.g., MLPs) are used.

Result: Backward oversmoothing leads to spurious stationary points, causing near-zero gradients while loss remains high. Deep GNNs are uniquely affected.

Conclusion: The study advances understanding of GNN optimization challenges, highlighting backward oversmoothing as a key issue.

Abstract: Oversmoothing has long been identified as a major limitation of Graph Neural
Networks (GNNs): input node features are smoothed at each layer and converge to
a non-informative representation, if the weights of the GNN are sufficiently
bounded. This assumption is crucial: if, on the contrary, the weights are
sufficiently large, then oversmoothing may not happen. Theoretically, GNN could
thus learn to not oversmooth. However it does not really happen in practice,
which prompts us to examine oversmoothing from an optimization point of view.
In this paper, we analyze backward oversmoothing, that is, the notion that
backpropagated errors used to compute gradients are also subject to
oversmoothing from output to input. With non-linear activation functions, we
outline the key role of the interaction between forward and backward smoothing.
Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit
many spurious stationary points: as soon as the last layer is trained, the
whole GNN is at a stationary point. As a result, we can exhibit regions where
gradients are near-zero while the loss stays high. The proof relies on the fact
that, unlike forward oversmoothing, backward errors are subjected to a linear
oversmoothing even in the presence of non-linear activation function, such that
the average of the output error plays a key role. Additionally, we show that
this phenomenon is specific to deep GNNs, and exhibit counter-example
Multi-Layer Perceptron. This paper is a step toward a more complete
comprehension of the optimization landscape specific to GNNs.

</details>


### [619] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/pdf/2505.16737)
*Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun*

Main category: cs.LG

TL;DR: The paper addresses safety degradation in fine-tuned LLMs and proposes a safety-aware probing (SAP) framework to mitigate risks while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs on benign data can still compromise safety, raising concerns despite pre-training safety alignment.

Method: Introduces SAP, a framework that integrates safety-aware probes into gradient propagation to identify and mitigate safety risks during fine-tuning.

Result: SAP reduces harmfulness below standard fine-tuning levels while achieving comparable task performance.

Conclusion: SAP effectively preserves model safety during fine-tuning without sacrificing performance, offering a practical solution to safety degradation.

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [620] [Meta-reinforcement learning with minimum attention](https://arxiv.org/pdf/2505.16741)
*Pilhwa Lee, Shashank Gupta*

Main category: cs.LG

TL;DR: Minimum attention in RL improves adaptation and efficiency, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Emulate biological control (e.g., motor learning) and enhance RL performance with minimal action principles.

Method: Model-based meta-learning with minimum attention, combining ensemble-based model learning and gradient-based meta-policy learning.

Result: Outperforms state-of-the-art RL algorithms in fast adaptation, variance reduction, and energy efficiency.

Conclusion: Minimum attention is effective for RL, offering advantages in adaptation, stability, and efficiency.

Abstract: Minimum attention applies the least action principle in the changes of
control concerning state and time, first proposed by Brockett. The involved
regularization is highly relevant in emulating biological control, such as
motor learning. We apply minimum attention in reinforcement learning (RL) as
part of the rewards and investigate its connection to meta-learning and
stabilization. Specifically, model-based meta-learning with minimum attention
is explored in high-dimensional nonlinear dynamics. Ensemble-based model
learning and gradient-based meta-policy learning are alternately performed.
Empirically, we show that the minimum attention does show outperforming
competence in comparison to the state-of-the-art algorithms in model-free and
model-based RL, i.e., fast adaptation in few shots and variance reduction from
the perturbations of the model and environment. Furthermore, the minimum
attention demonstrates the improvement in energy efficiency.

</details>


### [621] [Revenue Optimization with Price-Sensitive and Interdependent Demand](https://arxiv.org/pdf/2505.16748)
*Julien Laasri, Marc Revol*

Main category: cs.LG

TL;DR: The paper focuses on optimizing airline ticket pricing and quantity decisions to maximize revenue, using predefined demand data and price options from Air France.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of revenue maximization in airline ticket sales by leveraging optimization techniques on given demand and pricing constraints.

Method: Utilizes predefined demand data and price options, focusing on optimization to select the best pricing strategy for each product.

Result: Aims to achieve maximum revenue for direct flights by optimizing price selections from predetermined options.

Conclusion: The approach demonstrates how optimization can enhance revenue management in airline ticket sales under fixed constraints.

Abstract: As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],
Revenue Management aims to maximize an organization's revenue by considering
three types of decision categories: structural, pricing, and quantity. In this
document, our primary focus will be on decisions related to pricing and
quantity for the sale of airline tickets on a direct flight over a certain
number of time periods. More specifically, we will only focus on the
optimization aspect of this problem. We will assume the demand data to be
given, since Air France estimates it beforehand using real data. Similarly, we
assume all price options to be predetermined by Air France's algorithms and
verified by their analysts. Our objective will be to maximize the revenue of a
direct flight by choosing the prices for each product from the predefined set
of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur
ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un
organisme \`a partir de trois types de cat\'egories de d\'ecision :
structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons
principalement aux d\'ecisions de type prix et quantit\'e pour la vente de
billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.
Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du
probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car
elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees
r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on
nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se
basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es
par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct
en choisissant les prix de chaque produit parmi ceux impos\'es.

</details>


### [622] [PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects](https://arxiv.org/pdf/2505.16754)
*Hannah Markgraf, Michael Eichelbeck, Daria Cappey, Selin Demirtürk, Yara Schattschneider, Matthias Althoff*

Main category: cs.LG

TL;DR: PyTupli is a Python tool for creating, storing, and sharing offline RL datasets, addressing infrastructure gaps in the field.


<details>
  <summary>Details</summary>
Motivation: Offline RL lacks standardized solutions for managing and sharing datasets, hindering research scalability and collaboration.

Method: PyTupli provides a client library for dataset handling, fine-grained filtering, and a containerized server for secure deployment.

Result: The tool enables efficient dataset curation, sharing, and secure access, supporting collaborative offline RL research.

Conclusion: PyTupli enhances reproducibility and scalability in offline RL by streamlining dataset infrastructure.

Abstract: Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.

</details>


### [623] [Multi-Output Gaussian Processes for Graph-Structured Data](https://arxiv.org/pdf/2505.16755)
*Ayano Nakai-Kasai, Tadashi Wadayama*

Main category: cs.LG

TL;DR: The paper proposes a multi-output Gaussian process (MOGP) regression method for graph-structured data, capturing vertex and data correlations, with flexible kernel design and broad applicability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing Gaussian process methods for graph-structured data by enabling more flexible data configurations, model selection, and inference scenarios.

Method: The method is based on MOGP, allowing expressive kernel design and generalization of existing approaches.

Result: The proposed method outperforms existing techniques, as validated by experiments on synthetic and real data.

Conclusion: The MOGP-based regression method offers a versatile and powerful framework for graph-structured data analysis.

Abstract: Graph-structured data is a type of data to be obtained associated with a
graph structure where vertices and edges describe some kind of data
correlation. This paper proposes a regression method on graph-structured data,
which is based on multi-output Gaussian processes (MOGP), to capture both the
correlation between vertices and the correlation between associated data. The
proposed formulation is built on the definition of MOGP. This allows it to be
applied to a wide range of data configurations and scenarios. Moreover, it has
high expressive capability due to its flexibility in kernel design. It includes
existing methods of Gaussian processes for graph-structured data as special
cases and is possible to remove restrictions on data configurations, model
selection, and inference scenarios in the existing methods. The performance of
extensions achievable by the proposed formulation is evaluated through computer
experiments with synthetic and real data.

</details>


### [624] [FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting](https://arxiv.org/pdf/2505.16786)
*Fares B. Mehouachi, Saif Eddin Jabari*

Main category: cs.LG

TL;DR: FlowMixer is a neural architecture using constrained matrix operations for interpretable spatiotemporal modeling, combining statistical learning and dynamical systems theory.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between statistical learning and dynamical systems theory for interpretable spatiotemporal forecasting.

Method: Incorporates non-negative matrix mixing layers in a reversible framework, enabling algebraic manipulation of predictions without retraining.

Result: Demonstrates robust long-horizon forecasting and effective modeling of physical phenomena like chaotic attractors.

Conclusion: Architectural constraints can improve both predictive performance and interpretability in neural forecasting.

Abstract: We introduce FlowMixer, a neural architecture that leverages constrained
matrix operations to model structured spatiotemporal patterns. At its core,
FlowMixer incorporates non-negative matrix mixing layers within a reversible
mapping framework-applying transforms before mixing and their inverses
afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode
framework that bridges statistical learning with dynamical systems theory,
providing interpretable spatiotemporal patterns and facilitating direct
algebraic manipulation of prediction horizons without retraining. Extensive
experiments across diverse domains demonstrate FlowMixer's robust long-horizon
forecasting capabilities while effectively modeling physical phenomena such as
chaotic attractors and turbulent flows. These results suggest that
architectural constraints can simultaneously enhance predictive performance and
mathematical interpretability in neural forecasting systems.

</details>


### [625] [Learning Flexible Forward Trajectories for Masked Molecular Diffusion](https://arxiv.org/pdf/2505.16790)
*Hyunjin Seo, Taewon Kim, Sihyun Yu, SungSoo Ahn*

Main category: cs.LG

TL;DR: MDMs underperform in molecular generation due to state-clashing. MELD, a novel method with element-wise noise scheduling, improves performance significantly.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of masked diffusion models (MDMs) in molecular generation and address their underperformance due to state-clashing.

Method: Propose Masked Element-wise Learnable Diffusion (MELD), which uses per-element corruption trajectories and a parameterized noise scheduling network to avoid state-clashing.

Result: MELD boosts chemical validity from 15% to 93% on ZINC250K and achieves state-of-the-art property alignment in conditional generation.

Conclusion: MELD effectively mitigates state-clashing in MDMs, enhancing molecular generation quality and performance.

Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling
discrete data, while their potential in molecular generation remains
underexplored. In this work, we explore their potential and introduce the
surprising result that naively applying standards MDMs severely degrades the
performance. We identify the critical cause of this issue as a state-clashing
problem-where the forward diffusion of distinct molecules collapse into a
common state, resulting in a mixture of reconstruction targets that cannot be
learned using typical reverse diffusion process with unimodal predictions. To
mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that
orchestrates per-element corruption trajectories to avoid collision between
distinct molecular graphs. This is achieved through a parameterized noise
scheduling network that assigns distinct corruption rates to individual graph
elements, i.e., atoms and bonds. Extensive experiments on diverse molecular
benchmarks reveal that MELD markedly enhances overall generation quality
compared to element-agnostic noise scheduling, increasing the chemical validity
of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves
state-of-the-art property alignment in conditional generation tasks.

</details>


### [626] [Cohort-Based Active Modality Acquisition](https://arxiv.org/pdf/2505.16791)
*Tillmann Rheude, Roland Eils, Benjamin Wild*

Main category: cs.LG

TL;DR: The paper introduces CAMA, a method for prioritizing samples for additional modality acquisition in multimodal ML, using generative imputation and discriminative modeling to optimize resource use.


<details>
  <summary>Details</summary>
Motivation: Real-world ML often lacks complete multimodal data, and acquiring missing modalities is costly. Existing methods don't adequately address test-time and cohort-based acquisition.

Method: CAMA combines generative imputation and discriminative modeling to estimate the benefit of acquiring missing modalities, with upper-bound heuristics for benchmarking.

Result: Experiments show CAMA outperforms unimodal, entropy-based, and random acquisition strategies in guiding modality acquisition.

Conclusion: CAMA effectively optimizes cohort-level modality acquisition, improving resource utilization in constrained settings.

Abstract: Real-world machine learning applications often involve data from multiple
modalities that must be integrated effectively to make robust predictions.
However, in many practical settings, not all modalities are available for every
sample, and acquiring additional modalities can be costly. This raises the
question: which samples should be prioritized for additional modality
acquisition when resources are limited? While prior work has explored
individual-level acquisition strategies and training-time active learning
paradigms, test-time and cohort-based acquisition remain underexplored despite
their importance in many real-world settings. We introduce Cohort-based Active
Modality Acquisition (CAMA), a novel test-time setting to formalize the
challenge of selecting which samples should receive additional modalities. We
derive acquisition strategies that leverage a combination of generative
imputation and discriminative modeling to estimate the expected benefit of
acquiring missing modalities based on common evaluation metrics. We also
introduce upper-bound heuristics that provide performance ceilings to benchmark
acquisition strategies. Experiments on common multimodal datasets demonstrate
that our proposed imputation-based strategies can more effectively guide the
acquisition of new samples in comparison to those relying solely on unimodal
information, entropy guidance, and random selections. Our work provides an
effective solution for optimizing modality acquisition at the cohort level,
enabling better utilization of resources in constrained settings.

</details>


### [627] [A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents](https://arxiv.org/pdf/2505.16801)
*Eleftherios Kalafatis, Konstantinos Mitsis, Konstantia Zarkogianni, Maria Athanasiou, Konstantina Nikita*

Main category: cs.LG

TL;DR: The paper proposes a DRL-based framework to evaluate PCG in SGs, showing better performance with genetic algorithm NPCs over random ones.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing PCG's impact in SGs, aiming for personalized player experiences.

Method: Uses DRL game testing agents on three PCG versions (random and genetic algorithm-based NPCs) in a card game SG.

Result: DRL agents with genetic algorithm NPCs (Versions 2 & 3) outperformed random NPCs (Version 1) in win rate (97% vs. 94%) and training time.

Conclusion: The framework effectively evaluates PCG in SGs, with genetic algorithms enhancing performance.

Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content
generation (PCG) in the development process as a means of offering personalized
and enhanced player experience. However, the development of a framework to
assess the impact of PCG techniques when integrated into SGs remains
particularly challenging. This study proposes a methodology for automated
evaluation of PCG integration in SGs, incorporating deep reinforcement learning
(DRL) game testing agents. To validate the proposed framework, a previously
introduced SG featuring card game mechanics and incorporating three different
versions of PCG for nonplayer character (NPC) creation has been deployed.
Version 1 features random NPC creation, while versions 2 and 3 utilize a
genetic algorithm approach. These versions are used to test the impact of
different dynamic SG environments on the proposed framework's agents. The
obtained results highlight the superiority of the DRL game testing agents
trained on Versions 2 and 3 over those trained on Version 1 in terms of win
rate (i.e. number of wins per played games) and training time. More
specifically, within the execution of a test emulating regular gameplay, both
Versions 2 and 3 peaked at a 97% win rate and achieved statistically
significant higher (p=0009) win rates compared to those achieved in Version 1
that peaked at 94%. Overall, results advocate towards the proposed framework's
capability to produce meaningful data for the evaluation of procedurally
generated content in SGs.

</details>


### [628] [Contextual Learning for Stochastic Optimization](https://arxiv.org/pdf/2505.16829)
*Anna Heuser, Thomas Kesselheim*

Main category: cs.LG

TL;DR: The paper introduces learning from contextual value distributions, using convex surrogate loss to minimize Lévy distance, and derives polynomial sample complexity bounds for stochastic optimization problems.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by stochastic optimization, aiming to learn from samples of contextual value distributions to solve optimization problems with unknown distributions.

Method: A convex surrogate loss is minimized to learn empirical distributions for each context, ensuring small Lévy distance to the true distributions.

Result: Sample complexity bounds are derived for learning ϵ-optimal policies, shown to be polynomial for strongly monotone and stable optimization problems.

Conclusion: The approach is applicable to problems like Single-item Revenue Maximization, Pandora's Box, and Optimal Stopping, demonstrating practical utility.

Abstract: Motivated by stochastic optimization, we introduce the problem of learning
from samples of contextual value distributions. A contextual value distribution
can be understood as a family of real-valued distributions, where each sample
consists of a context $x$ and a random variable drawn from the corresponding
real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn
an empirical distribution $D'_x$ for each context, ensuring a small L\'evy
distance to $D_x$. We apply this result to obtain the sample complexity bounds
for the learning of an $\epsilon$-optimal policy for stochastic optimization
problems defined on an unknown contextual value distribution. The sample
complexity is shown to be polynomial for the general case of strongly monotone
and stable optimization problems, including Single-item Revenue Maximization,
Pandora's Box and Optimal Stopping.

</details>


### [629] [Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning](https://arxiv.org/pdf/2505.16833)
*Alihan Hüyük, Finale Doshi-Velez*

Main category: cs.LG

TL;DR: Strategic link scores quantify dependencies between actions in long-term planning, aiding in explaining RL agents, improving decision systems, and analyzing non-RL agents.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify dependencies between actions in long-term planning, where some actions sacrifice short-term gains for future benefits.

Method: Introduce strategic link scores to measure the drop in likelihood of a decision if a follow-up is unavailable. Apply these scores to explain RL agents, improve decision systems, and analyze non-RL agents.

Result: Strategic link scores effectively explain RL agents, enhance decision support systems, and characterize non-RL agent planning processes.

Conclusion: Strategic link scores provide a valuable tool for analyzing and improving long-term planning strategies across various applications.

Abstract: Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.

</details>


### [630] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/pdf/2505.16850)
*Tajamul Ashraf, Mohammed Mohsen Peerzada, Moloud Abdar, Yutong Xie, Yuyin Zhou, Xiaofeng Liu, Iqra Altaf Gillani, Janibul Bashir*

Main category: cs.LG

TL;DR: ATR-Bench introduces a unified framework for evaluating Federated Learning (FL) across Adaptation, Trust, and Reasoning dimensions, addressing gaps in standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation in FL hampers progress and fair comparison of methods.

Method: ATR-Bench benchmarks methods and datasets for Adaptation and Trust, while providing literature-driven insights for Reasoning.

Result: The framework offers systematic evaluation and real-world relevance, with a public codebase and curated repository.

Conclusion: ATR-Bench advances FL research by enabling holistic and standardized assessment.

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


### [631] [Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only](https://arxiv.org/pdf/2505.16856)
*Wei Xiao, Jiacheng Liu, Zifeng Zhuang, Runze Suo, Shangke Lyu, Donglin Wang*

Main category: cs.LG

TL;DR: PORL enables efficient online RL fine-tuning using only pre-trained policies, avoiding reliance on pre-trained Q-functions and addressing conservatism issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods require pre-trained Q-functions, limiting applicability and hindering exploration due to conservatism in offline RL.

Method: PORL initializes Q-functions from scratch during online phase to avoid pessimism, leveraging only pre-trained policies.

Result: PORL achieves competitive performance with advanced offline-to-online RL and online RL methods, and enables direct fine-tuning of BC policies.

Conclusion: PORL provides a novel, efficient approach for online RL fine-tuning without pre-trained Q-functions, expanding applicability.

Abstract: Improving the performance of pre-trained policies through online
reinforcement learning (RL) is a critical yet challenging topic. Existing
online RL fine-tuning methods require continued training with offline
pretrained Q-functions for stability and performance. However, these offline
pretrained Q-functions commonly underestimate state-action pairs beyond the
offline dataset due to the conservatism in most offline RL methods, which
hinders further exploration when transitioning from the offline to the online
setting. Additionally, this requirement limits their applicability in scenarios
where only pre-trained policies are available but pre-trained Q-functions are
absent, such as in imitation learning (IL) pre-training. To address these
challenges, we propose a method for efficient online RL fine-tuning using
solely the offline pre-trained policy, eliminating reliance on pre-trained
Q-functions. We introduce PORL (Policy-Only Reinforcement Learning
Fine-Tuning), which rapidly initializes the Q-function from scratch during the
online phase to avoid detrimental pessimism. Our method not only achieves
competitive performance with advanced offline-to-online RL algorithms and
online RL approaches that leverage data or policies prior, but also pioneers a
new path for directly fine-tuning behavior cloning (BC) policies.

</details>


### [632] [Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft](https://arxiv.org/pdf/2505.16857)
*Ertuğrul Keçeci, Müjde Güzelkaya, Tufan Kumbasar*

Main category: cs.LG

TL;DR: The paper introduces IC-SYSID, a federated learning algorithm for System Identification (SYSID), using incremental clustering (ClusterCraft) and merging (ClusterMerge) to handle multiple data sources without prior knowledge. It improves stability and performance in real-world SYSID tasks.


<details>
  <summary>Details</summary>
Motivation: To solve the SYSID problem in federated learning without relying on prior knowledge of datasets, addressing challenges like instability and scalability.

Method: IC-SYSID employs ClusterCraft for incremental clustering, ClusterMerge to reduce redundant clusters, regularization for stability, and mini-batch deep learning for large datasets.

Result: IC-SYSID achieves high SYSID performance and prevents unstable clusters in real-world vehicle dynamics learning.

Conclusion: IC-SYSID effectively addresses SYSID challenges in federated learning, offering a scalable and stable solution.

Abstract: This paper addresses the System Identification (SYSID) problem within the
framework of federated learning. We introduce a novel algorithm, Incremental
Clustering-based federated learning method for SYSID (IC-SYSID), designed to
tackle SYSID challenges across multiple data sources without prior knowledge.
IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to
eliminate the dependency on the prior knowledge of the dataset. CC starts with
a single cluster model and assigns similar local workers to the same clusters
by dynamically increasing the number of clusters. To reduce the number of
clusters generated by CC, we introduce ClusterMerge, where similar cluster
models are merged. We also introduce enhanced ClusterCraft to reduce the
generation of similar cluster models during the training. Moreover, IC-SYSID
addresses cluster model instability by integrating a regularization term into
the loss function and initializing cluster models with scaled Glorot
initialization. It also utilizes a mini-batch deep learning approach to manage
large SYSID datasets during local training. Through the experiments conducted
on a real-world representing SYSID problem, where a fleet of vehicles
collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high
SYSID performance while preventing the learning of unstable clusters.

</details>


### [633] [GCAL: Adapting Graph Models to Evolving Domain Shifts](https://arxiv.org/pdf/2505.16860)
*Ziyue Qiao, Qianyi Cai, Hao Dong, Jiawei Gu, Pengyang Wang, Meng Xiao, Xiao Luo, Hui Xiong*

Main category: cs.LG

TL;DR: GCAL introduces a bilevel optimization strategy for graph domain adaptation, addressing continuous domain shifts and catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail in handling evolving, multiple OOD graphs due to single-step adaptation limitations.

Method: GCAL uses a bilevel approach: 'adapt' phase for fine-tuning and 'generate memory' phase for condensing graphs into memories.

Result: GCAL outperforms existing methods in adaptability and knowledge retention.

Conclusion: GCAL effectively enhances sustainability and adaptability in graph domain adaptation.

Abstract: This paper addresses the challenge of graph domain adaptation on evolving,
multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation
methods are confined to single-step adaptation, making them ineffective in
handling continuous domain shifts and prone to catastrophic forgetting. This
paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed
to enhance model sustainability and adaptability across various graph domains.
GCAL employs a bilevel optimization strategy. The "adapt" phase uses an
information maximization approach to fine-tune the model with new graph domains
while re-adapting past memories to mitigate forgetting. Concurrently, the
"generate memory" phase, guided by a theoretical lower bound derived from
information bottleneck theory, involves a variational memory graph generation
module to condense original graphs into memories. Extensive experimental
evaluations demonstrate that GCAL substantially outperforms existing methods in
terms of adaptability and knowledge retention.

</details>


### [634] [A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams](https://arxiv.org/pdf/2505.16872)
*Mohammed Al-Qudah, Fadi AlMahamid*

Main category: cs.LG

TL;DR: The paper evaluates how preprocessing steps (normalization, transformation, feature selection) affect three ML models (RNN-LSTM, ANN, GBoosting) for IoT anomaly detection, finding GBoosting most accurate overall.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of systematic research on how preprocessing steps interact with ML models for IoT anomaly detection.

Method: Proposes a multi-step evaluation framework to assess preprocessing impacts on RNN-LSTM, ANN, and GBoosting using the IoTID20 dataset.

Result: GBoosting performs best across preprocessing configurations, RNN-LSTM benefits from z-score normalization, and ANN excels in recall for unsupervised scenarios.

Conclusion: The framework provides actionable insights to improve anomaly detection in IoT by analyzing preprocessing and ML model interactions.

Abstract: The rapid expansion of Internet of Things (IoT) devices has introduced
critical security challenges, underscoring the need for accurate anomaly
detection. Although numerous studies have proposed machine learning (ML)
methods for this purpose, limited research systematically examines how
different preprocessing steps--normalization, transformation, and feature
selection--interact with distinct model architectures. To address this gap,
this paper presents a multi-step evaluation framework assessing the combined
impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder
neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the
IoTID20 dataset shows that GBoosting consistently delivers superior accuracy
across preprocessing configurations, while RNN-LSTM shows notable gains with
z-score normalization and autoencoders excel in recall, making them well-suited
for unsupervised scenarios. By offering a structured analysis of preprocessing
decisions and their interplay with various ML techniques, the proposed
framework provides actionable guidance to enhance anomaly detection performance
in IoT environments.

</details>


### [635] [Structure-Aligned Protein Language Model](https://arxiv.org/pdf/2505.16896)
*Can Chen, David Heurtel-Depeiges, Robert M. Vernon, Christopher James Langmead, Yoshua Bengio, Quentin Fournier*

Main category: cs.LG

TL;DR: The paper introduces a dual-task framework to integrate structural knowledge into protein language models (pLMs) using contrastive learning and structural token prediction, improving performance on tasks like contact prediction.


<details>
  <summary>Details</summary>
Motivation: Current pLMs lack structural knowledge, limiting their effectiveness in biological applications.

Method: Combines latent-level contrastive learning (aligning pLM and pGNN representations) and a physical-level task (predicting structural tokens). Includes a residue loss selection module for reliable learning.

Result: Notable performance gains, e.g., 12.7% increase in ESM2 contact prediction.

Conclusion: The framework successfully enriches pLMs with structural knowledge, enhancing their utility for biological tasks.

Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases
excel at various downstream tasks but lack the structural knowledge essential
for many biological applications. To address this, we integrate structural
insights from pre-trained protein graph neural networks (pGNNs) into pLMs
through a latent-level contrastive learning task. This task aligns residue
representations from pLMs with those from pGNNs across multiple proteins,
enriching pLMs with inter-protein structural knowledge. Additionally, we
incorporate a physical-level task that infuses intra-protein structural
knowledge by optimizing pLMs to predict structural tokens. The proposed
dual-task framework effectively incorporates both inter-protein and
intra-protein structural knowledge into pLMs. Given the variability in the
quality of protein structures in PDB, we further introduce a residue loss
selection module, which uses a small model trained on high-quality structures
to select reliable yet challenging residue losses for the pLM to learn.
Applying our structure alignment method to the state-of-the-art ESM2 and
AMPLIFY results in notable performance gains across a wide range of tasks,
including a 12.7% increase in ESM2 contact prediction. The data, code, and
resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.

</details>


### [636] [Unsupervised Prompting for Graph Neural Networks](https://arxiv.org/pdf/2505.16903)
*Peyman Baghershahi, Sourav Medya*

Main category: cs.LG

TL;DR: The paper introduces an unsupervised prompting method for GNNs to address covariate shift without labeled data or parameter updates, outperforming existing labeled-data-dependent methods.


<details>
  <summary>Details</summary>
Motivation: Existing GNN prompting methods require labeled data and fine-tuning, while LLMs show success with minimal or no labeled data. This work aims to bridge this gap for GNNs.

Method: Proposes an unsupervised prompting method using consistency regularization and pseudo-labeling, with techniques to align distributions and reduce bias.

Result: The method outperforms state-of-the-art prompting methods that rely on labeled data, demonstrating strong generalization under covariate shift.

Conclusion: Unsupervised prompting for GNNs is viable and effective, offering a promising direction for pre-trained GNN applications without labeled data.

Abstract: Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to
address the semantic gap between pre-training and fine-tuning steps. However,
existing GNN prompting methods rely on labeled data and involve lightweight
fine-tuning for downstream tasks. Meanwhile, in-context learning methods for
Large Language Models (LLMs) have shown promising performance with no parameter
updating and no or minimal labeled data. Inspired by these approaches, in this
work, we first introduce a challenging problem setup to evaluate GNN prompting
methods. This setup encourages a prompting function to enhance a pre-trained
GNN's generalization to a target dataset under covariate shift without updating
the GNN's parameters and with no labeled data. Next, we propose a fully
unsupervised prompting method based on consistency regularization through
pseudo-labeling. We use two regularization techniques to align the prompted
graphs' distribution with the original data and reduce biased predictions.
Through extensive experiments under our problem setting, we demonstrate that
our unsupervised approach outperforms the state-of-the-art prompting methods
that have access to labels.

</details>


### [637] [Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype](https://arxiv.org/pdf/2505.16918)
*Nikola Tankovic, Robert Sajina*

Main category: cs.LG

TL;DR: The paper reviews CMAB methods and introduces a scalable, interpretable framework for offer selection, improving learning efficiency and generalization in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of fast-changing offers and enable knowledge transfer across similar offers for better learning efficiency.

Method: Extends CMAB methodology with multi-category contexts, efficient feature engineering, and modular design. Uses MPG and MF for nuanced interactions, implemented in Python.

Result: Achieves scalable, interpretable offer selection with transparent weight vectors and real-time preference tracking via LLM.

Conclusion: The framework enhances trust in automated decisions and demonstrates value for both research and real-world CMAB applications.

Abstract: This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)
methods and introduces an experimental framework for scalable, interpretable
offer selection, addressing the challenge of fast-changing offers. The approach
models context at the product category level, allowing offers to span multiple
categories and enabling knowledge transfer across similar offers. This improves
learning efficiency and generalization in dynamic environments. The framework
extends standard CMAB methodology to support multi-category contexts, and
achieves scalability through efficient feature engineering and modular design.
Advanced features such as MPG (Member Purchase Gap) and MF (Matrix
Factorization) capture nuanced user-offer interactions, with implementation in
Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models
yield transparent weight vectors, accessible via a large language model (LLM)
interface for real-time, user-level tracking and explanation of evolving
preferences. This enables the generation of detailed member profiles and
identification of behavioral patterns, supporting personalized offer
optimization and enhancing trust in automated decisions. By situating our
prototype alongside established paradigms like Generalized Linear Models and
Thompson Sampling, we demonstrate its value for both research and real-world
CMAB applications.

</details>


### [638] [Risk-Averse Reinforcement Learning with Itakura-Saito Loss](https://arxiv.org/pdf/2505.16925)
*Igor Udovichenko, Olivier Croissant, Anita Toleutaeva, Evgeny Burnaev, Alexander Korotin*

Main category: cs.LG

TL;DR: The paper introduces a numerically stable loss function for risk-averse reinforcement learning using the Itakura-Saito divergence, outperforming existing methods in financial scenarios.


<details>
  <summary>Details</summary>
Motivation: Risk-averse reinforcement learning is crucial in high-stakes fields but suffers from numerical instability due to exponent computations.

Method: Proposes a loss function based on the Itakura-Saito divergence for stable learning of state-value and action-value functions.

Result: The new loss function outperforms alternatives in financial scenarios, including those with known analytical solutions.

Conclusion: The introduced loss function provides a stable and effective solution for risk-averse reinforcement learning.

Abstract: Risk-averse reinforcement learning finds application in various high-stakes
fields. Unlike classical reinforcement learning, which aims to maximize
expected returns, risk-averse agents choose policies that minimize risk,
occasionally sacrificing expected value. These preferences can be framed
through utility theory. We focus on the specific case of the exponential
utility function, where we can derive the Bellman equations and employ various
reinforcement learning algorithms with few modifications. However, these
methods suffer from numerical instability due to the need for exponent
computation throughout the process. To address this, we introduce a numerically
stable and mathematically sound loss function based on the Itakura-Saito
divergence for learning state-value and action-value functions. We evaluate our
proposed loss function against established alternatives, both theoretically and
empirically. In the experimental section, we explore multiple financial
scenarios, some with known analytical solutions, and show that our loss
function outperforms the alternatives.

</details>


### [639] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/pdf/2505.16932)
*Noah Amsel, David Persson, Christopher Musco, Robert Gower*

Main category: cs.LG

TL;DR: Polar Express is a GPU-friendly algorithm for polar decomposition, optimized for deep learning by balancing efficiency and stability, outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: Traditional polar decomposition methods are inefficient for deep learning due to slow convergence or reliance on QR/inversions, necessitating a GPU-compatible, high-efficiency solution.

Method: Polar Express uses matrix-matrix multiplications and adapts polynomial updates via minimax optimization, ensuring rapid convergence and stability in bfloat16.

Result: The algorithm shows improved validation loss in Muon optimization for models like GPT-2, outperforming alternatives across learning rates.

Conclusion: Polar Express is a practical, efficient solution for polar decomposition in deep learning, combining speed, stability, and performance.

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [640] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/pdf/2505.16933)
*Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li*

Main category: cs.LG

TL;DR: LLaDA-V is a diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, showing competitive performance in multimodal tasks despite weaker textual performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of diffusion-based models in multimodal contexts, diverging from dominant autoregressive approaches.

Method: LLaDA-V combines a vision encoder and MLP connector to align visual features with language embeddings, built upon the LLaDA language diffusion model.

Result: LLaDA-V achieves competitive multimodal performance, narrowing gaps with stronger models like Qwen2-VL and outperforming hybrid autoregressive-diffusion MLLMs.

Conclusion: Diffusion-based large language models show promise for multimodal tasks, warranting further research.

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [641] [SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems](https://arxiv.org/pdf/2505.16936)
*Yizhuo Chen, Tianchen Wang, You Lyu, Yanlan Hu, Jinyang Li, Tomoyoshi Kimura, Hongjue Zhao, Yigong Hu, Denizhan Kara, Tarek Abdelzaher*

Main category: cs.LG

TL;DR: The paper introduces a self-supervised learning framework for placement-aware representation of multi-sensor IoT data, emphasizing spatial phenomena and sensor placement dependencies.


<details>
  <summary>Details</summary>
Motivation: To address the need for representing environmental state in IoT systems by distilling spatial phenomena from distributed multi-view sensor observations, overcoming limitations of current methods that ignore spatial nature.

Method: Develops a framework that learns dependencies between measurements and geometric observer layouts, guided by the duality between signals and observer positions, with theoretical grounding in information theory and occlusion-invariant learning.

Result: Demonstrates superior generalizability and robustness across diverse modalities, sensor placements, and tasks (vehicle monitoring, human activity recognition, earthquake localization).

Conclusion: The work advances self-supervised pretraining for IoT signals by explicitly modeling spatial dependencies, offering a robust and generalizable solution.

Abstract: This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.

</details>


### [642] [FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records](https://arxiv.org/pdf/2505.16941)
*Chao Pang, Vincent Jeanselme, Young Sang Choi, Xinzhuo Jiang, Zilin Jing, Aparajita Kashyap, Yuta Kobayashi, Yanwei Li, Florent Pollet, Karthik Natarajan, Shalmali Joshi*

Main category: cs.LG

TL;DR: The paper evaluates foundation models in healthcare, proposing a suite of clinically meaningful tasks and robust evaluation criteria to address gaps in understanding their clinical utility.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of foundation models in healthcare due to their ability to extract meaningful representations from EHR data, despite limited labeled data and lack of consensus on clinical utility.

Method: Proposes a suite of clinically relevant tasks and evaluates state-of-the-art foundation models on EHR data from 5 million patients across 14 tasks, measuring accuracy, calibration, and subpopulation performance.

Result: The study provides insights into the performance tradeoffs of foundation models based on pre-training, tokenization, and data representation strategies.

Conclusion: The research aims to improve empirical evaluation of EHR foundation models and guide future development in healthcare.

Abstract: Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.

</details>


### [643] [MixAT: Combining Continuous and Discrete Adversarial Training for LLMs](https://arxiv.org/pdf/2505.16947)
*Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev*

Main category: cs.LG

TL;DR: MixAT combines discrete and continuous adversarial attacks during training to improve LLM robustness, achieving better defense (ALO-ASR < 20%) with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current adversarial training methods for LLMs rely on continuous relaxations, leaving models vulnerable to discrete attacks. MixAT aims to bridge this gap.

Method: MixAT integrates stronger discrete and faster continuous attacks in training, evaluated using the ALO-ASR metric.

Result: MixAT outperforms prior defenses (ALO-ASR < 20% vs. > 50%) and maintains runtime efficiency.

Conclusion: MixAT offers a superior robustness-accuracy tradeoff, promising safer LLMs with minimal overhead.

Abstract: Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, quantization, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.

</details>


### [644] [Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning](https://arxiv.org/pdf/2505.16950)
*Adnan Oomerjee, Zafeirios Fountas, Zhongwei Yu, Haitham Bou-Ammar, Jun Wang*

Main category: cs.LG

TL;DR: The paper addresses the generalization limitations of Large Language Models by proposing a Transformer modification based on Information Bottleneck theory, improving reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with true abstract reasoning, relying on pattern interpolation. The work aims to enhance their generalization using Information Bottleneck theory.

Method: The authors propose a Transformer modification involving periodic global transformation of internal sequence-level representations (KV cache) to improve reasoning.

Result: The modified model outperforms vanilla Transformers and heuristic-driven pruning, achieving substantial gains on mathematical reasoning benchmarks.

Conclusion: The approach provides a principled framework for manipulating Transformer memory, addressing reasoning limitations beyond scaling.

Abstract: Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.

</details>


### [645] [A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization](https://arxiv.org/pdf/2505.16952)
*Shengyu Feng, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang*

Main category: cs.LG

TL;DR: The paper introduces FrontierCO, a benchmark for evaluating ML-based solvers in combinatorial optimization (CO) using realistic, large-scale data.


<details>
  <summary>Details</summary>
Motivation: Existing CO benchmarks lack sufficient training data and are often limited to small-scale, synthetic datasets, raising concerns about the practical effectiveness of ML-based solvers.

Method: FrontierCO covers eight CO problem types and evaluates 16 ML-based solvers, including graph neural networks and LLM agents, using industrial and research-derived instances.

Result: The benchmark provides insights into the strengths and limitations of current ML methods in CO, highlighting practical challenges.

Conclusion: FrontierCO aims to guide more robust and relevant advances in ML for CO by offering realistic problem difficulty and abundant training data.

Abstract: Machine learning (ML) has demonstrated considerable potential in supporting
model design and optimization for combinatorial optimization (CO) problems.
However, much of the progress to date has been evaluated on small-scale,
synthetic datasets, raising concerns about the practical effectiveness of
ML-based solvers in real-world, large-scale CO scenarios. Additionally, many
existing CO benchmarks lack sufficient training data, limiting their utility
for evaluating data-driven approaches. To address these limitations, we
introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO
problem types and evaluates 16 representative ML-based solvers--including graph
neural networks and large language model (LLM) agents. FrontierCO features
challenging instances drawn from industrial applications and frontier CO
research, offering both realistic problem difficulty and abundant training
data. Our empirical results provide critical insights into the strengths and
limitations of current ML methods, helping to guide more robust and practically
relevant advances at the intersection of machine learning and combinatorial
optimization. Our data is available at
https://huggingface.co/datasets/CO-Bench/FrontierCO.

</details>


### [646] [ICYM2I: The illusion of multimodal informativeness under missingness](https://arxiv.org/pdf/2505.16953)
*Young Sang Choi, Vincent Jeanselme, Pierre Elias, Shalmali Joshi*

Main category: cs.LG

TL;DR: The paper addresses biases in multimodal learning due to missing modalities and proposes ICYM2I, a framework for evaluating performance and information gain under missingness using inverse probability weighting.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning's potential is hindered by discrepancies between development and deployment modalities, leading to biased estimates of modality value.

Method: Introduces ICYM2I, a framework using inverse probability weighting to correct for missingness in multimodal learning.

Result: Demonstrates the framework's effectiveness on synthetic, semi-synthetic, and real-world medical datasets.

Conclusion: Accounting for missingness is crucial for accurate estimation of modality value in multimodal learning.

Abstract: Multimodal learning is of continued interest in artificial intelligence-based
applications, motivated by the potential information gain from combining
different types of data. However, modalities collected and curated during
development may differ from the modalities available at deployment due to
multiple factors including cost, hardware failure, or -- as we argue in this
work -- the perceived informativeness of a given modality. Na{\"i}ve estimation
of the information gain associated with including an additional modality
without accounting for missingness may result in improper estimates of that
modality's value in downstream tasks. Our work formalizes the problem of
missingness in multimodal learning and demonstrates the biases resulting from
ignoring this process. To address this issue, we introduce ICYM2I (In Case You
Multimodal Missed It), a framework for the evaluation of predictive performance
and information gain under missingness through inverse probability
weighting-based correction. We demonstrate the importance of the proposed
adjustment to estimate information gain under missingness on synthetic,
semi-synthetic, and real-world medical datasets.

</details>


### [647] [Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models](https://arxiv.org/pdf/2505.16959)
*Alessandro Favero, Antonio Sclocchi, Matthieu Wyart*

Main category: cs.LG

TL;DR: Diffusion models generalize before memorizing in overparameterized regimes, with memorization time proportional to dataset size. Early stopping optimizes generalization.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind generalization in diffusion models and the balance between generalization and memorization.

Method: Analyzed highly overparameterized diffusion models across image and language domains, and a simple probabilistic context-free grammar.

Result: Generalization occurs before memorization, with memorization time scaling with dataset size. A phase diagram summarizes the findings.

Conclusion: Early stopping, scaling with dataset size, optimizes generalization and avoids memorization, aiding hyperparameter transfer and privacy.

Abstract: Diffusion probabilistic models have become a cornerstone of modern generative
AI, yet the mechanisms underlying their generalization remain poorly
understood. In fact, if these models were perfectly minimizing their training
loss, they would just generate data belonging to their training set, i.e.,
memorize, as empirically found in the overparameterized regime. We revisit this
view by showing that, in highly overparameterized diffusion models,
generalization in natural data domains is progressively achieved during
training before the onset of memorization. Our results, ranging from image to
language diffusion models, systematically support the empirical law that
memorization time is proportional to the dataset size. Generalization vs.
memorization is then best understood as a competition between time scales. We
show that this phenomenology is recovered in diffusion models learning a simple
probabilistic context-free grammar with random rules, where generalization
corresponds to the hierarchical acquisition of deeper grammar rules as training
time grows, and the generalization cost of early stopping can be characterized.
We summarize these results in a phase diagram. Overall, our results support
that a principled early-stopping criterion - scaling with dataset size - can
effectively optimize generalization while avoiding memorization, with direct
implications for hyperparameter transfer and privacy-sensitive applications.

</details>


### [648] [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.16984)
*Mingyang Liu, Gabriele Farina, Asuman Ozdaglar*

Main category: cs.LG

TL;DR: UFT unifies SFT and RFT, outperforming both in reasoning tasks and breaking RFT's exponential sample complexity bottleneck.


<details>
  <summary>Details</summary>
Motivation: Address limitations of SFT (overfitting) and RFT (dependency on base model strength) in post-training LLMs.

Method: Propose Unified Fine-Tuning (UFT), integrating SFT and RFT into a single process for balanced exploration and supervision.

Result: UFT outperforms SFT and RFT across model sizes and exponentially accelerates convergence in long-horizon reasoning.

Conclusion: UFT bridges memorization and reasoning, offering a superior post-training paradigm for LLMs.

Abstract: Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.

</details>


### [649] [PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics](https://arxiv.org/pdf/2505.16992)
*Aleksandra Franz, Hao Wei, Luca Guastoni, Nils Thuerey*

Main category: cs.LG

TL;DR: PICT is a differentiable fluid simulator in PyTorch with GPU support, validated for accuracy and used to learn turbulence models efficiently.


<details>
  <summary>Details</summary>
Motivation: To address challenges in fluid simulation and leverage differentiable simulators for optimization and learning in physics.

Method: Developed PICT, a pressure-implicit solver, verified its accuracy, and used gradients for supervised/unsupervised learning of turbulence models.

Result: PICT trained models run faster than high-resolution references while maintaining or improving accuracy.

Conclusion: PICT is open-source, enabling broader use and insights into solver gradients and regularization techniques.

Abstract: Despite decades of advancements, the simulation of fluids remains one of the
most challenging areas of in scientific computing. Supported by the necessity
of gradient information in deep learning, differentiable simulators have
emerged as an effective tool for optimization and learning in physics
simulations. In this work, we present our fluid simulator PICT, a
differentiable pressure-implicit solver coded in PyTorch with
Graphics-processing-unit (GPU) support. We first verify the accuracy of both
the forward simulation and our derived gradients in various established
benchmarks like lid-driven cavities and turbulent channel flows before we show
that the gradients provided by our solver can be used to learn complicated
turbulence models in 2D and 3D. We apply both supervised and unsupervised
training regimes using physical priors to match flow statistics. In particular,
we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow
purely based on reference statistics. The low-resolution corrector trained with
our solver runs substantially faster than the highly resolved references, while
keeping or even surpassing their accuracy. Finally, we give additional insights
into the physical interpretation of different solver gradients, and motivate a
physically informed regularization technique. To ensure that the full potential
of PICT can be leveraged, it is published as open source:
https://github.com/tum-pbs/PICT.

</details>


### [650] [A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations](https://arxiv.org/pdf/2505.16996)
*Shalev Manor, Mohammad Kohandel*

Main category: cs.LG

TL;DR: A framework is introduced to ensure unique solutions in inverse problems involving differential equations, improving upon existing methods like PINNs, UDEs, and UPINNs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-uniqueness when identifying parameters and functions simultaneously in differential equations.

Method: The proposed framework establishes conditions for unique solutions and is tested on biological and ecological systems.

Result: The approach yields accurate and interpretable results, enhancing machine learning applications in complex systems.

Conclusion: This framework advances the use of machine learning in science and engineering by addressing non-uniqueness challenges.

Abstract: Inverse problems involving differential equations often require identifying
unknown parameters or functions from data. Existing approaches, such as
Physics-Informed Neural Networks (PINNs), Universal Differential Equations
(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective
at isolating either parameters or functions but can face challenges when
applied simultaneously due to solution non-uniqueness. In this work, we
introduce a framework that addresses these limitations by establishing
conditions under which unique solutions can be guaranteed. To illustrate, we
apply it to examples from biological systems and ecological dynamics,
demonstrating accurate and interpretable results. Our approach significantly
enhances the potential of machine learning techniques in modeling complex
systems in science and engineering.

</details>


### [651] [Guided Diffusion Sampling on Function Spaces with Applications to PDEs](https://arxiv.org/pdf/2505.17004)
*Jiachen Yao, Abbas Mammadov, Julius Berner, Gavin Kerrigan, Jong Chul Ye, Kamyar Azizzadenesheli, Anima Anandkumar*

Main category: cs.LG

TL;DR: A framework (FunDPS) for conditional sampling in PDE-based inverse problems using function-space diffusion and plug-and-play guidance, achieving high accuracy with minimal data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of recovering whole solutions from sparse or noisy measurements in PDE-based inverse problems.

Method: Trains an unconditional denoising model with neural operators, refines samples via gradient-based guidance, and extends Tweedie's formula to infinite-dimensional spaces.

Result: Achieves 32% better accuracy than baselines with 3% observation data and reduces sampling steps by 4x.

Conclusion: FunDPS is the first discretization-independent diffusion framework for PDEs, offering practical solutions for forward and inverse problems.

Abstract: We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS

</details>


### [652] [Understanding Prompt Tuning and In-Context Learning via Meta-Learning](https://arxiv.org/pdf/2505.17010)
*Tim Genewein, Kevin Wenliang Li, Jordi Grau-Moya, Anian Ruoss, Laurent Orseau, Marcus Hutter*

Main category: cs.LG

TL;DR: The paper explores optimal prompting through a Bayesian lens, highlighting its limitations and comparing prompt optimization methods like prefix-tuning and weight-tuning.


<details>
  <summary>Details</summary>
Motivation: To provide a conceptual understanding of prompting and its limitations, moving beyond empirical methods.

Method: Uses a Bayesian view to analyze prompting, supported by experiments with LSTMs and Transformers comparing prefix-tuning and weight-tuning.

Result: Shows that soft prefixes can be highly effective by manipulating activations in ways hard tokens cannot.

Conclusion: Optimal prompting is limited by Bayesian principles, and soft prefixes offer a powerful alternative beyond traditional methods.

Abstract: Prompting is one of the main ways to adapt a pretrained model to target
tasks. Besides manually constructing prompts, many prompt optimization methods
have been proposed in the literature. Method development is mainly empirically
driven, with less emphasis on a conceptual understanding of prompting. In this
paper we discuss how optimal prompting can be understood through a Bayesian
view, which also implies some fundamental limitations of prompting that can
only be overcome by tuning weights. The paper explains in detail how
meta-trained neural networks behave as Bayesian predictors over the pretraining
distribution, whose hallmark feature is rapid in-context adaptation. Optimal
prompting can be studied formally as conditioning these Bayesian predictors,
yielding criteria for target tasks where optimal prompting is and is not
possible. We support the theory with educational experiments on LSTMs and
Transformers, where we compare different versions of prefix-tuning and
different weight-tuning methods. We also confirm that soft prefixes, which are
sequences of real-valued vectors outside the token alphabet, can lead to very
effective prompts for trained and even untrained networks by manipulating
activations in ways that are not achievable by hard tokens. This adds an
important mechanistic aspect beyond the conceptual Bayesian theory.

</details>


### [653] [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/pdf/2505.17013)
*Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen*

Main category: cs.LG

TL;DR: The paper explores concept erasure in diffusion models, proposing two mechanisms and introducing a suite of evaluations to assess erasure effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the unclear thoroughness of existing concept erasure methods in diffusion models.

Method: Proposes two erasure mechanisms (likelihood reduction and interference with guidance) and introduces adversarial attacks, probing techniques, and alternative generation analysis for evaluation.

Result: Highlights the tension between minimizing side effects and robustness to adversarial prompts.

Conclusion: Emphasizes the need for comprehensive evaluation in concept erasure for diffusion models.

Abstract: Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.

</details>


### [654] [Interactive Post-Training for Vision-Language-Action Models](https://arxiv.org/pdf/2505.17016)
*Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Krähenbühl*

Main category: cs.LG

TL;DR: RIPT-VLA is a reinforcement-learning-based method for fine-tuning Vision-Language-Action models using sparse binary rewards, improving adaptability and success rates with minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models rely on offline expert data, limiting adaptability in low-data scenarios. RIPT-VLA aims to overcome this by enabling interactive post-training.

Method: Uses dynamic rollout sampling and leave-one-out advantage estimation for stable policy optimization, requiring only sparse rewards.

Result: Achieves 21.2% improvement on QueST and 97.5% success rate on OpenVLA-OFT. Efficiently boosts success from 4% to 97% in 15 iterations with one demo.

Conclusion: RIPT-VLA is a practical, effective paradigm for post-training VLAs with minimal supervision, generalizing across tasks and scenarios.

Abstract: We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based
interactive post-training paradigm that fine-tunes pretrained
Vision-Language-Action (VLA) models using only sparse binary success rewards.
Existing VLA training pipelines rely heavily on offline expert demonstration
data and supervised imitation, limiting their ability to adapt to new tasks and
environments under low-data regimes. RIPT-VLA addresses this by enabling
interactive post-training with a stable policy optimization algorithm based on
dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA
models, resulting in an improvement on the lightweight QueST model by 21.2%,
and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it
is computationally efficient and data-efficient: with only one demonstration,
RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success
rate within 15 iterations. Furthermore, we demonstrate that the policy learned
by RIPT-VLA generalizes across different tasks and scenarios and is robust to
the initial state context. These results highlight RIPT-VLA as a practical and
effective paradigm for post-training VLA models through minimal supervision.

</details>


### [655] [Compressing Neural Networks Using Tensor Networks with Exponentially Fewer Variational Parameters](https://arxiv.org/pdf/2305.06058)
*Yong Qing, Ke Li, Peng-Fei Zhou, Shi-Ju Ran*

Main category: cs.LG

TL;DR: A compression scheme using deep automatically differentiable tensor networks (ADTN) reduces NN parameters significantly, improving performance.


<details>
  <summary>Details</summary>
Motivation: To address issues like overfitting and high hardware costs caused by the high complexity of neural networks.

Method: Encode NN parameters into ADTN, which has exponentially fewer free parameters, and test on various NNs and datasets.

Result: Achieved superior compression, e.g., reducing VGG-16 layers from ~10^7 to 424 parameters while improving accuracy from 90.17% to 91.74%.

Conclusion: Deep tensor networks are highly efficient for NN parameter representation, outperforming traditional compression methods.

Abstract: Neural network (NN) designed for challenging machine learning tasks is in
general a highly nonlinear mapping that contains massive variational
parameters. High complexity of NN, if unbounded or unconstrained, might
unpredictably cause severe issues including \R{overfitting}, loss of
generalization power, and unbearable cost of hardware. In this work, we propose
a general compression scheme that significantly reduces the variational
parameters of NN's, despite of their specific types (linear, convolutional,
\textit{etc}), by encoding them to deep \R{automatically differentiable} tensor
network (ADTN) that contains exponentially-fewer free parameters. Superior
compression performance of our scheme is demonstrated on several
widely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets
(MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers in
VGG-16 with approximately $10^{7}$ parameters to two ADTN's with just 424
parameters, improving the testing accuracy on CIFAR-10 from $90.17\%$ to
$91.74\%$. We argue that the deep structure of ADTN is an essential reason for
the remarkable compression performance of ADTN, compared to existing
compression schemes that are mainly based on tensor
decompositions/factorization and shallow tensor networks. Our work suggests
deep TN as an exceptionally efficient mathematical structure for representing
the variational parameters of NN's, which exhibits superior compressibility
over the commonly-used matrices and multi-way arrays.

</details>


### [656] [Initialisation and Network Effects in Decentralised Federated Learning](https://arxiv.org/pdf/2403.15855)
*Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai*

Main category: cs.LG

TL;DR: Decentralized federated learning improves privacy and avoids single points of failure, with effectiveness influenced by network topology and initial conditions. A new initialization strategy based on eigenvector centrality enhances training efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and scalability of decentralized federated learning by addressing the impact of network topology and initial conditions on training performance.

Method: Proposes an uncoordinated initialization strategy for neural networks using eigenvector centrality of the communication network, and explores scaling behavior and parameter choices.

Result: The strategy significantly improves training efficiency and provides insights into the relationship between network structure and learning dynamics.

Conclusion: This work advances efficient and scalable neural network training in decentralized settings, highlighting the interplay between network topology and learning performance.

Abstract: Fully decentralised federated learning enables collaborative training of
individual machine learning models on a distributed network of communicating
devices while keeping the training data localised on each node. This approach
avoids central coordination, enhances data privacy and eliminates the risk of a
single point of failure. Our research highlights that the effectiveness of
decentralised federated learning is significantly influenced by the network
topology of connected devices and the learning models' initial conditions. We
propose a strategy for uncoordinated initialisation of the artificial neural
networks based on the distribution of eigenvector centralities of the
underlying communication network, leading to a radically improved training
efficiency. Additionally, our study explores the scaling behaviour and the
choice of environmental parameters under our proposed initialisation strategy.
This work paves the way for more efficient and scalable artificial neural
network training in a distributed and uncoordinated environment, offering a
deeper understanding of the intertwining roles of network structure and
learning dynamics.

</details>


### [657] [A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP](https://arxiv.org/pdf/2405.14273)
*Akira Kitaoka*

Main category: cs.LG

TL;DR: The paper proposes a projected subgradient method for inverse optimization in MILP, improving convergence efficiency and reducing computational costs compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inverse optimization in MILP suffer from slow convergence as the dimension of weights increases, necessitating a more efficient approach.

Method: A projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss is introduced.

Result: The method achieves faster convergence, with a theoretical bound on weight error, and reduces the number of MILP calls by over 7x.

Conclusion: The proposed method efficiently learns weights in MILP inverse optimization, offering significant computational advantages over known methods.

Abstract: We consider the inverse optimization problem of estimating the weights of the
objective function such that the given solution is an optimal solution for a
mixed integer linear program (MILP). In this inverse optimization problem, the
known methods exhibit inefficient convergence. Specifically, if $d$ denotes the
dimension of the weights and $k$ the number of iterations, then the error of
the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$
increases.We propose a projected subgradient method with a step size of
$k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate
that the proposed method efficiently learns the weights. In particular, we show
that there exists a constant $\gamma > 0$ such that the distance between the
learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)}
\exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal
solution is exactly recovered. Furthermore, experiments demonstrate that the
proposed method solves the inverse optimization problems of MILP using fewer
than $1/7$ the number of MILP calls required by known methods, and converges
within a finite number of iterations.

</details>


### [658] [How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities](https://arxiv.org/pdf/2407.08112)
*Jerry Huang*

Main category: cs.LG

TL;DR: The paper evaluates claims about infinite sequence length support in state-space and linear recurrent neural networks, finding practical gaps despite theoretical soundness.


<details>
  <summary>Details</summary>
Motivation: Long sequences are common in real-world scenarios, but deep neural networks struggle with them. Recent advances claim to support extended context length, but practical performance is unclear.

Method: The study empirically evaluates state-space and linear recurrent neural networks, comparing their performance with long-context LLMs.

Result: Recurrent models perform similarly to long-context LLMs, with inconsistent extrapolation capabilities due to varying inductive biases.

Conclusion: Further study is needed to understand why long-context models fail to meet expectations, highlighting gaps between theory and practice.

Abstract: Long sequences occur in abundance within real-world scenarios, hence properly
modelling them opens numerous down-stream use-cases. Deep neural networks,
however, have often struggled with these for a variety of reasons. Recent
advances, both in system engineering as well as model design, have enabled the
scaling up of model that are purported to support extended context length. In
particular, the state-space and linear recurrent neural network families of
models hypothetically can entend to infinite sequence lenth. However, is this
too good to be true? We conduct an evaluation to show that while such claims
may be sound theoretically, there remain large practical gaps that are
empirically observed. In particular, recurrent models still suffer in the same
settings as long-context LLMs with attention. We further show that different
inductive biases have inconsistent extrapolation capabilities, highlighting the
need to further study such paradigms and investigate why long-context models
seemingly fail to behave as one might expect.

</details>


### [659] [Logit Scaling for Out-of-Distribution Detection](https://arxiv.org/pdf/2409.01175)
*Andrija Djurisic, Rosanne Liu, Mladen Nikolic*

Main category: cs.LG

TL;DR: A post-hoc method called Logit Scaling (LTS) is proposed for OOD detection without needing training data or model retraining, showing strong performance across architectures.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods often require retraining or training data access and struggle with cross-architecture performance.

Method: LTS scales logits to distinguish ID and OOD samples, tested on multiple datasets and architectures.

Result: Achieves state-of-the-art performance, robustness, and adaptability across 3 ID and 14 OOD datasets and 9 architectures.

Conclusion: LTS offers a universally applicable solution for advanced OOD detection without compromising model integrity.

Abstract: The safe deployment of machine learning and AI models in open-world settings
hinges critically on the ability to detect out-of-distribution (OOD) data
accurately, data samples that contrast vastly from what the model was trained
with. Current approaches to OOD detection often require further training the
model, and/or statistics about the training data which may no longer be
accessible. Additionally, many existing OOD detection methods struggle to
maintain performance when transferred across different architectures. Our
research tackles these issues by proposing a simple, post-hoc method that does
not require access to the training data distribution, keeps a trained network
intact, and holds strong performance across a variety of architectures. Our
method, Logit Scaling (LTS), as the name suggests, simply scales the logits in
a manner that effectively distinguishes between in-distribution (ID) and OOD
samples. We tested our method on benchmarks across various scales, including
CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14
OOD datasets, as well as 9 model architectures. Overall, we demonstrate
state-of-the-art performance, robustness and adaptability across different
architectures, paving the way towards a universally applicable solution for
advanced OOD detection.

</details>


### [660] [Algorithm Configuration for Structured Pfaffian Settings](https://arxiv.org/pdf/2409.04367)
*Maria-Florina Balcan, Anh Tuan Nguyen, Dravyansh Sharma*

Main category: cs.LG

TL;DR: The paper introduces refined frameworks for theoretical guarantees in data-driven algorithm design, addressing challenges in parameterized algorithms by leveraging Pfaffian functions and piecewise structures.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of theoretical guarantees for data-driven algorithm design in parameterized algorithms due to complex utility functions.

Method: Proposes the Pfaffian GJ framework, extending the classical GJ framework to handle Pfaffian functions, and analyzes utility functions with refined piecewise structures.

Result: The framework provides learning guarantees for parameterized algorithms in distributional and online settings, applicable to broader function classes.

Conclusion: The refined frameworks enhance theoretical understanding and applicability of data-driven algorithm design for parameterized algorithms.

Abstract: Data-driven algorithm design automatically adapts algorithms to specific
application domains, achieving better performance. In the context of
parameterized algorithms, this approach involves tuning the algorithm's
hyperparameters using problem instances drawn from the problem distribution of
the target application domain. This can be achieved by maximizing empirical
utilities that measure the algorithms' performance as a function of their
hyperparameters, using problem instances. While empirical evidence supports the
effectiveness of data-driven algorithm design, providing theoretical guarantees
for several parameterized families remains challenging. This is due to the
intricate behaviors of their corresponding utility functions, which typically
admit piecewise discontinuous structures. In this work, we present refined
frameworks for providing learning guarantees for parameterized data-driven
algorithm design problems in both distributional and online learning settings.
For the distributional learning setting, we introduce the \textit{Pfaffian GJ
framework}, an extension of the classical \textit{GJ framework}, that is
capable of providing learning guarantees for function classes for which the
computation involves Pfaffian functions. Unlike the GJ framework, which is
limited to function classes with computation characterized by rational
functions, our proposed framework can deal with function classes involving
Pfaffian functions, which are much more general and widely applicable. We then
show that for many parameterized algorithms of interest, their utility function
possesses a \textit{refined piecewise structure}, which automatically
translates to learning guarantees using our proposed framework.

</details>


### [661] [Fair Class-Incremental Learning using Sample Weighting](https://arxiv.org/pdf/2410.01324)
*Jaeyoung Park, Minsu Kim, Steven Euijong Whang*

Main category: cs.LG

TL;DR: The paper addresses fairness in class-incremental learning, proposing a framework to mitigate unfair forgetting of sensitive groups by adjusting sample weights and solving optimization problems for fairness.


<details>
  <summary>Details</summary>
Motivation: Fairness is understudied in class-incremental learning, and naive training can lead to unfair forgetting of sensitive groups.

Method: Theoretical analysis of gradient vector directions and a Fairness-aware Sample Weighting (FSW) algorithm to adjust training weights and solve optimization problems.

Result: FSW achieves better accuracy-fairness tradeoff than state-of-the-art methods on real datasets.

Conclusion: The proposed framework effectively balances fairness and accuracy in class-incremental learning.

Abstract: Model fairness is becoming important in class-incremental learning for
Trustworthy AI. While accuracy has been a central focus in class-incremental
learning, fairness has been relatively understudied. However, naively using all
the samples of the current task for training results in unfair catastrophic
forgetting for certain sensitive groups including classes. We theoretically
analyze that forgetting occurs if the average gradient vector of the current
task data is in an "opposite direction" compared to the average gradient vector
of a sensitive group, which means their inner products are negative. We then
propose a fair class-incremental learning framework that adjusts the training
weights of current task samples to change the direction of the average gradient
vector and thus reduce the forgetting of underperforming groups and achieve
fairness. For various group fairness measures, we formulate optimization
problems to minimize the overall losses of sensitive groups while minimizing
the disparities among them. We also show the problems can be solved with linear
programming and propose an efficient Fairness-aware Sample Weighting (FSW)
algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff
results than state-of-the-art approaches on real datasets.

</details>


### [662] [Permissive Information-Flow Analysis for Large Language Models](https://arxiv.org/pdf/2410.03055)
*Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Köpf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, Santiago Zanella-Béguelin*

Main category: cs.LG

TL;DR: The paper proposes a permissive approach for dynamic information flow tracking in LLMs, focusing on propagating only influential input labels to improve security and privacy.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly integrated into software systems, raising security and privacy concerns due to potential data poisoning and leaks. Traditional conservative label propagation is impractical for diverse inputs.

Method: Two variations are implemented: (1) prompt-based retrieval augmentation and (2) a $k$-nearest-neighbors language model, compared to a baseline using introspection.

Result: The permissive label propagator outperforms the baseline in over 85% of cases in an LLM agent setting.

Conclusion: The approach is practical and effective for securing LLMs in diverse input scenarios.

Abstract: Large Language Models (LLMs) are rapidly becoming commodity components of
larger software systems. This poses natural security and privacy problems:
poisoned data retrieved from one component can change the model's behavior and
compromise the entire system, including coercing the model to spread
confidential data to untrusted components. One promising approach is to tackle
this problem at the system level via dynamic information flow (aka taint)
tracking. Unfortunately, this approach of propagating the most restrictive
input label to the output is too conservative for applications where LLMs
operate on inputs retrieved from diverse sources. In this paper, we propose a
novel, more permissive approach to propagate information flow labels through
LLM queries. The key idea behind our approach is to propagate only the labels
of the samples that were influential in generating the model output and to
eliminate the labels of unnecessary inputs. We implement and investigate the
effectiveness of two variations of this approach, based on (i) prompt-based
retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We
compare these with a baseline that uses introspection to predict the output
label. Our experimental results in an LLM agent setting show that the
permissive label propagator improves over the baseline in more than 85% of the
cases, which underscores the practicality of our approach.

</details>


### [663] [Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs](https://arxiv.org/pdf/2410.03565)
*Max Weltevrede, Caroline Horsch, Matthijs T. J. Spaan, Wendelin Böhmer*

Main category: cs.LG

TL;DR: Explore-Go, a method combining exploration phases with RL algorithms, improves generalization in zero-shot policy transfer by balancing state coverage and value function accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between increased exploration (improving generalization) and reduced value function accuracy in zero-shot policy transfer settings.

Method: Proposes Explore-Go, an exploration phase at the start of each episode, compatible with existing RL algorithms.

Result: Demonstrates improved generalization across environments when combined with popular RL methods.

Conclusion: Explore-Go offers a simple, effective modification to enhance agent generalization in contextual MDPs.

Abstract: In the zero-shot policy transfer (ZSPT) setting for contextual Markov
decision processes (MDP), agents train on a fixed set of contexts and must
generalise to new ones. Recent work has argued and demonstrated that increased
exploration can improve this generalisation, by training on more states in the
training contexts. In this paper, we demonstrate that training on more states
can indeed improve generalisation, but can come at a cost of reducing the
accuracy of the learned value function which should not benefit generalisation.
We hypothesise and demonstrate that using exploration to increase the agent's
coverage while also increasing the accuracy improves generalisation even more.
Inspired by this, we propose a method Explore-Go that implements an exploration
phase at the beginning of each episode, which can be combined with existing on-
and off-policy RL algorithms and significantly improves generalisation even in
partially observable MDPs. We demonstrate the effectiveness of Explore-Go when
combined with several popular algorithms and show an increase in generalisation
performance across several environments. With this, we hope to provide
practitioners with a simple modification that can improve the generalisation of
their agents.

</details>


### [664] [Domain-Oriented Time Series Inference Agents for Reasoning and Automated Analysis](https://arxiv.org/pdf/2410.04047)
*Wen Ye, Wei Yang, Defu Cao, Yizhou Zhang, Lumingyuan Tang, Jie Cai, Yan Liu*

Main category: cs.LG

TL;DR: TS-Reasoner is a domain-oriented time series agent combining natural language reasoning with numerical precision, outperforming general-purpose LLMs in time series tasks.


<details>
  <summary>Details</summary>
Motivation: Existing time series models lack flexibility, while LLMs struggle with numerical precision, necessitating a specialized solution.

Method: TS-Reasoner decomposes natural language instructions into structured workflows with statistical, logical, and domain-specific operators, and uses self-refinement for adaptive execution.

Result: TS-Reasoner significantly outperforms general-purpose LLMs in basic and complex time series tasks.

Conclusion: Domain-specialized agents like TS-Reasoner offer robust and interpretable time series reasoning.

Abstract: Real-world time series inference requires more than point forecasting. It
demands multi-step reasoning, constraint handling, domain knowledge
incorporation, and domain-specific workflow assembly. Existing time series
foundation models are limited to narrow tasks and lack flexibility to
generalize across diverse scenarios. On the other hand, large language models
(LLMs) struggle with numerical precision. To address these limitations, we
introduce TS-Reasoner, a Domain-Oriented Time Series Agent that integrates
natural language reasoning with precise numerical execution. TS-Reasoner
decomposes natural language instructions into structured workflows composed of
statistical, logical, and domain-specific operators, and incorporates a
self-refinement mechanism for adaptive execution. We evaluate its capabilities
through two axes: basic time series understanding and complex multi-step
inference, using the TimeSeriesExam benchmark and a newly constructed dataset.
Experimental results show that TS-Reasoner significantly outperforms
general-purpose LLMs, highlighting the promise of domain-specialized agents for
robust and interpretable time series reasoning.

</details>


### [665] [An Operator Splitting View of Federated Learning](https://arxiv.org/pdf/2108.05974)
*Saber Malekmohammadi, Kiarash Shaloudegi, Zeou Hu, Yaoliang Yu*

Main category: cs.LG

TL;DR: The paper unifies various federated learning (FL) algorithms using an operator splitting perspective, enabling easier comparison, refined convergence results, and new variants, while highlighting the importance of step size and proposing an efficient acceleration method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fragmented understanding and lack of formal comparison among existing FL algorithms.

Method: The method involves analyzing FL algorithms from an operator splitting viewpoint, unifying them for comparison, refining convergence results, and proposing new variants.

Result: The analysis reveals the critical role of step size in FL algorithms and introduces an efficient acceleration method without communication overhead. Numerical experiments validate the findings.

Conclusion: The conclusion is that the operator splitting perspective provides a unified framework for understanding, comparing, and improving FL algorithms, with practical benefits demonstrated through experiments.

Abstract: Over the past few years, the federated learning ($\texttt{FL}$) community has
witnessed a proliferation of new $\texttt{FL}$ algorithms. However, our
understating of the theory of $\texttt{FL}$ is still fragmented, and a
thorough, formal comparison of these algorithms remains elusive. Motivated by
this gap, we show that many of the existing $\texttt{FL}$ algorithms can be
understood from an operator splitting point of view. This unification allows us
to compare different algorithms with ease, to refine previous convergence
results and to uncover new algorithmic variants. In particular, our analysis
reveals the vital role played by the step size in $\texttt{FL}$ algorithms. The
unification also leads to a streamlined and economic way to accelerate
$\texttt{FL}$ algorithms, without incurring any communication overhead. We
perform numerical experiments on both convex and nonconvex models to validate
our findings.

</details>


### [666] [The Epochal Sawtooth Effect: Unveiling Training Loss Oscillations in Adam and Other Optimizers](https://arxiv.org/pdf/2410.10056)
*Qi Liu, Wanjing Ma*

Main category: cs.LG

TL;DR: The paper identifies the Epochal Sawtooth Effect (ESE), a sawtooth-shaped loss curve during training with adaptive optimizers like Adam, and explains its mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand and explain the recurring ESE pattern observed in adaptive gradient-based optimizers, particularly Adam.

Method: Empirical analysis of ESE, studying factors like β, batch size, and data shuffling, and replicating the effect via quadratic minimization tasks.

Result: Higher β₂ values lead to a linear loss increase, while lower values create a concave trend. The effect is linked to the adaptive learning rate controlled by the second moment estimate.

Conclusion: The paper provides theoretical and quantitative insights into ESE, demonstrating its generality in optimization scenarios.

Abstract: In this paper, we identify and analyze a recurring training loss pattern,
which we term the \textit{Epochal Sawtooth Effect (ESE)}, commonly observed
during training with adaptive gradient-based optimizers, particularly Adam
optimizer. This pattern is characterized by a sharp drop in loss at the
beginning of each epoch, followed by a gradual increase, resulting in a
sawtooth-shaped loss curve. Through empirical observations, we demonstrate that
while this effect is most pronounced with Adam, it persists, although less
severely, with other optimizers such as RMSProp.
  We provide an in-depth explanation of the underlying mechanisms that lead to
the Epochal Sawtooth Effect. The influences of factors like $\beta$, batch
size, data shuffling on this pattern have been studied. We quantify the
influence of $beta_2$ on the shape of the loss curve, showing that higher
values of $\beta_2$ result in a nearly linear increase in loss, while lower
values create a concave upward trend. Our analysis reveals that this behavior
stems from the adaptive learning rate controlled by the second moment estimate,
with $\beta_1$ playing a minimal role when $\beta_2$ is large.
  To support our analysis, we replicate this phenomenon through a controlled
quadratic minimization task. By incrementally solving a series of quadratic
optimization problems using Adam, we demonstrate that the Epochal Sawtooth
Effect can emerge even in simple optimization scenarios, reinforcing the
generality of this pattern. This paper provides both theoretical insights and
quantitative analysis, offering a comprehensive understanding of this
ubiquitous phenomenon in modern optimization techniques.

</details>


### [667] [Communication-Efficient Federated Learning With Data and Client Heterogeneity](https://arxiv.org/pdf/2206.10032)
*Hossein Zakerinia, Shayan Talaei, Giorgi Nadiradze, Dan Alistarh*

Main category: cs.LG

TL;DR: A new FedAvg variant addresses data heterogeneity, asynchrony, and communication constraints, showing convergence comparable to FedAvg and outperforming prior methods in experiments.


<details>
  <summary>Details</summary>
Motivation: To overcome practical challenges in Federated Learning (FL) like data heterogeneity, node asynchrony, and communication constraints.

Method: Introduces a variant of FedAvg that supports data heterogeneity, partial client asynchrony, and communication compression, with a novel convergence analysis.

Result: Demonstrates similar convergence to FedAvg in certain regimes and outperforms prior quantized and asynchronous approaches in experiments with up to 300 nodes.

Conclusion: The proposed algorithm effectively addresses key FL challenges while maintaining performance, offering a practical solution for large-scale FL.

Abstract: Federated Learning (FL) enables large-scale distributed training of machine
learning models, while still allowing individual nodes to maintain data
locally. However, executing FL at scale comes with inherent practical
challenges: 1) heterogeneity of the local node data distributions, 2)
heterogeneity of node computational speeds (asynchrony), but also 3)
constraints in the amount of communication between the clients and the server.
In this work, we present the first variant of the classic federated averaging
(FedAvg) algorithm which, at the same time, supports data heterogeneity,
partial client asynchrony, and communication compression. Our algorithm comes
with a novel, rigorous analysis showing that, in spite of these system
relaxations, it can provide similar convergence to FedAvg in interesting
parameter regimes. Experimental results in the rigorous LEAF benchmark on
setups of up to 300 nodes show that our algorithm ensures fast convergence for
standard federated tasks, improving upon prior quantized and asynchronous
approaches.

</details>


### [668] [Model-based Large Language Model Customization as Service](https://arxiv.org/pdf/2410.10481)
*Zhaomin Wu, Jizhou Guo, Junyi Hou, Bingsheng He, Lixin Fan, Qiang Yang*

Main category: cs.LG

TL;DR: Llamdex is a framework for customizing LLMs without sharing sensitive data, using pre-trained domain-specific models instead, achieving better accuracy and privacy than DP data synthesis.


<details>
  <summary>Details</summary>
Motivation: Current LLM customization requires uploading sensitive data for fine-tuning, posing privacy risks. DP data synthesis is ineffective due to excessive noise.

Method: Llamdex allows clients to upload pre-trained domain-specific models (optionally DP-protected) into the base LLM via connection modules, trained without sensitive data.

Result: Llamdex improves domain-specific accuracy by up to 26% over DP data synthesis under the same privacy constraints and maintains inference efficiency.

Conclusion: Llamdex offers a privacy-preserving, effective alternative for LLM customization, outperforming existing methods in accuracy and efficiency.

Abstract: Prominent Large Language Model (LLM) services from providers like OpenAI and
Google excel at general tasks but often underperform on domain-specific
applications. Current customization services for these LLMs typically require
users to upload data for fine-tuning, posing significant privacy risks. While
differentially private (DP) data synthesis presents a potential alternative,
its application commonly results in low effectiveness due to the introduction
of excessive noise on data for DP. To overcome this, we introduce Llamdex, a
novel framework that facilitates LLM customization as a service, where the
client uploads pre-trained domain-specific models rather than data. This
client-uploaded model, optionally protected by DP with much lower noise, is
inserted into the base LLM via connection modules. Significantly, these
connecting modules are trained without requiring sensitive domain data,
enabling clients to customize LLM services while preserving data privacy.
Experiments demonstrate that Llamdex improves domain-specific accuracy by up to
26\% over state-of-the-art private data synthesis methods under identical
privacy constraints and, by obviating the need for users to provide domain
context within queries, maintains inference efficiency comparable to the
original LLM service.

</details>


### [669] [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://arxiv.org/pdf/2305.04971)
*Peng Lu, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Philippe Langlais*

Main category: cs.LG

TL;DR: The paper introduces LABO, a bi-level optimization framework for learning instance-specific label regularization, improving upon conventional Label Smoothing (LS) by modeling non-target class probabilities dynamically.


<details>
  <summary>Details</summary>
Motivation: Conventional LS assumes equal probability for non-target classes, ignoring instance-specific variations. The authors aim to enhance regularization by adapting LS dynamically.

Method: Proposes LABO, a bi-level optimization framework, to learn optimal label smoothing without storing model parameters or outputs.

Result: LABO outperforms conventional LS in seven machine translation and three image classification tasks.

Conclusion: LABO provides a deterministic, interpretable, and efficient solution for label regularization, consistently improving performance across tasks.

Abstract: Regularization techniques are crucial to improving the generalization
performance and training efficiency of deep neural networks. Many deep learning
algorithms rely on weight decay, dropout, batch/layer normalization to converge
faster and generalize. Label Smoothing (LS) is another simple, versatile and
efficient regularization which can be applied to various supervised
classification tasks. Conventional LS, however, regardless of the training
instance assumes that each non-target class is equally likely. In this work, we
present a general framework for training with label regularization, which
includes conventional LS but can also model instance-specific variants. Based
on this formulation, we propose an efficient way of learning LAbel
regularization by devising a Bi-level Optimization (LABO) problem. We derive a
deterministic and interpretable solution of the inner loop as the optimal label
smoothing without the need to store the parameters or the output of a trained
model. Finally, we conduct extensive experiments and demonstrate our LABO
consistently yields improvement over conventional label regularization on
various fields, including seven machine translation and three image
classification tasks across various

</details>


### [670] [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/pdf/2410.19643)
*Nicolás Nieto, Simon B. Eickhoff, Christian Jung, Martin Reuter, Kersten Diers, Malte Kelm, Artur Lichtenberg, Federico Raimondo, Kaustubh R. Patil*

Main category: cs.LG

TL;DR: The paper evaluates ComBat-based data harmonization methods, identifies data leakage issues in unequal class balance scenarios, and proposes PrettYharmonize to avoid leakage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Combining biomedical datasets is common but introduces site-specific variability. Existing harmonization methods struggle with data leakage when class balance varies across sites.

Method: The study evaluates ComBat-based methods and introduces PrettYharmonize, which pretends target labels to prevent leakage. It tests both controlled and real-world MRI/clinical datasets.

Result: PrettYharmonize avoids data leakage and performs comparably to leakage-prone methods, especially in site-target-dependence cases.

Conclusion: PrettYharmonize is effective for harmonizing biomedical data without leakage, addressing limitations of existing methods.

Abstract: Machine learning (ML) models benefit from large datasets. Collecting data in
biomedical domains is costly and challenging, hence, combining datasets has
become a common practice. However, datasets obtained under different conditions
could present undesired site-specific variability. Data harmonization methods
aim to remove site-specific variance while retaining biologically relevant
information. This study evaluates the effectiveness of popularly used
ComBat-based methods for harmonizing data in scenarios where the class balance
is not equal across sites. We find that these methods struggle with data
leakage issues. To overcome this problem, we propose a novel approach
PrettYharmonize, designed to harmonize data by pretending the target labels. We
validate our approach using controlled datasets designed to benchmark the
utility of harmonization. Finally, using real-world MRI and clinical data, we
compare leakage-prone methods with PrettYharmonize and show that it achieves
comparable performance while avoiding data leakage, particularly in
site-target-dependence scenarios.

</details>


### [671] [Optimal Control of Nonlinear Systems with Unknown Dynamics](https://arxiv.org/pdf/2305.15188)
*Wenjian Hao, Paulo C. Heredia, Shaoshuai Mou*

Main category: cs.LG

TL;DR: A data-driven method for finding closed-loop optimal controllers for systems with unknown dynamics, using a novel gradient estimation framework combining the Koopman operator and actor-critic concepts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing controllers for systems with unknown dynamics without relying on exact system models.

Method: Introduces a gradient estimation framework integrating the Koopman operator with actor-critic methods to approximate cost function gradients for policy parameter tuning via gradient descent.

Result: Demonstrates effectiveness through comparisons with model-free reinforcement learning and outperforms model-based optimal control methods in simulations.

Conclusion: The proposed method successfully optimizes controllers for unknown systems, leveraging the Koopman operator's linearity and gradient-based tuning.

Abstract: This paper presents a data-driven method for finding a closed-loop optimal
controller, which minimizes a specified infinite-horizon cost function for
systems with unknown dynamics given any arbitrary initial state. Suppose the
closed-loop optimal controller can be parameterized by a given class of
functions, hereafter referred to as the policy. The proposed method introduces
a novel gradient estimation framework, which approximates the gradient of the
cost function with respect to the policy parameters via integrating the Koopman
operator with the classical concept of actor-critic. This enables the policy
parameters to be tuned iteratively using gradient descent to achieve an optimal
controller, leveraging the linearity of the Koopman operator. The convergence
analysis of the proposed framework is provided. The effectiveness of the method
is demonstrated through comparisons with a model-free reinforcement learning
approach, and its control performance is further evaluated through simulations
against model-based optimal control methods that solve the same optimal control
problem utilizing the exact system dynamics.

</details>


### [672] [Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity](https://arxiv.org/pdf/2305.19294)
*Camila Kolling, Till Speicher, Vedant Nanda, Mariya Toneva, Krishna P. Gummadi*

Main category: cs.LG

TL;DR: The paper introduces PNKA, a pointwise representational similarity measure, to address limitations in current debiasing evaluation methods, revealing new insights about fairness in ML models.


<details>
  <summary>Details</summary>
Motivation: Current evaluation measures for debiasing methods lack fine-grained analysis and generalizability across tasks, limiting their effectiveness.

Method: Proposes Pointwise Normalized Kernel Alignment (PNKA) to measure debiasing effects on intermediate representations of individuals.

Result: PNKA reveals group fairness impacts a small subset, while individual fairness uniformly affects all. It also shows debiasing methods may fail in language embeddings.

Conclusion: Existing debiasing evaluation measures are insufficient; PNKA offers a deeper understanding and aids fairness audits.

Abstract: Machine learning (ML) algorithms can often exhibit discriminatory behavior,
negatively affecting certain populations across protected groups. To address
this, numerous debiasing methods, and consequently evaluation measures, have
been proposed. Current evaluation measures for debiasing methods suffer from
two main limitations: (1) they primarily provide a global estimate of
unfairness, failing to provide a more fine-grained analysis, and (2) they
predominantly analyze the model output on a specific task, failing to
generalize the findings to other tasks. In this work, we introduce Pointwise
Normalized Kernel Alignment (PNKA), a pointwise representational similarity
measure that addresses these limitations by measuring how debiasing measures
affect the intermediate representations of individuals. On tabular data, the
use of PNKA reveals previously unknown insights: while group fairness
predominantly influences a small subset of the population, maintaining high
representational similarity for the majority, individual fairness constraints
uniformly impact representations across the entire population, altering nearly
every data point. We show that by evaluating representations using PNKA, we can
reliably predict the behavior of ML models trained on these representations.
Moreover, applying PNKA to language embeddings shows that existing debiasing
methods may not perform as intended, failing to remove biases from
stereotypical words and sentences. Our findings suggest that current evaluation
measures for debiasing methods are insufficient, highlighting the need for a
deeper understanding of the effects of debiasing methods, and show how
pointwise representational similarity metrics can help with fairness audits.

</details>


### [673] [On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics](https://arxiv.org/pdf/2306.01271)
*Binghui Li, Yuanzhi Li*

Main category: cs.LG

TL;DR: Deep nets in adversarial training generalize well for clean data but show a robust generalization gap (CGRO). This paper studies CGRO via representation complexity and training dynamics, proving and empirically validating the phenomenon.


<details>
  <summary>Details</summary>
Motivation: To understand why adversarial training achieves good clean generalization but suffers from robust overfitting (CGRO).

Method: Analyzes CGRO through representation complexity (proving ReLU nets can achieve CGRO with extra parameters) and training dynamics (studying a structured-data case with a two-layer convolutional network).

Result: ReLU nets with extra parameters can achieve CGRO, while robust classifiers require exponential complexity. A three-stage phase transition in training leads to robust memorization and CGRO.

Conclusion: The CGRO phenomenon in adversarial training is explained by representation complexity and training dynamics, supported by theoretical proofs and empirical validation.

Abstract: Similar to surprising performance in the standard deep learning, deep nets
trained by adversarial training also generalize well for unseen clean data
(natural data). However, despite adversarial training can achieve low robust
training error, there exists a significant robust generalization gap. We call
this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this
work, we study the CGRO phenomenon in adversarial training from two views:
representation complexity and training dynamics. Specifically, we consider a
binary classification setting with $N$ separated training data points. First,
we prove that, based on the assumption that we assume there is
$\operatorname{poly}(D)$-size clean classifier (where $D$ is the data
dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages
robust memorization to achieve the CGRO, while robust classifier still requires
exponential representation complexity in worst case. Next, we focus on a
structured-data case to analyze training dynamics, where we train a two-layer
convolutional network with $O(N D)$ width against adversarial perturbation. We
then show that a three-stage phase transition occurs during learning process
and the network provably converges to robust memorization regime, which thereby
results in the CGRO. Besides, we also empirically verify our theoretical
analysis by experiments in real-image recognition datasets.

</details>


### [674] [Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction](https://arxiv.org/pdf/2412.03188)
*Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: The paper explores semi-decentralized training for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility, showing comparable performance to centralized methods with better scalability and fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Centralized approaches struggle with scalability and reliability in large sensor networks, prompting the need for decentralized solutions.

Method: A simulation framework groups sensors into cloudlets, each training local ST-GNN models and exchanging updates, avoiding centralized aggregation.

Result: Semi-decentralized setups match centralized performance in vehicle speed predictions while improving scalability and fault tolerance.

Conclusion: Semi-decentralized ST-GNN training is viable for smart mobility, though challenges like region-specific performance and communication overhead remain.

Abstract: In smart mobility, large networks of geographically distributed sensors
produce vast amounts of high-frequency spatio-temporal data that must be
processed in real time to avoid major disruptions. Traditional centralized
approaches are increasingly unsuitable to this task, as they struggle to scale
with expanding sensor networks, and reliability issues in central components
can easily affect the whole deployment. To address these challenges, we explore
and adapt semi-decentralized training techniques for Spatio-Temporal Graph
Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation
framework where sensors are grouped by proximity into multiple cloudlets, each
handling a subgraph of the traffic graph, fetching node features from other
cloudlets to train its own local ST-GNN model, and exchanging model updates
with other cloudlets to ensure consistency, enhancing scalability and removing
reliance on a centralized aggregator. We perform extensive comparative
evaluation of four different ST-GNN training setups -- centralized, traditional
FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the
METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed
predictions. Experimental results show that semi-decentralized setups are
comparable to centralized approaches in performance metrics, while offering
advantages in terms of scalability and fault tolerance. In addition, we
highlight often overlooked issues in existing literature for distributed
ST-GNNs, such as the variation in model performance across different
geographical areas due to region-specific traffic patterns, and the significant
communication overhead and computational costs that arise from the large
receptive field of GNNs, leading to substantial data transfers and increased
computation of partial embeddings.

</details>


### [675] [Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination](https://arxiv.org/pdf/2311.02960)
*Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu*

Main category: cs.LG

TL;DR: The paper investigates how deep linear networks hierarchically learn features, showing progressive within-class compression and between-class discrimination, with empirical validation in nonlinear networks.


<details>
  <summary>Details</summary>
Motivation: To understand how deep networks perform hierarchical feature learning, focusing on intermediate features in linear networks.

Method: Define metrics for within-class compression and between-class discrimination, analyze feature evolution in deep linear networks, and validate empirically.

Result: Features compress within-class geometrically and discriminate between-class linearly, validated in nonlinear networks.

Conclusion: First quantitative characterization of feature evolution in deep linear networks, with practical implications for transfer learning.

Abstract: Over the past decade, deep learning has proven to be a highly effective tool
for learning meaningful features from raw data. However, it remains an open
question how deep networks perform hierarchical feature learning across layers.
In this work, we attempt to unveil this mystery by investigating the structures
of intermediate features. Motivated by our empirical findings that linear
layers mimic the roles of deep layers in nonlinear networks for feature
learning, we explore how deep linear networks transform input data into output
by investigating the output (i.e., features) of each layer after training in
the context of multi-class classification problems. Toward this goal, we first
define metrics to measure within-class compression and between-class
discrimination of intermediate features, respectively. Through theoretical
analysis of these two metrics, we show that the evolution of features follows a
simple and quantitative pattern from shallow to deep layers when the input data
is nearly orthogonal and the network weights are minimum-norm, balanced, and
approximate low-rank: Each layer of the linear network progressively compresses
within-class features at a geometric rate and discriminates between-class
features at a linear rate with respect to the number of layers that data have
passed through. To the best of our knowledge, this is the first quantitative
characterization of feature evolution in hierarchical representations of deep
linear networks. Empirically, our extensive experiments not only validate our
theoretical results numerically but also reveal a similar pattern in deep
nonlinear networks which aligns well with recent empirical studies. Moreover,
we demonstrate the practical implications of our results in transfer learning.
Our code is available at https://github.com/Heimine/PNC_DLN.

</details>


### [676] [Combining SNNs with Filtering for Efficient Neural Decoding in Implantable Brain-Machine Interfaces](https://arxiv.org/pdf/2312.15889)
*Biyan Zhou, Pao-Sheng Vincent Sun, Arindam Basu*

Main category: cs.LG

TL;DR: Combining signal filtering with Spiking Neural Networks (SNNs) improves decoding performance for wireless brain-machine interfaces, closing the gap with LSTMs at minimal cost.


<details>
  <summary>Details</summary>
Motivation: The challenge of high data rates in wireless brain-machine interfaces (iBMI) necessitates efficient edge computing solutions, with SNNs offering resource efficiency but lagging in accuracy.

Method: Traditional signal processing (Bessel filters) is combined with SNNs to enhance performance. Two block-bidirectional Bessel filters (low latency and high accuracy) are tested.

Result: The high-accuracy Bessel filter variant significantly improved SNN performance, with gains of ≈5% and 8% in R² for two SNN topologies.

Conclusion: This approach achieves state-of-the-art results, advancing the feasibility of decoder-integrated implants.

Abstract: While it is important to make implantable brain-machine interfaces (iBMI)
wireless to increase patient comfort and safety, the trend of increased channel
count in recent neural probes poses a challenge due to the concomitant increase
in the data rate. Extracting information from raw data at the source by using
edge computing is a promising solution to this problem, with integrated
intention decoders providing the best compression ratio. Recent benchmarking
efforts have shown recurrent neural networks to be the best solution. Spiking
Neural Networks (SNN) emerge as a promising solution for resource efficient
neural decoding while Long Short Term Memory (LSTM) networks achieve the best
accuracy. In this work, we show that combining traditional signal processing
techniques, namely signal filtering, with SNNs improve their decoding
performance significantly for regression tasks, closing the gap with LSTMs, at
little added cost. Results with different filters are shown with Bessel filters
providing best performance. Two block-bidirectional Bessel filters have been
used--one for low latency and another for high accuracy. Adding the high
accuracy variant of the Bessel filters to the output of ANN, SNN and variants
provided statistically significant benefits with maximum gains of $\approx 5\%$
and $8\%$ in $R^2$ for two SNN topologies (SNN\_Streaming and SNN\_3D). Our
work presents state of the art results for this dataset and paves the way for
decoder-integrated-implants of the future.

</details>


### [677] [Demystifying Variational Diffusion Models](https://arxiv.org/pdf/2401.06281)
*Fabio De Sousa Ribeiro, Ben Glocker*

Main category: cs.LG

TL;DR: The paper aims to simplify the understanding of diffusion models by synthesizing a holistic perspective using directed graphical modeling and variational inference, making it more accessible to practitioners and new researchers.


<details>
  <summary>Details</summary>
Motivation: The growing interest in diffusion models lacks a clear, accessible theoretical foundation, risking superficial understanding. The paper seeks to lower the barrier to entry by consolidating knowledge.

Method: The authors revisit predecessors like hierarchical latent variable models and use directed graphical modeling and variational inference to explain diffusion models.

Result: The proposed narrative is easier to follow, requiring fewer prerequisites compared to approaches based on non-equilibrium thermodynamics or stochastic differential equations.

Conclusion: A clearer, more accessible perspective on diffusion models is provided, aiding practitioners and new researchers in contextualizing advances in generative modeling.

Abstract: Despite the growing interest in diffusion models, gaining a deep
understanding of the model class remains an elusive endeavour, particularly for
the uninitiated in non-equilibrium statistical physics. Thanks to the rapid
rate of progress in the field, most existing work on diffusion models focuses
on either applications or theoretical contributions. Unfortunately, the
theoretical material is often inaccessible to practitioners and new
researchers, leading to a risk of superficial understanding in ongoing
research. Given that diffusion models are now an indispensable tool, a clear
and consolidating perspective on the model class is needed to properly
contextualize recent advances in generative modelling and lower the barrier to
entry for new researchers. To that end, we revisit predecessors to diffusion
models like hierarchical latent variable models and synthesize a holistic
perspective using only directed graphical modelling and variational inference
principles. The resulting narrative is easier to follow as it imposes fewer
prerequisites on the average reader relative to the view from non-equilibrium
thermodynamics or stochastic differential equations.

</details>


### [678] [FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization](https://arxiv.org/pdf/2403.12474)
*Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi*

Main category: cs.LG

TL;DR: FairSIN introduces a neutralization-based paradigm to improve fairness in GNNs by adding Fairness-facilitating Features (F3), avoiding the drawbacks of filtering-based methods.


<details>
  <summary>Details</summary>
Motivation: Current filtering-based fairness methods in GNNs may remove non-sensitive information, harming predictive performance. FairSIN aims to balance fairness and accuracy.

Method: FairSIN incorporates F3 into node features or representations before message passing, emphasizing heterogeneous neighbors' features to neutralize bias.

Result: Experiments on five datasets with three GNN backbones show FairSIN improves fairness metrics without sacrificing accuracy.

Conclusion: FairSIN offers a better trade-off between fairness and performance compared to filtering-based methods, validated by theoretical and empirical results.

Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling
graph-structured data, like other machine learning models, GNNs are also
susceptible to making biased predictions based on sensitive attributes, such as
race and gender. For fairness consideration, recent state-of-the-art (SOTA)
methods propose to filter out sensitive information from inputs or
representations, e.g., edge dropping or feature masking. However, we argue that
such filtering-based strategies may also filter out some non-sensitive feature
information, leading to a sub-optimal trade-off between predictive performance
and fairness. To address this issue, we unveil an innovative
neutralization-based paradigm, where additional Fairness-facilitating Features
(F3) are incorporated into node features or representations before message
passing. The F3 are expected to statistically neutralize the sensitive bias in
node representations and provide additional nonsensitive information. We also
provide theoretical explanations for our rationale, concluding that F3 can be
realized by emphasizing the features of each node's heterogeneous neighbors
(neighbors with different sensitive attributes). We name our method as FairSIN,
and present three implementation variants from both data-centric and
model-centric perspectives. Experimental results on five benchmark datasets
with three different GNN backbones show that FairSIN significantly improves
fairness metrics while maintaining high prediction accuracies.

</details>


### [679] [SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders](https://arxiv.org/pdf/2501.18052)
*Bartosz Cywiński, Kamil Deja*

Main category: cs.LG

TL;DR: SAeUron uses sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models, outperforming existing methods in unlearning tasks and blocking harmful content.


<details>
  <summary>Details</summary>
Motivation: Diffusion models can generate harmful content, and current unlearning methods lack transparency. SAeUron aims to address this by leveraging interpretable SAE features.

Method: SAEs are trained on diffusion model activations to capture sparse, interpretable features. A feature selection method enables precise interventions to block unwanted content.

Result: SAeUron outperforms existing methods on UnlearnCanvas and effectively removes nudity (I2P). It can unlearn multiple concepts simultaneously and resists adversarial attacks.

Conclusion: SAeUron provides a transparent and effective solution for unlearning unwanted concepts in diffusion models, with potential for broader applications.

Abstract: Diffusion models, while powerful, can inadvertently generate harmful or
undesirable content, raising significant ethical and safety concerns. Recent
machine unlearning approaches offer potential solutions but often lack
transparency, making it difficult to understand the changes they introduce to
the base model. In this work, we introduce SAeUron, a novel method leveraging
features learned by sparse autoencoders (SAEs) to remove unwanted concepts in
text-to-image diffusion models. First, we demonstrate that SAEs, trained in an
unsupervised manner on activations from multiple denoising timesteps of the
diffusion model, capture sparse and interpretable features corresponding to
specific concepts. Building on this, we propose a feature selection method that
enables precise interventions on model activations to block targeted content
while preserving overall performance. Our evaluation shows that SAeUron
outperforms existing approaches on the UnlearnCanvas benchmark for concepts and
style unlearning, and effectively eliminates nudity when evaluated with I2P.
Moreover, we show that with a single SAE, we can remove multiple concepts
simultaneously and that in contrast to other methods, SAeUron mitigates the
possibility of generating unwanted content under adversarial attack. Code and
checkpoints are available at https://github.com/cywinski/SAeUron.

</details>


### [680] [ImPORTance: Machine Learning-Driven Analysis of Global Port Significance and Network Dynamics for Improved Operational Efficiency](https://arxiv.org/pdf/2407.09571)
*Emanuele Carlini, Domenico Di Gangi, Vinicius Monteiro de Lira, Hanna Kavalionak, Amilcar Soares, Gabriel Spadon*

Main category: cs.LG

TL;DR: The paper analyzes global seaport importance using AIS data and machine learning, identifying geography and depth as key factors.


<details>
  <summary>Details</summary>
Motivation: To understand common characteristics of important ports and inform industry decision-making.

Method: Bottom-up network construction using three years of AIS data, combined with machine learning to assess port features.

Result: Geographical characteristics and port depth are key indicators of a port's importance.

Conclusion: The study provides data-driven insights for port development and infrastructure planning.

Abstract: Seaports play a crucial role in the global economy, and researchers have
sought to understand their significance through various studies. In this paper,
we aim to explore the common characteristics shared by important ports by
analyzing the network of connections formed by vessel movement among them. To
accomplish this task, we adopt a bottom-up network construction approach that
combines three years' worth of AIS (Automatic Identification System) data from
around the world, constructing a Ports Network that represents the connections
between different ports. Through this representation, we utilize machine
learning to assess the relative significance of various port features. Our
model examined such features and revealed that geographical characteristics and
the port's depth are indicators of a port's importance to the Ports Network.
Accordingly, this study employs a data-driven approach and utilizes machine
learning to provide a comprehensive understanding of the factors contributing
to the extent of ports. Our work aims to inform decision-making processes
related to port development, resource allocation, and infrastructure planning
within the industry.

</details>


### [681] [ReAugment: Model Zoo-Guided RL for Few-Shot Time Series Augmentation and Forecasting](https://arxiv.org/pdf/2409.06282)
*Haochen Yuan, Yutong Wang, Yihong Chen, Yunbo Wang, Xiaokang Yang*

Main category: cs.LG

TL;DR: ReAugment uses RL for time series data augmentation to address few-shot learning challenges by targeting overfit-prone samples and enhancing dataset diversity.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning in time series forecasting is difficult due to limited high-quality training data.

Method: ReAugment identifies overfit-prone samples via a forecasting model zoo and uses RL to adaptively augment these samples, improving diversity and targeting weak model regions.

Result: ReAugment improves performance in standard and few-shot time series forecasting tasks across various base models.

Conclusion: RL-based data augmentation (ReAugment) effectively enhances forecasting models by addressing overfitting and data scarcity.

Abstract: Time series forecasting, particularly in few-shot learning scenarios, is
challenging due to the limited availability of high-quality training data. To
address this, we present a pilot study on using reinforcement learning (RL) for
time series data augmentation. Our method, ReAugment, tackles three critical
questions: which parts of the training set should be augmented, how the
augmentation should be performed, and what advantages RL brings to the process.
Specifically, our approach maintains a forecasting model zoo, and by measuring
prediction diversity across the models, we identify samples with higher
probabilities for overfitting and use them as the anchor points for
augmentation. Leveraging RL, our method adaptively transforms the overfit-prone
samples into new data that not only enhances training set diversity but also
directs the augmented data to target regions where the forecasting models are
prone to overfitting. We validate the effectiveness of ReAugment across a wide
range of base models, showing its advantages in both standard time series
forecasting and few-shot learning tasks.

</details>


### [682] [Learning Fused State Representations for Control from Multi-View Observations](https://arxiv.org/pdf/2502.01316)
*Zeyu Wang, Yao-Hui Li, Xin Li, Hongyu Zang, Romain Laroche, Riashat Islam*

Main category: cs.LG

TL;DR: MFSC integrates bisimulation metric learning into MVRL for task-relevant representations and uses a mask token for robustness in missing views, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: MVRL struggles with learning compact, task-relevant representations due to redundancy, distractions, or missing views.

Method: Proposes MFSC with bisimulation metric learning and a mask token for latent reconstruction.

Result: MFSC outperforms existing MVRL methods, especially in scenarios with interference or missing views.

Conclusion: MFSC effectively enhances MVRL by improving representation learning and robustness.

Abstract: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with
multi-view observations, enabling them to perceive environment with greater
effectiveness and precision. Recent advancements in MVRL focus on extracting
latent representations from multiview observations and leveraging them in
control tasks. However, it is not straightforward to learn compact and
task-relevant representations, particularly in the presence of redundancy,
distracting information, or missing views. In this paper, we propose Multi-view
Fusion State for Control (MFSC), firstly incorporating bisimulation metric
learning into MVRL to learn task-relevant representations. Furthermore, we
propose a multiview-based mask and latent reconstruction auxiliary task that
exploits shared information across views and improves MFSC's robustness in
missing views by introducing a mask token. Extensive experimental results
demonstrate that our method outperforms existing approaches in MVRL tasks. Even
in more realistic scenarios with interference or missing views, MFSC
consistently maintains high performance.

</details>


### [683] [Statistical inference on black-box generative models in the data kernel perspective space](https://arxiv.org/pdf/2410.01106)
*Hayden Helm, Aranyak Acharyya, Brandon Duderstadt, Youngser Park, Carey E. Priebe*

Main category: cs.LG

TL;DR: The paper extends black-box generative model representations for model-level statistical inference, proving their effectiveness.


<details>
  <summary>Details</summary>
Motivation: As generative models impact grows, understanding collections of models without access to pre-training data or weights becomes crucial.

Method: Extends recent results on black-box generative model representations for inference tasks.

Result: Model-level representations are effective for multiple inference tasks.

Conclusion: The approach enables statistical inference on generative models without detailed model-level information.

Abstract: Generative models are capable of producing human-expert level content across
a variety of topics and domains. As the impact of generative models grows, it
is necessary to develop statistical methods to understand collections of
available models. These methods are particularly important in settings where
the user may not have access to information related to a model's pre-training
data, weights, or other relevant model-level covariates. In this paper we
extend recent results on representations of black-box generative models to
model-level statistical inference tasks. We demonstrate that the model-level
representations are effective for multiple inference tasks.

</details>


### [684] [InSTA: Towards Internet-Scale Training For Agents](https://arxiv.org/pdf/2502.06776)
*Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov*

Main category: cs.LG

TL;DR: A pipeline for training web navigation agents without human annotations uses LLMs to annotate tasks, complete trajectories, and filter data, achieving competitive performance with smaller models.


<details>
  <summary>Details</summary>
Motivation: Human demonstrations for training web agents are inefficient; this work aims to enable internet-scale training without laborious human annotations.

Method: A three-stage pipeline: (1) LLM annotates tasks, (2) LLM agents complete tasks, (3) LLM filters trajectories. Uses Qwen 3 1.7B for training.

Result: Agents achieve 56.9% success rate, outperforming larger models like Qwen 3 235B and Llama 4 Maverick, and reaching 94.7% of Gemini 2.5 Flash's performance.

Conclusion: LLMs are effective for data curation and training competitive web agents, reducing reliance on human data.

Abstract: The predominant approach for training web navigation agents is to gather
human demonstrations for a set of popular websites and hand-written tasks, but
it is becoming clear that human data is an inefficient resource. We develop a
pipeline to facilitate internet-scale training for agents without laborious
human annotations. In the first stage, an LLM annotates 150k sites with agentic
tasks. In the next stage, LLM agents complete tasks and produce trajectories.
In the final stage, an LLM filters trajectories by judging their success.
Language models are powerful data curation tools, identifying harmful content
with an accuracy of 97%, judging successful trajectories with an accuracy of
82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that
are competitive with frontier LLMs as web agents, while being smaller and
faster. Our top agent reaches a success rate of 56.9%, outperforming the data
collection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and
reaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,
models and data at: https://data-for-agents.github.io.

</details>


### [685] [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://arxiv.org/pdf/2410.03450)
*Junpeng Yue, Xinrun Xu, Börje F. Karlsson, Zongqing Lu*

Main category: cs.LG

TL;DR: MART improves embodied agents by fine-tuning an MLLM retriever using preference learning and trajectory abstraction, enhancing task success in unseen scenes.


<details>
  <summary>Details</summary>
Motivation: Current retrieval methods for MLLM agents focus on surface-level similarities, ignoring trajectory effectiveness for specific tasks.

Method: Proposes MART, which fine-tunes an MLLM retriever via preference learning and introduces Trajectory Abstraction for concise trajectory representation.

Result: Significantly improves task success rates in unseen environments compared to baselines.

Conclusion: MART offers a new paradigm for multimodal retrieval in embodied agents by prioritizing trajectory effectiveness.

Abstract: MLLM agents demonstrate potential for complex embodied tasks by retrieving
multimodal task-relevant trajectory data. However, current retrieval methods
primarily focus on surface-level similarities of textual or visual cues in
trajectories, neglecting their effectiveness for the specific task at hand. To
address this issue, we propose a novel method, MLLM As ReTriever (MART), which
enhances the performance of embodied agents by utilizing interaction data to
fine-tune an MLLM retriever based on preference learning, such that the
retriever fully considers the effectiveness of trajectories and prioritizes
them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism
that leverages MLLMs' summarization capabilities to represent trajectories with
fewer tokens while preserving key information, enabling agents to better
comprehend milestones in the trajectory. Experimental results across various
environments demonstrate our method significantly improves task success rates
in unseen scenes compared to baseline methods. This work presents a new
paradigm for multimodal retrieval in embodied agents, by fine-tuning a
general-purpose MLLM as the retriever to assess trajectory effectiveness. All
the code for benchmark tasks, simulator modifications, and the MLLM retriever
is available at https://github.com/PKU-RL/MART.

</details>


### [686] [Prot2Chat: Protein LLM with Early-Fusion of Text, Sequence and Structure](https://arxiv.org/pdf/2502.06846)
*Zhicong Wang, Zicheng Ma, Ziqiang Cao, Changlong Zhou, Jun Zhang, Yiqin Gao*

Main category: cs.LG

TL;DR: Prot2Chat framework integrates protein and text data for Q&A, using ProteinMPNN and LLM with LoRA for efficiency, showing superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Challenges in protein function understanding include multimodal integration, training parameters, flexibility, and evaluation metrics. Prot2Chat addresses these.

Method: Modified ProteinMPNN for unified encoding, LLM for question vectors, protein-text adapter for early fusion, LoRA for efficiency.

Result: Outperforms on datasets, excels in zero-shot prediction, and demonstrates strong generalization.

Conclusion: Prot2Chat effectively integrates protein and text data, offering a robust solution for protein Q&A with high performance.

Abstract: Motivation: Proteins are of great significance in living organisms. However,
understanding their functions encounters numerous challenges, such as
insufficient integration of multimodal information, a large number of training
parameters, limited flexibility of classification-based methods, and the lack
of systematic evaluation metrics for protein Q&A systems. To tackle these
issues, we propose the Prot2Chat framework. Results: We modified ProteinMPNN to
encode protein sequence and structural information in a unified way. We used a
large language model (LLM) to encode questions into vectors and developed a
protein-text adapter to compress protein information into virtual tokens based
on these vectors, achieving the early fusion of text and protein information.
Finally, the same LLM reads the virtual tokens and the questions to generate
answers. To optimize training efficiency, we froze the encoder and employed
Low-Rank Adaptation (LoRA) techniques for the LLM. Experiments on two datasets
show that both automated metrics and expert evaluations demonstrate the
superior performance of our model, and zero-shot prediction results highlight
its generalization ability. The models and codes are available at
https://github.com/ wangzc1233/Prot2Chat. Contact: zqcao@suda.edu.cn or
wangzc025@163.com Key words: Protein Q&A, Early-Fusion, LLM

</details>


### [687] [Robust Offline Imitation Learning from Diverse Auxiliary Data](https://arxiv.org/pdf/2410.03626)
*Udita Ghosh, Dripta S. Raychaudhuri, Jiachen Li, Konstantinos Karydis, Amit K. Roy-Chowdhury*

Main category: cs.LG

TL;DR: ROIDA improves offline imitation learning by leveraging diverse auxiliary data without quality assumptions, using high-reward samples for weighted behavioral cloning and TD learning for lower-quality data.


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior methods that rely on assumptions about auxiliary data quality and composition, which often fail when assumptions don't hold.

Method: ROIDA identifies high-quality transitions via a learned reward function for weighted behavioral cloning and applies TD learning for lower-quality samples to improve long-term returns.

Result: ROIDA achieves robust performance across diverse auxiliary datasets, outperforming methods dependent on specific data assumptions.

Conclusion: ROIDA effectively leverages both high and low-quality auxiliary data without assumptions, demonstrating consistent performance.

Abstract: Offline imitation learning enables learning a policy solely from a set of
expert demonstrations, without any environment interaction. To alleviate the
issue of distribution shift arising due to the small amount of expert data,
recent works incorporate large numbers of auxiliary demonstrations alongside
the expert data. However, the performance of these approaches rely on
assumptions about the quality and composition of the auxiliary data, and they
are rarely successful when those assumptions do not hold. To address this
limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data
(ROIDA). ROIDA first identifies high-quality transitions from the entire
auxiliary dataset using a learned reward function. These high-reward samples
are combined with the expert demonstrations for weighted behavioral cloning.
For lower-quality samples, ROIDA applies temporal difference learning to steer
the policy towards high-reward states, improving long-term returns. This
two-pronged approach enables our framework to effectively leverage both high
and low-quality data without any assumptions. Extensive experiments validate
that ROIDA achieves robust and consistent performance across multiple auxiliary
datasets with diverse ratios of expert and non-expert demonstrations. ROIDA
effectively leverages unlabeled auxiliary data, outperforming prior methods
reliant on specific data assumptions. Our code is available at
https://github.com/uditaghosh/roida.

</details>


### [688] [Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization](https://arxiv.org/pdf/2502.07523)
*Daniel Palenicek, Florian Vogt, Joe Watson, Jan Peters*

Main category: cs.LG

TL;DR: The paper explores scaling CrossQ's sample efficiency with higher update-to-data (UTD) ratios, identifies training challenges, and proposes weight normalization to stabilize training and improve performance.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency is a bottleneck in reinforcement learning; CrossQ shows promise but struggles with higher UTD ratios.

Method: Integrates weight normalization into CrossQ to stabilize training dynamics and maintain effective learning rates.

Result: Achieves competitive performance on 25 tasks, including complex environments like dog and humanoid, without drastic interventions.

Conclusion: Weight normalization offers a simple, robust solution for improving scalability and sample efficiency in model-free RL.

Abstract: Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics, which are
emphasized by higher UTD ratios. To address these, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
has been shown to prevent potential loss of plasticity and keeps the effective
learning rate constant. Our proposed approach reliably scales with increasing
UTD ratios, achieving competitive performance across 25 challenging continuous
control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably
the complex dog and humanoid environments. This work eliminates the need for
drastic interventions, such as network resets, and offers a simple yet robust
pathway for improving sample efficiency and scalability in model-free
reinforcement learning.

</details>


### [689] [SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries](https://arxiv.org/pdf/2410.19236)
*Darin Tsui, Aryan Musharaf, Yigit Efe Erginbas, Justin Singh Kang, Amirali Aghazadeh*

Main category: cs.LG

TL;DR: SHAP zero is a novel algorithm that reduces the computational cost of Shapley value computation for large-scale biological datasets, enabling efficient and scalable interpretability for sequence models.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable predictions in machine learning models for biological sequences has grown, with Shapley values being a standard for explanations. However, scaling Shapley-based interpretability globally is computationally expensive.

Method: SHAP zero amortizes Shapley value computation costs by leveraging a one-time model sketching step and uncovering connections between Shapley values, high-order feature interactions, and sparse Fourier transforms.

Result: SHAP zero explains predictions much faster than existing methods, revealing rich combinatorial interactions in models of guide RNA efficacy, DNA repair, and protein fitness.

Conclusion: SHAP zero enables efficient, scalable, and principled interpretability for black-box sequence models in biology.

Abstract: The growing adoption of machine learning models for biological sequences has
intensified the need for interpretable predictions, with Shapley values
emerging as a theoretically grounded standard for model explanation. While
effective for local explanations of individual input sequences, scaling
Shapley-based interpretability to extract global biological insights requires
evaluating thousands of sequences--incurring exponential computational cost per
query. We introduce SHAP zero, a novel algorithm that amortizes the cost of
Shapley value computation across large-scale biological datasets. After a
one-time model sketching step, SHAP zero enables near-zero marginal cost for
future queries by uncovering an underexplored connection between Shapley
values, high-order feature interactions, and the sparse Fourier transform of
the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and
protein fitness, SHAP zero explains predictions orders of magnitude faster than
existing methods, recovering rich combinatorial interactions previously
inaccessible at scale. This work opens the door to principled, efficient, and
scalable interpretability for black-box sequence models in biology.

</details>


### [690] [Classifier-Free Guidance: From High-Dimensional Analysis to Generalized Guidance Forms](https://arxiv.org/pdf/2502.07849)
*Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard*

Main category: cs.LG

TL;DR: CFG's distortions vanish in high dimensions, accurately reproducing the target distribution. Non-linear CFG variants show improved robustness and sample quality.


<details>
  <summary>Details</summary>
Motivation: To theoretically analyze CFG's behavior in high dimensions and explore its practical implications for generative models.

Method: High-dimensional analysis of CFG, introducing non-linear generalizations and validating with experiments on diffusion and flow-matching models.

Result: CFG distortions disappear in high dimensions; non-linear variants enhance robustness, fidelity, and diversity.

Conclusion: CFG's high-dimensional behavior ensures accurate target distribution reproduction, with non-linear extensions offering practical benefits.

Abstract: Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion and
flow-based generative models, enabling high-quality conditional generation. A
key theoretical challenge is characterizing the distribution induced by CFG,
particularly in high-dimensional settings relevant to real-world data. Previous
works have shown that CFG modifies the target distribution, steering it towards
a distribution sharper than the target one, more shifted towards the boundary
of the class. In this work, we provide a high-dimensional analysis of CFG,
showing that these distortions vanish as the data dimension grows. We present a
blessing-of-dimensionality result demonstrating that in sufficiently high and
infinite dimensions, CFG accurately reproduces the target distribution. Using
our high-dimensional theory, we show that there is a large family of guidances
enjoying this property, in particular non-linear CFG generalizations. We study
a simple non-linear power-law version, for which we demonstrate improved
robustness, sample fidelity and diversity. Our findings are validated with
experiments on class-conditional and text-to-image generation using
state-of-the-art diffusion and flow-matching models.

</details>


### [691] [KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2411.00278)
*Quan Zhou, Changhua Pei, Fei Sun, Jing Han, Zhengwei Gao, Dan Pei, Haiming Zhang, Gaogang Xie, Jianhui Li*

Main category: cs.LG

TL;DR: KAN-AD improves TSAD by focusing on smooth local patterns, using truncated Fourier expansions for robustness, achieving 15% better accuracy and 50% faster inference.


<details>
  <summary>Details</summary>
Motivation: Current TSAD methods overfit minor fluctuations; effective detection requires modeling normal behavior through smooth local patterns.

Method: Reformulates time series modeling with smooth univariate functions, replaces B-splines with truncated Fourier expansions, and introduces a lightweight learning mechanism.

Result: 15% average accuracy improvement (peaking at 27%) and 50% faster inference with fewer than 1,000 parameters.

Conclusion: KAN-AD is efficient, practical, and outperforms state-of-the-art methods in TSAD.

Abstract: Time series anomaly detection (TSAD) underpins real-time monitoring in cloud
services and web systems, allowing rapid identification of anomalies to prevent
costly failures. Most TSAD methods driven by forecasting models tend to overfit
by emphasizing minor fluctuations. Our analysis reveals that effective TSAD
should focus on modeling "normal" behavior through smooth local patterns. To
achieve this, we reformulate time series modeling as approximating the series
with smooth univariate functions. The local smoothness of each univariate
function ensures that the fitted time series remains resilient against local
disturbances. However, a direct KAN implementation proves susceptible to these
disturbances due to the inherently localized characteristics of B-spline
functions. We thus propose KAN-AD, replacing B-splines with truncated Fourier
expansions and introducing a novel lightweight learning mechanism that
emphasizes global patterns while staying robust to local disturbances. On four
popular TSAD benchmarks, KAN-AD achieves an average 15% improvement in
detection accuracy (with peaks exceeding 27%) over state-of-the-art baselines.
Remarkably, it requires fewer than 1,000 trainable parameters, resulting in a
50% faster inference speed compared to the original KAN, demonstrating the
approach's efficiency and practical viability.

</details>


### [692] [Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination](https://arxiv.org/pdf/2411.01642)
*Md Abrar Jahin, Md. Akmol Masud, M. F. Mridha, Nilanjan Dey, Zeyar Aung*

Main category: cs.LG

TL;DR: The paper introduces QRGCL, a quantum rationale-aware graph contrastive learning framework, to improve quark-gluon jet tagging by enhancing feature extraction and reducing labeled data dependency.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning frameworks for jet tagging lack effective rationale-aware augmentations and face computational inefficiencies, prompting the need for a more efficient and supervised approach.

Method: The proposed QRGCL integrates a quantum rationale generator (QRG) to guide feature extraction and improve discriminative performance, evaluated on a quark-gluon jet dataset.

Result: QRGCL achieves a 77.53% AUC score with only 45 QRG parameters, outperforming classical, quantum, and hybrid benchmarks.

Conclusion: QRGCL demonstrates potential for advancing jet tagging and other complex classification tasks in high-energy physics by addressing computational and feature extraction challenges.

Abstract: In high-energy physics, particle jet tagging plays a pivotal role in
distinguishing quark from gluon jets using data from collider experiments.
While graph-based deep learning methods have advanced this task beyond
traditional feature-engineered approaches, the complex data structure and
limited labeled samples present ongoing challenges. However, existing
contrastive learning (CL) frameworks struggle to leverage rationale-aware
augmentations effectively, often lacking supervision signals that guide the
extraction of salient features and facing computational efficiency issues such
as high parameter counts. In this study, we demonstrate that integrating a
quantum rationale generator (QRG) within our proposed Quantum Rationale-aware
Graph Contrastive Learning (QRGCL) framework significantly enhances jet
discrimination performance, reducing reliance on labeled data and capturing
discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL
achieves an AUC score of $77.53\%$ while maintaining a compact architecture of
only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and
GNN benchmarks. These results highlight QRGCL's potential to advance jet
tagging and other complex classification tasks in high-energy physics, where
computational efficiency and feature extraction limitations persist.

</details>


### [693] [Enhancing the Influence of Labels on Unlabeled Nodes in Graph Convolutional Networks](https://arxiv.org/pdf/2411.02279)
*Jincheng Huang, Yujie Mo, Xiaoshuang Shi, Lei Feng, Xiaofeng Zhu*

Main category: cs.LG

TL;DR: ELU-GCN improves GCNs by learning a new graph structure (ELU-graph) and using contrastive learning to enhance label utilization and generalization.


<details>
  <summary>Details</summary>
Motivation: Additional label information in GCNs doesn't always help; ELU-GCN aims to ensure positive influence and better utilization.

Method: Two-step framework: (1) learn ELU-graph for positive label influence, (2) use contrastive learning on GCN for representation.

Result: Theoretical proof of generalization ability; experiments show ELU-GCN's superiority.

Conclusion: ELU-GCN effectively enhances GCN performance by optimizing label utilization and representation learning.

Abstract: The message-passing mechanism of graph convolutional networks (i.e., GCNs)
enables label information to reach more unlabeled neighbors, thereby increasing
the utilization of labels. However, the additional label information does not
always contribute positively to the GCN. To address this issue, we propose a
new two-step framework called ELU-GCN. In the first stage, ELU-GCN conducts
graph learning to learn a new graph structure (i.e., ELU-graph), which allows
the additional label information to positively influence the predictions of
GCN. In the second stage, we design a new graph contrastive learning on the GCN
framework for representation learning by exploring the consistency and mutually
exclusive information between the learned ELU graph and the original graph.
Moreover, we theoretically demonstrate that the proposed method can ensure the
generalization ability of GCNs. Extensive experiments validate the superiority
of our method.

</details>


### [694] [Differential Privacy in Continual Learning: Which Labels to Update?](https://arxiv.org/pdf/2411.04680)
*Marlon Tobaben, Talal Alrawajfeh, Marcus Klasson, Mikko Heikkilä, Arno Solin, Antti Honkela*

Main category: cs.LG

TL;DR: Combining continual learning (CL) and differential privacy (DP) to retain knowledge while protecting sensitive data, addressing privacy leakage risks in label outputs.


<details>
  <summary>Details</summary>
Motivation: To reconcile CL's knowledge retention with strict privacy requirements for sensitive data, avoiding storage or memorization of individual samples.

Method: Mitigate privacy leakage by using a data-independent large label space, fine-tuning pre-trained models under DP, and learning labels separately with DP.

Result: Overly large label spaces minimally impact utility, while separate DP mechanisms risk losing small classes.

Conclusion: Balancing CL and DP requires careful handling of label spaces to maintain privacy without sacrificing utility.

Abstract: The goal of continual learning (CL) is to retain knowledge across tasks, but
this conflicts with strict privacy required for sensitive training data that
prevents storing or memorising individual samples. To address that, we combine
CL and differential privacy (DP). We highlight that failing to account for
privacy leakage through the set of labels a model can output can break the
privacy of otherwise valid DP algorithms. This is especially relevant in CL. We
show that mitigating the issue with a data-independent overly large label space
can have minimal negative impact on utility when fine-tuning a pre-trained
model under DP, while learning the labels with a separate DP mechanism risks
losing small classes.

</details>


### [695] [SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers](https://arxiv.org/pdf/2411.10510)
*Joseph Liu, Joshua Geddes, Ziyu Guo, Haomiao Jiang, Mahesh Kumar Nandwana*

Main category: cs.LG

TL;DR: SmoothCache accelerates DiT inference by caching and reusing similar layer outputs across timesteps, achieving 8-71% speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: DiT models are computationally expensive during inference due to repeated evaluations of attention and feed-forward modules.

Method: SmoothCache analyzes layer-wise representation errors from a calibration set to cache and reuse key features adaptively.

Result: Achieves 8-71% speedup while maintaining or improving generation quality across image, video, and audio tasks.

Conclusion: SmoothCache enables real-time DiT applications and enhances accessibility of these models.

Abstract: Diffusion Transformers (DiT) have emerged as powerful generative models for
various tasks, including image, video, and speech synthesis. However, their
inference process remains computationally expensive due to the repeated
evaluation of resource-intensive attention and feed-forward modules. To address
this, we introduce SmoothCache, a model-agnostic inference acceleration
technique for DiT architectures. SmoothCache leverages the observed high
similarity between layer outputs across adjacent diffusion timesteps. By
analyzing layer-wise representation errors from a small calibration set,
SmoothCache adaptively caches and reuses key features during inference. Our
experiments demonstrate that SmoothCache achieves 8% to 71% speed up while
maintaining or even improving generation quality across diverse modalities. We
showcase its effectiveness on DiT-XL for image generation, Open-Sora for
text-to-video, and Stable Audio Open for text-to-audio, highlighting its
potential to enable real-time applications and broaden the accessibility of
powerful DiT models.

</details>


### [696] [POPGym Arcade: Parallel Pixelated POMDPs](https://arxiv.org/pdf/2503.01450)
*Zekang Wang, Zhe He, Borong Zhang, Edan Toledo, Steven Morad*

Main category: cs.LG

TL;DR: POPGym Arcade offers pixel-based environments with shared observation/action spaces, including partial observability variants, and introduces tools to analyze policies, revealing memory-related brittleness and poisoning risks.


<details>
  <summary>Details</summary>
Motivation: To enable counterfactual studies on partial observability and analyze how agents use past information for decision-making.

Method: Developed hardware-accelerated environments with fully/partially observable variants and introduced mathematical tools for policy analysis under partial observability.

Result: Found that controlling partial observability is critical, long-term memory leads to brittle policies, and recurrent policies can be poisoned by old observations.

Conclusion: Highlights challenges in sim-to-real transfer, imitation learning, and offline RL due to memory brittleness and poisoning risks.

Abstract: We present the POPGym Arcade, a collection of hardware-accelerated,
pixel-based environments with shared observation and action spaces. Each
environment includes fully and partially observable variants, enabling
counterfactual studies on partial observability. We also introduce mathematical
tools for analyzing policies under partial observability, which reveal how
agents recall past information to make decisions. Our analysis shows (1) that
controlling for partial observability is critical and (2) that agents with
long-term memory learn brittle policies that struggle to generalize. Finally,
we demonstrate that recurrent policies can be "poisoned" by old,
out-of-distribution observations, with implications for sim-to-real transfer,
imitation learning, and offline reinforcement learning.

</details>


### [697] [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/pdf/2412.04614)
*Jiahai Feng, Stuart Russell, Jacob Steinhardt*

Main category: cs.LG

TL;DR: Pretrained LMs generalize implications of finetuned facts via extractive structures, learned during pretraining when facts precede implications.


<details>
  <summary>Details</summary>
Motivation: To understand how LMs generalize implications of facts and the mechanisms enabling this, focusing on extractive structures.

Method: Introduces extractive structures (informative, upstream, downstream components) and tests hypotheses via data ordering and weight grafting effects in models like OLMo-7b, Llama 3-8b.

Result: Empirical validation shows extractive structures enable generalization, with fact learning occurring in early/late layers, leading to different generalization forms.

Conclusion: Extractive structures explain LM generalization, learned when facts precede implications, with implications for model design and pretraining strategies.

Abstract: Pretrained language models (LMs) can generalize to implications of facts that
they are finetuned on. For example, if finetuned on ``John Doe lives in Tokyo,"
LMs can correctly answer ``What language do the people in John Doe's city
speak?'' with ``Japanese''. However, little is known about the mechanisms that
enable this generalization or how they are learned during pretraining. We
introduce extractive structures as a framework for describing how components in
LMs (e.g., MLPs or attention heads) coordinate to enable this generalization.
The structures consist of informative components that store training facts as
weight changes, and upstream and downstream extractive components that query
and process the stored information to produce the correct implication. We
hypothesize that extractive structures are learned during pretraining when
encountering implications of previously known facts. This yields two
predictions: a data ordering effect where extractive structures can be learned
only if facts precede their implications, and a weight grafting effect where
extractive structures can be transferred to predict counterfactual
implications. We empirically demonstrate these phenomena in the OLMo-7b, Llama
3-8b, Gemma 2-9b, and Qwen 2-7b models. Of independent interest, our results
also indicate that fact learning can occur at both early and late layers, which
lead to different forms of generalization.

</details>


### [698] [Steer LLM Latents for Hallucination Detection](https://arxiv.org/pdf/2503.01917)
*Seongheon Park, Xuefeng Du, Min-Hsuan Yeh, Haobo Wang, Yixuan Li*

Main category: cs.LG

TL;DR: The paper introduces the Truthfulness Separator Vector (TSV) to detect hallucinations in LLMs by reshaping the representation space, achieving state-of-the-art performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs hinder safe deployment; existing methods using latent space embeddings fail to clearly separate truthful and hallucinated content.

Method: Proposes TSV, a lightweight steering vector trained on labeled exemplars and augmented with unlabeled LLM generations using optimal transport-based pseudo-labeling and confidence filtering.

Result: TSV achieves state-of-the-art performance, generalizes well across datasets, and requires minimal labeled data.

Conclusion: TSV provides a practical solution for enhancing hallucination detection in real-world LLM applications without altering model parameters.

Abstract: Hallucinations in LLMs pose a significant concern to their safe deployment in
real-world applications. Recent approaches have leveraged the latent space of
LLMs for hallucination detection, but their embeddings, optimized for
linguistic coherence rather than factual accuracy, often fail to clearly
separate truthful and hallucinated content. To this end, we propose the
Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector
that reshapes the LLM's representation space during inference to enhance the
separation between truthful and hallucinated outputs, without altering model
parameters. Our two-stage framework first trains TSV on a small set of labeled
exemplars to form compact and well-separated clusters. It then augments the
exemplar set with unlabeled LLM generations, employing an optimal
transport-based algorithm for pseudo-labeling combined with a confidence-based
filtering process. Extensive experiments demonstrate that TSV achieves
state-of-the-art performance with minimal labeled data, exhibiting strong
generalization across datasets and providing a practical solution for
real-world LLM applications.

</details>


### [699] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2412.07067)
*Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai*

Main category: cs.LG

TL;DR: MoE-CAP is a benchmark for MoE systems, highlighting the trade-offs between Cost, Accuracy, and Performance (CAP) in sparse MoE architectures.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to accurately capture the trade-offs in MoE systems, complicating deployment decisions.

Method: Introduces MoE-CAP benchmark, CAP Radar Diagram, and sparsity-aware metrics (S-MBU, S-MFU) for performance evaluation.

Result: Optimal CAP balance is hard to achieve; MoE systems often sacrifice one dimension for the other two.

Conclusion: MoE-CAP provides a framework for better benchmarking and understanding of MoE system trade-offs.

Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [700] [Transformers for molecular property prediction: Domain adaptation efficiently improves performance](https://arxiv.org/pdf/2503.03360)
*Afnan Sultan, Max Rausch-Dupont, Shahrukh Khan, Olga Kalinina, Dietrich Klakow, Andrea Volkamer*

Main category: cs.LG

TL;DR: Large-scale pre-training beyond 400K-800K molecules doesn't improve molecular property prediction, but domain adaptation with small, domain-specific datasets significantly boosts performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of pre-training dataset size and chemically informed objectives on molecular property prediction performance.

Method: Assessed transformer models using pre-training and domain adaptation, comparing performance across datasets and against Random Forest baselines.

Result: Domain adaptation with small datasets (≤4K molecules) significantly improves performance, outperforming larger models like MolFormer and matching MolBERT.

Conclusion: Aligning pre-training and adaptation with chemically meaningful tasks and domain-relevant data enhances transformer performance in molecular property prediction.

Abstract: Over the past six years, molecular transformer models have become key tools
in drug discovery. Most existing models are pre-trained on large, unlabeled
datasets such as ZINC or ChEMBL. However, the extent to which large-scale
pre-training improves molecular property prediction remains unclear. This study
evaluates transformer models for this task while addressing their limitations.
We explore how pre-training dataset size and chemically informed objectives
impact performance. Our results show that increasing the dataset beyond
approximately 400K to 800K molecules from large-scale unlabeled databases does
not enhance performance across seven datasets covering five ADME endpoints:
lipophilicity, permeability, solubility (two datasets), microsomal stability
(two datasets), and plasma protein binding. In contrast, domain adaptation on a
small, domain-specific dataset (less than or equal 4K molecules) using
multi-task regression of physicochemical properties significantly boosts
performance (P-value less than 0.001). A model pre-trained on 400K molecules
and adapted with domain-specific data outperforms larger models such as
MolFormer and performs comparably to MolBERT. Benchmarks against Random Forest
(RF) baselines using descriptors and Morgan fingerprints show that chemically
and physically informed features consistently yield better performance across
model types. While RF remains a strong baseline, we identify concrete practices
to enhance transformer performance. Aligning pre-training and adaptation with
chemically meaningful tasks and domain-relevant data presents a promising
direction for molecular property prediction. Our models are available on
HuggingFace for easy use and adaptation.

</details>


### [701] [Joint Hierarchical Representation Learning of Samples and Features via Informed Tree-Wasserstein Distance](https://arxiv.org/pdf/2501.03627)
*Ya-Wei Eileen Lin, Ronald R. Coifman, Gal Mishne, Ronen Talmon*

Main category: cs.LG

TL;DR: Proposes an unsupervised method for joint hierarchical representation learning of samples and features using Tree-Wasserstein Distance (TWD), alternating between modes to refine trees and TWDs.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data often have hierarchical structures in both samples and features, but existing methods address only one mode at a time.

Method: Alternates between constructing trees for one mode and computing TWD for the other, refining both iteratively.

Result: Convergence is proven theoretically; method improves performance in tasks like link prediction, node classification, sparse approximation, and Wasserstein distance learning.

Conclusion: The method effectively captures hierarchical structures in data and enhances downstream tasks, outperforming baselines.

Abstract: High-dimensional data often exhibit hierarchical structures in both modes:
samples and features. Yet, most existing approaches for hierarchical
representation learning consider only one mode at a time. In this work, we
propose an unsupervised method for jointly learning hierarchical
representations of samples and features via Tree-Wasserstein Distance (TWD).
Our method alternates between the two data modes. It first constructs a tree
for one mode, then computes a TWD for the other mode based on that tree, and
finally uses the resulting TWD to build the second mode's tree. By repeatedly
alternating through these steps, the method gradually refines both trees and
the corresponding TWDs, capturing meaningful hierarchical representations of
the data. We provide a theoretical analysis showing that our method converges.
We show that our method can be integrated into hyperbolic graph convolutional
networks as a pre-processing technique, improving performance in link
prediction and node classification tasks. In addition, our method outperforms
baselines in sparse approximation and unsupervised Wasserstein distance
learning tasks on word-document and single-cell RNA-sequencing datasets.

</details>


### [702] [All-atom Diffusion Transformers: Unified generative modelling of molecules and materials](https://arxiv.org/pdf/2503.03965)
*Chaitanya K. Joshi, Xiang Fu, Yi-Lun Liao, Vahe Gharakhanyan, Benjamin Kurt Miller, Anuroop Sriram, Zachary W. Ulissi*

Main category: cs.LG

TL;DR: ADiT is a unified latent diffusion framework for generating both molecules and materials using the same model, achieving state-of-the-art results with standard Transformers.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for 3D atomic systems are highly specific to target systems, despite shared underlying physics. ADiT aims to unify generative processes for molecules and materials.

Method: ADiT combines an autoencoder for shared latent space mapping and a diffusion model for generating new latent embeddings, using standard Transformers with minimal biases.

Result: ADiT achieves state-of-the-art performance on MP20, QM9, and GEOM-DRUGS datasets, with faster training and inference than equivariant models. Scaling improves performance predictably.

Conclusion: ADiT represents progress toward generalizable foundation models for generative chemistry, with open-source availability.

Abstract: Diffusion models are the standard toolkit for generative modelling of 3D
atomic systems. However, for different types of atomic systems -- such as
molecules and materials -- the generative processes are usually highly specific
to the target system despite the underlying physics being the same. We
introduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusion
framework for jointly generating both periodic materials and non-periodic
molecular systems using the same model: (1) An autoencoder maps a unified,
all-atom representations of molecules and materials to a shared latent
embedding space; and (2) A diffusion model is trained to generate new latent
embeddings that the autoencoder can decode to sample new molecules or
materials. Experiments on MP20, QM9 and GEOM-DRUGS datasets demonstrate that
jointly trained ADiT generates realistic and valid molecules as well as
materials, obtaining state-of-the-art results on par with molecule and
crystal-specific models. ADiT uses standard Transformers with minimal inductive
biases for both the autoencoder and diffusion model, resulting in significant
speedups during training and inference compared to equivariant diffusion
models. Scaling ADiT up to half a billion parameters predictably improves
performance, representing a step towards broadly generalizable foundation
models for generative chemistry. Open source code:
https://github.com/facebookresearch/all-atom-diffusion-transformer

</details>


### [703] [Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity](https://arxiv.org/pdf/2501.16168)
*Artavazd Maranjyan, Alexander Tyurin, Peter Richtárik*

Main category: cs.LG

TL;DR: Ringmaster ASGD is a new Asynchronous SGD method achieving optimal time complexity under heterogeneous worker computation times.


<details>
  <summary>Details</summary>
Motivation: Existing Asynchronous SGD methods fail to achieve optimal time complexity under heterogeneous computation times, creating inefficiencies as worker numbers scale.

Method: Proposes Ringmaster ASGD, designed to handle heterogeneous and dynamically fluctuating worker computation times.

Result: Theoretical analysis shows Ringmaster ASGD achieves optimal time complexity, meeting lower bounds for such scenarios.

Conclusion: Ringmaster ASGD is the first Asynchronous SGD method to achieve optimal time complexity under challenging conditions.

Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone
method for parallelizing learning in distributed machine learning. However, its
performance suffers under arbitrarily heterogeneous computation times across
workers, leading to suboptimal time complexity and inefficiency as the number
of workers scales. While several Asynchronous SGD variants have been proposed,
recent findings by Tyurin & Richt\'arik (NeurIPS 2023) reveal that none achieve
optimal time complexity, leaving a significant gap in the literature. In this
paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to
address these limitations and tame the inherent challenges of Asynchronous SGD.
We establish, through rigorous theoretical analysis, that Ringmaster ASGD
achieves optimal time complexity under arbitrarily heterogeneous and
dynamically fluctuating worker computation times. This makes it the first
Asynchronous SGD method to meet the theoretical lower bounds for time
complexity in such scenarios.

</details>


### [704] [Joint Pricing and Resource Allocation: An Optimal Online-Learning Approach](https://arxiv.org/pdf/2501.18049)
*Jianyu Xu, Xuan Wang, Yu-Xiang Wang, Jiashuo Jiang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning problem on dynamic pricing and resource
allocation, where we make joint pricing and inventory decisions to maximize the
overall net profit. We consider the stochastic dependence of demands on the
price, which complicates the resource allocation process and introduces
significant non-convexity and non-smoothness to the problem. To solve this
problem, we develop an efficient algorithm that utilizes a "Lower-Confidence
Bound (LCB)" meta-strategy over multiple OCO agents. Our algorithm achieves
$\tilde{O}(\sqrt{Tmn})$ regret (for $m$ suppliers and $n$ consumers), which is
optimal with respect to the time horizon $T$. Our results illustrate an
effective integration of statistical learning methodologies with complex
operations research problems.

</details>


### [705] [Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts](https://arxiv.org/pdf/2503.05066)
*Shwai He, Weilin Cai, Jiayi Huang, Ang Li*

Main category: cs.LG

TL;DR: The paper addresses the inefficiency in Mixture of Experts (MoE) inference due to imbalanced token-to-expert assignments, proposing Capacity-Aware Inference with token drop and reroute techniques to improve speed and resource utilization.


<details>
  <summary>Details</summary>
Motivation: MoE's inference inefficiency stems from the Straggler Effect, where imbalanced token assignments cause some experts to be overloaded, leading to poor resource use and higher latency.

Method: Proposes Capacity-Aware Inference: (1) Capacity-Aware Token Drop to discard overloaded tokens, and (2) Capacity-Aware Token Reroute to redistribute tokens to underutilized experts.

Result: Experiments show a 0.2% performance increase and 1.94× inference speedup on Mixtral-8×7B-Instruct.

Conclusion: The proposed techniques effectively mitigate the Straggler Effect, enhancing MoE inference efficiency.

Abstract: The Mixture of Experts (MoE) is an effective architecture for scaling large
language models by leveraging sparse expert activation, optimizing the
trade-off between performance and efficiency. However, under expert
parallelism, MoE suffers from inference inefficiencies due to imbalanced
token-to-expert assignment, where some experts are overloaded while others
remain underutilized. This imbalance leads to poor resource utilization and
increased latency, as the most burdened expert dictates the overall delay, a
phenomenon we define as the \textbf{\textit{Straggler Effect}}. To mitigate
this, we propose Capacity-Aware Inference, including two key techniques: (1)
\textbf{\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens
to regulate the maximum latency of MoE, and (2) \textbf{\textit{Capacity-Aware
Token Reroute}}, which reallocates overflowed tokens to underutilized experts,
balancing the token distribution. These techniques collectively optimize both
high-load and low-load expert utilization, leading to a more efficient MoE
inference pipeline. Extensive experiments demonstrate the effectiveness of our
methods, showing significant improvements in inference efficiency, e.g., 0.2\%
average performance increase and a 1.94$\times$ inference speedup on
Mixtral-8$\times$7B-Instruct.

</details>


### [706] [ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning](https://arxiv.org/pdf/2502.00775)
*Artavazd Maranjyan, El Mehdi Saad, Peter Richtárik, Francesco Orabona*

Main category: cs.LG

TL;DR: ATA (Adaptive Task Allocation) optimizes task distribution in distributed machine learning by adapting to unknown worker computation times, outperforming greedy methods in efficiency and cost.


<details>
  <summary>Details</summary>
Motivation: Asynchronous methods in distributed machine learning often waste resources due to unpredictable computation times across devices. ATA aims to optimize task allocation without prior knowledge of these times.

Method: ATA dynamically adapts task allocation based on observed worker computation times, achieving near-optimal performance without requiring prior knowledge.

Result: Theoretical and experimental results show ATA matches the performance of methods with prior knowledge and significantly reduces resource costs compared to greedy approaches.

Conclusion: ATA provides an efficient, adaptive solution for task allocation in distributed learning, mitigating the inefficiencies of greedy methods.

Abstract: Asynchronous methods are fundamental for parallelizing computations in
distributed machine learning. They aim to accelerate training by fully
utilizing all available resources. However, their greedy approach can lead to
inefficiencies using more computation than required, especially when
computation times vary across devices. If the computation times were known in
advance, training could be fast and resource-efficient by assigning more tasks
to faster workers. The challenge lies in achieving this optimal allocation
without prior knowledge of the computation time distributions. In this paper,
we propose ATA (Adaptive Task Allocation), a method that adapts to
heterogeneous and random distributions of worker computation times. Through
rigorous theoretical analysis, we show that ATA identifies the optimal task
allocation and performs comparably to methods with prior knowledge of
computation times. Experimental results further demonstrate that ATA is
resource-efficient, significantly reducing costs compared to the greedy
approach, which can be arbitrarily expensive depending on the number of
workers.

</details>


### [707] [Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity](https://arxiv.org/pdf/2502.01171)
*Erpai Luo, Xinran Wei, Lin Huang, Yunyang Li, Han Yang, Zaishuo Xia, Zun Wang, Chang Liu, Bin Shao, Jia Zhang*

Main category: cs.LG

TL;DR: SPHNet introduces adaptive sparsity to reduce computational costs in SE(3) equivariant networks for Hamiltonian matrix prediction, achieving high accuracy and speedup.


<details>
  <summary>Details</summary>
Motivation: High computational costs of SE(3) equivariant networks limit scalability for large molecular systems.

Method: SPHNet uses sparse gates and a Three-phase Sparsity Scheduler to selectively reduce tensor product operations.

Result: Achieves state-of-the-art accuracy with up to 7x speedup on QH9 and PubchemQH datasets.

Conclusion: SPHNet's sparsification techniques improve efficiency and scalability of SE(3) equivariant networks, broadening their applicability.

Abstract: Hamiltonian matrix prediction is pivotal in computational chemistry, serving
as the foundation for determining a wide range of molecular properties. While
SE(3) equivariant graph neural networks have achieved remarkable success in
this domain, their substantial computational cost--driven by high-order tensor
product (TP) operations--restricts their scalability to large molecular systems
with extensive basis sets. To address this challenge, we introduce SPHNet, an
efficient and scalable equivariant network, that incorporates adaptive SParsity
into Hamiltonian prediction. SPHNet employs two innovative sparse gates to
selectively constrain non-critical interaction combinations, significantly
reducing tensor product computations while maintaining accuracy. To optimize
the sparse representation, we develop a Three-phase Sparsity Scheduler,
ensuring stable convergence and achieving high performance at sparsity rates of
up to 70%. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that
SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup
over existing models. Beyond Hamiltonian prediction, the proposed
sparsification techniques also hold significant potential for improving the
efficiency and scalability of other SE(3) equivariant networks, further
broadening their applicability and impact. Our code can be found at
https://github.com/microsoft/SPHNet.

</details>


### [708] [Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback](https://arxiv.org/pdf/2503.17682)
*Jiaming Ji, Xinyu Chen, Rui Pan, Conghui Zhang, Han Zhu, Jiahao Li, Donghai Hong, Boyuan Chen, Jiayi Zhou, Kaile Wang, Juntao Dai, Chi-Min Chan, Yida Tang, Sirui Han, Yike Guo, Yaodong Yang*

Main category: cs.LG

TL;DR: The paper introduces Safe RLHF-V, a multimodal safety alignment framework, addressing safety risks in MLLMs through a dataset (BeaverTails-V), a guardrail system (Beaver-Guard-V), and constrained optimization, improving safety and helpfulness.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) pose safety risks, necessitating alignment to prevent undesired behaviors while preserving capabilities. Existing datasets lack disentangled safety constraints, and their integration into optimization for multimodal models is unexplored.

Method: The framework includes: (I) BeaverTails-V dataset with dual preference annotations and multi-level safety labels, (II) Beaver-Guard-V guardrail system for proactive defense, and (III) constrained optimization for multimodal safety alignment.

Result: Safe RLHF-V improves model safety by 34.2% and helpfulness by 34.3%, with Beaver-Guard-V enhancing precursor model safety by 40.9% over five filtering rounds.

Conclusion: Safe RLHF-V successfully addresses safety alignment in MLLMs, demonstrating significant improvements in both safety and helpfulness through a systematic framework.

Abstract: Multimodal large language models (MLLMs) are essential for building
general-purpose AI assistants; however, they pose increasing safety risks. How
can we ensure safety alignment of MLLMs to prevent undesired behaviors? Going
further, it is critical to explore how to fine-tune MLLMs to preserve
capabilities while meeting safety constraints. Fundamentally, this challenge
can be formulated as a min-max optimization problem. However, existing datasets
have not yet disentangled single preference signals into explicit safety
constraints, hindering systematic investigation in this direction. Moreover, it
remains an open question whether such constraints can be effectively
incorporated into the optimization process for multi-modal models. In this
work, we present the first exploration of the Safe RLHF-V -- the first
multimodal safety alignment framework. The framework consists of:
$\mathbf{(I)}$ BeaverTails-V, the first open-source dataset featuring dual
preference annotations for helpfulness and safety, supplemented with
multi-level safety labels (minor, moderate, severe); $\mathbf{(II)}$
Beaver-Guard-V, a multi-level guardrail system to proactively defend against
unsafe queries and adversarial attacks. Applying the guard model over five
rounds of filtering and regeneration significantly enhances the precursor
model's overall safety by an average of 40.9%. $\mathbf{(III)}$ Based on dual
preference, we initiate the first exploration of multi-modal safety alignment
within a constrained optimization. Experimental results demonstrate that Safe
RLHF effectively improves both model helpfulness and safety. Specifically, Safe
RLHF-V enhances model safety by 34.2% and helpfulness by 34.3%.

</details>


### [709] [Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension](https://arxiv.org/pdf/2502.05075)
*Yijun Dong, Yicheng Li, Yunai Li, Jason D. Lee, Qi Lei*

Main category: cs.LG

TL;DR: Weak-to-strong (W2S) generalization improves performance by finetuning a strong model on weak pseudo-labels, leveraging low-dimensional subspaces to reduce variance.


<details>
  <summary>Details</summary>
Motivation: To understand why W2S finetuning outperforms the weak teacher, focusing on the role of low-dimensional feature spaces.

Method: Analyzes W2S in ridgeless regression, characterizing variance reduction in shared and discrepant subspaces of strong and weak models.

Result: Variance is inherited in shared subspaces but reduced in discrepant ones, with performance gap recovery scaling with sample size.

Conclusion: W2S benefits from model discrepancy and low-dimensionality, supported by experiments on synthetic and real-world tasks.

Abstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a
strong (large) student model is trained on pseudo-labels generated by a weak
teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to
understand this phenomenon through the observation that FT often occurs in
intrinsically low-dimensional spaces. Leveraging the low intrinsic
dimensionality of FT, we analyze W2S in the ridgeless regression setting from a
variance reduction perspective. For a strong student-weak teacher pair with
sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s,
\mathcal{V}_w$, we provide an exact characterization of the variance that
dominates the generalization error of W2S. This unveils a virtue of discrepancy
between the strong and weak models in W2S: the variance of the weak teacher is
inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while
reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy
$\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Our
analysis further casts light on the sample complexities and the scaling of
performance gap recovery in W2S. The analysis is supported by experiments on
synthetic regression problems, as well as real vision and NLP tasks.

</details>


### [710] [Stroke Disease Classification Using Machine Learning with Feature Selection Techniques](https://arxiv.org/pdf/2504.00485)
*Mahade Hasan, Farhana Yasmin, Xue Yu*

Main category: cs.LG

TL;DR: The paper evaluates nine ML algorithms for heart disease prediction, with XGBoost achieving 99% accuracy and other high metrics.


<details>
  <summary>Details</summary>
Motivation: Heart disease is a major global health issue, and existing ML models lack sufficient accuracy for reliable early detection.

Method: Applied nine ML algorithms with feature selection, hyperparameter tuning, and a novel voting system. Evaluated using accuracy, precision, recall, F1-score, and ROC AUC.

Result: XGBoost outperformed others with 99% accuracy, precision, F1-score, 98% recall, and 100% ROC AUC.

Conclusion: The study presents a highly accurate approach for early heart disease diagnosis, enhancing preventive healthcare.

Abstract: Heart disease remains a leading cause of mortality and morbidity worldwide,
necessitating the development of accurate and reliable predictive models to
facilitate early detection and intervention. While state of the art work has
focused on various machine learning approaches for predicting heart disease,
but they could not able to achieve remarkable accuracy. In response to this
need, we applied nine machine learning algorithms XGBoost, logistic regression,
decision tree, random forest, k-nearest neighbors (KNN), support vector machine
(SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear
regression to predict heart disease based on a range of physiological
indicators. Our approach involved feature selection techniques to identify the
most relevant predictors, aimed at refining the models to enhance both
performance and interpretability. The models were trained, incorporating
processes such as grid search hyperparameter tuning, and cross-validation to
minimize overfitting. Additionally, we have developed a novel voting system
with feature selection techniques to advance heart disease classification.
Furthermore, we have evaluated the models using key performance metrics
including accuracy, precision, recall, F1-score, and the area under the
receiver operating characteristic curve (ROC AUC). Among the models, XGBoost
demonstrated exceptional performance, achieving 99% accuracy, precision,
F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach
to early heart disease diagnosis and preventive healthcare.

</details>


### [711] [Beyond Benign Overfitting in Nadaraya-Watson Interpolators](https://arxiv.org/pdf/2502.07480)
*Daniel Barzilai, Guy Kornowski, Ohad Shamir*

Main category: cs.LG

TL;DR: The paper revisits the Nadaraya-Watson estimator to explore its generalization behavior, revealing multiple overfitting patterns and suggesting hyperparameter tuning insights.


<details>
  <summary>Details</summary>
Motivation: To understand the generalization behavior of interpolating predictors, particularly the Nadaraya-Watson estimator, in the context of noisy training data.

Method: Analyzing the NW estimator by varying a bandwidth-like hyperparameter to study its generalization capabilities.

Result: Identifies multiple overfitting behaviors (catastrophic, benign, tempered) and suggests over-estimating data's intrinsic dimension is safer for tuning.

Conclusion: Classical interpolating methods exhibit complex generalization behaviors, with hyperparameter tuning insights for practical applications.

Abstract: In recent years, there has been much interest in understanding the
generalization behavior of interpolating predictors, which overfit on noisy
training data. Whereas standard analyses are concerned with whether a method is
consistent or not, recent observations have shown that even inconsistent
predictors can generalize well. In this work, we revisit the classic
interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),
and study its generalization capabilities through this modern viewpoint. In
particular, by varying a single bandwidth-like hyperparameter, we prove the
existence of multiple overfitting behaviors, ranging non-monotonically from
catastrophic, through benign, to tempered. Our results highlight how even
classical interpolating methods can exhibit intricate generalization behaviors.
In addition, for the purpose of tuning the hyperparameter, the results suggest
that over-estimating the intrinsic dimension of the data is less harmful than
under-estimating it. Numerical experiments complement our theory, demonstrating
the same phenomena.

</details>


### [712] [Dion: Distributed Orthonormalized Updates](https://arxiv.org/pdf/2504.05295)
*Kwangjun Ahn, Byron Xu, Natalie Abreu, John Langford*

Main category: cs.LG

TL;DR: Dion (DIstributed OrthoNormalization) is a scalable optimizer for large-scale distributed LLM training, enabling efficient orthonormal matrix updates without full gradient synchronization.


<details>
  <summary>Details</summary>
Motivation: Orthonormal matrix updates improve neural network optimization but are challenging to apply efficiently in distributed training systems.

Method: Dion uses low-rank approximation and decoupled momentum buffers to avoid full gradient synchronization while maintaining numerical equivalence. It works with DDP, FSDP, and TP parallelism.

Result: Dion's benefits scale with model and batch size, as shown in evaluations on models from 120M to 3B parameters.

Conclusion: Dion is a practical solution for efficient orthonormal updates in distributed LLM training, with advantages growing for larger models.

Abstract: Recent work has shown that orthonormal matrix updates speed up neural network
optimization, improve training stability, and offer better hyperparameter
transfer across model sizes. Applying these updates efficiently when model
weights and optimizer states are sharded across a large-scale distributed LLM
training system remains a major challenge. We introduce Dion (DIstributed
OrthoNormalization), a scalable and communication-efficient orthonormalizing
optimizer. Dion leverages low-rank approximation and decoupled momentum
buffers, eliminating the need for full gradient synchronization while producing
numerically equivalent results. It is compatible with simultaneous DDP, FSDP,
and TP parallelism, and it computes an orthonormalized update without
unsharding a full parameter matrix on any single device. We evaluate Dion on
language models from 120M to 3B parameters and find that its benefits improve
with increasing model size and batch size.

</details>


### [713] [Optimizing Asynchronous Federated Learning: A~Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency](https://arxiv.org/pdf/2502.08206)
*Abdelkrim Alahyane, Céline Comte, Matthieu Jonckheere, Éric Moulines*

Main category: cs.LG

TL;DR: The paper analyzes asynchronous federated learning (FL) to address the straggler effect, optimizing design choices like concurrency and routing to balance gradient staleness and system throughput, improving accuracy by 10-30%.


<details>
  <summary>Details</summary>
Motivation: Synchronous FL scales poorly due to stragglers; asynchronous methods like FedAsync exist but lack understanding of design impacts. This work aims to model and optimize these choices.

Method: Uses stochastic modeling to analyze asynchronous FL, focusing on concurrency and routing. Introduces metrics for staleness and throughput, optimizing loss via closed-form expressions and numerical bounds.

Result: Proves a discrete Little's law variant for staleness, optimizes loss per update, and introduces a new metric balancing staleness and throughput, achieving 10-30% accuracy gains.

Conclusion: Optimizing asynchronous FL requires balancing staleness and throughput; the proposed metrics and methods significantly improve accuracy.

Abstract: Synchronous federated learning (FL) scales poorly with the number of clients
due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync
address this limitation by enabling asynchronous communication between clients
and the central server. In this work, we rely on stochastic modeling and
analysis to better understand the impact of design choices in asynchronous FL
algorithms, such as the concurrency level and routing probabilities, and we
leverage this knowledge to optimize loss. Compared to most existing studies, we
account for the joint impact of heterogeneous and variable service speeds and
heterogeneous datasets at the clients. We characterize in particular a
fundamental trade-off for optimizing asynchronous FL: minimizing gradient
estimation errors by avoiding model parameter staleness, while also speeding up
the system by increasing the throughput of model updates. Our two main
contributions can be summarized as follows. First, we prove a discrete variant
of Little's law to derive a closed-form expression for relative delay, a metric
that quantifies staleness. This allows us to efficiently minimize the average
loss per model update, which has been the gold standard in literature to date.
Second, we observe that naively optimizing this metric leads us to slow down
the system drastically by overemphazing staleness at the detriment of
throughput. This motivates us to introduce an alternative metric that also
takes system speed into account, for which we derive a tractable upper-bound
that can be minimized numerically. Extensive numerical results show that these
optimizations enhance accuracy by 10% to 30%.

</details>


### [714] [TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning](https://arxiv.org/pdf/2504.05585)
*Yuxuan Li, Yicheng Gao, Ning Yang, Stephen Xia*

Main category: cs.LG

TL;DR: TW-CRL, an IRL framework, uses time-weighted contrastive learning to derive dense rewards from both successful and failed demonstrations, improving RL performance in episodic tasks with sparse rewards and trap states.


<details>
  <summary>Details</summary>
Motivation: Episodic RL tasks suffer from sparse rewards and hidden trap states, making learning inefficient. Existing methods lack mechanisms to avoid irreversible failures without explicit negative rewards.

Method: TW-CRL employs Inverse Reinforcement Learning with temporal weighting to learn dense rewards from diverse demonstrations, identifying critical states for success or failure.

Result: TW-CRL outperforms state-of-the-art methods in navigation and robotic manipulation tasks, showing better efficiency and robustness.

Conclusion: TW-CRL effectively addresses sparse rewards and trap states in RL, enhancing learning through dense reward signals and improved exploration.

Abstract: Episodic tasks in Reinforcement Learning (RL) often pose challenges due to
sparse reward signals and high-dimensional state spaces, which hinder efficient
learning. Additionally, these tasks often feature hidden "trap states" --
irreversible failures that prevent task completion but do not provide explicit
negative rewards to guide agents away from repeated errors. To address these
issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an
Inverse Reinforcement Learning (IRL) framework that leverages both successful
and failed demonstrations. By incorporating temporal information, TW-CRL learns
a dense reward function that identifies critical states associated with success
or failure. This approach not only enables agents to avoid trap states but also
encourages meaningful exploration beyond simple imitation of expert
trajectories. Empirical evaluations on navigation tasks and robotic
manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art
methods, achieving improved efficiency and robustness.

</details>


### [715] [DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization](https://arxiv.org/pdf/2502.12413)
*Jiaqi Wang, Yuhang Zhou, Zhixiong Zhang, Qiguang Chen, Yongqiang Chen, James Cheng*

Main category: cs.LG

TL;DR: The paper addresses over-invariance in invariant learning (IL) for out-of-distribution generalization, proposing DivIL to mitigate it with contrastive learning and random masking.


<details>
  <summary>Details</summary>
Motivation: Over-invariance in IL methods can degrade generalization by losing important invariant features due to strong constraints.

Method: Proposes Diverse Invariant Learning (DivIL), combining unsupervised contrastive learning and random masking to compensate for invariant constraints.

Result: Experiments on 12 datasets and 6 models validate the over-invariance issue and DivIL's effectiveness.

Conclusion: DivIL successfully mitigates over-invariance, improving generalization across diverse datasets and models.

Abstract: Out-of-distribution generalization is a common problem that expects the model
to perform well in the different distributions even far from the train data. A
popular approach to addressing this issue is invariant learning (IL), in which
the model is compiled to focus on invariant features instead of spurious
features by adding strong constraints during training. However, there are some
potential pitfalls of strong invariant constraints. Due to the limited number
of diverse environments and over-regularization in the feature space, it may
lead to a loss of important details in the invariant features while alleviating
the spurious correlations, namely the over-invariance, which can also degrade
the generalization performance. We theoretically define the over-invariance and
observe that this issue occurs in various classic IL methods. To alleviate this
issue, we propose a simple approach Diverse Invariant Learning (DivIL) by
adding the unsupervised contrastive learning and the random masking mechanism
compensatory for the invariant constraints, which can be applied to various IL
methods. Furthermore, we conduct experiments across multiple modalities across
12 datasets and 6 classic models, verifying our over-invariance insight and the
effectiveness of our DivIL framework. Our code is available at
https://github.com/kokolerk/DivIL.

</details>


### [716] [Architecture independent generalization bounds for overparametrized deep ReLU networks](https://arxiv.org/pdf/2504.05695)
*Thomas Chen, Chun-Kai Kevin Chien, Patricia Muñoz Ewald, Andrew G. Moore*

Main category: cs.LG

TL;DR: Overparametrized neural networks generalize well, with test error independent of overparametrization and VC dimension, relying on data geometry and network properties.


<details>
  <summary>Details</summary>
Motivation: To understand how overparametrized neural networks generalize despite high complexity and lack of dependence on VC dimension.

Method: Explicit bounds based on metric geometry of data, activation function regularity, and weight norms. Construct zero-loss minimizers for ReLU networks without gradient descent.

Result: Generalization error is independent of network architecture, with bounds tied to data geometry and network properties.

Conclusion: Overparametrization in neural networks can lead to good generalization without reliance on traditional complexity measures like VC dimension.

Abstract: We prove that overparametrized neural networks are able to generalize with a
test error that is independent of the level of overparametrization, and
independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds
that only depend on the metric geometry of the test and training sets, on the
regularity properties of the activation function, and on the operator norms of
the weights and norms of biases. For overparametrized deep ReLU networks with a
training sample size bounded by the input space dimension, we explicitly
construct zero loss minimizers without use of gradient descent, and prove that
the generalization error is independent of the network architecture.

</details>


### [717] [Model-Based Exploration in Truthful Monitored Markov Decision Processes](https://arxiv.org/pdf/2502.16772)
*Alireza Kazemipour, Simone Parisi, Matthew E. Taylor, Michael Bowling*

Main category: cs.LG

TL;DR: The paper introduces a model-based algorithm for Monitored Markov Decision Processes (Mon-MDPs) to address limitations of existing methods, offering faster convergence, leveraging known monitors, and providing finite-sample guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing Mon-MDP algorithms fail to exploit problem structure, lack worst-case guarantees, and have only asymptotic convergence proofs. This work aims to overcome these limitations.

Method: A model-based algorithm using two instances of model-based interval estimation: one for capturing observable rewards and another for learning the minimax-optimal policy.

Result: Faster convergence in benchmarks, especially with known monitors, and finite-sample performance bounds.

Conclusion: The proposed algorithm improves upon prior work by addressing key limitations and providing theoretical and empirical advantages.

Abstract: A tenet of reinforcement learning is that the agent always observes rewards.
However, this is not true in many realistic settings, e.g., a human observer
may not always be available to provide rewards, sensors may be limited or
malfunctioning, or rewards may be inaccessible during deployment. Monitored
Markov decision processes (Mon-MDPs) have recently been proposed to model such
settings. However, existing Mon-MDP algorithms have several limitations: they
do not fully exploit the problem structure, cannot leverage a known monitor,
lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific
initialization, and offer only asymptotic convergence proofs. This paper makes
three contributions. First, we introduce a model-based algorithm for Mon-MDPs
that addresses these shortcomings. The algorithm employs two instances of
model-based interval estimation: one to ensure that observable rewards are
reliably captured, and another to learn the minimax-optimal policy. Second, we
empirically demonstrate the advantages. We show faster convergence than prior
algorithms in over four dozen benchmarks, and even more dramatic improvement
when the monitoring process is known. Third, we present the first finite-sample
bound on performance. We show convergence to a minimax-optimal policy even when
some rewards are never observable.

</details>


### [718] [Similarity-Distance-Magnitude Universal Verification](https://arxiv.org/pdf/2502.20167)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: The paper introduces the SDM activation function to improve neural network robustness by incorporating similarity and distance-awareness, alongside existing magnitude-awareness, to better estimate predictive uncertainty. It also addresses human-interpretable uncertainty mapping and demonstrates robustness to distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network robustness and provide interpretable uncertainty estimates by integrating similarity, distance, and magnitude-awareness into the softmax function.

Method: Develops the SDM activation function, which combines similarity, distance, and magnitude-awareness. Uses a learned transform over class-conditional empirical CDFs for uncertainty estimation.

Result: SDM calibration provides robust uncertainty estimates, handles distribution shifts, and supports selective classification and conditional branching.

Conclusion: The SDM function improves uncertainty estimation and interpretability, with applications in selective classification and LLMs, supported by open-source implementation.

Abstract: We address the neural network robustness problem by adding Similarity (i.e.,
correctly predicted depth-matches into training)-awareness and
Distance-to-training-distribution-awareness to the existing output Magnitude
(i.e., decision-boundary)-awareness of the softmax function. The resulting SDM
activation function provides strong signals of the relative epistemic
(reducible) predictive uncertainty. We use this novel behavior to further
address the complementary HCI problem of mapping the output to
human-interpretable summary statistics over relevant partitions of a held-out
calibration set. Estimates of prediction-conditional uncertainty are obtained
via a parsimonious learned transform over the class-conditional empirical CDFs
of the output of a final-layer SDM activation function. For decision-making and
as an intrinsic model check, estimates of class-conditional accuracy are
obtained by further partitioning the high-probability regions of this
calibrated output into class-conditional, region-specific CDFs. The uncertainty
estimates from SDM calibration are remarkably robust to test-time distribution
shifts and out-of-distribution inputs; incorporate awareness of the effective
sample size; provide estimates of uncertainty from the learning and data
splitting processes; and are well-suited for selective classification and
conditional branching for additional test-time compute based on the predictive
uncertainty, as for selective LLM generation, routing, and composition over
multiple models and retrieval. Finally, we construct SDM networks, LLMs with
uncertainty-aware verification and interpretability-by-exemplar as intrinsic
properties. We provide open-source software implementing these results.

</details>


### [719] [Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling](https://arxiv.org/pdf/2504.10612)
*Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Lea Bogensperger, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze*

Main category: cs.LG

TL;DR: Energy Matching integrates flow-based models with energy-based flexibility, improving generative performance and regularization for inverse problems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of generative models in handling partial observations and additional priors, which energy-based models (EBMs) handle well.

Method: Proposes Energy Matching, a framework combining flow-based approaches with EBM flexibility, using a scalar field for dynamic parameterization.

Result: Outperforms existing EBMs on CIFAR-10 and ImageNet in fidelity, supports diverse mode exploration, and simplifies EBM frameworks.

Conclusion: Energy Matching advances EBM capabilities, enabling wider adoption in generative modeling across domains.

Abstract: The most widely used generative models map noise and data distributions by
matching flows or scores. However, they struggle to incorporate partial
observations and additional priors--something energy-based models (EBMs) handle
elegantly by simply adding corresponding scalar energy terms. We address this
issue by proposing Energy Matching, a framework that endows flow-based
approaches with the flexibility of EBMs. Far from the data manifold, samples
move along curl-free, optimal transport paths from noise to data. As they
approach the data manifold, an entropic energy term guides the system into a
Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in
terms of fidelity, while retaining simulation-free training of transport-based
approaches away from the data manifold. Furthermore, we leverage the method's
flexibility to introduce an interaction energy that supports diverse mode
exploration, which we demonstrate in a controlled protein-generation setting.
Our approach focuses on learning a scalar potential energy--without
time-conditioning, auxiliary generators, or additional networks--which marks a
significant departure from recent EBM methods. We believe that this simplified
framework significantly advances EBMs capabilities and paves the way for their
wider adoption in generative modeling across diverse domains.

</details>


### [720] [Remasking Discrete Diffusion Models with Inference-Time Scaling](https://arxiv.org/pdf/2503.00307)
*Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: ReMDM introduces a remasking diffusion model sampler to enable iterative refinement in masked discrete diffusion, improving output quality and controllability.


<details>
  <summary>Details</summary>
Motivation: Modern masked discrete diffusion lacks iterative refinement, limiting its ability to correct errors during generation.

Method: ReMDM applies a custom remasking backward process to pretrained masked diffusion models, enabling inference-time compute scaling.

Result: ReMDM improves output quality in natural language, discretized images, and scientific domains like molecule design, while maintaining quality under limited compute.

Conclusion: ReMDM enhances masked diffusion models by enabling iterative refinement and improving controllability, with applications across diverse domains.

Abstract: Part of the success of diffusion models stems from their ability to perform
iterative refinement, i.e., repeatedly correcting outputs during generation.
However, modern masked discrete diffusion lacks this capability: when a token
is generated, it cannot be updated again, even when it introduces an error.
Here, we address this limitation by introducing the remasking diffusion model
(ReMDM) sampler, a method that can be applied to pretrained masked diffusion
models in a principled way and that is derived from a discrete diffusion model
with a custom remasking backward process. Most interestingly, ReMDM endows
discrete diffusion with a form of inference-time compute scaling. By increasing
the number of sampling steps, ReMDM generates natural language outputs that
approach the quality of autoregressive models, whereas when the computation
budget is limited, ReMDM better maintains quality. ReMDM also improves sample
quality of masked diffusion models for discretized images, and in scientific
domains such as molecule design, ReMDM facilitates diffusion guidance and
pushes the Pareto frontier of controllability relative to classical masking and
uniform noise diffusion. We provide the code along with a blog post on the
project page: https://remdm.github.io

</details>


### [721] [Split Gibbs Discrete Diffusion Posterior Sampling](https://arxiv.org/pdf/2503.01161)
*Wenda Chu, Zihui Wu, Yifan Chen, Yang Song, Yisong Yue*

Main category: cs.LG

TL;DR: SGDD, a plug-and-play discrete diffusion posterior sampling algorithm, improves performance by 30% over baselines in discrete-state spaces like DNA design and music infilling.


<details>
  <summary>Details</summary>
Motivation: Posterior sampling in discrete-state spaces using discrete diffusion models is challenging compared to continuous models.

Method: SGDD uses split Gibbs sampling for reward-guided generation and solving inverse problems.

Result: SGDD converges to the target posterior and outperforms baselines by 30% in benchmarks.

Conclusion: SGDD is a state-of-the-art method for discrete posterior sampling with broad applications.

Abstract: We study the problem of posterior sampling in discrete-state spaces using
discrete diffusion models. While posterior sampling methods for continuous
diffusion models have achieved remarkable progress, analogous methods for
discrete diffusion models remain challenging. In this work, we introduce a
principled plug-and-play discrete diffusion posterior sampling algorithm based
on split Gibbs sampling, which we call SGDD. Our algorithm enables
reward-guided generation and solving inverse problems in discrete-state spaces.
We demonstrate the convergence of SGDD to the target posterior distribution and
verify this through controlled experiments on synthetic benchmarks. Our method
enjoys state-of-the-art posterior sampling performance on a range of benchmarks
for discrete data, including DNA sequence design, discrete image inverse
problems, and music infilling, achieving more than 30% improved performance
compared to existing baselines.

</details>


### [722] [Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity](https://arxiv.org/pdf/2504.18078)
*Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang*

Main category: cs.LG

TL;DR: A privacy-preserving distributed PV disaggregation framework using Personalized Federated Learning (PFL) is proposed to address challenges in estimating behind-the-meter PV generation, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of distributed PV installations complicates energy management due to unobservable generation, necessitating accurate PV disaggregation while addressing privacy and data heterogeneity.

Method: A two-level PFL framework combines local transformer-based models for solar irradiance embeddings and adaptive local aggregation, with global aggregation for cross-center knowledge sharing.

Result: Experiments on real-world data show the framework outperforms benchmarks in accuracy and robustness.

Conclusion: The proposed PFL-based framework effectively addresses privacy and heterogeneity challenges in PV disaggregation, offering a scalable solution for grid management.

Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.

</details>


### [723] [ExMAG: Learning of Maximally Ancestral Graphs](https://arxiv.org/pdf/2503.08245)
*Petr Ryšavý, Pavel Rytíř, Xiaoyu He, Georgios Korpas, Jakub Mareček*

Main category: cs.LG

TL;DR: A score-based branch-and-cut algorithm for learning maximally ancestral graphs in mixed graphs, outperforming state-of-the-art methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Extending acyclicity to mixed graphs (with directed and undirected edges) is crucial for causal learning, especially with confounders.

Method: Proposes a score-based branch-and-cut algorithm for learning maximally ancestral graphs.

Result: The algorithm is more accurate and faster than existing methods on small and medium-sized synthetic instances.

Conclusion: The proposed algorithm effectively improves accuracy and efficiency in learning maximally ancestral graphs.

Abstract: In mixed graphs, there are both directed and undirected edges. An extension
of acyclicity to this mixed-graph setting is known as maximally ancestral
graphs. This extension is of considerable interest in causal learning in the
presence of confounders. There, directed edges represent a clear direction of
causality, while undirected edges represent confounding. We propose a
score-based branch-and-cut algorithm for learning maximally ancestral graphs.
The algorithm produces more accurate results than state-of-the-art methods,
while being faster to run on small and medium-sized synthetic instances.

</details>


### [724] [DRIP: DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device Training](https://arxiv.org/pdf/2504.08364)
*Marcus Rüb, Daniel Konegen, Patrick Selle, Axel Sikora, Daniel Mueller-Gritschneder*

Main category: cs.LG

TL;DR: A novel algorithm uses Grad-CAM to dynamically decide data point retention for model training, optimizing storage and performance on embedded devices.


<details>
  <summary>Details</summary>
Motivation: Reducing labeling effort, optimizing on-device training for embedded systems, and enhancing model performance by selecting critical data points.

Method: Introduces a DRIP Score computed via Grad-CAM to quantify data point importance, enabling dynamic retention decisions without full dataset access.

Result: Achieves accuracy matching or surpassing full-dataset training while saving up to 39% storage on four benchmark datasets.

Conclusion: First algorithm for online data retention decisions without full dataset access, balancing performance and storage efficiency.

Abstract: Selecting data points for model training is critical in machine learning.
Effective selection methods can reduce the labeling effort, optimize on-device
training for embedded systems with limited data storage, and enhance the model
performance. This paper introduces a novel algorithm that uses Grad-CAM to make
online decisions about retaining or discarding data points. Optimized for
embedded devices, the algorithm computes a unique DRIP Score to quantify the
importance of each data point. This enables dynamic decision-making on whether
a data point should be stored for potential retraining or discarded without
compromising model performance. Experimental evaluations on four benchmark
datasets demonstrate that our approach can match or even surpass the accuracy
of models trained on the entire dataset, all while achieving storage savings of
up to 39\%. To our knowledge, this is the first algorithm that makes online
decisions about data point retention without requiring access to the entire
dataset.

</details>


### [725] [A New Approach to Backtracking Counterfactual Explanations: A Unified Causal Framework for Efficient Model Interpretability](https://arxiv.org/pdf/2505.02435)
*Pouria Fatemi, Ehsan Sharifian, Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: BRACE is an efficient method for generating counterfactual explanations with causal reasoning, addressing limitations of traditional and newer approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional counterfactual methods lack causal relationships, while newer causal methods are computationally expensive. BRACE aims to bridge this gap.

Method: BRACE uses backtracking counterfactuals with causal reasoning to generate actionable explanations efficiently.

Result: Experiments show BRACE provides deeper insights into model outputs and generalizes previous techniques.

Conclusion: BRACE offers a practical and efficient solution for interpretable counterfactual explanations with causal reasoning.

Abstract: Counterfactual explanations enhance interpretability by identifying
alternative inputs that produce different outputs, offering localized insights
into model decisions. However, traditional methods often neglect causal
relationships, leading to unrealistic examples. While newer approaches
integrate causality, they are computationally expensive. To address these
challenges, we propose an efficient method called BRACE based on backtracking
counterfactuals that incorporates causal reasoning to generate actionable
explanations. We first examine the limitations of existing methods and then
introduce our novel approach and its features. We also explore the relationship
between our method and previous techniques, demonstrating that it generalizes
them in specific scenarios. Finally, experiments show that our method provides
deeper insights into model outputs.

</details>


### [726] [ORION Grounded in Context: Retrieval-Based Method for Hallucination Detection](https://arxiv.org/pdf/2504.15771)
*Assaf Gerner, Netta Madvil, Nadav Barak, Alex Zaikman, Jonatan Liberman, Liron Hamra, Rotem Brazilay, Shay Tsadok, Yaron Friedman, Neal Harow, Noam Bressler, Shir Chorev, Philip Tannor*

Main category: cs.LG

TL;DR: The paper introduces 'Grounded in Context,' a framework for detecting hallucinations in LLM outputs, achieving high accuracy in factual consistency tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucinated answers in LLM applications despite advancements in grounded content generation.

Method: Integrates retrieval and NLI models within a 512-token context window to predict factual consistency between premises and hypotheses.

Result: Achieves an F1 score of 0.83 in RAGTruth's response-level classification task, outperforming comparable frameworks.

Conclusion: The framework is effective for hallucination detection in diverse use cases like summarization and RAG.

Abstract: Despite advancements in grounded content generation, production Large
Language Models (LLMs) based applications still suffer from hallucinated
answers. We present "Grounded in Context" - a member of Deepchecks' ORION
(Output Reasoning-based InspectiON) family of lightweight evaluation models. It
is our framework for hallucination detection, designed for production-scale
long-context data and tailored to diverse use cases, including summarization,
data extraction, and RAG. Inspired by RAG architecture, our method integrates
retrieval and Natural Language Inference (NLI) models to predict factual
consistency between premises and hypotheses using an encoder-based model with
only a 512-token context window. Our framework identifies unsupported claims
with an F1 score of 0.83 in RAGTruth's response-level classification task,
matching methods that trained on the dataset, and outperforming all comparable
frameworks using similar-sized models.

</details>


### [727] [Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation](https://arxiv.org/pdf/2505.05181)
*Bojian Yin, Federico Corradi*

Main category: cs.LG

TL;DR: SVP replaces backpropagation with hierarchical variational inference, enabling local updates and reducing memory usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's scalability and memory limitations hinder deep learning efficiency.

Method: SVP treats layer activations as latent variables, optimizes local ELBOs, uses random projections to prevent collapse, and adds feature alignment for consistency.

Result: SVP matches BP's accuracy, reduces memory by 4x, and improves scalability across architectures and datasets.

Conclusion: SVP offers a probabilistic, scalable alternative to BP, enhancing modularity and interpretability in neural networks.

Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on
global gradient synchronization limits scalability and imposes significant
memory overhead. We propose Stochastic Variational Propagation (SVP), a
scalable alternative that reframes training as hierarchical variational
inference. SVP treats layer activations as latent variables and optimizes local
Evidence Lower Bounds (ELBOs), enabling independent, local updates while
preserving global coherence. However, directly applying KL divergence in
layer-wise ELBOs risks inter-layer's representation collapse due to excessive
compression. To prevent this, SVP projects activations into low-dimensional
spaces via fixed random matrices, ensuring information preservation and
representational diversity. Combined with a feature alignment loss for
inter-layer consistency, SVP achieves competitive accuracy with BP across
diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to
ImageNet), reduces memory usage by up to 4x, and significantly improves
scalability. More broadly, SVP introduces a probabilistic perspective to deep
representation learning, opening pathways toward more modular and interpretable
neural network design.

</details>


### [728] [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/pdf/2505.06108)
*Lennart Justen*

Main category: cs.LG

TL;DR: The study evaluates 27 Large Language Models (LLMs) on eight biology benchmarks, showing significant improvements in performance, with some models surpassing expert-level capabilities. Extended reasoning features improved results, while chain-of-thought did not. Benchmark limitations were identified.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the biological capabilities of frontier LLMs and track their advancements over time.

Method: Evaluated 27 LLMs from major AI developers on eight biology benchmarks through ten independent runs per benchmark.

Result: Top models showed dramatic improvements, with some exceeding expert-level performance. Extended reasoning features enhanced results, but chain-of-thought did not. Benchmarks revealed saturation and data errors.

Conclusion: The study underscores the rapid progress of LLMs in biology but calls for more sophisticated evaluation methods due to benchmark limitations.

Abstract: This study systematically evaluates 27 frontier Large Language Models on
eight biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with OpenAI's o3 now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including the biology subsets of GPQA and WMDP and
LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.

</details>


### [729] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/pdf/2505.09861)
*John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei*

Main category: cs.LG

TL;DR: A unified transformer-based attribution model for marketing, handling member-level and aggregate data, plus external factors, implemented at LinkedIn with significant impact.


<details>
  <summary>Details</summary>
Motivation: To improve marketing intelligence by accurately attributing conversions using data-driven methods, addressing diverse data types and external influences.

Method: A transformer-based approach integrating member-level, aggregate-level data, and external macro factors.

Result: Successful large-scale implementation at LinkedIn, demonstrating significant impact.

Conclusion: The approach offers valuable insights broadly applicable to marketing and ad tech, enhancing attribution accuracy and business outcomes.

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [730] [Fractal Graph Contrastive Learning](https://arxiv.org/pdf/2505.11356)
*Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang*

Main category: cs.LG

TL;DR: FractalGCL introduces a theory-driven GCL framework using fractal self-similarity to ensure global structural consistency, improving performance and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods lack explicit control over global structural consistency in data augmentations, limiting performance.

Method: FractalGCL uses renormalisation-based augmentation and a fractal-dimension-aware contrastive loss, with a one-shot estimator to reduce computational overhead.

Result: FractalGCL achieves state-of-the-art results, outperforming baselines by ~7% on traffic networks, and reduces training time by ~61%.

Conclusion: FractalGCL effectively addresses global structural consistency in GCL, enhancing performance and efficiency.

Abstract: While Graph Contrastive Learning (GCL) has attracted considerable attention
in the field of graph self-supervised learning, its performance heavily relies
on data augmentations that are expected to generate semantically consistent
positive pairs. Existing strategies typically resort to random perturbations or
local structure preservation, yet lack explicit control over global structural
consistency between augmented views. To address this limitation, we propose
Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that
leverages fractal self-similarity to enforce global topological coherence.
FractalGCL introduces two key innovations: a renormalisation-based augmentation
that generates structurally aligned positive views via box coverings; and a
fractal-dimension-aware contrastive loss that aligns graph embeddings according
to their fractal dimensions. While combining the two innovations markedly
boosts graph-representation quality, it also adds non-trivial computational
overhead. To mitigate the computational overhead of fractal dimension
estimation, we derive a one-shot estimator by proving that the dimension
discrepancy between original and renormalised graphs converges weakly to a
centred Gaussian distribution. This theoretical insight enables a reduction in
dimension computation cost by an order of magnitude, cutting overall training
time by approximately 61%. The experiments show that FractalGCL not only
delivers state-of-the-art results on standard benchmarks but also outperforms
traditional baselines on traffic networks by an average margin of about
remarkably 7%. Codes are available at
(https://anonymous.4open.science/r/FractalGCL-0511).

</details>


### [731] [UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models](https://arxiv.org/pdf/2505.11654)
*Yuhang Liu, Yingxue Zhang, Xin Zhang, Ling Tian, Yanhua Li, Jun Luo*

Main category: cs.LG

TL;DR: UrbanMind is a spatial-temporal LLM framework for urban dynamics prediction, combining multifaceted data fusion, semantic-aware prompting, and test-time adaptation to achieve high accuracy and robust generalization.


<details>
  <summary>Details</summary>
Motivation: Existing neural network and LLM-based methods struggle with generalization and integrating spatial-temporal data for urban dynamics prediction.

Method: UrbanMind uses Muffin-MAE for data fusion, semantic-aware prompting for contextual encoding, and test-time adaptation for dynamic adjustment.

Result: UrbanMind outperforms baselines in accuracy and generalization, even in zero-shot settings.

Conclusion: UrbanMind effectively bridges the gap in urban dynamics prediction by leveraging LLMs with specialized techniques for robust performance.

Abstract: Understanding and predicting urban dynamics is crucial for managing
transportation systems, optimizing urban planning, and enhancing public
services. While neural network-based approaches have achieved success, they
often rely on task-specific architectures and large volumes of data, limiting
their ability to generalize across diverse urban scenarios. Meanwhile, Large
Language Models (LLMs) offer strong reasoning and generalization capabilities,
yet their application to spatial-temporal urban dynamics remains underexplored.
Existing LLM-based methods struggle to effectively integrate multifaceted
spatial-temporal data and fail to address distributional shifts between
training and testing data, limiting their predictive reliability in real-world
applications. To bridge this gap, we propose UrbanMind, a novel
spatial-temporal LLM framework for multifaceted urban dynamics prediction that
ensures both accurate forecasting and robust generalization. At its core,
UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with
specialized masking strategies that capture intricate spatial-temporal
dependencies and intercorrelations among multifaceted urban dynamics.
Additionally, we design a semantic-aware prompting and fine-tuning strategy
that encodes spatial-temporal contextual details into prompts, enhancing LLMs'
ability to reason over spatial-temporal patterns. To further improve
generalization, we introduce a test time adaptation mechanism with a test data
reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by
reconstructing LLM-generated embeddings. Extensive experiments on real-world
urban datasets across multiple cities demonstrate that UrbanMind consistently
outperforms state-of-the-art baselines, achieving high accuracy and robust
generalization, even in zero-shot settings.

</details>


### [732] [A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection](https://arxiv.org/pdf/2505.12586)
*Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, Mohsen Imani*

Main category: cs.LG

TL;DR: A lightweight, plug-in detection framework for adversarial examples in DNNs, leveraging internal layer inconsistencies without external models or adversarial data.


<details>
  <summary>Details</summary>
Motivation: DNNs are vulnerable to adversarial examples, and existing detection methods are inefficient or lack generalizability.

Method: Uses the A Few Large Shifts Assumption and proposes Recovery Testing (RT) and Logit-layer Testing (LT) to detect adversarial perturbations.

Result: Achieves state-of-the-art detection performance on CIFAR-10, CIFAR-100, and ImageNet with minimal computational overhead.

Conclusion: The framework is efficient, generalizable, and maintains clean accuracy, offering a practical defense against adversarial attacks.

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial
examples--subtle, imperceptible perturbations that can lead to incorrect
predictions. While detection-based defenses offer a practical alternative to
adversarial training, many existing methods depend on external models, complex
architectures, heavy augmentations, or adversarial data, limiting their
efficiency and generalizability. We introduce a lightweight, plug-in detection
framework that leverages internal layer-wise inconsistencies within the target
model itself, requiring only benign data for calibration. Our approach is
grounded in the A Few Large Shifts Assumption, which posits that adversarial
perturbations typically induce large representation shifts in a small subset of
layers. Building on this, we propose two complementary strategies--Recovery
Testing (RT) and Logit-layer Testing (LT)--to expose internal disruptions
caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under
both standard and adaptive threat models, our method achieves state-of-the-art
detection performance with negligible computational overhead and no compromise
to clean accuracy. The code is available here:
https://github.com/c0510gy/AFLS-AED.

</details>


### [733] [Treatment Effect Estimation for Optimal Decision-Making](https://arxiv.org/pdf/2505.13092)
*Dennis Frauen, Valentyn Melnychuk, Jonas Schweisthal, Mihaela van der Schaar, Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper reveals that two-stage CATE estimators, though optimal for estimating CATE, may underperform in decision-making due to their focus on irrelevant regions. It proposes a retargeted learning objective and a neural method to improve decision performance, validated empirically and theoretically.


<details>
  <summary>Details</summary>
Motivation: Understanding the gap between CATE estimation and decision-making performance, especially with widely used two-stage estimators like DR-learner.

Method: Proposes a novel two-stage learning objective to balance CATE estimation and decision performance, along with a neural method for optimization.

Result: Shows that traditional CATE estimators can be suboptimal for decisions, while the proposed method improves performance.

Conclusion: First work to adapt two-stage CATE estimators for optimal decision-making, bridging theory and practice.

Abstract: Decision-making across various fields, such as medicine, heavily relies on
conditional average treatment effects (CATEs). Practitioners commonly make
decisions by checking whether the estimated CATE is positive, even though the
decision-making performance of modern CATE estimators is poorly understood from
a theoretical perspective. In this paper, we study optimal decision-making
based on two-stage CATE estimators (e.g., DR-learner), which are considered
state-of-the-art and widely used in practice. We prove that, while such
estimators may be optimal for estimating CATE, they can be suboptimal when used
for decision-making. Intuitively, this occurs because such estimators
prioritize CATE accuracy in regions far away from the decision boundary, which
is ultimately irrelevant to decision-making. As a remedy, we propose a novel
two-stage learning objective that retargets the CATE to balance CATE estimation
error and decision performance. We then propose a neural method that optimizes
an adaptively-smoothed approximation of our learning objective. Finally, we
confirm the effectiveness of our method both empirically and theoretically. In
sum, our work is the first to show how two-stage CATE estimators can be adapted
for optimal decision-making.

</details>


### [734] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/pdf/2505.14625)
*Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran*

Main category: cs.LG

TL;DR: The paper identifies false negatives in RL-based LLM training, where verifiers wrongly reject correct outputs, and proposes TinyV, a lightweight verifier, to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To address the problem of false negatives in RL training of LLMs, which impair learning by depriving models of correct feedback.

Method: Analyzes the Big-Math-RL-Verified dataset, introduces TinyV, a lightweight LLM-based verifier, to dynamically identify and recover false negatives.

Result: TinyV improves pass rates by up to 10% and accelerates convergence in math-reasoning benchmarks.

Conclusion: False negatives significantly hinder RL training; TinyV offers a practical solution to enhance LLM fine-tuning.

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [735] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/pdf/2505.14033)
*Guoming Li, Jian Yang, Yifan Chen*

Main category: cs.LG

TL;DR: The paper introduces Coarsening-guided Partition-wise Filtering (CPF), a framework unifying graph-wise and node-wise filtering for GNNs, addressing limitations in handling heterophilic graphs and avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: The rigid uniform filtering in conventional GNNs struggles with heterophilic graphs, while node-wise filtering risks overfitting. A unified framework is needed for better adaptability and theoretical insights.

Method: CPF performs filtering on node partitions: first structure-aware via graph coarsening, then feature-aware via k-means clustering.

Result: CPF outperforms other paradigms in node classification and anomaly detection, demonstrating its efficacy.

Conclusion: CPF provides a balanced solution for graphs with homophily and heterophily, avoiding excessive parameterization and overfitting.

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [736] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/pdf/2505.14214)
*Mattes Mollenhauer, Nicole Mücke, Dimitri Meunier, Arthur Gretton*

Main category: cs.LG

TL;DR: The paper analyzes ridge regression in reproducing kernel Hilbert spaces under heavy-tailed noise, showing optimal convergence rates and robustness.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of ridge regression performance beyond subexponential noise, addressing heavy-tailed noise with finite higher moments.

Method: Uses the integral operator framework and a Fuk-Nagaev inequality for Hilbert-space valued random variables to derive excess risk bounds.

Result: Achieves optimal convergence rates under standard eigenvalue decay, demonstrating robustness against heavy-tailed noise.

Conclusion: Regularized least squares is asymptotically robust to heavy-tailed noise, with rates matching those under subexponential noise.

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [737] [Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC](https://arxiv.org/pdf/2505.15030)
*Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan*

Main category: cs.LG

TL;DR: The paper introduces a methodology for evaluating on-device LLMs, revealing insights on performance, quantization, and power consumption for efficient edge deployment.


<details>
  <summary>Details</summary>
Motivation: To address performance limitations of on-device LLMs due to reduced capacity and compression, ensuring privacy benefits.

Method: Systematic evaluation of models (0.5B-14B parameters) and seven PTQ methods on commodity laptops, analyzing metrics like BPW and power consumption.

Result: Key findings include near-linear scaling with BPW, a practical threshold at ~3.5 BPW, marginal accuracy loss with low BPW, and power consumption trends.

Conclusion: Provides guidelines for deploying and optimizing LLMs on edge devices, balancing performance and resource constraints.

Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training quantization (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
low-bit quantization consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.

</details>


### [738] [A Temporal Difference Method for Stochastic Continuous Dynamics](https://arxiv.org/pdf/2505.15544)
*Haruki Settai, Naoya Takeishi, Takehisa Yairi*

Main category: cs.LG

TL;DR: A model-free reinforcement learning approach targeting the HJB equation is proposed, overcoming the need for known dynamics in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing HJB-based RL methods require known dynamics, limiting their applicability. The paper aims to address this by developing a model-free approach.

Method: The paper introduces a model-free temporal difference method that targets the HJB equation without needing explicit dynamics.

Result: The proposed method shows qualitative and empirical advantages over transition kernel-based formulations.

Conclusion: This work bridges stochastic optimal control and model-free RL, offering a promising direction for future research.

Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.

</details>


### [739] [Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs](https://arxiv.org/pdf/2505.15747)
*Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase*

Main category: cs.LG

TL;DR: A novel framework integrates fragmented multi-modal Alzheimer's disease data using LLMs and knowledge graphs, revealing new correlations without requiring patient ID matching.


<details>
  <summary>Details</summary>
Motivation: Traditional multimodal analysis in AD research requires matched patient IDs, limiting data integration. This framework aims to overcome this by enabling population-level integration of diverse data types.

Method: The approach combines statistical analysis of MRI, gene expression, biomarkers, EEG, and clinical data into a knowledge graph, analyzed by LLMs to extract correlations and generate hypotheses.

Result: Identified novel relationships, such as metabolic risk factors linked to tau protein via neuroinflammation, and validated findings with cross-dataset consistency (variance <15%) and expert review (Cohen's k=0.82).

Conclusion: The framework enables conceptual-level cross-modal integration, offering new insights into AD pathology and generating testable hypotheses for future research.

Abstract: We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [740] [Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring Fraud Detection in Travel Planning](https://arxiv.org/pdf/2505.16557)
*Junchi Yao, Jianhua Xu, Tianyu Xin, Ziyi Wang, Shenzhe Zhu, Shu Yang, Di Wang*

Main category: cs.MA

TL;DR: WandaPlan is introduced to evaluate fraud risks in LLM-based multi-agent planning systems, revealing weaknesses in existing frameworks and proposing an anti-fraud agent for mitigation.


<details>
  <summary>Details</summary>
Motivation: The reliance on platforms prone to fraudulent information (e.g., fake reviews) in multi-agent planning systems poses risks like financial losses and poor user experiences.

Method: WandaPlan, an evaluation environment with deceptive content, assesses system performance across three fraud cases: Misinformation Fraud, Team-Coordinated Multi-Person Fraud, and Level-Escalating Multi-Round Fraud.

Result: Existing frameworks prioritize task efficiency over data authenticity, showing significant weaknesses. WandaPlan proves generalizable for real-world risk assessment.

Conclusion: An anti-fraud agent is proposed to mitigate fraud risks, offering a solution for reliable planning in real-world applications.

Abstract: The rise of Large Language Model-based Multi-Agent Planning has leveraged
advanced frameworks to enable autonomous and collaborative task execution. Some
systems rely on platforms like review sites and social media, which are prone
to fraudulent information, such as fake reviews or misleading descriptions.
This reliance poses risks, potentially causing financial losses and harming
user experiences. To evaluate the risk of planning systems in real-world
applications, we introduce \textbf{WandaPlan}, an evaluation environment
mirroring real-world data and injected with deceptive content. We assess system
performance across three fraud cases: Misinformation Fraud, Team-Coordinated
Multi-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal
significant weaknesses in existing frameworks that prioritize task efficiency
over data authenticity. At the same time, we validate WandaPlan's
generalizability, capable of assessing the risks of real-world open-source
planning frameworks. To mitigate the risk of fraud, we propose integrating an
anti-fraud agent, providing a solution for reliable planning.

</details>


### [741] [Homotopy-Aware Multi-Agent Path Planning on Plane](https://arxiv.org/pdf/2310.01945)
*Kazumi Kasaura*

Main category: cs.MA

TL;DR: Proposes a homotopy-aware multi-agent path planning framework using Dynnikov coordinates, proving its efficiency and completeness under assumptions.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for efficient and homotopically distinct path planning for swarms in obstacle-filled planar domains.

Method: Combines Dynnikov coordinates with revised prioritized planning to generate distinct solutions.

Result: Faster than non-Dynnikov methods and avoids local optima in trajectory planning.

Conclusion: The framework is effective for homotopy-aware path planning in continuous environments.

Abstract: We propose an efficient framework using Dynnikov coordinates for
homotopy-aware multi-agent path planning in planar domains that may contain
obstacles. We developed a method for generating multiple homotopically distinct
solutions for the multi-agent path planning problem in planar domains by
combining our framework with revised prioritized planning and proved its
completeness under specific assumptions. Experimentally, we demonstrated that
our method is significantly faster than a method without Dynnikov coordinates.
We also confirmed experimentally that homotopy-aware planning contributes to
avoiding locally optimal solutions when searching for low-cost trajectories for
a swarm of agents in a continuous environment.

</details>


### [742] [Multi-Agent Corridor Generating Algorithm](https://arxiv.org/pdf/2410.12397)
*Arseniy Pertzovsky, Roni Stern, Roie Zivan, Ariel Felner*

Main category: cs.MA

TL;DR: MACGA and MACGA+PIBT algorithms solve dense MAPF problems by creating corridors and integrating PIBT, outperforming baselines in success rate, runtime, and makespan.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with dense MAPF instances, necessitating efficient collision-free pathfinding for multi-agent systems.

Method: Agents build corridors (connected vertices) toward goals and evacuate others to avoid collisions. MACGA+PIBT integrates PIBT for improved performance.

Result: Polynomial-time algorithms with reachability guarantees; experimental results show superior success rate, runtime, and makespan.

Conclusion: MACGA and MACGA+PIBT effectively address dense MAPF challenges, offering scalable and reliable solutions.

Abstract: In this paper, we propose the Multi-Agent Corridor Generating Algorithm
(MACGA) for solving the Multi-agent Pathfinding (MAPF) problem, where a group
of agents need to find non-colliding paths to their target locations. Existing
approaches struggle to solve dense MAPF instances. In MACGA, the agents build
\emph{corridors}, which are sequences of connected vertices, from current
locations towards agents' goals, and evacuate other agents out of the corridors
to avoid collisions and deadlocks. We also present the MACGA+PIBT algorithm,
which integrates the well-known rule-based PIBT algorithm into MACGA to improve
runtime and solution quality. The proposed algorithms run in polynomial time
and have a reachability property, i.e., every agent is guaranteed to reach its
goal location at some point. We demonstrate experimentally that MACGA and
MACGA+PIBT outperform baseline algorithms in terms of success rate, runtime,
and makespan across diverse MAPF benchmark grids.

</details>


### [743] [Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor](https://arxiv.org/pdf/2503.02189)
*Dickness Kakitahi Kwesiga, Angshuman Guin, Michael Hunter*

Main category: cs.MA

TL;DR: The paper introduces a multi-agent proximal policy optimization (MA-PPO) algorithm for adaptive traffic signal control, outperforming traditional methods in simulated real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Address gaps in multi-agent RL for traffic control, particularly the lack of testing in real-world conditions and the potential superiority of policy-based methods in partially observable environments.

Method: Formulated a MA-PPO algorithm with a centralized-critic architecture, tested on a simulated seven-intersection corridor and compared with actuated-coordinated signal control (ASC).

Result: MA-PPO outperformed ASC, showing 2% and 24% improvements in travel time for primary and secondary directions, respectively, and significant crossing time reductions.

Conclusion: MA-PPO is stable, robust, and adaptable to traffic demand changes, demonstrating its effectiveness for real-world adaptive traffic control.

Abstract: Previous studies that have formulated multi-agent reinforcement learning (RL)
algorithms for adaptive traffic signal control have primarily used value-based
RL methods. However, recent literature has shown that policy-based methods may
perform better in partially observable environments. Additionally, RL methods
remain largely untested for real-world normally signal timing plans because of
the simplifying assumptions common in the literature. The current study
attempts to address these gaps and formulates a multi-agent proximal policy
optimization (MA-PPO) algorithm to implement adaptive and coordinated traffic
control along an arterial corridor. The formulated MA-PPO has a
centralized-critic architecture under a centralized training and decentralized
execution framework. Agents are designed to allow selection and implementation
of up to eight signal phases, as commonly implemented in field controllers. The
formulated algorithm is tested on a simulated real-world seven intersection
corridor. The speed of convergence for each agent was found to depend on the
size of the action space, which depends on the number and sequence of signal
phases. The performance of the formulated MA-PPO adaptive control algorithm is
compared with the field implemented actuated-coordinated signal control (ASC),
modeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The
trained MA-PPO performed significantly better than the ASC for all movements.
Compared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the
primary and secondary coordination directions, respectively. For cross streets
movements MA-PPO also showed significant crossing time reductions. Volume
sensitivity experiments revealed that the formulated MA-PPO demonstrated good
stability, robustness, and adaptability to changes in traffic demand.

</details>


### [744] [QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?](https://arxiv.org/pdf/2504.12961)
*Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu*

Main category: cs.MA

TL;DR: QLLM, a novel MARL algorithm using LLMs for credit assignment, outperforms existing methods by addressing limitations like imprecise attribution and poor scalability.


<details>
  <summary>Details</summary>
Motivation: Credit assignment in MARL is challenging due to imprecise attribution, limited interpretability, and scalability issues in existing methods.

Method: QLLM leverages LLMs to automatically construct credit assignment functions via TFCAF and a coder-evaluator framework for code generation and refinement.

Result: QLLM outperforms state-of-the-art baselines in MARL benchmarks, showing strong generalization and compatibility with mixing networks.

Conclusion: QLLM is a promising, versatile solution for complex multi-agent scenarios, addressing key MARL challenges.

Abstract: Credit assignment has remained a fundamental challenge in multi-agent
reinforcement learning (MARL). Previous studies have primarily addressed this
issue through value decomposition methods under the centralized training with
decentralized execution paradigm, where neural networks are utilized to
approximate the nonlinear relationship between individual Q-values and the
global Q-value. Although these approaches have achieved considerable success in
various benchmark tasks, they still suffer from several limitations, including
imprecise attribution of contributions, limited interpretability, and poor
scalability in high-dimensional state spaces. To address these challenges, we
propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic
construction of credit assignment functions using large language models (LLMs).
Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit
allocation process is represented as a direct and expressive nonlinear
functional formulation. A custom-designed \textit{coder-evaluator} framework is
further employed to guide the generation, verification, and refinement of
executable code by LLMs, significantly mitigating issues such as hallucination
and shallow reasoning during inference. Extensive experiments conducted on
several standard MARL benchmarks demonstrate that the proposed method
consistently outperforms existing state-of-the-art baselines. Moreover, QLLM
exhibits strong generalization capability and maintains compatibility with a
wide range of MARL algorithms that utilize mixing networks, positioning it as a
promising and versatile solution for complex multi-agent scenarios.

</details>


### [745] [OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration](https://arxiv.org/pdf/2505.11765)
*Shijun Li, Hilaf Hasson, Joydeep Ghosh*

Main category: cs.MA

TL;DR: OMAC is a framework for optimizing Multi-Agent Systems (MAS) powered by LLMs, addressing five key dimensions of agent functionality and collaboration. It outperforms state-of-the-art methods in tasks like code generation and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based MAS rely on handcrafted methods, lacking systematic design and optimization. OMAC aims to fill this gap.

Method: OMAC introduces a general algorithm with two actors (Semantic Initializer and Contrastive Comparator) for single-dimension optimization and extends it for joint optimization across multiple dimensions.

Result: OMAC shows superior performance in code generation, arithmetic reasoning, and general reasoning tasks.

Conclusion: OMAC provides a systematic and effective approach for optimizing LLM-based MAS, enhancing their capabilities in complex tasks.

Abstract: Agents powered by advanced large language models (LLMs) have demonstrated
impressive capabilities across diverse complex applications. Recently,
Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate
with each other, have exhibited enhanced capabilities in complex tasks, such as
high-quality code generation and arithmetic reasoning. However, the development
of such systems often relies on handcrafted methods, and the literature on
systematic design and optimization of LLM-based MAS remains limited.
  In this work, we introduce OMAC, a general framework designed for holistic
optimization of LLM-based MAS. Specifically, we identify five key optimization
dimensions for MAS, encompassing both agent functionality and collaboration
structure. Building upon these dimensions, we first propose a general
algorithm, utilizing two actors termed the Semantic Initializer and the
Contrastive Comparator, to optimize any single dimension. Then, we present an
algorithm for joint optimization across multiple dimensions. Extensive
experiments demonstrate the superior performance of OMAC on code generation,
arithmetic reasoning, and general reasoning tasks against state-of-the-art
approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [746] [MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing](https://arxiv.org/pdf/2505.16279)
*Junjie Zheng, Zihao Chen, Chaofan Ding, Yunming Liang, Yihan Fan, Huan Yang, Lei Xie, Xinhan Di*

Main category: cs.MM

TL;DR: A multi-modal generative framework improves movie dubbing by analyzing visual inputs and generating high-quality speech, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Current dubbing technology lacks adaptation to styles, handling of dialogue types, and consideration of speaker details like age and gender.

Method: Uses a multi-modal VLM for visual analysis and large speech models for dubbing, supported by an annotated dataset.

Result: Achieves improvements in LSE-D (1.09%), SPK-SIM (8.80%), EMO-SIM (19.08%), and MCD (18.74%) over SOTA.

Conclusion: The framework enhances dubbing quality by addressing key challenges and leveraging multi-modal inputs.

Abstract: Current movie dubbing technology can produce the desired speech using a
reference voice and input video, maintaining perfect synchronization with the
visuals while effectively conveying the intended emotions. However, crucial
aspects of movie dubbing, including adaptation to various dubbing styles,
effective handling of dialogue, narration, and monologues, as well as
consideration of subtle details such as speaker age and gender, remain
insufficiently explored. To tackle these challenges, we introduce a multi-modal
generative framework. First, it utilizes a multi-modal large vision-language
model (VLM) to analyze visual inputs, enabling the recognition of dubbing types
and fine-grained attributes. Second, it produces high-quality dubbing using
large speech generation models, guided by multi-modal inputs. Additionally, a
movie dubbing dataset with annotations for dubbing types and subtle details is
constructed to enhance movie understanding and improve dubbing quality for the
proposed multi-modal framework. Experimental results across multiple benchmark
datasets show superior performance compared to state-of-the-art (SOTA) methods.
In details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to
1.09%, 8.80%, 19.08%, and 18.74%, respectively.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [747] [ASVspoof2019 vs. ASVspoof5: Assessment and Comparison](https://arxiv.org/pdf/2505.15911)
*Avishai Weizman, Yehuda Ben-Shimol, Itshak Lapidot*

Main category: eess.AS

TL;DR: ASVspoof5 introduces mismatches in both genuine and spoofed speech, making the challenge harder than ASVspoof2019, where mismatches were only in spoofing attacks.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of mismatched conditions in both genuine and spoofed speech on spoofing-robust automatic speaker verification systems.

Method: Qualitative and quantitative comparisons of ASVspoof5 and ASVspoof2019 databases.

Result: ASVspoof5 is more challenging due to shifts in genuine speech toward spoofed speech and harder attacks.

Conclusion: The new conditions in ASVspoof5 increase difficulty, highlighting the need for more robust countermeasures.

Abstract: ASVspoof challenges are designed to advance the understanding of spoofing
speech attacks and encourage the development of robust countermeasure systems.
These challenges provide a standardized database for assessing and comparing
spoofing-robust automatic speaker verification solutions. The ASVspoof5
challenge introduces a shift in database conditions compared to ASVspoof2019.
While ASVspoof2019 has mismatched conditions only in spoofing attacks in the
evaluation set, ASVspoof5 incorporates mismatches in both bona fide and spoofed
speech statistics. This paper examines the impact of these mismatches,
presenting qualitative and quantitative comparisons within and between the two
databases. We show the increased difficulty for genuine and spoofed speech and
demonstrate that in ASVspoof5, not only are the attacks more challenging, but
the genuine speech also shifts toward spoofed speech compared to ASVspoof2019.

</details>


### [748] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2505.15957)
*Chih-Kai Yang, Neo S. Ho, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper proposes a systematic taxonomy for evaluating large audio-language models (LALMs) across four dimensions, addressing the lack of structured benchmarks in the field.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented and unstructured nature of existing benchmarks for LALMs, the authors aim to provide a comprehensive taxonomy for standardized evaluation.

Method: The authors conduct a survey and categorize LALM evaluations into four dimensions: General Auditory Awareness, Knowledge and Reasoning, Dialogue-oriented Ability, and Fairness, Safety, and Trustworthiness.

Result: A structured taxonomy is proposed, along with detailed insights into each category and challenges in the field. The paper also highlights future research directions.

Conclusion: This is the first survey focused on LALM evaluations, offering clear guidelines and a maintained collection of surveyed papers to support future advancements.

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [749] [Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives](https://arxiv.org/pdf/2505.15965)
*Gowtham Premananth, Vinith Kugathasan, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The paper explores accented English speech, revealing simpler coordination patterns and higher pitch than native speech, and proposes a method to quantify accent strength without phonetic transcriptions.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in speech-processing systems due to non-native accented speech, aiming to improve inclusivity and robustness.

Method: The research uses articulatory and acoustic analysis, eigenspectra, and Vocal Tract Variable-based coordination features to quantify accent strength.

Result: Findings show distinct patterns in accented speech and offer a resource-efficient method for accent strength measurement.

Conclusion: The study opens new research avenues for speech intelligibility and aids in developing inclusive speech-processing systems.

Abstract: Advancements in AI-driven speech-based applications have transformed diverse
industries ranging from healthcare to customer service. However, the increasing
prevalence of non-native accented speech in global interactions poses
significant challenges for speech-processing systems, which are often trained
on datasets dominated by native speech. This study investigates accented
English speech through articulatory and acoustic analysis, identifying simpler
coordination patterns and higher average pitch than native speech. Using
eigenspectra and Vocal Tract Variable-based coordination features, we establish
an efficient method for quantifying accent strength without relying on
resource-intensive phonetic transcriptions. Our findings provide a new avenue
for research on the impacts of accents on speech intelligibility and offer
insights for developing inclusive, robust speech processing systems that
accommodate diverse linguistic communities.

</details>


### [750] [Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation](https://arxiv.org/pdf/2505.16044)
*Gowtham Premananth, Philip Resnik, Sonia Bansal, Deanna L. Kelly, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The paper proposes a multimodal deep learning approach to estimate individual symptom severity in schizophrenia, moving beyond binary classification for better clinical utility.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods oversimplify schizophrenia by treating it as a binary classification task, limiting clinical applicability.

Method: Unimodal models for speech, video, and text inputs are developed, followed by a multimodal framework to enhance accuracy and robustness.

Result: The approach captures detailed symptom profiles, improving diagnostic precision and supporting personalized treatment.

Conclusion: This scalable and objective tool enhances mental health assessment by addressing the complexity of schizophrenia.

Abstract: Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.

</details>


### [751] [AudioMorphix: Training-free audio editing with diffusion probabilistic models](https://arxiv.org/pdf/2505.16076)
*Jinhua Liang, Yuanzhe Chen, Yi Yuan, Dongya Jia, Xiaobin Zhuang, Zhuo Chen, Yuping Wang, Yuxuan Wang*

Main category: eess.AS

TL;DR: AudioMorphix is a training-free audio editor for precise, localized modifications in spectrograms, achieving high fidelity and precision in tasks like addition, removal, and pitch shifting.


<details>
  <summary>Details</summary>
Motivation: Existing audio editing methods struggle with precision and fidelity. AudioMorphix addresses this by enabling localized modifications while preserving the original recording's integrity.

Method: AudioMorphix operates on spectrograms, using morphing theory to blend sounds seamlessly. It optimizes noised latent with energy functions and enhances self-attention layers for detail preservation.

Result: The method performs well on tasks like addition, removal, time shifting, and pitch shifting, demonstrating high fidelity and precision.

Conclusion: AudioMorphix advances audio editing with its training-free, precise approach, supported by a new evaluation benchmark.

Abstract: Editing sound with precision is a crucial yet underexplored challenge in
audio content creation. While existing works can manipulate sounds by text
instructions or audio exemplar pairs, they often struggled to modify audio
content precisely while preserving fidelity to the original recording. In this
work, we introduce a novel editing approach that enables localized
modifications to specific time-frequency regions while keeping the remaining of
the audio intact by operating on spectrograms directly. To achieve this, we
propose AudioMorphix, a training-free audio editor that manipulates a target
region on the spectrogram by referring to another recording. Inspired by
morphing theory, we conceptualize audio mixing as a process where different
sounds blend seamlessly through morphing and can be decomposed back into
individual components via demorphing. Our AudioMorphix optimizes the noised
latent conditioned on raw input and reference audio while rectifying the guided
diffusion process through a series of energy functions. Additionally, we
enhance self-attention layers with a cache mechanism to preserve detailed
characteristics from the original recordings. To advance audio editing
research, we devise a new evaluation benchmark, which includes a curated
dataset with a variety of editing instructions. Extensive experiments
demonstrate that AudioMorphix yields promising performance on various audio
editing tasks, including addition, removal, time shifting and stretching, and
pitch shifting, achieving high fidelity and precision. Demo and code are
available at this url.

</details>


### [752] [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/pdf/2505.16220)
*Liang-Yeh Shen, Shi-Xin Fang, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee*

Main category: eess.AS

TL;DR: Meta-PerSER is a meta-learning framework for personalized Speech Emotion Recognition (SER) that adapts to individual listeners' interpretations, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Conventional SER systems ignore individual nuances by relying on aggregated annotations, leading to inconsistent predictions.

Method: Uses MAML enhanced with Combined-Set Meta-Training, Derivative Annealing, and per-layer per-step learning rates, integrating pre-trained self-supervised models for robust representations.

Result: Outperforms baselines on IEMOCAP corpus in seen and unseen data scenarios.

Conclusion: Meta-PerSER shows promise for personalized emotion recognition by adapting quickly with few labeled examples.

Abstract: This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.

</details>


### [753] [Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection](https://arxiv.org/pdf/2505.16351)
*Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: Dysfluent-WFST is a zero-shot decoder for simultaneous phoneme transcription and dysfluency detection, outperforming traditional methods without extra training.


<details>
  <summary>Details</summary>
Motivation: Traditional dysfluency detection methods lack clinical insight and misclassify context-dependent cases, necessitating a better approach.

Method: Uses Dysfluent-WFST, a zero-shot decoder with upstream encoders like WavLM, requiring no additional training.

Result: Achieves state-of-the-art performance in phonetic error rate and dysfluency detection on simulated and real speech data.

Conclusion: Explicit modeling of pronunciation behavior in decoding improves dysfluency processing more effectively than complex architectures.

Abstract: Automatic detection of speech dysfluency aids speech-language pathologists in
efficient transcription of disordered speech, enhancing diagnostics and
treatment planning. Traditional methods, often limited to classification,
provide insufficient clinical insight, and text-independent models misclassify
dysfluency, especially in context-dependent cases. This work introduces
Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes
and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with
upstream encoders like WavLM and requires no additional training. It achieves
state-of-the-art performance in both phonetic error rate and dysfluency
detection on simulated and real speech data. Our approach is lightweight,
interpretable, and effective, demonstrating that explicit modeling of
pronunciation behavior in decoding, rather than complex architectures, is key
to improving dysfluency processing systems.

</details>


### [754] [Multi-Channel Sequence-to-Sequence Neural Diarization: Experimental Results for The MISP 2025 Challenge](https://arxiv.org/pdf/2505.16387)
*Ming Cheng, Fei Su, Cancan Li, Juan Liu, Ming Li*

Main category: eess.AS

TL;DR: The paper presents a speaker diarization system for the MISP 2025 Challenge, using S2SND and its multi-channel extension (MC-S2SND), achieving top performance with an 8.09% DER.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced speaker diarization system for the MISP 2025 Challenge, improving accuracy through multi-channel audio processing.

Method: Extends the S2SND framework to MC-S2SND, refining initial single-channel predictions with multi-channel audio.

Result: Achieves a DER of 8.09%, ranking first in the challenge.

Conclusion: The MC-S2SND system effectively improves speaker diarization accuracy, demonstrating its superiority in the MISP 2025 Challenge.

Abstract: This paper describes the speaker diarization system developed for the
Multimodal Information-Based Speech Processing (MISP) 2025 Challenge. First, we
utilize the Sequence-to-Sequence Neural Diarization (S2SND) framework to
generate initial predictions using single-channel audio. Then, we extend the
original S2SND framework to create a new version, Multi-Channel
Sequence-to-Sequence Neural Diarization (MC-S2SND), which refines the initial
results using multi-channel audio. The final system achieves a diarization
error rate (DER) of 8.09% on the evaluation set of the competition database,
ranking first place in the speaker diarization task of the MISP 2025 Challenge.

</details>


### [755] [UBGAN: Enhancing Coded Speech with Blind and Guided Bandwidth Extension](https://arxiv.org/pdf/2505.16404)
*Kishan Gupta, Srikanth Korse, Andreas Brendel, Nicola Pia, Guillaume Fuchs*

Main category: eess.AS

TL;DR: UBGAN is a modular GAN-based solution for bandwidth extension in speech codecs, enhancing WB to SWB signals with variants for guided and blind operation.


<details>
  <summary>Details</summary>
Motivation: To improve the perceptual quality of coded speech by extending bandwidth without requiring end-to-end retraining of neural codecs.

Method: Proposes UBGAN, a lightweight GAN operating in the subband domain, with guided and blind variants for flexibility.

Result: UBGAN successfully extends WB signals to SWB, demonstrating generalization across codecs and bitrates.

Conclusion: UBGAN enhances operational flexibility and perceptual quality for both conventional and neural speech codecs.

Abstract: In practical application of speech codecs, a multitude of factors such as the
quality of the radio connection, limiting hardware or required user experience
necessitate trade-offs between achievable perceptual quality, engendered
bitrate and computational complexity. Most conventional and neural speech
codecs operate on wideband (WB) speech signals to achieve this compromise. To
further enhance the perceptual quality of coded speech, bandwidth extension
(BWE) of the transmitted speech is an attractive and popular technique in
conventional speech coding. In contrast, neural speech codecs are typically
trained end-to-end to a specific set of requirements and are often not easily
adaptable. In particular, they are typically trained to operate at a single
fixed sampling rate. With the Universal Bandwidth Extension Generative
Adversarial Network (UBGAN), we propose a modular and lightweight GAN-based
solution that increases the operational flexibility of a wide range of
conventional and neural codecs. Our model operates in the subband domain and
extends the bandwidth of WB signals from 8 kHz to 16 kHz, resulting in
super-wideband (SWB) signals. We further introduce two variants, guided-UBGAN
and blind-UBGAN, where the guided version transmits quantized learned
representation as a side information at a very low bitrate additional to the
bitrate of the codec, while blind-BWE operates without such side-information.
Our subjective assessments demonstrate the advantage of UBGAN applied to WB
codecs and highlight the generalization capacity of our proposed method across
multiple codecs and bitrates.

</details>


### [756] [Attractor-Based Speech Separation of Multiple Utterances by Unknown Number of Speakers](https://arxiv.org/pdf/2505.16607)
*Yuzhu Wang, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen*

Main category: eess.AS

TL;DR: A speech separation model for unknown speaker counts with dynamic estimation and activity detection, outperforming existing methods in noisy and reverberant conditions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of single-channel speech separation with unknown and variable speaker counts, including multi-utterance scenarios.

Method: Proposes an attractor-based architecture combining local and global temporal modeling, evaluated on a synthesized multi-speaker multi-utterance dataset with noise.

Result: Accurately estimates speaker counts, detects activities, and separates utterances correctly in known and unknown scenarios.

Conclusion: The attractor-based system is effective for dynamic speech separation in complex acoustic environments.

Abstract: This paper addresses the problem of single-channel speech separation, where
the number of speakers is unknown, and each speaker may speak multiple
utterances. We propose a speech separation model that simultaneously performs
separation, dynamically estimates the number of speakers, and detects
individual speaker activities by integrating an attractor module. The proposed
system outperforms existing methods by introducing an attractor-based
architecture that effectively combines local and global temporal modeling for
multi-utterance scenarios. To evaluate the method in reverberant and noisy
conditions, a multi-speaker multi-utterance dataset was synthesized by
combining Librispeech speech signals with WHAM! noise signals. The results
demonstrate that the proposed system accurately estimates the number of
sources. The system effectively detects source activities and separates the
corresponding utterances into correct outputs in both known and unknown source
count scenarios.

</details>


### [757] [HPP-Voice: A Large-Scale Evaluation of Speech Embeddings for Multi-Phenotypic Classification](https://arxiv.org/pdf/2505.16490)
*David Krongauz, Hido Pinto, Sarah Kohn, Yanir Marmor, Eran Segal*

Main category: eess.AS

TL;DR: The paper introduces HPP-Voice, a dataset of Hebrew speech recordings linked to health phenotypes, and compares 14 speech embedding models for health condition classification, showing gender-specific effectiveness.


<details>
  <summary>Details</summary>
Motivation: To explore non-invasive detection of medical phenotypes through paralinguistic speech cues and identify effective speech embedding models for health screening.

Method: Analyzed 7,188 Hebrew speech recordings (30-second counting tasks) linked to 15 health phenotypes, comparing 14 speech embedding models (e.g., speaker identification, MFCCs) for classification performance.

Result: Speaker identification models outperformed MFCCs and demographics for conditions like sleep apnea (AUC 0.64) and showed gender-specific effectiveness (e.g., better for males in respiratory/sleep conditions, females in smoking/anxiety).

Conclusion: A simple counting task can enable large-scale health screening, with certain embedding models (e.g., speaker identification) being more effective for specific conditions, guiding future vocal biomarker research.

Abstract: Human speech contains paralinguistic cues that reflect a speaker's
physiological and neurological state, potentially enabling non-invasive
detection of various medical phenotypes. We introduce the Human Phenotype
Project Voice corpus (HPP-Voice): a dataset of 7,188 recordings in which
Hebrew-speaking adults count for 30 seconds, with each speaker linked to up to
15 potentially voice-related phenotypes spanning respiratory, sleep, mental
health, metabolic, immune, and neurological conditions. We present a systematic
comparison of 14 modern speech embedding models, where modern speech embeddings
from these 30-second counting tasks outperform MFCCs and demographics for
downstream health condition classifications. We found that embedding learned
from a speaker identification model can predict objectively measured moderate
to severe sleep apnea in males with an AUC of 0.64 $\pm$ 0.03, while MFCC and
demographic features led to AUCs of 0.56 $\pm$ 0.02 and 0.57 $\pm$ 0.02,
respectively. Additionally, our results reveal gender-specific patterns in
model effectiveness across different medical domains. For males, speaker
identification and diarization models consistently outperformed speech
foundation models for respiratory conditions (e.g., asthma: 0.61 $\pm$ 0.03 vs.
0.56 $\pm$ 0.02) and sleep-related conditions (insomnia: 0.65 $\pm$ 0.04 vs.
0.59 $\pm$ 0.05). For females, speaker diarization models performed best for
smoking status (0.61 $\pm$ 0.02 vs 0.55 $\pm$ 0.02), while Hebrew-specific
models performed best (0.59 $\pm$ 0.02 vs. 0.58 $\pm$ 0.02) in classifying
anxiety compared to speech foundation models. Our findings provide evidence
that a simple counting task can support large-scale, multi-phenotypic voice
screening and highlight which embedding families generalize best to specific
conditions, insights that can guide future vocal biomarker research and
clinical deployment.

</details>


### [758] [Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate](https://arxiv.org/pdf/2505.16845)
*Hanglei Zhang, Yiwei Guo, Zhihan Li, Xiang Hao, Xie Chen, Kai Yu*

Main category: eess.AS

TL;DR: Proposes Temporally Flexible Coding (TFC) for neural speech codecs, introducing variable frame rate (VFR) to optimize bitrate and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current neural speech codecs use Constant Frame Rate (CFR), which is inefficient for speech segments with varying information density.

Method: Introduces TFC, enabling variable frame rate (VFR) based on temporal entropy for dynamic bitrate adjustment.

Result: TFC achieves optimal reconstruction quality with high flexibility and maintains performance at lower frame rates.

Conclusion: TFC is promising for low-frame-rate neural speech codecs, enhancing efficiency for downstream tasks.

Abstract: Most neural speech codecs achieve bitrate adjustment through intra-frame
mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,
speech segments inherently have time-varying information density (e.g., silent
intervals versus voiced regions). This property makes CFR not optimal in terms
of bitrate and token sequence length, hindering efficiency in real-time
applications. In this work, we propose a Temporally Flexible Coding (TFC)
technique, introducing variable frame rate (VFR) into neural speech codecs for
the first time. TFC enables seamlessly tunable average frame rates and
dynamically allocates frame rates based on temporal entropy. Experimental
results show that a codec with TFC achieves optimal reconstruction quality with
high flexibility, and maintains competitive performance even at lower frame
rates. Our approach is promising for the integration with other efforts to
develop low-frame-rate neural speech codecs for more efficient downstream
tasks.

</details>


### [759] [Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean](https://arxiv.org/pdf/2505.16616)
*Javier Perez, Dimme de Groot, Jorge Martinez*

Main category: eess.AS

TL;DR: The paper investigates biases in objective speech quality measures (PESQ and ViSQOL) for Turkish and Korean, revealing performance disparities compared to English and emphasizing the need for diverse language datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on how objective speech quality measures perform on non-English languages, focusing on Turkish and Korean.

Method: Comparison of PESQ and ViSQOL scores for Turkish and Korean against English as a baseline, analyzing correlations and disparities.

Result: Turkish samples scored higher on ViSQOL, and Turkish male speakers showed the highest correlation between PESQ and ViSQOL.

Conclusion: The study highlights biases in speech quality metrics and calls for developing datasets with diverse languages to ensure fair assessment.

Abstract: Objective speech quality measures are widely used to assess the performance
of video conferencing platforms and telecommunication systems. They predict
human-rated speech quality and are crucial for assessing the systems quality of
experience. Despite the widespread use, the quality measures are developed on a
limited set of languages. This can be problematic since the performance on
unseen languages is consequently not guaranteed or even studied. Here we raise
awareness to this issue by investigating the performance of two objective
speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English
as baseline, we show that Turkish samples have significantly higher ViSQOL
scores and that for Turkish male speakers the correlation between PESQ and
ViSQOL is highest. These results highlight the need to explore biases across
metrics and to develop a labeled speech quality dataset with a variety of
languages.

</details>


### [760] [Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting](https://arxiv.org/pdf/2505.16735)
*Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho*

Main category: eess.AS

TL;DR: The paper proposes Modality Adversarial Learning (MAL) to bridge the gap between audio and text embeddings for keyword spotting, enhancing performance through phoneme-level alignment and deep metric learning.


<details>
  <summary>Details</summary>
Motivation: The challenge of comparing heterogeneous audio and text embeddings in open-vocabulary keyword spotting motivates the need for modality-invariant representations.

Method: The approach combines deep metric learning (DML) for phoneme-level alignment and Modality Adversarial Learning (MAL) to reduce modality differences.

Result: Experiments on WSJ and LibriPhrase datasets show the method's effectiveness in improving embedding comparison.

Conclusion: MAL and DML together enhance multi-modal embedding alignment, addressing modality heterogeneity in keyword spotting.

Abstract: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic
and text embeddings are typically compared at either the phoneme or utterance
level. To facilitate this, we optimize acoustic and text encoders using deep
metric learning (DML), enabling direct comparison of multi-modal embeddings in
a shared embedding space. However, the inherent heterogeneity between audio and
text modalities presents a significant challenge. To address this, we propose
Modality Adversarial Learning (MAL), which reduces the domain gap in
heterogeneous modality representations. Specifically, we train a modality
classifier adversarially to encourage both encoders to generate
modality-invariant embeddings. Additionally, we apply DML to achieve
phoneme-level alignment between audio and text, and conduct comprehensive
comparisons across various DML objectives. Experiments on the Wall Street
Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the
proposed approach.

</details>


### [761] [SEED: Speaker Embedding Enhancement Diffusion Model](https://arxiv.org/pdf/2505.16798)
*KiHyun Nam, Jungwoo Heo, Jee-weon Jung, Gangin Park, Chaeyoung Jung, Ha-Jin Yu, Joon Son Chung*

Main category: eess.AS

TL;DR: A diffusion-based method refines speaker embeddings to address performance degradation in real-world speaker recognition systems, improving accuracy by up to 19.6% without requiring speaker labels or pipeline modifications.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in speaker recognition due to environmental mismatch is a key challenge.

Method: Proposes a diffusion model that adds noise to embeddings (clean/noisy) and reconstructs them to clean versions, applied during inference.

Result: Improves recognition accuracy by up to 19.6% in mismatch scenarios without affecting conventional performance.

Conclusion: The method effectively mitigates environmental mismatch issues without additional labels or pipeline changes.

Abstract: A primary challenge when deploying speaker recognition systems in real-world
applications is performance degradation caused by environmental mismatch. We
propose a diffusion-based method that takes speaker embeddings extracted from a
pre-trained speaker recognition model and generates refined embeddings. For
training, our approach progressively adds Gaussian noise to both clean and
noisy speaker embeddings extracted from clean and noisy speech, respectively,
via forward process of a diffusion model, and then reconstructs them to clean
embeddings in the reverse process. While inferencing, all embeddings are
regenerated via diffusion process. Our method needs neither speaker label nor
any modification to the existing speaker recognition pipeline. Experiments on
evaluation sets simulating environment mismatch scenarios show that our method
can improve recognition accuracy by up to 19.6% over baseline models while
retaining performance on conventional scenarios. We publish our code here
https://github.com/kaistmm/seed-pytorch

</details>


### [762] [Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation](https://arxiv.org/pdf/2505.16911)
*Ofir Yaish, Yehuda Mishaly, Eliya Nachmani*

Main category: eess.AS

TL;DR: Active Speech Enhancement (ASE) improves speech intelligibility by shaping the signal, outperforming traditional noise cancellation methods.


<details>
  <summary>Details</summary>
Motivation: To enhance speech quality beyond noise suppression by actively modifying the signal for better intelligibility and perceptual quality.

Method: Proposes a Transformer-Mamba-based architecture with a task-specific loss function for joint optimization of noise suppression and signal enrichment.

Result: Outperforms existing baselines in tasks like denoising, dereverberation, and declipping.

Conclusion: ASE is effective for targeted speech enhancement in challenging acoustic environments.

Abstract: We introduce a new paradigm for active sound modification: Active Speech
Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on
suppressing external interference, ASE goes further by actively shaping the
speech signal -- both attenuating unwanted noise components and amplifying
speech-relevant frequencies -- to improve intelligibility and perceptual
quality. To enable this, we propose a novel Transformer-Mamba-based
architecture, along with a task-specific loss function designed to jointly
optimize interference suppression and signal enrichment. Our method outperforms
existing baselines across multiple speech processing tasks -- including
denoising, dereverberation, and declipping -- demonstrating the effectiveness
of active, targeted modulation in challenging acoustic environments.

</details>


### [763] [ShiftySpeech: A Large-Scale Synthetic Speech Dataset with Distribution Shifts](https://arxiv.org/pdf/2502.05674)
*Ashi Garg, Zexin Cai, Lin Zhang, Henry Li Xinyuan, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews*

Main category: eess.AS

TL;DR: The paper introduces ShiftySpeech, a benchmark for evaluating synthetic speech detection under distribution shifts, revealing performance degradation in state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To assess if low error rates on academic benchmarks hold under realistic, shifting conditions like speaker changes, new synthesis methods, and acoustic variations.

Method: Developed ShiftySpeech, a large-scale benchmark with 3,000+ hours of synthetic speech across diverse domains, TTS systems, vocoders, and languages.

Result: State-of-the-art detection methods degrade significantly under controlled distribution shifts.

Conclusion: Synthetic speech detection methods should be cautiously used in production, considering potential distribution shifts.

Abstract: The problem of synthetic speech detection has enjoyed considerable attention,
with recent methods achieving low error rates across several established
benchmarks. However, to what extent can low error rates on academic benchmarks
translate to more realistic conditions? In practice, while the training set is
fixed at one point in time, test-time conditions may exhibit distribution
shifts relative to the training conditions, such as changes in speaker
characteristics, emotional expressiveness, language and acoustic conditions,
and the emergence of novel synthesis methods. Although some existing datasets
target subsets of these distribution shifts, systematic analysis remains
difficult due to inconsistencies between source data and synthesis systems
across datasets. This difficulty is further exacerbated by the rapid
development of new text-to-speech (TTS) and vocoder systems, which continually
expand the diversity of synthetic speech. To enable systematic benchmarking of
model performance under distribution shifts, we introduce ShiftySpeech, a
large-scale benchmark comprising over 3,000 hours of synthetic speech across 7
source domains, 6 TTS systems, 12 vocoders, and 3 languages. ShiftySpeech is
specifically designed to evaluate model generalization under controlled
distribution shifts while ensuring broad coverage of modern synthetic speech
generation techniques. It fills a key gap in current benchmarks by supporting
fine-grained, controlled analysis of generalization robustness. All tested
distribution shifts significantly degrade detection performance of
state-of-the-art detection approaches based on self-supervised features.
Overall, our findings suggest that reliance on synthetic speech detection
methods in production environments should be carefully evaluated based on
anticipated distribution shifts.

</details>


### [764] [Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising](https://arxiv.org/pdf/2505.13830)
*Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling*

Main category: eess.AS

TL;DR: A neural codec-based speech denoiser is integrated with LauraTTS to improve zero-shot TTS by addressing noise in audio prompts.


<details>
  <summary>Details</summary>
Motivation: LLM-based zero-shot TTS methods degrade in quality when audio prompts contain noise.

Method: Proposes a codec denoiser with an audio codec, token denoiser, and embedding refiner to clean noisy acoustic tokens.

Result: Outperforms state-of-the-art SE methods and enhances LauraTTS performance.

Conclusion: The integrated system achieves noise-robust zero-shot TTS, surpassing methods using additional SE models.

Abstract: Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend
to preserve the acoustic environment of the audio prompt, leading to
degradation in synthesized speech quality when the audio prompt contains noise.
In this paper, we propose a novel neural codec-based speech denoiser and
integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve
noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio
codec, a token denoiser, and an embedding refiner. The token denoiser predicts
the first two groups of clean acoustic tokens from the noisy ones, which can
serve as the acoustic prompt for LauraTTS to synthesize high-quality
personalized speech or be converted to clean speech waveforms through the
embedding refiner and codec decoder. Experimental results show that our
proposed codec denoiser outperforms state-of-the-art speech enhancement (SE)
methods, and the proposed noise-robust LauraTTS surpasses the approach using
additional SE models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [765] [MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models](https://arxiv.org/pdf/2505.15822)
*Jhon Lopez, Carlos Hinojosa, Henry Arguello, Bernard Ghanem*

Main category: eess.IV

TL;DR: MambaStyle introduces an efficient single-stage encoder-based GAN inversion method using vision state-space models (VSSMs) to balance reconstruction, editability, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing GAN inversion methods struggle to balance high reconstruction quality, effective editability, and computational efficiency.

Method: MambaStyle integrates VSSMs into its architecture for high-quality image inversion and flexible editing with fewer parameters and lower computational complexity.

Result: MambaStyle achieves superior inversion accuracy, editing quality, and computational efficiency, with faster inference and reduced model complexity.

Conclusion: MambaStyle is suitable for real-time applications due to its efficiency and effectiveness in GAN inversion and editing.

Abstract: The task of inverting real images into StyleGAN's latent space to manipulate
their attributes has been extensively studied. However, existing GAN inversion
methods struggle to balance high reconstruction quality, effective editability,
and computational efficiency. In this paper, we introduce MambaStyle, an
efficient single-stage encoder-based approach for GAN inversion and editing
that leverages vision state-space models (VSSMs) to address these challenges.
Specifically, our approach integrates VSSMs within the proposed architecture,
enabling high-quality image inversion and flexible editing with significantly
fewer parameters and reduced computational complexity compared to
state-of-the-art methods. Extensive experiments show that MambaStyle achieves a
superior balance among inversion accuracy, editing quality, and computational
efficiency. Notably, our method achieves superior inversion and editing results
with reduced model complexity and faster inference, making it suitable for
real-time applications.

</details>


### [766] [RadarRGBD A Multi-Sensor Fusion Dataset for Perception with RGB-D and mmWave Radar](https://arxiv.org/pdf/2505.15860)
*Tieshuai Song, Jiandong Ye, Ao Guo, Guidong He, Bin Yang*

Main category: eess.IV

TL;DR: A new multi-sensor dataset, RadarRGBD, is introduced, combining RGB-D and high-resolution millimeter-wave radar data to address gaps in existing datasets. A method to improve depth map quality is also proposed.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack high-quality millimeter-wave radar data, limiting research on multi-sensor fusion, especially in challenging conditions like adverse weather and low-light environments.

Method: The RadarRGBD dataset includes RGB-D data, radar point clouds, and raw radar matrices. A depth estimation framework is fine-tuned using absolute depth information and pseudo-relative depth scale.

Result: The proposed method effectively fills missing regions in sensor data, improving depth map quality.

Conclusion: RadarRGBD provides a valuable resource for multi-sensor fusion research, and the depth optimization method enhances data quality. The dataset will be publicly available.

Abstract: Multi-sensor fusion has significant potential in perception tasks for both
indoor and outdoor environments. Especially under challenging conditions such
as adverse weather and low-light environments, the combined use of
millimeter-wave radar and RGB-D sensors has shown distinct advantages. However,
existing multi-sensor datasets in the fields of autonomous driving and robotics
often lack high-quality millimeter-wave radar data. To address this gap, we
present a new multi-sensor dataset:RadarRGBD. This dataset includes RGB-D data,
millimeter-wave radar point clouds, and raw radar matrices, covering various
indoor and outdoor scenes, as well as low-light environments. Compared to
existing datasets, RadarRGBD employs higher-resolution millimeter-wave radar
and provides raw data, offering a new research foundation for the fusion of
millimeter-wave radar and visual sensors. Furthermore, to tackle the noise and
gaps in depth maps captured by Kinect V2 due to occlusions and mismatches, we
fine-tune an open-source relative depth estimation framework, incorporating the
absolute depth information from the dataset for depth supervision. We also
introduce pseudo-relative depth scale information to further optimize the
global depth scale estimation. Experimental results demonstrate that the
proposed method effectively fills in missing regions in sensor data. Our
dataset and related documentation will be publicly available at:
https://github.com/song4399/RadarRGBD.

</details>


### [767] [P3Net: Progressive and Periodic Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/pdf/2505.15861)
*Zhenyan Yao, Miao Zhang, Lanhu Wu, Yongri Piao, Feng Tian, Weibing Sun, Huchuan Lu*

Main category: eess.IV

TL;DR: The paper introduces a progressive and periodic perturbation mechanism (P3M) and a boundary-focused loss to improve semi-supervised medical image segmentation by dynamically adjusting perturbations and enhancing boundary predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using perturbation mechanisms effectively in semi-supervised learning and ensuring accurate predictions in boundary regions.

Method: Proposes P3M for dynamic perturbation adjustment and a boundary-focused loss to enhance sensitivity to details.

Result: Achieves state-of-the-art performance on 2D and 3D datasets and shows scalability by extending to other methods.

Conclusion: P3M and the boundary-focused loss are effective, scalable solutions for improving semi-supervised medical image segmentation.

Abstract: Perturbation with diverse unlabeled data has proven beneficial for
semi-supervised medical image segmentation (SSMIS). While many works have
successfully used various perturbation techniques, a deeper understanding of
learning perturbations is needed. Excessive or inappropriate perturbation can
have negative effects, so we aim to address two challenges: how to use
perturbation mechanisms to guide the learning of unlabeled data through labeled
data, and how to ensure accurate predictions in boundary regions. Inspired by
human progressive and periodic learning, we propose a progressive and periodic
perturbation mechanism (P3M) and a boundary-focused loss. P3M enables dynamic
adjustment of perturbations, allowing the model to gradually learn them. Our
boundary-focused loss encourages the model to concentrate on boundary regions,
enhancing sensitivity to intricate details and ensuring accurate predictions.
Experimental results demonstrate that our method achieves state-of-the-art
performance on two 2D and 3D datasets. Moreover, P3M is extendable to other
methods, and the proposed loss serves as a universal tool for improving
existing methods, highlighting the scalability and applicability of our
approach.

</details>


### [768] [Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI](https://arxiv.org/pdf/2505.15984)
*Yamin Arefeen, Brett Levac, Bhairav Patel, Chang Ho, Jonathan I. Tamir*

Main category: eess.IV

TL;DR: The paper proposes using diffusion probabilistic generative models to accelerate MRI scans in NICUs, addressing challenges like low SNR and limited data.


<details>
  <summary>Details</summary>
Motivation: To reduce scan times for neonatal MRI in NICUs, which are currently long due to low SNR and limited receive coils.

Method: Developed a training pipeline with modified network architectures, class embedding vectors, self-supervised denoising, and posterior sample averaging. Evaluated via under-sampling experiments and a clinical reader study.

Result: Combined methods improved reconstruction quality, and the model worked at two acceleration rates without retraining. Reader study confirmed clinical adequacy.

Conclusion: The pipeline successfully reduces scan time for in-NICU neonatal MRI using diffusion models.

Abstract: Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of
brain abnormalities during early life development. Permanent magnet scanners
operating in the neonatal intensive care unit (NICU) facilitate MRI of sick
infants, but have long scan times due to lower signal-to-noise ratios (SNR) and
limited receive coils. This work accelerates in-NICU MRI with diffusion
probabilistic generative models by developing a training pipeline accounting
for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal
MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical
Center. We propose a pipeline to handle the low quantity and SNR of our
real-world dataset (1) modifying existing network architectures to support
varying resolutions; (2) training a single model on all data with learned class
embedding vectors; (3) applying self-supervised denoising before training; and
(4) reconstructing by averaging posterior samples. Retrospective under-sampling
experiments, accounting for signal decay, evaluated each item of our proposed
methodology. A clinical reader study with practicing pediatric
neuroradiologists evaluated our proposed images reconstructed from 1.5x
under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior
samples yields quantitative improvements in reconstruction. The generative
model decouples the learned prior from the measurement model and functions at
two acceleration rates without re-training. The reader study suggests that
proposed images reconstructed from approximately 1.5x under-sampled data are
adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the
proposed pipeline to handle challenging real-world datasets could reduce scan
time of in-NICU neonatal MRI.

</details>


### [769] [Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets](https://arxiv.org/pdf/2505.16027)
*Qinmei Xu, Yiheng Li, Xianghao Zhan, Ahmet Gorkem Er, Brittany Dashevsky, Chuanjun Xu, Mohammed Alawad, Mengya Yang, Liu Ya, Changsheng Zhou, Xiao Li, Haruka Itakura, Olivier Gevaert*

Main category: eess.IV

TL;DR: Foundation models outperform CNNs in CXR interpretation, with MAVL leading in performance, though all models struggle with pediatric cases.


<details>
  <summary>Details</summary>
Motivation: Evaluate the real-world performance and generalizability of vision-language foundation models versus traditional CNNs in CXR interpretation across diverse populations and tasks.

Method: Benchmarked eight models (five foundation, three CNN) on 37 tasks using six public and three private datasets. Performance metrics included AUROC and AUPRC.

Result: Foundation models, especially MAVL, outperformed CNNs. Performance dropped significantly for pediatric cases (AUROC: 0.57 vs. 0.88 in adults).

Conclusion: Structured supervision and prompt design enhance radiologic AI. Future work should focus on geographic expansion and ensemble modeling for clinical use.

Abstract: Foundation models leveraging vision-language pretraining have shown promise
in chest X-ray (CXR) interpretation, yet their real-world performance across
diverse populations and diagnostic tasks remains insufficiently evaluated. This
study benchmarks the diagnostic performance and generalizability of foundation
models versus traditional convolutional neural networks (CNNs) on multinational
CXR datasets. We evaluated eight CXR diagnostic models - five vision-language
foundation models and three CNN-based architectures - across 37 standardized
classification tasks using six public datasets from the USA, Spain, India, and
Vietnam, and three private datasets from hospitals in China. Performance was
assessed using AUROC, AUPRC, and other metrics across both shared and
dataset-specific tasks. Foundation models outperformed CNNs in both accuracy
and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and
structured supervision, achieved the highest performance on public (mean AUROC:
0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,
ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed
reduced performance on pediatric cases, with average AUROC dropping from 0.88
+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings
highlight the value of structured supervision and prompt design in radiologic
AI and suggest future directions including geographic expansion and ensemble
modeling for clinical deployment. Code for all evaluated models is available at
https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE

</details>


### [770] [Comprehensive Lung Disease Detection Using Deep Learning Models and Hybrid Chest X-ray Data with Explainable AI](https://arxiv.org/pdf/2505.16028)
*Shuvashis Sarker, Shamim Rahim Refat, Faika Fairuj Preotee, Tanvir Rouf Shawon, Raihan Tanvir*

Main category: eess.IV

TL;DR: Deep learning models on a hybrid dataset achieve 99% accuracy in detecting lung conditions from X-rays, with explainable AI enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Improving diagnostic accuracy for lung diseases like COVID-19 and pneumonia using diverse datasets and advanced models.

Method: Applied CNN, VGG16, VGG19, InceptionV3, Xception, ResNet50V2, InceptionResNetV2, MobileNetV2, and DenseNet121 to hybrid and individual datasets, using LIME for explainability.

Result: Hybrid dataset improved model accuracy, with VGG16, Xception, ResNet50V2, and DenseNet121 achieving 99% accuracy.

Conclusion: Hybrid datasets and explainable AI enhance model robustness and reliability for medical imaging.

Abstract: Advanced diagnostic instruments are crucial for the accurate detection and
treatment of lung diseases, which affect millions of individuals globally. This
study examines the effectiveness of deep learning and transfer learning models
using a hybrid dataset, created by merging four individual datasets from
Bangladesh and global sources. The hybrid dataset significantly enhances model
accuracy and generalizability, particularly in detecting COVID-19, pneumonia,
lung opacity, and normal lung conditions from chest X-ray images. A range of
models, including CNN, VGG16, VGG19, InceptionV3, Xception, ResNet50V2,
InceptionResNetV2, MobileNetV2, and DenseNet121, were applied to both
individual and hybrid datasets. The results showed superior performance on the
hybrid dataset, with VGG16, Xception, ResNet50V2, and DenseNet121 each
achieving an accuracy of 99%. This consistent performance across the hybrid
dataset highlights the robustness of these models in handling diverse data
while maintaining high accuracy. To understand the models implicit behavior,
explainable AI techniques were employed to illuminate their black-box nature.
Specifically, LIME was used to enhance the interpretability of model
predictions, especially in cases of misclassification, contributing to the
development of reliable and interpretable AI-driven solutions for medical
imaging.

</details>


### [771] [OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates](https://arxiv.org/pdf/2505.16091)
*Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang*

Main category: eess.IV

TL;DR: OSCAR is a one-step diffusion codec for image compression, addressing computational overhead and bit-rate flexibility by modeling compressed latents as noisy variants and using a single generative model.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods for image compression are computationally intensive and require separate models for different bit-rates, leading to high costs.

Method: OSCAR models compressed latents as noisy variants of original latents, mapping bit-rates to pseudo diffusion timesteps, enabling one-step denoising with a single model.

Result: OSCAR achieves superior performance in quantitative and visual quality metrics while significantly improving inference efficiency.

Conclusion: OSCAR offers an efficient and flexible solution for diffusion-based image compression, reducing computational and storage costs.

Abstract: Pretrained latent diffusion models have shown strong potential for lossy
image compression, owing to their powerful generative priors. Most existing
diffusion-based methods reconstruct images by iteratively denoising from random
noise, guided by compressed latent representations. While these approaches have
achieved high reconstruction quality, their multi-step sampling process incurs
substantial computational overhead. Moreover, they typically require training
separate models for different compression bit-rates, leading to significant
training and storage costs. To address these challenges, we propose a one-step
diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our
method views compressed latents as noisy variants of the original latents,
where the level of distortion depends on the bit-rate. This perspective allows
them to be modeled as intermediate states along a diffusion trajectory. By
establishing a mapping from the compression bit-rate to a pseudo diffusion
timestep, we condition a single generative model to support reconstructions at
multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich
structural information, thereby making one-step denoising feasible. Thus, OSCAR
replaces iterative sampling with a single denoising pass, significantly
improving inference efficiency. Extensive experiments demonstrate that OSCAR
achieves superior performance in both quantitative and visual quality metrics.
The code and models will be released at https://github.com/jp-guo/OSCAR.

</details>


### [772] [Compressing Human Body Video with Interactive Semantics: A Generative Approach](https://arxiv.org/pdf/2505.16152)
*Bolin Chen, Shanzhi Yin, Hanwei Zhu, Lingyu Zhu, Zihan Zhang, Jie Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye*

Main category: eess.IV

TL;DR: Proposes a method to compress human body videos with interactive semantics, enabling controllable editing and efficient transmission, outperforming VVC and generative schemes.


<details>
  <summary>Details</summary>
Motivation: To enhance video coding by making it interactive and controllable through semantic-level representations, particularly for human body videos, aiming to improve digital human communication in the metaverse.

Method: Uses a 3D human model to disentangle complex motion into configurable embeddings, which are edited, compressed, and transmitted. The decoder reconstructs high-quality video from these semantics.

Result: Achieves better compression performance at ultra-low bitrates compared to VVC and generative schemes, while enabling interactive coding without extra processes.

Conclusion: The framework advances human body video compression and interaction, with potential applications in metaverse communication.

Abstract: In this paper, we propose to compress human body video with interactive
semantics, which can facilitate video coding to be interactive and controllable
by manipulating semantic-level representations embedded in the coded bitstream.
In particular, the proposed encoder employs a 3D human model to disentangle
nonlinear dynamics and complex motion of human body signal into a series of
configurable embeddings, which are controllably edited, compactly compressed,
and efficiently transmitted. Moreover, the proposed decoder can evolve the
mesh-based motion fields from these decoded semantics to realize the
high-quality human body video reconstruction. Experimental results illustrate
that the proposed framework can achieve promising compression performance for
human body videos at ultra-low bitrate ranges compared with the
state-of-the-art video coding standard Versatile Video Coding (VVC) and the
latest generative compression schemes. Furthermore, the proposed framework
enables interactive human body video coding without any additional
pre-/post-manipulation processes, which is expected to shed light on
metaverse-related digital human communication in the future.

</details>


### [773] [Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression](https://arxiv.org/pdf/2505.16177)
*Linfeng Qi, Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu*

Main category: eess.IV

TL;DR: GLC models (GLC-image and GLC-video) use latent space coding for high-realism, high-fidelity compression at ultra-low bitrates, outperforming pixel-space methods.


<details>
  <summary>Details</summary>
Motivation: Pixel-space distortion misaligns with human perception, making high-realism and high-fidelity compression at ultra-low bitrates challenging.

Method: Transform coding in VQ-VAE latent space with improved hyper prior modules and code-prediction-based loss for semantic consistency.

Result: GLC-image achieves <0.04 bpp with 45% fewer bits than MS-ILLM; GLC-video saves 65.3% bitrate over PLVC.

Conclusion: GLC models excel in ultra-low bitrate compression by leveraging latent space advantages over pixel-space methods.

Abstract: Most existing approaches for image and video compression perform transform
coding in the pixel space to reduce redundancy. However, due to the
misalignment between the pixel-space distortion and human perception, such
schemes often face the difficulties in achieving both high-realism and
high-fidelity at ultra-low bitrate. To solve this problem, we propose
\textbf{G}enerative \textbf{L}atent \textbf{C}oding (\textbf{GLC}) models for
image and video compression, termed GLC-image and GLC-Video. The transform
coding of GLC is conducted in the latent space of a generative vector-quantized
variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent
space offers greater sparsity, richer semantics and better alignment with human
perception, and show its advantages in achieving high-realism and high-fidelity
compression. To further enhance performance, we improve the hyper prior by
introducing a spatial categorical hyper module in GLC-image and a
spatio-temporal categorical hyper module in GLC-video. Additionally, the
code-prediction-based loss function is proposed to enhance the semantic
consistency. Experiments demonstrate that our scheme shows high visual quality
at ultra-low bitrate for both image and video compression. For image
compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp,
achieving the same FID as previous SOTA model MS-ILLM while using $45\%$ fewer
bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves
65.3\% bitrate saving over PLVC in terms of DISTS.

</details>


### [774] [PCMamba: Physics-Informed Cross-Modal State Space Model for Dual-Camera Compressive Hyperspectral Imaging](https://arxiv.org/pdf/2505.16373)
*Ge Meng, Zhongnan Cai, Jingyan Tu, Yingying Wang, Chenxin Li, Yue Huang, Xinghao Ding*

Main category: eess.IV

TL;DR: The paper proposes PCMamba, a Physics-Informed Cross-Modal State Space Model Network, for high-quality hyperspectral imaging (HSI) reconstruction by integrating physical properties like temperature and emissivity into the model.


<details>
  <summary>Details</summary>
Motivation: Existing methods for HSI reconstruction focus on spectral and spatial information separately, creating a bottleneck. The paper aims to leverage physical properties (temperature, emissivity, texture) to improve reconstruction.

Method: PCMamba incorporates the physical imaging process into a Mamba-based model, disentangling key physical properties. It uses a Cross-Modal Scanning Mamba Block (CSMB) for inter-modal interaction.

Result: Experiments show PCMamba outperforms state-of-the-art methods in both quantitative and qualitative metrics.

Conclusion: PCMamba effectively integrates physical insights into HSI reconstruction, achieving superior performance.

Abstract: Panchromatic (PAN) -assisted Dual-Camera Compressive Hyperspectral Imaging
(DCCHI) is a key technology in snapshot hyperspectral imaging. Existing
research primarily focuses on exploring spectral information from 2D
compressive measurements and spatial information from PAN images in an explicit
manner, leading to a bottleneck in HSI reconstruction. Various physical
factors, such as temperature, emissivity, and multiple reflections between
objects, play a critical role in the process of a sensor acquiring
hyperspectral thermal signals. Inspired by this, we attempt to investigate the
interrelationships between physical properties to provide deeper theoretical
insights for HSI reconstruction. In this paper, we propose a Physics-Informed
Cross-Modal State Space Model Network (PCMamba) for DCCHI, which incorporates
the forward physical imaging process of HSI into the linear complexity of Mamba
to facilitate lightweight and high-quality HSI reconstruction. Specifically, we
analyze the imaging process of hyperspectral thermal signals to enable the
network to disentangle the three key physical properties-temperature,
emissivity, and texture. By fully exploiting the potential information embedded
in 2D measurements and PAN images, the HSIs are reconstructed through a
physics-driven synthesis process. Furthermore, we design a Cross-Modal Scanning
Mamba Block (CSMB) that introduces inter-modal pixel-wise interaction with
positional inductive bias by cross-scanning the backbone features and PAN
features. Extensive experiments conducted on both real and simulated datasets
demonstrate that our method significantly outperforms SOTA methods in both
quantitative and qualitative metrics.

</details>


### [775] [Quantum-Driven Multihead Inland Waterbody Detection With Transformer-Encoded CYGNSS Delay-Doppler Map Data](https://arxiv.org/pdf/2505.16391)
*Chia-Hsiang Lin, Jhao-Ting Lin, Po-Ying Chiu, Shih-Ping Chen, Charles C. H. Lin*

Main category: eess.IV

TL;DR: The paper proposes a quantum deep network (QUEEN) for inland waterbody detection (IWD) using delay-Doppler maps (DDM) from CYGNSS, outperforming traditional methods with high precision and speed.


<details>
  <summary>Details</summary>
Motivation: High-fidelity IWD mapping is essential for water resources and agricultural planning, but existing technologies are insufficient.

Method: Uses a customized transformer to encode DDM data, then processes it with QUEEN for classification, leveraging quantum unitary-computing features.

Result: IWD-QUEEN achieves high-precision detection, e.g., in the Amazon River Basin, and operates in near-real-time.

Conclusion: The method, with its non-quantum counterpart IWD-Transformer, offers a practical, accessible solution for IWD.

Abstract: Inland waterbody detection (IWD) is critical for water resources management
and agricultural planning. However, the development of high-fidelity IWD
mapping technology remains unresolved. We aim to propose a practical solution
based on the easily accessible data, i.e., the delay-Doppler map (DDM) provided
by NASA's Cyclone Global Navigation Satellite System (CYGNSS), which
facilitates effective estimation of physical parameters on the Earth's surface
with high temporal resolution and wide spatial coverage. Specifically, as
quantum deep network (QUEEN) has revealed its strong proficiency in addressing
classification-like tasks, we encode the DDM using a customized transformer,
followed by feeding the transformer-encoded DDM (tDDM) into a highly entangled
QUEEN to distinguish whether the tDDM corresponds to a hydrological region. In
recent literature, QUEEN has achieved outstanding performances in numerous
challenging remote sensing tasks (e.g., hyperspectral restoration, change
detection, and mixed noise removal, etc.), and its high effectiveness stems
from the fundamentally different way it adopts to extract features (the
so-called quantum unitary-computing features). The meticulously designed
IWD-QUEEN retrieves high-precision river textures, such as those in Amazon
River Basin in South America, demonstrating its superiority over traditional
classification methods and existing global hydrography maps. IWD-QUEEN,
together with its parallel quantum multihead scheme, works in a near-real-time
manner (i.e., millisecond-level computing per DDM). To broaden accessibility
for users of traditional computers, we also provide the non-quantum counterpart
of our method, called IWD-Transformer, thereby increasing the impact of this
work.

</details>


### [776] [Liver Cirrhosis Stage Estimation from MRI with Deep Learning](https://arxiv.org/pdf/2502.18225)
*Jun Zeng, Debesh Jha, Ertugrul Aktas, Elif Keles, Alpay Medetalibeyoglu, Matthew Antalek, Federica Proietto Salanitri, Amir A. Borhani, Daniela P. Ladner, Gorkem Durak, Ulas Bagci*

Main category: eess.IV

TL;DR: An end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of cirrhosis is challenging but crucial to prevent severe complications like decompensation and cancer. Current methods often miss early stages.

Method: Integrates multi-scale feature learning with sequence-specific attention mechanisms to detect subtle tissue variations across cirrhosis stages. Uses CirrMRI600+ dataset (628 MRI scans from 339 patients).

Result: Achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, outperforming traditional radiomics-based approaches.

Conclusion: Sets new benchmarks for automated cirrhosis staging and offers insights for clinically applicable deep learning systems. Code is publicly available.

Abstract: We present an end-to-end deep learning framework for automated liver
cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe
scarring (fibrosis) of the liver and a common endpoint of various chronic liver
diseases. Early diagnosis is vital to prevent complications such as
decompensation and cancer, which significantly decreases life expectancy.
However, diagnosing cirrhosis in its early stages is challenging, and patients
often present with life-threatening complications. Our approach integrates
multi-scale feature learning with sequence-specific attention mechanisms to
capture subtle tissue variations across cirrhosis progression stages. Using
CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution
MRI scans from 339 patients, we demonstrate state-of-the-art performance in
three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on
T1W and 63.8% on T2W sequences, significantly outperforming traditional
radiomics-based approaches. Through extensive ablation studies, we show that
our architecture effectively learns stage-specific imaging biomarkers. We
establish new benchmarks for automated cirrhosis staging and provide insights
for developing clinically applicable deep learning systems. The source code
will be available at https://github.com/JunZengz/CirrhosisStage.

</details>


### [777] [Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection](https://arxiv.org/pdf/2505.05291)
*Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar*

Main category: eess.IV

TL;DR: Self-supervised learning (SSL) pretrained Vision Transformers (ViTs) outperform domain-specific models in identifying age-related macular degeneration (AMD), challenging the need for in-domain pretraining.


<details>
  <summary>Details</summary>
Motivation: To determine if in-domain pretraining is necessary for robust AMD identification in retinal imaging, comparing SSL-pretrained ViTs on natural vs. ophthalmic data.

Method: Benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets (70,000 images) for AMD identification.

Result: iBOT pretrained on natural images achieved the highest generalization (AUROCs 0.80-0.97), outperforming domain-specific models (0.78-0.96) and a baseline ViT-L (0.68-0.91).

Conclusion: Foundation models improve AMD identification, questioning the necessity of in-domain pretraining. The BRAMD dataset is released for open access.

Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to
learn robust representations from large-scale natural image datasets, enhancing
their generalization across domains. In retinal imaging, foundation models
pretrained on either natural or ophthalmic data have shown promise, but the
benefits of in-domain pretraining remain uncertain. To investigate this, we
benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets
totaling 70,000 expert-annotated images for the task of moderate-to-late
age-related macular degeneration (AMD) identification. Our results show that
iBOT pretrained on natural images achieves the highest out-of-distribution
generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models,
which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,
which achieved AUROCs of 0.68-0.91. These findings highlight the value of
foundation models in improving AMD identification and challenge the assumption
that in-domain pretraining is necessary. Furthermore, we release BRAMD, an
open-access dataset (n=587) of DFIs with AMD labels from Brazil.

</details>
