<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 185]
- [cs.CV](#cs.CV) [Total: 180]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.LG](#cs.LG) [Total: 186]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 15]
- [eess.IV](#eess.IV) [Total: 28]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/pdf/2506.03259)
*Michael E. Garcia-Alcoser, Mobina GhojoghNejad, Fakrul Islam Tushar, David Kim, Kyle J. Lafata, Geoffrey D. Rubin, Joseph Y. Lo*

Main category: cs.CL

TL;DR: Lightweight LLMs outperform rule-based methods for CT report annotation, showing high agreement and generalization across organ systems with zero-shot prompting.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs in automating disease annotation of CT radiology reports compared to rule-based methods.

Method: Retrospective study analyzing 40,833 CT reports, testing three open-weight LLMs with zero-shot prompting, and evaluating performance using Cohen's Kappa and F1 scores.

Result: Gemma-3 27B and Llama-3.1 8B showed the highest agreement and performance, outperforming the rule-based algorithm.

Conclusion: LLMs offer a flexible, efficient solution for CT report annotation, though binary labels may not fully capture report nuances.

Abstract: Purpose: This study aims to evaluate the effectiveness of large language
models (LLMs) in automating disease annotation of CT radiology reports. We
compare a rule-based algorithm (RBA), RadBERT, and three lightweight
open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)
CT reports.
  Materials and Methods: This retrospective study analyzed 40,833 CT reports
from 29,540 patients, with 1,789 CAP reports manually annotated across three
organ systems. External validation was conducted using the CT-RATE dataset.
Three open-weight LLMs were tested with zero-shot prompting. Performance was
evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.
  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and
Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the
manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed
by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE
dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3
27B close behind (0.89). Performance differences were mainly due to differing
labeling practices, especially for lung atelectasis.
  Conclusion: Lightweight LLMs outperform rule-based methods for CT report
annotation and generalize across organ systems with zero-shot prompting.
However, binary labels alone cannot capture the full nuance of report language.
LLMs can provide a flexible, efficient solution aligned with clinical judgment
and user needs.

</details>


### [2] [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/pdf/2506.04070)
*Yi Zhao, Siqi Wang, Jing Li*

Main category: cs.CL

TL;DR: The paper introduces LaF-GRPO, a method using LLMs to simulate VI user responses for generating better navigation instructions, and presents NIG4VI, a 27k-sample benchmark. Results show improved instruction quality and usability.


<details>
  <summary>Details</summary>
Motivation: Navigation instruction generation for visually impaired individuals is underexplored but critical. The study aims to produce precise, usable instructions.

Method: Proposes LaF-GRPO, where an LLM simulates VI user responses to guide a Vision-Language Model post-training, reducing real-world data needs. Introduces NIG4VI benchmark for training and testing.

Result: LaF-GRPO improves instruction quality (e.g., BLEU +14%, METEOR 0.542 vs. GPT-4o's 0.323) and yields safer, more intuitive instructions.

Conclusion: LaF-GRPO effectively enhances navigation instruction generation for VI users, supported by the NIG4VI benchmark.

Abstract: Navigation instruction generation for visually impaired (VI) individuals
(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses
on producing precise, in-situ, step-by-step navigation instructions that are
practically usable by VI users. Concretely, we propose LaF-GRPO
(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate
rewards guiding the Vision-Language Model (VLM) post-training. This enhances
instruction usability while reducing costly real-world data needs. To
facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced
benchmark. It provides diverse navigation scenarios with accurate spatial
coordinates, supporting detailed, open-ended in-situ instruction generation.
Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative
metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542
vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and
benchmark are available at
\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.

</details>


### [3] [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/pdf/2506.03268)
*Cristiano Chesi*

Main category: cs.CL

TL;DR: A summary of responses to a target paper in the Italian Journal of Linguistics.


<details>
  <summary>Details</summary>
Motivation: To address and reflect on the feedback and discussions generated by the target paper.

Method: Review and analysis of the replies received.

Result: Insights and perspectives from the responses are highlighted.

Conclusion: The paper concludes with reflections on the discourse and contributions to the field.

Abstract: This is the final remark on the replies received to my target paper in the
Italian Journal of Linguistics

</details>


### [4] [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/pdf/2506.03278)
*Christodoulos Constantinides, Dhaval Patel, Shuxin Lin, Claudio Guerrero, Sunil Dagajirao Patil, Jayant Kalagnanam*

Main category: cs.CL

TL;DR: FailureSensorIQ is a novel MCQA benchmarking system for LLMs, focusing on Industry 4.0 scenarios. It evaluates reasoning, domain knowledge, and robustness, revealing gaps in LLM performance.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to reason about complex, domain-specific industrial scenarios, shifting from purely data-driven to domain-driven modeling decisions.

Method: Uses Perturbation-Uncertainty-Complexity analysis, Expert Evaluation, Asset-Specific Knowledge Gap analysis, and ReAct agents with external knowledge-bases to evaluate LLMs.

Result: Closed-source models approach expert-level performance but show fragility to perturbations and knowledge gaps.

Conclusion: FailureSensorIQ highlights LLMs' limitations in industrial reasoning and provides tools (benchmark, leaderboard, feature selector) for future research.

Abstract: We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)
benchmarking system designed to assess the ability of Large Language Models
(LLMs) to reason and understand complex, domain-specific scenarios in Industry
4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects
of reasoning through failure modes, sensor data, and the relationships between
them across various industrial assets. Through this work, we envision a
paradigm shift where modeling decisions are not only data-driven using
statistical tools like correlation analysis and significance tests, but also
domain-driven by specialized LLMs which can reason about the key contributors
and useful patterns that can be captured with feature engineering. We evaluate
the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and
Mistral-on FailureSensorIQ from different lens using
Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study,
Asset-Specific Knowledge Gap analysis, ReAct agent using external
knowledge-bases. Even though closed-source models with strong reasoning
capabilities approach expert-level performance, the comprehensive benchmark
reveals a significant drop in performance that is fragile to perturbations,
distractions, and inherent knowledge gaps in the models. We also provide a
real-world case study of how LLMs can drive the modeling decisions on 3
different failure prediction datasets related to various assets. We release:
(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ
benchmark and Hugging Face leaderboard based on MCQA built from non-textual
data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature
selection scikit-learn pipeline. The software is available at
https://github.com/IBM/FailureSensorIQ.

</details>


### [5] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/pdf/2506.03292)
*Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu, Michael Sklar, Christopher Potts, Atticus Geiger*

Main category: cs.CL

TL;DR: HyperSteer, a hypernetwork-based method, generates effective steering vectors for language models using natural language prompts, outperforming unsupervised and supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for steering language models either lack guarantees on individual vector efficacy (unsupervised) or require extensive data/training (supervised). HyperSteer aims to bridge this gap.

Method: HyperSteer uses hypernetwork architectures trained end-to-end to generate steering vectors conditioned on natural language prompts and LM internals.

Result: HyperSteer scales well with thousands of prompts, outperforming state-of-the-art methods, even on unseen prompts, and matches steering-via-prompting performance.

Conclusion: HyperSteer offers a scalable, effective solution for steering language models, combining the strengths of unsupervised and supervised methods.

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [6] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/pdf/2506.03295)
*Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, Wenhu Chen*

Main category: cs.CL

TL;DR: One-shot Critique Fine-Tuning (CFT) efficiently unleashes LLMs' reasoning potential with minimal compute, outperforming RL methods.


<details>
  <summary>Details</summary>
Motivation: RL is expensive and unstable for improving LLMs' reasoning; CFT offers a simpler, cost-effective alternative.

Method: CFT constructs critique data from diverse model solutions and teacher critiques, fine-tuning models like Qwen and Llama.

Result: Qwen-Math-7B-CFT improves 15% on math and 16% on logic tasks with just 5 GPU hours, matching RL with 20x less compute.

Conclusion: One-shot CFT is a robust, efficient method to enhance LLMs' reasoning capabilities.

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [7] [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/pdf/2506.03681)
*Pradeep Rangappa, Andres Carofilis, Jeena Prakash, Shashi Kumar, Sergio Burdisso, Srikanth Madikeri, Esau Villatoro-Tello, Bidisha Sharma, Petr Motlicek, Kadri Hacioglu, Shankar Venkatesan, Saurabh Vyas, Andreas Stolcke*

Main category: cs.CL

TL;DR: A robust data selection pipeline improves ASR adaptation by filtering pseudo-labels from Whisper and Zipformer models, reducing dataset size while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning pretrained ASR models for specific domains is challenging for small organizations due to limited labeled data and computational resources.

Method: Proposes a data selection approach integrating WER prediction, NER, and CER analysis to extract high-quality training segments from pseudo-labels.

Result: Fine-tuning on filtered 100-hour dataset achieves similar performance (12.3% WER) as the 7500-hour baseline; observed on call center and Fisher English data.

Conclusion: The method effectively reduces dataset size without compromising ASR performance, making domain adaptation more feasible for resource-limited settings.

Abstract: Fine-tuning pretrained ASR models for specific domains is challenging for
small organizations with limited labeled data and computational resources.
Here, we explore different data selection pipelines and propose a robust
approach that improves ASR adaptation by filtering pseudo-labels generated
using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach
integrates multiple selection strategies -- including word error rate (WER)
prediction, named entity recognition (NER), and character error rate (CER)
analysis -- to extract high-quality training segments. We evaluate our method
on Whisper and Zipformer using a 7500-hour baseline, comparing it to a
CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on
7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our
filtering reduces the dataset to 100 hours (1.4%) with similar performance; a
similar trend is observed on Fisher English.

</details>


### [8] [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/pdf/2506.03301)
*Daham M. Mustafa, Abhishek Nadgeri, Diego Collarana, Benedikt T. Arnold, Christoph Quix, Christoph Lange, Stefan Decker*

Main category: cs.CL

TL;DR: Using GPT-4 and ODRL ontology, the study automates policy generation from natural language, achieving 91.95% accuracy in knowledge graphs for dataspaces.


<details>
  <summary>Details</summary>
Motivation: To automate the creation of usage policies in ODRL by leveraging large language models and curated ontology documentation.

Method: Uses GPT-4 with ODRL ontology and documentation as prompts, applying heuristics for end-to-end KG construction.

Result: Achieves up to 91.95% accuracy in generating knowledge graphs for 12 cultural domain use cases.

Conclusion: Curated ontology documentation effectively guides policy generation, demonstrating high accuracy in automated KG construction.

Abstract: This study presents an approach that uses large language models such as GPT-4
to generate usage policies in the W3C Open Digital Rights Language ODRL
automatically from natural language instructions. Our approach uses the ODRL
ontology and its documentation as a central part of the prompt. Our research
hypothesis is that a curated version of existing ontology documentation will
better guide policy generation. We present various heuristics for adapting the
ODRL ontology and its documentation to guide an end-to-end KG construction
process. We evaluate our approach in the context of dataspaces, i.e.,
distributed infrastructures for trustworthy data exchange between multiple
participating organizations for the cultural domain. We created a benchmark
consisting of 12 use cases of varying complexity. Our evaluation shows
excellent results with up to 91.95% accuracy in the resulting knowledge graph.

</details>


### [9] [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/pdf/2506.03722)
*Yinfeng Xia, Huiyan Li, Chenyang Le, Manhong Wang, Yutao Sun, Xingyang Ma, Yanmin Qian*

Main category: cs.CL

TL;DR: A novel prefix-to-prefix training framework for streaming speech recognition using Whisper, featuring Continuous Integrate-and-Fire and Monotonic Finite Look-ahead Attention for low-latency, high-quality results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating large pre-trained speech models like Whisper into streaming systems while maintaining efficiency and quality.

Method: Fine-tuning Whisper with a prefix-to-prefix framework, Continuous Integrate-and-Fire for alignment, Monotonic Finite Look-ahead Attention for context handling, and wait-k decoding for consistency.

Result: Achieves a controllable trade-off between latency and quality, suitable for streaming applications.

Conclusion: The proposed framework effectively enables streaming recognition with Whisper, balancing performance and latency.

Abstract: Applying large pre-trained speech models like Whisper has shown promise in
reducing training costs for various speech tasks. However, integrating these
models into streaming systems remains a challenge. This paper presents a novel
prefix-to-prefix training framework for streaming recognition by fine-tuning
the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to
establish a quasi-monotonic alignment between continuous speech sequences and
discrete text tokens. Additionally, we design Monotonic Finite Look-ahead
Attention, allowing each token to attend to infinite left-context and finite
right-context from the speech sequences. We also employ the wait-k decoding
strategy to simplify the decoding process while ensuring consistency between
training and testing. Our theoretical analysis and experiments demonstrate that
this approach achieves a controllable trade-off between latency and quality,
making it suitable for various streaming applications.

</details>


### [10] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/pdf/2506.03303)
*Mustafa Eyceoz, Nikhil Shivakumar Nayak, Hao Wang, Ligong Han, Akash Srivastava*

Main category: cs.CL

TL;DR: Hopscotch is a method to skip less important attention blocks in causal language models, optimizing performance with minimal output quality loss.


<details>
  <summary>Details</summary>
Motivation: Modern causal language models use many attention blocks, but not all are necessary for every task, leading to inefficiency.

Method: Hopscotch identifies and skips low-contribution blocks, using trainable scaling parameters to mitigate hidden state shifts.

Result: Applied to Llama-3.1-8B and Qwen2.5-7B, it skips four blocks with <2% performance drop.

Conclusion: Hopscotch efficiently reduces computation without modifying weights or requiring additional data.

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [11] [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/pdf/2506.03832)
*Omer Moussa, Mariya Toneva*

Main category: cs.CL

TL;DR: Brain-tuned speech models improve semantic alignment with human brain processing and exhibit hierarchical structure from acoustic to semantic representations.


<details>
  <summary>Details</summary>
Motivation: To investigate if brain-tuned speech models better reflect the brain's intermediate stages of processing compared to pretrained models.

Method: Fine-tuning speech models using human brain recordings (brain-tuning) and analyzing layer-wise alignment with brain regions.

Result: Late layers of brain-tuned models align better with semantic regions, while early layers focus on acoustic features.

Conclusion: Brain-tuned models perform better and mirror human speech hierarchy, making them superior for studying speech processing.

Abstract: Pretrained self-supervised speech models excel in speech tasks but do not
reflect the hierarchy of human speech processing, as they encode rich semantics
in middle layers and poor semantics in late layers. Recent work showed that
brain-tuning (fine-tuning models using human brain recordings) improves speech
models' semantic understanding. Here, we examine how well brain-tuned models
further reflect the brain's intermediate stages of speech processing. We find
that late layers of brain-tuned models substantially improve over pretrained
models in their alignment with semantic language regions. Further layer-wise
probing reveals that early layers remain dedicated to low-level acoustic
features, while late layers become the best at complex high-level tasks. These
findings show that brain-tuned models not only perform better but also exhibit
a well-defined hierarchical processing going from acoustic to semantic
representations, making them better model organisms for human speech
processing.

</details>


### [12] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/pdf/2506.03310)
*Guillermo Marco, Julio Gonzalo, Víctor Fresno*

Main category: cs.CL

TL;DR: The study explains conflicting AI vs. human text evaluations by showing reader preferences shape perceived quality, identifying two reader profiles.


<details>
  <summary>Details</summary>
Motivation: Resolve inconsistencies in AI vs. human literary text evaluations by examining reader preferences.

Method: Analyzed 1,471 stories using 17 textual features, modeled reader preferences, and clustered them in a preference space.

Result: Two reader profiles emerged: surface-focused (non-experts) and holistic (experts), explaining divergent quality assessments.

Conclusion: Reader-sensitive evaluation frameworks are needed for fair AI-generated text assessment.

Abstract: Recent studies comparing AI-generated and human-authored literary texts have
produced conflicting results: some suggest AI already surpasses human quality,
while others argue it still falls short. We start from the hypothesis that such
divergences can be largely explained by genuine differences in how readers
interpret and value literature, rather than by an intrinsic quality of the
texts evaluated. Using five public datasets (1,471 stories, 101 annotators
including critics, students, and lay readers), we (i) extract 17 reference-less
textual features (e.g., coherence, emotional variance, average sentence
length...); (ii) model individual reader preferences, deriving feature
importance vectors that reflect their textual priorities; and (iii) analyze
these vectors in a shared "preference space". Reader vectors cluster into two
profiles: 'surface-focused readers' (mainly non-experts), who prioritize
readability and textual richness; and 'holistic readers' (mainly experts), who
value thematic development, rhetorical variety, and sentiment dynamics. Our
results quantitatively explain how measurements of literary quality are a
function of how text features align with each reader's preferences. These
findings advocate for reader-sensitive evaluation frameworks in the field of
creative text generation.

</details>


### [13] [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/pdf/2506.04037)
*Dan Oneata, Leanne Nortje, Yevgen Matusevych, Herman Kamper*

Main category: cs.CL

TL;DR: Bilingual visually grounded speech (VGS) models show a weaker mutual exclusivity (ME) bias compared to monolingual models, with visual embeddings explaining the reduced bias.


<details>
  <summary>Details</summary>
Motivation: To investigate how bilingualism affects the ME bias in VGS models, given that bilingual children may use ME less due to cross-lingual ambiguity.

Method: Training bilingual VGS models on combinations of English, French, and Dutch, then analyzing their ME bias and visual embeddings.

Result: Bilingual models exhibit a weaker ME bias than monolingual ones, with combined visual embeddings showing smaller variance for familiar data.

Conclusion: The study provides insights into the ME bias in VGS models and explains the reduced bias in bilingual settings through visual embedding analysis.

Abstract: Mutual exclusivity (ME) is a strategy where a novel word is associated with a
novel object rather than a familiar one, facilitating language learning in
children. Recent work has found an ME bias in a visually grounded speech (VGS)
model trained on English speech with paired images. But ME has also been
studied in bilingual children, who may employ it less due to cross-lingual
ambiguity. We explore this pattern computationally using bilingual VGS models
trained on combinations of English, French, and Dutch. We find that bilingual
models generally exhibit a weaker ME bias than monolingual models, though
exceptions exist. Analyses show that the combined visual embeddings of
bilingual models have a smaller variance for familiar data, partly explaining
the increase in confusion between novel and familiar concepts. We also provide
new insights into why the ME bias exists in VGS models in the first place. Code
and data: https://github.com/danoneata/me-vgs

</details>


### [14] [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/pdf/2506.04076)
*Jhen-Ke Lin, Hao-Chien Lu, Chung-Chun Wang, Hong-Yun Lin, Berlin Chen*

Main category: cs.CL

TL;DR: Fine-tuning Whisper models with precise disfluency labeling improves ASR accuracy for verbatim L2 speech transcription.


<details>
  <summary>Details</summary>
Motivation: Accurate capture of disfluencies is crucial for automatic speaking assessment, but many ASR systems discard or generalize hesitations, losing important details.

Method: Fine-tuned Whisper models on the Speak & Improve 2025 corpus using LoRA, comparing three annotation schemes: Pure (removing hesitations), Rich (generic tags), and Extra (acoustically precise fillers).

Result: The "Extra" scheme achieved 5.5% WER, an 11.3% relative improvement over the "Pure" scheme (6.2% WER).

Conclusion: Explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.

Abstract: Verbatim transcription for automatic speaking assessment demands accurate
capture of disfluencies, crucial for downstream tasks like error analysis and
feedback. However, many ASR systems discard or generalize hesitations, losing
important acoustic details. We fine-tune Whisper models on the Speak & Improve
2025 corpus using low-rank adaptation (LoRA), without recourse to external
audio training data. We compare three annotation schemes: removing hesitations
(Pure), generic tags (Rich), and acoustically precise fillers inferred by
Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge
system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge
experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra"
scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme
(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling
significantly enhances ASR accuracy for verbatim L2 speech transcription.

</details>


### [15] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/pdf/2506.03312)
*Celia Chen, Scotty Beland, Ingo Burghardt, Jill Byczek, William J. Conway, Eric Cotugno, Sadaf Davre, Megan Fletcher, Rajesh Kumar Gnanasekaran, Kristin Hamilton, Marilyn Harbert, Jordan Heustis, Tanaya Jha, Emily Klein, Hayden Kramer, Alex Leitch, Jessica Perkins, Casi Sherman, Celia Sterrn, Logan Stevens, Rebecca Zarrella, Jennifer Golbeck*

Main category: cs.CL

TL;DR: A cross-platform dataset of 30,000 posts hand-coded for violent threats is introduced, achieving high classification accuracy in machine learning analysis across platforms.


<details>
  <summary>Details</summary>
Motivation: Violent threats on social media are a significant issue, and high-quality data is needed for research and detection.

Method: A hand-coded dataset of violent posts is created and evaluated using machine learning with an existing YouTube dataset.

Result: High classification accuracy is achieved across different platforms and coding criteria.

Conclusion: The findings support cross-platform content-classification strategies and better understanding of violent content.

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [16] [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/pdf/2506.04077)
*Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen*

Main category: cs.CL

TL;DR: A novel training paradigm using LLMs and synthesized speech improves automated speaking assessment (ASA) for opinion expressions by addressing data scarcity.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled recordings limits prompt diversity and scoring reliability in ASA for opinion expressions.

Method: Leverages LLMs to generate diverse responses, converts them to synthesized speech, and uses dynamic importance loss for training. A multimodal LLM integrates text and speech features for scoring.

Result: Outperforms methods using real data or conventional augmentation on the LTTC dataset.

Conclusion: The approach mitigates low-resource constraints and enables ASA on opinion expressions with cross-modal information.

Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered
by the scarcity of labeled recordings, which restricts prompt diversity and
undermines scoring reliability. To address this challenge, we propose a novel
training paradigm that leverages a large language models (LLM) to generate
diverse responses of a given proficiency level, converts responses into
synthesized speech via speaker-aware text-to-speech synthesis, and employs a
dynamic importance loss to adaptively reweight training instances based on
feature distribution differences between synthesized and real speech.
Subsequently, a multimodal large language model integrates aligned textual
features with speech signals to predict proficiency scores directly.
Experiments conducted on the LTTC dataset show that our approach outperforms
methods relying on real data or conventional augmentation, effectively
mitigating low-resource constraints and enabling ASA on opinion expressions
with cross-modal information.

</details>


### [17] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/pdf/2506.03357)
*Aldan Creo, Héctor Cerezo-Costas, Pedro Alonso-Doval, Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: The paper introduces 'Ask a Local,' a method for detecting hallucinations in LLMs by leveraging specialized models' surprise at domain-specific inaccuracies, showing consistent multilingual performance without language-specific adaptations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of hallucinations in LLMs, where models generate plausible but incorrect information, by proposing a scalable, multilingual detection method.

Method: The approach computes divergence between perplexity distributions of language-specialized models to identify hallucinated spans, avoiding the need for external data or training.

Result: Demonstrated consistent performance across 14 languages, with IoU scores around 0.3 and strong results for Italian (0.42) and Catalan (0.38).

Conclusion: The method is scalable, effective across languages, and released for further research in multilingual hallucination detection.

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [18] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/pdf/2506.03360)
*Zihui Ma, Lingyao Li, Juan Li, Wenyue Hua, Jingxiao Liu, Qingyuan Feng, Yuki Miura*

Main category: cs.CL

TL;DR: The paper proposes a 3M pipeline using multimodal large language models (MLLMs) for rapid disaster damage assessment, showing strong correlation with seismic data but varying performance based on language, distance, and modality.


<details>
  <summary>Details</summary>
Motivation: Rapid disaster assessment is hindered by limited ground sensors and delayed reporting. Social media offers real-time data but is challenging due to its unstructured nature.

Method: A structured 3M pipeline leveraging MLLMs to analyze multimodal social media data, evaluated across two earthquakes using macro- and micro-level analyses.

Result: MLLMs effectively integrate image-text signals and correlate with seismic data, though performance varies by language, epicentral distance, and modality.

Conclusion: MLLMs show promise for disaster assessment, providing a foundation for future real-time crisis applications. Code and data are publicly available.

Abstract: Rapid, fine-grained disaster damage assessment is essential for effective
emergency response, yet remains challenging due to limited ground sensors and
delays in official reporting. Social media provides a rich, real-time source of
human-centric observations, but its multimodal and unstructured nature presents
challenges for traditional analytical methods. In this study, we propose a
structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that
leverages multimodal large language models (MLLMs) to assess disaster impacts.
We evaluate three foundation models across two major earthquake events using
both macro- and micro-level analyses. Results show that MLLMs effectively
integrate image-text signals and demonstrate a strong correlation with
ground-truth seismic data. However, performance varies with language,
epicentral distance, and input modality. This work highlights the potential of
MLLMs for disaster assessment and provides a foundation for future research in
applying MLLMs to real-time crisis contexts. The code and data are released at:
https://github.com/missa7481/EMNLP25_earthquake

</details>


### [19] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/pdf/2506.03408)
*Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu*

Main category: cs.CL

TL;DR: A survey on integrating large language models (LLMs) into trajectory prediction, categorizing recent work into five directions and analyzing methods, design choices, and challenges.


<details>
  <summary>Details</summary>
Motivation: The growing interest in leveraging LLMs' semantic and reasoning capabilities to enhance autonomous systems' trajectory prediction.

Method: Categorizes recent work into five directions, analyzing representative methods and design choices for each.

Result: Provides a unified perspective on how language can enrich trajectory prediction, bridging NLP and trajectory prediction.

Conclusion: The survey highlights the potential of LLMs in trajectory prediction while identifying open challenges for future research.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [20] [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/pdf/2506.03424)
*Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet*

Main category: cs.CL

TL;DR: DistRAG enhances LLMs' spatial reasoning by retrieving geodesic distances between locations, enabling them to answer distance-based questions they couldn't before.


<details>
  <summary>Details</summary>
Motivation: LLMs lack reliable spatial reasoning, especially for distances, limiting their use in tasks like POI recommendation and itinerary planning.

Method: DistRAG encodes geodesic distances in a graph and retrieves relevant subgraphs for context, supplementing LLMs' linguistic knowledge.

Result: The method allows LLMs to answer distance-based reasoning questions accurately.

Conclusion: DistRAG is a flexible step toward equipping LLMs with a rudimentary 'world model' for spatial tasks.

Abstract: Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.

</details>


### [21] [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/pdf/2506.03434)
*Ahmad Dawar Hakimi, Ali Modarressi, Philipp Wicke, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper analyzes how the OLMo-7B model's attention heads and FFNs evolve during pre-training, revealing their roles, stability, and transitions in representing factual knowledge.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and reliability of LLMs by understanding how they acquire and store factual knowledge.

Method: Tracking the roles of attention heads and FFNs in OLMo-7B during pre-training, classifying them into four roles, and examining stability and transitions.

Result: LLMs start with general-purpose components that later specialize; attention heads show high turnover, while FFNs remain stable. Location-based relations converge earlier than name-based ones.

Conclusion: The study provides a mechanistic view of knowledge formation in LLMs, showing adaptive learning and task complexity's impact on acquisition.

Abstract: Understanding how large language models (LLMs) acquire and store factual
knowledge is crucial for enhancing their interpretability and reliability. In
this work, we analyze the evolution of factual knowledge representation in the
OLMo-7B model by tracking the roles of its attention heads and feed forward
networks (FFNs) over the course of pre-training. We classify these components
into four roles: general, entity, relation-answer, and fact-answer specific,
and examine their stability and transitions. Our results show that LLMs
initially depend on broad, general-purpose components, which later specialize
as training progresses. Once the model reliably predicts answers, some
components are repurposed, suggesting an adaptive learning process. Notably,
attention heads display the highest turnover. We also present evidence that
FFNs remain more stable throughout training. Furthermore, our probing
experiments reveal that location-based relations converge to high accuracy
earlier in training than name-based relations, highlighting how task complexity
shapes acquisition dynamics. These insights offer a mechanistic view of
knowledge formation in LLMs.

</details>


### [22] [Transformers in Speech Processing: A Survey](https://arxiv.org/pdf/2303.11607)
*Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Muhammad Usama, Junaid Qadir*

Main category: cs.CL

TL;DR: A survey on transformers in speech processing, covering applications, challenges, and potential solutions.


<details>
  <summary>Details</summary>
Motivation: Explore transformers' potential for modeling long-range dependencies in speech sequences and unify research across speech technology subfields.

Method: Comprehensive survey consolidating findings from diverse speech technology domains.

Result: Identifies challenges of transformers in speech processing and suggests solutions.

Conclusion: Provides a resource for researchers to advance speech technology using transformers.

Abstract: The remarkable success of transformers in the field of natural language
processing has sparked the interest of the speech-processing community, leading
to an exploration of their potential for modeling long-range dependencies
within speech sequences. Recently, transformers have gained prominence across
various speech-related domains, including automatic speech recognition, speech
synthesis, speech translation, speech para-linguistics, speech enhancement,
spoken dialogue systems, and numerous multimodal applications. In this paper,
we present a comprehensive survey that aims to bridge research studies from
diverse subfields within speech technology. By consolidating findings from
across the speech technology landscape, we provide a valuable resource for
researchers interested in harnessing the power of transformers to advance the
field. We identify the challenges encountered by transformers in speech
processing while also offering insights into potential solutions to address
these issues.

</details>


### [23] [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/pdf/2506.03458)
*Zahra Bokaei, Walid Magdy, Bonnie Webber*

Main category: cs.CL

TL;DR: The paper explores toxic language detection in Persian, comparing methods like fine-tuning, data enrichment, and transfer learning, highlighting the role of cultural context in transfer learning effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored area of toxic language detection in Persian and improve online safety by identifying effective detection methods.

Method: Comparison of fine-tuning, data enrichment, zero-shot/few-shot learning, and cross-lingual transfer learning, with emphasis on cultural context in transfer learning.

Result: Cultural similarity between languages enhances transfer learning performance, while distinct cultures yield lower improvements.

Conclusion: Cultural context significantly impacts transfer learning for toxic language detection in Persian, suggesting its importance in model development.

Abstract: Toxic language detection is crucial for creating safer online environments
and limiting the spread of harmful content. While toxic language detection has
been under-explored in Persian, the current work compares different methods for
this task, including fine-tuning, data enrichment, zero-shot and few-shot
learning, and cross-lingual transfer learning. What is especially compelling is
the impact of cultural context on transfer learning for this task: We show that
the language of a country with cultural similarities to Persian yields better
results in transfer learning. Conversely, the improvement is lower when the
language comes from a culturally distinct country. Warning: This paper contains
examples of toxic language that may disturb some readers. These examples are
included for the purpose of research on toxic detection.

</details>


### [24] [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/pdf/2506.03476)
*Chuyuan Li, Raymond Li, Thalia S. Field, Giuseppe Carenini*

Main category: cs.CL

TL;DR: Delta-KNN, a novel demonstration selection strategy, improves in-context learning (ICL) for Alzheimer's Disease (AD) diagnosis using patient-generated text, outperforming conventional methods and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Early intervention in AD can benefit from linguistic analysis, but conventional ICL methods perform poorly due to task complexity.

Method: Introduces Delta-KNN, combining delta scores for training example gains and KNN-based retrieval to dynamically select optimal examples.

Result: Delta-KNN outperforms existing ICL baselines across three LLMs, achieving state-of-the-art results with Llama-3.1.

Conclusion: Delta-KNN enhances AD diagnosis via ICL, demonstrating superior performance and potential for clinical applications.

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that
leads to dementia, and early intervention can greatly benefit from analyzing
linguistic abnormalities. In this work, we explore the potential of Large
Language Models (LLMs) as health assistants for AD diagnosis from
patient-generated text using in-context learning (ICL), where tasks are defined
through a few input-output examples. Empirical results reveal that conventional
ICL methods, such as similarity-based selection, perform poorly for AD
diagnosis, likely due to the inherent complexity of this task. To address this,
we introduce Delta-KNN, a novel demonstration selection strategy that enhances
ICL performance. Our method leverages a delta score to assess the relative
gains of each training example, coupled with a KNN-based retriever that
dynamically selects optimal "representatives" for a given input. Experiments on
two AD detection datasets across three open-source LLMs demonstrate that
Delta-KNN consistently outperforms existing ICL baselines. Notably, when using
the Llama-3.1 model, our approach achieves new state-of-the-art results,
surpassing even supervised classifiers.

</details>


### [25] [Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities](https://arxiv.org/pdf/2503.04721)
*Guan-Ting Lin, Jiachen Lian, Tingle Li, Qirui Wang, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee*

Main category: cs.CL

TL;DR: The paper introduces Full-Duplex-Bench, a benchmark for evaluating full-duplex spoken dialogue models (SDMs) on interactive behaviors like pause handling and turn-taking, aiming to improve naturalness in conversations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of SDMs are limited to turn-based metrics or coarse analyses, lacking systematic assessment of interactive behaviors essential for natural dialogue.

Method: The authors propose Full-Duplex-Bench, a framework using automatic metrics to evaluate pause handling, backchanneling, turn-taking, and interruption management.

Result: The benchmark provides a consistent, reproducible, and fast evaluation setup for SDMs.

Conclusion: By releasing the benchmark and code, the authors aim to advance SDM development toward more natural and engaging spoken dialogue systems.

Abstract: Spoken dialogue modeling poses challenges beyond text-based language
modeling, requiring real-time interaction, turn-taking, and backchanneling.
While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing
one turn at a time - emerging full-duplex SDMs can listen and speak
simultaneously, enabling more natural conversations. However, current
evaluations remain limited, focusing mainly on turn-based metrics or coarse
corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a
benchmark that systematically evaluates key interactive behaviors: pause
handling, backchanneling, turn-taking, and interruption management. Our
framework uses automatic metrics for consistent, reproducible assessment and
provides a fair, fast evaluation setup. By releasing our benchmark and code, we
aim to advance spoken dialogue modeling and foster the development of more
natural and engaging SDMs.

</details>


### [26] [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/pdf/2506.03483)
*Jun Rao, Zepeng Lin, Xuebo Liu, Xiaopeng Ke, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang*

Main category: cs.CL

TL;DR: APT enhances domain-specific LLM performance using self-generated weakness data, preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Balancing domain-specific improvements with general model utility is challenging.

Method: APT uses self-generated dis-preferred weakness data (bad/similar cases) for targeted training.

Result: APT retains generic capacity and outperforms existing methods on downstream tasks.

Conclusion: APT effectively boosts domain-specific performance without compromising broader applicability.

Abstract: Large Language Models (LLMs) often require domain-specific fine-tuning to
address targeted tasks, which risks degrading their general capabilities.
Maintaining a balance between domain-specific enhancements and general model
utility is a key challenge. This paper proposes a novel approach named APT
(Weakness Case Acquisition and Iterative Preference Training) to enhance
domain-specific performance with self-generated dis-preferred weakness data
(bad cases and similar cases). APT uniquely focuses on training the model using
only those samples where errors occur, alongside a small, similar set of
samples retrieved for this purpose. This targeted training minimizes
interference with the model's existing knowledge base, effectively retaining
generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3
models across various benchmarks demonstrate that APT ensures no reduction in
generic capacity and achieves superior performance on downstream tasks compared
to various existing methods. This validates our method as an effective strategy
for enhancing domain-specific capabilities without sacrificing the model's
broader applicability.

</details>


### [27] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/pdf/2506.03484)
*Melkamu Abay Mersha, Mesay Gemeda Yigezu, Atnafu Lambebo Tonja, Hassan Shakil, Samer Iskander, Olga Kolesnikova, Jugal Kalita*

Main category: cs.CL

TL;DR: The paper introduces XAI-Guided Context-Aware Data Augmentation, a framework using XAI to enhance data augmentation by preserving task-relevant features, improving model accuracy for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of labeled data scarcity and noise in conventional data augmentation for low-resource languages.

Method: Proposes XAI-SR-BT and XAI-PR-BT, leveraging XAI to modify less critical features while preserving task-relevant ones, with an iterative feedback loop.

Result: Improves accuracy by 6.6% and 8.1% for hate speech and sentiment analysis, outperforming baselines by 4.8% and 5%.

Conclusion: The framework offers a controlled, interpretable, and context-aware solution, advancing AI model training with XAI.

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [28] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/pdf/2506.03489)
*Mingxu Tao, Jie Hu, Mingchuan Yang, Yunhuai Liu, Dongyan Zhao, Yansong Feng*

Main category: cs.CL

TL;DR: EpiCoDe improves LLM performance in data-scarce settings without extra training by combining model extrapolation and contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated data is costly, limiting LLMs' downstream task capabilities. EpiCoDe addresses this by enhancing models without additional training.

Method: Uses model extrapolation to combine a finetuned model with its inferior version, then applies contrastive decoding to reduce errors by comparing logit scores.

Result: EpiCoDe outperforms existing methods across three tasks and four LLMs, showing significant and robust improvement.

Conclusion: EpiCoDe is effective in data-scarce scenarios, and a new theoretical framework explains its success via contrastive decoding.

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [29] [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/pdf/2506.03490)
*Shigeng Chen, Linhao Luo, Zhangchi Qiu, Yanan Cao, Carl Yang, Shirui Pan*

Main category: cs.CL

TL;DR: The paper introduces MedEditBench, a framework to evaluate knowledge editing (KE) methods in the medical domain, revealing their limitations and proposing SGR-Edit for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current KE methods lack effectiveness in the complex medical domain, requiring generalization and interpretability for decision-making.

Method: The authors propose MedEditBench, a benchmark with three editing paradigms, and SGR-Edit, which uses model-derived rationales for editing.

Result: Existing KE methods show superficial memorization, while SGR-Edit significantly improves generalization and reasoning.

Conclusion: The study provides insights into medical KE, including knowledge localization and sequential editing, offering practical guidance for real-world applications.

Abstract: Recently, knowledge editing (KE) has emerged as a promising approach to
update specific facts in Large Language Models (LLMs) without the need for full
retraining. Despite the effectiveness in general-domain benchmarks, their
applicability to complex medical domain remains largely unexplored. Medical
knowledge editing is particularly challenging, as it requires LLMs to
internalize the knowledge and generalize to unseen scenarios for effective and
interpretable decision-making. In this work, we propose a novel framework
called MedEditBench to rigorously evaluate the effectiveness of existing KE
methods in the medical domain. In MedEditBench, we introduce a new medical
knowledge editing benchmark as well as three different knowledge editing
paradigms, which are designed to assess the impact of different knowledge
sources for editing. Our findings indicate that current KE methods result in
only superficial memorization of the injected information, failing to
generalize to new scenarios. To overcome this limitation, we present
Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived
rationales as the target knowledge for editing, thereby uncovering the
underlying reasoning process and demonstrating significant improvements over
existing KE approaches. Additionally, we offer deeper insights into medical
knowledge editing, including the localization of medical knowledge in LLMs and
the impact of sequential editing on evolving knowledge. This could provide
practical guidance for implementing KE methods in real-world medical
applications.

</details>


### [30] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/pdf/2506.03501)
*Yuchen Guo, Zhicheng Dou, Huy H. Nguyen, Ching-Chun Chang, Saku Sugawara, Isao Echizen*

Main category: cs.CL

TL;DR: The paper addresses the challenge of detecting human involvement in AI-generated text, proposing a BERTScore metric and a multi-task RoBERTa-based regressor for robust participation detection.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated content, especially in academic settings, necessitates better detection methods beyond binary classification to account for varying human involvement.

Method: The authors use BERTScore to measure human involvement and train a multi-task RoBERTa-based regressor on token classification. They create a continuous dataset simulating academic scenarios for evaluation.

Result: Their method outperforms existing detectors, achieving an F1 score of 0.9423 and a regressor mean squared error of 0.004, with some generalizability across generative models.

Conclusion: The proposed approach effectively detects varying levels of human involvement in AI-generated text, addressing limitations of binary classification methods.

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [31] [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/pdf/2506.03510)
*Seungcheol Park, Sojin Lee, Jongjin Kim, Jinsik Lee, Hyunjik Jo, U Kang*

Main category: cs.CL

TL;DR: SPRINT is a sublayer pruning method for LLMs that improves speed without losing accuracy by considering latency reduction and sublayer tunability.


<details>
  <summary>Details</summary>
Motivation: The slow inference speed of LLMs due to stacked sublayers limits their practical use, and existing pruning methods lack accuracy by ignoring sublayer characteristics.

Method: SPRINT selects sublayers to prune based on latency reduction and tunability, iteratively pruning and tuning remaining sublayers.

Result: SPRINT achieves a 23.88%p higher accuracy on zero-shot benchmarks compared to existing methods, offering the best accuracy-speedup trade-off.

Conclusion: SPRINT effectively accelerates LLMs while maintaining accuracy by intelligently pruning and tuning sublayers.

Abstract: How can we accelerate large language models(LLMs) without sacrificing
accuracy? The slow inference speed of LLMs hinders us to benefit from their
remarkable performance in diverse applications. This is mainly because numerous
sublayers are stacked together in LLMs. Sublayer pruning compresses and
expedites LLMs via removing unnecessary sublayers. However, existing sublayer
pruning algorithms are limited in accuracy since they naively select sublayers
to prune, overlooking the different characteristics of each sublayer. In this
paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability
Information), an accurate sublayer pruning method for LLMs. SPRINT accurately
selects a target sublayer to prune by considering 1) the amount of latency
reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively
prunes redundant sublayers and swiftly tunes the parameters of remaining
sublayers. Experiments show that SPRINT achieves the best accuracy-speedup
trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense
reasoning benchmarks compared to existing pruning algorithms.

</details>


### [32] [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/pdf/2506.03519)
*Yangyang Zhao, Ben Niu, Libo Qin, Shihan Wang*

Main category: cs.CL

TL;DR: The paper combines Evolutionary Algorithms (EAs) with Deep Reinforcement Learning (DRL) to balance exploration and exploitation in task-oriented dialogue systems, introducing an elite individual injection mechanism to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: DRL struggles with balancing exploration and exploitation in high-dimensional spaces, leading to poor convergence. EAs offer global search capabilities, but their integration with DRL is challenging due to natural language flexibility.

Method: The study integrates EA's global search with DRL's local optimization and proposes an elite individual injection mechanism to enhance EA's efficiency.

Result: Experiments on four datasets show improved balance between exploration and exploitation, with the elite mechanism reducing exploration time.

Conclusion: The approach successfully integrates EA and DRL, enhancing performance and efficiency in task-oriented dialogue policy optimization.

Abstract: Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue
systems to optimize dialogue policy, but it struggles to balance exploration
and exploitation due to the high dimensionality of state and action spaces.
This challenge often results in local optima or poor convergence. Evolutionary
Algorithms (EAs) have been proven to effectively explore the solution space of
neural networks by maintaining population diversity. Inspired by this, we
innovatively combine the global search capabilities of EA with the local
optimization of DRL to achieve a balance between exploration and exploitation.
Nevertheless, the inherent flexibility of natural language in dialogue tasks
complicates this direct integration, leading to prolonged evolutionary times.
Thus, we further propose an elite individual injection mechanism to enhance
EA's search efficiency by adaptively introducing best-performing individuals
into the population. Experiments across four datasets show that our approach
significantly improves the balance between exploration and exploitation,
boosting performance. Moreover, the effectiveness of the EII mechanism in
reducing exploration time has been demonstrated, achieving an efficient
integration of EA and DRL on task-oriented dialogue policy tasks.

</details>


### [33] [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/pdf/2506.03523)
*Chong Li, Jiajun Zhang, Chengqing Zong*

Main category: cs.CL

TL;DR: TokAlign is an efficient method to align and transfer token-level knowledge between LLMs by replacing vocabularies, improving multilingual text compression and reducing perplexity.


<details>
  <summary>Details</summary>
Motivation: Tokenization inefficiencies in new domains or languages slow down LLM training and hinder knowledge transfer.

Method: TokAlign aligns source to target vocabulary via a mapping matrix, rearranges model parameters, and fine-tunes for the new vocabulary.

Result: Reduces perplexity from 3.4e² to 1.2e², improves compression, and enables efficient token-level distillation (+4.4% over sentence-level).

Conclusion: TokAlign effectively unifies vocabularies, enhances LLM performance, and enables efficient knowledge transfer with minimal training steps.

Abstract: Tokenization serves as a foundational step for Large Language Models (LLMs)
to process text. In new domains or languages, the inefficiency of the tokenizer
will slow down the training and generation of LLM. The mismatch in vocabulary
also hinders deep knowledge transfer between LLMs like token-level
distillation. To mitigate this gap, we propose an efficient method named
TokAlign to replace the vocabulary of LLM from the token co-occurrences view,
and further transfer the token-level knowledge between models. It first aligns
the source vocabulary to the target one by learning a one-to-one mapping matrix
for token IDs. Model parameters, including embeddings, are rearranged and
progressively fine-tuned for the new vocabulary. Our method significantly
improves multilingual text compression rates and vocabulary initialization for
LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods
to 1.2$\text{e}^2$ after initialization. Experimental results on models across
multiple parameter scales demonstrate the effectiveness and generalization of
TokAlign, which costs as few as 5k steps to restore the performance of the
vanilla model. After unifying vocabularies between LLMs, token-level
distillation can remarkably boost (+4.4% than sentence-level distillation) the
base model, costing only 235M tokens.

</details>


### [34] [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/pdf/2506.03524)
*Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Coder is an open-source LLM series (8B size) that minimizes human effort in code pretraining data by using a model-centric pipeline, achieving top performance in code-related tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for code pretraining data rely heavily on human effort, limiting scalability and introducing biases. Seed-Coder aims to reduce human involvement and improve efficiency.

Method: Uses a model-centric pipeline for scoring and filtering code data. Includes supervised fine-tuning, preference optimization, and LongCoT reinforcement learning for reasoning.

Result: Achieves state-of-the-art performance in code generation, completion, editing, reasoning, and software engineering tasks, outperforming similar and larger models.

Conclusion: Seed-Coder demonstrates the effectiveness of minimizing human involvement in data construction while maintaining high performance in diverse code-related tasks.

Abstract: Code data in large language model (LLM) pretraining is recognized crucial not
only for code-related tasks but also for enhancing general intelligence of
LLMs. Current open-source LLMs often heavily rely on human effort to produce
their code pretraining data, such as employing hand-crafted filtering rules
tailored to individual programming languages, or using human-annotated data to
train quality filters. However, these approaches are inherently limited in
scalability, prone to subjective biases, and costly to extend and maintain
across diverse programming languages. To address these challenges, we introduce
Seed-Coder, a series of open-source LLMs comprising base, instruct and
reasoning models of 8B size, minimizing human involvement in data construction.
Our code pretraining data is produced by a model-centric data pipeline, which
predominantly leverages LLMs for scoring and filtering code data. The instruct
model is further trained via supervised fine-tuning and preference
optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)
reinforcement learning to improve multi-step code reasoning. Seed-Coder
achieves state-of-the-art results among open-source models of similar size and
even surpasses some much larger models, demonstrating superior performance in
code generation, code completion, code editing, code reasoning, and software
engineering tasks.

</details>


### [35] [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/pdf/2506.03533)
*Apurva Gandhi, Graham Neubig*

Main category: cs.CL

TL;DR: Go-Browse improves web agent performance by collecting diverse data via structured exploration, achieving better results than GPT-4o mini and state-of-the-art sub-10B models.


<details>
  <summary>Details</summary>
Motivation: Digital agents often struggle with understanding unfamiliar environments, like web browsing.

Method: Go-Browse frames data collection as graph search for efficient exploration, tested on WebArena with 10K trajectories and 40K steps.

Result: Fine-tuning a 7B model on the dataset yields a 21.7% success rate, outperforming benchmarks.

Conclusion: Structured exploration enhances agent performance, demonstrating scalability and effectiveness.

Abstract: One of the fundamental problems in digital agents is their lack of
understanding of their environment. For instance, a web browsing agent may get
lost in unfamiliar websites, uncertain what pages must be visited to achieve
its goals. To address this, we propose Go-Browse, a method for automatically
collecting diverse and realistic web agent data at scale through structured
exploration of web environments. Go-Browse achieves efficient exploration by
framing data collection as a graph search, enabling reuse of information across
exploration episodes. We instantiate our method on the WebArena benchmark,
collecting a dataset of 10K successful task-solving trajectories and 40K
interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on
this dataset achieves a success rate of 21.7% on the WebArena benchmark,
beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for
sub-10B parameter models by 2.9%.

</details>


### [36] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/pdf/2506.03541)
*Xiaofeng Zhou, Heyan Huang, Lizi Liao*

Main category: cs.CL

TL;DR: A novel Debate and Reflect (D&R) framework improves smaller LLMs by facilitating multi-turn debates with teacher models, combined with Tree-structured Direct Preference Optimization (T-DPO) for efficient training.


<details>
  <summary>Details</summary>
Motivation: High computational demands of large LLMs limit adoption; current distillation methods lack lasting performance gains.

Method: D&R framework for debates between smaller and teacher models, plus T-DPO for hierarchical training from debate logs.

Result: Significant accuracy, robustness, and generalization improvements in smaller models, outperforming baselines.

Conclusion: The D&R framework and T-DPO offer an effective solution for enhancing smaller LLMs sustainably.

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [37] [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/pdf/2506.03557)
*Lin Sun, Chuang Liu, Peng Liu, Bingyang Li, Weijia Lu, Ning Wu*

Main category: cs.CL

TL;DR: BPO addresses DPO's DCR issue by balancing chosen and rejected responses, improving accuracy without extra constraints.


<details>
  <summary>Details</summary>
Motivation: DPO neglects absolute reward magnitudes, causing degraded chosen responses (DCR).

Method: BPO introduces balanced reward margin and gap adaptor to dynamically optimize responses.

Result: BPO outperforms DPO and variants, e.g., +10.1% accuracy with Llama-3.1-8B-Instruct.

Conclusion: BPO resolves DCR with minimal code changes, maintaining compatibility with DPO frameworks.

Abstract: Direct Preference Optimization (DPO) have emerged as a popular method for
aligning Large Language Models (LLMs) with human preferences. While DPO
effectively preserves the relative ordering between chosen and rejected
responses through pairwise ranking losses, it often neglects absolute reward
magnitudes. This oversight can decrease the likelihood of chosen responses and
increase the risk of generating out-of-distribution responses, leading to poor
performance. We term this issue Degraded Chosen Responses (DCR).To address this
issue, we propose Balanced Preference Optimization (BPO), a novel framework
that dynamically balances the optimization of chosen and rejected responses
through two key components: balanced reward margin and gap adaptor. Unlike
previous methods, BPO can fundamentally resolve DPO's DCR issue, without
introducing additional constraints to the loss function. Experimental results
on multiple mathematical reasoning tasks show that BPO significantly
outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%
to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses
DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over
Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a
single line of code modification, making it simple to implement and fully
compatible with existing DPO-based frameworks.

</details>


### [38] [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/pdf/2506.03558)
*Jiawei Chen, Xinyan Guan, Qianhao Yuan, Guozhao Mo, Weixiang Zhou, Yaojie Lu, Hongyu Lin, Ben He, Le Sun, Xianpei Han*

Main category: cs.CL

TL;DR: Skeleton-Guided Multi-Turn Dialogue Generation improves multi-turn instruction synthesis by modeling human intent, enhancing coherence and task success.


<details>
  <summary>Details</summary>
Motivation: Addressing the neglect of cross-turn coherence in current instruction data synthesis methods, which leads to context drift and reduced task completion.

Method: A two-stage framework: (1) Intent Modeling for global dialogue structure, and (2) Skeleton Generation to align queries with intent.

Result: Models fine-tuned on ConsistentChat show 20-30% better chat consistency and up to 15% higher task success.

Conclusion: The proposed method significantly outperforms existing datasets in multi-turn dialogue coherence and task completion.

Abstract: Current instruction data synthesis methods primarily focus on single-turn
instructions and often neglect cross-turn coherence, resulting in context drift
and reduced task completion rates in extended conversations. To address this
limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a
framework that constrains multi-turn instruction synthesis by explicitly
modeling human conversational intent. It operates in two stages: (1) Intent
Modeling, which captures the global structure of human dialogues by assigning
each conversation to one of nine well-defined intent trajectories, ensuring a
coherent and goal-oriented information flow; and (2) Skeleton Generation, which
constructs a structurally grounded sequence of user queries aligned with the
modeled intent, thereby serving as a scaffold that constrains and guides the
downstream instruction synthesis process. Based on this process, we construct
ConsistentChat, a multi-turn instruction dataset with approximately 15,000
multi-turn conversations and 224,392 utterances. Experiments on the Light,
Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat
achieve a 20-30% improvement in chat consistency and up to a 15% increase in
task success rate, significantly outperforming models trained on existing
single-turn and multi-turn instruction datasets.

</details>


### [39] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/pdf/2506.03566)
*Langlin Huang, Chengsong Huang, Jixuan Leng, Di Huang, Jiaxin Huang*

Main category: cs.CL

TL;DR: PosS improves LLM inference by using position-specialized draft layers to enhance token prediction accuracy, reducing error accumulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods degrade in draft token prediction quality due to error accumulation in later positions.

Method: Introduces Position Specialists (PosS), multiple draft layers specialized for specific positions to handle feature deviation.

Result: PosS improves token acceptance rate and speed-up ratio on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets.

Conclusion: PosS effectively enhances draft model performance, offering better efficiency and accuracy in LLM inference.

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [40] [MiMo-VL Technical Report](https://arxiv.org/pdf/2506.03569)
*Xiaomi LLM-Core Team, :, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia*

Main category: cs.CL

TL;DR: MiMo-VL-7B-SFT and MiMo-VL-7B-RL are open-source vision-language models excelling in visual understanding and multimodal reasoning, outperforming competitors and setting new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To advance vision-language models by combining high-quality reasoning data and mixed reinforcement learning for superior performance.

Method: Four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse rewards.

Result: Outperforms Qwen2.5-VL-7B on 35/40 tasks, scores 59.4 on OlympiadBench, and 56.1 on OSWorld-G, surpassing specialized models.

Conclusion: The models set new standards, highlighting the value of reasoning data and mixed RL, with open-source resources for reproducibility.

Abstract: We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language
models delivering state-of-the-art performance in both general visual
understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B
on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing
models with up to 78B parameters. For GUI grounding applications, it sets a new
standard with 56.1 on OSWorld-G, even outperforming specialized models such as
UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)
with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward
signals. We identify the importance of incorporating high-quality reasoning
data with long Chain-of-Thought into pre-training stages, and the benefits of
mixed RL despite challenges in simultaneous multi-domain optimization. We also
contribute a comprehensive evaluation suite covering 50+ tasks to promote
reproducibility and advance the field. The model checkpoints and full
evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.

</details>


### [41] [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/pdf/2506.03570)
*Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu*

Main category: cs.CL

TL;DR: FreePRM is a weakly supervised framework for training Process Reward Models (PRMs) without step-level labels, outperforming supervised and open-source PRMs.


<details>
  <summary>Details</summary>
Motivation: Training PRMs typically requires costly step-level labels, which are hard to obtain at scale. FreePRM addresses this by avoiding the need for such labels.

Method: FreePRM generates pseudo step-level labels from final outcomes and uses Buffer Probability to reduce noise in pseudo labeling.

Result: FreePRM achieves a 53.0% F1 score on ProcessBench, outperforming supervised and open-source PRMs by significant margins.

Conclusion: FreePRM offers a cost-effective alternative for PRM training, reducing reliance on step-level annotations while maintaining strong performance.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated that
Process Reward Models (PRMs) play a crucial role in enhancing model
performance. However, training PRMs typically requires step-level labels,
either manually annotated or automatically generated, which can be costly and
difficult to obtain at scale. To address this challenge, we introduce FreePRM,
a weakly supervised framework for training PRMs without access to ground-truth
step-level labels. FreePRM first generates pseudo step-level labels based on
the correctness of final outcome, and then employs Buffer Probability to
eliminate impact of noise inherent in pseudo labeling. Experimental results
show that FreePRM achieves an average F1 score of 53.0% on ProcessBench,
outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared
to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B
(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by
+10.9%. This work introduces a new paradigm in PRM training, significantly
reducing reliance on costly step-level annotations while maintaining strong
performance.

</details>


### [42] [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/pdf/2506.03573)
*Lin Sun, Can Zhang*

Main category: cs.CL

TL;DR: The paper introduces Exchange-of-Perspective (EoP), a framework to enhance LLM performance by exchanging problem perspectives, showing significant improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with fixed problem formulations, limiting their comprehension and performance. EoP aims to overcome this by diversifying problem perspectives.

Method: EoP exchanges perspectives across problem definitions to avoid fixed mindsets. Experiments were conducted on 8 benchmarks using models like GPT-3.5-Turbo and GPT-4.

Result: EoP improved performance: 3.6% on AQuA, 7.7% on Math, and 3.5% on OlympiadBench Maths.

Conclusion: EoP effectively enhances LLM performance by broadening problem comprehension, demonstrating its potential for diverse NLP tasks.

Abstract: Large language models (LLMs) have made significant advancements in addressing
diverse natural language processing (NLP) tasks. However, their performance is
often limited by inherent comprehension of problems. To address this
limitation, we propose Exchange-of-Perspective (EoP), a novel framework
designed to exchange perspectives across different definitions of problem, so
that it can break the fixed mindset from any particular formulation of the
question. We conducted extensive and comprehensive experiments on 8 benchmarks.
The results show that EoP can significantly improve performance. For instance,
compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we
observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP
demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a
3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using
Qwen-2.5-72b.

</details>


### [43] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/pdf/2506.03576)
*Zirui Chen, Xin Wang, Zhao Li, Wenbin Guo, Dongxiao He*

Main category: cs.CL

TL;DR: KG-BiLM is a bidirectional LM framework that unifies knowledge graphs and language models for richer semantic understanding, outperforming baselines in link prediction.


<details>
  <summary>Details</summary>
Motivation: Existing approaches prioritize either graph structure or textual semantics, lacking a unified framework for global KG connectivity, nuanced linguistic context, and reasoning semantics.

Method: KG-BiLM integrates Bidirectional Knowledge Attention, Knowledge-Masked Prediction, and Contrastive Graph Semantic Aggregation to fuse KG structure with LM semantics.

Result: KG-BiLM outperforms baselines in link prediction, especially on large-scale graphs with complex multi-hop relations.

Conclusion: KG-BiLM effectively unifies structural information and textual semantics, bridging the gap between KGs and LMs.

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


### [44] [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/pdf/2506.03580)
*Enrico Benedetti, Akiko Aizawa, Florian Boudin*

Main category: cs.CL

TL;DR: The study explores using Pre-trained Language Models (PLMs) to generate or retrieve example sentences for L2 Japanese learners, finding retrieval methods preferred over generative ones despite rating disagreements.


<details>
  <summary>Details</summary>
Motivation: To enhance language learning by providing diverse, proficiency-aligned example sentences using PLMs.

Method: PLMs are used for quality scoring in a retrieval system and as direct sentence generators via zero-shot learning. Sentences are evaluated for difficulty, diversity, and naturalness by learners, native speakers, and GPT-4.

Result: Retrieval approach was preferred, especially for beginner and advanced learners, while generative methods scored lower. Disagreement among raters was noted except for difficulty.

Conclusion: PLMs show promise for improving sentence suggestion systems in language learning, with retrieval methods being more effective.

Abstract: Providing example sentences that are diverse and aligned with learners'
proficiency levels is essential for fostering effective language acquisition.
This study examines the use of Pre-trained Language Models (PLMs) to produce
example sentences targeting L2 Japanese learners. We utilize PLMs in two ways:
as quality scoring components in a retrieval system that draws from a newly
curated corpus of Japanese sentences, and as direct sentence generators using
zero-shot learning. We evaluate the quality of sentences by considering
multiple aspects such as difficulty, diversity, and naturalness, with a panel
of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our
findings suggest that there is inherent disagreement among participants on the
ratings of sentence qualities, except for difficulty. Despite that, the
retrieval approach was preferred by all evaluators, especially for beginner and
advanced target proficiency, while the generative approaches received lower
scores on average. Even so, our experiments highlight the potential for using
PLMs to enhance the adaptability of sentence suggestion systems and therefore
improve the language learning journey.

</details>


### [45] [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/pdf/2506.03592)
*Viktor Hangya, Fabian Küch, Darina Gold*

Main category: cs.CL

TL;DR: The paper proposes reformulating NLG tasks into cheaper NLU formats to reduce evaluation time for LLM training, showing strong correlation and significant time savings.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational burden of evaluating LLM capabilities like reasoning and code generation during training by using cheaper NLU alternatives.

Method: Reformulate generative (NLG) tasks into NLU formats and test performance correlation across 8 LMs and 4 capabilities.

Result: Strong correlation between original and reformulated tasks, with over 35x average reduction in evaluation time.

Conclusion: Cheaper NLU alternatives can effectively assess LLM capabilities during training, enabling more efficient monitoring.

Abstract: Iterative evaluation of LLMs during training is essential to ensure expected
capability development, but can be time- and compute-intensive. While NLU
tasks, where the model selects from fixed answer choices, are cheap to
evaluate, essential capabilities like reasoning and code generation rely on the
more time-consuming NLG (token-by-token generation) format. In this work, our
aim is to decrease the computational burden of NLG benchmarks in order to
enable monitoring crucial LLM capabilities during model training. We
reformulate generative tasks into computationally cheaper NLU alternatives. We
test the performance correlation between the original and reformulated tasks
using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code
generation, factual knowledge and reading comprehension. Our results show a
strong correlation between task formats, supporting capability assessment via
cheaper alternatives and achieving over 35x average reduction in evaluation
time. We plan to publish our benchmark adaptions.

</details>


### [46] [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/pdf/2506.03593)
*Ray Groshan, Michael Ginn, Alexis Palmer*

Main category: cs.CL

TL;DR: The paper compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages, finding that the latter is beneficial only when synthetic data aligns closely with the training distribution.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty about whether linguistically-motivated data augmentation outperforms naive methods, given the lack of systematic empirical comparisons.

Method: A comprehensive comparison of augmentation strategies for Uspanteko and Arapaho, evaluated on machine translation and interlinear glossing tasks.

Result: Linguistically-motivated strategies outperform naive ones only when synthetic data closely matches the training distribution.

Conclusion: Linguistically-motivated augmentation is beneficial but depends on alignment with the training data distribution.

Abstract: Data augmentation, a widely-employed technique for addressing data scarcity,
involves generating synthetic data examples which are then used to augment
available training data. Researchers have seen surprising success from simple
methods, such as random perturbations from natural examples, where models seem
to benefit even from data with nonsense words, or data that doesn't conform to
the rules of the language. A second line of research produces synthetic data
that does in fact follow all linguistic constraints; these methods require some
linguistic expertise and are generally more challenging to implement. No
previous work has done a systematic, empirical comparison of both
linguistically-naive and linguistically-motivated data augmentation strategies,
leaving uncertainty about whether the additional time and effort of
linguistically-motivated data augmentation work in fact yields better
downstream performance.
  In this work, we conduct a careful and comprehensive comparison of
augmentation strategies (both linguistically-naive and
linguistically-motivated) for two low-resource languages with different
morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness
of many different strategies and their combinations across two important
sequence-to-sequence tasks for low-resource languages: machine translation and
interlinear glossing. We find that linguistically-motivated strategies can have
benefits over naive approaches, but only when the new examples they produce are
not significantly unlike the training data distribution.

</details>


### [47] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/pdf/2506.03598)
*Zetong Tang, Qian Ma, Di Wu*

Main category: cs.CL

TL;DR: AP-SQL bridges resource-efficient small models and powerful large models for Text-to-SQL, using schema filtering, retrieval-augmented generation, and prompt-driven methods with CoT/GoT templates.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of using resource-intensive models in constrained environments for Text-to-SQL tasks.

Method: Decomposes the task into schema filtering, retrieval-augmented generation, and prompt-driven schema linking/SQL generation, with fine-tuned LLMs and CoT/GoT prompts.

Result: Effective performance demonstrated on Spider benchmarks.

Conclusion: AP-SQL successfully balances efficiency and capability for Text-to-SQL translation.

Abstract: Using the best Text-to-SQL methods in resource-constrained environments is
challenging due to their reliance on resource-intensive open-source models.
This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to
bridge the gap between resource-efficient small open-source models and the
powerful capabilities of large closed-source models for Text-to-SQL
translation. Our method decomposes the task into schema filtering,
retrieval-augmented text-to-SQL generation based on in-context examples, and
prompt-driven schema linking and SQL generation. To improve schema selection
accuracy, we fine-tune large language models. Crucially, we also explore the
impact of prompt engineering throughout the process, leveraging
Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly
enhance the model's reasoning for accurate SQL generation. Comprehensive
evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [48] [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/pdf/2506.03616)
*Eunki Kim, Sangryul Kim, James Thorne*

Main category: cs.CL

TL;DR: The paper introduces Dynamic Inserting Tokens Training (DIT), a method to enhance reasoning in LLMs by strategically inserting [PAUSE] tokens where model confidence is low, improving performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning in LLMs by dynamically identifying and addressing low-confidence positions in sequences, moving beyond heuristic token insertion.

Method: DIT inserts [PAUSE] tokens at positions with lowest token log-likelihood, enhancing predictive capabilities for subsequent tokens.

Result: DIT outperforms traditional fine-tuning and prior token insertion methods, achieving accuracy gains up to 4.7%p on GSM8K and 3.4%p pass@1 on MBPP.

Conclusion: DIT offers a dynamic, model-based approach to reasoning enhancement, expanding research possibilities beyond heuristic methods.

Abstract: To enhance reasoning capabilities, previous works have explored incorporating
special-purpose tokens into the training process. These strategies strengthen
the learning mechanism of transformer-based large language models (LLMs).
Building on prior research, in which inserting dummy tokens consecutively just
before reasoning steps can enhance effectiveness, we introduce a novel approach
termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions
within sequences where model confidence is lowest according to token
log-likelihood. Strategically inserting [PAUSE] tokens on these positions
bolsters the model's predictive capabilities for subsequent tokens.
Experimental results across diverse datasets and models, from the 2.7B model to
the 8B model, demonstrate that DIT consistently outperforms traditional
fine-tuning and previous token insertion methods. With this simple yet
effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on
AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work
shows a model-based, dynamic approach rather than a heuristic one, thereby
broadening the scope of research in reasoning.

</details>


### [49] [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/pdf/2506.03619)
*Ayuto Tsutsumi, Yuu Jinnai*

Main category: cs.CL

TL;DR: The paper evaluates cultural awareness in LLMs using Japanese folktales (Yokai) and introduces YokaiEval, a benchmark dataset. Results show Japanese-trained models outperform English-centric ones.


<details>
  <summary>Details</summary>
Motivation: LLMs often lack cultural knowledge beyond English-speaking communities, marginalizing non-English cultures. The study aims to assess and improve cultural awareness in LLMs.

Method: The study introduces YokaiEval, a dataset of 809 multiple-choice questions about Yokai, and evaluates 31 Japanese and multilingual LLMs.

Result: Japanese-trained models, especially those based on Llama-3, achieve higher accuracy than English-centric models.

Conclusion: The study highlights the need for culturally diverse training in LLMs and demonstrates the effectiveness of language-specific pretraining.

Abstract: Although Large Language Models (LLMs) have demonstrated strong language
understanding and generation abilities across various languages, their cultural
knowledge is often limited to English-speaking communities, which can
marginalize the cultures of non-English communities. To address the problem,
evaluation of the cultural awareness of the LLMs and the methods to develop
culturally aware LLMs have been investigated. In this study, we focus on
evaluating knowledge of folktales, a key medium for conveying and circulating
culture. In particular, we focus on Japanese folktales, specifically on
knowledge of Yokai. Yokai are supernatural creatures originating from Japanese
folktales that continue to be popular motifs in art and entertainment today.
Yokai have long served as a medium for cultural expression, making them an
ideal subject for assessing the cultural awareness of LLMs. We introduce
YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions
(each with four options) designed to probe knowledge about yokai. We evaluate
the performance of 31 Japanese and multilingual LLMs on this dataset. The
results show that models trained with Japanese language resources achieve
higher accuracy than English-centric models, with those that underwent
continued pretraining in Japanese, particularly those based on Llama-3,
performing especially well. The code and dataset are available at
https://github.com/CyberAgentA ILab/YokaiEval.

</details>


### [50] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/pdf/2506.03627)
*Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, Yiwen Zhang*

Main category: cs.CL

TL;DR: RoP is a novel prompting strategy to enhance LLM robustness against input perturbations, combining error correction and guidance stages for improved performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to input perturbations, degrading performance, but existing prompting strategies lack explicit mitigation for such issues.

Method: RoP involves two stages: Error Correction (generating adversarial examples for prompt correction) and Guidance (optimal prompting for robust inference).

Result: RoP significantly improves LLM robustness across tasks, maintaining accuracy with minimal degradation compared to clean inputs.

Conclusion: RoP is a practical and effective method for enhancing LLM robustness in real-world applications.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks by effectively utilizing a prompting strategy. However, they are
highly sensitive to input perturbations, such as typographical errors or slight
character order errors, which can substantially degrade their performance.
Despite advances in prompting techniques, developing a prompting strategy that
explicitly mitigates the negative impact of such perturbations remains an open
challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a
novel prompting strategy specifically designed to enhance the robustness of
LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error
Correction stage, RoP applies diverse perturbation methods to generate
adversarial examples, which are then used to construct prompts that
automatically correct input errors. In the Guidance stage, RoP generates an
optimal guidance prompting based on the corrected input, steering the model
toward more robust and accurate inferences. Through comprehensive experiments
spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate
that RoP significantly improves LLMs' robustness against adversarial
perturbations. Notably, it maintains model accuracy with only minimal
degradation compared to clean input scenarios, thereby establishing RoP as a
practical and effective approach for enhancing LLM robustness in real-world
applications.

</details>


### [51] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/pdf/2506.03637)
*Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, Wei Ye*

Main category: cs.CL

TL;DR: The paper introduces RewardAnything, a generalizable reward model that follows natural language principles, addressing the rigidity and bias of traditional reward models trained on fixed datasets.


<details>
  <summary>Details</summary>
Motivation: Current reward models are inflexible and resource-intensive, limiting their adaptability to diverse real-world tasks. The goal is to create a model that can dynamically follow natural language specifications for broader application.

Method: The authors propose RewardAnything, a reward model designed to adhere to natural language principles, and introduce RABench to evaluate generalization across diverse principles.

Result: RewardAnything achieves state-of-the-art performance on traditional benchmarks and excels in adapting to new principles without retraining.

Conclusion: RewardAnything offers a flexible, efficient solution for aligning large language models using natural language principles, demonstrating superior generalization and practical integration with existing methods.

Abstract: Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.

</details>


### [52] [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/pdf/2506.03659)
*Yinuo Wang, Robert E. Mercer, Frank Rudzicz, Sudipta Singha Roy, Pengjie Ren, Zhumin Chen, Xindi Wang*

Main category: cs.CL

TL;DR: This survey explores trustworthiness in medical QA systems, focusing on six key dimensions (Factuality, Robustness, Fairness, Safety, Explainability, Calibration) and their evaluation in LLM-based systems. It reviews benchmarks, improvement techniques, and identifies open challenges for future research.


<details>
  <summary>Details</summary>
Motivation: Ensuring trustworthiness in medical QA systems is critical for patient safety and clinical effectiveness, especially as LLMs are increasingly used in healthcare.

Method: The survey systematically examines six trustworthiness dimensions in medical QA, reviews evaluation benchmarks, and analyzes techniques like retrieval-augmented grounding and adversarial fine-tuning.

Result: The study compiles benchmarks and techniques for improving trustworthiness but highlights challenges like scalable expert evaluation and real-world deployment.

Conclusion: Future research should address open challenges to advance the safe and reliable deployment of LLM-powered medical QA systems.

Abstract: Trustworthiness in healthcare question-answering (QA) systems is important
for ensuring patient safety, clinical effectiveness, and user confidence. As
large language models (LLMs) become increasingly integrated into medical
settings, the reliability of their responses directly influences clinical
decision-making and patient outcomes. However, achieving comprehensive
trustworthiness in medical QA poses significant challenges due to the inherent
complexity of healthcare data, the critical nature of clinical scenarios, and
the multifaceted dimensions of trustworthy AI. In this survey, we
systematically examine six key dimensions of trustworthiness in medical QA,
i.e., Factuality, Robustness, Fairness, Safety, Explainability, and
Calibration. We review how each dimension is evaluated in existing LLM-based
medical QA systems. We compile and compare major benchmarks designed to assess
these dimensions and analyze evaluation-guided techniques that drive model
improvements, such as retrieval-augmented grounding, adversarial fine-tuning,
and safety alignment. Finally, we identify open challenges-such as scalable
expert evaluation, integrated multi-dimensional metrics, and real-world
deployment studies-and propose future research directions to advance the safe,
reliable, and transparent deployment of LLM-powered medical QA.

</details>


### [53] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/pdf/2506.03665)
*Hernán Maina, Guido Ivetta, Mateo Lione Stuto, Julian Martin Eisenschlos, Jorge Sánchez, Luciana Benotti*

Main category: cs.CL

TL;DR: ROSA, a decoding strategy, improves VQA performance for misaligned text in images, addressing challenges faced by visually impaired users.


<details>
  <summary>Details</summary>
Motivation: Current VQA systems struggle with text recognition in photos taken by visually impaired individuals due to common framing issues.

Method: Introduced ROtated SAmpling (ROSA), a decoding strategy to handle incorrectly oriented text in images.

Result: ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.

Conclusion: ROSA effectively addresses the gap in VQA benchmarks for text-rich images with misaligned text.

Abstract: Visually impaired people could benefit from Visual Question Answering (VQA)
systems to interpret text in their surroundings. However, current models often
struggle with recognizing text in the photos taken by this population. Through
in-depth interviews with visually impaired individuals, we identified common
framing conventions that frequently result in misaligned text. Existing VQA
benchmarks primarily feature well-oriented text captured by sighted users,
under-representing these challenges. To address this gap, we introduce ROtated
SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich
images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7
absolute points in the best-performing model.

</details>


### [54] [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/pdf/2506.03690)
*Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang*

Main category: cs.CL

TL;DR: The paper introduces γ-PO, a dynamic target margin preference optimization algorithm for aligning LLMs, improving performance by 4.4% over baselines while being resource-efficient.


<details>
  <summary>Details</summary>
Motivation: Alignment of LLMs is critical for safety and reliability, but existing methods like DPO are sensitive to noisy data. γ-PO addresses this by dynamically adjusting reward margins.

Method: γ-PO introduces instance-specific margin calibration to prioritize high-confidence pairs and suppress noise, remaining compatible with DPO variants.

Result: γ-PO achieves a 4.4% average improvement on benchmarks like AlpacaEval2 and Arena-Hard, with minimal impact on training efficiency.

Conclusion: γ-PO is a robust, plug-and-play solution for enhancing LLM alignment, offering better performance and noise resilience.

Abstract: The alignment of Large Language Models (LLMs) is crucial for ensuring their
safety and reliability in practical applications. Direct Preference
Optimization (DPO) has emerged as an efficient method that directly optimizes
models using preference pairs, significantly reducing resource demands.
However, the effectiveness of DPO heavily depends on the data quality, which is
frequently compromised by noise. In this work, we propose $\gamma$-PO, a
dynamic target margin preference optimization algorithm that adjust reward
margins at the pairwise level. By introducing instance-specific margin
calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those
demonstrating higher reward margins) while suppressing potential noise from
ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible
with variants of DPO that rely on reward margin between preference pairs.
Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an
average 4.4\% improvement over other baselines, setting new benchmarks for
state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code
changes and has a negligible impact on training efficiency, making it a robust
solution for enhancing LLMs alignment. Our codes are available at
\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

</details>


### [55] [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/pdf/2506.03700)
*Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng*

Main category: cs.CL

TL;DR: AdaDecode accelerates LLM decoding by generating tokens at intermediate layers when confidence is high, achieving up to 1.73x speedup without auxiliary models or output discrepancies.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in LLMs is inefficient due to sequential token generation, limiting hardware parallelism. Existing methods like speculative decoding and layer skipping have drawbacks.

Method: AdaDecode adaptively generates tokens at intermediate layers when confidence is high, defers remaining computations, and verifies outputs for consistency.

Result: AdaDecode achieves up to 1.73x speedup in decoding throughput while maintaining output parity with standard autoregressive decoding.

Conclusion: AdaDecode offers an efficient, hardware-friendly solution for LLM decoding without compromising output quality.

Abstract: Large language models (LLMs) are increasingly used for long-content
generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency
becomes a critical bottleneck: Autoregressive decoding is inherently limited by
its sequential token generation process, where each token must be generated
before the next can be processed. This sequential dependency restricts the
ability to fully leverage modern hardware's parallel processing capabilities.
Existing methods like speculative decoding and layer skipping offer potential
speedups but have notable drawbacks: speculative decoding relies on an
auxiliary "drafter" model, which can be challenging to acquire and increases
memory overhead, while layer skipping may introduce discrepancies in the
outputs due to the missing key-value cache at skipped layers. In this work, we
propose AdaDecode, which accelerates LLM decoding without requiring auxiliary
models or changes to the original model parameters, while ensuring output
consistency. AdaDecode leverages the insight that many tokens can accurately be
generated at intermediate layers, as further layers often do not significantly
alter predictions once the model reaches a certain confidence. By adaptively
generating tokens at intermediate layers when confidence is high, AdaDecode
enables the next token's computation to begin immediately. The remaining layer
computations for early-predicted tokens are deferred and executed in parallel
with subsequent tokens when needed, maximizing hardware utilization and
reducing decoding latency. A final verification step ensures that early
predictions match the results of standard autoregressive decoding, preserving
output parity. Experiments across diverse generation tasks shows that AdaDecode
consistently achieves superior decoding throughput with up to 1.73x speedup,
while guaranteeing output parity with standard autoregressive decoding.

</details>


### [56] [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/pdf/2506.03704)
*Pei-Yun Lin, Yen-lung Tsai*

Main category: cs.CL

TL;DR: ScoreRAG improves automated news generation by combining retrieval-augmented generation, consistency scoring, and structured summarization to reduce hallucinations and enhance quality.


<details>
  <summary>Details</summary>
Motivation: Current news generation methods suffer from hallucinations, factual inconsistencies, and lack of domain expertise. ScoreRAG aims to address these issues.

Method: A multi-stage framework involving retrieval, consistency scoring, reranking, and structured summarization to guide news article generation.

Result: ScoreRAG enhances accuracy, coherence, informativeness, and professionalism in generated news articles.

Conclusion: ScoreRAG provides a robust solution for high-quality automated news generation, with code and demo available.

Abstract: This research introduces ScoreRAG, an approach to enhance the quality of
automated news generation. Despite advancements in Natural Language Processing
and large language models, current news generation methods often struggle with
hallucinations, factual inconsistencies, and lack of domain-specific expertise
when producing news articles. ScoreRAG addresses these challenges through a
multi-stage framework combining retrieval-augmented generation, consistency
relevance evaluation, and structured summarization. The system first retrieves
relevant news documents from a vector database, maps them to complete news
items, and assigns consistency relevance scores based on large language model
evaluations. These documents are then reranked according to relevance, with
low-quality items filtered out. The framework proceeds to generate graded
summaries based on relevance scores, which guide the large language model in
producing complete news articles following professional journalistic standards.
Through this methodical approach, ScoreRAG aims to significantly improve the
accuracy, coherence, informativeness, and professionalism of generated news
articles while maintaining stability and consistency throughout the generation
process. The code and demo are available at:
https://github.com/peiyun2260/ScoreRAG.

</details>


### [57] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/pdf/2506.03723)
*Chaeyun Jang, Moonseok Choi, Yegon Kim, Hyungi Lee, Juho Lee*

Main category: cs.CL

TL;DR: Fine-tuning LLMs with scalar confidence labels elicits self-verification behavior, improving calibration and accuracy in CoT reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of research on confidence calibration for chain-of-thought reasoning in LLMs, aiming for safer deployment.

Method: Supervised fine-tuning with scalar confidence labels and a rethinking method for test-time scaling.

Result: Improved calibration, accuracy, and interpretability on tasks like GSM8K, MATH-500, and ARC-Challenge.

Conclusion: Confidence-aware fine-tuning enhances LLM performance and aligns reasoning with confidence, without explicit supervision.

Abstract: Uncertainty calibration is essential for the safe deployment of large
language models (LLMs), particularly when users rely on verbalized confidence
estimates. While prior work has focused on classifiers or short-form
generation, confidence calibration for chain-of-thought (CoT) reasoning remains
largely unexplored. Surprisingly, we find that supervised fine-tuning with
scalar confidence labels alone suffices to elicit self-verification behavior of
language models, without any explicit reasoning supervision or reinforcement
learning-based rewards. Despite being trained only to produce a verbalized
confidence score without any self-verifying examples, the model learns to
generate longer and self-checking responses for low-confidence queries while
providing more concise answers for high-confidence ones. We further propose a
simple rethinking method that boosts performance via test-time scaling based on
calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such
as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning
improves both calibration and accuracy, while also enhancing interpretability
by aligning the model's reasoning path with its confidence.

</details>


### [58] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/pdf/2506.03735)
*Junling Wang, Anna Rutkiewicz, April Yi Wang, Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Math2Visual automates the creation of pedagogically meaningful visuals for math word problems, improving educational content generation.


<details>
  <summary>Details</summary>
Motivation: Manual creation of visuals for math word problems is labor-intensive, and automated methods are lacking.

Method: Math2Visual uses a pre-defined visual language and teacher-informed design space to generate visuals, and evaluates/fine-tunes Text-to-Image models.

Result: A dataset of 1,903 visuals was created, and fine-tuned TTI models showed improved educational visual generation.

Conclusion: Math2Visual sets a benchmark for automated, pedagogically meaningful visuals and highlights challenges in multimodal educational content.

Abstract: Visuals are valuable tools for teaching math word problems (MWPs), helping
young learners interpret textual descriptions into mathematical expressions
before solving them. However, creating such visuals is labor-intensive and
there is a lack of automated methods to support this process. In this paper, we
present Math2Visual, an automatic framework for generating pedagogically
meaningful visuals from MWP text descriptions. Math2Visual leverages a
pre-defined visual language and a design space grounded in interviews with math
teachers, to illustrate the core mathematical relationships in MWPs. Using
Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate
Text-to-Image (TTI) models for their ability to generate visuals that align
with our design. We further fine-tune several TTI models with our dataset,
demonstrating improvements in educational visual generation. Our work
establishes a new benchmark for automated generation of pedagogically
meaningful visuals and offers insights into key challenges in producing
multimodal educational content, such as the misrepresentation of mathematical
relationships and the omission of essential visual elements.

</details>


### [59] [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/pdf/2506.03761)
*Hongcheng Guo, Zheyong Xie, Shaosheng Cao, Boyang Wang, Weiting Liu, Zheyu Ye, Zhoujun Li, Zuozhu Liu*

Main category: cs.CL

TL;DR: Pet-Bench is a benchmark for evaluating LLMs in virtual pet companionship, focusing on self-evolution and interactive engagement, with 7,500+ tasks. Testing 28 LLMs shows performance varies by model size and capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic benchmarking for LLMs in virtual pet companionship, aiming for more realistic and emotionally rich interactions.

Method: Introduces Pet-Bench, a benchmark with tasks like intelligent scheduling, memory-based dialogues, and psychological conversations.

Result: Evaluation of 28 LLMs reveals performance variations tied to model size and capabilities, highlighting the need for specialized optimization.

Conclusion: Pet-Bench provides a foundational tool for advancing emotionally immersive human-pet interactions and benchmarking LLM abilities in this domain.

Abstract: As interest in using Large Language Models (LLMs) for interactive and
emotionally rich experiences grows, virtual pet companionship emerges as a
novel yet underexplored application. Existing approaches focus on basic pet
role-playing interactions without systematically benchmarking LLMs for
comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated
benchmark that evaluates LLMs across both self-interaction and
human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes
self-evolution and developmental behaviors alongside interactive engagement,
offering a more realistic reflection of pet companionship. It features diverse
tasks such as intelligent scheduling, memory-based dialogues, and psychological
conversations, with over 7,500 interaction instances designed to simulate
complex pet behaviors. Evaluation of 28 LLMs reveals significant performance
variations linked to model size and inherent capabilities, underscoring the
need for specialized optimization in this domain. Pet-Bench serves as a
foundational resource for benchmarking pet-related LLM abilities and advancing
emotionally immersive human-pet interactions.

</details>


### [60] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/pdf/2506.03762)
*Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu*

Main category: cs.CL

TL;DR: AhaKV reduces bias in KV cache eviction by adaptively tuning softmax scale, improving global context retention in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current KV cache eviction methods are biased, favoring initial tokens and limiting global context access.

Method: Proposes AhaKV, which adjusts softmax scale based on attention entropy and uses value vectors for refined scoring.

Result: AhaKV mitigates bias, retains crucial tokens globally, and achieves SOTA results on benchmarks.

Conclusion: AhaKV effectively addresses KV cache bias, enhancing LLM inference efficiency and performance.

Abstract: Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) KV cache consumes a lot of memory during inference. While several
works propose reducing the KV cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
AhaKV utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed AhaKV on different models with a
fixed cache budget. Experiments show that AhaKV successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.

</details>


### [61] [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/pdf/2506.03763)
*Quang Hieu Pham, Thuy Duong Nguyen, Tung Pham, Anh Tuan Luu, Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: ClozeMath, a new fine-tuning approach for LLMs, improves mathematical reasoning by predicting masked equations from solutions, outperforming baselines like Masked Thought.


<details>
  <summary>Details</summary>
Motivation: Current LLM training on next-word prediction may not fully mimic human learning, especially in mathematical reasoning.

Method: ClozeMath uses a text-infilling task to predict masked equations from given solutions, akin to cloze exercises.

Result: ClozeMath outperforms Masked Thought on GSM8K, MATH, and GSM-Symbolic datasets, with Beam Search and Chain-of-Thought decoding enhancing robustness.

Conclusion: ClozeMath is a promising approach for enhancing LLMs' mathematical reasoning, validated by ablation studies on architectural choices.

Abstract: The capabilities of large language models (LLMs) have been enhanced by
training on data that reflects human thought processes, such as the
Chain-of-Thought format. However, evidence suggests that the conventional
scheme of next-word prediction may not fully capture how humans learn to think.
Inspired by how humans generalize mathematical reasoning, we propose a new
approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our
ClozeMath involves a text-infilling task that predicts masked equations from a
given solution, analogous to cloze exercises used in human learning.
Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the
strong baseline Masked Thought in performance and robustness, with two
test-time scaling decoding algorithms, Beam Search and Chain-of-Thought
decoding. Additionally, we conduct an ablation study to analyze the effects of
various architectural and implementation choices on our approach.

</details>


### [62] [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/pdf/2506.03781)
*Seungcheol Park, Jeongin Bae, Beomseok Kwon, Minjun Kim, Byeongwook Kim, Se Jung Kwon, U Kang, Dongsoo Lee*

Main category: cs.CL

TL;DR: UniQuanF unifies BCQ and UQ for efficient LLM quantization, achieving higher accuracy without extra deployment costs.


<details>
  <summary>Details</summary>
Motivation: Current quantization schemes (BCQ and UQ) lack combined expressiveness and optimizability, limiting LLM deployment efficiency.

Method: UniQuanF integrates flexible mapping (UQ) and non-uniform levels (BCQ), with unified initialization and optimization techniques.

Result: UniQuanF outperforms BCQ and UQ, achieving up to 4.60% higher accuracy on GSM8K.

Conclusion: UniQuanF provides a superior, cost-free quantization solution for LLMs by unifying BCQ and UQ strengths.

Abstract: How can we quantize large language models while preserving accuracy?
Quantization is essential for deploying large language models (LLMs)
efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are
promising quantization schemes that have strong expressiveness and
optimizability, respectively. However, neither scheme leverages both
advantages. In this paper, we propose UniQuanF (Unified Quantization with
Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses
both strong expressiveness and optimizability by unifying the flexible mapping
technique in UQ and non-uniform quantization levels of BCQ. We propose unified
initialization, and local and periodic mapping techniques to optimize the
parameters in UniQuanF precisely. After optimization, our unification theorem
removes computational and memory overhead, allowing us to utilize the superior
accuracy of UniQuanF without extra deployment costs induced by the unification.
Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ
methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.

</details>


### [63] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/pdf/2506.03785)
*Isik Baran Sandan, Tu Anh Dinh, Jan Niehues*

Main category: cs.CL

TL;DR: Knockout Assessment improves LLM-as-a-Judge accuracy by using iterative pairwise comparisons in a knockout tournament system.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-Judge methods lack a global ranking perspective due to reliance on individual or single-round pairwise assessments.

Method: Proposes Knockout Assessment, a tournament-based iterative pairwise comparison method for LLMs.

Result: Improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average.

Conclusion: Knockout Assessment aligns LLM evaluations more closely with human scoring.

Abstract: Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.

</details>


### [64] [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/pdf/2506.03793)
*Sidharth Pulipaka, Sparsh Jain, Ashwin Sankar, Raj Dabre*

Main category: cs.CL

TL;DR: Cadence, a punctuation restoration model, outperforms state-of-the-art methods for multilingual and spontaneous speech transcripts, addressing challenges in NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with punctuation restoration in spontaneous speech, impacting downstream NLP tasks like translation and summarization.

Method: Cadence, adapted from a pretrained large language model, handles clean text and spontaneous speech, supporting 22 Indian languages and English.

Result: Cadence surpasses previous models in performance, though challenges remain with domain shift and rare punctuation marks.

Conclusion: Pretrained language models are effective for multilingual punctuation restoration, with Cadence offering practical value for low-resource NLP pipelines.

Abstract: Punctuation plays a vital role in structuring meaning, yet current models
often struggle to restore it accurately in transcripts of spontaneous speech,
especially in the presence of disfluencies such as false starts and
backtracking. These limitations hinder the performance of downstream tasks like
translation, text to speech, summarization, etc. where sentence boundaries are
critical for preserving quality. In this work, we introduce Cadence, a
generalist punctuation restoration model adapted from a pretrained large
language model. Cadence is designed to handle both clean written text and
highly spontaneous spoken transcripts. It surpasses the previous state of the
art in performance while expanding support from 14 to all 22 Indian languages
and English. We conduct a comprehensive analysis of model behavior across
punctuation types and language families, identifying persistent challenges
under domain shift and with rare punctuation marks. Our findings demonstrate
the efficacy of utilizing pretrained language models for multilingual
punctuation restoration and highlight Cadence practical value for low resource
NLP pipelines at scale.

</details>


### [65] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/pdf/2506.03827)
*Zhenhui Liu, Chunyuan Yuan, Ming Pang, Zheng Fang, Li Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao*

Main category: cs.CL

TL;DR: The paper introduces MoBGM, a multi-objective model for query rewriting in e-commerce search ads, improving relevance, authenticity, and revenue.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in existing query rewriting methods that fail to optimize relevance, authenticity, and revenue simultaneously.

Method: Proposes MoBGM with a discriminator, generator, and preference alignment module to align and optimize multiple objectives.

Result: Outperforms state-of-the-art methods in offline and online tests, creating significant commercial value.

Conclusion: MoBGM is feasible, robust, and effective for enhancing search ad retrieval systems.

Abstract: Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.

</details>


### [66] [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/pdf/2506.03820)
*Ahmad Mustapha Wali, Sergiu Nisioi*

Main category: cs.CL

TL;DR: The paper introduces a method to correct writing anomalies in Hausa texts using transformer-based models, achieving improved NLP metrics.


<details>
  <summary>Details</summary>
Motivation: Hausa texts often contain writing errors that hinder NLP applications, necessitating automated correction solutions.

Method: The approach involves fine-tuning transformer models (M2M100, AfriTEVA, mBART, Opus-MT) on a synthetic parallel dataset of 450,000 noisy-clean Hausa sentences.

Result: Experiments show significant improvements in F1, BLEU, METEOR scores, and reductions in CER and WER.

Conclusion: The study provides a robust methodology, dataset, and models to enhance Hausa text quality, benefiting NLP for low-resource languages.

Abstract: Hausa texts are often characterized by writing anomalies such as incorrect
character substitutions and spacing errors, which sometimes hinder natural
language processing (NLP) applications. This paper presents an approach to
automatically correct the anomalies by finetuning transformer-based models.
Using a corpus gathered from several public sources, we created a large-scale
parallel dataset of over 450,000 noisy-clean Hausa sentence pairs by
introducing synthetically generated noise, fine-tuned to mimic realistic
writing errors. Moreover, we adapted several multilingual and African
language-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT
variants for this correction task using SentencePiece tokenization. Our
experimental results demonstrate significant increases in F1, BLEU and METEOR
scores, as well as reductions in Character Error Rate (CER) and Word Error Rate
(WER). This research provides a robust methodology, a publicly available
dataset, and effective models to improve Hausa text quality, thereby advancing
NLP capabilities for the language and offering transferable insights for other
low-resource languages.

</details>


### [67] [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/pdf/2506.03822)
*Fabian Karl, Ansgar Scherp*

Main category: cs.CL

TL;DR: CRAWLDoc is a method for contextual ranking of linked web documents to improve metadata extraction from diverse web sources.


<details>
  <summary>Details</summary>
Motivation: Challenges in metadata extraction due to varying web layouts and data formats.

Method: CRAWLDoc retrieves and embeds linked resources (PDFs, ORCID profiles, etc.) into a unified representation for ranking.

Result: Demonstrates robust, layout-independent ranking on a manually labeled dataset of 600 publications.

Conclusion: CRAWLDoc improves metadata extraction across diverse web layouts and formats; code and dataset are publicly available.

Abstract: Publication databases rely on accurate metadata extraction from diverse web
sources, yet variations in web layouts and data formats present challenges for
metadata providers. This paper introduces CRAWLDoc, a new method for contextual
ranking of linked web documents. Starting with a publication's URL, such as a
digital object identifier, CRAWLDoc retrieves the landing page and all linked
web resources, including PDFs, ORCID profiles, and supplementary materials. It
embeds these resources, along with anchor texts and the URLs, into a unified
representation. For evaluating CRAWLDoc, we have created a new, manually
labeled dataset of 600 publications from six top publishers in computer
science. Our method CRAWLDoc demonstrates a robust and layout-independent
ranking of relevant documents across publishers and data formats. It lays the
foundation for improved metadata extraction from web documents with various
layouts and formats. Our source code and dataset can be accessed at
https://github.com/FKarl/CRAWLDoc.

</details>


### [68] [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/pdf/2506.03861)
*Qiuhan Han, Qian Wang, Atsushi Yoshikawa, Masayuki Yamamura*

Main category: cs.CL

TL;DR: PulseReddit dataset aligns Reddit discussions with cryptocurrency market stats for HFT. LLM-based MAS using PulseReddit outperforms baselines, especially in bull markets, and adapts well across market regimes.


<details>
  <summary>Details</summary>
Motivation: Social media like Reddit offers untapped data for HFT, but its impact on short-term trading is underexplored.

Method: Introduces PulseReddit dataset and uses LLM-based MAS to analyze social sentiment's effect on trading performance.

Result: MAS with PulseReddit data achieves better trading outcomes, particularly in bull markets, and adapts to various market conditions.

Conclusion: PulseReddit and findings highlight social media's value in HFT, offering insights for LLM selection and MAS research.

Abstract: High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding
rapid decision-making. Social media platforms like Reddit offer valuable, yet
underexplored, information for such high-frequency, short-term trading. This
paper introduces \textbf{PulseReddit}, a novel dataset that is the first to
align large-scale Reddit discussion data with high-frequency cryptocurrency
market statistics for short-term trading analysis. We conduct an extensive
empirical study using Large Language Model (LLM)-based Multi-Agent Systems
(MAS) to investigate the impact of social sentiment from PulseReddit on trading
performance. Our experiments conclude that MAS augmented with PulseReddit data
achieve superior trading outcomes compared to traditional baselines,
particularly in bull markets, and demonstrate robust adaptability across
different market regimes. Furthermore, our research provides conclusive
insights into the performance-efficiency trade-offs of different LLMs,
detailing significant considerations for practical model selection in HFT
applications. PulseReddit and our findings establish a foundation for advanced
MAS research in HFT, demonstrating the tangible benefits of integrating social
media.

</details>


### [69] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/pdf/2506.03880)
*Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, Jianhua Tao*

Main category: cs.CL

TL;DR: RadialRouter is a new LLM routing framework using a lightweight Transformer (RadialFormer) to optimize LLM selection, outperforming existing methods by 9.2% and 5.8% in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM routing methods lack effectiveness due to insufficient exploration of query-LLM relationships.

Method: RadialRouter employs RadialFormer, a Transformer-based backbone, and refines selection with a combined Kullback-Leibler divergence and contrastive loss objective.

Result: Outperforms existing methods by 9.2% (Balance) and 5.8% (Cost First) on RouterBench, with adaptability to dynamic LLM pools.

Conclusion: RadialRouter demonstrates superior performance and practical potential for efficient LLM routing.

Abstract: The rapid advancements in large language models (LLMs) have led to the
emergence of routing techniques, which aim to efficiently select the optimal
LLM from diverse candidates to tackle specific tasks, optimizing performance
while reducing costs. Current LLM routing methods are limited in effectiveness
due to insufficient exploration of the intrinsic connection between user
queries and the characteristics of LLMs. To address this issue, in this paper,
we present RadialRouter, a novel framework for LLM routing which employs a
lightweight Transformer-based backbone with a radial structure named
RadialFormer to articulate the query-LLMs relationship. The optimal LLM
selection is performed based on the final states of RadialFormer. The pipeline
is further refined by an objective function that combines Kullback-Leibler
divergence with the query-query contrastive loss to enhance robustness.
Experimental results on RouterBench show that RadialRouter significantly
outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost
First scenarios, respectively. Additionally, its adaptability toward different
performance-cost trade-offs and the dynamic LLM pool demonstrates practical
application potential.

</details>


### [70] [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/pdf/2506.03867)
*Jacqueline Rowe, Mateusz Klimaszewski, Liane Guillou, Shannon Vallor, Alexandra Birch*

Main category: cs.CL

TL;DR: EuroGEST is a dataset for measuring gender bias in LLMs across 30 European languages, revealing strong stereotypes and showing larger models encode bias more strongly.


<details>
  <summary>Details</summary>
Motivation: Most gender bias benchmarks are English-centric; EuroGEST addresses this gap for multilingual fairness studies.

Method: Expands an expert-informed benchmark using translation tools, quality metrics, and morphological heuristics, validated by human evaluations.

Result: Strongest stereotypes: women as beautiful, empathetic, neat; men as leaders, strong, tough, professional. Larger models show stronger bias.

Conclusion: Highlights need for multilingual fairness studies and provides scalable tools for auditing gender bias in LLMs.

Abstract: Large language models increasingly support multiple languages, yet most
benchmarks for gender bias remain English-centric. We introduce EuroGEST, a
dataset designed to measure gender-stereotypical reasoning in LLMs across
English and 29 European languages. EuroGEST builds on an existing
expert-informed benchmark covering 16 gender stereotypes, expanded in this work
using translation tools, quality estimation metrics, and morphological
heuristics. Human evaluations confirm that our data generation method results
in high accuracy of both translations and gender labels across languages. We
use EuroGEST to evaluate 24 multilingual language models from six model
families, demonstrating that the strongest stereotypes in all models across all
languages are that women are \textit{beautiful,} \textit{empathetic} and
\textit{neat} and men are \textit{leaders}, \textit{strong, tough} and
\textit{professional}. We also show that larger models encode gendered
stereotypes more strongly and that instruction finetuning does not consistently
reduce gendered stereotypes. Our work highlights the need for more multilingual
studies of fairness in LLMs and offers scalable methods and resources to audit
gender bias across languages.

</details>


### [71] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/pdf/2506.03884)
*Utkarsh Pathak, Chandra Sai Krishna Gunda, Anusha Prakash, Keshav Agarwal, Hema A. Murthy*

Main category: cs.CL

TL;DR: The paper proposes a zero-shot TTS synthesis method for under-resourced Indian languages by augmenting shared phone representations and adapting text parsing rules, achieving intelligible and natural speech for several languages.


<details>
  <summary>Details</summary>
Motivation: India's linguistic diversity (1369 languages, 22 official) lacks digital resources, making TTS training challenging. The goal is to enable TTS for languages with no existing resources.

Method: Augments shared phone representations and modifies text parsing rules to match target language phonotactics, reducing synthesis overhead and enabling rapid adaptation.

Result: Successfully generated intelligible and natural speech for Sanskrit, Maharashtrian and Canara Konkani, Maithili, and Kurukh.

Conclusion: The approach is effective and has potential to expand speech technology access for under-represented languages.

Abstract: Text-to-speech (TTS) systems typically require high-quality studio data and
accurate transcriptions for training. India has 1369 languages, with 22
official using 13 scripts. Training a TTS system for all these languages, most
of which have no digital resources, seems a Herculean task. Our work focuses on
zero-shot synthesis, particularly for languages whose scripts and phonotactics
come from different families. The novelty of our work is in the augmentation of
a shared phone representation and modifying the text parsing rules to match the
phonotactics of the target language, thus reducing the synthesiser overhead and
enabling rapid adaptation. Intelligible and natural speech was generated for
Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging
linguistic connections across languages with suitable synthesisers. Evaluations
confirm the effectiveness of this approach, highlighting its potential to
expand speech technology access for under-represented languages.

</details>


### [72] [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/pdf/2506.03887)
*Junyi Chen, Shihao Bai, Zaijun Wang, Siyu Wu, Chuheng Du, Hailong Yang, Ruihao Gong, Shengzhong Liu, Fan Wu, Guihai Chen*

Main category: cs.CL

TL;DR: Pre$^3$ optimizes LLM decoding for LR(1) grammars by using deterministic pushdown automata (DPDA), reducing runtime overhead and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for structured LLM outputs (e.g., JSON) using LR(1) grammars suffer from inefficiency due to runtime parsing overhead, especially in large batches.

Method: Pre$^3$ precomputes prefix-conditioned edges to enable parallel processing and transforms LR(1) transition graphs into DPDA, eliminating runtime path exploration.

Result: Pre$^3$ reduces time per output token (TPOT) by up to 40% and increases throughput by up to 36%.

Conclusion: Pre$^3$ efficiently integrates into LLM frameworks, significantly improving structured generation performance.

Abstract: Extensive LLM applications demand efficient structured generations,
particularly for LR(1) grammars, to produce outputs in specified formats (e.g.,
JSON). Existing methods primarily parse LR(1) grammars into a pushdown
automaton (PDA), leading to runtime execution overhead for context-dependent
token processing, especially inefficient under large inference batches. To
address these issues, we propose Pre$^3$ that exploits deterministic pushdown
automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by
precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables
ahead-of-time edge analysis and thus makes parallel transition processing
possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$
introduces a novel approach that transforms LR(1) transition graphs into DPDA,
eliminating the need for runtime path exploration and achieving edge
transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into
standard LLM inference frameworks, reducing time per output token (TPOT) by up
to 40% and increasing throughput by up to 36% in our experiments. Our code is
available at https://github.com/ModelTC/lightllm.

</details>


### [73] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/pdf/2506.03941)
*Vivian Nguyen, Lillian Lee, Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: The paper introduces an unsupervised method to detect pivotal moments in conversations, validated in crisis counseling, showing alignment with human perception and conversational outcomes.


<details>
  <summary>Details</summary>
Motivation: To assist in domains like mental health crisis counseling by identifying moments where conversation outcomes can significantly diverge based on responses.

Method: An unsupervised computational approach detecting pivotal moments by analyzing variance in expected outcomes based on potential responses.

Result: Validated by human perception (longer response times) and conversational trajectory changes. Explored counselor responses' impact on session outcomes.

Conclusion: The method effectively identifies pivotal moments, aiding in understanding and improving consequential conversations.

Abstract: During a conversation, there can come certain moments where its outcome hangs
in the balance. In these pivotal moments, how one responds can put the
conversation on substantially different trajectories leading to significantly
different outcomes. Systems that can detect when such moments arise could
assist conversationalists in domains with highly consequential outcomes, such
as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting
such pivotal moments as they happen, in an online fashion. Our approach relies
on the intuition that a moment is pivotal if our expectation of the outcome
varies widely depending on what might be said next. By applying our method to
crisis counseling conversations, we first validate it by showing that it aligns
with human perception -- counselors take significantly longer to respond during
moments detected by our method -- and with the eventual conversational
trajectory -- which is more likely to change course at these times. We then use
our framework to explore the relation of the counselor's response during
pivotal moments with the eventual outcome of the session.

</details>


### [74] [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/pdf/2506.03901)
*Yuxin Zhang, Yan Wang, Yongrui Chen, Shenyu Zhang, Xinbang Dai, Sheng Bi, Guilin Qi*

Main category: cs.CL

TL;DR: The paper introduces Magic Mushroom, a benchmark for evaluating retrieval noise in RAG systems, highlighting their sensitivity to noise and the need for improved robustness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture real-world retrieval noise, undermining reliable assessment of RAG systems.

Method: Defines four categories of retrieval noise and introduces Magic Mushroom, a benchmark with configurable noise combinations for controlled evaluation.

Result: LLMs and RAG denoising strategies show significant sensitivity to noise distributions, with room for improvement.

Conclusion: Magic Mushroom is a valuable tool for advancing noise-robust RAG systems, aiding real-world deployment.

Abstract: Retrieval-Augmented Generation (RAG) systems enhance Large Language Models
(LLMs) by incorporating external retrieved information, mitigating issues such
as hallucination and outdated knowledge.
  However, RAG systems are highly sensitive to retrieval noise prevalent in
real-world scenarios.
  Existing benchmarks fail to emulate the complex and heterogeneous noise
distributions encountered in real-world retrieval environments, undermining
reliable robustness assessment.
  In this paper, we define four categories of retrieval noise based on
linguistic properties and noise characteristics, aiming to reflect the
heterogeneity of noise in real-world scenarios.
  Building on this, we introduce Magic Mushroom, a benchmark for replicating
"magic mushroom" noise: contexts that appear relevant on the surface but
covertly mislead RAG systems.
  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer
pairs.
  More importantly, Magic Mushroom enables researchers to flexibly configure
combinations of retrieval noise according to specific research objectives or
application scenarios, allowing for highly controlled evaluation setups.
  We evaluate LLM generators of varying parameter scales and classic RAG
denoising strategies under diverse noise distributions to investigate their
performance dynamics during progressive noise encroachment.
  Our analysis reveals that both generators and denoising strategies have
significant room for improvement and exhibit extreme sensitivity to noise
distributions.
  Magic Mushroom emerges as a promising tool for evaluating and advancing
noise-robust RAG systems, accelerating their widespread deployment in
real-world applications.
  The Magic Mushroom benchmark is available at the
https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.

</details>


### [75] [The Harmonic Structure of Information Contours](https://arxiv.org/pdf/2506.03902)
*Eleftheria Tsipidi, Samuel Kiegeland, Franz Nowak, Tianyang Xu, Ethan Wilcox, Alex Warstadt, Ryan Cotterell, Mario Giulianelli*

Main category: cs.CL

TL;DR: The paper explores periodic fluctuations in information rate in language, proposing an implicit pressure towards periodicity. Using harmonic regression and time scaling, it finds consistent periodic patterns across multiple languages, linking them to discourse structure.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why language information rate fluctuates, challenging the uniform information density hypothesis by suggesting periodicity as an alternative explanation.

Method: The authors apply harmonic regression and introduce time scaling to detect periodic patterns in information rate across English, Spanish, German, Dutch, Basque, and Brazilian Portuguese texts.

Result: Consistent periodic patterns in information rate are found, with dominant frequencies aligning with discourse structure, indicating meaningful linguistic organization.

Conclusion: The findings connect information rate fluctuations to discourse structure and provide a framework for analyzing structural pressures in language.

Abstract: The uniform information density (UID) hypothesis proposes that speakers aim
to distribute information evenly throughout a text, balancing production effort
and listener comprehension difficulty. However, language typically does not
maintain a strictly uniform information rate; instead, it fluctuates around a
global average. These fluctuations are often explained by factors such as
syntactic constraints, stylistic choices, or audience design. In this work, we
explore an alternative perspective: that these fluctuations may be influenced
by an implicit linguistic pressure towards periodicity, where the information
rate oscillates at regular intervals, potentially across multiple frequencies
simultaneously. We apply harmonic regression and introduce a novel extension
called time scaling to detect and test for such periodicity in information
contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and
Brazilian Portuguese, we find consistent evidence of periodic patterns in
information rate. Many dominant frequencies align with discourse structure,
suggesting these oscillations reflect meaningful linguistic organization.
Beyond highlighting the connection between information rate and discourse
structure, our approach offers a general framework for uncovering structural
pressures at various levels of linguistic granularity.

</details>


### [76] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/pdf/2506.03913)
*Claire Barale, Michael Rovatsos, Nehal Bhuta*

Main category: cs.CL

TL;DR: The paper evaluates ML methods for fairness in legal decisions, finding limitations in current approaches and advocating for context-aware, legally grounded methods.


<details>
  <summary>Details</summary>
Motivation: To assess whether statistical ML methods can effectively evaluate fairness in discretionary legal contexts like refugee adjudication.

Method: Empirical evaluation of three ML approaches (feature-based analysis, semantic clustering, predictive modeling) on 59,000+ Canadian refugee decisions.

Result: ML methods produce divergent signals, rely on procedural features, and fail to capture legal reasoning, highlighting limitations in statistical fairness evaluation.

Conclusion: Current computational approaches are insufficient; fairness evaluation in law requires integrating legal reasoning and institutional context.

Abstract: Legal decisions are increasingly evaluated for fairness, consistency, and
bias using machine learning (ML) techniques. In high-stakes domains like
refugee adjudication, such methods are often applied to detect disparities in
outcomes. Yet it remains unclear whether statistical methods can meaningfully
assess fairness in legal contexts shaped by discretion, normative complexity,
and limited ground truth.
  In this paper, we empirically evaluate three common ML approaches
(feature-based analysis, semantic clustering, and predictive modeling) on a
large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our
experiments show that these methods produce divergent and sometimes
contradictory signals, that predictive modeling often depends on contextual and
procedural features rather than legal features, and that semantic clustering
fails to capture substantive legal reasoning.
  We show limitations of statistical fairness evaluation, challenge the
assumption that statistical regularity equates to fairness, and argue that
current computational approaches fall short of evaluating fairness in legally
discretionary domains. We argue that evaluating fairness in law requires
methods grounded not only in data, but in legal reasoning and institutional
context.

</details>


### [77] [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/pdf/2506.03916)
*Agostina Calabrese, Tom Sherborne, Björn Ross, Mirella Lapata*

Main category: cs.CL

TL;DR: The paper addresses hate speech detection challenges by introducing U-PLEAD, a synthetic dataset, to improve model generalization beyond training data biases.


<details>
  <summary>Details</summary>
Motivation: Current hate speech detection models struggle with generalization due to dataset biases and sentence-level labels, failing to capture hate speech structure.

Method: The authors create U-PLEAD, a synthetic dataset with balanced expression frequencies, and combine it with real data for training.

Result: Training with U-PLEAD and real data improves compositional generalization and achieves state-of-the-art performance on PLEAD.

Conclusion: Balancing expression frequencies across contexts enhances hate speech detection model generalization.

Abstract: Hate speech detection is key to online content moderation, but current models
struggle to generalise beyond their training data. This has been linked to
dataset biases and the use of sentence-level labels, which fail to teach models
the underlying structure of hate speech. In this work, we show that even when
models are trained with more fine-grained, span-level annotations (e.g.,
"artists" is labeled as target and "are parasites" as dehumanising comparison),
they struggle to disentangle the meaning of these labels from the surrounding
context. As a result, combinations of expressions that deviate from those seen
during training remain particularly difficult for models to detect. We
investigate whether training on a dataset where expressions occur with equal
frequency across all contexts can improve generalisation. To this end, we
create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel
compositional generalisation benchmark of ~8,000 manually validated posts.
Training on a combination of U-PLEAD and real data improves compositional
generalisation while achieving state-of-the-art performance on the
human-sourced PLEAD.

</details>


### [78] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/pdf/2506.03922)
*Zhaolu Kang, Junhao Gong, Jiaxu Yan, Wanke Xia, Yian Wang, Ziwen Wang, Huaxuan Ding, Zhuo Cheng, Wenhao Cao, Zhiyuan Feng, Siqi He, Shannan Yan, Junzhe Chen, Xiaomin He, Chaoya Jiang, Wei Ye, Kaidong Yu, Xuelong Li*

Main category: cs.CL

TL;DR: HSSBench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) on Humanities and Social Sciences (HSS) tasks, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLM benchmarks focus on STEM disciplines, neglecting HSS needs like interdisciplinary thinking and abstract-visual linking.

Method: HSSBench includes 13,000 samples in six categories, generated via a collaborative pipeline of experts and automated agents, covering six UN languages.

Result: Testing 20+ MLLMs shows HSSBench poses significant challenges, even for top models.

Conclusion: HSSBench aims to inspire research into improving MLLMs' cross-disciplinary reasoning and knowledge integration.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [79] [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/pdf/2506.03923)
*Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: LLMs exhibit framing bias in comparative math problems, influenced by terms like 'more', 'less', or 'equal'. The MathComp benchmark reveals this bias, mitigated partially by chain-of-thought prompting. Demographic terms amplify bias, highlighting fairness concerns.


<details>
  <summary>Details</summary>
Motivation: To understand how semantic framing biases LLM reasoning, especially in comparative contexts, and to evaluate robustness and fairness.

Method: Introduced MathComp, a benchmark of 300 comparison scenarios tested under 14 prompt variants across three LLM families. Analyzed framing bias and mitigation via chain-of-thought prompting.

Result: Framing terms systematically steer predictions. Chain-of-thought helps but varies in effectiveness. Demographic terms exacerbate bias.

Conclusion: Framing biases are critical blind spots; benchmarks should account for them to ensure robust and fair LLM reasoning.

Abstract: Large language models (LLMs) are known to be sensitive to input phrasing, but
the mechanisms by which semantic cues shape reasoning remain poorly understood.
We investigate this phenomenon in the context of comparative math problems with
objective ground truth, revealing a consistent and directional framing bias:
logically equivalent questions containing the words ``more'', ``less'', or
``equal'' systematically steer predictions in the direction of the framing
term. To study this effect, we introduce MathComp, a controlled benchmark of
300 comparison scenarios, each evaluated under 14 prompt variants across three
LLM families. We find that model errors frequently reflect linguistic steering,
systematic shifts toward the comparative term present in the prompt.
Chain-of-thought prompting reduces these biases, but its effectiveness varies:
free-form reasoning is more robust, while structured formats may preserve or
reintroduce directional drift. Finally, we show that including demographic
identity terms (e.g., ``a woman'', ``a Black person'') in input scenarios
amplifies directional drift, despite identical underlying quantities,
highlighting the interplay between semantic framing and social referents. These
findings expose critical blind spots in standard evaluation and motivate
framing-aware benchmarks for diagnosing reasoning robustness and fairness in
LLMs.

</details>


### [80] [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/pdf/2506.03949)
*Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu*

Main category: cs.CL

TL;DR: The paper introduces TableEval, a new benchmark for evaluating LLMs on realistic TableQA tasks, addressing limitations like simple table structures, data leakage, and monolingual focus. It also proposes SEAT, a semantic accuracy evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Existing TableQA benchmarks are limited by simplistic table structures, monolingual data, and data leakage, failing to capture real-world complexities.

Method: TableEval includes diverse table structures from four domains and three languages, with data collected from recent real-world documents. SEAT evaluates semantic accuracy at the sub-question level.

Result: SEAT aligns well with human judgment, and experiments reveal gaps in state-of-the-art LLMs' ability to handle complex TableQA tasks.

Conclusion: TableEval and SEAT provide a robust framework for evaluating LLMs on realistic TableQA tasks, highlighting areas for future improvement.

Abstract: LLMs have shown impressive progress in natural language processing. However,
they still face significant challenges in TableQA, where real-world
complexities such as diverse table structures, multilingual data, and
domain-specific reasoning are crucial. Existing TableQA benchmarks are often
limited by their focus on simple flat tables and suffer from data leakage.
Furthermore, most benchmarks are monolingual and fail to capture the
cross-lingual and cross-domain variability in practical applications. To
address these limitations, we introduce TableEval, a new benchmark designed to
evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes
tables with various structures (such as concise, hierarchical, and nested
tables) collected from four domains (including government, finance, academia,
and industry reports). Besides, TableEval features cross-lingual scenarios with
tables in Simplified Chinese, Traditional Chinese, and English. To minimize the
risk of data leakage, we collect all data from recent real-world documents.
Considering that existing TableQA metrics fail to capture semantic accuracy, we
further propose SEAT, a new evaluation framework that assesses the alignment
between model responses and reference answers at the sub-question level.
Experimental results have shown that SEAT achieves high agreement with human
judgment. Extensive experiments on TableEval reveal critical gaps in the
ability of state-of-the-art LLMs to handle these complex, real-world TableQA
tasks, offering insights for future improvements. We make our dataset available
here: https://github.com/wenge-research/TableEval.

</details>


### [81] [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/pdf/2506.03968)
*Chiwei Zhu, Benfeng Xu, Xiaorui Wang, Zhendong Mao*

Main category: cs.CL

TL;DR: The paper introduces a method for generating diverse and complex synthetic instructions for aligning large language models (LLMs) using attributed grounding, achieving leading benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating synthetic instructions lack grounding or complexity, limiting their effectiveness for LLM alignment.

Method: The proposed framework uses attributed grounding: top-down attribution to real instructions and bottom-up synthesis from web documents to generate diverse and complex instructions.

Result: A dataset of 1 million instructions (SynthQuestions) was created, and models trained on it showed leading performance on benchmarks, scaling with more web corpora.

Conclusion: Attributed grounding enables scalable generation of high-quality synthetic instructions, improving LLM alignment performance.

Abstract: The pursuit of diverse, complex, and large-scale instruction data is crucial
for automatically aligning large language models (LLMs). While there are
methods capable of generating synthetic instructions at scale, they either
suffer from limited grounding sources, leading to a narrow distribution, or
rely on trivial extensions that fail to produce meaningful trajectories in
terms of complexity. In contrast, instructions that benefit efficient alignment
are typically crafted with cognitive insights and grounded in real-world use
cases. In this paper, we synthesize such instructions using attributed
grounding, which involves 1) a top-down attribution process that grounds a
selective set of real instructions to situated users, and 2) a bottom-up
synthesis process that leverages web documents to first generate a situation,
then a meaningful instruction. This framework allows us to harvest diverse and
complex instructions at scale, utilizing the vast range of web documents.
Specifically, we construct a dataset of 1 million instructions, called
SynthQuestions, and demonstrate that models trained on it achieve leading
performance on several common benchmarks, with improvements that continually
scale with more web corpora. Data, models and codes will be available at
https://github.com/Ignoramus0817/SynthQuestions.

</details>


### [82] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/pdf/2506.04043)
*Mikel K. Ngueajio, Flor Miriam Plaza-del-Arco, Yi-Ling Chung, Danda B. Rawat, Amanda Cercas Curry*

Main category: cs.CL

TL;DR: The paper evaluates LLM-generated counter-narratives for hate speech, focusing on persona, readability, tone, and ethics, revealing issues with verbosity and accessibility.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the affective tone, accessibility, and ethical risks of automated counter-narratives for hate speech.

Method: Proposes a framework to evaluate CNs across four dimensions, testing three prompting strategies on GPT-4o-Mini, Cohere's CommandR-7B, and LLaMA 3.1-70B using MT-Conan and HatEval datasets.

Result: LLM-generated CNs are often verbose and less accessible, though emotionally guided prompts improve empathy and readability. Safety and effectiveness concerns persist.

Conclusion: While LLMs show promise for CN generation, improvements in accessibility and ethical robustness are needed.

Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating
online hate speech, yet concerns about their affective tone, accessibility, and
ethical risks remain. We propose a framework for evaluating Large Language
Model (LLM)-generated CNs across four dimensions: persona framing, verbosity
and readability, affective tone, and ethical robustness. Using GPT-4o-Mini,
Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting
strategies on the MT-Conan and HatEval datasets. Our findings reveal that
LLM-generated CNs are often verbose and adapted for people with college-level
literacy, limiting their accessibility. While emotionally guided prompts yield
more empathetic and readable responses, there remain concerns surrounding
safety and effectiveness.

</details>


### [83] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/pdf/2506.03978)
*Hieu Trung Nguyen, Bao Nguyen, Viet Anh Nguyen*

Main category: cs.CL

TL;DR: Selective pruning of attention heads in transformers improves reasoning performance. SPRINT, a contrastive learning framework, dynamically selects optimal heads to prune, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Observation that selective pruning enhances reasoning capabilities, especially in challenging tasks.

Method: Proposes SPRINT, a contrastive learning framework aligning question and head embeddings to dynamically select optimal pruning configurations.

Result: SPRINT outperforms best-of-N and random head selection on MATH500 and GSM8K datasets.

Conclusion: Selective pruning via SPRINT enhances reasoning performance, offering a novel approach to model optimization.

Abstract: Model pruning in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
pruning of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


### [84] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/pdf/2506.04044)
*Aleksey Kudelya, Alexander Shirnin*

Main category: cs.CL

TL;DR: LIBU is a lightweight algorithm for unlearning specific knowledge in large language models without retraining, using influence functions and second-order optimization.


<details>
  <summary>Details</summary>
Motivation: To remove sensitive content from LLMs efficiently without retraining or losing overall utility.

Method: Combines influence functions to erase data influence and second-order optimization to stabilize utility.

Result: Effective for unlearning in various tasks, maintaining model utility.

Conclusion: LIBU is a practical solution for unlearning in LLMs.

Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an
algorithm to solve the task of unlearning - removing specific knowledge from a
large language model without retraining from scratch and compromising its
overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large
Language Models). The algorithm combines classical \textit{influence functions}
to remove the influence of the data from the model and \textit{second-order
optimization} to stabilize the overall utility. Our experiments show that this
lightweight approach is well applicable for unlearning LLMs in different kinds
of task.

</details>


### [85] [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/pdf/2506.03980)
*Takeshi Saga, Catherine Pelachaud*

Main category: cs.CL

TL;DR: A multimodal VAP model with pre-trained audio and face encoders outperforms state-of-the-art models in turn-taking prediction by capturing subtle expressions.


<details>
  <summary>Details</summary>
Motivation: To improve turn-taking prediction in human-machine interaction by leveraging multimodal cues (audio and face) for better performance.

Method: Proposes a multimodal VAP model enhanced with pre-trained audio and face encoders to capture subtle expressions.

Result: The model performs competitively, sometimes better than state-of-the-art models on turn-taking metrics.

Conclusion: The proposed multimodal approach effectively improves turn-taking prediction, with code and models made publicly available.

Abstract: Turn-taking management is crucial for any social interaction. Still, it is
challenging to model human-machine interaction due to the complexity of the
social context and its multimodal nature. Unlike conventional systems based on
silence duration, previous existing voice activity projection (VAP) models
successfully utilized a unified representation of turn-taking behaviors as
prediction targets, which improved turn-taking prediction performance.
Recently, a multimodal VAP model outperformed the previous state-of-the-art
model by a significant margin. In this paper, we propose a multimodal model
enhanced with pre-trained audio and face encoders to improve performance by
capturing subtle expressions. Our model performed competitively, and in some
cases, even better than state-of-the-art models on turn-taking metrics. All the
source codes and pretrained models are available at
https://github.com/sagatake/VAPwithAudioFaceEncoders.

</details>


### [86] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/pdf/2506.04050)
*Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, Ayoub Bagheri*

Main category: cs.CL

TL;DR: The paper explores using XAI methods to reduce AI-generated text detectability and introduces an ensemble classifier for robust detection. Token replacement strategies based on XAI insights lower single-classifier detection but are countered by the ensemble approach.


<details>
  <summary>Details</summary>
Motivation: To address the detectability of AI-generated text (AIGT) by leveraging explainable AI (XAI) to identify and modify influential tokens, while ensuring robust detection through ensemble methods.

Method: Train an ensemble classifier to detect AIGT, use SHAP and LIME to identify influential tokens, and propose four token replacement strategies to reduce detectability.

Result: Token replacement reduces single-classifier detection, but the ensemble classifier remains robust across languages and domains.

Conclusion: XAI can make AIGT harder to detect, but ensemble-based detection is necessary to counter token-level manipulations.

Abstract: Generative models, especially large language models (LLMs), have shown
remarkable progress in producing text that appears human-like. However, they
often exhibit patterns that make their output easier to detect than text
written by humans. In this paper, we investigate how explainable AI (XAI)
methods can be used to reduce the detectability of AI-generated text (AIGT)
while also introducing a robust ensemble-based detection approach. We begin by
training an ensemble classifier to distinguish AIGT from human-written text,
then apply SHAP and LIME to identify tokens that most strongly influence its
predictions. We propose four explainability-based token replacement strategies
to modify these influential tokens. Our findings show that these token
replacement approaches can significantly diminish a single classifier's ability
to detect AIGT. However, our ensemble classifier maintains strong performance
across multiple languages and domains, showing that a multi-model approach can
mitigate the impact of token-level manipulations. These results show that XAI
methods can make AIGT harder to detect by focusing on the most influential
tokens. At the same time, they highlight the need for robust, ensemble-based
detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [87] [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/pdf/2506.03984)
*Carolin Holtermann, Paul Röttger, Anne Lauscher*

Main category: cs.CL

TL;DR: The paper evaluates language models' ability to jointly reason over time and space using the GeoTemp dataset, finding strong temporal reasoning but limitations in combining temporal and geographic knowledge.


<details>
  <summary>Details</summary>
Motivation: To explore the largely untested abilities of language models in reasoning over both time and space, beyond isolated or simple scenarios.

Method: Creation of the GeoTemp dataset (320k prompts across 289 cities, 217 countries, 37 time zones) and evaluation of eight open chat models from three families.

Result: Models perform well on temporal reasoning, improve with scale, but struggle with tasks combining temporal and geographic knowledge. Performance correlates with low perplexity for location names, not specific regions.

Conclusion: Prompt formulation heavily impacts performance; direct geographic knowledge injection helps, while chain-of-thought prompting can hinder simpler tasks.

Abstract: Reasoning over time and space is essential for understanding our world.
However, the abilities of language models in this area are largely unexplored
as previous work has tested their abilities for logical reasoning in terms of
time and space in isolation or only in simple or artificial environments. In
this paper, we present the first evaluation of the ability of language models
to jointly reason over time and space. To enable our analysis, we create
GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37
time zones. Using GeoTemp, we evaluate eight open chat models of three
different model families for different combinations of temporal and geographic
knowledge. We find that most models perform well on reasoning tasks involving
only temporal knowledge and that overall performance improves with scale.
However, performance remains constrained in tasks that require connecting
temporal and geographical information. We do not find clear correlations of
performance with specific geographic regions. Instead, we find a significant
performance increase for location names with low model perplexity, suggesting
their repeated occurrence during model training. We further demonstrate that
their performance is heavily influenced by prompt formulation - a direct
injection of geographical knowledge leads to performance gains, whereas,
surprisingly, techniques like chain-of-thought prompting decrease performance
on simpler tasks.

</details>


### [88] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/pdf/2506.04051)
*Tim Franzmeyer, Archie Sravankumar, Lijuan Liu, Yuning Mao, Rui Hou, Sinong Wang, Jakob N. Foerster, Luke Zettlemoyer, Madian Khabsa*

Main category: cs.CL

TL;DR: HALT is a post-training method for LLMs to abstain from generating incorrect responses by aligning responses with the model's capabilities, improving correctness by 15% on average.


<details>
  <summary>Details</summary>
Motivation: To address LLM hallucination by ensuring models only generate content when confident, improving reliability.

Method: HALT splits responses into factual fragments, identifies incorrect ones using ground truth, and replaces or removes them based on a tunable threshold.

Result: HALT improves mean correctness by 15%, F1 score by 4%, and achieves 87% correctness in a Llama3-70B model while maintaining 53% response completeness.

Conclusion: HALT effectively balances response completeness and correctness, enhancing LLM reliability.

Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they
can produce incorrect answers when they lack knowledge or capability -- a
problem known as hallucination. We instead propose post-training an LLM to
generate content only when confident in its correctness and to otherwise
(partially) abstain. Specifically, our method, HALT, produces
capability-aligned post-training data that encodes what the model can and
cannot reliably generate. We generate this data by splitting responses of the
pretrained LLM into factual fragments (atomic statements or reasoning steps),
and use ground truth information to identify incorrect fragments. We achieve
capability-aligned finetuning responses by either removing incorrect fragments
or replacing them with "Unsure from Here" -- according to a tunable threshold
that allows practitioners to trade off response completeness and mean
correctness of the response's fragments. We finetune four open-source models
for biography writing, mathematics, coding, and medicine with HALT for three
different trade-off thresholds. HALT effectively trades off response
completeness for correctness, increasing the mean correctness of response
fragments by 15% on average, while resulting in a 4% improvement in the F1
score (mean of completeness and correctness of the response) compared to the
relevant baselines. By tuning HALT for highest correctness, we train a single
reliable Llama3-70B model with correctness increased from 51% to 87% across all
four domains while maintaining 53% of the response completeness achieved with
standard finetuning.

</details>


### [89] [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/pdf/2506.03989)
*Alex Laitenberger, Christopher D. Manning, Nelson F. Liu*

Main category: cs.CL

TL;DR: DOS RAG, a simple retrieve-then-read method, matches or outperforms complex multi-stage RAG pipelines like ReadAgent and RAPTOR in long-context QA tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether multi-stage RAG pipelines still provide benefits over simpler methods given the capabilities of modern long-context LMs.

Method: Controlled evaluation of QA tasks under scaled token budgets, comparing multi-stage pipelines (ReadAgent, RAPTOR) with baselines, including DOS RAG.

Result: DOS RAG consistently matches or outperforms more complex methods on long-context QA benchmarks.

Conclusion: DOS RAG should be a baseline for future RAG evaluations, balancing simplicity and effectiveness as models evolve.

Abstract: With the rise of long-context language models (LMs) capable of processing
tens of thousands of tokens in a single pass, do multi-stage
retrieval-augmented generation (RAG) pipelines still offer measurable benefits
over simpler, single-stage approaches? To assess this question, we conduct a
controlled evaluation for QA tasks under systematically scaled token budgets,
comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three
baselines, including DOS RAG (Document's Original Structure RAG), a simple
retrieve-then-read method that preserves original passage order. Despite its
straightforward design, DOS RAG consistently matches or outperforms more
intricate methods on multiple long-context QA benchmarks. We recommend
establishing DOS RAG as a simple yet strong baseline for future RAG
evaluations, pairing it with emerging embedding and language models to assess
trade-offs between complexity and effectiveness as model capabilities evolve.

</details>


### [90] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/pdf/2506.04078)
*Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-Med is a new benchmark for evaluating LLMs in medicine, addressing limitations of existing benchmarks with real-world questions and an automated evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Current medical benchmarks lack real-world clinical scenarios and robust evaluation methods, necessitating a more reliable tool for assessing LLMs in medicine.

Method: Developed LLMEval-Med with 2,996 questions from real-world EHRs and expert-designed scenarios, using an automated pipeline with expert checklists and human-machine validation.

Result: Evaluated 13 LLMs, providing insights for safe and effective deployment in medicine.

Conclusion: LLMEval-Med improves LLM evaluation in medicine by addressing existing benchmark limitations and ensuring reliability through expert feedback.

Abstract: Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.

</details>


### [91] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/pdf/2506.03990)
*Hongzhi Zhang, Jingyuan Zhang, Xingguang Ji, Qi Wang, Fuzheng Zhang*

Main category: cs.CL

TL;DR: DynTok introduces dynamic video token compression, reducing tokens to 44.4% of original size while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing video modeling methods generate excessive visual tokens for long videos, increasing computational overhead.

Method: DynTok adaptively splits and merges visual tokens, compressing low-density regions while preserving essential content.

Result: Achieves 65.3% on Video-MME and 72.5% on MLVU, with better performance as video frames increase.

Conclusion: Exposes redundancy in video tokens and suggests more efficient video modeling techniques.

Abstract: Typical video modeling methods, such as LLava, represent videos as sequences
of visual tokens, which are then processed by the LLM backbone for effective
video understanding. However, this approach leads to a massive number of visual
tokens, especially for long videos. A practical solution is to first extract
relevant visual information from the large visual context before feeding it
into the LLM backbone, thereby reducing computational overhead. In this work,
we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression
strategy. DynTok adaptively splits visual tokens into groups and merges them
within each group, achieving high compression in regions with low information
density while preserving essential content. Our method reduces the number of
tokens to 44.4% of the original size while maintaining comparable performance.
It further benefits from increasing the number of video frames and achieves
65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective
compression method, we expose the redundancy in video token representations and
offer insights for designing more efficient video modeling techniques.

</details>


### [92] [EuroLLM-9B: Technical Report](https://arxiv.org/pdf/2506.04079)
*Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Manuel Faysse, Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-9B is a European-made large language model supporting 35 languages, addressing underrepresentation in existing models. It includes innovative data filtering and synthetic datasets, achieving competitive performance and being openly released.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of European languages in open large language models and support multilingual needs of European citizens.

Method: Developed EuroLLM-9B with a focus on tokenizer design, data filtering (EuroFilter), and synthetic datasets (EuroBlocks-Synthetic). Detailed pre-training and post-training procedures.

Result: Competitive performance on multilingual benchmarks and machine translation tasks, establishing EuroLLM-9B as a leading open European LLM.

Conclusion: EuroLLM-9B successfully addresses language underrepresentation, performs well, and is openly released to support research and adoption.

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.

</details>


### [93] [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/pdf/2506.03993)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: The paper introduces 'Words of Warmth,' a large-scale lexicon of word associations with warmth, trust, and sociability, demonstrating its reliability and applications in child development and bias research.


<details>
  <summary>Details</summary>
Motivation: To explore the dimensions of warmth (trust and sociability) and competence in social assessments, and provide a resource for studying their development and impact.

Method: Creation of a manually derived lexicon of word associations for over 26k English words, validated for reliability, and applied to study child development and bias.

Result: The lexicon is highly reliable and useful for analyzing word acquisition in children and researching biases and stereotypes.

Conclusion: 'Words of Warmth' is a valuable resource for social psychology research, enabling studies on warmth dimensions and their societal implications.

Abstract: Social psychologists have shown that Warmth (W) and Competence (C) are the
primary dimensions along which we assess other people and groups. These
dimensions impact various aspects of our lives from social competence and
emotion regulation to success in the work place and how we view the world. More
recent work has started to explore how these dimensions develop, why they have
developed, and what they constitute. Of particular note, is the finding that
warmth has two distinct components: Trust (T) and Sociability (S). In this
work, we introduce Words of Warmth, the first large-scale repository of
manually derived word--warmth (as well as word--trust and word--sociability)
associations for over 26k English words. We show that the associations are
highly reliable. We use the lexicons to study the rate at which children
acquire WCTS words with age. Finally, we show that the lexicon enables a wide
variety of bias and stereotype research through case studies on various target
entities. Words of Warmth is freely available at:
http://saifmohammad.com/warmth.html

</details>


### [94] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/pdf/2506.03994)
*Dan Oneata, Desmond Elliott, Stella Frank*

Main category: cs.CL

TL;DR: The paper compares how well foundation models represent semantic features of concrete objects, finding multimodal encoders slightly outperform language-only models, while image-only encoders match language models even on non-visual attributes.


<details>
  <summary>Details</summary>
Motivation: To understand how large-scale models represent semantic features of objects, contrasting human learning grounded in sensorimotor experience.

Method: Probing tasks evaluate models (image-only, multimodal, language-only) on predicting semantic feature norms (McRae and Binder datasets).

Result: Multimodal encoders slightly outperform language-only models; image-only encoders perform comparably, even on non-visual attributes.

Conclusion: Results highlight insights into unimodal learning and modality complementarity, questioning the necessity of multimodal training for certain tasks.

Abstract: Human learning and conceptual representation is grounded in sensorimotor
experience, in contrast to state-of-the-art foundation models. In this paper,
we investigate how well such large-scale models, trained on vast quantities of
data, represent the semantic feature norms of concrete object concepts, e.g. a
ROSE is red, smells sweet, and is a flower. More specifically, we use probing
tasks to test which properties of objects these models are aware of. We
evaluate image encoders trained on image data alone, as well as
multimodally-trained image encoders and language-only models, on predicting an
extended denser version of the classic McRae norms and the newer Binder dataset
of attribute ratings. We find that multimodal image encoders slightly
outperform language-only approaches, and that image-only encoders perform
comparably to the language models, even on non-visual attributes that are
classified as "encyclopedic" or "function". These results offer new insights
into what can be learned from pure unimodal learning, and the complementarity
of the modalities.

</details>


### [95] [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/pdf/2506.04020)
*An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Zhuang Li*

Main category: cs.CL

TL;DR: The paper introduces QQSUM, a task for summarizing diverse customer opinions into key points (KPs) and quantifying their prevalence to answer product queries better than single-perspective PQA systems. The proposed QQSUM-RAG model outperforms RAG baselines in quality and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing PQA systems lack diversity in answers, failing to represent varied customer opinions. QQSUM addresses this by summarizing and quantifying diverse viewpoints.

Method: QQSUM-RAG extends RAG with few-shot learning, training a KP-oriented retriever and summary generator to produce diverse and representative KP summaries.

Result: QQSUM-RAG outperforms state-of-the-art RAG baselines in textual quality and quantification accuracy.

Conclusion: QQSUM-RAG effectively captures diverse opinions and quantifies their prevalence, improving PQA systems by providing more comprehensive answers.

Abstract: Review-based Product Question Answering (PQA) allows e-commerce platforms to
automatically address customer queries by leveraging insights from user
reviews. However, existing PQA systems generate answers with only a single
perspective, failing to capture the diversity of customer opinions. In this
paper we introduce a novel task Quantitative Query-Focused Summarization
(QQSUM), which aims to summarize diverse customer opinions into representative
Key Points (KPs) and quantify their prevalence to effectively answer user
queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its
generated answers still fall short of capturing the full diversity of
viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,
employs few-shot learning to jointly train a KP-oriented retriever and a KP
summary generator, enabling KP-based summaries that capture diverse and
representative opinions. Experimental results demonstrate that QQSUM-RAG
achieves superior performance compared to state-of-the-art RAG baselines in
both textual quality and quantification accuracy of opinions. Our source code
is available at: https://github.com/antangrocket1312/QQSUMM

</details>


### [96] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/pdf/2506.04098)
*Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin*

Main category: cs.CL

TL;DR: TextAtari is a benchmark for evaluating language agents on long-horizon decision-making tasks using textual descriptions of Atari games. It tests models like Qwen2.5-7B, Gemma-7B, and Llama3.1-8B across various agent frameworks, revealing performance gaps compared to humans.


<details>
  <summary>Details</summary>
Motivation: To bridge sequential decision-making and natural language processing by creating a challenging benchmark for language agents.

Method: Translates Atari game visuals into text, evaluates models across 100 tasks using unsupervised representation learning (AtariARI), and tests three agent frameworks.

Result: Significant performance gaps between language agents and humans in long-horizon tasks, highlighting challenges in reasoning and planning.

Conclusion: TextAtari offers a standardized framework to advance research in language models and planning.

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning.

</details>


### [97] [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/pdf/2506.04032)
*Sina Rashidian, Nan Li, Jonathan Amar, Jong Ha Lee, Sam Pugh, Eric Yang, Geoff Masterson, Myoung Cha, Yugang Jia, Akhil Vaid*

Main category: cs.CL

TL;DR: A Patient Simulator using real EHR data was developed to train AI health agents, validated by clinicians with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a realistic synthetic test environment for AI health agents using real patient data.

Method: Derived patient vignettes from EHR data, simulated multi-turn conversations, and evaluated with expert clinicians.

Result: 97.7% consistency with vignettes and 99% relevance in case summaries.

Conclusion: The simulator effectively trains AI agents for healthcare conversations at scale.

Abstract: Background: We present a Patient Simulator that leverages real world patient
encounters which cover a broad range of conditions and symptoms to provide
synthetic test subjects for development and testing of healthcare agentic
models. The simulator provides a realistic approach to patient presentation and
multi-turn conversation with a symptom-checking agent. Objectives: (1) To
construct and instantiate a Patient Simulator to train and test an AI health
agent, based on patient vignettes derived from real EHR data. (2) To test the
validity and alignment of the simulated encounters provided by the Patient
Simulator to expert human clinical providers. (3) To illustrate the evaluation
framework of such an LLM system on the generated realistic, data-driven
simulations -- yielding a preliminary assessment of our proposed system.
Methods: We first constructed realistic clinical scenarios by deriving patient
vignettes from real-world EHR encounters. These vignettes cover a variety of
presenting symptoms and underlying conditions. We then evaluate the performance
of the Patient Simulator as a simulacrum of a real patient encounter across
over 500 different patient vignettes. We leveraged a separate AI agent to
provide multi-turn questions to obtain a history of present illness. The
resulting multiturn conversations were evaluated by two expert clinicians.
Results: Clinicians scored the Patient Simulator as consistent with the patient
vignettes in those same 97.7% of cases. The extracted case summary based on the
conversation history was 99% relevant. Conclusions: We developed a methodology
to incorporate vignettes derived from real healthcare patient data to build a
simulation of patient responses to symptom checking agents. The performance and
alignment of this Patient Simulator could be used to train and test a
multi-turn conversational AI agent at scale.

</details>


### [98] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/pdf/2506.04131)
*Disha Sheshanarayana, Tanishka Magar, Ayushi Mittal, Neelam Chaplot*

Main category: cs.CL

TL;DR: The paper introduces LegalCon, a dataset for detecting manipulation in courtroom conversations, and CLAIM, a framework to analyze manipulation, aiming to improve fairness in legal processes.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored application of NLP in detecting manipulation in legal jargon, which can influence judicial decisions.

Method: Developed LegalCon (annotated dataset) and CLAIM (two-stage, intent-driven multi-agent framework) for manipulation analysis.

Result: CLAIM enhances context-aware manipulation analysis, improving fairness and transparency in judicial processes.

Conclusion: The work advances NLP in legal discourse and supports fairness in legal decision-making; dataset and code are publicly available.

Abstract: Courtrooms are places where lives are determined and fates are sealed, yet
they are not impervious to manipulation. Strategic use of manipulation in legal
jargon can sway the opinions of judges and affect the decisions. Despite the
growing advancements in NLP, its application in detecting and analyzing
manipulation within the legal domain remains largely unexplored. Our work
addresses this gap by introducing LegalCon, a dataset of 1,063 annotated
courtroom conversations labeled for manipulation detection, identification of
primary manipulators, and classification of manipulative techniques, with a
focus on long conversations. Furthermore, we propose CLAIM, a two-stage,
Intent-driven Multi-agent framework designed to enhance manipulation analysis
by enabling context-aware and informed decision-making. Our results highlight
the potential of incorporating agentic frameworks to improve fairness and
transparency in judicial processes. We hope that this contributes to the
broader application of NLP in legal discourse analysis and the development of
robust tools to support fairness in legal decision-making. Our code and data
are available at https://github.com/Disha1001/CLAIM.

</details>


### [99] [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/pdf/2506.04041)
*Claire Barale, Leslie Barrett, Vikram Sunil Bajaj, Michael Rovatsos*

Main category: cs.CL

TL;DR: LexTime is a new dataset for evaluating LLMs' event ordering in legal texts, showing improved accuracy in legal contexts but challenges with linguistic complexities.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack expert evaluation of legal language, creating a gap in understanding LLMs' temporal reasoning in legal contexts.

Method: Introduces LexTime, a dataset of 512 annotated event pairs from U.S. Federal Complaints, analyzing LLMs' performance on legal event ordering.

Result: LLMs perform better on legal event ordering (+10.5%), with longer contexts and implicit events boosting accuracy to 80.8%. Legal complexities remain challenging.

Conclusion: Legal-specific modeling strategies are needed to improve temporal event reasoning in LLMs, addressing context length and linguistic features.

Abstract: Temporal reasoning in legal texts is important for applications like case law
analysis and compliance monitoring. However, existing datasets lack expert
language evaluation, leaving a gap in understanding how LLMs manage event
ordering in legal contexts. We introduce LexTime, the first dataset designed to
evaluate LLMs' event ordering capabilities in legal language, consisting of 512
instances from U.S. Federal Complaints with annotated event pairs and their
temporal relations. Our findings show that (1) LLMs are more accurate on legal
event ordering than on narrative (up to +10.5%); (2) longer input contexts and
implicit events boost accuracy, reaching 80.8% for implicit-explicit event
pairs; (3) legal linguistic complexities and nested clauses remain a challenge.
We investigate how context length, explicit vs implicit event pairs, and legal
language features affect model performance, demonstrating the need for specific
modeling strategies to enhance temporal event reasoning.

</details>


### [100] [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/pdf/2506.04042)
*Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Ji Xiang, Weiping Wang*

Main category: cs.CL

TL;DR: The paper addresses the issue of uncontrollable knowledge editing in large language models due to shortcut learning, proposing a two-stage optimization process to balance subject and relation features for better control.


<details>
  <summary>Details</summary>
Motivation: Current locate-then-edit methods for knowledge editing in language models often modify unrelated knowledge due to shortcut learning, leading to uncontrollable outcomes.

Method: A novel two-stage optimization process is introduced to balance the learning of subject and relation features, preventing over-learning of the subject feature.

Result: The proposed method successfully avoids shortcut learning and achieves optimal performance in controllable knowledge editing.

Conclusion: The two-stage optimization process effectively balances feature learning, enabling more controllable and precise knowledge editing in language models.

Abstract: Knowledge editing aims to alternate the target knowledge predicted by large
language models while ensuring the least side effects on unrelated knowledge.
An effective way to achieve knowledge editing is to identify pivotal parameters
for predicting factual associations and modify them with an optimization
process to update the predictions. However, these locate-then-edit methods are
uncontrollable since they tend to modify most unrelated relations connected to
the subject of target editing. We unveil that this failure of controllable
editing is due to a shortcut learning issue during the optimization process.
Specifically, we discover two crucial features that are the subject feature and
the relation feature for models to learn during optimization, but the current
optimization process tends to over-learning the subject feature while
neglecting the relation feature. To eliminate this shortcut learning of the
subject feature, we propose a novel two-stage optimization process that
balances the learning of the subject feature and the relation feature.
Experimental results demonstrate that our approach successfully prevents
knowledge editing from shortcut learning and achieves the optimal overall
performance, contributing to controllable knowledge editing.

</details>


### [101] [On Support Samples of Next Word Prediction](https://arxiv.org/pdf/2506.04047)
*Yuqian Li, Yupei Du, Yufang Liu, Feifei Feng, Mou Xiao Feng, Yuanbin Wu*

Main category: cs.CL

TL;DR: The paper explores data-centric interpretability in language models, identifying support samples that influence predictions and highlighting the role of non-support samples in preventing overfitting and shaping learning.


<details>
  <summary>Details</summary>
Motivation: Understanding the rationale behind language model decisions is challenging, prompting a focus on data-centric interpretability.

Method: The study uses the representer theorem to identify support samples (promoting or deterring predictions) and analyzes their impact.

Result: Support samples are intrinsic and predictable pre-training, while non-support samples are crucial for preventing overfitting and representation learning, especially in deeper layers.

Conclusion: The findings provide insights into data-model interactions, enhancing interpretability of language model behavior.

Abstract: Language models excel in various tasks by making complex decisions, yet
understanding the rationale behind these decisions remains a challenge. This
paper investigates \emph{data-centric interpretability} in language models,
focusing on the next-word prediction task. Using representer theorem, we
identify two types of \emph{support samples}-those that either promote or deter
specific predictions. Our findings reveal that being a support sample is an
intrinsic property, predictable even before training begins. Additionally,
while non-support samples are less influential in direct predictions, they play
a critical role in preventing overfitting and shaping generalization and
representation learning. Notably, the importance of non-support samples
increases in deeper layers, suggesting their significant role in intermediate
representation formation.These insights shed light on the interplay between
data and model decisions, offering a new dimension to understanding language
model behavior and interpretability.

</details>


### [102] [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/pdf/2506.04065)
*Muling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu Huang, Di Liang, LI Miao, Shihan Dou, Changze Lv, Zhenghua Wang, Zhibo Xu, Lina Chen, Tianlong Li, Xiaoqing Zheng, Xuanjing Huang*

Main category: cs.CL

TL;DR: Customized Curriculum Learning (CCL) improves LLM training by adapting difficulty to model capabilities and using guided prompting for better sample utilization, outperforming uniform training on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in post-training LLMs due to rigid difficulty metrics and poor sample utilization.

Method: Introduces model-adaptive difficulty definition and Guided Prompting to dynamically adjust sample difficulty.

Result: CCL outperforms uniform training on five mathematical reasoning benchmarks.

Conclusion: CCL enhances sample utilization and model performance in both supervised fine-tuning and reinforcement learning.

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
various reasoning tasks, yet post-training is constrained by inefficient sample
utilization and inflexible difficulty samples processing. To address these
limitations, we propose Customized Curriculum Learning (CCL), a novel framework
with two key innovations. First, we introduce model-adaptive difficulty
definition that customizes curriculum datasets based on each model's individual
capabilities rather than using predefined difficulty metrics. Second, we
develop "Guided Prompting," which dynamically reduces sample difficulty through
strategic hints, enabling effective utilization of challenging samples that
would otherwise degrade performance. Comprehensive experiments on supervised
fine-tuning and reinforcement learning demonstrate that CCL significantly
outperforms uniform training approaches across five mathematical reasoning
benchmarks, confirming its effectiveness across both paradigms in enhancing
sample utilization and model performance.

</details>


### [103] [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/pdf/2506.04072)
*Meiqing Jin, Liam Dugan, Chris Callison-Burch*

Main category: cs.CL

TL;DR: Controllable generation techniques, like future discriminators, can adapt LLM outputs for beginner language learners, improving comprehensibility from 40.4% to 84.3%. A new metric, Token Miss Rate (TMR), is introduced to evaluate output difficulty.


<details>
  <summary>Details</summary>
Motivation: LLMs generate text at near-native complexity, making them unsuitable for beginner learners (CEFR: A1-A2). The paper explores methods to adapt LLM outputs for absolute beginners without fine-tuning.

Method: Modular controllable generation techniques, including future discriminators, are tested. Evaluation involves automatic metrics and a user study with Japanese learners.

Result: Prompting alone fails, but future discriminators significantly improve comprehensibility (40.4% to 84.3%). TMR is introduced as a strong metric for evaluating output difficulty.

Conclusion: Controllable generation techniques effectively adapt LLM outputs for beginners. The paper releases tools and data to support future AI-assisted language learning research.

Abstract: Practicing conversations with large language models (LLMs) presents a
promising alternative to traditional in-person language learning. However, most
LLMs generate text at a near-native level of complexity, making them ill-suited
for beginner learners (CEFR: A1-A2). In this paper, we investigate whether
controllable generation techniques -- specifically modular methods that do not
require model fine-tuning -- can adapt LLM outputs to better support absolute
beginners. We evaluate these methods through both automatic metrics and a user
study with university-level learners of Japanese. Our findings show that while
prompting alone fails to control output difficulty, the use of future
discriminators (Yang and Klein, 2021) significantly improves output
comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel
token-level evaluation metric, Token Miss Rate (TMR), that quantifies the
proportion of incomprehensible tokens per utterance and correlates strongly
with human judgments. To support future research in AI-assisted language
learning, we release our code, models, annotation tools, and dataset.

</details>


### [104] [Rectified Sparse Attention](https://arxiv.org/pdf/2506.04108)
*Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei*

Main category: cs.CL

TL;DR: ReSA combines block-sparse attention with periodic dense rectification to improve long-sequence generation efficiency while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Addressing KV cache misalignment in sparse decoding methods, which degrades generation quality.

Method: Proposes Rectified Sparse Attention (ReSA), blending block-sparse attention with periodic dense rectification to refresh the KV cache.

Result: Achieves near-lossless quality with up to 2.42× speedup at 256K sequence length.

Conclusion: ReSA is a practical solution for efficient long-context inference.

Abstract: Efficient long-sequence generation is a critical challenge for Large Language
Models. While recent sparse decoding methods improve efficiency, they suffer
from KV cache misalignment, where approximation errors accumulate and degrade
generation quality. In this work, we propose Rectified Sparse Attention (ReSA),
a simple yet effective method that combines block-sparse attention with
periodic dense rectification. By refreshing the KV cache at fixed intervals
using a dense forward pass, ReSA bounds error accumulation and preserves
alignment with the pretraining distribution. Experiments across math reasoning,
language modeling, and retrieval tasks demonstrate that ReSA achieves
near-lossless generation quality with significantly improved efficiency.
Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at
256K sequence length, making it a practical solution for scalable long-context
inference. Code is available at https://aka.ms/ReSA-LM.

</details>


### [105] [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/pdf/2506.04139)
*Ratna Kandala, Katie Hoemann*

Main category: cs.CL

TL;DR: The study evaluates Dutch-specific LLMs for predicting emotional valence in Flemish narratives, finding them less accurate than traditional tools like LIWC and Pattern. It highlights the need for culturally tailored models.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capabilities in capturing emotional valence in spontaneous, context-dependent Flemish narratives, advancing computational linguistics and emotion research.

Method: Analyzed 25,000 textual responses from 102 Dutch-speaking participants, comparing Dutch LLMs' valence predictions to LIWC and Pattern.

Result: Dutch-tuned LLMs underperform in accurately predicting emotional valence compared to traditional tools, emphasizing the need for better culturally tailored models.

Conclusion: Advocates for developing comprehensive datasets and fine-tuning LLMs for low-resource languages like Flemish to improve valence analysis and bridge gaps in computational linguistics and emotion research.

Abstract: Understanding the nuances in everyday language is pivotal for advancements in
computational linguistics & emotions research. Traditional lexicon-based tools
such as LIWC and Pattern have long served as foundational instruments in this
domain. LIWC is the most extensively validated word count based text analysis
tool in the social sciences and Pattern is an open source Python library
offering functionalities for NLP. However, everyday language is inherently
spontaneous, richly expressive, & deeply context dependent. To explore the
capabilities of LLMs in capturing the valences of daily narratives in Flemish,
we first conducted a study involving approximately 25,000 textual responses
from 102 Dutch-speaking participants. Each participant provided narratives
prompted by the question, "What is happening right now and how do you feel
about it?", accompanied by self-assessed valence ratings on a continuous scale
from -50 to +50. We then assessed the performance of three Dutch-specific LLMs
in predicting these valence scores, and compared their outputs to those
generated by LIWC and Pattern. Our findings indicate that, despite advancements
in LLM architectures, these Dutch tuned models currently fall short in
accurately capturing the emotional valence present in spontaneous, real-world
narratives. This study underscores the imperative for developing culturally and
linguistically tailored models/tools that can adeptly handle the complexities
of natural language use. Enhancing automated valence analysis is not only
pivotal for advancing computational methodologies but also holds significant
promise for psychological research with ecologically valid insights into human
daily experiences. We advocate for increased efforts in creating comprehensive
datasets & finetuning LLMs for low-resource languages like Flemish, aiming to
bridge the gap between computational linguistics & emotion research.

</details>


### [106] [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/pdf/2506.04142)
*Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao*

Main category: cs.CL

TL;DR: The paper addresses data contamination in LLM evaluations by analyzing contaminated models, identifying shortcut neurons, and proposing a patching method to mitigate contamination, validated by strong correlation with a trustworthy benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations rely on public benchmarks prone to data contamination, compromising fairness. Building new benchmarks is costly, so the authors aim to tackle contamination by analyzing the models themselves.

Method: The authors identify shortcut neurons through comparative and causal analysis and propose shortcut neuron patching to suppress them.

Result: Experiments show the method effectively mitigates contamination, with evaluation results strongly correlating (Spearman ρ > 0.95) with MixEval, a trustworthy benchmark.

Conclusion: The proposed method reliably reveals true model capabilities and generalizes across benchmarks and hyperparameter settings, offering a trustworthy evaluation approach.

Abstract: The development of large language models (LLMs) depends on trustworthy
evaluation. However, most current evaluations rely on public benchmarks, which
are prone to data contamination issues that significantly compromise fairness.
Previous researches have focused on constructing dynamic benchmarks to address
contamination. However, continuously building new benchmarks is costly and
cyclical. In this work, we aim to tackle contamination by analyzing the
mechanisms of contaminated models themselves. Through our experiments, we
discover that the overestimation of contaminated models is likely due to
parameters acquiring shortcut solutions in training. We further propose a novel
method for identifying shortcut neurons through comparative and causal
analysis. Building on this, we introduce an evaluation method called shortcut
neuron patching to suppress shortcut neurons. Experiments validate the
effectiveness of our approach in mitigating contamination. Additionally, our
evaluation results exhibit a strong linear correlation with MixEval, a recently
released trustworthy benchmark, achieving a Spearman coefficient ($\rho$)
exceeding 0.95. This high correlation indicates that our method closely reveals
true capabilities of the models and is trustworthy. We conduct further
experiments to demonstrate the generalizability of our method across various
benchmarks and hyperparameter settings. Code:
https://github.com/GaryStack/Trustworthy-Evaluation

</details>


### [107] [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/pdf/2506.04156)
*Sarvesh Soni, Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ArchEHR-QA is a new expert-annotated dataset for evaluating AI-generated responses to patient questions about EHRs, with benchmarks for LLMs showing answer-first prompting works best.


<details>
  <summary>Details</summary>
Motivation: Addressing patient information needs from EHRs requires robust datasets to evaluate AI systems, which are currently lacking.

Method: Created ArchEHR-QA using real-world ICU/ED cases, annotated questions, clinical notes, and answers. Evaluated three LLMs with three prompting strategies.

Result: Answer-first prompting performed best, with Llama 4 scoring highest. Common issues included omitted evidence and hallucinations.

Conclusion: ArchEHR-QA provides a benchmark for patient-centered EHR QA, highlighting the need for improved factual and relevant AI responses.

Abstract: Patients have distinct information needs about their hospitalization that can
be addressed using clinical evidence from electronic health records (EHRs).
While artificial intelligence (AI) systems show promise in meeting these needs,
robust datasets are needed to evaluate the factual accuracy and relevance of
AI-generated responses. To our knowledge, no existing dataset captures patient
information needs in the context of their EHRs. We introduce ArchEHR-QA, an
expert-annotated dataset based on real-world patient cases from intensive care
unit and emergency department settings. The cases comprise questions posed by
patients to public health forums, clinician-interpreted counterparts, relevant
clinical note excerpts with sentence-level relevance annotations, and
clinician-authored answers. To establish benchmarks for grounded EHR question
answering (QA), we evaluated three open-weight large language models
(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:
generating (1) answers with citations to clinical note sentences, (2) answers
before citations, and (3) answers from filtered citations. We assessed
performance on two dimensions: Factuality (overlap between cited note sentences
and ground truth) and Relevance (textual and semantic similarity between system
and reference answers). The final dataset contains 134 patient cases. The
answer-first prompting approach consistently performed best, with Llama 4
achieving the highest scores. Manual error analysis supported these findings
and revealed common issues such as omitted key clinical evidence and
contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong
benchmark for developing and evaluating patient-centered EHR QA systems,
underscoring the need for further progress toward generating factual and
relevant responses in clinical contexts.

</details>


### [108] [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/pdf/2506.04179)
*Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen*

Main category: cs.CL

TL;DR: SkipGPT introduces dynamic layer pruning for LLMs, optimizing efficiency by addressing token-level and layer-specific dynamics, reducing parameters by 40% without performance loss.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs of LLMs by addressing inefficiencies in static pruning methods, which ignore token-level and layer-specific dynamics.

Method: SkipGPT uses global token-aware routing and decoupled pruning policies for MLP and self-attention layers, with a two-stage optimization for stability.

Result: SkipGPT reduces over 40% of model parameters while matching or exceeding original model performance.

Conclusion: SkipGPT enables scalable, resource-efficient LLM deployment by balancing dynamic efficiency and preserved expressivity.

Abstract: Large language models (LLMs) achieve remarkable performance across tasks but
incur substantial computational costs due to their deep, multi-layered
architectures. Layer pruning has emerged as a strategy to alleviate these
inefficiencies, but conventional static pruning methods overlook two critical
dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level
heterogeneity demands context-aware pruning decisions, and (2) vertical
dynamics, where the distinct functional roles of MLP and self-attention layers
necessitate component-specific pruning policies. We introduce SkipGPT, a
dynamic layer pruning framework designed to optimize computational resource
allocation through two core innovations: (1) global token-aware routing to
prioritize critical tokens, and (2) decoupled pruning policies for MLP and
self-attention components. To mitigate training instability, we propose a
two-stage optimization paradigm: first, a disentangled training phase that
learns routing strategies via soft parameterization to avoid premature pruning
decisions, followed by parameter-efficient LoRA fine-tuning to restore
performance impacted by layer removal. Extensive experiments demonstrate that
SkipGPT reduces over 40% of model parameters while matching or exceeding the
performance of the original dense model across benchmarks. By harmonizing
dynamic efficiency with preserved expressivity, SkipGPT advances the practical
deployment of scalable, resource-aware LLMs. Our code is publicly available at:
https://github.com/EIT-NLP/SkipGPT.

</details>


### [109] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/pdf/2506.04226)
*Akshat Gupta, Maochuan Lu, Thomas Hartvigsen, Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: The paper reduces the excessive precomputation cost in knowledge editing methods like MEMIT by showing that only a tiny fraction of hidden vectors (less than 0.3%) is needed, saving significant time.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of precomputing hidden vectors for knowledge editing methods like MEMIT is impractical, especially as model sizes grow.

Method: The authors theoretically determine the minimum required hidden vectors and empirically demonstrate editing with less than 0.3% of the original precomputation.

Result: Knowledge editing can be performed with drastically fewer hidden vectors, reducing precomputation time from hours to minutes.

Conclusion: The excessive precomputation in knowledge editing is unnecessary, and significant efficiency gains are achievable with minimal hidden vectors.

Abstract: Knowledge editing methods like MEMIT are able to make data and compute
efficient updates of factual knowledge by using a single sentence to update
facts and their consequences. However, what is often overlooked is a
"precomputation step", which requires a one-time but significant computational
cost. The authors of MEMIT originally precompute approximately 44 million
hidden vectors per edited layer, which requires a forward pass over 44 million
tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single
GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this
precomputation time grows with model size. In this paper, we show that this
excessive computational cost is unnecessary. Knowledge editing using MEMIT and
related methods, such as ROME and EMMET, can be performed by pre-computing a
very small portion of the 44 million hidden vectors. We first present the
theoretical minimum number of hidden vector precomputation required for
solutions of these editing methods to exist. We then empirically show that
knowledge editing using these methods can be done by pre-computing
significantly fewer hidden vectors. Specifically, we show that the
precomputation step can be done with less than 0.3% of the originally
stipulated number of hidden vectors. This saves a significant amount of
precomputation time and allows users to begin editing new models within a few
minutes.

</details>


### [110] [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/pdf/2506.04180)
*Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: SuperWriter-Agent, an agent-based framework, improves long-form text generation by incorporating structured thinking and hierarchical DPO, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in long-form text generation like coherence and consistency as sequence length increases.

Method: Proposes SuperWriter-Agent with structured thinking (planning/refinement), supervised fine-tuning of a 7B model, and hierarchical DPO with MCTS.

Result: Achieves state-of-the-art performance in benchmarks, surpassing larger models in automatic and human evaluations.

Conclusion: Structured thinking and hierarchical DPO significantly enhance long-form text generation quality.

Abstract: Long-form text generation remains a significant challenge for large language
models (LLMs), particularly in maintaining coherence, ensuring logical
consistency, and preserving text quality as sequence length increases. To
address these limitations, we propose SuperWriter-Agent, an agent-based
framework designed to enhance the quality and consistency of long-form text
generation. SuperWriter-Agent introduces explicit structured thinking-through
planning and refinement stages into the generation pipeline, guiding the model
to follow a more deliberate and cognitively grounded process akin to that of a
professional writer. Based on this framework, we construct a supervised
fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a
hierarchical Direct Preference Optimization (DPO) procedure that uses Monte
Carlo Tree Search (MCTS) to propagate final quality assessments and optimize
each generation step accordingly. Empirical results across diverse benchmarks
demonstrate that SuperWriter-LM achieves state-of-the-art performance,
surpassing even larger-scale baseline models in both automatic evaluation and
human evaluation. Furthermore, comprehensive ablation studies demonstrate the
effectiveness of hierarchical DPO and underscore the value of incorporating
structured thinking steps to improve the quality of long-form text generation.

</details>


### [111] [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/pdf/2506.04182)
*Ruiqi Zhang, Changyi Xiao, Yixin Cao*

Main category: cs.CL

TL;DR: The paper compares long and short Chain-of-Thought (CoT) prompting, finding long CoT marginally better but costly. It proposes SwitchCoT, a dynamic framework that adapts to resource constraints, reducing costs by 50% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high token usage of long CoT prompting and explore its trade-offs with short CoT, aiming for a balanced solution.

Method: Empirical analysis of long vs. short CoT, followed by the development of SwitchCoT, a budget-aware framework for adaptive strategy selection.

Result: SwitchCoT reduces inference costs by up to 50% and matches or exceeds the performance of standalone long or short CoT under budget constraints.

Conclusion: Dynamic CoT strategy selection (SwitchCoT) optimizes accuracy and efficiency, making it suitable for varied resource scenarios.

Abstract: With the rapid advancement of large reasoning models, long Chain-of-Thought
(CoT) prompting has demonstrated strong performance on complex tasks. However,
this often comes with a significant increase in token usage. In this paper, we
conduct a comprehensive empirical analysis comparing long and short CoT
strategies. Our findings reveal that while long CoT can lead to performance
improvements, its benefits are often marginal relative to its significantly
higher token consumption. Specifically, long CoT tends to outperform when ample
generation budgets are available, whereas short CoT is more effective under
tighter budget constraints. These insights underscore the need for a dynamic
approach that selects the proper CoT strategy based on task context and
resource availability. To address this, we propose SwitchCoT, an automatic
framework that adaptively chooses between long and short CoT strategies to
balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is
designed to be budget-aware, making it broadly applicable across scenarios with
varying resource constraints. Experimental results demonstrate that SwitchCoT
can reduce inference costs by up to 50% while maintaining high accuracy.
Notably, under limited token budgets, it achieves performance comparable to, or
even exceeding, that of using either long or short CoT alone.

</details>


### [112] [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/pdf/2506.04185)
*Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu*

Main category: cs.CL

TL;DR: R-Search is a reinforcement learning framework that enhances LLMs' reasoning by integrating deep search interactions, improving response quality in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with optimal reasoning-search interaction trajectories, leading to suboptimal responses.

Method: Proposes R-Search, a reinforcement learning framework with multi-reward signals to guide LLMs in dynamically deciding when to retrieve or reason.

Result: Outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain) on seven datasets.

Conclusion: R-Search effectively improves LLMs' reasoning and search integration, enhancing performance in logic- and knowledge-intensive tasks.

Abstract: Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.

</details>


### [113] [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/pdf/2308.09583)
*Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang*

Main category: cs.CL

TL;DR: WizardMath enhances LLMs' mathematical reasoning using RLEIF, outperforming top models like GPT-4 on benchmarks GSM8k and MATH.


<details>
  <summary>Details</summary>
Motivation: Existing open-source LLMs lack math-specific optimization despite strong NLP performance.

Method: Uses Reinforcement Learning from Evol-Instruct Feedback (RLEIF) for math reasoning without external tools.

Result: WizardMath-Mistral 7B surpasses top open-source LLMs; WizardMath 70B beats GPT-3.5-Turbo, Claude 2, Gemini Pro, and GPT-4-early-version.

Conclusion: Instruction evolution and process supervision are key to exceptional math performance in LLMs.

Abstract: Large language models (LLMs), such as GPT-4, have shown remarkable
performance in natural language processing (NLP) tasks, including challenging
mathematical reasoning. However, most existing open-source models are only
pre-trained on large-scale internet data and without math-related optimization.
In this paper, we present WizardMath, which enhances the mathematical CoT
reasoning abilities of LLMs without using external python tools, by applying
our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method
to the domain of math. Through extensive experiments on two mathematical
reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary
capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier
open-source LLMs by a substantial margin with higher data efficiency.
Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini
Pro and GPT-4-early-version. Additionally, our preliminary exploration
highlights the pivotal role of instruction evolution and process supervision in
achieving exceptional math performance. For more details refer to
https://github.com/nlpxucan/WizardLM

</details>


### [114] [Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models](https://arxiv.org/pdf/2310.12049)
*Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing*

Main category: cs.CL

TL;DR: A text scoring framework using generative LLMs (CGCoT) improves accuracy for short texts without needing large labeled datasets, outperforming traditional methods like Wordfish and matching supervised models like RoBERTa-Large.


<details>
  <summary>Details</summary>
Motivation: Existing text scoring methods are limited by corpus size, short text handling, or reliance on labeled data. The goal is to leverage LLMs for better performance.

Method: CGCoT uses researcher-designed prompts with LLMs to generate concept-specific breakdowns, compares them pairwise, and aggregates scores via a probability model.

Result: CGCoT achieves higher correlation with human judgments than unsupervised methods (e.g., Wordfish) and matches supervised models (e.g., RoBERTa-Large) with minimal labeled data.

Conclusion: Combining human expertise with LLMs (CGCoT) shows promise for text scoring tasks, especially for short texts and limited labeled data.

Abstract: Existing text scoring methods require a large corpus, struggle with short
texts, or require hand-labeled data. We develop a text scoring framework that
leverages generative large language models (LLMs) to (1) set texts against the
backdrop of information from the near-totality of the web and digitized media,
and (2) effectively transform pairwise text comparisons from a reasoning
problem to a pattern recognition task. Our approach, concept-guided
chain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with
an LLM to generate a concept-specific breakdown for each text, akin to guidance
provided to human coders. We then pairwise compare breakdowns using an LLM and
aggregate answers into a score using a probability model. We apply this
approach to better understand speech reflecting aversion to specific political
parties on Twitter, a topic that has commanded increasing interest because of
its potential contributions to democratic backsliding. We achieve stronger
correlations with human judgments than widely used unsupervised text scoring
methods like Wordfish. In a supervised setting, besides a small pilot dataset
to develop CGCoT prompts, our measures require no additional hand-labeled data
and produce predictions on par with RoBERTa-Large fine-tuned on thousands of
hand-labeled tweets. This project showcases the potential of combining human
expertise and LLMs for scoring tasks.

</details>


### [115] [CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks](https://arxiv.org/pdf/2406.02524)
*Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler*

Main category: cs.CL

TL;DR: CheckEmbed (CE) is a scalable and accurate method for verifying LLM outputs by reducing answers to embedding vectors, outperforming prior methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Verifying LLM outputs for complex tasks is challenging; existing methods like BERTScore and SelfCheckGPT have limitations in granularity and scalability.

Method: CE uses modern embedding models (e.g., SFR-Embedding-Mistral) to convert LLM answers into single embedding vectors for fast, semantic comparisons at the whole-answer level.

Result: CE outperforms 13 baselines in accuracy and scalability, reliably detecting hallucinations in various tasks and generalizing to other modalities like vision.

Conclusion: CE is a practical, versatile framework for verifying LLM outputs, addressing key limitations of prior methods.

Abstract: Large Language Models (LLMs) are transforming a wide range of domains, yet
verifying their outputs remains a significant challenge, especially for complex
open-ended tasks such as consolidation, summarization, and knowledge
extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,
and accurate verification method. CE reduces each LLM answer to a single
embedding vector using powerful modern embedding LLM models like
SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied
on weaker encoders like BERT, forcing them to operate at token or sentence
granularity. In contrast, CE performs fast, semantically rich comparisons
directly at the whole-answer level, overcoming key limitations in both accuracy
and scalability. We conduct a comprehensive design and time complexity analysis
across 13 verification baselines, including classical text scorers (e.g.,
BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators
(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,
versatility, and simplicity of CE. Empirical results show that CE reliably
detects hallucinations in both closed and open-ended tasks. We further present
evidence that CE generalizes beyond text to other modalities such as vision,
establishing it as a practical and versatile verification framework.

</details>


### [116] [AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models](https://arxiv.org/pdf/2406.09295)
*Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong*

Main category: cs.CL

TL;DR: AlignMMBench is a new benchmark for evaluating Vision-Language Models (VLMs) in Chinese visual contexts, addressing gaps in nuanced alignment assessments. It includes diverse tasks, images, and question-answer pairs, with a rule-calibrated evaluator (CritiqueVLM) and a quantitative "alignment score."


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack nuanced evaluation of VLMs' alignment capabilities, especially for Chinese visual contexts.

Method: Introduces AlignMMBench with real-world scenarios, 13 tasks, 1,054 images, and 4,978 QA pairs. Uses CritiqueVLM for evaluation and an "alignment score" metric.

Result: Evaluates representative VLMs, revealing their capabilities and limitations.

Conclusion: AlignMMBench fills a critical gap in VLM evaluation, offering a robust framework for nuanced alignment assessment in Chinese contexts.

Abstract: Evaluating the alignment capabilities of large Vision-Language Models (VLMs)
is essential for determining their effectiveness as helpful assistants.
However, existing benchmarks primarily focus on basic abilities using nonverbal
methods, such as yes-no and multiple-choice questions. In this paper, we
address this gap by introducing AlignMMBench, which provides more nuanced
evaluations of alignment capabilities and is the first benchmark specifically
designed for Chinese visual contexts. This benchmark is meticulously curated
from real-world scenarios and internet sources, encompassing thirteen specific
tasks across three categories, and includes both single-turn and multi-turn
dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench
encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the
evaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that
exceeds GPT-4's evaluation ability. Additionally, we measure the "alignment
score", a quantitative metric designed to assess the robustness and stability
of models across diverse prompts. Finally, we evaluate the performance of
representative VLMs on AlignMMBench, offering insights into the capabilities
and limitations of different VLM architectures. The evaluation code and data
are available at https://github.com/THUDM/AlignMMBench.

</details>


### [117] [UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](https://arxiv.org/pdf/2406.12784)
*Xunzhi Wang, Zhuowei Zhang, Gaonan Chen, Qiongyu Li, Bitong Luo, Zhixin Han, Haotian Wang, Zhiyu li, Hang Gao, Mengting Hu*

Main category: cs.CL

TL;DR: UBench is a new benchmark for evaluating LLM uncertainty using confidence intervals, addressing challenges like internal model access and high costs. It tests 20 LLMs, revealing insights on methods, open-source competitiveness, and prompt effects.


<details>
  <summary>Details</summary>
Motivation: Existing methods for benchmarking LLM uncertainty are impractical for closed-source models due to requirements like internal access or training. UBench aims to provide a more accessible solution.

Method: UBench uses confidence intervals and includes 11,978 multiple-choice questions across knowledge, language, understanding, and reasoning. It compares uncertainty methods and evaluates 20 LLMs.

Result: Confidence interval-based methods are effective. Open-source models compete with closed-source ones. CoT and RP prompts improve reliability, while temperature effects vary.

Conclusion: UBench offers a practical, effective way to benchmark LLM uncertainty, highlighting the potential of open-source models and specific prompts for reliability.

Abstract: Despite recent progress in systematic evaluation frameworks, benchmarking the
uncertainty of large language models (LLMs) remains a highly challenging task.
Existing methods for benchmarking the uncertainty of LLMs face three key
challenges: the need for internal model access, additional training, or high
computational costs. This is particularly unfavorable for closed-source models.
To this end, we introduce UBench, a new benchmark for evaluating the
uncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence
intervals. It encompasses 11,978 multiple-choice questions spanning knowledge,
language, understanding, and reasoning capabilities. Based on this, we conduct
extensive experiments. This includes comparisons with other advanced
uncertainty estimation methods, the assessment of the uncertainty of 20 LLMs,
and an exploration of the effects of Chain-of-Thought (CoT) prompts,
role-playing (RP) prompts, and temperature on model uncertainty. Our analysis
reveals several crucial insights: 1) Our confidence interval-based methods are
highly effective for uncertainty quantification; 2) Regarding uncertainty,
outstanding open-source models show competitive performance versus
closed-source models; 3) CoT and RP prompts present potential ways to improve
model reliability, while the influence of temperature changes follows no
universal rule. Our implementation is available at
https://github.com/Cyno2232/UBENCH.

</details>


### [118] [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/pdf/2406.18173)
*Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji*

Main category: cs.CL

TL;DR: UIO-LLMs optimize memory-enhanced transformers for long-context tasks, extending context windows (e.g., 4K to 100K tokens) with minimal added parameters and linear inference cost.


<details>
  <summary>Details</summary>
Motivation: Handling long texts is difficult for LLMs due to limited context windows, necessitating efficient memory and optimization techniques.

Method: UIO-LLMs use a shared encoder-decoder framework and treat transformers as RNNs, refining training with TBPTT and incremental optimization for unbiased gradients.

Result: Achieves 100K token context for Llama2-7b-chat with only 2% extra parameters and near-linear inference cost.

Conclusion: UIO-LLMs effectively address long-context challenges in LLMs with scalable and efficient optimization.

Abstract: Managing long texts is challenging for large language models (LLMs) due to
limited context window sizes. This study introduces UIO-LLMs, an unbiased
incremental optimization approach for memory-enhanced transformers under
long-context settings. We initially conceptualize the process as a streamlined
encoder-decoder framework where the weights-shared encoder and decoder
respectively encapsulate a context segment into memories and leverage these
memories to predict outputs of the subsequent segment. Subsequently, by
treating our memory-enhanced transformers as fully-connected recurrent neural
networks (RNNs), we refine the training process using the Truncated
Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative
incremental optimization techniques. These techniques not only diminish time
complexity but also address the bias in gradient computation through an
unbiased optimization process. UIO-LLMs successfully handle long context, such
as extending the context window of Llama2-7b-chat from 4K to 100K tokens with
minimal 2% additional parameters, while keeping the inference cost nearly
linear as context length increases.

</details>


### [119] [REAL: Response Embedding-based Alignment for LLMs](https://arxiv.org/pdf/2409.17169)
*Honggen Zhang, Xufeng Zhao, Igor Molybog, June Zhang*

Main category: cs.CL

TL;DR: REAL improves LLM alignment by selecting dissimilar response pairs for labeling, reducing bias and saving annotation costs.


<details>
  <summary>Details</summary>
Motivation: Human bias in response pair annotation and high costs in preference dataset construction motivate the need for a more efficient alignment method.

Method: REAL uses response embedding similarity to select less ambiguous preference pairs, operating independently of prompts in an off-policy setting.

Result: Experiments show REAL enhances alignment, reduces labeling errors, and saves up to 65% of annotators' work.

Conclusion: Focusing on distinct response pairs improves LLM alignment efficiency and reduces label errors.

Abstract: Aligning large language models (LLMs) to human preferences is a crucial step
in building helpful and safe AI tools, which usually involve training on
supervised datasets. Popular algorithms such as Direct Preference Optimization
(DPO) rely on pairs of AI-generated responses ranked according to human
annotation. The response pair annotation process might bring human bias.
Building a correct preference dataset is the costly part of the alignment
pipeline. To improve annotation efficiency and quality in the LLMs alignment,
we propose REAL: Response Embedding-based Alignment for LLMs, a strategy for
constructing a high-quality training dataset that focuses on acquiring the less
ambiguous preference pairs for labeling out of a set of response candidates.
Our selection process is based on the similarity of embedding responses
independently of prompts, which guarantees the selection process in an
off-policy setting, avoiding adaptively measuring the similarity during the
training. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF
benchmarks indicate that choosing dissimilar response pairs enhances the direct
alignment of LLMs while reducing inherited labeling errors. The model aligned
with dissimilar response pairs obtained a better margin and win rate on the
dialogue task. Our findings suggest that focusing on distinct pairs can reduce
the label error and improve LLM alignment efficiency, saving up to $65\%$ of
annotators' work.

</details>


### [120] [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/pdf/2410.01444)
*Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng*

Main category: cs.CL

TL;DR: The paper explores whether language models (LMs) reflect the simplicity of language enabled by compositionality, using intrinsic dimension (ID) of representations to measure feature complexity. It finds that dataset compositionality correlates with ID, with nonlinear and linear dimensions encoding semantic and superficial aspects, respectively.


<details>
  <summary>Details</summary>
Motivation: To understand if LMs capture the intrinsic simplicity of language, driven by compositionality, and how this is reflected in the geometric complexity of their representations.

Method: Analyzes the relationship between dataset compositionality and the intrinsic dimension (ID) of LM representations, examining learned linguistic features during training.

Result: Dataset compositionality is reflected in the ID of representations, with nonlinear and linear dimensions encoding semantic and superficial linguistic aspects, respectively.

Conclusion: LMs' representations align with linguistic compositionality, with geometric complexity revealing distinct encoding of semantic and superficial features.

Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite
lexicon can generate an unbounded number of sentences. That is, language,
though seemingly high-dimensional, can be explained using relatively few
degrees of freedom. An open question is whether contemporary language models
(LMs) reflect the intrinsic simplicity of language that is enabled by
compositionality. We take a geometric view of this problem by relating the
degree of compositionality in a dataset to the intrinsic dimension (ID) of its
representations under an LM, a measure of feature complexity. We find not only
that the degree of dataset compositionality is reflected in representations'
ID, but that the relationship between compositionality and geometric complexity
arises due to learned linguistic features over training. Finally, our analyses
reveal a striking contrast between nonlinear and linear dimensionality, showing
they respectively encode semantic and superficial aspects of linguistic
composition.

</details>


### [121] [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/pdf/2410.09300)
*Yu Fei, Yasaman Razeghi, Sameer Singh*

Main category: cs.CL

TL;DR: NUDGING is a training-free algorithm that aligns large language models (LLMs) at inference time using a small aligned model, achieving comparable or better performance than large aligned models with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Alignment of LLMs typically requires training aligned versions for each base model, which is computationally expensive. NUDGING aims to provide a cost-efficient alternative by leveraging token-level guidance from a small aligned model.

Method: NUDGING uses a small aligned model to generate nudging tokens when the base model's uncertainty is high during decoding, guiding the base model's output without additional training.

Result: NUDGING achieves zero-shot performance comparable to or surpassing large aligned models, even when using a significantly smaller aligned model (7x-14x smaller). It also enables cross-model collaboration, e.g., Gemma-2-27b with Llama-27b-chat outperforming Llama-2-70b-chat.

Conclusion: NUDGING offers a modular, cost-efficient solution for LLM alignment, reducing computational overhead while maintaining or improving performance.

Abstract: Large language models (LLMs) require alignment to effectively and safely
follow user instructions. This process necessitates training an aligned version
for every base model, resulting in significant computational overhead. In this
work, we propose NUDGING, a simple, training-free algorithm that aligns any
base model at inference time using a small aligned model. NUDGING is motivated
by recent findings that alignment primarily alters the model's behavior on a
small subset of stylistic tokens (e.g., discourse markers). We find that base
models are significantly more uncertain when generating these tokens. Building
on this insight, NUDGING employs a small aligned model to generate nudging
tokens to guide the base model's output during decoding when the base model's
uncertainty is high, with only a minor additional inference overhead. We
evaluate NUDGING across 3 model families on a diverse range of open-instruction
tasks. Without any training, nudging a large base model with a 7x-14x smaller
aligned model achieves zero-shot performance comparable to, and sometimes
surpassing, that of large aligned models. By operating at the token level,
NUDGING enables off-the-shelf collaboration between model families. For
instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat
on various tasks. Overall, our work offers a modular and cost-efficient
solution to LLM alignment. Our code and demo are available at:
https://fywalter.github.io/nudging/ .

</details>


### [122] [LoGU: Long-form Generation with Uncertainty Expressions](https://arxiv.org/pdf/2410.14309)
*Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang*

Main category: cs.CL

TL;DR: The paper introduces LoGU, a task for long-form generation with uncertainty, addressing challenges like Uncertainty Suppression and Misalignment. It proposes a refinement-based data collection framework and a two-stage training pipeline (SFT and DPO) to improve uncertainty expression, reducing hallucinations while maintaining response quality.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate factually incorrect content (hallucinations). Existing uncertainty modeling focuses on short-form QA, but real-world applications need longer responses. This work aims to improve uncertainty expression in long-form generation.

Method: A refinement-based data collection framework and two-stage training pipeline (supervised fine-tuning and direct preference optimization) are proposed. The framework refines uncertainty based on atomic claims.

Result: Experiments on three datasets show the method improves accuracy, reduces hallucinations, and maintains response comprehensiveness.

Conclusion: The proposed approach effectively addresses uncertainty challenges in long-form generation, enhancing model reliability and factual correctness.

Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities, they
still struggle with generating factually incorrect content (i.e.,
hallucinations). A promising approach to mitigate this issue is enabling models
to express uncertainty when unsure. Previous research on uncertainty modeling
has primarily focused on short-form QA, but realworld applications often
require much longer responses. In this work, we introduce the task of Long-form
Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty
Suppression, where models hesitate to express uncertainty, and Uncertainty
Misalignment, where models convey uncertainty inaccurately. To tackle these
challenges, we propose a refinement-based data collection framework and a
two-stage training pipeline. Our framework adopts a divide-and-conquer
strategy, refining uncertainty based on atomic claims. The collected data are
then used in training through supervised fine-tuning (SFT) and direct
preference optimization (DPO) to enhance uncertainty expression. Extensive
experiments on three long-form instruction following datasets show that our
method significantly improves accuracy, reduces hallucinations, and maintains
the comprehensiveness of responses.

</details>


### [123] [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/pdf/2410.16502)
*Jason Chan, Robert Gaizauskas, Zhixue Zhao*

Main category: cs.CL

TL;DR: The paper introduces RULEBREAKERS, a dataset to evaluate LLMs' ability to handle rulebreaker scenarios, finding most models, including GPT-4o, perform poorly due to over-rigid logic application and poor world knowledge utilization.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' human-like reasoning in rulebreaker scenarios, contrasting formal logic's limitations with human common sense.

Method: Creation of the RULEBREAKERS dataset and evaluation of seven LLMs, including GPT-4o, focusing on their accuracy and reasoning patterns.

Result: Most LLMs achieve mediocre accuracy, over-applying logical rules and underutilizing world knowledge, diverging from human-like reasoning.

Conclusion: Current LLMs struggle with rulebreaker scenarios, highlighting risks of relying on formal logic for improving reasoning, as it may widen the gap between LLMs and human-like reasoning.

Abstract: Formal logic enables computers to reason in natural language by representing
sentences in symbolic forms and applying rules to derive conclusions. However,
in what our study characterizes as "rulebreaker" scenarios, this method can
lead to conclusions that are typically not inferred or accepted by humans given
their common sense and factual knowledge. Inspired by works in cognitive
science, we create RULEBREAKERS, the first dataset for rigorously evaluating
the ability of large language models (LLMs) to recognize and respond to
rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven
LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on
RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules
unlike what is expected from typical human reasoners. Further analysis suggests
that this apparent failure is potentially associated with the models' poor
utilization of their world knowledge and their attention distribution patterns.
Whilst revealing a limitation of current LLMs, our study also provides a timely
counterbalance to a growing body of recent works that propose methods relying
on formal logic to improve LLMs' general reasoning capabilities, highlighting
their risk of further increasing divergence between LLMs and human-like
reasoning.

</details>


### [124] [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/pdf/2411.01747)
*Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou*

Main category: cs.CL

TL;DR: Proposes a dynamic LLM agent framework for open-ended scenarios, enabling action creation and reuse, outperforming fixed-action methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of fixed-action LLM agents in real-world, open-ended environments by enhancing flexibility and reducing human effort.

Method: Uses a framework where agents generate and execute programs in a general-purpose language, accumulating actions for reuse.

Result: Outperforms fixed-action methods, improves adaptability, and handles unforeseen edge cases effectively.

Conclusion: The dynamic framework enhances LLM agent capabilities in complex, open-ended scenarios.

Abstract: Existing LLM agent systems typically select actions from a fixed and
predefined set at every step. While this approach is effective in closed,
narrowly scoped environments, it presents two major challenges for real-world,
open-ended scenarios: (1) it significantly restricts the planning and acting
capabilities of LLM agents, and (2) it requires substantial human effort to
enumerate and implement all possible actions, which is impractical in complex
environments with a vast number of potential actions. To address these
limitations, we propose an LLM agent framework that can dynamically create and
compose actions as needed. In this framework, the agent interacts with its
environment by generating and executing programs written in a general-purpose
programming language. Moreover, generated actions are accumulated over time for
future reuse. Our extensive experiments across multiple benchmarks show that
this framework significantly improves flexibility and outperforms prior methods
that rely on a fixed action set. Notably, it enables LLM agents to adapt and
recover in scenarios where predefined actions are insufficient or fail due to
unforeseen edge cases. Our code can be found in
https://github.com/adobe-research/dynasaur.

</details>


### [125] [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/pdf/2411.02430)
*Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Zhitao Zhang*

Main category: cs.CL

TL;DR: The paper introduces MECEC, a task for explaining emotional causes in multimodal conversations, and proposes FAME-Net, a method leveraging LLMs to analyze visual data for emotion cause explanation.


<details>
  <summary>Details</summary>
Motivation: Existing emotion cause research is limited to single textual modality, lacking nuanced explanations and multimodal capability.

Method: Develops the ECEM dataset and FAME-Net, which uses LLMs to analyze facial expressions in videos for emotion cause identification.

Result: FAME-Net outperforms baselines on the ECEM dataset.

Conclusion: The work advances multimodal emotion cause explanation, with FAME-Net showing strong performance.

Abstract: Multimodal conversation, a crucial form of human communication, carries rich
emotional content, making the exploration of the causes of emotions within it a
research endeavor of significant importance. However, existing research on the
causes of emotions typically employs an utterance selection method within a
single textual modality to locate causal utterances. This approach remains
limited to coarse-grained assessments, lacks nuanced explanations of emotional
causation, and demonstrates inadequate capability in identifying multimodal
emotional triggers. Therefore, we introduce a task-\textbf{Multimodal Emotion
Cause Explanation in Conversation (MECEC)}. This task aims to generate a
summary based on the multimodal context of conversations, clearly and
intuitively describing the reasons that trigger a given emotion. To adapt to
this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM
combines video clips with detailed explanations of character emotions, helping
to explore the causal factors behind emotional expression in multimodal
conversations. A novel approach, FAME-Net, is further proposed, that harnesses
the power of Large Language Models (LLMs) to analyze visual data and accurately
interpret the emotions conveyed through facial expressions in videos. By
exploiting the contagion effect of facial emotions, FAME-Net effectively
captures the emotional causes of individuals engaged in conversations. Our
experimental results on the newly constructed dataset show that FAME-Net
outperforms several excellent baselines. Code and dataset are available at
https://github.com/3222345200/FAME-Net.

</details>


### [126] [Enabling LLM Knowledge Analysis via Extensive Materialization](https://arxiv.org/pdf/2411.04920)
*Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Simon Razniewski*

Main category: cs.CL

TL;DR: The paper introduces GPTKB, a knowledge base derived from GPT-4o-mini, to comprehensively analyze LLM factual knowledge, addressing biases in prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing LLM knowledge are limited by small samples and experimenter bias, hindering comprehensive understanding.

Method: Proposes recursive querying and result consolidation to materialize LLM knowledge, creating GPTKB with 101M relational triples.

Result: GPTKB enables large-scale analysis of GPT-4o-mini's knowledge, assessing scale, accuracy, bias, cutoff, and consistency.

Conclusion: GPTKB is a milestone for LLM research, offering insights into the scope and structure of LLM knowledge.

Abstract: Large language models (LLMs) have majorly advanced NLP and AI, and next to
their ability to perform a wide range of procedural tasks, a major success
factor is their internalized factual knowledge. Since Petroni et al. (2019),
analyzing this knowledge has gained attention. However, most approaches
investigate one question at a time via modest-sized pre-defined samples,
introducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents
the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's
predisposition.
  To address this challenge, we propose a novel methodology to comprehensively
materialize an LLM's factual knowledge through recursive querying and result
consolidation. Our approach is a milestone for LLM research, for the first time
providing constructive insights into the scope and structure of LLM knowledge
(or beliefs).
  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million
relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB
to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,
accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible
at https://gptkb.org

</details>


### [127] [Improving Radiology Report Conciseness and Structure via Local Large Language Models](https://arxiv.org/pdf/2411.05042)
*Iryna Hartsock, Cyrillo Araujo, Les Folio, Ghulam Rasool*

Main category: cs.CL

TL;DR: The study used locally deployed LLMs to make radiology reports concise and structured, reducing word count by 53% and improving clarity.


<details>
  <summary>Details</summary>
Motivation: Lengthy, unstructured radiology reports hinder quick identification of critical findings and increase missed information risks.

Method: Private LLMs (e.g., Mixtral) were tested on 814 reports using LangChain, with five prompting strategies to condense and structure reports.

Result: Mixtral outperformed alternatives, reducing word count by 53% and improving adherence to formatting requirements.

Conclusion: Locally deployed LLMs can streamline radiology reporting, enhancing clarity and clinical workflows.

Abstract: Radiology reports are often lengthy and unstructured, posing challenges for
referring physicians to quickly identify critical imaging findings while
increasing the risk of missed information. This retrospective study aimed to
enhance radiology reports by making them concise and well-structured, with
findings organized by relevant organs. To achieve this, we utilized private
large language models (LLMs) deployed locally within our institution's
firewall, ensuring data security and minimizing computational costs. Using a
dataset of 814 radiology reports from seven board-certified body radiologists
at Moffitt Cancer Center, we tested five prompting strategies within the
LangChain framework. After evaluating several models, the Mixtral LLM
demonstrated superior adherence to formatting requirements compared to
alternatives like Llama. The optimal strategy involved condensing reports first
and then applying structured formatting based on specific instructions,
reducing verbosity while improving clarity. Across all radiologists and
reports, the Mixtral LLM reduced redundant word counts by more than 53%. These
findings highlight the potential of locally deployed, open-source LLMs to
streamline radiology reporting. By generating concise, well-structured reports,
these models enhance information retrieval and better meet the needs of
referring physicians, ultimately improving clinical workflows.

</details>


### [128] [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/pdf/2411.08243)
*Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi*

Main category: cs.CL

TL;DR: The study audits the Helpful and Harmless (HH) dataset to evaluate its effectiveness in mitigating harms in large language models (LLMs) through human feedback (LHF), revealing quality issues and disparate safety impacts.


<details>
  <summary>Details</summary>
Motivation: To assess the quality and effectiveness of human feedback (LHF) in steering LLMs toward safer and more helpful outputs, given its widespread but unclear impact.

Method: The study involves (1) manual and automated evaluation of the HH dataset, (2) experiments on its impact on model safety, and (3) analysis of influential papers citing the dataset.

Result: The audit uncovers conceptualization failures and quality issues in the HH dataset, leading to uneven safety behaviors across demographic groups.

Conclusion: The findings advocate for more nuanced, context-sensitive approaches to safety mitigation in LLMs.

Abstract: In an effort to mitigate the harms of large language models (LLMs), learning
from human feedback (LHF) has been used to steer LLMs towards outputs that are
intended to be both less harmful and more helpful. Despite the widespread
adoption of LHF in practice, the quality of this feedback and its effectiveness
as a safety mitigation technique remain unclear. This study addresses these
issues by auditing the widely-used Helpful and Harmless (HH) dataset by
Anthropic. Our work includes: (1) a thorough investigation of the dataset's
content through both manual and automated evaluation; (2) experiments
demonstrating the dataset's impact on models' safety; and (3) an analysis of
the 100 most influential papers citing this dataset. Through our audit, we
showcase how conceptualization failures and quality issues identified in the HH
dataset can create additional harms by leading to disparate safety behaviors
across demographic groups. Our findings highlight the need for more nuanced,
context-sensitive approaches to safety mitigation in LLMs.

</details>


### [129] [A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects](https://arxiv.org/pdf/2411.10371)
*Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu*

Main category: cs.CL

TL;DR: A survey on Event Causality Identification (ECI) in NLP, introducing a classification framework for SECI and DECI methods, discussing challenges, evaluations, and future directions.


<details>
  <summary>Details</summary>
Motivation: To systematically organize and clarify existing ECI methods, address challenges, and guide future research in this evolving field.

Method: Proposes a classification framework for ECI methods, categorizing them into SECI (feature pattern-based, ML-based, deep semantic encoding, prompt-based fine-tuning, causal knowledge pre-training) and DECI (deep semantic encoding, event graph reasoning, prompt-based fine-tuning). Includes multilingual, cross-lingual, and zero-shot ECI using LLMs.

Result: Quantitative evaluations on four benchmark datasets highlight the strengths and limitations of each method.

Conclusion: Identifies unresolved challenges and outlines future research directions for ECI.

Abstract: Event Causality Identification (ECI) has emerged as a pivotal task in natural
language processing (NLP), aimed at automatically detecting causal
relationships between events in text. In this comprehensive survey, we
systematically elucidate the foundational principles and technical frameworks
of ECI, proposing a novel classification framework to categorize and clarify
existing methods. {We discuss associated challenges, provide quantitative
evaluations, and outline future directions for this dynamic and rapidly
evolving field. We first delineate key definitions, problem formalization, and
evaluation protocols of ECI. Our classification framework organizes ECI methods
based on two primary tasks: Sentence-level Event Causality Identification
(SECI) and Document-level Event Causality Identification (DECI). For SECI, we
review methods including feature pattern-based matching, machine learning-based
classification, deep semantic encoding, prompt-based fine-tuning, and causal
knowledge pre-training, alongside common data augmentation strategies. For
DECI, we focus on techniques such as deep semantic encoding, event graph
reasoning, and prompt-based fine-tuning. We dedicate specific discussions to
advancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI
leveraging Large Language Models (LLMs). Furthermore, we analyze the strengths,
limitations, and unresolved challenges of each method. Extensive quantitative
evaluations are conducted on four benchmark datasets to assess various ECI
methods. Finally, we explore future research directions.

</details>


### [130] [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/pdf/2412.05237)
*Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue*

Main category: cs.CL

TL;DR: The paper introduces a scalable method to create a large-scale multimodal instruction-tuning dataset with rich rationales, improving MLLMs' reasoning capabilities and achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-tuning datasets for MLLMs are simplistic, lacking intermediate rationales, limiting reasoning potential.

Method: A scalable, cost-effective approach using open models to generate 12M instruction-response pairs with detailed rationales.

Result: Training on this dataset boosts reasoning, with SOTA gains on benchmarks like MathVerse (+8.1%) and MuirBench (+13.3%).

Conclusion: The dataset and method enhance MLLM reasoning, with key components like rewriting and self-filtering proving critical.

Abstract: Open-source multimodal large language models (MLLMs) have shown significant
potential in a broad range of multimodal tasks. However, their reasoning
capabilities remain constrained by existing instruction-tuning datasets, which
were predominately repurposed from academic datasets such as VQA, AI2D, and
ChartQA. These datasets target simplistic tasks, and only provide phrase-level
answers without any intermediate rationales. To address these challenges, we
introduce a scalable and cost-effective method to construct a large-scale
multimodal instruction-tuning dataset with rich intermediate rationales
designed to elicit CoT reasoning. Using only open models, we create a dataset
containing 12M instruction-response pairs to cover diverse, reasoning-intensive
tasks with detailed and faithful rationales. Experiments demonstrate that
training MLLMs on this dataset significantly improves reasoning capabilities,
achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%),
MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates
notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation
studies further highlight the importance of key components, such as rewriting
and self-filtering, in the dataset construction process.

</details>


### [131] [Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation](https://arxiv.org/pdf/2412.15255)
*Jonibek Mansurov, Akhmed Sakip, Alham Fikri Aji*

Main category: cs.CL

TL;DR: Knowledge distillation can be exploited to manipulate benchmark scores via 'Data Laundering,' revealing vulnerabilities in AI evaluation.


<details>
  <summary>Details</summary>
Motivation: To expose flaws in current evaluation practices and highlight the need for more robust benchmarks.

Method: Introduces 'Data Laundering,' a process for covertly transferring benchmark-specific knowledge through intermediate training steps, tested with a 2-layer BERT model.

Result: Achieves up to 75% accuracy improvement on GPQA without genuine reasoning, showing the method's potential for misuse.

Conclusion: Calls for urgent improvements in evaluation methods to ensure integrity and accurate reflection of model capabilities.

Abstract: In this paper, we show that knowledge distillation can be subverted to
manipulate language model benchmark scores, revealing a critical vulnerability
in current evaluation practices. We introduce "Data Laundering," a process that
enables the covert transfer of benchmark-specific knowledge through seemingly
legitimate intermediate training steps. Through extensive experiments with a
2-layer BERT student model, we show how this approach can achieve substantial
improvements in benchmark accuracy (up to 75\% on GPQA) without developing
genuine reasoning capabilities. Notably, this method can be exploited
intentionally or even unintentionally, as researchers may inadvertently adopt
this method and inflate scores without realising the implications. While our
findings demonstrate the effectiveness of this technique, we present them as a
cautionary tale highlighting the urgent need for more robust evaluation methods
in AI. This work aims to contribute to the ongoing discussion about evaluation
integrity in AI development and the need for benchmarks that more accurately
reflect true model capabilities. The code is available at
https://github.com/mbzuai-nlp/data_laundering.

</details>


### [132] [Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria](https://arxiv.org/pdf/2412.21006)
*Joonwon Jang, Jaehee Kim, Wonbin Kweon, Seonghyeon Lee, Hwanjo Yu*

Main category: cs.CL

TL;DR: A novel sentence-level rationale reduction framework using verbosity criteria improves LLM performance by 7.71% and reduces tokens by 19.87%.


<details>
  <summary>Details</summary>
Motivation: To reduce inference costs in LLMs by eliminating redundant reasoning sentences without losing performance.

Method: Uses likelihood-based criteria (verbosity) to identify and remove redundant reasoning sentences.

Result: Improves performance by 7.71% and reduces token generation by 19.87% compared to full rationale models.

Conclusion: The framework effectively balances performance and efficiency in LLMs by targeting redundant reasoning.

Abstract: Large Language Models (LLMs) rely on generating extensive intermediate
reasoning units (e.g., tokens, sentences) to enhance final answer quality
across a wide range of complex tasks. While this approach has proven effective,
it inevitably increases substantial inference costs. Previous methods adopting
token-level reduction without clear criteria result in poor performance
compared to models trained with complete rationale. To address this challenge,
we propose a novel sentence-level rationale reduction framework leveraging
likelihood-based criteria, verbosity, to identify and remove redundant
reasoning sentences. Unlike previous approaches, our method leverages verbosity
to selectively remove redundant reasoning sentences while preserving reasoning
capabilities. Our experimental results across various reasoning tasks
demonstrate that our method improves performance by an average of 7.71% while
reducing token generation by 19.87% compared to model trained with complete
reasoning paths.

</details>


### [133] [ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability](https://arxiv.org/pdf/2501.05855)
*Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan*

Main category: cs.CL

TL;DR: The paper introduces an automated simulatability framework using LLMs to evaluate concept-based explanations, addressing both concept quality and communication effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics for concept-based explanations often neglect how effectively concepts are communicated to users, focusing only on concept quality.

Method: Proposes using large language models (LLMs) as simulators to measure simulatability, enabling scalable and consistent evaluation of concept explanations.

Result: LLMs provide consistent rankings of explanation methods, validating the framework's reliability.

Conclusion: The framework offers a scalable and comprehensive way to evaluate concept explanations, with LLMs serving as effective simulators.

Abstract: Concept-based explanations work by mapping complex model computations to
human-understandable concepts. Evaluating such explanations is very difficult,
as it includes not only the quality of the induced space of possible concepts
but also how effectively the chosen concepts are communicated to users.
Existing evaluation metrics often focus solely on the former, neglecting the
latter. We introduce an evaluation framework for measuring concept explanations
via automated simulatability: a simulator's ability to predict the explained
model's outputs based on the provided explanations. This approach accounts for
both the concept space and its interpretation in an end-to-end evaluation.
Human studies for simulatability are notoriously difficult to enact,
particularly at the scale of a wide, comprehensive empirical evaluation (which
is the subject of this work). We propose using large language models (LLMs) as
simulators to approximate the evaluation and report various analyses to make
such approximations reliable. Our method allows for scalable and consistent
evaluation across various models and datasets. We report a comprehensive
empirical evaluation using this framework and show that LLMs provide consistent
rankings of explanation methods. Code available at
https://github.com/AnonymousConSim/ConSim.

</details>


### [134] [Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions](https://arxiv.org/pdf/2501.16748)
*Garima Chhikara, Abhishek Kumar, Abhijnan Chakraborty*

Main category: cs.CL

TL;DR: The study evaluates LLMs' ability to recognize and respond to Indian subcultures (Little Traditions) and their interplay with dominant narratives (Great Traditions), revealing challenges in practical application despite some cultural nuance recognition.


<details>
  <summary>Details</summary>
Motivation: To address concerns about cultural bias in LLMs, focusing on under-represented Indian subcultures like caste, kinship, marriage, and religion.

Method: Case studies assessing LLMs' responses to Little Traditions, exploring prompting strategies and regional language prompts.

Result: LLMs can articulate cultural nuances but struggle in practical, context-specific scenarios.

Conclusion: First study on LLMs and Indian subcultures, highlighting challenges in embedding cultural diversity in AI.

Abstract: Large Language Models (LLMs) have shown remarkable advancements but also
raise concerns about cultural bias, often reflecting dominant narratives at the
expense of under-represented subcultures. In this study, we evaluate the
capacity of LLMs to recognize and accurately respond to the Little Traditions
within Indian society, encompassing localized cultural practices and
subcultures such as caste, kinship, marriage, and religion. Through a series of
case studies, we assess whether LLMs can balance the interplay between dominant
Great Traditions and localized Little Traditions. We explore various prompting
strategies and further investigate whether using prompts in regional languages
enhances the models cultural sensitivity and response quality. Our findings
reveal that while LLMs demonstrate an ability to articulate cultural nuances,
they often struggle to apply this understanding in practical, context-specific
scenarios. To the best of our knowledge, this is the first study to analyze
LLMs engagement with Indian subcultures, offering critical insights into the
challenges of embedding cultural diversity in AI systems.

</details>


### [135] [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/pdf/2502.00675)
*Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang*

Main category: cs.CL

TL;DR: ReFoRCE is a Text-to-SQL agent that leads the Spider 2.0 benchmark by addressing challenges like complex schemas, diverse SQL dialects, and sophisticated queries through innovative methods like schema compression, self-refinement, and majority-vote consensus.


<details>
  <summary>Details</summary>
Motivation: Deploying Text-to-SQL systems in enterprise environments is difficult due to large schemas, diverse SQL dialects, and complex query requirements.

Method: ReFoRCE uses database information compression, self-refinement, majority-vote consensus, and iterative column exploration to handle these challenges.

Result: ReFoRCE achieves state-of-the-art scores of 35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.

Conclusion: ReFoRCE sets a new benchmark for Text-to-SQL systems by effectively addressing real-world deployment challenges.

Abstract: We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0
leaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL
scenarios. While Text-to-SQL systems enable natural language queries over
structured databases, deploying them in enterprise environments remains
difficult due to large, complex schemas (with over 1,000 columns), diverse SQL
dialects (e.g., BigQuery, Snowflake), and sophisticated query requirements
(e.g., transformations and analytics). ReFoRCE addresses these challenges
through: (a) database information compression via pattern-based table grouping
and LLM-guided schema linking to alleviate long-context issues; (b)
self-refinement to iteratively correct syntax and semantic errors across
dialects; (c) majority-vote consensus to select high-confidence candidates
while deferring ambiguous cases arising from sophisticated queries; and (d)
iterative column exploration guided by execution feedback to resolve those
deferred cases. ReFoRCE achieves new state-of-the-art results, with scores of
35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite.

</details>


### [136] [Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models](https://arxiv.org/pdf/2502.13656)
*Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen*

Main category: cs.CL

TL;DR: Proposes a method to control LLM-generated sentence pairs for better semantic distinctions, improving sentence embeddings with ranking information, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Manual labels for sentence embeddings limit scalability; existing LLM-based methods ignore ranking information for fine-grained semantics.

Method: Control LLM generation direction in latent space for meaningful divergence, then refine embeddings by integrating ranking and semantic info.

Result: Achieves new SOTA performance on benchmarks with modest ranking synthesis cost.

Conclusion: Controlled LLM generation and ranking integration enhance sentence embeddings effectively.

Abstract: Sentence embedding is essential for many NLP tasks, with contrastive learning
methods achieving strong performance using annotated datasets like NLI. Yet,
the reliance on manual labels limits scalability. Recent studies leverage large
language models (LLMs) to generate sentence pairs, reducing annotation
dependency. However, they overlook ranking information crucial for fine-grained
semantic distinctions. To tackle this challenge, we propose a method for
controlling the generation direction of LLMs in the latent space. Unlike
unconstrained generation, the controlled approach ensures meaningful semantic
divergence. Then, we refine exist sentence embedding model by integrating
ranking information and semantic information. Experiments on multiple
benchmarks demonstrate that our method achieves new SOTA performance with a
modest cost in ranking sentence synthesis.

</details>


### [137] [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/pdf/2502.13946)
*Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li*

Main category: cs.CL

TL;DR: The paper identifies template-anchored safety alignment as a vulnerability in LLMs, where safety mechanisms overly rely on templates, making them prone to jailbreak attacks. Detaching safety from templates is suggested for robustness.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs are vulnerable to jailbreak attacks, focusing on the role of templates in safety alignment.

Method: Conducted experiments and mechanistic analyses on various aligned LLMs to verify template-anchored safety alignment.

Result: Found that template reliance makes LLMs susceptible to attacks; detaching safety from templates mitigates vulnerabilities.

Conclusion: Future research should reduce template reliance for more robust safety alignment in LLMs.

Abstract: The safety alignment of large language models (LLMs) remains vulnerable, as
their initial behavior can be easily jailbroken by even relatively simple
attacks. Since infilling a fixed template between the input instruction and
initial model output is a common practice for existing LLMs, we hypothesize
that this template is a key factor behind their vulnerabilities: LLMs'
safety-related decision-making overly relies on the aggregated information from
the template region, which largely influences these models' safety behavior. We
refer to this issue as template-anchored safety alignment. In this paper, we
conduct extensive experiments and verify that template-anchored safety
alignment is widespread across various aligned LLMs. Our mechanistic analyses
demonstrate how it leads to models' susceptibility when encountering
inference-time jailbreak attacks. Furthermore, we show that detaching safety
mechanisms from the template region is promising in mitigating vulnerabilities
to jailbreak attacks. We encourage future research to develop more robust
safety alignment techniques that reduce reliance on the template region.

</details>


### [138] [Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems](https://arxiv.org/pdf/2502.14019)
*Myra Cheng, Su Lin Blodgett, Alicia DeVrio, Lisa Egede, Alexandra Olteanu*

Main category: cs.CL

TL;DR: The paper explores interventions to reduce anthropomorphic behaviors in text generation systems, compiling an inventory of methods and developing a conceptual framework for evaluation.


<details>
  <summary>Details</summary>
Motivation: Concerns about harmful outcomes from human-like system outputs, such as over-reliance or emotional dependence, drive the need for interventions.

Method: Combines prior literature with a crowdsourcing study where participants edited outputs to reduce human-like qualities.

Result: Developed an inventory of interventions and a conceptual framework to categorize and evaluate them.

Conclusion: Provides empirical and theoretical grounding for designing and assessing interventions to mitigate anthropomorphic behaviors.

Abstract: As text generation systems' outputs are increasingly anthropomorphic --
perceived as human-like -- scholars have also increasingly raised concerns
about how such outputs can lead to harmful outcomes, such as users over-relying
or developing emotional dependence on these systems. How to intervene on such
system outputs to mitigate anthropomorphic behaviors and their attendant
harmful outcomes, however, remains understudied. With this work, we aim to
provide empirical and theoretical grounding for developing such interventions.
To do so, we compile an inventory of interventions grounded both in prior
literature and a crowdsourcing study where participants edited system outputs
to make them less human-like. Drawing on this inventory, we also develop a
conceptual framework to help characterize the landscape of possible
interventions, articulate distinctions between different types of
interventions, and provide a theoretical basis for evaluating the effectiveness
of different interventions.

</details>


### [139] [Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models](https://arxiv.org/pdf/2502.14748)
*Zongxia Li, Lorena Calvo-Bartolomé, Alexander Hoyle, Paiheng Xu, Alden Dima, Juan Francisco Fung, Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: LLMs outperform traditional topic models in readability and exploration but struggle with domain-specific data and require human supervision to mitigate issues like hallucination. Traditional models like LDA remain effective but less user-friendly.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs versus traditional topic models for large corpus understanding, especially in real-world applications.

Method: Comparison of unsupervised and supervised LLM-based approaches with traditional topic models (e.g., LDA) on two datasets.

Result: LLMs produce more readable topics but are overly generic for domain-specific data. Human supervision improves exploration but increases effort. LDA remains effective but less user-friendly.

Conclusion: LLMs need human help for domain-specific data due to scaling and hallucination issues, while traditional models like LDA are reliable but less intuitive.

Abstract: A common use of NLP is to facilitate the understanding of large document
collections, with a shift from using traditional topic models to Large Language
Models. Yet the effectiveness of using LLM for large corpus understanding in
real-world applications remains under-explored. This study measures the
knowledge users acquire with unsupervised, supervised LLM-based exploratory
approaches or traditional topic models on two datasets. While LLM-based methods
generate more human-readable topics and show higher average win probabilities
than traditional models for data exploration, they produce overly generic
topics for domain-specific datasets that do not easily allow users to learn
much about the documents. Adding human supervision to the LLM generation
process improves data exploration by mitigating hallucination and
over-genericity but requires greater human effort. In contrast, traditional.
models like Latent Dirichlet Allocation (LDA) remain effective for exploration
but are less user-friendly. We show that LLMs struggle to describe the haystack
of large corpora without human help, particularly domain-specific data, and
face scaling and hallucination limitations due to context length constraints.

</details>


### [140] [Social Genome: Grounded Social Reasoning Abilities of Multimodal Models](https://arxiv.org/pdf/2502.15109)
*Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency*

Main category: cs.CL

TL;DR: SOCIAL GENOME is a benchmark for evaluating multimodal models' fine-grained social reasoning abilities using annotated reasoning traces from 272 videos.


<details>
  <summary>Details</summary>
Motivation: To address the need for AI systems to interpret and respond to human communication in social contexts by evaluating their social reasoning abilities.

Method: Introduces SOCIAL GENOME, a dataset with 272 videos and 1,486 human-annotated reasoning traces (5,777 steps) referencing visual, verbal, vocal cues, and external knowledge.

Result: Identifies performance gaps in state-of-the-art models, highlighting opportunities for improving grounded social reasoning.

Conclusion: SOCIAL GENOME provides a holistic evaluation framework for advancing multimodal models' social reasoning capabilities.

Abstract: Social reasoning abilities are crucial for AI systems to effectively
interpret and respond to multimodal human communication and interaction within
social contexts. We introduce SOCIAL GENOME, the first benchmark for
fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL
GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning
traces related to inferences about these interactions. These traces contain
5,777 reasoning steps that reference evidence from visual cues, verbal cues,
vocal cues, and external knowledge (contextual knowledge external to videos).
SOCIAL GENOME is also the first modeling challenge to study external knowledge
in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate
semantic and structural qualities of model-generated social reasoning traces.
We demonstrate the utility of SOCIAL GENOME through experiments with
state-of-the-art models, identifying performance gaps and opportunities for
future research to improve the grounded social reasoning abilities of
multimodal models.

</details>


### [141] [D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models](https://arxiv.org/pdf/2502.16540)
*Hong Cai Chen, Yi Pin Xu, Yang Zhang*

Main category: cs.CL

TL;DR: D2S-FLOW automates SPICE model generation using LLMs, improving precision and efficiency over manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual retrieval of component parameters from datasheets is labor-intensive and time-consuming.

Method: Uses AGDF, HDER, and HNEN mechanisms for precise parameter extraction and standardization.

Result: Achieves EM 0.86, F1 0.92, EC 0.96, reduces API tokens by 38%, and minimizes irrelevant info to 4%.

Conclusion: D2S-FLOW offers an efficient automated solution for circuit design.

Abstract: In electronic design, engineers often manually search through extensive
documents to retrieve component parameters required for constructing SPICE
models, a process that is both labor-intensive and time-consuming. To address
this challenge, we present an automated framework called D2S-FLOW that
leverages large language models (LLMs) to extract electrical parameters from
datasheets and generate SPICE models with high precision and efficiency,
significantly reducing the need for manual intervention. Unlike traditional RAG
systems, D2S-FLOW employs a workflow to enhance precision in handling
unstructured documents and inconsistent naming conventions through three
innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical
Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity
Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER
utilizes document structure for precise parameter localization, and HNEN
standardizes terminology via semantic inference. Experimental results
demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1
score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the
strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it
reduces API token consumption by 38% and minimizes the irrelevant information
ratio to 4%, showcasing substantial improvements in resource efficiency. This
research provides an effective automated solution for circuit design.

</details>


### [142] [Sliding Window Attention Training for Efficient Large Language Models](https://arxiv.org/pdf/2502.18845)
*Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao*

Main category: cs.CL

TL;DR: SWAT introduces Sliding Window Attention Training to address the inefficiency of Transformers in handling long sequences, replacing softmax with sigmoid and using balanced ALiBi and Rotary Position Embedding for better performance.


<details>
  <summary>Details</summary>
Motivation: The quadratic computational complexity of Transformers for long sequences is a bottleneck, and existing solutions compromise performance or add complexity.

Method: SWAT replaces softmax with sigmoid and combines ALiBi and Rotary Position Embedding for efficient long-context handling.

Result: SWAT achieves state-of-the-art performance on eight benchmarks compared to linear recurrent architectures.

Conclusion: SWAT offers a simple yet efficient solution for long-context handling in Transformers without compromising performance.

Abstract: Recent advances in transformer-based Large Language Models (LLMs) have
demonstrated remarkable capabilities across various tasks. However, their
quadratic computational complexity concerning sequence length remains a
significant bottleneck for processing long documents. As a result, many efforts
like sparse attention and state space models have been proposed to improve the
efficiency of LLMs over long sequences. Though effective, these approaches
compromise the performance or introduce structural complexity. This calls for a
simple yet efficient model that preserves the fundamental Transformer
architecture. To this end, we introduce SWAT, which enables efficient
long-context handling via Sliding Window Attention Training. This paper first
attributes the inefficiency of Transformers to the attention sink phenomenon
resulting from the high variance of softmax operation. Then, we replace softmax
with the sigmoid function and utilize a balanced ALiBi and Rotary Position
Embedding for efficient information compression and retention. Experiments
demonstrate that SWAT achieves SOTA performance compared with state-of-the-art
linear recurrent architectures on eight benchmarks. Code is available at
https://github.com/Fzkuji/swat-attention.

</details>


### [143] [Where Are We? Evaluating LLM Performance on African Languages](https://arxiv.org/pdf/2502.19582)
*Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: The paper highlights the underrepresentation of African languages in NLP due to historical biases and data inequities, using the Sahara benchmark to evaluate LLMs and advocate for policy reforms.


<details>
  <summary>Details</summary>
Motivation: Address the underrepresentation of African languages in NLP caused by historical policies favoring foreign languages and data inequities.

Method: Integrate theoretical insights on Africa's language landscape with empirical evaluation using the Sahara benchmark to assess LLM performance.

Result: LLMs perform unevenly across African languages, with many Indigenous languages marginalized due to sparse data.

Conclusion: A dual approach of theoretical understanding and empirical evaluation is needed to promote linguistic diversity in AI for African communities.

Abstract: Africa's rich linguistic heritage remains underrepresented in NLP, largely
due to historical policies that favor foreign languages and create significant
data inequities. In this paper, we integrate theoretical insights on Africa's
language landscape with an empirical evaluation using Sahara - a comprehensive
benchmark curated from large-scale, publicly accessible datasets capturing the
continent's linguistic diversity. By systematically assessing the performance
of leading large language models (LLMs) on Sahara, we demonstrate how
policy-induced data variations directly impact model effectiveness across
African languages. Our findings reveal that while a few languages perform
reasonably well, many Indigenous languages remain marginalized due to sparse
data. Leveraging these insights, we offer actionable recommendations for policy
reforms and inclusive data practices. Overall, our work underscores the urgent
need for a dual approach - combining theoretical understanding with empirical
evaluation - to foster linguistic diversity in AI for African communities.

</details>


### [144] [DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation](https://arxiv.org/pdf/2503.01622)
*Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky*

Main category: cs.CL

TL;DR: DOVE is a large-scale dataset for evaluating LLM sensitivity to prompt variations, offering insights into robust evaluation practices.


<details>
  <summary>Details</summary>
Motivation: Addressing the sensitivity of LLMs to arbitrary prompt dimensions and questioning single-prompt evaluation practices.

Method: Creating DOVE, a dataset with thousands of prompt perturbations per instance, and evaluating LLM families against it.

Result: Findings include efficient prompt selection, reduced sensitivity with few-shot examples, and identification of inherently hard instances.

Conclusion: DOVE, with 250M+ perturbations, aims to improve LLM evaluation robustness and is publicly available for community use.

Abstract: Recent work found that LLMs are sensitive to a wide range of arbitrary prompt
dimensions, including the type of delimiters, answer enumerators, instruction
wording, and more. This throws into question popular single-prompt evaluation
practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale
dataset containing prompt perturbations of various evaluation benchmarks. In
contrast to previous work, we examine LLM sensitivity from an holistic
perspective, and assess the joint effects of perturbations along various
dimensions, resulting in thousands of perturbations per instance. We evaluate
several model families against DOVE, leading to several findings, including
efficient methods for choosing well-performing prompts, observing that few-shot
examples reduce sensitivity, and identifying instances which are inherently
hard across all perturbations. DOVE consists of more than 250M prompt
perturbations and model outputs, which we make publicly available to spur a
community-wide effort toward meaningful, robust, and efficient evaluation.
  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/

</details>


### [145] [On the Acquisition of Shared Grammatical Representations in Bilingual Language Models](https://arxiv.org/pdf/2503.03962)
*Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen*

Main category: cs.CL

TL;DR: The paper investigates crosslingual transfer in small bilingual models, focusing on structural priming to understand shared multilingual representations. It reveals asymmetrical effects and limitations for less similar language pairs.


<details>
  <summary>Details</summary>
Motivation: To understand how monolingual models adapt to a second language and explore shared multilingual representations using structural priming.

Method: Training small bilingual models with controlled data quantity and language exposure order, then analyzing structural priming effects.

Result: Asymmetrical crosslingual priming effects and reduced robustness for less similar language pairs, suggesting limitations in shared representations.

Conclusion: Crosslingual transfer is influenced by language similarity and exposure, with implications for multilingual model design and human priming studies.

Abstract: Crosslingual transfer is crucial to contemporary language models'
multilingual capabilities, but how it occurs is not well understood. We ask
what happens to a monolingual language model when it begins to be trained on a
second language. Specifically, we train small bilingual models for which we
control the amount of data for each language and the order of language
exposure. To find evidence of shared multilingual representations, we turn to
structural priming, a method used to study grammatical representations in
humans. We first replicate previous crosslingual structural priming results and
find that after controlling for training data quantity and language exposure,
there are asymmetrical effects across language pairs and directions. We argue
that this asymmetry may shape hypotheses about human structural priming
effects. We also find that structural priming effects are less robust for less
similar language pairs, highlighting potential limitations of crosslingual
transfer learning and shared representations for typologically diverse
languages.

</details>


### [146] [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/pdf/2503.06706)
*Ming Zhang, Yuhui Wang, Yujiong Shen, Tingyi Yang, Changhao Jiang, Yilong Wu, Shihan Dou, Qinhao Chen, Zhiheng Xi, Zhihao Zhang, Yi Dong, Zhen Wang, Zhihui Fei, Mingyang Wan, Tao Liang, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces the PFDial dataset for process-driven dialogue tasks, showing that smaller models trained on it can outperform larger models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Process-driven dialogue systems under strict constraints are challenging for LLMs, despite their general dialogue capabilities.

Method: The PFDial dataset is constructed from 440 flowcharts converted into structured five-tuples, and models are trained on this data.

Result: A 7B model with 800 samples and a 0.5B model achieve over 90% accuracy, with the 8B model outperforming GPT-4o by up to 43.88%.

Conclusion: The PFDial dataset effectively enhances model performance for constrained dialogue tasks, with insights on dataset formats and backward transitions.

Abstract: Process-driven dialogue systems, which operate under strict predefined
process constraints, are essential in customer service and equipment
maintenance scenarios. Although Large Language Models (LLMs) have shown
remarkable progress in dialogue and reasoning, they still struggle to solve
these strictly constrained dialogue tasks. To address this challenge, we
construct Process Flow Dialogue (PFDial) dataset, which contains 12,705
high-quality Chinese dialogue instructions derived from 440 flowcharts
containing 5,055 process nodes. Based on PlantUML specification, each UML
flowchart is converted into atomic dialogue units i.e., structured five-tuples.
Experimental results demonstrate that a 7B model trained with merely 800
samples, and a 0.5B model trained on total data both can surpass 90% accuracy.
Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of
11.00%. We further evaluate models' performance on challenging backward
transitions in process flows and conduct an in-depth analysis of various
dataset formats to reveal their impact on model performance in handling
decision and sequential branches. The data is released in
https://github.com/KongLongGeFDU/PFDial.

</details>


### [147] [LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data](https://arxiv.org/pdf/2503.08042)
*Naomi Baes, Raphaël Merx, Nick Haslam, Ekaterina Vylomova, Haim Dubossarsky*

Main category: cs.CL

TL;DR: LSC-Eval is a three-stage framework for evaluating Lexical Semantic Change (LSC) methods using synthetic datasets, focusing on Sentiment, Intensity, and Breadth dimensions.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of historical benchmark datasets to validate LSC measurement methods.

Method: Proposes LSC-Eval: (1) generates synthetic datasets via In-Context Learning, (2) evaluates method sensitivity, (3) assesses suitability for specific dimensions/domains.

Result: Validates synthetic benchmarks, shows tailored methods detect SIB changes, and highlights challenges for state-of-the-art models in affective dimensions.

Conclusion: LSC-Eval is a valuable tool for dimension- and domain-specific benchmarking, especially in social sciences.

Abstract: Lexical Semantic Change (LSC) provides insight into cultural and social
dynamics. Yet, the validity of methods for measuring different kinds of LSC
remains unestablished due to the absence of historical benchmark datasets. To
address this gap, we propose LSC-Eval, a novel three-stage general-purpose
evaluation framework to: (1) develop a scalable methodology for generating
synthetic datasets that simulate theory-driven LSC using In-Context Learning
and a lexical database; (2) use these datasets to evaluate the sensitivity of
computational methods to synthetic change; and (3) assess their suitability for
detecting change in specific dimensions and domains. We apply LSC-Eval to
simulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions,
as defined in the SIBling framework, using examples from psychology. We then
evaluate the ability of selected methods to detect these controlled
interventions. Our findings validate the use of synthetic benchmarks,
demonstrate that tailored methods effectively detect changes along SIB
dimensions, and reveal that a state-of-the-art LSC model faces challenges in
detecting affective dimensions of LSC. LSC-Eval offers a valuable tool for
dimension- and domain-specific benchmarking of LSC methods, with particular
relevance to the social sciences.

</details>


### [148] [An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)](https://arxiv.org/pdf/2503.10267)
*Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajič, Jindřich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kytöniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, Farrokh Mehryary, Vladislav Mikhailov, Nikita Moghe, Amanda Myntti, Dayyán O'Brien, Stephan Oepen, Proyag Pal, Jousia Piha, Sampo Pyysalo, Gema Ramírez-Sánchez, David Samuel, Pavel Stepachev, Jörg Tiedemann, Dušan Variš, Tereza Vojtěchová, Jaume Zaragoza-Bernabeu*

Main category: cs.CL

TL;DR: HPLT v2 is a high-quality multilingual dataset with 8T monolingual tokens and 380M parallel sentence pairs, covering 193 and 51 languages respectively, with released code and performance evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of building clean and diverse multilingual datasets for training large language models.

Method: Extends the HPLT project by collecting and processing multilingual monolingual and parallel corpora, documenting the pipeline, and releasing reproducible code.

Result: The dataset includes 8T tokens (193 languages) and 380M sentence pairs (51 languages), with quality analysis and evaluations showing its effectiveness.

Conclusion: HPLT v2 is a valuable resource for training language models and machine translation systems, with demonstrated performance benefits.

Abstract: Training state-of-the-art large language models requires vast amounts of
clean and diverse textual data. However, building suitable multilingual
datasets remains a challenge. In this work, we present HPLT v2, a collection of
high-quality multilingual monolingual and parallel corpora, extending prior
work of the HPLT project. The monolingual portion of the data contains 8T
tokens covering 193 languages, while the parallel data contains 380M sentence
pairs covering 51 languages. We document the entire data pipeline and release
the code to reproduce it. We provide extensive analysis of the quality and
characteristics of our data. Finally, we evaluate the performance of language
models and machine translation systems trained on HPLT v2, demonstrating its
value.

</details>


### [149] [Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set](https://arxiv.org/pdf/2503.10515)
*Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich*

Main category: cs.CL

TL;DR: LLMs generalize discourse knowledge across languages and frameworks, with multilingual models performing best, especially in intermediate layers.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs capture generalizable discourse knowledge beyond language and framework constraints.

Method: Developed a unified discourse relation label set and probed 23 LLMs of varying sizes and multilingual capabilities for discourse relation classification.

Result: LLMs, especially multilingual ones, generalize discourse knowledge; intermediate layers show the most language generalization.

Conclusion: LLMs encode generalizable discourse abstractions, with multilingual training enhancing cross-lingual and cross-framework performance.

Abstract: Discourse understanding is essential for many NLP tasks, yet most existing
work remains constrained by framework-dependent discourse representations. This
work investigates whether large language models (LLMs) capture discourse
knowledge that generalizes across languages and frameworks. We address this
question along two dimensions: (1) developing a unified discourse relation
label set to facilitate cross-lingual and cross-framework discourse analysis,
and (2) probing LLMs to assess whether they encode generalizable discourse
abstractions. Using multilingual discourse relation classification as a
testbed, we examine a comprehensive set of 23 LLMs of varying sizes and
multilingual capabilities. Our results show that LLMs, especially those with
multilingual training corpora, can generalize discourse information across
languages and frameworks. Further layer-wise analyses reveal that language
generalization at the discourse level is most salient in the intermediate
layers. Lastly, our error analysis provides an account of challenging relation
classes.

</details>


### [150] [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/pdf/2503.15358)
*Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps, Marco Idiart*

Main category: cs.CL

TL;DR: The paper introduces SemEval-2025 Task 1: AdMiRe, focusing on improving NLP models' ability to interpret idiomatic expressions in multimodal and multilingual contexts. Tasks include image ranking and sequence prediction, with top methods using pretrained LLMs and vision-language models.


<details>
  <summary>Details</summary>
Motivation: Idiomatic expressions are challenging for NLP models, even with advancements in LLMs. The paper aims to address this gap by creating tasks to evaluate and enhance models' idiomaticity representation.

Method: Datasets and tasks were designed for SemEval-2025 Task 1 (AdMiRe), involving image ranking and sequence prediction. Methods combined pretrained LLMs and vision-language models in mixture-of-experts setups.

Result: Top-performing methods achieved human-level performance by leveraging multiple queries and smoothing weaknesses in idiomaticity representation.

Conclusion: The AdMiRe task successfully advanced idiomaticity representation in NLP, demonstrating the effectiveness of combining LLMs and vision-language models.

Abstract: Idiomatic expressions present a unique challenge in NLP, as their meanings
are often not directly inferable from their constituent words. Despite recent
advancements in Large Language Models (LLMs), idiomaticity remains a
significant obstacle to robust semantic representation. We present datasets and
tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity
Representation), which challenges the community to assess and improve models'
ability to interpret idiomatic expressions in multimodal contexts and in
multiple languages. Participants competed in two subtasks: ranking images based
on their alignment with idiomatic or literal meanings, and predicting the next
image in a sequence. The most effective methods achieved human-level
performance by leveraging pretrained LLMs and vision-language models in
mixture-of-experts settings, with multiple queries used to smooth over the
weaknesses in these models' representations of idiomaticity.

</details>


### [151] [Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey](https://arxiv.org/pdf/2503.15850)
*Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei*

Main category: cs.CL

TL;DR: The paper discusses the challenges of uncertainty quantification (UQ) in Large Language Models (LLMs) and proposes a new taxonomy to categorize UQ methods for improving reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used in high-stakes domains but often produce incorrect yet plausible responses, necessitating better UQ methods to enhance trustworthiness.

Method: The authors introduce a taxonomy for UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). They evaluate existing techniques and assess their applicability.

Result: The study highlights the limitations of traditional UQ methods for LLMs and identifies unique uncertainty sources like input ambiguity and decoding stochasticity.

Conclusion: Scalable, interpretable, and robust UQ approaches are needed to improve LLM reliability, addressing open challenges in the field.

Abstract: Large Language Models (LLMs) excel in text generation, reasoning, and
decision-making, enabling their adoption in high-stakes domains such as
healthcare, law, and transportation. However, their reliability is a major
concern, as they often produce plausible but incorrect responses. Uncertainty
quantification (UQ) enhances trustworthiness by estimating confidence in
outputs, enabling risk mitigation and selective prediction. However,
traditional UQ methods struggle with LLMs due to computational constraints and
decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,
such as input ambiguity, reasoning path divergence, and decoding stochasticity,
that extend beyond classical aleatoric and epistemic uncertainty. To address
this, we introduce a new taxonomy that categorizes UQ methods based on
computational efficiency and uncertainty dimensions (input, reasoning,
parameter, and prediction uncertainty). We evaluate existing techniques, assess
their real-world applicability, and identify open challenges, emphasizing the
need for scalable, interpretable, and robust UQ approaches to enhance LLM
reliability.

</details>


### [152] [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/pdf/2503.23512)
*Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Xinhang Yuan, Li Sun, Yi Xin, Keqin Li, Kuan Lu, Menghao Huo, Jiaqi Chen, Tianyu Shi*

Main category: cs.CL

TL;DR: SCORE improves coherence in AI-generated stories by tracking item statuses and using retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Maintaining coherence and emotional depth in AI-generated narratives is challenging.

Method: SCORE uses TF-IDF and cosine similarity for retrieval-augmented generation to resolve inconsistencies.

Result: SCORE enhances narrative coherence significantly over baseline GPT models.

Conclusion: SCORE provides a robust method for refining AI-generated stories.

Abstract: Large Language Models (LLMs) can generate creative and engaging narratives
from user-specified input, but maintaining coherence and emotional depth
throughout these AI-generated stories remains a challenge. In this work, we
propose SCORE, a framework for Story Coherence and Retrieval Enhancement,
designed to detect and resolve narrative inconsistencies. By tracking key item
statuses and generating episode summaries, SCORE uses a Retrieval-Augmented
Generation (RAG) approach, incorporating TF-IDF and cosine similarity to
identify related episodes and enhance the overall story structure. Results from
testing multiple LLM-generated stories demonstrate that SCORE significantly
improves the consistency and stability of narrative coherence compared to
baseline GPT models, providing a more robust method for evaluating and refining
AI-generated narratives.

</details>


### [153] [Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset](https://arxiv.org/pdf/2503.23899)
*Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery*

Main category: cs.CL

TL;DR: Rubrik's CUBE introduces a rubric and dataset to evaluate LLM-generated explanations, revealing issues like lack of conciseness.


<details>
  <summary>Details</summary>
Motivation: Address the unreliability of LLM explanations by providing a structured evaluation method.

Method: Developed a rubric (CUBE) and collected 26k explanations annotated by humans and LLMs for quality.

Result: Found explanations vary by task and difficulty, with conciseness being a key issue.

Conclusion: CUBE offers a tool to assess and improve LLM explanations, with data and rubric publicly available.

Abstract: The performance and usability of Large-Language Models (LLMs) are driving
their use in explanation generation tasks. However, despite their widespread
adoption, LLM explanations have been found to be unreliable, making it
difficult for users to distinguish good from bad explanations. To address this
issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of
26k explanations, written and later quality-annotated using the rubric by both
humans and six open- and closed-source LLMs. The CUBE dataset focuses on two
reasoning and two language tasks, providing the necessary diversity for us to
effectively test our proposed rubric. Using Rubrik, we find that explanations
are influenced by both task and perceived difficulty. Low quality stems
primarily from a lack of conciseness in LLM-generated explanations, rather than
cohesion and word choice. The full dataset, rubric, and code are available at
https://github.com/RubriksCube/rubriks_cube.

</details>


### [154] [Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding](https://arxiv.org/pdf/2504.00030)
*Aayush Gautam, Susav Shrestha, Narasimha Reddy*

Main category: cs.CL

TL;DR: GammaTune and GammaTune+ are adaptive algorithms for speculative decoding in LLMs, dynamically adjusting speculation length to improve speedup and reduce wasted computation.


<details>
  <summary>Details</summary>
Motivation: Optimizing speculation length in speculative decoding is crucial for maximizing speedup and minimizing wasted computation in LLM inference.

Method: Introduces GammaTune and GammaTune+, training-free adaptive algorithms that adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism.

Result: Outperforms other approaches, achieving average speedups of 15% (GammaTune) and 16% (GammaTune+) with reduced performance variance.

Conclusion: GammaTune is a robust and efficient solution for real-world deployment in LLM inference.

Abstract: Speculative decoding accelerates large language model (LLM) inference by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, selecting an optimal speculation length is
critical for maximizing speedup while minimizing wasted computation. We
introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive
algorithms that dynamically adjust speculation length based on token acceptance
rates using a heuristic-based switching mechanism. Evaluated on SpecBench
across multiple tasks and model pairs, our method outperforms other
heuristic-based approaches and fixed-length speculative decoding, achieving an
average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%)
with \textit{GammaTune+}, while reducing performance variance. This makes
\textit{GammaTune} a robust and efficient solution for real-world deployment.

</details>


### [155] [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/pdf/2504.05154)
*Geyang Guo, Tarek Naous, Hiromi Wakaki, Yukiko Nishimura, Yuki Mitsufuji, Alan Ritter, Wei Xu*

Main category: cs.CL

TL;DR: The paper introduces CARE, a multilingual resource for training culturally aware language models (LMs) using native human preferences, showing that high-quality native preferences outperform larger generic data.


<details>
  <summary>Details</summary>
Motivation: To address the understudied impact of preference tuning on LMs' ability to handle culturally diverse queries and improve cultural awareness.

Method: Introduces CARE, a dataset with 3,490 culturally specific questions and 31.7k responses with native judgments, used to train LMs.

Result: High-quality native preferences enhance cultural awareness in LMs, with stronger initial cultural models benefiting more.

Conclusion: CARE improves LM cultural awareness, highlighting gaps in models from regions with varying access to culturally relevant data.

Abstract: Language Models (LMs) are typically tuned with human preferences to produce
helpful responses, but the impact of preference tuning on the ability to handle
culturally diverse queries remains understudied. In this paper, we
systematically analyze how native human cultural preferences can be
incorporated into the preference learning process to train more culturally
aware LMs. We introduce \textbf{CARE}, a multilingual resource containing 3,490
culturally specific questions and 31.7k responses with native judgments. We
demonstrate how a modest amount of high-quality native preferences improves
cultural awareness across various LMs, outperforming larger generic preference
data. Our analyses reveal that models with stronger initial cultural
performance benefit more from alignment, leading to gaps among models developed
in different regions with varying access to culturally relevant data. CARE will
be made publicly available at https://github.com/Guochry/CARE.

</details>


### [156] [Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation](https://arxiv.org/pdf/2504.05276)
*Yucheng Chu, Peng He, Hang Li, Haoyu Han, Kaiqi Yang, Yu Xue, Tingting Li, Joseph Krajcik, Jiliang Tang*

Main category: cs.CL

TL;DR: An adaptive RAG framework improves grading accuracy in science education by dynamically retrieving domain-specific knowledge for LLMs.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' grading performance by addressing their limitations in domain knowledge through retrieval-augmented generation.

Method: Proposes an adaptive RAG framework combining semantic search and curated educational sources for dynamic knowledge retrieval.

Result: Experimental results show improved grading accuracy over baseline LLM approaches.

Conclusion: RAG-enhanced grading systems offer reliable and efficient support for automated assessment.

Abstract: Short answer assessment is a vital component of science education, allowing
evaluation of students' complex three-dimensional understanding. Large language
models (LLMs) that possess human-like ability in linguistic tasks are
increasingly popular in assisting human graders to reduce their workload.
However, LLMs' limitations in domain knowledge restrict their understanding in
task-specific requirements and hinder their ability to achieve satisfactory
performance. Retrieval-augmented generation (RAG) emerges as a promising
solution by enabling LLMs to access relevant domain-specific knowledge during
assessment. In this work, we propose an adaptive RAG framework for automated
grading that dynamically retrieves and incorporates domain-specific knowledge
based on the question and student answer context. Our approach combines
semantic search and curated educational sources to retrieve valuable reference
materials. Experimental results in a science education dataset demonstrate that
our system achieves an improvement in grading accuracy compared to baseline LLM
approaches. The findings suggest that RAG-enhanced grading systems can serve as
reliable support with efficient performance gains.

</details>


### [157] [Identifying Aspects in Peer Reviews](https://arxiv.org/pdf/2504.06910)
*Sheng Lu, Ilia Kuznetsov, Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper addresses the challenge of standardizing peer review by proposing a data-driven approach to define and derive aspects from reviews, enabling computational support and improved quality control.


<details>
  <summary>Details</summary>
Motivation: The growing volume of submissions strains peer review, necessitating computational support. Current approaches lack formalization of review aspects, motivating a data-driven solution.

Method: The authors propose an operational definition of aspects and develop a data-driven schema to derive them from peer reviews. They introduce a dataset for analysis and demonstrate its utility in applications like LLM-generated review detection.

Result: The work provides a foundation for principled investigation of review aspects and showcases their impact on downstream tasks, such as detecting AI-generated reviews.

Conclusion: The study advances the standardization of peer review through data-driven aspect analysis, paving the way for NLP applications to support the process.

Abstract: Peer review is central to academic publishing, but the growing volume of
submissions is straining the process. This motivates the development of
computational approaches to support peer review. While each review is tailored
to a specific paper, reviewers often make assessments according to certain
aspects such as Novelty, which reflect the values of the research community.
This alignment creates opportunities for standardizing the reviewing process,
improving quality control, and enabling computational support. While prior work
has demonstrated the potential of aspect analysis for peer review assistance,
the notion of aspect remains poorly formalized. Existing approaches often
derive aspects from review forms and guidelines, yet data-driven methods for
aspect identification are underexplored. To address this gap, our work takes a
bottom-up approach: we propose an operational definition of aspect and develop
a data-driven schema for deriving aspects from a corpus of peer reviews. We
introduce a dataset of peer reviews augmented with aspects and show how it can
be used for community-level review analysis. We further show how the choice of
aspects can impact downstream applications, such as LLM-generated review
detection. Our results lay a foundation for a principled and data-driven
investigation of review aspects, and pave the path for new applications of NLP
to support peer review.

</details>


### [158] [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/pdf/2504.13677)
*Andrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, Sinead Williamson*

Main category: cs.CL

TL;DR: Mutual biases between UQ methods and correctness functions distort AUROC rankings, compromising benchmark integrity. Length biases are a key issue, with LM-as-a-judge methods being the least biased.


<details>
  <summary>Details</summary>
Motivation: To improve the safety and reliability of Language Models (LMs) by addressing biases in Uncertainty Quantification (UQ) evaluations.

Method: Formally prove mutual biases skew AUROC rankings and empirically test 7 correctness functions across 4 datasets, 4 models, and 8 UQ methods.

Result: Length biases in correctness functions distort UQ assessments, with LM-as-a-judge methods showing the least bias.

Conclusion: LM-as-a-judge methods offer a fairer path for UQ evaluation by minimizing length biases.

Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving
their safety and reliability. Evaluations often use metrics like AUROC to
assess how well UQ methods (e.g., negative sequence probabilities) correlate
with task correctness functions (e.g., ROUGE-L). We show that mutual
biases--when both UQ methods and correctness functions are biased by the same
factors--systematically distort evaluation. First, we formally prove that any
mutual bias non-randomly skews AUROC rankings, compromising benchmark
integrity. Second, we confirm this happens empirically by testing 7 widely used
correctness functions, from lexical-based and embedding-based metrics to
LM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our
analysis shows that length biases in correctness functions distort UQ
assessments by interacting with length biases in UQ methods. We identify
LM-as-a-judge methods as the least length-biased, offering a promising path for
a fairer UQ evaluation.

</details>


### [159] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/pdf/2504.14175)
*Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park*

Main category: cs.CL

TL;DR: LLM-based query expansion methods show performance gains in zero-shot retrieval, but knowledge leakage in benchmarks may inflate these results.


<details>
  <summary>Details</summary>
Motivation: To investigate if performance gains in LLM-based query expansion are due to knowledge leakage in benchmarks rather than true effectiveness.

Method: Analyze generated documents for entailment of ground-truth evidence in fact verification tasks.

Result: Performance improvements occurred when generated documents included sentences entailed by gold evidence, suggesting knowledge leakage.

Conclusion: Knowledge leakage in benchmarks may artificially boost the perceived performance of LLM-based query expansion methods.

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyze whether the generated documents contain information
entailed by ground-truth evidence and assess their impact on performance. Our
findings indicate that, on average, performance improvements consistently
occurred for claims whose generated documents included sentences entailed by
gold evidence. This suggests that knowledge leakage may be present in
fact-verification benchmarks, potentially inflating the perceived performance
of LLM-based query expansion methods.

</details>


### [160] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/pdf/2504.14194)
*Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He*

Main category: cs.CL

TL;DR: Meta-rater introduces a multi-dimensional data selection method (professionalism, readability, reasoning, cleanliness) for LLM pre-training, outperforming single-dimension approaches by doubling convergence speed and improving task performance.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for LLMs are limited by single-dimensional evaluation or redundancy-focused strategies, hindering transparency and optimization of data quality.

Method: Proposes Meta-rater, integrating four quality dimensions with learned optimal weightings via proxy models to predict validation loss.

Result: Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, scaling to 7.2B parameters.

Conclusion: Holistic, multi-dimensional quality integration significantly outperforms conventional methods, offering a scalable paradigm for enhancing pre-training efficiency and model capability.

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose four
dimensions to evaluate data quality: professionalism, readability, reasoning,
and cleanliness. We further introduce Meta-rater,a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with advantages that scale to models as large as 7.2B
parameters. Our work establishes that holistic, multi-dimensional quality
integration significantly outperforms conventional single-dimension approaches,
offering a scalable paradigm for enhancing pre-training efficiency and model
capability. To advance future research, we release scripts, data, and models at
https://github.com/opendatalab/Meta-rater.

</details>


### [161] [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/pdf/2505.11441)
*Xianzhen Luo, Shijie Xuyang, Tianhao Cheng, Zheng Chu, Houyi Li, ziqi wang, Siming Huang, Qingfu Zhu, Qiufeng Wang, Xiangyu Zhang, Shuigeng Zhou, Wanxiang Che*

Main category: cs.CL

TL;DR: The paper explores the relationship between data compression and Code LLMs, revealing a logarithmic (not linear) link between compression (BPC) and code intelligence. It introduces Format Annealing for fair evaluation and uses a novel GitHub-derived dataset.


<details>
  <summary>Details</summary>
Motivation: To clarify the relationship between compression and Code LLMs' intelligence, addressing gaps in prior work that overlooked code diversity and fair evaluation.

Method: Evaluates diverse Code LLMs on multi-language, multi-task benchmarks and introduces Format Annealing for fair assessment. Measures compression via BPC on a new GitHub dataset.

Result: Empirical findings show a logarithmic (not linear) relationship between BPC and code intelligence, refining prior hypotheses.

Conclusion: The work offers a nuanced understanding of compression's role in code intelligence and provides a robust evaluation framework for Code LLMs.

Abstract: Understanding the relationship between data compression and the capabilities
of Large Language Models (LLMs) is crucial, especially in specialized domains
like code intelligence. Prior work posited a linear relationship between
compression and general intelligence. However, it overlooked the multifaceted
nature of code that encompasses diverse programming languages and tasks, and
struggled with fair evaluation of modern Code LLMs. We address this by
evaluating a diverse array of open-source Code LLMs on comprehensive
multi-language, multi-task code benchmarks. To address the challenge of
efficient and fair evaluation of pre-trained LLMs' code intelligence, we
introduce \textit{Format Annealing}, a lightweight, transparent training
methodology designed to assess the intrinsic capabilities of these pre-trained
models equitably. Compression efficacy, measured as bits-per-character (BPC),
is determined using a novel, large-scale, and previously unseen code validation
set derived from GitHub. Our empirical results reveal a fundamental logarithmic
relationship between measured code intelligence and BPC. This finding refines
prior hypotheses of linearity, which we suggest are likely observations of the
logarithmic curve's tail under specific, limited conditions. Our work provides
a more nuanced understanding of compression's role in developing code
intelligence and contributes a robust evaluation framework in the code domain.

</details>


### [162] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/pdf/2505.11626)
*Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Parul Kalra, Rafael Castrillo*

Main category: cs.CL

TL;DR: THELMA is a reference-free framework with six metrics for evaluating RAG-based QA applications, aiding in monitoring and improving pipelines without labeled data.


<details>
  <summary>Details</summary>
Motivation: To provide a holistic, fine-grained evaluation of RAG QA applications without needing labeled sources or reference responses.

Method: Proposes THELMA, a framework with six interdependent metrics for evaluating RAG QA pipelines.

Result: Findings show how THELMA metrics can identify specific RAG components needing improvement.

Conclusion: THELMA enables effective evaluation and enhancement of RAG QA applications without reliance on labeled data.

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [163] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/pdf/2505.14590)
*Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper proposes MCIP, a refined version of MCP, to address safety risks in decentralized architectures. It introduces a taxonomy for unsafe behaviors, benchmarks for LLM evaluation, and shows improved safety performance.


<details>
  <summary>Details</summary>
Motivation: MCP's decentralized architecture poses safety risks, which are underexplored. The paper aims to systematically analyze and enhance MCP's safety mechanisms.

Method: The MAESTRO framework guides the analysis of MCP's safety gaps. MCIP is proposed, along with a taxonomy of unsafe behaviors, benchmarks, and training data for LLMs.

Result: Experiments reveal LLM vulnerabilities in MCP interactions and demonstrate significant safety improvements using the proposed approach.

Conclusion: The paper successfully enhances MCP safety by introducing MCIP, a taxonomy, and benchmarks, proving their effectiveness in improving LLM safety performance.

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps. Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [164] [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/pdf/2505.17427)
*Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

Main category: cs.CL

TL;DR: T$^2$ (Think-to-Think) is a framework that dynamically adjusts reasoning depth in LLMs for CQA, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Prior methods lack adaptability and introduce human bias, failing to utilize LLMs' inherent reasoning capabilities.

Method: T$^2$ decomposes questions, generates similar examples with strategies, evaluates them, and applies the best strategy.

Result: T$^2$ outperforms baselines in accuracy and reduces computational overhead by up to 25.2%.

Conclusion: T$^2$ effectively balances reasoning depth and efficiency, enhancing LLM performance in CQA.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
performance in Contextual Question Answering (CQA). However, prior approaches
typically employ elaborate reasoning strategies regardless of question
complexity, leading to low adaptability. Recent efficient test-time scaling
methods introduce budget constraints or early stop mechanisms to avoid
overthinking for straightforward questions. But they add human bias to the
reasoning process and fail to leverage models' inherent reasoning capabilities.
To address these limitations, we present T$^2$: Think-to-Think, a novel
framework that dynamically adapts reasoning depth based on question complexity.
T$^2$ leverages the insight that if an LLM can effectively solve similar
questions using specific reasoning strategies, it can apply the same strategy
to the original question. This insight enables to adoption of concise reasoning
for straightforward questions while maintaining detailed analysis for complex
problems. T$^2$ works through four key steps: decomposing questions into
structural elements, generating similar examples with candidate reasoning
strategies, evaluating these strategies against multiple criteria, and applying
the most appropriate strategy to the original question. Experimental evaluation
across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves
higher accuracy than baseline methods but also reduces computational overhead
by up to 25.2\%.

</details>


### [165] [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/pdf/2505.19176)
*Zhuo Liu, Moxin Li, Xun Deng, Qifan Wang, Fuli Feng*

Main category: cs.CL

TL;DR: AGDe-Judge is a framework to reduce teacher preference bias in LLM-based evaluation systems by incorporating an unbiased assistant model and a three-stage debiasing process.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of teacher preference bias in proxy judge models trained using evaluation data from powerful teacher models, which can skew results.

Method: The authors propose AGDe-Judge, a three-stage framework that uses an unbiased assistant model to complement training data and debias labels and feedback.

Result: Experiments show AGDe-Judge effectively reduces bias while maintaining strong performance across six benchmarks.

Conclusion: AGDe-Judge successfully mitigates teacher preference bias, improving the reliability of LLM-based evaluations.

Abstract: LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to
evaluate the quality of LLM-generated responses, gaining popularity for its
cost-effectiveness and strong alignment with human evaluations. However,
training proxy judge models using evaluation data generated by powerful teacher
models introduces a critical yet previously overlooked issue: teacher
preference bias, where the proxy judge model learns a biased preference for
responses from the teacher model. To tackle this problem, we propose a novel
setting that incorporates an additional assistant model, which is not biased
toward the teacher model's responses, to complement the training data. Building
on this setup, we introduce AGDe-Judge, a three-stage framework designed to
debias from both the labels and feedbacks in the training data. Extensive
experiments demonstrate that AGDe-Judge effectively reduces teacher preference
bias while maintaining strong performance across six evaluation benchmarks.
Code is available at https://github.com/Liuz233/AGDe-Judge.

</details>


### [166] [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/pdf/2505.20015)
*Ramon Ferrer-i-Cancho*

Main category: cs.CL

TL;DR: The paper introduces a new class of optimality for coding systems, linking Zipf's law to linear displacement from optimal coding, and identifies human languages as members of this class.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between Zipf's law and optimal coding, and to classify communication systems based on their adherence to power-law distributions.

Method: The study analyzes coding systems, focusing on linear displacement from optimality, and examines human languages and other species' communication systems for adherence to Zipf's law.

Result: Human languages fit the new class, while some animal communication systems (e.g., dolphins, humpback whales) might, but others with exponential distributions cannot. A straight line in double logarithmic plots indicates optimal coding.

Conclusion: Zipf's law likely stems from compression, and the study provides testable conditions for its emergence in compressing systems.

Abstract: Here we present a new class of optimality for coding systems. Members of that
class are displaced linearly from optimal coding and thus exhibit Zipf's law,
namely a power-law distribution of frequency ranks. Within that class, Zipf's
law, the size-rank law and the size-probability law form a group-like
structure. We identify human languages that are members of the class. All
languages showing sufficient agreement with Zipf's law are potential members of
the class. In contrast, there are communication systems in other species that
cannot be members of that class for exhibiting an exponential distribution
instead but dolphins and humpback whales might. We provide a new insight into
plots of frequency versus rank in double logarithmic scale. For any system, a
straight line in that scale indicates that the lengths of optimal codes under
non-singular coding and under uniquely decodable encoding are displaced by a
linear function whose slope is the exponent of Zipf's law. For systems under
compression and constrained to be uniquely decodable, such a straight line may
indicate that the system is coding close to optimality. We provide support for
the hypothesis that Zipf's law originates from compression and define testable
conditions for the emergence of Zipf's law in compressing systems.

</details>


### [167] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/pdf/2505.20538)
*Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, Stéphanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li*

Main category: cs.CL

TL;DR: AstroVisBench is introduced as the first benchmark for evaluating LLMs in astronomy-specific workflows and visualization, revealing gaps in their utility for scientific research.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to generate correct scientific insights through data processing and visualization, a capability not previously evaluated.

Method: Developed AstroVisBench to test LLMs on astronomy workflows and visualization, validated by professional astronomers.

Result: State-of-the-art LLMs show significant limitations in assisting astronomy research effectively.

Conclusion: AstroVisBench provides a foundation for improving LLMs in visualization-based scientific workflows across various domains.

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [168] [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/pdf/2505.20645)
*Kai Chen, Zihao He, Taiwei Shi, Kristina Lerman*

Main category: cs.CL

TL;DR: Steer-Bench is a benchmark to evaluate LLMs' ability to align outputs with community-specific norms, revealing gaps in steerability compared to human experts.


<details>
  <summary>Details</summary>
Motivation: Assessing LLMs' steerability for real-world applications, as current evaluations underrepresent diverse community norms.

Method: Steer-Bench uses 30 contrasting subreddit pairs, 10,000 instruction-response pairs, and 5,500 multiple-choice questions with silver labels.

Result: Best LLMs achieve ~65% accuracy, lagging behind human experts (81%), with some models 15+ points behind.

Conclusion: Steer-Bench highlights significant gaps in LLMs' community-sensitive steerability, emphasizing the need for improved alignment with diverse perspectives.

Abstract: Steerability, or the ability of large language models (LLMs) to adapt outputs
to align with diverse community-specific norms, perspectives, and communication
styles, is critical for real-world applications but remains under-evaluated. We
introduce Steer-Bench, a benchmark for assessing population-specific steering
using contrasting Reddit communities. Covering 30 contrasting subreddit pairs
across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs
and validated 5,500 multiple-choice question with corresponding silver labels
to test alignment with diverse community norms. Our evaluation of 13 popular
LLMs using Steer-Bench reveals that while human experts achieve an accuracy of
81% with silver labels, the best-performing models reach only around 65%
accuracy depending on the domain and configuration. Some models lag behind
human-level alignment by over 15 percentage points, highlighting significant
gaps in community-sensitive steerability. Steer-Bench is a benchmark to
systematically assess how effectively LLMs understand community-specific
instructions, their resilience to adversarial steering attempts, and their
ability to accurately represent diverse cultural and ideological perspectives.

</details>


### [169] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/pdf/2505.20875)
*Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi*

Main category: cs.CL

TL;DR: Trans-EnV evaluates LLMs on diverse English varieties, revealing performance disparities up to 46.3% lower on non-standard varieties.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations focus on Standard American English, neglecting global English diversity, raising fairness concerns.

Method: Trans-EnV transforms SAE datasets into 38 English varieties using expert knowledge and LLM-based transformations.

Result: Performance drops significantly on non-standard varieties, with accuracy decreasing by up to 46.3%.

Conclusion: Comprehensive linguistic robustness evaluation is crucial for fair LLM performance across diverse English varieties.

Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American
English (SAE), often overlooking the diversity of global English varieties.
This narrow focus may raise fairness concerns as degraded performance on
non-standard varieties can lead to unequal benefits for users worldwide.
Therefore, it is critical to extensively evaluate the linguistic robustness of
LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a
framework that automatically transforms SAE datasets into multiple English
varieties to evaluate the linguistic robustness. Our framework combines (1)
linguistics expert knowledge to curate variety-specific features and
transformation guidelines from linguistic literature and corpora, and (2)
LLM-based transformations to ensure both linguistic validity and scalability.
Using Trans-EnV, we transform six benchmark datasets into 38 English varieties
and evaluate seven state-of-the-art LLMs. Our results reveal significant
performance disparities, with accuracy decreasing by up to 46.3% on
non-standard varieties. These findings highlight the importance of
comprehensive linguistic robustness evaluation across diverse English
varieties. Each construction of Trans-EnV was validated through rigorous
statistical testing and consultation with a researcher in the field of second
language acquisition, ensuring its linguistic validity. Our code and datasets
are publicly available at https://github.com/jiyounglee-0523/TransEnV and
https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1.

</details>


### [170] [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/pdf/2505.21082)
*Jieyong Kim, Tongyoung Kim, Soojin Yoon, Jaehyung Kim, Dongha Lee*

Main category: cs.CL

TL;DR: RPM is a framework for reasoning-level personalization in black-box LLMs, outperforming response-level methods by aligning model reasoning with user logic.


<details>
  <summary>Details</summary>
Motivation: Black-box LLMs lack personalization in reasoning, leading to generalized responses. RPM addresses this by tailoring reasoning processes to user-specific logic.

Method: RPM constructs user-specific factors from history, builds personalized reasoning paths, and retrieves reasoning-aligned examples for inference.

Result: RPM consistently outperforms response-level personalization methods in diverse tasks.

Conclusion: Reasoning-level personalization enhances accuracy and interpretability in black-box LLMs.

Abstract: Large language models (LLMs) have recently achieved impressive performance
across a wide range of natural language tasks and are now widely used in
real-world applications. Among them, black-box LLMs--served via APIs without
access to model internals--are especially dominant due to their scalability and
ease of deployment. Despite their strong capabilities, these models typically
produce generalized responses that overlook personal preferences and reasoning
styles. This has led to growing interest in black-box LLM personalization,
which aims to tailor model outputs to user-specific context without modifying
model parameters. However, existing approaches primarily focus on
response-level personalization, attempting to match final outputs without
modeling personal thought process. To address this limitation, we propose RPM,
a framework for reasoning-level personalization that aligns the model's
reasoning process with a user's personalized logic. RPM first constructs
statistical user-specific factors by extracting and grouping
response-influential features from user history. It then builds personalized
reasoning paths that reflect how these factors are used in context. In the
inference stage, RPM retrieves reasoning-aligned examples for new queries via
feature-level similarity and performs inference conditioned on the structured
factors and retrieved reasoning paths, enabling the model to follow
user-specific reasoning trajectories. This reasoning-level personalization
enhances both predictive accuracy and interpretability by grounding model
outputs in user-specific logic through structured information. Extensive
experiments across diverse tasks show that RPM consistently outperforms
response-level personalization methods, demonstrating the effectiveness of
reasoning-level personalization in black-box LLMs.

</details>


### [171] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/pdf/2505.23001)
*Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi*

Main category: cs.CL

TL;DR: DyePack is a framework using backdoor attacks to detect if models trained on benchmark test sets, ensuring low false positive rates.


<details>
  <summary>Details</summary>
Motivation: Open benchmarks are vulnerable to test set contamination; DyePack addresses this by identifying models that misuse test data.

Method: DyePack mixes backdoor samples with test data, using stochastic targets for exact FPR computation.

Result: Detects contamination with FPRs as low as 0.000073% on MMLU-Pro and 0.127% on Alpaca.

Conclusion: DyePack effectively flags contaminated models with provably low false positives, ensuring benchmark integrity.

Abstract: Open benchmarks are essential for evaluating and advancing large language
models, offering reproducibility and transparency. However, their accessibility
makes them likely targets of test set contamination. In this work, we introduce
DyePack, a framework that leverages backdoor attacks to identify models that
used benchmark test sets during training, without requiring access to the loss,
logits, or any internal details of the model. Like how banks mix dye packs with
their money to mark robbers, DyePack mixes backdoor samples with the test data
to flag models that trained on it. We propose a principled design incorporating
multiple backdoors with stochastic targets, enabling exact false positive rate
(FPR) computation when flagging every model. This provably prevents false
accusations while providing strong evidence for every detected case of
contamination. We evaluate DyePack on five models across three datasets,
covering both multiple-choice and open-ended generation tasks. For
multiple-choice questions, it successfully detects all contaminated models with
guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard
using eight backdoors. For open-ended generation tasks, it generalizes well and
identifies all contaminated models on Alpaca with a guaranteed false positive
rate of just 0.127% using six backdoors.

</details>


### [172] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/pdf/2505.23276)
*Maged S. Al-Shaibani, Moataz Ahmed*

Main category: cs.CL

TL;DR: The paper investigates machine-generated Arabic text, identifying detectable linguistic patterns and developing a BERT-based detection model with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges LLMs pose to information integrity, especially in low-resource languages like Arabic, by detecting machine-generated text.

Method: Examined Arabic text generation strategies (title-only, content-aware, refinement) across models (ALLaM, Jais, Llama, GPT-4) using stylometric analysis. Developed BERT-based detection models.

Result: Detected distinctive linguistic patterns in machine-generated Arabic text. BERT models achieved up to 99.9% F1-score in formal contexts.

Conclusion: The study provides a foundation for robust detection systems to preserve information integrity in Arabic, highlighting domain-specific challenges.

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [173] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/pdf/2505.23811)
*Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen*

Main category: cs.CL

TL;DR: LayerIF is a data-driven framework using Influence Functions to estimate layer-wise training quality in LLMs, improving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook data influence, relying on model-centric heuristics, limiting LLM performance.

Method: LayerIF isolates layer gradients and computes influence on validation loss to derive task-specific layer importance.

Result: The framework improves expert allocation in LoRA-MoE and sparsity distribution for pruning, enhancing task performance.

Conclusion: LayerIF provides a principled, task-sensitive approach for layer-wise quality estimation, benefiting LLM applications.

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance. It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [174] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/pdf/2505.24554)
*Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti*

Main category: cs.CL

TL;DR: Bench4KE is an API-based benchmarking system for evaluating Knowledge Engineering automation tools, starting with Competency Question generation, using standardized metrics and a curated gold standard.


<details>
  <summary>Details</summary>
Motivation: The lack of standardization in evaluating LLM-based KE automation tools hinders methodological rigor and comparability.

Method: Introduces Bench4KE, a system with a gold standard dataset and similarity metrics to assess CQ generation tools.

Result: Provides a comparative analysis of four LLM-based CQ generation systems, setting a baseline for future work.

Conclusion: Bench4KE addresses evaluation gaps in KE automation and is extensible for other tasks like SPARQL query generation.

Abstract: The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.

</details>


### [175] [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/pdf/2505.24688)
*Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui*

Main category: cs.CL

TL;DR: Soft Reasoning improves LLM reasoning by embedding-based search with perturbation and Bayesian optimization, enhancing accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning due to limited diversity and inefficient search.

Method: Proposes Soft Reasoning: embedding perturbation for exploration and Bayesian optimization for refinement, guided by a verifier.

Result: Improves reasoning accuracy and coherence with minimal computation, scalable and model-agnostic.

Conclusion: Soft Reasoning offers a robust, efficient solution for enhancing LLM reasoning without heuristic search reliance.

Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited
diversity and inefficient search. We propose Soft Reasoning, an embedding-based
search framework that optimises the embedding of the first token to guide
generation. It combines (1) embedding perturbation for controlled exploration
and (2) Bayesian optimisation to refine embeddings via a verifier-guided
objective, balancing exploration and exploitation. This approach improves
reasoning accuracy and coherence while avoiding reliance on heuristic search.
Experiments demonstrate superior correctness with minimal computation, making
it a scalable, model-agnostic solution.

</details>


### [176] [MultiHoax: A Dataset of Multi-hop False-Premise Questions](https://arxiv.org/pdf/2506.00264)
*Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: The paper introduces MultiHoax, a benchmark to evaluate LLMs' ability to detect false premises in multi-step reasoning tasks, revealing their struggles in handling such complexities.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' critical reasoning skills, especially for multi-hop false-premise questions (FPQs), which are crucial for reliable outputs in high-stakes domains.

Method: The authors create MultiHoax, a benchmark dataset spanning seven countries and ten knowledge categories, using Wikipedia for factual reasoning.

Result: State-of-the-art LLMs struggle with false premise detection across countries, knowledge categories, and multi-hop reasoning types.

Conclusion: Improved false premise detection and robust multi-hop reasoning capabilities are needed in LLMs.

Abstract: As Large Language Models are increasingly deployed in high-stakes domains,
their ability to detect false assumptions and reason critically is crucial for
ensuring reliable outputs. False-premise questions (FPQs) serve as an important
evaluation method by exposing cases where flawed assumptions lead to incorrect
responses. While existing benchmarks focus on single-hop FPQs, real-world
reasoning often requires multi-hop inference, where models must verify
consistency across multiple reasoning steps rather than relying on
surface-level cues. To address this gap, we introduce MultiHoax, a benchmark
for evaluating LLMs' ability to handle false premises in complex, multi-step
reasoning tasks. Our dataset spans seven countries and ten diverse knowledge
categories, using Wikipedia as the primary knowledge source to enable factual
reasoning across regions. Experiments reveal that state-of-the-art LLMs
struggle to detect false premises across different countries, knowledge
categories, and multi-hop reasoning types, highlighting the need for improved
false premise detection and more robust multi-hop reasoning capabilities in
LLMs.

</details>


### [177] [ARIA: Training Language Agents with Intention-Driven Reward Aggregation](https://arxiv.org/pdf/2506.00539)
*Ruihan Yang, Yikai Zhang, Aili Chen, Xintao Wang, Siyu Yuan, Jiangjie Chen, Deqing Yang, Yanghua Xiao*

Main category: cs.CL

TL;DR: ARIA reduces reward variance in LLM-based RL by clustering actions in intention space, improving performance by 9.95% on average.


<details>
  <summary>Details</summary>
Motivation: Addressing reward sparsity and high variance in large action spaces for LLM-based agents in open-ended environments.

Method: Projects language actions into a low-dimensional intention space, clustering semantically similar actions for shared rewards.

Result: Reduces policy gradient variance and achieves 9.95% average performance gain across tasks.

Conclusion: ARIA effectively improves RL training for language agents by densifying reward signals in intention space.

Abstract: Large language models (LLMs) have enabled agents to perform complex reasoning
and decision-making through free-form language interactions. However, in
open-ended language action environments (e.g., negotiation or question-asking
games), the action space can be formulated as a joint distribution over tokens,
resulting in an exponentially large action space. Sampling actions in such a
space can lead to extreme reward sparsity, which brings large reward variance,
hindering effective reinforcement learning (RL). To address this, we propose
ARIA, a method that Aggregates Rewards in Intention space to enable efficient
and effective language Agents training. ARIA aims to project natural language
actions from the high-dimensional joint token distribution space into a
low-dimensional intention space, where semantically similar actions are
clustered and assigned shared rewards. This intention-aware reward aggregation
reduces reward variance by densifying reward signals, fostering better policy
optimization. Extensive experiments demonstrate that ARIA not only
significantly reduces policy gradient variance, but also delivers substantial
performance gains of an average of 9.95% across four downstream tasks,
consistently outperforming offline and online RL baselines.

</details>


### [178] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/pdf/2506.01723)
*Soyoung Oh, Xinting Huang, Mathis Pink, Michael Hahn, Vera Demberg*

Main category: cs.CL

TL;DR: The paper investigates how a large transformer model processes idioms, identifying specific mechanisms for retrieving figurative meanings while maintaining literal interpretations.


<details>
  <summary>Details</summary>
Motivation: Idioms challenge language models due to their non-compositional meanings, requiring models to handle both figurative and literal interpretations.

Method: Mechanistic interpretability tools are used to analyze how a pretrained transformer (LLama3.2-1B-base) processes idioms, focusing on attention and MLP layers.

Result: Three processing steps are identified: figurative meaning retrieval in early layers, suppression of literal interpretation, and parallel paths for both meanings.

Conclusion: The study provides mechanistic evidence for idiom comprehension in transformers, highlighting dual-path processing.

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [179] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/pdf/2506.02132)
*Michael Li, Nishant Subramani*

Main category: cs.CL

TL;DR: The paper investigates how modern transformer-based language models encode lexical identity and inflectional morphology, revealing consistent patterns across architectures and sizes.


<details>
  <summary>Details</summary>
Motivation: To understand how contemporary large language models (LLMs) represent linguistic information compared to early models like BERT and GPT-2.

Method: Train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features across 16 models.

Result: Lexical information is concentrated linearly in early layers and nonlinearly in later layers, while inflectional information remains uniformly accessible and linearly separable.

Conclusion: Transformer models organize linguistic information similarly across architectures, suggesting these properties are fundamental for next-token prediction.

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing

</details>


### [180] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/pdf/2506.02426)
*Maryam Berijanian, Kuldeep Singh, Amin Sehati*

Main category: cs.CL

TL;DR: Comparative analysis of three AI agent architectures for relation classification using LLMs, showing multi-agent coordination outperforms few-shot prompting and nears fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: Address challenges in entity relationship classification with limited labeled data and complex relational structures.

Method: Evaluate three agentic architectures: reflective self-evaluation, hierarchical task decomposition, and dynamic example generation with cooperative/adversarial prompting.

Result: Multi-agent coordination outperforms few-shot prompting and approaches fine-tuned model performance.

Conclusion: Provides practical guidance for designing modular, generalizable LLM-based systems for structured relation extraction.

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at https://github.com/maryambrj/ALIEN.git.

</details>


### [181] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/pdf/2506.02442)
*Utsav Maskey, Mark Dras, Usman Naseem*

Main category: cs.CL

TL;DR: The paper evaluates LLM safety on long-tail encrypted texts, identifying vulnerabilities like mismatched-generalization attacks and assessing safeguards.


<details>
  <summary>Details</summary>
Motivation: To understand LLM safety in handling encrypted or obfuscated texts and identify potential risks.

Method: A two-dimensional framework tests LLM safety: instruction refusal and generation safety, followed by experiments on decryption capabilities.

Result: Models decrypting ciphers may fail safety tests, leading to unsafe responses or over-refusal. Pre- and post-LLM safeguards are evaluated.

Conclusion: The study highlights LLM safety gaps in long-tail scenarios and suggests directions for robust safety mechanisms.

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [182] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/pdf/2506.02544)
*Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie*

Main category: cs.CL

TL;DR: CoRe-MMRAG addresses inconsistencies in multimodal knowledge retrieval by reconciling parametric and retrieved knowledge, improving reliability and performance.


<details>
  <summary>Details</summary>
Motivation: To resolve challenges of Parametric-Retrieved Knowledge Inconsistency (PRKI) and Visual-Textual Knowledge Inconsistency (VTKI) in Multimodal RAG.

Method: A four-stage pipeline: internal response generation, multimodal evidence selection, external response generation, and integration. Includes a specialized training paradigm.

Result: Achieves 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA benchmarks.

Conclusion: CoRe-MMRAG effectively reconciles knowledge inconsistencies, enhancing multimodal retrieval-augmented generation.

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose Cross-source knowledge \textbf{Re}conciliation for
Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively
reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a
four-stage pipeline: it first generates an internal response from parametric
knowledge, then selects the most relevant multimodal evidence via joint
similarity assessment, generates an external response, and finally integrates
both to produce a reliable answer. Additionally, a specialized training
paradigm enhances knowledge source discrimination, multimodal integration, and
unified answer generation. Experiments on KB-VQA benchmarks show that
CoRe-MMRAG achieves substantial improvements over baseline methods, achieving
5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.

</details>


### [183] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/pdf/2506.02689)
*Liang Yue, Yihong Tang, Kehai Chen, Jie Liu, Min Zhang*

Main category: cs.CL

TL;DR: MASTER is a data augmentation method using multi-agent interactions to generate high-quality fine-tuning data, improving model performance and reasoning.


<details>
  <summary>Details</summary>
Motivation: Obtaining high-quality fine-tuning data for large NLP models is challenging due to data collection difficulties and costs.

Method: MASTER enriches data through multi-agent interactions, simulating teaching scenarios to create teacher-student interaction data.

Result: Models fine-tuned with MASTER-augmented data (BOOST-QA) excel in benchmarks and show strong multitask generalization, especially in reasoning.

Conclusion: MASTER effectively addresses data scarcity, enhancing model performance and offering insights for future research.

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [184] [On Entity Identification in Language Models](https://arxiv.org/pdf/2506.02701)
*Masaki Sakata, Sho Yokoi, Benjamin Heinzerling, Takumi Ito, Kentaro Inui*

Main category: cs.CL

TL;DR: The paper analyzes how language models (LMs) internally represent named entities, focusing on ambiguity and variability in mentions. It proposes a clustering-based framework to evaluate entity mention identification and distinction, achieving precision and recall metrics of 0.66 to 0.9. Findings reveal compact, low-dimensional entity representations in early LM layers and their impact on word prediction.


<details>
  <summary>Details</summary>
Motivation: To understand how LMs internally identify and distinguish named entity mentions, addressing the challenges of ambiguity and variability in entity-mention relationships.

Method: A clustering-based framework is applied to LM internal representations, quantifying how mentions of the same entity cluster and mentions of different entities separate. Experiments involve five Transformer-based autoregressive models.

Result: LMs effectively identify and distinguish entities, with precision and recall metrics ranging from 0.66 to 0.9. Entity information is compactly represented in early layers and influences word prediction.

Conclusion: The study reveals isomorphism between LM representations and real-world entity-centric knowledge, offering insights into how LMs organize and use entity information internally.

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [185] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/pdf/2506.03106)
*Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng*

Main category: cs.CL

TL;DR: Critique-GRPO, an RL framework combining natural language and numerical feedback, outperforms traditional methods by leveraging critiques for refinement, improving reasoning tasks by 4.5-5%.


<details>
  <summary>Details</summary>
Motivation: Addressing performance plateaus, limited self-reflection, and persistent failures in RL with numerical feedback alone.

Method: Proposes Critique-GRPO, integrating natural language critiques and numerical feedback for policy optimization, tested on Qwen models.

Result: Outperforms supervised and RL-based fine-tuning, improving pass@1 scores by ~4.5-5%, surpassing expert-demonstration baselines.

Conclusion: Critique-GRPO effectively combines feedback types for better RL performance, with insights on exploration efficiency.

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [186] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/pdf/2506.03162)
*Damith Chamalke Senadeera, Xiaoyun Yang, Dimitrios Kollias, Gregory Slabaugh*

Main category: cs.CV

TL;DR: Proposes Dual Branch VideoMamba with GCTF for efficient violence detection in videos, combining spatial and temporal features via SSM backbone, achieving SOTA performance on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitations of CNNs and Transformers in handling long-term dependencies and computational efficiency for automated violence detection in surveillance videos.

Method: Introduces a dual-branch architecture with SSM backbone: one branch for spatial features, another for temporal dynamics, fused via a gating mechanism (GCTF). Merges RWF-2000, RLVS, and VioPeru datasets for benchmarking.

Result: Achieves state-of-the-art performance on the new benchmark, balancing accuracy and computational efficiency.

Conclusion: Demonstrates the potential of SSMs for scalable, real-time violence detection in surveillance systems.

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [187] [Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs](https://arxiv.org/pdf/2506.03168)
*Dawen Jiang, Zhishu Shen, Qiushi Zheng, Tiehua Zhang, Wei Xiang, Jiong Jin*

Main category: cs.CV

TL;DR: Farm-LightSeek integrates LLMs with edge computing for smart agriculture, addressing challenges like expert reliance and real-time decision-making. It features a closed-loop architecture, cross-modal monitoring, and lightweight LLM deployment, proving effective in real-world tests.


<details>
  <summary>Details</summary>
Motivation: Global population growth and climate change demand efficient agricultural IoT systems. Current AI-based smart agriculture faces issues like expert reliance, multimodal data fusion, and real-time edge decision-making.

Method: Farm-LightSeek uses edge computing and LLMs to collect real-time farmland data (images, weather, etc.), performs cross-modal reasoning and disease detection at edge nodes, and enables low-latency decisions with cloud collaboration.

Result: Experiments on real-world datasets show Farm-LightSeek achieves reliable performance in critical tasks despite edge resource limitations.

Conclusion: Farm-LightSeek advances real-time agricultural solutions and demonstrates the potential of deeper LLM-IoT integration in agriculture.

Abstract: Amid the challenges posed by global population growth and climate change,
traditional agricultural Internet of Things (IoT) systems is currently
undergoing a significant digital transformation to facilitate efficient big
data processing. While smart agriculture utilizes artificial intelligence (AI)
technologies to enable precise control, it still encounters significant
challenges, including excessive reliance on agricultural expert knowledge,
difficulties in fusing multimodal data, poor adaptability to dynamic
environments, and bottlenecks in real-time decision-making at the edge. Large
language models (LLMs), with their exceptional capabilities in knowledge
acquisition and semantic understanding, provide a promising solution to address
these challenges. To this end, we propose Farm-LightSeek, an edge-centric
multimodal agricultural IoT data analytics framework that integrates LLMs with
edge computing. This framework collects real-time farmland multi-source data
(images, weather, geographic information) via sensors, performs cross-modal
reasoning and disease detection at edge nodes, conducts low-latency management
decisions, and enables cloud collaboration for model updates. The main
innovations of Farm-LightSeek include: (1) an agricultural
"perception-decision-action" closed-loop architecture; (2) cross-modal adaptive
monitoring; and (3)a lightweight LLM deployment strategy balancing performance
and efficiency. Experiments conducted on two real-world datasets demonstrate
that Farm-LightSeek consistently achieves reliable performance in
mission-critical tasks, even under the limitations of edge computing resources.
This work advances intelligent real-time agricultural solutions and highlights
the potential for deeper integration of agricultural IoT with LLMs.

</details>


### [188] [Sounding that Object: Interactive Object-Aware Image to Audio Generation](https://arxiv.org/pdf/2506.04214)
*Tingle Li, Baihe Huang, Xiaobin Zhuang, Dongya Jia, Jiawei Chen, Yuping Wang, Zhuo Chen, Gopala Anumanchipalli, Yuxuan Wang*

Main category: cs.CV

TL;DR: Proposes an interactive object-aware audio generation model using a conditional latent diffusion framework, validated by multi-modal attention and image segmentation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating accurate sounds for complex audio-visual scenes with multiple objects and sound sources.

Method: Integrates object-centric learning into a conditional latent diffusion model, using multi-modal attention to associate image regions with sounds. Employs image segmentation for interactive sound generation at the object level.

Result: Outperforms baselines in aligning objects with their sounds, validated quantitatively and qualitatively.

Conclusion: The model effectively grounds sound generation in user-selected visual objects, ensuring accurate audio alignment.

Abstract: Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/

</details>


### [189] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/pdf/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: A hybrid-ensemble-based group pose estimation method is proposed to improve real-time multi-person pose detection, enhancing robustness and accuracy for health applications.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on pose estimation for human movement measurement and its potential to supplement videos drives the need for improved methods.

Method: The approach combines modified group pose estimation and real-time pose estimation, using pose transformation for feature identification and training on benchmark datasets.

Result: The method outperforms others in real-time pose estimation, offering better occlusion robustness and dense regression accuracy.

Conclusion: The proposed method shows promise for real-time applications, particularly in improving human health life span.

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [190] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/pdf/2506.03170)
*Murthy L, Subarna Tripathi*

Main category: cs.CV

TL;DR: Proposes a method for neural fingerprinting in text-to-image diffusion models using cyclic error correcting codes to achieve perfect attribution accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses the risk of misuse in open-source text-to-image generative models by improving attribution accuracy.

Method: Leverages cyclic error correcting codes from coding theory for neural fingerprinting.

Result: Aims for perfect attribution accuracy, addressing the trade-off with generation quality.

Conclusion: The proposed method is deployable due to its potential for perfect accuracy.

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [191] [ReactDiff: Latent Diffusion for Facial Reaction Generation](https://arxiv.org/pdf/2505.14151)
*Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang*

Main category: cs.CV

TL;DR: ReactDiff is a framework for generating listener facial reactions from speaker audio-visual clips, using multi-modal inputs and latent diffusion for diversity and realism.


<details>
  <summary>Details</summary>
Motivation: The challenge is capturing relevance between video and audio while balancing appropriateness, realism, and diversity in facial reaction generation.

Method: ReactDiff integrates a Multi-Modality Transformer with conditional diffusion in latent space, using intra- and inter-class attention for fine-grained interaction.

Result: ReactDiff achieves a facial reaction correlation of 0.26 and diversity score of 0.094, outperforming existing methods.

Conclusion: ReactDiff enhances facial reaction generation with improved diversity and realism, demonstrated by superior performance metrics.

Abstract: Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.

</details>


### [192] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/pdf/2506.03171)
*Ghulam Mujtaba, Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum is a lightweight method for real-time, personalized video summarization on edge devices, using thumbnail containers and efficient neural architectures to reduce computational complexity while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of computational efficiency, personalization, and privacy in video summarization, especially on resource-constrained edge devices.

Method: Uses thumbnail containers and a hierarchical 2D CNN model to identify user-preferred content and generate fast-forward summaries locally.

Result: Demonstrates real-time summarization on devices like Jetson Nano, tailored to individual preferences for movies, sports, and TV shows.

Conclusion: EdgeVidSum successfully balances efficiency, personalization, and privacy, making it suitable for modern video consumption on edge devices.

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [193] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/pdf/2506.03173)
*Xiaoyi Liu, Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE is a physics-informed multimodal world model for unbounded surface growth, using a shared latent state and physics-aware prediction. It outperforms baselines in physical intelligence tasks.


<details>
  <summary>Details</summary>
Motivation: To advance physical intelligence by modeling unbounded accretive surface growth from partial, multisensory observations.

Method: Uses a unified context encoder, physics-aware predictor, and Modality-Agnostic Growth Embedding (MAGE) with techniques like Age Positional Encoding and Energy-Gated Message-Passing.

Result: Outperforms specialized baselines in tasks like topology recognition and cross-modal retrieval, demonstrating robustness.

Conclusion: FOLIAGE establishes a new multimodal pathway for physical intelligence, excelling in dynamic environments.

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [194] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/pdf/2506.03174)
*Koki Matsuishi, Kosuke Ukita, Tsuyoshi Okita*

Main category: cs.CV

TL;DR: The paper proposes AURA-MFM, a multimodal foundation model integrating third-person video, motion capture, IMU, and text for detailed human activity analysis, outperforming existing methods in retrieval and activity recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models lack detailed full-body human activity analysis, prompting the development of AURA-MFM to incorporate more comprehensive modalities.

Method: AURA-MFM integrates four modalities (third-person video, motion capture, IMU, and text) and uses a Transformer-based IMU encoder for enhanced performance.

Result: The model achieved superior results in zero-shot action recognition (F1-score: 0.6226, accuracy: 0.7320) compared to existing methods (F1-score: 0.0747, accuracy: 0.1961).

Conclusion: AURA-MFM advances multimodal human activity analysis by leveraging diverse data sources and outperforms current methods in key tasks.

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


### [195] [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/pdf/2506.03179)
*Qi Li, Runpeng Yu, Xinchao Wang*

Main category: cs.CV

TL;DR: Vid-SME is a new method for detecting improperly used videos in training datasets of multimodal large language models (MLLMs), addressing privacy concerns by leveraging Sharma-Mittal entropy (SME) and temporal variations in video frames.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of MLLMs in video understanding raises data privacy concerns, especially with sensitive video content in training datasets. Existing methods for membership inference attacks (MIAs) fail in the video domain.

Method: Vid-SME uses model output confidence and adaptive parameterization to compute SME for video inputs, comparing natural and reversed frames to derive membership scores.

Result: Experiments show Vid-SME is highly effective in detecting training set membership for various video understanding LLMs.

Conclusion: Vid-SME successfully addresses the challenge of detecting improperly used videos in MLLM training, offering a scalable and robust solution.

Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities
in handling complex multimodal tasks and are increasingly adopted in video
understanding applications. However, their rapid advancement raises serious
data privacy concerns, particularly given the potential inclusion of sensitive
video content, such as personal recordings and surveillance footage, in their
training datasets. Determining improperly used videos during training remains a
critical and unresolved challenge. Despite considerable progress on membership
inference attacks (MIAs) for text and image data in MLLMs, existing methods
fail to generalize effectively to the video domain. These methods suffer from
poor scalability as more frames are sampled and generally achieve negligible
true positive rates at low false positive rates (TPR@Low FPR), mainly due to
their failure to capture the inherent temporal variations of video frames and
to account for model behavior differences as the number of frames varies. To
address these challenges, we introduce Vid-SME, the first membership inference
method tailored for video data used in video understanding LLMs (VULLMs).
Vid-SME leverages the confidence of model output and integrates adaptive
parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By
leveraging the SME difference between natural and temporally-reversed video
frames, Vid-SME derives robust membership scores to determine whether a given
video is part of the model's training set. Experiments on various self-trained
and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

</details>


### [196] [TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models](https://arxiv.org/pdf/2506.03182)
*Shivani Chiranjeevi, Hossein Zaremehrjerdi, Zi K. Deng, Talukder Z. Jubery, Ari Grele, Arti Singh, Asheesh K Singh, Soumik Sarkar, Nirav Merchant, Harold F. Greeney, Baskar Ganapathysubramanian, Chinmay Hegde*

Main category: cs.CV

TL;DR: TerraIncognita is a benchmark for evaluating AI models in identifying unknown insect species from images, combining known and rare species data to test hierarchical classification and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Address the urgent biodiversity crisis by improving slow, manual insect species discovery methods with AI-driven solutions.

Method: Introduces TerraIncognita, a dynamic benchmark with expertly annotated images of known and rare insect species, assessing models' hierarchical classification, OOD detection, and explanation generation.

Result: Top models achieve >90% F1 at Order level but drop to <2% at Species level, showing the challenge of fine taxonomic prediction.

Conclusion: TerraIncognita provides an evolving platform for AI benchmarking in biodiversity, with regular updates to include new species data.

Abstract: The rapid global loss of biodiversity, particularly among insects, represents
an urgent ecological crisis. Current methods for insect species discovery are
manual, slow, and severely constrained by taxonomic expertise, hindering timely
conservation actions. We introduce TerraIncognita, a dynamic benchmark designed
to evaluate state-of-the-art multimodal models for the challenging problem of
identifying unknown, potentially undescribed insect species from image data.
Our benchmark dataset combines a mix of expertly annotated images of insect
species likely known to frontier AI models, and images of rare and poorly known
species, for which few/no publicly available images exist. These images were
collected from underexplored biodiversity hotspots, realistically mimicking
open-world discovery scenarios faced by ecologists. The benchmark assesses
models' proficiency in hierarchical taxonomic classification, their capability
to detect and abstain from out-of-distribution (OOD) samples representing novel
species, and their ability to generate explanations aligned with expert
taxonomic knowledge. Notably, top-performing models achieve over 90\% F1 at the
Order level on known species, but drop below 2\% at the Species level,
highlighting the sharp difficulty gradient from coarse to fine taxonomic
prediction (Order $\rightarrow$ Family $\rightarrow$ Genus $\rightarrow$
Species). TerraIncognita will be updated regularly, and by committing to
quarterly dataset expansions (of both known and novel species), will provide an
evolving platform for longitudinal benchmarking of frontier AI methods. All
TerraIncognita data, results, and future updates are available
\href{https://baskargroup.github.io/TerraIncognita/}{here}.

</details>


### [197] [UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation](https://arxiv.org/pdf/2506.04134)
*Jinting Wang, Shan Yang, Li Liu*

Main category: cs.CV

TL;DR: UniCUE is a unified framework for CSV2S that directly generates speech from CS videos, avoiding intermediate text, and improves performance significantly.


<details>
  <summary>Details</summary>
Motivation: Current CSV2S methods rely on intermediate text, leading to error propagation and misalignment. UniCUE aims to address these issues by directly mapping CS videos to speech.

Method: UniCUE integrates CSR for fine-grained visual-semantic alignment, uses a semantic alignment pool, a VisioPhonetic adapter, and a pose-aware visual processor.

Result: UniCUE reduces Word Error Rate by 78.3% and improves lip-speech synchronization by 32% compared to single CSV2S.

Conclusion: UniCUE effectively bridges the gap between CS videos and speech, outperforming existing methods.

Abstract: Cued Speech (CS) enhances lipreading through hand coding, providing precise
speech perception support for the hearing-impaired. CS Video-to-Speech
generation (CSV2S) task aims to convert the CS visual expressions (CS videos)
of hearing-impaired individuals into comprehensible speech signals. Direct
generation of speech from CS video (called single CSV2S) yields poor
performance due to insufficient CS data. Current research mostly focuses on CS
Recognition (CSR), which convert video content into linguistic text. Based on
this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech
system. This combined architecture relies on text as an intermediate medium for
stepwise cross-modal alignment, which may lead to error propagation and
temporal misalignment between speech and video dynamics. To address these
challenges, we propose a novel approach that directly generates speech from CS
videos without relying on intermediate text. Building upon this, we propose
UniCUE, the first unified framework for CSV2S, whose core innovation lies in
the integration of the CSR task that provides fine-grained visual-semantic
information to facilitate speech generation from CS videos. More precisely, (1)
a novel fine-grained semantic alignment pool to ensure precise mapping between
visual features and speech contents; (2) a VisioPhonetic adapter to bridge
cross-task representations, ensuring seamless compatibility between two
distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is
introduced to enhance fine-grained spatiotemporal correlations between lip and
hand movements in CS video. Experiments on our new established Chinese CS
dataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our
UniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech
synchronization by 32% compared to the single CSV2S.

</details>


### [198] [Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset](https://arxiv.org/pdf/2506.03184)
*Mahe Zabin, Ho-Jin Choi, Md. Monirul Islam, Jia Uddin*

Main category: cs.CV

TL;DR: The paper explores how tuning parameters (pooling, activation function, optimizer) affect a DCNN's performance, finding maxpooling with Adam optimizer and tanh activation works best.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of parameter tuning on DCNN performance for crack image classification.

Method: Used a DCNN with 2 convolutional, 2 pooling, 1 dropout, and 1 dense layer, tested on a crack image dataset with maxpooling, Adam optimizer, and tanh activation.

Result: Maxpooling with Adam optimizer and tanh activation yielded the best performance.

Conclusion: Optimal DCNN performance for crack classification is achieved with specific parameter tuning.

Abstract: The performance of a classifier depends on the tuning of its parame ters. In
this paper, we have experimented the impact of various tuning parameters on the
performance of a deep convolutional neural network (DCNN). In the ex perimental
evaluation, we have considered a DCNN classifier that consists of 2
convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.
To observe the impact of pooling, activation function, and optimizer tuning pa
rameters, we utilized a crack image dataset having two classes: negative and
pos itive. The experimental results demonstrate that with the maxpooling, the
DCNN demonstrates its better performance for adam optimizer and tanh activation
func tion.

</details>


### [199] [Continual Learning in Vision-Language Models via Aligned Model Merging](https://arxiv.org/pdf/2506.03189)
*Ghada Sokar, Gintare Karolina Dziugaite, Anurag Arnab, Ahmet Iscen, Pablo Samuel Castro, Cordelia Schmid*

Main category: cs.CV

TL;DR: A model merging approach for continual learning balances stability and plasticity, reducing forgetting and improving generalization in Vision-Language Models.


<details>
  <summary>Details</summary>
Motivation: Sequential fine-tuning in continual learning favors plasticity over stability, leading to catastrophic forgetting and bias towards recent tasks.

Method: Proposes merging newly trained task parameters with prior ones, using a mechanism to align weights and avoid interference.

Result: Reduces forgetting, enhances robustness to task orders/similarities, and improves generalization in VLMs.

Conclusion: Model merging offers a balanced solution for continual learning, outperforming sequential fine-tuning.

Abstract: Continual learning is conventionally tackled through sequential fine-tuning,
a process that, while enabling adaptation, inherently favors plasticity over
the stability needed to retain prior knowledge. While existing approaches
attempt to mitigate catastrophic forgetting, a bias towards recent tasks
persists as they build upon this sequential nature. In this work we present a
new perspective based on model merging to maintain stability while still
retaining plasticity. Rather than just sequentially updating the model weights,
we propose merging newly trained task parameters with previously learned ones,
promoting a better balance. To maximize the effectiveness of the merging
process, we propose a simple mechanism that promotes learning aligned weights
with previous ones, thereby avoiding interference when merging. We evaluate
this approach on large Vision-Language Models (VLMs), and demonstrate its
effectiveness in reducing forgetting, increasing robustness to various task
orders and similarities, and improving generalization.

</details>


### [200] [MINT: Memory-Infused Prompt Tuning at Test-time for CLIP](https://arxiv.org/pdf/2506.03190)
*Jiaming Yi, Ruirui Pan, Jishen Yang, Xiulong Yang*

Main category: cs.CV

TL;DR: MINT introduces a Memory Prompt Bank for dynamic adaptation of VLMs to test-time data shifts, improving generalization without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods fail to fully utilize internal model knowledge for adapting to hierarchical visual semantics.

Method: MINT uses a Memory Prompt Bank (MPB) with key-value prompt pairs, retrieved by test image features to assemble Associative Prompts for encoder injection.

Result: MINT enables rapid, precise VLM adaptation at test time without source data or retraining.

Conclusion: MINT effectively addresses generalization challenges in VLMs under distribution shifts by leveraging memory-infused prompt tuning.

Abstract: Improving the generalization ability of Vision-Language Pre-trained Models
(VLMs) under test-time data distribution shifts remains a critical challenge.
The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging
the model's internal knowledge, particularly in dynamically adapting to complex
and hierarchical visual semantic information. In this paper, we propose
Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue.
Inspired by human associative memory theory, MINT introduces a Memory Prompt
Bank (MPB), which stores learnable key-value prompt pairs that work as a memory
of previously seen samples. During the test time, relevant prompt pairs in the
MPB are retrieved by the hierarchical visual features of test images to
dynamically assemble Associative Prompts. The associative prompts are then
injected into the image encoder for fine-grained, customized visual contextual
guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,
precise VLM adaptation at test time by leveraging this MPB-acquired memory,
without source data or retraining. The code is available at
https://github.com/Jamieyi2004/MINT.

</details>


### [201] [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/pdf/2506.03191)
*Muhammad Islam, Tao Huang, Euijoon Ahn, Usman Naseem*

Main category: cs.CV

TL;DR: Survey on multimodal GenAI and LLMs for human motion understanding and generation, focusing on text-to-motion synthesis.


<details>
  <summary>Details</summary>
Motivation: To explore how textual descriptions can guide realistic human motion generation and advance motion synthesis technologies.

Method: Analyzes generative approaches like autoregressive models, diffusion models, GANs, VAEs, and transformers, evaluating their strengths and limitations.

Result: Highlights advances in text-conditioned motion generation and the role of LLMs in improving semantic alignment and coherence.

Conclusion: Demonstrates the transformative potential of text-to-motion GenAI and LLMs in various applications, while addressing challenges in realism and efficiency.

Abstract: This paper presents an in-depth survey on the use of multimodal Generative
Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)
for human motion understanding and generation, offering insights into emerging
methods, architectures, and their potential to advance realistic and versatile
motion synthesis. Focusing exclusively on text and motion modalities, this
research investigates how textual descriptions can guide the generation of
complex, human-like motion sequences. The paper explores various generative
approaches, including autoregressive models, diffusion models, Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
transformer-based models, by analyzing their strengths and limitations in terms
of motion quality, computational efficiency, and adaptability. It highlights
recent advances in text-conditioned motion generation, where textual inputs are
used to control and refine motion outputs with greater precision. The
integration of LLMs further enhances these models by enabling semantic
alignment between instructions and motion, improving coherence and contextual
relevance. This systematic survey underscores the transformative potential of
text-to-motion GenAI and LLM architectures in applications such as healthcare,
humanoids, gaming, animation, and assistive technologies, while addressing
ongoing challenges in generating efficient and realistic human motion.

</details>


### [202] [Human Fall Detection using Transfer Learning-based 3D CNN](https://arxiv.org/pdf/2506.03193)
*Ekram Alam, Abu Sufian, Paramartha Dutta, Marco Leo*

Main category: cs.CV

TL;DR: A vision-based fall detection system for seniors using a pre-trained 3D CNN and SVM classifier, achieving efficient classification with minimal training time.


<details>
  <summary>Details</summary>
Motivation: Addressing the health issue of accidental falls in the growing senior population by automating fall detection.

Method: Utilizes a pre-trained 3D CNN for spatio-temporal feature extraction and trains only an SVM classifier, validated with stratified shuffle split.

Result: Tested on GMDCSA and CAUCAFall datasets, demonstrating effective fall detection.

Conclusion: The proposed system is efficient and practical for real-world fall detection in seniors.

Abstract: Unintentional or accidental falls are one of the significant health issues in
senior persons. The population of senior persons is increasing steadily. So,
there is a need for an automated fall detection monitoring system. This paper
introduces a vision-based fall detection system using a pre-trained 3D CNN.
Unlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The
proposed model leverages the original learned weights of a 3D CNN model
pre-trained on the Sports1M dataset to extract the spatio-temporal features.
Only the SVM classifier was trained, which saves the time required to train the
3D CNN. Stratified shuffle five split cross-validation has been used to split
the dataset into training and testing data. Extracted features from the
proposed 3D CNN model were fed to an SVM classifier to classify the activity as
fall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the
experiment. The source code for this work can be accessed via the following
link: https://github.com/ekramalam/HFD_3DCNN.

</details>


### [203] [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/pdf/2506.03194)
*Rynaa Grover, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Nilay Pande*

Main category: cs.CV

TL;DR: HueManity benchmark reveals MLLMs' poor performance on nuanced visual perception tasks compared to humans and traditional CV models.


<details>
  <summary>Details</summary>
Motivation: To assess and highlight the limitations of MLLMs in visual perception tasks, particularly in precise pattern recognition.

Method: Created HueManity, a dataset of 83,850 images with alphanumeric strings in Ishihara-style patterns, and evaluated nine MLLMs against human and ResNet50 baselines.

Result: MLLMs performed poorly (33.6% on 'easy', 3% on 'hard' tasks) vs. humans (~100%, 95.6%) and ResNet50 (96.5%, 94.5%).

Conclusion: Current MLLMs lack perceptual robustness; HueManity is open-sourced to encourage research in bridging this gap.

Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual
reasoning, but their performance on nuanced perceptual tasks remains
surprisingly limited. We present HueManity, a benchmark designed to assess
visual perception in MLLMs. The dataset comprises 83,850 images featuring
two-character alphanumeric strings embedded in Ishihara test style dot
patterns, challenging models on precise pattern recognition. Our evaluation of
nine state-of-the-art MLLMs on HueManity demonstrates a significant performance
deficit compared to human and traditional computer vision baselines. The
best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a
striking 3% on the alphanumeric `hard' task. In contrast, human participants
achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model
reached accuracies of 96.5% and 94.5%. These results highlight a critical gap
in the visual capabilities of current MLLMs. Our analysis further explores
potential architectural and training-paradigm factors contributing to this
perceptual gap in MLLMs. We open-source HueManity dataset and code to foster
further research in improving perceptual robustness of MLLMs.

</details>


### [204] [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/pdf/2506.03195)
*Yunqi Hong, Sohyun An, Andrew Bai, Neil Y. C. Lin, Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: AutoSEP is a self-supervised prompt learning framework that enhances MLLMs' fine-grained image classification without training, improving accuracy by 13% over zero-shot and 5% over baselines.


<details>
  <summary>Details</summary>
Motivation: Fine-grained image classification is challenging for MLLMs due to overlooked subtle details. AutoSEP addresses this by leveraging unlabeled data to guide MLLMs.

Method: AutoSEP iteratively improves a description prompt using unlabeled data and instance-level scoring, requiring only black-box MLLM access.

Result: AutoSEP outperforms baselines, improving accuracy by 13% over zero-shot and 5% over the best baseline.

Conclusion: AutoSEP effectively enhances MLLMs' fine-grained classification without training, demonstrating the value of self-supervised prompt learning.

Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on
general zero-shot image classification tasks, fine-grained image classification
remains challenging. It demands precise attention to subtle visual details to
distinguish between visually similar subcategories--details that MLLMs may
easily overlook without explicit guidance. To address this, we introduce
AutoSEP, an iterative self-supervised prompt learning framework designed to
enhance MLLM fine-grained classification capabilities in a fully unsupervised
manner. Our core idea is to leverage unlabeled data to learn a description
prompt that guides MLLMs in identifying crucial discriminative features within
an image, and boosts classification accuracy. We developed an automatic
self-enhancing prompt learning framework called AutoSEP to iteratively improve
the description prompt using unlabeled data, based on instance-level
classification scoring function. AutoSEP only requires black-box access to
MLLMs, eliminating the need for any training or fine-tuning. We evaluate our
approach on multiple fine-grained classification datasets. It consistently
outperforms other unsupervised baselines, demonstrating the effectiveness of
our self-supervised optimization framework. Notably, AutoSEP on average
improves 13 percent over standard zero-shot classification and 5 percent over
the best-performing baselines. Code is available at:
https://github.com/yq-hong/AutoSEP

</details>


### [205] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/pdf/2506.03197)
*Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Yanjie Liang, Zuming Huang, Haozhe Wang, Jun Huang, Ling Chen, Wei Chu, Yuan Qi*

Main category: cs.CV

TL;DR: layoutRL is an end-to-end reinforcement learning framework for parsing scanned documents, achieving state-of-the-art performance with its Infinity-Parser model.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-stage pipelines for document parsing suffer from error propagation and limited adaptability to diverse layouts.

Method: Uses layoutRL, a reinforcement learning framework optimizing composite rewards (edit distance, paragraph count, reading order), and a dataset (Infinity-Doc-55K) with synthetic and real-world documents.

Result: Infinity-Parser outperforms specialist pipelines and general-purpose models in OCR, table/formula extraction, and reading order detection.

Conclusion: The framework and dataset will be publicly released to advance robust document understanding.

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [206] [FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment](https://arxiv.org/pdf/2506.03198)
*Hao Yin, Lijun Gu, Paritosh Parmar, Lin Xu, Tianxiao Guo, Weiwei Fu, Yang Zhang, Tianyou Zheng*

Main category: cs.CV

TL;DR: The paper introduces FLEX, a multi-modal, large-scale dataset for Action Quality Assessment (AQA) in fitness, addressing gaps in current AQA methods by incorporating sEMG signals, multi-view data, and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Current AQA methods are limited to single-view competitive sports and lack fitness-specific assessment. The need for better fitness training outcomes drives the creation of FLEX.

Method: FLEX collects multi-modal data (RGB video, 3D pose, sEMG, physiological info) from 38 subjects performing 20 weight-loaded actions across 3 skill levels. It uses knowledge graphs for annotation.

Result: Baseline experiments show multimodal, multiview data and fine-grained annotations improve model performance.

Conclusion: FLEX advances AQA towards multi-modal, multi-action scenarios and supports AI integration in fitness.

Abstract: With the increasing awareness of health and the growing desire for aesthetic
physique, fitness has become a prevailing trend. However, the potential risks
associated with fitness training, especially with weight-loaded fitness
actions, cannot be overlooked. Action Quality Assessment (AQA), a technology
that quantifies the quality of human action and provides feedback, holds the
potential to assist fitness enthusiasts of varying skill levels in achieving
better training outcomes. Nevertheless, current AQA methodologies and datasets
are limited to single-view competitive sports scenarios and RGB modality and
lack professional assessment and guidance of fitness actions. To address this
gap, we propose the FLEX dataset, the first multi-modal, multi-action,
large-scale dataset that incorporates surface electromyography (sEMG) signals
into AQA. FLEX utilizes high-precision MoCap to collect 20 different
weight-loaded actions performed by 38 subjects across 3 different skill levels
for 10 repetitions each, containing 5 different views of the RGB video, 3D
pose, sEMG, and physiological information. Additionally, FLEX incorporates
knowledge graphs into AQA, constructing annotation rules in the form of penalty
functions that map weight-loaded actions, action keysteps, error types, and
feedback. We conducted various baseline methodologies on FLEX, demonstrating
that multimodal data, multiview data, and fine-grained annotations
significantly enhance model performance. FLEX not only advances AQA
methodologies and datasets towards multi-modal and multi-action scenarios but
also fosters the integration of artificial intelligence within the fitness
domain. Dataset and code are available at
https://haoyin116.github.io/FLEX_Dataset.

</details>


### [207] [Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission](https://arxiv.org/pdf/2506.03211)
*Wanting Yang, Zehui Xiong, Qianqian Yang, Ping Zhang, Merouane Debbah, Rahim Tafazolli*

Main category: cs.CV

TL;DR: GenSeC-PC is a novel semantic communication method for point cloud transmission, leveraging cross-modal fusion (images and point clouds) and generative priors for efficient, robust, and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Efficient point cloud transmission is crucial for autonomous driving and extended reality, but existing methods lack robustness and compression efficiency.

Method: Uses a cross-modal semantic encoder (fusing images and point clouds) and a decoder based on PointDif, with channel-adaptive joint coding and rectified denoising diffusion for speed.

Result: Superior reconstruction and compression efficiency, robust performance under low SNR, bandwidth limits, and varying conditions.

Conclusion: GenSeC-PC outperforms existing methods by enabling analog transmission and eliminating error-free side information needs.

Abstract: With the rapid development of autonomous driving and extended reality,
efficient transmission of point clouds (PCs) has become increasingly important.
In this context, we propose a novel channel-adaptive cross-modal generative
semantic communication (SemCom) for PC transmission, called GenSeC-PC.
GenSeC-PC employs a semantic encoder that fuses images and point clouds, where
images serve as non-transmitted side information. Meanwhile, the decoder is
built upon the backbone of PointDif. Such a cross-modal design not only ensures
high compression efficiency but also delivers superior reconstruction
performance compared to PointDif. Moreover, to ensure robust transmission and
reduce system complexity, we design a streamlined and asymmetric
channel-adaptive joint semantic-channel coding architecture, where only the
encoder needs the feedback of average signal-to-noise ratio (SNR) and available
bandwidth. In addition, rectified denoising diffusion implicit models is
employed to accelerate the decoding process to the millisecond level, enabling
real-time PC communication. Unlike existing methods, GenSeC-PC leverages
generative priors to ensure reliable reconstruction even from noisy or
incomplete source PCs. More importantly, it supports fully analog transmission,
improving compression efficiency by eliminating the need for error-free side
information transmission common in prior SemCom approaches. Simulation results
confirm the effectiveness of cross-modal semantic extraction and dual-metric
guided fine-tuning, highlighting the framework's robustness across diverse
conditions, including low SNR, bandwidth limitations, varying numbers of 2D
images, and previously unseen objects.

</details>


### [208] [ConMamba: Contrastive Vision Mamba for Plant Disease Detection](https://arxiv.org/pdf/2506.03213)
*Abdullah Al Mamun, Miaohua Zhang, David Ahmedt-Aristizabal, Zeeshan Hayder, Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: ConMamba, a self-supervised learning framework using Vision Mamba Encoder and dynamic contrastive loss, outperforms existing methods for Plant Disease Detection.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for PDD require costly annotated datasets, and SSL approaches face computational inefficiency and poor feature alignment.

Method: Proposes ConMamba with Vision Mamba Encoder (bidirectional SSM) and dual-level contrastive loss for dynamic feature alignment.

Result: ConMamba outperforms state-of-the-art methods on three benchmark datasets.

Conclusion: ConMamba offers an efficient, robust solution for PDD by addressing computational and feature alignment challenges.

Abstract: Plant Disease Detection (PDD) is a key aspect of precision agriculture.
However, existing deep learning methods often rely on extensively annotated
datasets, which are time-consuming and costly to generate. Self-supervised
Learning (SSL) offers a promising alternative by exploiting the abundance of
unlabeled data. However, most existing SSL approaches suffer from high
computational costs due to convolutional neural networks or transformer-based
architectures. Additionally, they struggle to capture long-range dependencies
in visual representation and rely on static loss functions that fail to align
local and global features effectively. To address these challenges, we propose
ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates
the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model
(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce
a dual-level contrastive loss with dynamic weight adjustment to optimize
local-global feature alignment. Experimental results on three benchmark
datasets demonstrate that ConMamba significantly outperforms state-of-the-art
methods across multiple evaluation metrics. This provides an efficient and
robust solution for PDD.

</details>


### [209] [OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data](https://arxiv.org/pdf/2506.03224)
*Jinwei Zeng, Yu Liu, Guozhen Zhang, Jingtao Ding, Yuming Lin, Jian Yuan, Yong Li*

Main category: cs.CV

TL;DR: OpenCarbon uses satellite images and POI data to predict urban carbon emissions, overcoming challenges with cross-modality fusion and spatial correlations, achieving a 26.6% performance gain.


<details>
  <summary>Details</summary>
Motivation: Accurate high-resolution carbon emission estimation is vital for governance and mitigation, but traditional methods are data-intensive. Open data and learning techniques offer a solution.

Method: Incorporates satellite images (macroscopic, static) and POI data (fine-grained, dynamic) in OpenCarbon, featuring cross-modality fusion and neighborhood-informed aggregation to address intertwined functionalities and spatial correlations.

Result: Achieves a 26.6% performance gain on R2, with generalizability tests and case studies validating its effectiveness.

Conclusion: OpenCarbon effectively captures urban functionalities' relation to emissions, aiding efficient carbon governance and mitigation planning.

Abstract: Accurately estimating high-resolution carbon emissions is crucial for
effective emission governance and mitigation planning. While conventional
methods for precise carbon accounting are hindered by substantial data
collection efforts, the rise of open data and advanced learning techniques
offers a promising solution. Once an open data-based prediction model is
developed and trained, it can easily infer emissions for new areas based on
available open data. To address this, we incorporate two modalities of open
data, satellite images and point-of-interest (POI) data, to predict
high-resolution urban carbon emissions, with satellite images providing
macroscopic and static and POI data offering fine-grained and relatively
dynamic functionality information. However, estimating high-resolution carbon
emissions presents two significant challenges: the intertwined and implicit
effects of various functionalities on carbon emissions, and the complex spatial
contiguity correlations that give rise to the agglomeration effect. Our model,
OpenCarbon, features two major designs that target the challenges: a
cross-modality information extraction and fusion module to extract
complementary functionality information from two modules and model their
interactions, and a neighborhood-informed aggregation module to capture the
spatial contiguity correlations. Extensive experiments demonstrate our model's
superiority, with a significant performance gain of 26.6\% on R2. Further
generalizability tests and case studies also show OpenCarbon's capacity to
capture the intrinsic relation between urban functionalities and carbon
emissions, validating its potential to empower efficient carbon governance and
targeted carbon mitigation planning. Codes and data are available:
https://github.com/JinweiZzz/OpenCarbon.

</details>


### [210] [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/pdf/2506.03229)
*Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper proposes a collaborative consistency regularization (Co-Reg) method for learning from noisy partial labels annotated by pre-trained vision-language models (VLMs), addressing instance-dependent noise and improving performance with few-shot manual annotations.


<details>
  <summary>Details</summary>
Motivation: To replace manual annotation workflows with pre-trained VLMs and address the challenge of instance-dependent noise in noisy partial label learning (NPLL).

Method: A Co-Reg method involving two neural networks for collaborative label purification via 'Co-Pseudo-Labeling' and consistency regularization in label and feature spaces.

Result: Outperforms other denoising and disambiguation algorithms, demonstrating effectiveness and potential for weakly-supervised learning with VLMs.

Conclusion: The method shows promise for integrating weakly-supervised learning into pre-trained model knowledge distillation, validated by extensive experiments.

Abstract: In the context of noisy partial label learning (NPLL), each training sample
is associated with a set of candidate labels annotated by multiple noisy
annotators. With the emergence of high-performance pre-trained vision-language
models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these
models to replace time-consuming manual annotation workflows and achieve
"manual-annotation-free" training for downstream tasks has become a highly
promising research avenue. This paper focuses on learning from noisy partial
labels annotated by pre-trained VLMs and proposes an innovative collaborative
consistency regularization (Co-Reg) method. Unlike the symmetric noise
primarily addressed in traditional noisy label learning, the noise generated by
pre-trained models is instance-dependent, embodying the underlying patterns of
the pre-trained models themselves, which significantly increases the learning
difficulty for the model. To address this, we simultaneously train two neural
networks that implement collaborative purification of training labels through a
"Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization
constraints in both the label space and feature representation space. Our
method can also leverage few-shot manually annotated valid labels to further
enhance its performances. Comparative experiments with different denoising and
disambiguation algorithms, annotation manners, and pre-trained model
application schemes fully validate the effectiveness of the proposed method,
while revealing the broad prospects of integrating weakly-supervised learning
techniques into the knowledge distillation process of pre-trained models.

</details>


### [211] [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/pdf/2506.03275)
*Austin Silveria, Soham V. Govande, Daniel Y. Fu*

Main category: cs.CV

TL;DR: Chipmunk speeds up DiT inference by reducing redundancy via dynamic sparsity, achieving significant speedups without quality loss.


<details>
  <summary>Details</summary>
Motivation: DiTs incur high compute costs due to redundant activations across steps, suggesting potential for optimization.

Method: Chipmunk uses dynamic sparsity to recompute only fast-changing activations, with voxel-based reordering and column-sparse kernels for efficiency.

Result: Achieves up to 2.16x speedup on HunyuanVideo and 1.41x on FLUX.1-dev, with further gains when stacked with caching.

Conclusion: Chipmunk effectively reduces DiT inference costs without compromising generation quality.

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
sparsity at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic sparsity introduces two systems
challenges: (1) sparse attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic sparsity patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise sparsity. We implement column-sparse kernels utilizing efficient
sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of sparsity patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.

</details>


### [212] [Learning Optical Flow Field via Neural Ordinary Differential Equation](https://arxiv.org/pdf/2506.03290)
*Leyla Mirvakhabova, Hong Cai, Jisoo Jeong, Hanno Ackermann, Farhad Zanjani, Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces a neural ODE-based approach for optical flow estimation, dynamically adjusting refinement steps for better performance.


<details>
  <summary>Details</summary>
Motivation: Fixed refinement steps in existing neural networks for optical flow estimation may lead to suboptimal performance, as they don't adapt to input data.

Method: Uses neural ODEs to predict flow derivatives, enabling dynamic adjustment of refinement steps. Replicates updates of recurrent cells with greater generality.

Result: Achieves significant improvement over baselines and existing models, often requiring only one refinement step.

Conclusion: The neural ODE approach offers a flexible and efficient alternative to fixed-step refinement in optical flow estimation.

Abstract: Recent works on optical flow estimation use neural networks to predict the
flow field that maps positions of one image to positions of the other. These
networks consist of a feature extractor, a correlation volume, and finally
several refinement steps. These refinement steps mimic the iterative
refinements performed by classical optimization algorithms and are usually
implemented by neural layers (e.g., GRU) which are recurrently executed for a
fixed and pre-determined number of steps. However, relying on a fixed number of
steps may result in suboptimal performance because it is not tailored to the
input data. In this paper, we introduce a novel approach for predicting the
derivative of the flow using a continuous model, namely neural ordinary
differential equations (ODE). One key advantage of this approach is its
capacity to model an equilibrium process, dynamically adjusting the number of
compute steps based on the data at hand. By following a particular neural
architecture, ODE solver, and associated hyperparameters, our proposed model
can replicate the exact same updates as recurrent cells used in existing works,
offering greater generality. Through extensive experimental analysis on optical
flow benchmarks, we demonstrate that our approach achieves an impressive
improvement over baseline and existing models, all while requiring only a
single refinement step.

</details>


### [213] [SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports](https://arxiv.org/pdf/2506.03335)
*Dheeraj Khanna, Jerrin Bright, Yuhao Chen, John S. Zelek*

Main category: cs.CV

TL;DR: SportMamba is a hybrid MOT technique for team sports, addressing challenges like non-linear motion and occlusions with a mamba-attention mechanism and adaptive spatial association.


<details>
  <summary>Details</summary>
Motivation: Current MOT methods struggle in team sports due to non-linear motion and occlusions, leading to poor tracking performance.

Method: Introduces a mamba-attention mechanism for non-linear motion modeling and a height-adaptive spatial association metric to reduce ID switches. Also uses adaptive buffers for fast-motion scenarios.

Result: Achieves state-of-the-art performance on SportsMOT and generalizes well to VIP-HTD (ice hockey).

Conclusion: SportMamba effectively addresses MOT challenges in dynamic team sports, outperforming existing methods.

Abstract: Multi-object tracking (MOT) in team sports is particularly challenging due to
the fast-paced motion and frequent occlusions resulting in motion blur and
identity switches, respectively. Predicting player positions in such scenarios
is particularly difficult due to the observed highly non-linear motion
patterns. Current methods are heavily reliant on object detection and
appearance-based tracking, which struggle to perform in complex team sports
scenarios, where appearance cues are ambiguous and motion patterns do not
necessarily follow a linear pattern. To address these challenges, we introduce
SportMamba, an adaptive hybrid MOT technique specifically designed for tracking
in dynamic team sports. The technical contribution of SportMamba is twofold.
First, we introduce a mamba-attention mechanism that models non-linear motion
by implicitly focusing on relevant embedding dependencies. Second, we propose a
height-adaptive spatial association metric to reduce ID switches caused by
partial occlusions by accounting for scale variations due to depth changes.
Additionally, we extend the detection search space with adaptive buffers to
improve associations in fast-motion scenarios. Our proposed technique,
SportMamba, demonstrates state-of-the-art performance on various metrics in the
SportsMOT dataset, which is characterized by complex motion and severe
occlusion. Furthermore, we demonstrate its generalization capability through
zero-shot transfer to VIP-HTD, an ice hockey dataset.

</details>


### [214] [Seeing the Arrow of Time in Large Multimodal Models](https://arxiv.org/pdf/2506.03340)
*Zihui Xue, Mi Luo, Kristen Grauman*

Main category: cs.CV

TL;DR: ArrowRL, an RL-based training strategy with reverse reward, improves temporal perception in LMMs, validated by AoTBench and standard VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LMMs' inability to perceive temporal directionality in videos, hindering deeper temporal understanding.

Method: Introduces ArrowRL, a reinforcement learning strategy with a reverse reward to instill AoT awareness by contrasting forward and reversed video interpretations.

Result: Substantial improvements on AoTBench and standard VQA benchmarks, with peak accuracy gains of over 20% and 10% respectively.

Conclusion: ArrowRL effectively enhances AoT understanding in LMMs, demonstrating the need for dedicated temporal perception.

Abstract: The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is
fundamental to video comprehension, yet remains a significant challenge for
modern large multimodal models (LMMs). Current LMMs struggle to perceive and
utilize temporal directionality in video when responding to language queries,
obstructing deeper temporal understanding. We tackle this deficiency by first
providing a critical analysis of existing benchmarks and models. We then
introduce ArrowRL, a reinforcement learning (RL)-based training strategy with
an innovative reverse reward that instills AoT awareness by encouraging
divergent video interpretations between forward and reversed visual frames. For
rigorous evaluation, we additionally develop AoTBench, a new multi-faceted
benchmark probing temporally challenging questions. Experiments show ArrowRL
greatly advances temporal perception: it not only achieves substantial
improvements on our challenging AoTBench but also demonstrably boosts
performance on standard video question answering (VQA) benchmarks (with peak
accuracy gains reaching over 20% and 10% respectively). This validates
ArrowRL's effectiveness and highlights the critical need for dedicated AoT
understanding in LMMs.

</details>


### [215] [Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers](https://arxiv.org/pdf/2506.03345)
*Chien-Fu, Huang, Katherine Sieg, Leonid Karlinksy, Nash Flores, Rebekah Sheraw, Xin Zhang*

Main category: cs.CV

TL;DR: The paper proposes using vision transformer (ViT) networks for automatic defect classification in semiconductor wafer images, achieving over 90% accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Manual defect classification is slow, labor-intensive, and prone to bias, necessitating automated solutions.

Method: Applied ViT networks, transfer learning (DinoV2), and semi-supervised learning to classify 11 defect types from SEM images.

Result: Achieved over 90% classification accuracy with fewer than 15 images per defect class.

Conclusion: The framework offers a fast, flexible, and platform-agnostic solution for in-house defect classification.

Abstract: Controlling defects in semiconductor processes is important for maintaining
yield, improving production cost, and preventing time-dependent critical
component failures. Electron beam-based imaging has been used as a tool to
survey wafers in the line and inspect for defects. However, manual
classification of images for these nano-scale defects is limited by time, labor
constraints, and human biases. In recent years, deep learning computer vision
algorithms have shown to be effective solutions for image-based inspection
applications in industry. This work proposes application of vision transformer
(ViT) neural networks for automatic defect classification (ADC) of scanning
electron microscope (SEM) images of wafer defects. We evaluated our proposed
methods on 300mm wafer semiconductor defect data from our fab in IBM Albany. We
studied 11 defect types from over 7400 total images and investigated the
potential of transfer learning of DinoV2 and semi-supervised learning for
improved classification accuracy and efficient computation. We were able to
achieve classification accuracies of over 90% with less than 15 images per
defect class. Our work demonstrates the potential to apply the proposed
framework for a platform agnostic in-house classification tool with faster
turnaround time and flexibility.

</details>


### [216] [YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency](https://arxiv.org/pdf/2506.03645)
*Hansen Feng, Lizhi Wang, Yiqi Huang, Tong Li, Lin Zhu, Hua Huang*

Main category: cs.CV

TL;DR: YOND is a novel blind raw image denoising method that generalizes to unknown cameras using synthetic data, featuring CNE, EM-VST, and SNR-Net modules.


<details>
  <summary>Details</summary>
Motivation: Addressing the camera-specific data dependency issue in existing learning-based denoising methods.

Method: Proposes three modules: CNE for noise estimation, EM-VST for eliminating camera dependency, and SNR-Net for controllable denoising.

Result: Demonstrates superior practicality on unknown cameras and challenging cases.

Conclusion: YOND offers a robust, flexible solution for blind raw image denoising, with code publicly available.

Abstract: The rapid advancement of photography has created a growing demand for a
practical blind raw image denoising method. Recently, learning-based methods
have become mainstream due to their excellent performance. However, most
existing learning-based methods suffer from camera-specific data dependency,
resulting in performance drops when applied to data from unknown cameras. To
address this challenge, we introduce a novel blind raw image denoising method
named YOND, which represents You Only Need a Denoiser. Trained solely on
synthetic data, YOND can generalize robustly to noisy raw images captured by
diverse unknown cameras. Specifically, we propose three key modules to
guarantee the practicality of YOND: coarse-to-fine noise estimation (CNE),
expectation-matched variance-stabilizing transform (EM-VST), and SNR-guided
denoiser (SNR-Net). Firstly, we propose CNE to identify the camera noise
characteristic, refining the estimated noise parameters based on the coarse
denoised image. Secondly, we propose EM-VST to eliminate camera-specific data
dependency, correcting the bias expectation of VST according to the noisy
image. Finally, we propose SNR-Net to offer controllable raw image denoising,
supporting adaptive adjustments and manual fine-tuning. Extensive experiments
on unknown cameras, along with flexible solutions for challenging cases,
demonstrate the superior practicality of our method. The source code will be
publicly available at the
\href{https://fenghansen.github.io/publication/YOND}{project homepage}.

</details>


### [217] [Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views](https://arxiv.org/pdf/2506.03371)
*Xiaonan Wang, Bo Shao, Hansaem Kim*

Main category: cs.CV

TL;DR: The paper introduces KoreaGEO Bench, a fine-grained, multimodal geolocation benchmark for Korean street views, addressing gaps in current benchmarks by providing detailed evaluations of vision-language models (VLMs) under varying input modalities.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for image-based geolocation are coarse-grained, linguistically biased, and lack multimodal and privacy-aware evaluations, raising concerns about location privacy risks in social media posts.

Method: The authors create KoreaGEO Bench, a dataset of 1,080 high-resolution Korean street view images with multi-contextual annotations and captions. They evaluate ten VLMs using a three-path protocol to analyze accuracy, spatial bias, and reasoning behavior.

Result: Results show modality-driven shifts in localization precision and structural prediction biases toward core cities.

Conclusion: KoreaGEO Bench provides a comprehensive evaluation framework for VLMs in geolocation tasks, highlighting biases and precision variations based on input modalities.

Abstract: Recent advances in vision-language models (VLMs) have enabled accurate
image-based geolocation, raising serious concerns about location privacy risks
in everyday social media posts. However, current benchmarks remain
coarse-grained, linguistically biased, and lack multimodal and privacy-aware
evaluations. To address these gaps, we present KoreaGEO Bench, the first
fine-grained, multimodal geolocation benchmark for Korean street views. Our
dataset comprises 1,080 high-resolution images sampled across four urban
clusters and nine place types, enriched with multi-contextual annotations and
two styles of Korean captions simulating real-world privacy exposure. We
introduce a three-path evaluation protocol to assess ten mainstream VLMs under
varying input modalities and analyze their accuracy, spatial bias, and
reasoning behavior. Results reveal modality-driven shifts in localization
precision and highlight structural prediction biases toward core cities.

</details>


### [218] [A Foundation Model for Spatial Proteomics](https://arxiv.org/pdf/2506.03373)
*Muhammad Shaban, Yuzhou Chang, Huaying Qiu, Yao Yu Yeo, Andrew H. Song, Guillaume Jaume, Yuchen Wang, Luca L. Weishaupt, Tong Ding, Anurag Vaidya, Abdallah Lamane, Daniel Shao, Mohammed Zidane, Yunhao Bai, Paige McCallum, Shuli Luo, Wenrui Wu, Yang Wang, Precious Cramer, Chi Ngai Chan, Pierre Stephan, Johanna Schaffenrath, Jia Le Lee, Hendrik A. Michel, Caiwei Tian, Cristina Almagro-Perez, Sophia J. Wagner, Sharifa Sahai, Ming Y. Lu, Richard J. Chen, Andrew Zhang, Mark Edward M. Gonzales, Ahmad Makky, Jia-Ying Joey Lee, Hao Cheng, Nourhan El Ahmar, Sayed Matar, Maximilian Haist, Darci Phillips, Yuqi Tan, Garry P. Nolan, W. Richard Burack, Jacob D. Estes, Jonathan T. C. Liu, Toni K Choueiri, Neeraj Agarwal, Marc Barry, Scott J. Rodig, Long Phi Le, Georg Gerber, Christian M. Schürch, Fabian J. Theis, Youn H Kim, Joe Yeong, Sabina Signoretti, Brooke E. Howitt, Lit-Hsin Loo, Qin Ma, Sizun Jiang, Faisal Mahmood*

Main category: cs.CV

TL;DR: KRONOS is a foundation model for spatial proteomics, trained self-supervised on 47M image patches. It excels in tasks like cell phenotyping and patient stratification, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the limited impact of foundation models in spatial proteomics, KRONOS was developed to handle high-dimensional, multi-channel imaging data.

Method: KRONOS uses self-supervised training on diverse protein markers, tissues, and imaging platforms, with architectural adaptations for multiplex imaging.

Result: KRONOS achieves top performance in 11 cohorts for tasks like phenotyping and treatment prediction, and enables segmentation-free analysis.

Conclusion: KRONOS is a scalable, data-efficient tool for spatial proteomics, publicly available for cross-institutional use.

Abstract: Foundation models have begun to transform image analysis by acting as
pretrained generalist backbones that can be adapted to many tasks even when
post-training data are limited, yet their impact on spatial proteomics, imaging
that maps proteins at single-cell resolution, remains limited. Here, we
introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was
trained in a self-supervised manner on over 47 million image patches covering
175 protein markers, 16 tissue types, and 8 fluorescence-based imaging
platforms. We introduce key architectural adaptations to address the
high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.
We demonstrate that KRONOS learns biologically meaningful representations
across multiple scales, ranging from cellular and microenvironment to tissue
levels, enabling it to address diverse downstream tasks, including cell
phenotyping, region classification, and patient stratification. Evaluated
across 11 independent cohorts, KRONOS achieves state-of-the-art performance
across cell phenotyping, treatment response prediction, and retrieval tasks,
and is highly data-efficient. KRONOS also introduces the paradigm of
segmentation-free patch-level processing for efficient and scalable spatial
proteomics analysis, allowing cross-institutional comparisons, and as an image
reverse search engine for spatial patterns. Together, these results position
KRONOS as a flexible and scalable tool for spatial proteomics. The model is
publicly accessible at https://github.com/mahmoodlab/KRONOS.

</details>


### [219] [Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery](https://arxiv.org/pdf/2506.03388)
*Pengyu Chen, Xiao Huang, Teng Fei, Sicheng Wang*

Main category: cs.CV

TL;DR: The study explores how urban sounds align with visual scenes using multimodal methods, finding street view embeddings align better with sounds, while remote sensing segmentation better interprets ecological categories.


<details>
  <summary>Details</summary>
Motivation: To tap into the untapped potential of environmental soundscapes in large-scale geographic analysis by investigating their correspondence with visual scenes.

Method: A multimodal approach integrating geo-referenced sound recordings with street-level and remote sensing imagery, using models like AST, CLIP, RemoteCLIP, CLIPSeg, and Seg-Earth OV for embeddings and segmentation.

Result: Street view embeddings align better with sounds, while remote sensing segmentation is more effective for ecological categories under the BGA framework.

Conclusion: Embedding-based models offer superior semantic alignment, while segmentation-based methods provide interpretable links between visual structure and acoustic ecology, advancing multimodal urban sensing.

Abstract: Environmental soundscapes convey substantial ecological and social
information regarding urban environments; however, their potential remains
largely untapped in large-scale geographic analysis. In this study, we
investigate the extent to which urban sounds correspond with visual scenes by
comparing various visual representation strategies in capturing acoustic
semantics. We employ a multimodal approach that integrates geo-referenced sound
recordings with both street-level and remote sensing imagery across three major
global cities: London, New York, and Tokyo. Utilizing the AST model for audio,
along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV
for semantic segmentation, we extract embeddings and class-level features to
evaluate cross-modal similarity. The results indicate that street view
embeddings demonstrate stronger alignment with environmental sounds compared to
segmentation outputs, whereas remote sensing segmentation is more effective in
interpreting ecological categories through a Biophony--Geophony--Anthrophony
(BGA) framework. These findings imply that embedding-based models offer
superior semantic alignment, while segmentation-based methods provide
interpretable links between visual structure and acoustic ecology. This work
advances the burgeoning field of multimodal urban sensing by offering novel
perspectives for incorporating sound into geospatial analysis.

</details>


### [220] [Temporal Vegetation Index-Based Unsupervised Crop Stress Detection via Eigenvector-Guided Contrastive Learning](https://arxiv.org/pdf/2506.03394)
*Shafqaat Ahmad*

Main category: cs.CV

TL;DR: EigenCL is an unsupervised contrastive learning framework for early crop stress detection using NDRE dynamics, outperforming traditional methods with 76% early detection and high downstream accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of crop stress is crucial for yield preservation, but traditional methods like NDRE are limited by late detection and reliance on labeled data.

Method: EigenCL uses temporal NDRE dynamics and eigen decomposition to define stress-aware similarity for contrastive learning, avoiding visual augmentations.

Result: Achieved 76% early stress detection (12 days ahead), superior clustering (Silhouette: 0.748), and high classification accuracy (95% k-NN, 91% logistic regression).

Conclusion: EigenCL provides a scalable, label-free solution for early stress detection, validated for real-world agricultural use.

Abstract: Early detection of crop stress is vital for minimizing yield loss and
enabling timely intervention in precision agriculture. Traditional approaches
using NDRE often detect stress only after visible symptoms appear or require
labeled datasets, limiting scalability. This study introduces EigenCL, a novel
unsupervised contrastive learning framework guided by temporal NDRE dynamics
and biologically grounded eigen decomposition. Using over 10,000 Sentinel-2
NDRE image patches from drought-affected Iowa cornfields, we constructed
five-point NDRE time series per patch and derived an RBF similarity matrix. The
principal eigenvector explaining 76% of the variance and strongly correlated (r
= 0.95) with raw NDRE values was used to define stress-aware similarity for
contrastive embedding learning. Unlike existing methods that rely on visual
augmentations, EigenCL pulls embeddings together based on biologically similar
stress trajectories and pushes apart divergent ones. The learned embeddings
formed physiologically meaningful clusters, achieving superior clustering
metrics (Silhouette: 0.748, DBI: 0.35) and enabling 76% early stress detection
up to 12 days before conventional NDRE thresholds. Downstream classification
yielded 95% k-NN and 91% logistic regression accuracy. Validation on an
independent 2023 Nebraska dataset confirmed generalizability without
retraining. EigenCL offers a label-free, scalable approach for early stress
detection that aligns with underlying plant physiology and is suitable for
real-world deployment in data-scarce agricultural environments.

</details>


### [221] [ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads](https://arxiv.org/pdf/2506.03433)
*Yifan Li, Xin Li, Tianqin Li, Wenbin He, Yu Kong, Liu Ren*

Main category: cs.CV

TL;DR: ViT-Split improves VFM efficiency by splitting layers into extractor and adapter components, reducing training time and maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in VFM adapters, such as early gradient backpropagation and excessive tuning, while better utilizing prior knowledge.

Method: Proposes ViT-Split, which divides VFM layers into extractor and adapter, introduces task and prior heads, and freezes the VFM backbone.

Result: ViT-Split reduces training time by up to 4x and achieves comparable or better performance on tasks like segmentation and detection.

Conclusion: ViT-Split is an efficient and effective alternative to existing VFM adapters, leveraging prior knowledge while minimizing training overhead.

Abstract: Vision foundation models (VFMs) have demonstrated remarkable performance
across a wide range of downstream tasks. While several VFM adapters have shown
promising results by leveraging the prior knowledge of VFMs, we identify two
inefficiencies in these approaches. First, the interaction between
convolutional neural network (CNN) and VFM backbone triggers early layer
gradient backpropagation. Second, existing methods require tuning all
components, adding complexity. Besides, these adapters alter VFM features,
underutilizing the prior knowledge. To tackle these challenges, we propose a
new approach called ViT-Split, based on a key observation: the layers of
several VFMs, like DINOv2, can be divided into two distinct components: an
extractor for learning low-level features and an adapter for learning
task-specific features. Leveraging this insight, we eliminate the CNN branch
and introduce two heads, task head and prior head, to the frozen VFM. The task
head is designed to learn task-specific features, mitigating the early gradient
propagation issue. The prior head is used to leverage the multi-scale prior
features from the frozen VFM, reducing tuning parameters and overfitting.
Extensive experiments on various tasks (e.g., segmentation, detection, depth
estimation, and visual question answering) validate the effectiveness and
efficiency of ViT-Split. Specifically, ViT-Split reduces training time up to
$4\times$ while achieving comparable or even better results on ADE20K, compared
to other VFM adapters.

</details>


### [222] [Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos](https://arxiv.org/pdf/2506.03440)
*Tanqiu Qiao, Ruochen Li, Frederick W. B. Li, Yoshiki Kubotani, Shigeo Morishima, Hubert P. H. Shum*

Main category: cs.CV

TL;DR: The paper proposes GeoVis-GNN, a method for Human-Object Interaction (HOI) recognition by fusing visual and geometric features using dual-attention and graph learning, and introduces the MPHOI-120 dataset for real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Understanding HOI in videos requires combining visual and geometric features without losing their unique strengths, which is challenging.

Method: Proposes GeoVis-GNN, a bottom-up approach with dual-attention feature fusion and interdependent entity graph learning.

Result: Achieves state-of-the-art performance in various HOI scenarios, including complex concurrent interactions.

Conclusion: GeoVis-GNN effectively fuses multimodal features and advances HOI recognition, supported by the new MPHOI-120 dataset.

Abstract: Human-Object Interaction (HOI) recognition in videos requires understanding
both visual patterns and geometric relationships as they evolve over time.
Visual and geometric features offer complementary strengths. Visual features
capture appearance context, while geometric features provide structural
patterns. Effectively fusing these multimodal features without compromising
their unique characteristics remains challenging. We observe that establishing
robust, entity-specific representations before modeling interactions helps
preserve the strengths of each modality. Therefore, we hypothesize that a
bottom-up approach is crucial for effective multimodal fusion. Following this
insight, we propose the Geometric Visual Fusion Graph Neural Network
(GeoVis-GNN), which uses dual-attention feature fusion combined with
interdependent entity graph learning. It progressively builds from
entity-specific representations toward high-level interaction understanding. To
advance HOI recognition to real-world scenarios, we introduce the Concurrent
Partial Interaction Dataset (MPHOI-120). It captures dynamic multi-person
interactions involving concurrent actions and partial engagement. This dataset
helps address challenges like complex human-object dynamics and mutual
occlusions. Extensive experiments demonstrate the effectiveness of our method
across various HOI scenarios. These scenarios include two-person interactions,
single-person activities, bimanual manipulations, and complex concurrent
partial interactions. Our method achieves state-of-the-art performance.

</details>


### [223] [RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions](https://arxiv.org/pdf/2506.03448)
*Bimsara Pathiraja, Maitreya Patel, Shivam Singh, Yezhou Yang, Chitta Baral*

Main category: cs.CV

TL;DR: RefEdit addresses the challenge of editing complex scenes with multiple entities, outperforming baselines with a scalable synthetic data pipeline and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex scenes containing multiple entities, highlighting the need for a robust solution.

Method: RefEdit, trained on 20,000 synthetic editing triplets, leverages a scalable data generation pipeline.

Result: RefEdit outperforms baselines trained on millions of samples and achieves state-of-the-art performance.

Conclusion: RefEdit sets a new benchmark for instruction-based image editing in complex scenes, with released data and checkpoints for reproducibility.

Abstract: Despite recent advances in inversion and instruction-based image editing,
existing approaches primarily excel at editing single, prominent objects but
significantly struggle when applied to complex scenes containing multiple
entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous
real-world benchmark rooted in RefCOCO, where even baselines trained on
millions of samples perform poorly. To overcome this limitation, we introduce
RefEdit -- an instruction-based editing model trained on our scalable synthetic
data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,
outperforms the Flux/SD3 model-based baselines trained on millions of data.
Extensive evaluations across various benchmarks demonstrate that our model not
only excels in referring expression tasks but also enhances performance on
traditional benchmarks, achieving state-of-the-art results comparable to
closed-source methods. We release data \& checkpoint for reproducibility.

</details>


### [224] [The effects of using created synthetic images in computer vision training](https://arxiv.org/pdf/2506.03449)
*John W. Smutny*

Main category: cs.CV

TL;DR: Using synthetic images from Unreal Engine 4 can supplement real datasets for CV models, reducing reliance on real images while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of limited or costly real-world datasets and the risks of poisoned images from the internet.

Method: Testing synthetic images' validity by evaluating model accuracy in binary classification tasks (Cat vs Dog, Weld Defect Detection) and measuring synthetic image quality with pre-trained CV models.

Result: Adding >60% synthetic images narrows the test-training accuracy gap to ~1-2%. Using 10% real images with synthetic data halves classification error rates.

Conclusion: Synthetic images enable researchers to use only 10% of real images, offering a cost-effective and scalable solution for data-scarce projects.

Abstract: This paper investigates how rendering engines, like Unreal Engine 4 (UE), can
be used to create synthetic images to supplement datasets for deep computer
vision (CV) models in image abundant and image limited use cases. Using
rendered synthetic images from UE can provide developers and businesses with a
method of accessing nearly unlimited, reproducible, agile, and cheap training
sets for their customers and applications without the threat of poisoned images
from the internet or the cost of collecting them. The validity of these
generated images are examined by testing the change in model test accuracy in
two different sized CV models across two binary classification cases (Cat vs
Dog and Weld Defect Detection). In addition, this paper provides an
implementation of how to measure the quality of synthetic images by using
pre-trained CV models as auditors. Results imply that for large (VGG16) and
small (MobileNetV3-small) parameter deep CV models, adding >60% additional
synthetic images to a real image dataset during model training can narrow the
test-training accuracy gap to ~1-2% without a conclusive effect on test
accuracy compared to using real world images alone. Likewise, adding <10%
additional real training images to synthetic only training sets decreased the
classification error rate in half, then decreasing further when adding more
real training images. For these cases tested, using synthetic images from
rendering engines allow researchers to only use 10% of their real images during
training, compared to the traditional 50-70%. This research serves as an
example of how to create synthetic images, guidelines on how to use the images,
potential restrictions and possible performance improvements for data-scarce
projects.

</details>


### [225] [CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets with Sparse-Aspect Multi-Baseline Data](https://arxiv.org/pdf/2406.04158)
*Da Li, Guoqiang Zhao, Chen Yao, Kaiqiang Zhu, Houjun Sun, Jiacheng Bao*

Main category: cs.CV

TL;DR: The paper proposes CMAR-Net, a cross-modal 3D-SAR reconstruction network, to enhance sparse-aspect multi-baseline SAR imaging by fusing 2D optical images and SAR data, outperforming CS and DL methods.


<details>
  <summary>Details</summary>
Motivation: Existing DL methods for sparse 3D SAR imaging rely solely on high-resolution radar images, missing complementary data from other sources, which limits performance.

Method: CMAR-Net uses cross-modal supervision from 2D optical images and differentiable rendering to train efficiently and reconstruct accurate 3D images from sparse SAR data.

Result: CMAR-Net improves PSNR by 75.83% and SSIM by 47.85% over state-of-the-art methods, while reducing dataset construction costs.

Conclusion: The work demonstrates the potential of cross-modal learning for SAR 3D imaging and offers a cost-effective framework for radar imaging research.

Abstract: Sparse-aspect multi-baseline Synthetic Aperture Radar (SAR) three-dimensional
(3D) tomography is a crucial remote sensing technique. Compared to full-aspect
observation, it needs only a few observation aspects to achieve a sufficiently
clear 3D scene reconstruction, providing a cost-effective alternative. In the
past, compressive sensing (CS) was the mainstream approach for sparse 3D SAR
imaging. Recently, deep learning (DL) revolutionizes this field through its
powerful data-driven representation capabilities and efficient inference
characteristics. However, existing DL methods primarily depend on
high-resolution radar images for supervising the training of deep neural
networks (DNNs). This unimodal approach precludes the incorporation of
complementary information from other data sources, thereby limiting potential
improvements in imaging performance. In this paper, we propose a Cross-Modal
3D-SAR Reconstruction Network (CMAR-Net) that enhances 3D SAR imaging by fusing
heterogeneous information. Leveraging cross-modal supervision from 2D optical
images and error transfer guaranteed by differentiable rendering, CMAR-Net
achieves efficient training and reconstructs highly sparse-aspect
multi-baseline SAR image into visually structured and accurate 3D images,
particularly for vehicle targets. Extensive experiments on simulated and
real-world datasets demonstrate that CMAR-Net significantly outperforms
state-of-the-art sparse reconstruction algorithms based on CS and DL, with
average improvements of 75.83% in PSNR and 47.85% in SSIM. Furthermore, our
method eliminates the need for time-consuming full-aperture data preprocessing
and relies solely on computer-rendered optical images, significantly reducing
dataset construction costs. This work highlights the potential of cross-modal
learning for multi-baseline SAR 3D imaging and introduces a novel framework for
radar imaging research.

</details>


### [226] [RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels](https://arxiv.org/pdf/2506.03461)
*Nan Xiang, Lifeng Xing, Dequan Jin*

Main category: cs.CV

TL;DR: RoNFA is a robust neural field-based method for few-shot image classification with noisy labels, outperforming state-of-the-art methods even with label errors.


<details>
  <summary>Details</summary>
Motivation: Label errors in few-shot learning reduce accuracy, necessitating robust models.

Method: RoNFA uses two neural fields for feature and category representation, with adaptive receptive fields for accuracy.

Result: RoNFA surpasses state-of-the-art FSL methods on noisy datasets, even outperforming clean-set results.

Conclusion: RoNFA offers strong robustness against label noise and superior few-shot learning performance.

Abstract: In few-shot learning (FSL), the labeled samples are scarce. Thus, label
errors can significantly reduce classification accuracy. Since label errors are
inevitable in realistic learning tasks, improving the robustness of the model
in the presence of label errors is critical. This paper proposes a new robust
neural field-based image approach (RoNFA) for few-shot image classification
with noisy labels. RoNFA consists of two neural fields for feature and category
representation. They correspond to the feature space and category set. Each
neuron in the field for category representation (FCR) has a receptive field
(RF) on the field for feature representation (FFR) centered at the
representative neuron for its category generated by soft clustering. In the
prediction stage, the range of these receptive fields adapts according to the
neuronal activation in FCR to ensure prediction accuracy. These learning
strategies provide the proposed model with excellent few-shot learning
capability and strong robustness against label noises. The experimental results
on real-world FSL datasets with three different types of label noise
demonstrate that the proposed method significantly outperforms state-of-the-art
FSL methods. Its accuracy obtained in the presence of noisy labels even
surpasses the results obtained by state-of-the-art FSL methods trained on clean
support sets, indicating its strong robustness against noisy labels.

</details>


### [227] [MamFusion: Multi-Mamba with Temporal Fusion for Partially Relevant Video Retrieval](https://arxiv.org/pdf/2506.03473)
*Xinru Ying, Jiaqi Mo, Jingyang Lin, Canghong Jin, Fangfang Wang, Lina Wei*

Main category: cs.CV

TL;DR: The paper introduces MamFusion, a multi-Mamba module framework for Partially Relevant Video Retrieval (PRVR), leveraging long-term state space modeling to improve retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of identifying partially relevant untrimmed videos by tackling information redundancy in long-sequence video content.

Method: Proposes MamFusion with Temporal T-to-V and V-to-T Fusion to model temporal relationships between text queries and video moments.

Result: Achieves state-of-the-art performance in retrieval effectiveness on large-scale datasets.

Conclusion: MamFusion effectively enhances PRVR by integrating long-term video content understanding with text-video relevance.

Abstract: Partially Relevant Video Retrieval (PRVR) is a challenging task in the domain
of multimedia retrieval. It is designed to identify and retrieve untrimmed
videos that are partially relevant to the provided query. In this work, we
investigate long-sequence video content understanding to address information
redundancy issues. Leveraging the outstanding long-term state space modeling
capability and linear scalability of the Mamba module, we introduce a
multi-Mamba module with temporal fusion framework (MamFusion) tailored for PRVR
task. This framework effectively captures the state-relatedness in long-term
video content and seamlessly integrates it into text-video relevance
understanding, thereby enhancing the retrieval process. Specifically, we
introduce Temporal T-to-V Fusion and Temporal V-to-T Fusion to explicitly model
temporal relationships between text queries and video moments, improving
contextual awareness and retrieval accuracy. Extensive experiments conducted on
large-scale datasets demonstrate that MamFusion achieves state-of-the-art
performance in retrieval effectiveness. Code is available at the link:
https://github.com/Vision-Multimodal-Lab-HZCU/MamFusion.

</details>


### [228] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/pdf/2506.03525)
*Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal*

Main category: cs.CV

TL;DR: Video-SKoT improves video understanding by using skill-aware CoT supervisions and expert learning, outperforming baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing CoT methods struggle with domain-specific skills in video understanding, necessitating a more adaptive approach.

Method: Video-SKoT constructs skill-based CoT annotations and employs skill-specific expert learning with lightweight adapters.

Result: The framework outperforms baselines on three video understanding benchmarks.

Conclusion: Video-SKoT effectively adapts CoT reasoning to diverse video domains, enhancing performance and skill understanding.

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [229] [Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets](https://arxiv.org/pdf/2505.12532)
*Ahmet Bilican, M. Akın Yılmaz, A. Murat Tekalp, R. Gökberk Cinbiş*

Main category: cs.CV

TL;DR: WaveFT is a novel PEFT method for efficient adaptation of large models, outperforming LoRA in few-parameter regimes by learning sparse updates in the wavelet domain.


<details>
  <summary>Details</summary>
Motivation: The need for efficient adaptation of large foundation models under tight compute and memory constraints, where existing methods like LoRA lack granularity and effectiveness.

Method: WaveFT learns highly sparse updates in the wavelet domain of residual matrices, enabling precise control of trainable parameters and fine-grained capacity adjustment.

Result: WaveFT significantly outperforms LoRA and other PEFT methods in personalized text-to-image generation, achieving better subject fidelity, prompt alignment, and image diversity, especially at low parameter counts.

Conclusion: WaveFT is ideal for extreme parameter-efficient scenarios, offering superior performance with remarkably low parameter counts.

Abstract: Efficiently adapting large foundation models is critical, especially with
tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)
methods such as LoRA offer limited granularity and effectiveness in
few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT
method that learns highly sparse updates in the wavelet domain of residual
matrices. WaveFT allows precise control of trainable parameters, offering
fine-grained capacity adjustment and excelling with remarkably low parameter
count, potentially far fewer than LoRA's minimum, ideal for extreme
parameter-efficient scenarios. Evaluated on personalized text-to-image
generation using Stable Diffusion XL as baseline, WaveFT significantly
outperforms LoRA and other PEFT methods, especially at low parameter counts;
achieving superior subject fidelity, prompt alignment, and image diversity.

</details>


### [230] [Heterogeneous Skeleton-Based Action Representation Learning](https://arxiv.org/pdf/2506.03481)
*Hongsong Wang, Xiaoyan Ma, Jidong Kuang, Jie Gui*

Main category: cs.CV

TL;DR: The paper proposes a framework for heterogeneous skeleton-based action recognition, addressing variability in joint dimensions and topological structures. It includes skeleton processing and unified representation learning, validated on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing models overlook skeleton heterogeneity, limiting their applicability. This work aims to handle diverse skeleton data for broader action recognition tasks.

Method: The framework processes skeletons via 2D-to-3D conversion and skeleton-specific prompts, adds semantic motion encoding, and uses a shared backbone for unified representation.

Result: Experiments on NTU-60, NTU-120, and PKU-MMD II show the method's effectiveness in action understanding tasks.

Conclusion: The approach successfully handles heterogeneous skeletons and is applicable to robots with varied humanoid structures.

Abstract: Skeleton-based human action recognition has received widespread attention in
recent years due to its diverse range of application scenarios. Due to the
different sources of human skeletons, skeleton data naturally exhibit
heterogeneity. The previous works, however, overlook the heterogeneity of human
skeletons and solely construct models tailored for homogeneous skeletons. This
work addresses the challenge of heterogeneous skeleton-based action
representation learning, specifically focusing on processing skeleton data that
varies in joint dimensions and topological structures. The proposed framework
comprises two primary components: heterogeneous skeleton processing and unified
representation learning. The former first converts two-dimensional skeleton
data into three-dimensional skeleton via an auxiliary network, and then
constructs a prompted unified skeleton using skeleton-specific prompts. We also
design an additional modality named semantic motion encoding to harness the
semantic information within skeletons. The latter module learns a unified
action representation using a shared backbone network that processes different
heterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and
PKU-MMD II datasets demonstrate the effectiveness of our method in various
tasks of action understanding. Our approach can be applied to action
recognition in robots with different humanoid structures.

</details>


### [231] [Dirty and Clean-Label attack detection using GAN discriminators](https://arxiv.org/pdf/2506.01224)
*John W. Smutny*

Main category: cs.CV

TL;DR: GAN discriminators can detect mislabeled and poisoned images in a single class, providing a threshold for identifying such threats with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring image-label integrity in deep learning models without impractical manual inspection or time-consuming retraining.

Method: Using GAN discriminators trained on a single class to identify mislabeled and modified images, with threshold calibration for detection.

Result: GAN discriminators achieved 100% poison detection at perturbation epsilon ≥ 0.20 after threshold calibration.

Conclusion: GAN discriminators offer a practical solution for protecting high-value classes in computer vision models against label manipulation.

Abstract: Gathering enough images to train a deep computer vision model is a constant
challenge. Unfortunately, collecting images from unknown sources can leave your
model s behavior at risk of being manipulated by a dirty-label or clean-label
attack unless the images are properly inspected. Manually inspecting each
image-label pair is impractical and common poison-detection methods that
involve re-training your model can be time consuming. This research uses GAN
discriminators to protect a single class against mislabeled and different
levels of modified images. The effect of said perturbation on a basic
convolutional neural network classifier is also included for reference. The
results suggest that after training on a single class, GAN discriminator s
confidence scores can provide a threshold to identify mislabeled images and
identify 100% of the tested poison starting at a perturbation epsilon magnitude
of 0.20, after decision threshold calibration using in-class samples.
Developers can use this report as a basis to train their own discriminators to
protect high valued classes in their CV models.

</details>


### [232] [CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model](https://arxiv.org/pdf/2506.03502)
*Yuxuan Chen, Haipeng Xie*

Main category: cs.CV

TL;DR: CHIME is a framework for time series diffusion models that improves multi-scale feature alignment and generative capabilities through decomposition, adaptive integration, and feature hallucination.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for time series struggle with multi-scale feature alignment and generative generalization across entities and long-time scales.

Method: CHIME uses multi-scale decomposition, adaptive integration, and a feature hallucination module for conditional denoising.

Result: CHIME achieves state-of-the-art performance and strong generalization in few-shot scenarios on real-world datasets.

Conclusion: CHIME effectively addresses challenges in time series diffusion models, enhancing generative capabilities and feature alignment.

Abstract: The denoising diffusion probabilistic model has become a mainstream
generative model, achieving significant success in various computer vision
tasks. Recently, there has been initial exploration of applying diffusion
models to time series tasks. However, existing studies still face challenges in
multi-scale feature alignment and generative capabilities across different
entities and long-time scales. In this paper, we propose CHIME, a conditional
hallucination and integrated multi-scale enhancement framework for time series
diffusion models. By employing multi-scale decomposition and adaptive
integration, CHIME captures the decomposed features of time series, achieving
in-domain distribution alignment between generated and original samples. In
addition, we introduce a feature hallucination module in the conditional
denoising process, enabling the transfer of temporal features through the
training of category-independent transformation layers. Experimental results on
publicly available real-world datasets demonstrate that CHIME achieves
state-of-the-art performance and exhibits excellent generative generalization
capabilities in few-shot scenarios.

</details>


### [233] [DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network](https://arxiv.org/pdf/2506.03571)
*Chong Hyun Lee, Kibae Lee*

Main category: cs.CV

TL;DR: DaigNet introduces a novel object detection method using diagonal constraints on GCN adjacency matrices, outperforming YOLO variants in accuracy.


<details>
  <summary>Details</summary>
Motivation: To simplify object detection by eliminating anchor box design and improving accuracy through graph-based constraints.

Method: Uses GCN with hard/soft diagonal constraints on adjacency matrices and complementary loss functions, integrated into YOLO's detection head.

Result: Achieves 7.5% higher mAP50 on Pascal VOC and outperforms YOLOv3u, YOLOv5u, and YOLOv8 on MS COCO.

Conclusion: DaigNet is a feasible, anchor-free detector with superior performance over YOLO models.

Abstract: We propose DaigNet, a new approach to object detection with which we can
detect an object bounding box using diagonal constraints on adjacency matrix of
a graph convolutional network (GCN). We propose two diagonalization algorithms
based on hard and soft constraints on adjacency matrix and two loss functions
using diagonal constraint and complementary constraint. The DaigNet eliminates
the need for designing a set of anchor boxes commonly used. To prove
feasibility of our novel detector, we adopt detection head in YOLO models.
Experiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than
YOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%
higher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.

</details>


### [234] [EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation](https://arxiv.org/pdf/2506.03512)
*Daikun Liu, Lei Cheng, Teng Wang, changyin Sun*

Main category: cs.CV

TL;DR: EDCFlow is a lightweight event-based optical flow network that combines temporal feature differences and cost volumes for high-quality flow estimation at higher resolutions with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from redundant computations and limited scalability to higher resolutions for flow refinement.

Method: Uses an attention-based multi-scale temporal feature difference layer and adaptively fuses high-resolution difference features with low-resolution correlation features.

Result: Achieves better performance with lower complexity and superior generalization compared to existing methods.

Conclusion: EDCFlow is effective for high-resolution flow estimation and can enhance RAFT-like methods as a plug-and-play module.

Abstract: Recent learning-based methods for event-based optical flow estimation utilize
cost volumes for pixel matching but suffer from redundant computations and
limited scalability to higher resolutions for flow refinement. In this work, we
take advantage of the complementarity between temporally dense feature
differences of adjacent event frames and cost volume and present a lightweight
event-based optical flow network (EDCFlow) to achieve high-quality flow
estimation at a higher resolution. Specifically, an attention-based multi-scale
temporal feature difference layer is developed to capture diverse motion
patterns at high resolution in a computation-efficient manner. An adaptive
fusion of high-resolution difference motion features and low-resolution
correlation motion features is performed to enhance motion representation and
model generalization. Notably, EDCFlow can serve as a plug-and-play refinement
module for RAFT-like event-based methods to enhance flow details. Extensive
experiments demonstrate that EDCFlow achieves better performance with lower
complexity compared to existing methods, offering superior generalization.

</details>


### [235] [ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/pdf/2506.03582)
*Rui Yann, Xianglei Xing*

Main category: cs.CV

TL;DR: ViTSGMM is an efficient semi-supervised image recognition network that improves generalization with limited labeled data using a hierarchical mixture density mechanism and mutual information optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalization on limited labeled data and rely on complex architectures.

Method: Constructs a hierarchical mixture density classification mechanism, optimizing mutual information to retain discriminative features.

Result: Achieves state-of-the-art performance on STL-10 and CIFAR-10/100 with minimal labeled data; identifies and fixes data leakage in STL-10.

Conclusion: ViTSGMM is highly efficient and reliable, addressing key limitations in semi-supervised learning.

Abstract: We present ViTSGMM, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, while their
generalization ability when dealing with extremely limited labeled data remains
to be improved. To address these limitations, we construct a hierarchical
mixture density classification decision mechanism by optimizing mutual
information between feature representations and target classes, compressing
redundant information while retaining crucial discriminative components.
Experimental results demonstrate that our method achieves state-of-the-art
performance on STL-10 and CIFAR-10/100 datasets when using negligible labeled
samples. Notably, this paper also reveals a long-overlooked data leakage issue
in the STL-10 dataset for semi-supervised learning tasks and removes duplicates
to ensure the reliability of experimental results. Code available at
https://github.com/Shu1L0n9/ViTSGMM.

</details>


### [236] [DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models](https://arxiv.org/pdf/2506.03517)
*Ziyi Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ashkan Mirzaei, Igor Gilitschenski, Sergey Tulyakov, Aliaksandr Siarohin*

Main category: cs.CV

TL;DR: DenseDPO improves Direct Preference Optimization (DPO) for text-to-video diffusion models by addressing motion bias and enabling fine-grained comparisons through aligned video pairs and segment-level labeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome biases in DPO training data, where annotators favor low-motion clips due to fewer artifacts, and to enable more precise comparisons.

Method: DenseDPO creates aligned video pairs from corrupted ground truth videos, labels preferences on short segments, and uses Vision Language Models (VLMs) for automatic annotation.

Result: DenseDPO improves motion generation with one-third the labeled data, matches DPO in text alignment and quality, and achieves near-human performance with VLM labels.

Conclusion: DenseDPO effectively addresses DPO's limitations, offering a more efficient and unbiased approach for training text-to-video models.

Abstract: Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.

</details>


### [237] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/pdf/2506.03589)
*Huy Le, Nhat Chung, Tung Kieu, Anh Nguyen, Ngan Le*

Main category: cs.CV

TL;DR: BiMa is a framework to reduce visual-linguistic biases in text-video retrieval by enhancing video embeddings and disentangling text features, showing strong performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: TVR systems often overlook key details due to biases in datasets, limiting model effectiveness.

Method: BiMa generates scene elements for videos, integrates them into embeddings for visual debiasing, and disentangles text features into content and bias components for textual debiasing.

Result: BiMa achieves competitive performance on five TVR benchmarks and excels in out-of-distribution retrieval tasks.

Conclusion: BiMa effectively mitigates biases in TVR systems, improving retrieval accuracy and robustness.

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [238] [Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation](https://arxiv.org/pdf/2506.03521)
*Weinan He, Zilei Wang, Yixin Zhang*

Main category: cs.CV

TL;DR: The paper proposes TASC and UniMS for UniDA, leveraging vision-language models to improve semantic center alignment and open-set detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing domain and category shift challenges in UniDA by improving semantic center alignment and robustness.

Method: Uses TASC for semantic center search in text space and UniMS for open-set detection, combining greedy search and gradient descent.

Result: Achieves state-of-the-art performance across four benchmarks under various category shifts.

Conclusion: The method is effective and robust for UniDA, with potential for broader applications.

Abstract: Universal Domain Adaptation (UniDA) focuses on transferring source domain
knowledge to the target domain under both domain shift and unknown category
shift. Its main challenge lies in identifying common class samples and aligning
them. Current methods typically obtain target domain semantics centers from an
unconstrained continuous image representation space. Due to domain shift and
the unknown number of clusters, these centers often result in complex and less
robust alignment algorithm. In this paper, based on vision-language models, we
search for semantic centers in a semantically meaningful and discrete text
representation space. The constrained space ensures almost no domain bias and
appropriate semantic granularity for these centers, enabling a simple and
robust adaptation algorithm. Specifically, we propose TArget Semantics
Clustering (TASC) via Text Representations, which leverages information
maximization as a unified objective and involves two stages. First, with the
frozen encoders, a greedy search-based framework is used to search for an
optimal set of text embeddings to represent target semantics. Second, with the
search results fixed, encoders are refined based on gradient descent,
simultaneously achieving robust domain alignment and private class clustering.
Additionally, we propose Universal Maximum Similarity (UniMS), a scoring
function tailored for detecting open-set samples in UniDA. Experimentally, we
evaluate the universality of UniDA algorithms under four category shift
scenarios. Extensive experiments on four benchmarks demonstrate the
effectiveness and robustness of our method, which has achieved state-of-the-art
performance.

</details>


### [239] [Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting](https://arxiv.org/pdf/2506.03538)
*Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu*

Main category: cs.CV

TL;DR: Proposes Asymmetric Dual 3DGS, a framework for stable 3D reconstruction from in-the-wild images by leveraging stochastic artifacts and enforcing consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in 3D reconstruction from inconsistent lighting and transient distractors, aiming to reduce visual artifacts and improve stability.

Method: Trains two 3D Gaussian Splatting models with consistency constraints and divergent masking to suppress artifacts. Introduces Dynamic EMA Proxy for efficiency.

Result: Outperforms existing methods on real-world datasets, achieving stable and consistent reconstructions with high efficiency.

Conclusion: The framework effectively mitigates artifacts and improves reconstruction quality, with plans to release codes and models.

Abstract: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.

</details>


### [240] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/pdf/2506.03614)
*Zhanhui Zhou, Lingjie Chen, Chao Yang, Chaochao Lu*

Main category: cs.CV

TL;DR: VLMs can reconstruct harmful content from benign-looking patches due to visual stitching, bypassing data moderation and posing safety risks.


<details>
  <summary>Details</summary>
Motivation: To highlight the vulnerability of VLMs to adversarial data poisoning where harmful content is split into benign patches, evading moderation and later being reconstructed.

Method: Demonstrated visual stitching in VLMs by splitting images into patches with unique IDs, then simulating adversarial poisoning with harmful patches labeled as "safe" or "unsafe".

Result: VLMs can correctly verbalize IDs or harmful descriptions from patched data, showing the risk of visual stitching.

Conclusion: Visual stitching in VLMs enables harmful content reconstruction, revealing a critical safety loophole in data moderation.

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [241] [WIFE-Fusion:Wavelet-aware Intra-inter Frequency Enhancement for Multi-model Image Fusion](https://arxiv.org/pdf/2506.03555)
*Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui*

Main category: cs.CV

TL;DR: WIFE-Fusion is a multimodal image fusion framework that enhances frequency-domain feature exploration and interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack frequency-domain feature exploration and interactive relationships, limiting fusion quality.

Method: Proposes Intra-Frequency Self-Attention (IFSA) and Inter-Frequency Interaction (IFI) for enriched feature extraction and aggregation.

Result: Superior performance on five datasets across three multimodal fusion tasks.

Conclusion: WIFE-Fusion effectively addresses limitations of current methods, offering precise feature extraction and unified modeling.

Abstract: Multimodal image fusion effectively aggregates information from diverse
modalities, with fused images playing a crucial role in vision systems.
However, existing methods often neglect frequency-domain feature exploration
and interactive relationships. In this paper, we propose wavelet-aware
Intra-inter Frequency Enhancement Fusion (WIFE-Fusion), a multimodal image
fusion framework based on frequency-domain components interactions. Its core
innovations include: Intra-Frequency Self-Attention (IFSA) that leverages
inherent cross-modal correlations and complementarity through interactive
self-attention mechanisms to extract enriched frequency-domain features, and
Inter-Frequency Interaction (IFI) that enhances enriched features and filters
latent features via combinatorial interactions between heterogeneous
frequency-domain components across modalities. These processes achieve precise
source feature extraction and unified modeling of feature
extraction-aggregation. Extensive experiments on five datasets across three
multimodal fusion tasks demonstrate WIFE-Fusion's superiority over current
specialized and unified fusion methods. Our code is available at
https://github.com/Lmmh058/WIFE-Fusion.

</details>


### [242] [Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation](https://arxiv.org/pdf/2506.03621)
*Chaehun Shin, Jooyoung Choi, Johan Barthelemy, Jungbeom Lee, Sungroh Yoon*

Main category: cs.CV

TL;DR: SFO introduces a comparative learning framework for zero-shot subject-driven generation, using synthetic negatives and timestep reweighting to enhance subject fidelity.


<details>
  <summary>Details</summary>
Motivation: Improving subject fidelity in zero-shot subject-driven generation by addressing limitations of supervised fine-tuning methods.

Method: Uses synthetic negative targets via Condition-Degradation Negative Sampling (CDNS) and pairwise comparison, with reweighted diffusion timesteps.

Result: Outperforms baselines in subject fidelity and text alignment on a benchmark.

Conclusion: SFO with CDNS is effective for enhancing subject-driven generation.

Abstract: We present Subject Fidelity Optimization (SFO), a novel comparative learning
framework for zero-shot subject-driven generation that enhances subject
fidelity. Beyond supervised fine-tuning methods that rely only on positive
targets and use the diffusion loss as in the pre-training stage, SFO introduces
synthetic negative targets and explicitly guides the model to favor positives
over negatives through pairwise comparison. For negative targets, we propose
Condition-Degradation Negative Sampling (CDNS), which automatically generates
distinctive and informative negatives by intentionally degrading visual and
textual cues without expensive human annotations. Moreover, we reweight the
diffusion timesteps to focus finetuning on intermediate steps where subject
details emerge. Extensive experiments demonstrate that SFO with CDNS
significantly outperforms baselines in terms of both subject fidelity and text
alignment on a subject-driven generation benchmark. Project page:
https://subjectfidelityoptimization.github.io/

</details>


### [243] [A Large-Scale Referring Remote Sensing Image Segmentation Dataset and Benchmark](https://arxiv.org/pdf/2506.03583)
*Zhigang Yang, Huiguang Yao, Linmao Tian, Xuezhi Zhao, Qiang Li, Qi Wang*

Main category: cs.CV

TL;DR: The paper introduces NWPU-Refer, the largest RRSIS dataset, and MRSNet, a novel framework for referring remote sensing image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing RRSIS datasets lack resolution, diversity, and category coverage, limiting model generalization and real-world applicability.

Method: Proposes MRSNet with Intra-scale Feature Interaction Module (IFIM) and Hierarchical Feature Interaction Module (HFIM) for fine-grained and cross-scale feature fusion.

Result: MRSNet achieves state-of-the-art performance on the NWPU-Refer dataset.

Conclusion: The dataset and MRSNet framework advance RRSIS research, with publicly available resources.

Abstract: Referring Remote Sensing Image Segmentation is a complex and challenging task
that integrates the paradigms of computer vision and natural language
processing. Existing datasets for RRSIS suffer from critical limitations in
resolution, scene diversity, and category coverage, which hinders the
generalization and real-world applicability of refer segmentation models. To
facilitate the development of this field, we introduce NWPU-Refer, the largest
and most diverse RRSIS dataset to date, comprising 15,003 high-resolution
images (1024-2048px) spanning 30+ countries with 49,745 annotated targets
supporting single-object, multi-object, and non-object segmentation scenarios.
Additionally, we propose the Multi-scale Referring Segmentation Network
(MRSNet), a novel framework tailored for the unique demands of RRSIS. MRSNet
introduces two key innovations: (1) an Intra-scale Feature Interaction Module
(IFIM) that captures fine-grained details within each encoder stage, and (2) a
Hierarchical Feature Interaction Module (HFIM) to enable seamless cross-scale
feature fusion, preserving spatial integrity while enhancing discriminative
power. Extensive experiments conducte on the proposed NWPU-Refer dataset
demonstrate that MRSNet achieves state-of-the-art performance across multiple
evaluation metrics, validating its effectiveness. The dataset and code are
publicly available at https://github.com/CVer-Yang/NWPU-Refer.

</details>


### [244] [Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/pdf/2506.03642)
*Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei Wang, Liqiang Nie*

Main category: cs.CV

TL;DR: A framework combining SpatialMind (structured prompting) and ScanForgeQA (scalable QA dataset) enhances 3D spatial reasoning in pre-trained VLMs without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Address spatial uncertainty and data scarcity in 3D spatial reasoning for VLMs.

Method: Uses SpatialMind for structured prompting and ScanForgeQA for fine-tuning via automated dataset construction.

Result: Improves 3D spatial reasoning in VLMs across benchmarks, validating the framework's effectiveness.

Conclusion: The framework advances visual-spatial understanding and offers insights for future research.

Abstract: Visual-spatial understanding, the ability to infer object relationships and
layouts from visual input, is fundamental to downstream tasks such as robotic
navigation and embodied interaction. However, existing methods face spatial
uncertainty and data scarcity, limiting the 3D spatial reasoning capability of
pre-trained vision-language models (VLMs). To address these challenges, we
present a unified framework for enhancing 3D spatial reasoning in pre-trained
VLMs without modifying their architecture. This framework combines SpatialMind,
a structured prompting strategy that decomposes complex scenes and questions
into interpretable reasoning steps, with ScanForgeQA, a scalable
question-answering dataset built from diverse 3D simulation scenes through an
automated construction process designed for fine-tuning. Extensive experiments
across multiple benchmarks demonstrate the individual and combined
effectiveness of our prompting and fine-tuning strategies, and yield insights
that may inspire future research on visual-spatial understanding.

</details>


### [245] [Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts](https://arxiv.org/pdf/2506.03591)
*Jiaxing Zhang, Xinyi Zeng, Hao Tang*

Main category: cs.CV

TL;DR: UTAMoE, a Unified Task-Aware Mixture-of-Experts framework, decouples autoregressive transformer modules to resolve task objective conflicts in multimodal LLMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing intrinsic Task Objective Conflicts in multimodal LLMs between understanding (high-level semantics) and generation (fine-grained details), which existing solutions fail to resolve due to autoregressive architecture limitations.

Method: Proposes UTAMoE, a framework using a Task-Aware MoE Layer to decouple internal autoregressive modules, creating task-specific optimization paths, and a Two-Stage Training Strategy for task differentiation and coordination.

Result: UTAMoE mitigates task conflicts and achieves state-of-the-art performance on multimodal benchmarks, validated by visualizations and ablation studies.

Conclusion: The UTAMoE framework effectively resolves task objective conflicts in multimodal LLMs, enhancing performance and coordination across tasks.

Abstract: Unified multimodal large language models (MLLMs) based on end-to-end
autoregressive (AR) transformers effectively integrate both understanding and
generation tasks within a single framework. However, intrinsic Task Objective
Conflicts between high-level semantic abstraction in understanding and
fine-grained detail preservation in generation pose significant challenges,
often leading to suboptimal trade-offs and task interference. Existing
solutions, such as decoupling shared visual encoders, fall short of
fundamentally resolving these conflicts due to inherent AR architecture. In
this paper, we propose a novel approach that decouples internal components of
AR to resolve task objective conflicts. Specifically, we design UTAMoE, a
Unified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal
AR modules via a Task-Aware MoE Layer to create task-specific optimization
subpaths. To enhance task differentiation while maintaining overall
coordination, we introduce a novel Two-Stage Training Strategy. Extensive
experiments on multimodal benchmarks demonstrate that UTAMoE mitigates task
objective conflicts, achieving state-of-the-art performance across various
tasks. Visualizations and ablation studies further validate the effectiveness
of our approach.

</details>


### [246] [MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://arxiv.org/pdf/2506.03654)
*Xiaochun Lei, Siqi Wu, Weilin Wu, Zetao Jiang*

Main category: cs.CV

TL;DR: MambaNeXt-YOLO is a new object detection framework combining CNNs and Mamba for efficient real-time detection, achieving 66.6% mAP at 31.9 FPS on PASCAL VOC.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational complexity of Transformers and the need for efficient real-time object detection on edge devices.

Method: Hybrid MambaNeXt Block (CNNs + Mamba), Multi-branch Asymmetric Fusion Pyramid Network (MAFPN), and edge-focused optimization.

Result: 66.6% mAP at 31.9 FPS on PASCAL VOC, deployable on edge devices like NVIDIA Jetson Xavier NX.

Conclusion: MambaNeXt-YOLO effectively balances accuracy and efficiency for real-time object detection, suitable for edge deployments.

Abstract: Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6\% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.

</details>


### [247] [ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning](https://arxiv.org/pdf/2506.03596)
*Feng Han, Yang Jiao, Shaoxiang Chen, Junhao Xu, Jingjing Chen, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: ControlThinker is a novel framework that enhances controllable image generation by enriching text prompts with latent semantics from control images, improving visual quality and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the semantic gap between sparse text prompts and target images in controllable image generation, which current methods struggle with.

Method: Uses a 'comprehend-then-generate' paradigm, leveraging a MLLM for visual reasoning to mine latent semantics from control images and enrich prompts. Includes a metric-based ORM to handle ambiguity.

Result: Effectively reduces the semantic gap, improving visual quality and semantic consistency across benchmarks.

Conclusion: ControlThinker offers a robust solution for bridging the semantic gap in controllable image generation, with publicly available code and models.

Abstract: The field of controllable image generation has seen significant advancements,
with various architectures improving generation layout consistency with control
signals. However, contemporary methods still face challenges in bridging the
semantic gap between input text prompts with sparse semantics and the target
images, often over-relying on low-level control signals to infer regional
details. To address this challenge, we propose ControlThinker, a novel
framework that employs a "comprehend-then-generate" paradigm. Firstly, by
incentivizing the visual reasoning capability of a MLLM, latent semantics from
control images are mined to enrich text prompts. This enriched semantic
understanding then seamlessly aids in image generation without the need for
additional complex modifications. To further tackle the uncertainty arising
from the ambiguity of control images, we encourage broader exploration of
reasoning trajectories and select the optimal one using a metric-based output
reward model (ORM). Extensive experimental results demonstrate that
ControlThinker effectively mitigates the semantic gap between raw text prompts
and target images, resulting in improved visual quality and semantic
consistency across a wide range of benchmarks. The code and models are
available at https://github.com/Maplebb/ControlThinker.

</details>


### [248] [Accelerating SfM-based Pose Estimation with Dominating Set](https://arxiv.org/pdf/2506.03667)
*Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar*

Main category: cs.CV

TL;DR: A preprocessing technique using graph theory's dominating set speeds up SfM-based pose estimation, improving speed 1.5-14.48x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance real-time applications like AR, VR, and robotics by accelerating SfM-based pose estimation without sacrificing accuracy.

Method: Leverages a dominating set from graph theory to preprocess SfM models, reducing reference images and point cloud size.

Result: Achieved speed improvements of 1.5-14.48x, reduced reference images by 17-23x, and point cloud size by 2.27-4x.

Conclusion: The method efficiently balances speed and accuracy, offering a practical solution for real-time 3D pose estimation.

Abstract: This paper introduces a preprocessing technique to speed up
Structure-from-Motion (SfM) based pose estimation, which is critical for
real-time applications like augmented reality (AR), virtual reality (VR), and
robotics. Our method leverages the concept of a dominating set from graph
theory to preprocess SfM models, significantly enhancing the speed of the pose
estimation process without losing significant accuracy. Using the OnePose
dataset, we evaluated our method across various SfM-based pose estimation
techniques. The results demonstrate substantial improvements in processing
speed, ranging from 1.5 to 14.48 times, and a reduction in reference images and
point cloud size by factors of 17-23 and 2.27-4, respectively. This work offers
a promising solution for efficient and accurate 3D pose estimation, balancing
speed and accuracy in real-time applications.

</details>


### [249] [Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision](https://arxiv.org/pdf/2506.03605)
*Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori*

Main category: cs.CV

TL;DR: A framework for generating 6DoF manipulation trajectories from action descriptions using large-scale video datasets, validated on HOT3D.


<details>
  <summary>Details</summary>
Motivation: Developing interactive robots requires diverse manipulation demonstrations, which are hard to gather at scale.

Method: Leverages Exo-Ego4D video datasets to extract trajectories, then trains visual and point cloud-based language models for trajectory generation.

Result: Models successfully generate valid object trajectories in the HOT3D dataset.

Conclusion: Establishes a baseline for generating manipulation trajectories from action descriptions in egocentric vision.

Abstract: Learning to use tools or objects in common scenes, particularly handling them
in various ways as instructed, is a key challenge for developing interactive
robots. Training models to generate such manipulation trajectories requires a
large and diverse collection of detailed manipulation demonstrations for
various objects, which is nearly unfeasible to gather at scale. In this paper,
we propose a framework that leverages large-scale ego- and exo-centric video
datasets -- constructed globally with substantial effort -- of Exo-Ego4D to
extract diverse manipulation trajectories at scale. From these extracted
trajectories with the associated textual action description, we develop
trajectory generation models based on visual and point cloud-based language
models. In the recently proposed egocentric vision-based in-a-quality
trajectory dataset of HOT3D, we confirmed that our models successfully generate
valid object trajectories, establishing a training dataset and baseline models
for the novel task of generating 6DoF manipulation trajectories from action
descriptions in egocentric vision.

</details>


### [250] [How PARTs assemble into wholes: Learning the relative composition of images](https://arxiv.org/pdf/2506.03682)
*Melika Ayoughi, Samira Abnar, Chen Huang, Chris Sandino, Sayeri Lala, Eeshan Gunesh Dhekane, Dan Busbridge, Shuangfei Zhai, Vimal Thilak, Josh Susskind, Pascal Mettes, Paul Groth, Hanlin Goh*

Main category: cs.CV

TL;DR: PART is a self-supervised learning method that uses continuous relative transformations between off-grid patches to improve spatial understanding, outperforming grid-based methods in tasks like object detection.


<details>
  <summary>Details</summary>
Motivation: Existing grid-based approaches fail to capture the fluid and continuous nature of real-world object compositions, limiting their effectiveness.

Method: PART leverages continuous relative transformations between off-grid patches to model how parts relate to each other in a continuous space.

Result: PART outperforms grid-based methods (e.g., MAE, DropPos) in tasks requiring precise spatial understanding and maintains competitive performance in global classification tasks.

Conclusion: PART offers a promising new direction for universal self-supervised pretraining across diverse datatypes, including images, EEG signals, video, and audio.

Abstract: The composition of objects and their parts, along with object-object
positional relationships, provides a rich source of information for
representation learning. Hence, spatial-aware pretext tasks have been actively
explored in self-supervised learning. Existing works commonly start from a grid
structure, where the goal of the pretext task involves predicting the absolute
position index of patches within a fixed grid. However, grid-based approaches
fall short of capturing the fluid and continuous nature of real-world object
compositions. We introduce PART, a self-supervised learning approach that
leverages continuous relative transformations between off-grid patches to
overcome these limitations. By modeling how parts relate to each other in a
continuous space, PART learns the relative composition of images-an off-grid
structural relative positioning process that generalizes beyond occlusions and
deformations. In tasks requiring precise spatial understanding such as object
detection and time series prediction, PART outperforms strong grid-based
methods like MAE and DropPos, while also maintaining competitive performance on
global classification tasks with minimal hyperparameter tuning. By breaking
free from grid constraints, PART opens up an exciting new trajectory for
universal self-supervised pretraining across diverse datatypes-from natural
images to EEG signals-with promising potential in video, medical imaging, and
audio.

</details>


### [251] [Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI](https://arxiv.org/pdf/2506.03607)
*Wing Man Casca Kwok, Yip Chiu Tung, Kunal Bhagchandani*

Main category: cs.CV

TL;DR: Transformer-based image captioning models optimized for edge devices enable real-time AI in IoT applications, balancing performance and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Industrial automation and IoT applications require real-time AI for tasks like robotics and inspection, but edge devices face computational limitations.

Method: Evaluated resource-effective transformer models and applied knowledge distillation to optimize performance on edge devices.

Result: Demonstrated accelerated inference on resource-constrained devices while maintaining model performance.

Conclusion: Optimized transformer models enable effective real-time AI at the edge for industrial and IoT applications.

Abstract: Edge computing decentralizes processing power to network edge, enabling
real-time AI-driven decision-making in IoT applications. In industrial
automation such as robotics and rugged edge AI, real-time perception and
intelligence are critical for autonomous operations. Deploying
transformer-based image captioning models at the edge can enhance machine
perception, improve scene understanding for autonomous robots, and aid in
industrial inspection.
  However, these edge or IoT devices are often constrained in computational
resources for physical agility, yet they have strict response time
requirements. Traditional deep learning models can be too large and
computationally demanding for these devices. In this research, we present
findings of transformer-based models for image captioning that operate
effectively on edge devices. By evaluating resource-effective transformer
models and applying knowledge distillation techniques, we demonstrate inference
can be accelerated on resource-constrained devices while maintaining model
performance using these techniques.

</details>


### [252] [OSGNet @ Ego4D Episodic Memory Challenge 2025](https://arxiv.org/pdf/2506.03710)
*Yisen Feng, Haoyu Zhang, Qiaohui Chu, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie*

Main category: cs.CV

TL;DR: The paper presents a winning solution for the Ego4D Episodic Memory Challenge, using an early fusion-based model to improve video localization accuracy across three tasks.


<details>
  <summary>Details</summary>
Motivation: Existing late fusion strategies for video localization are suboptimal, prompting the need for a more effective approach.

Method: An early fusion-based video localization model is employed to address the tasks.

Result: The method achieved first place in all three tracks: Natural Language Queries, Goal Step, and Moment Queries.

Conclusion: The early fusion-based model proves effective for precise video localization, as demonstrated by its top performance in the challenge.

Abstract: In this report, we present our champion solutions for the three egocentric
video localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.
All tracks require precise localization of the interval within an untrimmed
egocentric video. Previous unified video localization approaches often rely on
late fusion strategies, which tend to yield suboptimal results. To address
this, we adopt an early fusion-based video localization model to tackle all
three tasks, aiming to enhance localization accuracy. Ultimately, our method
achieved first place in the Natural Language Queries, Goal Step, and Moment
Queries tracks, demonstrating its effectiveness. Our code can be found at
https://github.com/Yisen-Feng/OSGNet.

</details>


### [253] [PDSE: A Multiple Lesion Detector for CT Images using PANet and Deformable Squeeze-and-Excitation Block](https://arxiv.org/pdf/2506.03608)
*Di Fan, Heng Yu, Zhiyuan Xu*

Main category: cs.CV

TL;DR: The paper introduces PDSE, a one-stage lesion detection framework for CT scans, enhancing Retinanet with low-level feature maps and adaptive SE blocks for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Detecting diverse lesions in CT scans is challenging due to varying types, sizes, and locations, prompting the need for more accurate and efficient methods.

Method: PDSE redesigns Retinanet by incorporating low-level feature maps and adaptive SE blocks with channel feature map attention to enhance lesion detection.

Result: The method achieves state-of-the-art performance, improving small and multiscaled object detection, with an mAP over 0.20 on the DeepLesion benchmark.

Conclusion: PDSE demonstrates superior accuracy and efficiency in lesion detection, setting a new benchmark for multimodal CT image analysis.

Abstract: Detecting lesions in Computed Tomography (CT) scans is a challenging task in
medical image processing due to the diverse types, sizes, and locations of
lesions. Recently, various one-stage and two-stage framework networks have been
developed to focus on lesion localization. We introduce a one-stage lesion
detection framework, PDSE, by redesigning Retinanet to achieve higher accuracy
and efficiency for detecting lesions in multimodal CT images. Specifically, we
enhance the path aggregation flow by incorporating a low-level feature map.
Additionally, to improve model representation, we utilize the adaptive
Squeeze-and-Excitation (SE) block and integrate channel feature map attention.
This approach has resulted in achieving new state-of-the-art performance. Our
method significantly improves the detection of small and multiscaled objects.
When evaluated against other advanced algorithms on the public DeepLesion
benchmark, our algorithm achieved an mAP of over 0.20.

</details>


### [254] [ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices](https://arxiv.org/pdf/2506.03737)
*Hao Yu, Tangyu Jiang, Shuning Jia, Shannan Yan, Shunning Liu, Haolong Qian, Guanghao Li, Shuting Dong, Huaisong Zhang, Chun Yuan*

Main category: cs.CV

TL;DR: ComRoPE improves Rotary Positional Encoding (RoPE) by introducing trainable commuting angle matrices, enhancing performance and flexibility in positional encoding.


<details>
  <summary>Details</summary>
Motivation: Traditional position encoding methods lack robustness and flexibility, and RoPE's manual rotation matrices limit model capacity.

Method: ComRoPE generalizes RoPE with trainable commuting angle matrices, ensuring scalability and positional robustness.

Result: ComRoPE outperforms state-of-the-art by 1.6% at training resolution and 2.9% at higher resolution on ImageNet-1K.

Conclusion: ComRoPE offers a versatile, scalable solution for positional encoding, with potential for future research.

Abstract: The Transformer architecture has revolutionized various regions since it was
proposed, and its effectiveness largely depends on the ability to encode
positional information. Traditional position encoding methods exhibit
significant limitations due to lack of robustness and flexibility of position.
Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these
issues, which integrates positional information by rotating the embeddings in
the attention mechanism. However, RoPE requires manually defined rotation
matrices with limited transformation space, constraining the model's capacity.
In this work, we propose ComRoPE, which generalizes RoPE by defining it in
terms of trainable commuting angle matrices. Specifically, we demonstrate that
pairwise commutativity of these matrices is essential for RoPE to achieve
scalability and positional robustness. We formally define the RoPE Equation,
which is an essential condition that ensures consistent performance with
position offsets. Based on the theoretical analysis, we present two types of
trainable commuting angle matrices as sufficient solutions to the RoPE
equation, which significantly improve performance, surpassing the current
state-of-the-art method by 1.6% at training resolution and 2.9% at higher
resolution on the ImageNet-1K dataset. Furthermore, our framework shows
versatility in generalizing to existing RoPE formulations and offering new
insights for future positional encoding research. To ensure reproducibility,
the source code and instructions are available at
https://github.com/Longin-Yu/ComRoPE

</details>


### [255] [Isharah: A Large-Scale Multi-Scene Dataset for Continuous Sign Language Recognition](https://arxiv.org/pdf/2506.03615)
*Sarah Alyami, Hamzah Luqman, Sadam Al-Azani, Maad Alowaifeer, Yazeed Alharbi, Yaser Alonaizan*

Main category: cs.CV

TL;DR: The paper introduces Isharah, a large multi-scene dataset for continuous sign language recognition (CSLR) collected in unconstrained environments, addressing limitations of existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing CSLR datasets are limited and collected in controlled settings, hindering robust real-world SLR systems.

Method: Isharah is created using smartphone cameras, capturing 30,000 video clips with high variability in recording settings. It includes gloss-level annotations.

Result: The dataset supports CSLR and sign language translation (SLT) benchmarks, including signer-independent and unseen-sentence tasks.

Conclusion: Isharah enhances real-world SLR and SLT development by providing diverse, linguistically rich data.

Abstract: Current benchmarks for sign language recognition (SLR) focus mainly on
isolated SLR, while there are limited datasets for continuous SLR (CSLR), which
recognizes sequences of signs in a video. Additionally, existing CSLR datasets
are collected in controlled settings, which restricts their effectiveness in
building robust real-world CSLR systems. To address these limitations, we
present Isharah, a large multi-scene dataset for CSLR. It is the first dataset
of its type and size that has been collected in an unconstrained environment
using signers' smartphone cameras. This setup resulted in high variations of
recording settings, camera distances, angles, and resolutions. This variation
helps with developing sign language understanding models capable of handling
the variability and complexity of real-world scenarios. The dataset consists of
30,000 video clips performed by 18 deaf and professional signers. Additionally,
the dataset is linguistically rich as it provides a gloss-level annotation for
all dataset's videos, making it useful for developing CSLR and sign language
translation (SLT) systems. This paper also introduces multiple sign language
understanding benchmarks, including signer-independent and unseen-sentence
CSLR, along with gloss-based and gloss-free SLT. The Isharah dataset is
available on https://snalyami.github.io/Isharah_CSLR/.

</details>


### [256] [SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution](https://arxiv.org/pdf/2506.03740)
*Jianfeng Wu, Nannan Xu*

Main category: cs.CV

TL;DR: The paper proposes SAAT, a novel Transformer-based model for single image super-resolution, combining channel and spatial attention synergistically to improve feature utilization and performance.


<details>
  <summary>Details</summary>
Motivation: Current Transformer-based super-resolution methods neglect useful cross-channel and spatial structural information due to nonoverlapping window self-attention. The paper aims to address this by exploring the synergistic relationship between channel and spatial attention.

Method: Introduces SAAT with two key components: CWSAG (combines channel attention and shifted window attention) and SWSAG (leverages spatial attention). These enhance feature fusion and structural feature extraction.

Result: SAAT achieves performance comparable to state-of-the-art models under the same parameter count, demonstrating effectiveness in super-resolution.

Conclusion: SAAT successfully integrates channel and spatial attention synergistically, improving super-resolution results and feature utilization.

Abstract: Single image super-resolution is a well-known downstream task which aims to
restore low-resolution images into high-resolution images. At present, models
based on Transformers have shone brightly in the field of super-resolution due
to their ability to capture long-term dependencies in information. However,
current methods typically compute self-attention in nonoverlapping windows to
save computational costs, and the standard self-attention computation only
focuses on its results, thereby neglecting the useful information across
channels and the rich spatial structural information generated in the
intermediate process. Channel attention and spatial attention have,
respectively, brought significant improvements to various downstream visual
tasks in terms of extracting feature dependency and spatial structure
relationships, but the synergistic relationship between channel and spatial
attention has not been fully explored yet.To address these issues, we propose a
novel model. Synergistic Alternating Aggregation Transformer (SAAT), which can
better utilize the potential information of features. In SAAT, we introduce the
Efficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial
& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines
efficient channel attention with shifted window attention, enhancing non-local
feature fusion, and producing more visually appealing results. On the other
hand, SWSAG leverages spatial attention to capture rich structured feature
information, thereby enabling SAAT to more effectively extract structural
features.Extensive experimental results and ablation studies demonstrate the
effectiveness of SAAT in the field of super-resolution. SAAT achieves
performance comparable to that of the state-of-the-art (SOTA) under the same
quantity of parameters.

</details>


### [257] [FingerVeinSyn-5M: A Million-Scale Dataset and Benchmark for Finger Vein Recognition](https://arxiv.org/pdf/2506.03635)
*Yinfan Wang, Jie Gui, Baosheng Yu, Qi Li, Zhenan Sun, Juho Kannala, Guoying Zhao*

Main category: cs.CV

TL;DR: The paper introduces FVeinSyn, a synthetic generator for finger vein patterns, and FingerVeinSyn-5M, the largest finger vein dataset with 5M samples. Pretraining on it boosts performance by 53.91%.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale public finger vein datasets limits deep learning advancements in this field.

Method: Developed FVeinSyn to generate synthetic finger vein patterns with rich variations and created FingerVeinSyn-5M dataset.

Result: Pretraining on FingerVeinSyn-5M and fine-tuning with minimal real data yields a 53.91% performance gain.

Conclusion: FingerVeinSyn-5M addresses dataset scarcity, enabling better deep learning models for finger vein recognition.

Abstract: A major challenge in finger vein recognition is the lack of large-scale
public datasets. Existing datasets contain few identities and limited samples
per finger, restricting the advancement of deep learning-based methods. To
address this, we introduce FVeinSyn, a synthetic generator capable of producing
diverse finger vein patterns with rich intra-class variations. Using FVeinSyn,
we created FingerVeinSyn-5M -- the largest available finger vein dataset --
containing 5 million samples from 50,000 unique fingers, each with 100
variations including shift, rotation, scale, roll, varying exposure levels,
skin scattering blur, optical blur, and motion blur. FingerVeinSyn-5M is also
the first to offer fully annotated finger vein images, supporting deep learning
applications in this field. Models pretrained on FingerVeinSyn-5M and
fine-tuned with minimal real data achieve an average 53.91\% performance gain
across multiple benchmarks. The dataset is publicly available at:
https://github.com/EvanWang98/FingerVeinSyn-5M.

</details>


### [258] [Images are Worth Variable Length of Representations](https://arxiv.org/pdf/2506.03643)
*Lingjun Mao, Rodolfo Corona, Xin Liang, Wenhao Yan, Zineng Tang*

Main category: cs.CV

TL;DR: DOVE is a dynamic vision encoder that adapts token count per image based on complexity, improving efficiency and semantic feature capture.


<details>
  <summary>Details</summary>
Motivation: Fixed-length token sequences in vision encoders ignore varying image information, leading to inefficiency.

Method: DOVE dynamically adjusts token count per image and introduces query-conditioned tokenization for targeted feature extraction.

Result: DOVE reduces token count while maintaining quality, outperforming fixed-length methods in tasks.

Conclusion: DOVE offers efficient, adaptive tokenization with superior semantic feature extraction.

Abstract: Most existing vision encoders map images into a fixed-length sequence of
tokens, overlooking the fact that different images contain varying amounts of
information. For example, a visually complex image (e.g., a cluttered room)
inherently carries more information and thus deserves more tokens than a simple
image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a
dynamic vision encoder that produces a variable number of visual tokens (i.e.,
continuous representation vectors) to reconstruct each image. Our results show
that DOVE significantly reduces the average number of tokens while maintaining
high reconstruction quality. In several linear probing and downstream
multimodal tasks, it outperforms existing autoencoder-based tokenization
methods when using far fewer tokens, capturing more expressive semantic
features compared to fixed-length encoding. We further extend DOVE with
query-conditioned tokenization. By guiding the model to focus on query-relevant
regions, it achieves more efficient and targeted semantic extraction. Our code
and checkpoints are available at https://dove-encoder.github.io/dove-encoder.

</details>


### [259] [EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation](https://arxiv.org/pdf/2506.03652)
*Cheng Zhang, Hongxia xie, Bin Wen, Songhan Zuo, Ruoxuan Zhang, Wen-huang Cheng*

Main category: cs.CV

TL;DR: The paper introduces the EmoArt Dataset, a comprehensive emotion-annotated art dataset, to address challenges in generating emotionally expressive and abstract artistic images using text-to-image diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models lack the ability to generate emotionally expressive and abstract art due to the absence of large-scale, fine-grained emotional datasets.

Method: The authors present the EmoArt Dataset, containing 132,664 artworks with structured annotations, and evaluate popular text-to-image diffusion models for emotional alignment.

Result: The dataset provides benchmarks for emotion-driven image synthesis, aiding fields like affective computing, multimodal learning, and computational art.

Conclusion: The EmoArt Dataset advances emotion-driven image generation and supports applications in art therapy and creative design.

Abstract: With the rapid advancement of diffusion models, text-to-image generation has
achieved significant progress in image resolution, detail fidelity, and
semantic alignment, particularly with models like Stable Diffusion 3.5, Stable
Diffusion XL, and FLUX 1. However, generating emotionally expressive and
abstract artistic images remains a major challenge, largely due to the lack of
large-scale, fine-grained emotional datasets. To address this gap, we present
the EmoArt Dataset -- one of the most comprehensive emotion-annotated art
datasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,
Impressionism, Expressionism, Abstract Art), offering rich stylistic and
cultural diversity. Each image includes structured annotations: objective scene
descriptions, five key visual attributes (brushwork, composition, color, line,
light), binary arousal-valence labels, twelve emotion categories, and potential
art therapy effects. Using EmoArt, we systematically evaluate popular
text-to-image diffusion models for their ability to generate emotionally
aligned images from text. Our work provides essential data and benchmarks for
emotion-driven image synthesis and aims to advance fields such as affective
computing, multimodal learning, and computational art, enabling applications in
art therapy and creative design. The dataset and more details can be accessed
via our project website.

</details>


### [260] [INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning](https://arxiv.org/pdf/2506.03660)
*Wei Luo, Haiming Yao, Yunkang Cao, Qiyu Chen, Ang Gao, Weiming Shen, Weihang Zhang, Wenyong Yu*

Main category: cs.CV

TL;DR: INP-Former is a novel anomaly detection method that extracts intrinsic normal prototypes (INPs) from test images, avoiding reliance on external references. It achieves state-of-the-art performance across various tasks and settings, including zero-shot detection.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods rely on external normal references, which may misalign with test images due to variations. INP-Former leverages intrinsic normal information within the test image itself for better alignment and accuracy.

Method: INP-Former extracts INPs from test images using an INP Extractor and ensures their coherence with a dedicated loss. An INP-guided Decoder reconstructs normal tokens, with reconstruction errors indicating anomalies. A Soft Mining Loss prioritizes hard samples during training.

Result: INP-Former outperforms existing methods in single-class, multi-class, and few-shot anomaly detection tasks. It also shows zero-shot capability and is further improved in INP-Former++.

Conclusion: INP-Former is a versatile and universal solution for anomaly detection, leveraging intrinsic normal information for superior performance across diverse settings.

Abstract: Anomaly detection (AD) is essential for industrial inspection and medical
diagnosis, yet existing methods typically rely on ``comparing'' test images to
normal references from a training set. However, variations in appearance and
positioning often complicate the alignment of these references with the test
image, limiting detection accuracy. We observe that most anomalies manifest as
local variations, meaning that even within anomalous images, valuable normal
information remains. We argue that this information is useful and may be more
aligned with the anomalies since both the anomalies and the normal information
originate from the same image. Therefore, rather than relying on external
normality from the training set, we propose INP-Former, a novel method that
extracts Intrinsic Normal Prototypes (INPs) directly from the test image.
Specifically, we introduce the INP Extractor, which linearly combines normal
tokens to represent INPs. We further propose an INP Coherence Loss to ensure
INPs can faithfully represent normality for the testing image. These INPs then
guide the INP-guided Decoder to reconstruct only normal tokens, with
reconstruction errors serving as anomaly scores. Additionally, we propose a
Soft Mining Loss to prioritize hard-to-optimize samples during training.
INP-Former achieves state-of-the-art performance in single-class, multi-class,
and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a
versatile and universal solution for AD. Remarkably, INP-Former also
demonstrates some zero-shot AD capability. Furthermore, we propose a soft
version of the INP Coherence Loss and enhance INP-Former by incorporating
residual learning, leading to the development of INP-Former++. The proposed
method significantly improves detection performance across single-class,
multi-class, semi-supervised, few-shot, and zero-shot settings.

</details>


### [261] [Zero-Shot Temporal Interaction Localization for Egocentric Videos](https://arxiv.org/pdf/2506.03662)
*Erhang Zhang, Junyi Ma, Yin-Dong Zheng, Yixuan Zhou, Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot TIL method for egocentric videos, using self-adaptive sampling and closed-loop feedback to improve grasp action localization.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on annotated data, causing domain bias and inefficiency, while existing ZS-TAL approaches lack precision.

Method: EgoLoc uses 2D/3D observations and hand velocities for sampling, with closed-loop feedback for refinement.

Result: Outperforms state-of-the-art baselines in temporal interaction localization.

Conclusion: EgoLoc offers high accuracy and efficiency for HOI action localization in egocentric videos.

Abstract: Locating human-object interaction (HOI) actions within video serves as the
foundation for multiple downstream tasks, such as human behavior analysis and
human-robot skill transfer. Current temporal action localization methods
typically rely on annotated action and object categories of interactions for
optimization, which leads to domain bias and low deployment efficiency.
Although some recent works have achieved zero-shot temporal action localization
(ZS-TAL) with large vision-language models (VLMs), their coarse-grained
estimations and open-loop pipelines hinder further performance improvements for
temporal interaction localization (TIL). To address these issues, we propose a
novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp
actions for human-object interaction in egocentric videos. EgoLoc introduces a
self-adaptive sampling strategy to generate reasonable visual prompts for VLM
reasoning. By absorbing both 2D and 3D observations, it directly samples
high-quality initial guesses around the possible contact/separation timestamps
of HOI according to 3D hand velocities, leading to high inference accuracy and
efficiency. In addition, EgoLoc generates closed-loop feedback from visual and
dynamic cues to further refine the localization results. Comprehensive
experiments on the publicly available dataset and our newly proposed benchmark
demonstrate that EgoLoc achieves better temporal interaction localization for
egocentric videos compared to state-of-the-art baselines. We will release our
code and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.

</details>


### [262] [JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting](https://arxiv.org/pdf/2506.03872)
*Yang Xiao, Guoan Xu, Qiang Wu, Wenjing Jia*

Main category: cs.CV

TL;DR: JointSplat is a unified framework for sparse-view 3D reconstruction, combining optical flow and depth via probabilistic optimization, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing methods (mislocation, artifacts, noise, inconsistency) for sparse-view 3D reconstruction.

Method: Uses a probabilistic optimization mechanism to fuse depth and flow, with a multi-view depth-consistency loss.

Result: Outperforms SOTA on RealEstate10K and ACID, showing robustness and high fidelity.

Conclusion: JointSplat effectively combines flow and depth for superior sparse-view 3D reconstruction.

Abstract: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.

</details>


### [263] [Intersectional Bias in Pre-Trained Image Recognition Models](https://arxiv.org/pdf/2506.03664)
*Valerie Krug, Sebastian Stober*

Main category: cs.CV

TL;DR: The paper investigates biases in ImageNet classifiers for facial images, focusing on age, race, and gender intersections, using linear probes and activation maps.


<details>
  <summary>Details</summary>
Motivation: To address the risk of perpetuating biases in deep learning models built on pre-trained classifiers.

Method: Linear classifier probes and visualization of activations as topographic maps.

Result: ImageNet classifiers differentiate ages strongly, with weaker associations for ethnicities and gender distinctions in middle-aged groups.

Conclusion: The study highlights biases in pre-trained models, emphasizing the need for mitigation strategies.

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [264] [BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation](https://arxiv.org/pdf/2506.03675)
*Jialei Chen, Xu Zheng, Danda Pani Paudel, Luc Van Gool, Hiroshi Murase, Daisuke Deguchi*

Main category: cs.CV

TL;DR: BiXFormer improves multi-modal semantic segmentation by integrating Unified Modality Matching (UMM) and Cross Modality Alignment (CMA), achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing methods fuse multi-modal data into a unified representation, limiting each modality's potential. BiXFormer aims to maximize modality effectiveness and handle missing modalities.

Method: BiXFormer categorizes inputs into RGB and non-RGB (X), processes them separately, and uses UMM (MAM and CM) and CMA to enhance feature matching and alignment.

Result: Achieves mIoU improvements of +2.75% and +22.74% over prior methods on synthetic and real-world benchmarks.

Conclusion: BiXFormer effectively leverages multi-modal strengths, mitigates missing modality impacts, and outperforms existing approaches.

Abstract: Utilizing multi-modal data enhances scene understanding by providing
complementary semantic and geometric information. Existing methods fuse
features or distill knowledge from multiple modalities into a unified
representation, improving robustness but restricting each modality's ability to
fully leverage its strengths in different situations. We reformulate
multi-modal semantic segmentation as a mask-level classification task and
propose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross
Modality Alignment (CMA) to maximize modality effectiveness and handle missing
modalities. Specifically, BiXFormer first categorizes multi-modal inputs into
RGB and X, where X represents any non-RGB modalities, e.g., depth, allowing
separate processing for each. This design leverages the well-established
pretraining for RGB, while addressing the relative lack of attention to X
modalities. Then, we propose UMM, which includes Modality Agnostic Matching
(MAM) and Complementary Matching (CM). MAM assigns labels to features from all
modalities without considering modality differences, leveraging each modality's
strengths. CM then reassigns unmatched labels to remaining unassigned features
within their respective modalities, ensuring that each available modality
contributes to the final prediction and mitigating the impact of missing
modalities. Moreover, to further facilitate UMM, we introduce CMA, which
enhances the weaker queries assigned in CM by aligning them with optimally
matched queries from MAM. Experiments on both synthetic and real-world
multi-modal benchmarks demonstrate the effectiveness of our method, achieving
significant improvements in mIoU of +2.75% and +22.74% over the prior arts.

</details>


### [265] [PRJ: Perception-Retrieval-Judgement for Generated Images](https://arxiv.org/pdf/2506.03683)
*Qiang Fu, Zonglei Jing, Zonghao Ying, Xiaoqian Li*

Main category: cs.CV

TL;DR: The paper introduces PRJ, a framework for detecting toxic AI-generated visual content by mimicking human reasoning, improving accuracy and interpretability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current AI safety systems in detecting nuanced and context-dependent harmful content.

Method: Proposes PRJ, a three-stage framework: perception (image to language), retrieval (external knowledge), and judgement (toxicity evaluation).

Result: PRJ outperforms existing systems in accuracy and robustness, offering better interpretability and granularity.

Conclusion: PRJ provides a more effective and interpretable solution for detecting harmful AI-generated visual content.

Abstract: The rapid progress of generative AI has enabled remarkable creative
capabilities, yet it also raises urgent concerns regarding the safety of
AI-generated visual content in real-world applications such as content
moderation, platform governance, and digital media regulation. This includes
unsafe material such as sexually explicit images, violent scenes, hate symbols,
propaganda, and unauthorized imitations of copyrighted artworks. Existing image
safety systems often rely on rigid category filters and produce binary outputs,
lacking the capacity to interpret context or reason about nuanced,
adversarially induced forms of harm. In addition, standard evaluation metrics
(e.g., attack success rate) fail to capture the semantic severity and dynamic
progression of toxicity. To address these limitations, we propose
Perception-Retrieval-Judgement (PRJ), a cognitively inspired framework that
models toxicity detection as a structured reasoning process. PRJ follows a
three-stage design: it first transforms an image into descriptive language
(perception), then retrieves external knowledge related to harm categories and
traits (retrieval), and finally evaluates toxicity based on legal or normative
rules (judgement). This language-centric structure enables the system to detect
both explicit and implicit harms with improved interpretability and categorical
granularity. In addition, we introduce a dynamic scoring mechanism based on a
contextual toxicity risk matrix to quantify harmfulness across different
semantic dimensions. Experiments show that PRJ surpasses existing safety
checkers in detection accuracy and robustness while uniquely supporting
structured category-level toxicity interpretation.

</details>


### [266] [DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](https://arxiv.org/pdf/2506.03933)
*Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, Anders Holst*

Main category: cs.CV

TL;DR: DiffCAP is a diffusion-based method to neutralize adversarial corruptions in Vision Language Models (VLMs), outperforming existing defenses with minimal tuning and faster denoising.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to adversarial perturbations, which can cause unreliable outputs despite being imperceptible to humans.

Method: DiffCAP injects Gaussian noise into perturbed inputs until embeddings stabilize, then uses a pretrained diffusion model to denoise and recover clean representations.

Result: DiffCAP consistently outperforms other defenses across datasets, VLMs, and attack strengths, reducing tuning complexity and diffusion time.

Conclusion: DiffCAP offers a robust, practical solution for secure VLM deployment in adversarial settings, supported by theory and experiments.

Abstract: Vision Language Models (VLMs) have shown remarkable capabilities in
multimodal understanding, yet their susceptibility to perturbations poses a
significant threat to their reliability in real-world applications. Despite
often being imperceptible to humans, these perturbations can drastically alter
model outputs, leading to erroneous interpretations and decisions. This paper
introduces DiffCAP, a novel diffusion-based purification strategy that can
effectively neutralize adversarial corruptions in VLMs. We observe that adding
minimal noise to an adversarially corrupted image significantly alters its
latent embedding with respect to VLMs. Building on this insight, DiffCAP
cumulatively injects random Gaussian noise into adversarially perturbed input
data. This process continues until the embeddings of two consecutive noisy
images reach a predefined similarity threshold, indicating a potential approach
to neutralize the adversarial effect. Subsequently, a pretrained diffusion
model is employed to denoise the stabilized image, recovering a clean
representation suitable for the VLMs to produce an output. Through extensive
experiments across six datasets with three VLMs under varying attack strengths
in three task scenarios, we show that DiffCAP consistently outperforms existing
defense techniques by a substantial margin. Notably, DiffCAP significantly
reduces both hyperparameter tuning complexity and the required diffusion time,
thereby accelerating the denoising process. Equipped with strong theoretical
and empirical support, DiffCAP provides a robust and practical solution for
securely deploying VLMs in adversarial environments.

</details>


### [267] [DSSAU-Net:U-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation](https://arxiv.org/pdf/2506.03684)
*Zunhui Xia, Hongxing Li, Libin Lan*

Main category: cs.CV

TL;DR: A sparse self-attention network (DSSAU-Net) is proposed for accurate segmentation of fetal head and pubic symphysis in ultrasound images, improving childbirth diagnosis.


<details>
  <summary>Details</summary>
Motivation: Traditional vaginal examinations are subjective and inaccurate; ultrasound-assisted diagnosis using AoP and HSD parameters offers a better alternative, requiring precise segmentation of FH and PS.

Method: DSSAU-Net uses Dual Sparse Selection Attention blocks in a U-shaped encoder-decoder architecture, with sparse token selection at region and pixel levels, skip connections, and multiscale feature fusion.

Result: DSSAU-Net achieved fourth place in the MICCAI IUGC 2024 competition, validating its effectiveness in segmentation and classification tasks.

Conclusion: DSSAU-Net is a computationally efficient and effective solution for FH and PS segmentation, enhancing ultrasound-assisted childbirth diagnosis.

Abstract: In the childbirth process, traditional methods involve invasive vaginal
examinations, but research has shown that these methods are both subjective and
inaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way
to assess fetal head position via two key parameters: Angle of Progression
(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal
head (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth
delivery process. Therefore, accurate segmentation of FH and PS is crucial. In
this work, we propose a sparse self-attention network architecture with good
performance and high computational efficiency, named DSSAU-Net, for the
segmentation of FH and PS. Specifically, we stack varying numbers of Dual
Sparse Selection Attention (DSSA) blocks at each stage to form a symmetric
U-shaped encoder-decoder network architecture. For a given query, DSSA is
designed to explicitly perform one sparse token selection at both the region
and pixel levels, respectively, which is beneficial for further reducing
computational complexity while extracting the most relevant features. To
compensate for the information loss during the upsampling process, skip
connections with convolutions are designed. Additionally, multiscale feature
fusion is employed to enrich the model's global and local information. The
performance of DSSAU-Net has been validated using the Intrapartum Ultrasound
Grand Challenge (IUGC) 2024 \textit{test set} provided by the organizer in the
MICCAI IUGC 2024
competition\footnote{\href{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}},
where we win the fourth place on the tasks of classification and segmentation,
demonstrating its effectiveness. The codes will be available at
https://github.com/XiaZunhui/DSSAU-Net.

</details>


### [268] [Advancements in Artificial Intelligence Applications for Cardiovascular Disease Research](https://arxiv.org/pdf/2506.03698)
*Yuanlin Mo, Haishan Huang, Bocheng Liang, Weibo Ma*

Main category: cs.CV

TL;DR: AI advancements in cardiovascular medicine improve diagnostics via deep learning but face challenges in data validation. Future focus is on hybrid models for personalized care.


<details>
  <summary>Details</summary>
Motivation: To explore AI's role in enhancing cardiovascular diagnostics through medical imaging and signals, addressing current limitations.

Method: Utilizes deep learning (CNNs, GANs) for automated analysis of CT, MRI, ECG, and US data.

Result: AI surpasses human accuracy and efficiency but lacks robust validation for input data.

Conclusion: AI holds transformative potential but requires validation protocols and hybrid models for reliable clinical use.

Abstract: Recent advancements in artificial intelligence (AI) have revolutionized
cardiovascular medicine, particularly through integration with computed
tomography (CT), magnetic resonance imaging (MRI), electrocardiography (ECG)
and ultrasound (US). Deep learning architectures, including convolutional
neural networks and generative adversarial networks, enable automated analysis
of medical imaging and physiological signals, surpassing human capabilities in
diagnostic accuracy and workflow efficiency. However, critical challenges
persist, including the inability to validate input data accuracy, which may
propagate diagnostic errors. This review highlights AI's transformative
potential in precision diagnostics while underscoring the need for robust
validation protocols to ensure clinical reliability. Future directions
emphasize hybrid models integrating multimodal data and adaptive algorithms to
refine personalized cardiovascular care.

</details>


### [269] [OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2506.03706)
*Aditya Gandhamal, Aniruddh Sikdar, Suresh Sundaram*

Main category: cs.CV

TL;DR: OV-COAST improves open-vocabulary semantic segmentation by using cost aggregation with optimal transport, enhancing out-of-domain generalization and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To enhance out-of-domain generalization in open-vocabulary semantic segmentation by aligning visual-language features using optimal transport theory.

Method: Proposes a two-stage optimization: first solves optimal transport using cost volume and Sinkhorn distance, then uses the solution to train the CAT-Seg model.

Result: Achieves superior performance, surpassing CAT-Seg by 1.72% and SAN-B by 4.9% mIoU on the MESS benchmark.

Conclusion: OV-COAST effectively improves segmentation performance by leveraging optimal transport for feature alignment.

Abstract: Open-vocabulary semantic segmentation (OVSS) entails assigning semantic
labels to each pixel in an image using textual descriptions, typically
leveraging world models such as CLIP. To enhance out-of-domain generalization,
we propose Cost Aggregation with Optimal Transport (OV-COAST) for
open-vocabulary semantic segmentation. To align visual-language features within
the framework of optimal transport theory, we employ cost volume to construct a
cost matrix, which quantifies the distance between two distributions. Our
approach adopts a two-stage optimization strategy: in the first stage, the
optimal transport problem is solved using cost volume via Sinkhorn distance to
obtain an alignment solution; in the second stage, this solution is used to
guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS
models on the MESS benchmark, where our approach notably improves the
performance of the cost-aggregation model CAT-Seg with ViT-B backbone,
achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %
mIoU. The code is available at
https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .

</details>


### [270] [AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives](https://arxiv.org/pdf/2506.03709)
*Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram*

Main category: cs.CV

TL;DR: The paper introduces AetherVision-Bench, a benchmark for evaluating open-vocabulary semantic segmentation (OVSS) models across aerial and ground perspectives, addressing cross-domain generalization challenges.


<details>
  <summary>Details</summary>
Motivation: To improve the practical efficacy of OVSS models in real-world applications by addressing their cross-domain generalization issues, particularly in embodied AI systems like autonomous navigation.

Method: Creation of AetherVision-Bench, a multi-angle segmentation benchmark, and evaluation of state-of-the-art OVSS models to identify key performance factors.

Result: The benchmark provides insights into the performance of zero-shot transfer models across different viewing angles and sensor modalities.

Conclusion: The work establishes a foundation for future research by pioneering a robustness benchmark for OVSS models.

Abstract: Open-vocabulary semantic segmentation (OVSS) involves assigning labels to
each pixel in an image based on textual descriptions, leveraging world models
like CLIP. However, they encounter significant challenges in cross-domain
generalization, hindering their practical efficacy in real-world applications.
Embodied AI systems are transforming autonomous navigation for ground vehicles
and drones by enhancing their perception abilities, and in this study, we
present AetherVision-Bench, a benchmark for multi-angle segmentation across
aerial, and ground perspectives, which facilitates an extensive evaluation of
performance across different viewing angles and sensor modalities. We assess
state-of-the-art OVSS models on the proposed benchmark and investigate the key
factors that impact the performance of zero-shot transfer models. Our work
pioneers the creation of a robustness benchmark, offering valuable insights and
establishing a foundation for future research.

</details>


### [271] [PlückeRF: A Line-based 3D Representation for Few-view Reconstruction](https://arxiv.org/pdf/2506.03713)
*Sam Bahrami, Dylan Campbell*

Main category: cs.CV

TL;DR: A feed-forward 3D reconstruction method improves multi-view information utilization with a novel PlückeRF representation, outperforming triplane and state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing few-view 3D reconstruction by better leveraging multi-view information, addressing limitations of current learned priors.

Method: Introduces PlückeRF, a structured feature-augmented line representation, connecting 3D locations with input pixel rays for improved information sharing.

Result: Demonstrates superior reconstruction quality compared to triplane and leading feedforward methods.

Conclusion: The PlückeRF representation effectively harnesses multi-view data, advancing feed-forward 3D reconstruction.

Abstract: Feed-forward 3D reconstruction methods aim to predict the 3D structure of a
scene directly from input images, providing a faster alternative to per-scene
optimization approaches. Significant progress has been made in single-view and
few-view reconstruction using learned priors that infer object shape and
appearance, even for unobserved regions. However, there is substantial
potential to enhance these methods by better leveraging information from
multiple views when available. To address this, we propose a few-view
reconstruction model that more effectively harnesses multi-view information.
Our approach introduces a simple mechanism that connects the 3D representation
with pixel rays from the input views, allowing for preferential sharing of
information between nearby 3D locations and between 3D locations and nearby
pixel rays. We achieve this by defining the 3D representation as a set of
structured, feature-augmented lines; the Pl\"uckeRF representation. Using this
representation, we demonstrate improvements in reconstruction quality over the
equivalent triplane representation and state-of-the-art feedforward
reconstruction methods.

</details>


### [272] [FSHNet: Fully Sparse Hybrid Network for 3D Object Detection](https://arxiv.org/pdf/2506.03714)
*Shuai Liu, Mingyue Cui, Boyang Li, Quanmin Liang, Tinghe Hong, Kai Huang, Yunxiao Shan, Kai Huang*

Main category: cs.CV

TL;DR: FSHNet improves sparse 3D detectors by enhancing long-range feature extraction and optimizing network performance with dynamic label assignment and sparse upsampling.


<details>
  <summary>Details</summary>
Motivation: Sparse 3D detectors lack long-range interactions and center features, weakening feature extraction and optimization.

Method: FSHNet uses SlotFormer for better feature extraction, dynamic sparse label assignment for optimization, and sparse upsampling for detail preservation.

Result: FSHNet shows effectiveness on Waymo, nuScenes, and Argoverse2 benchmarks.

Conclusion: FSHNet addresses key limitations of sparse 3D detectors, improving performance and feature extraction.

Abstract: Fully sparse 3D detectors have recently gained significant attention due to
their efficiency in long-range detection. However, sparse 3D detectors extract
features only from non-empty voxels, which impairs long-range interactions and
causes the center feature missing. The former weakens the feature extraction
capability, while the latter hinders network optimization. To address these
challenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet
incorporates a proposed SlotFormer block to enhance the long-range feature
extraction capability of existing sparse encoders. The SlotFormer divides
sparse voxels using a slot partition approach, which, compared to traditional
window partition, provides a larger receptive field. Additionally, we propose a
dynamic sparse label assignment strategy to deeply optimize the network by
providing more high-quality positive samples. To further enhance performance,
we introduce a sparse upsampling module to refine downsampled voxels,
preserving fine-grained details crucial for detecting small objects. Extensive
experiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the
effectiveness of FSHNet. The code is available at
https://github.com/Say2L/FSHNet.

</details>


### [273] [HUMOF: Human Motion Forecasting in Interactive Social Scenes](https://arxiv.org/pdf/2506.03753)
*Caiyi Sun, Yujing Sun, Xiao Han, Zemin Yang, Jiawei Liu, Xinge Zhu, Siu Ming Yiu, Yuexin Ma*

Main category: cs.CV

TL;DR: A hierarchical interaction feature representation and coarse-to-fine reasoning module improve human motion prediction in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Complex scenes with abundant interactions complicate human behavior analysis and motion prediction, challenging existing methods.

Method: Proposes hierarchical interaction features (high-level for context, low-level for details) and a coarse-to-fine reasoning module using spatial and frequency perspectives.

Result: Achieves state-of-the-art performance on four public datasets.

Conclusion: The method effectively addresses challenges in complex scenes, enhancing motion prediction accuracy.

Abstract: Complex scenes present significant challenges for predicting human behaviour
due to the abundance of interaction information, such as human-human and
humanenvironment interactions. These factors complicate the analysis and
understanding of human behaviour, thereby increasing the uncertainty in
forecasting human motions. Existing motion prediction methods thus struggle in
these complex scenarios. In this paper, we propose an effective method for
human motion forecasting in interactive scenes. To achieve a comprehensive
representation of interactions, we design a hierarchical interaction feature
representation so that high-level features capture the overall context of the
interactions, while low-level features focus on fine-grained details. Besides,
we propose a coarse-to-fine interaction reasoning module that leverages both
spatial and frequency perspectives to efficiently utilize hierarchical
features, thereby enhancing the accuracy of motion predictions. Our method
achieves state-of-the-art performance across four public datasets. Code will be
released when this paper is published.

</details>


### [274] [CoLa: Chinese Character Decomposition with Compositional Latent Components](https://arxiv.org/pdf/2506.03798)
*Fan Shi, Haiyang Yu, Bin Li, Xiangyang Xue*

Main category: cs.CV

TL;DR: The paper proposes CoLa, a deep latent variable model for Chinese character recognition (CCR) that learns compositional components without human-defined schemes, improving zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Existing CCR methods rely on predefined decomposition schemes, ignoring the learning-to-learn capability, which limits generalization. The paper aims to address this by leveraging cognitive principles of compositionality and learning-to-learn.

Method: The authors introduce CoLa, a deep latent variable model that autonomously learns compositional latent components of Chinese characters, enabling recognition and matching in latent space for zero-shot CCR.

Result: CoLa outperforms previous methods in zero-shot CCR and demonstrates interpretable component learning. It also shows cross-dataset generalization by analyzing oracle bone characters.

Conclusion: CoLa effectively addresses the limitations of human-defined decomposition schemes, offering a scalable and generalizable approach to CCR by learning compositional components autonomously.

Abstract: Humans can decompose Chinese characters into compositional components and
recombine them to recognize unseen characters. This reflects two cognitive
principles: Compositionality, the idea that complex concepts are built on
simpler parts; and Learning-to-learn, the ability to learn strategies for
decomposing and recombining components to form new concepts. These principles
provide inductive biases that support efficient generalization. They are
critical to Chinese character recognition (CCR) in solving the zero-shot
problem, which results from the common long-tail distribution of Chinese
character datasets. Existing methods have made substantial progress in modeling
compositionality via predefined radical or stroke decomposition. However, they
often ignore the learning-to-learn capability, limiting their ability to
generalize beyond human-defined schemes. Inspired by these principles, we
propose a deep latent variable model that learns Compositional Latent
components of Chinese characters (CoLa) without relying on human-defined
decomposition schemes. Recognition and matching can be performed by comparing
compositional latent components in the latent space, enabling zero-shot
character recognition. The experiments illustrate that CoLa outperforms
previous methods in both character the radical zero-shot CCR. Visualization
indicates that the learned components can reflect the structure of characters
in an interpretable way. Moreover, despite being trained on historical
documents, CoLa can analyze components of oracle bone characters, highlighting
its cross-dataset generalization ability.

</details>


### [275] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/pdf/2506.04039)
*Jiulong Wu, Zhengliang Shi, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Lingyong Yan, Min Cao, Min Zhang*

Main category: cs.CV

TL;DR: EMPO improves trustworthiness in LVLMs by enhancing modality alignment and reducing hallucinations using automatically constructed preference data.


<details>
  <summary>Details</summary>
Motivation: Addressing hallucinations in LVLMs caused by modality misalignment and LLM backbone issues.

Method: Proposes Entity-centric Multimodal Preference Optimization (EMPO) and uses open-source data to construct preference datasets.

Result: Reduces hallucination rates by 85.9% on Object-HalBench and 49.8% on MM-HalBench.

Conclusion: EMPO effectively enhances modality alignment and reduces hallucinations in LVLMs.

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [276] [ConText: Driving In-context Learning for Text Removal and Segmentation](https://arxiv.org/pdf/2506.03799)
*Fei Zhang, Pei Zhang, Baosong Yang, Fei Huang, Yanfeng Wang, Ya Zhang*

Main category: cs.CV

TL;DR: The paper introduces ConText, a model adapting visual in-context learning (V-ICL) to OCR tasks like text removal and segmentation, using task-chaining and context-aware aggregation for improved reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing V-ICL methods use simplistic prompts, limiting reasoning. The paper aims to enhance this by introducing intermediate steps and addressing visual heterogeneity.

Method: Proposes a task-chaining compositor (image-removal-segmentation) and context-aware aggregation. Also uses self-prompting to handle visual heterogeneity.

Result: ConText achieves state-of-the-art performance on in- and out-of-domain benchmarks.

Conclusion: The proposed enhancements significantly improve V-ICL for OCR tasks, with ConText setting new benchmarks.

Abstract: This paper presents the first study on adapting the visual in-context
learning (V-ICL) paradigm to optical character recognition tasks, specifically
focusing on text removal and segmentation. Most existing V-ICL generalists
employ a reasoning-as-reconstruction approach: they turn to using a
straightforward image-label compositor as the prompt and query input, and then
masking the query label to generate the desired output. This direct prompt
confines the model to a challenging single-step reasoning process. To address
this, we propose a task-chaining compositor in the form of
image-removal-segmentation, providing an enhanced prompt that elicits reasoning
with enriched intermediates. Additionally, we introduce context-aware
aggregation, integrating the chained prompt pattern into the latent query
representation, thereby strengthening the model's in-context reasoning. We also
consider the issue of visual heterogeneity, which complicates the selection of
homogeneous demonstrations in text recognition. Accordingly, this is
effectively addressed through a simple self-prompting strategy, preventing the
model's in-context learnability from devolving into specialist-like,
context-free inference. Collectively, these insights culminate in our ConText
model, which achieves new state-of-the-art across both in- and out-of-domain
benchmarks. The code is available at https://github.com/Ferenas/ConText.

</details>


### [277] [Animal Pose Labeling Using General-Purpose Point Trackers](https://arxiv.org/pdf/2506.03868)
*Zhuoyang Pan, Boxiao Pan, Guandao Yang, Adam W. Harley, Leonidas Guibas*

Main category: cs.CV

TL;DR: A new animal pose labeling pipeline using test-time optimization achieves state-of-the-art performance with reasonable annotation costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for animal pose estimation lack reliability due to insufficient training datasets, which are hard to collect because of animal morphology variations.

Method: The proposed pipeline fine-tunes a lightweight appearance embedding in a pre-trained point tracker using sparse annotated frames, then applies it to the rest of the video.

Result: The method achieves state-of-the-art performance.

Conclusion: The pipeline is a valuable tool for automatic animal behavior quantification.

Abstract: Automatically estimating animal poses from videos is important for studying
animal behaviors. Existing methods do not perform reliably since they are
trained on datasets that are not comprehensive enough to capture all necessary
animal behaviors. However, it is very challenging to collect such datasets due
to the large variations in animal morphology. In this paper, we propose an
animal pose labeling pipeline that follows a different strategy, i.e. test time
optimization. Given a video, we fine-tune a lightweight appearance embedding
inside a pre-trained general-purpose point tracker on a sparse set of annotated
frames. These annotations can be obtained from human labelers or off-the-shelf
pose detectors. The fine-tuned model is then applied to the rest of the frames
for automatic labeling. Our method achieves state-of-the-art performance at a
reasonable annotation cost. We believe our pipeline offers a valuable tool for
the automatic quantification of animal behavior. Visit our project webpage at
https://zhuoyang-pan.github.io/animal-labeling.

</details>


### [278] [Video, How Do Your Tokens Merge?](https://arxiv.org/pdf/2506.03885)
*Sam Pollard, Michael Wray*

Main category: cs.CV

TL;DR: Token merging in video transformers speeds up processing by 2.5X with minimal accuracy loss (-0.55% for ViViT).


<details>
  <summary>Details</summary>
Motivation: Address the high compute demands of video transformers by exploring token merging without retraining.

Method: Training-free token merging applied to four video transformers on three datasets for action recognition.

Result: Achieved 2.5X speedup while maintaining accuracy (avg. -0.55% for ViViT).

Conclusion: Token merging is effective for video transformers, offering speed improvements without significant accuracy trade-offs.

Abstract: Video transformer models require huge amounts of compute resources due to the
spatio-temporal scaling of the input. Tackling this, recent methods have
proposed to drop or merge tokens for image models, whether randomly or via
learned methods. Merging tokens has many benefits: it can be plugged into any
vision transformer, does not require model re-training, and it propagates
information that would otherwise be dropped through the model. Before now,
video token merging has not been evaluated on temporally complex datasets for
video understanding. In this work, we explore training-free token merging for
video to provide comprehensive experiments and find best practices across four
video transformers on three datasets that exhibit coarse and fine-grained
action recognition. Our results showcase the benefits of video token merging
with a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\%$ for
ViViT). Code available at
https://github.com/sjpollard/video-how-do-your-tokens-merge.

</details>


### [279] [Joint Video Enhancement with Deblurring, Super-Resolution, and Frame Interpolation Network](https://arxiv.org/pdf/2506.03892)
*Giyong Choi, HyunWook Park*

Main category: cs.CV

TL;DR: A joint video enhancement method (DSFN) is proposed to address multiple degradation factors simultaneously, outperforming sequential approaches in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Sequential video enhancement is inefficient and sub-optimal due to ignoring combined degradation factors. A joint approach is needed.

Method: DSFN integrates a joint deblurring and super-resolution (JDSR) module and a triple-frame-based frame interpolation (TFBFI) module to enhance videos.

Result: DSFN outperforms state-of-the-art sequential methods with smaller network size and faster processing.

Conclusion: The joint approach of DSFN is more effective for video enhancement, handling multiple degradations simultaneously.

Abstract: Video quality is often severely degraded by multiple factors rather than a
single factor. These low-quality videos can be restored to high-quality videos
by sequentially performing appropriate video enhancement techniques. However,
the sequential approach was inefficient and sub-optimal because most video
enhancement approaches were designed without taking into account that multiple
factors together degrade video quality. In this paper, we propose a new joint
video enhancement method that mitigates multiple degradation factors
simultaneously by resolving an integrated enhancement problem. Our proposed
network, named DSFN, directly produces a high-resolution, high-frame-rate, and
clear video from a low-resolution, low-frame-rate, and blurry video. In the
DSFN, low-resolution and blurry input frames are enhanced by a joint deblurring
and super-resolution (JDSR) module. Meanwhile, intermediate frames between
input adjacent frames are interpolated by a triple-frame-based frame
interpolation (TFBFI) module. The proper combination of the proposed modules of
DSFN can achieve superior performance on the joint video enhancement task.
Experimental results show that the proposed method outperforms other sequential
state-of-the-art techniques on public datasets with a smaller network size and
faster processing time.

</details>


### [280] [Learning from Noise: Enhancing DNNs for Event-Based Vision through Controlled Noise Injection](https://arxiv.org/pdf/2506.03918)
*Marcin Kowalczyk, Kamil Jeziorek, Tomasz Kryjak*

Main category: cs.CV

TL;DR: A novel noise-injection training method improves neural network robustness against event data noise, outperforming traditional filtering techniques in object classification.


<details>
  <summary>Details</summary>
Motivation: Event-based sensors are advantageous but noisy, degrading deep learning model performance. Traditional filtering removes useful data, prompting a need for noise-resilient training.

Method: Proposes noise-injection training, introducing controlled noise into training data to enhance model robustness. Evaluated on multiple datasets and architectures.

Result: Achieves stable performance across noise levels, outperforms filtering methods, and attains highest classification accuracy.

Conclusion: Noise-injection training is a viable alternative to traditional event-data filtering for object classification.

Abstract: Event-based sensors offer significant advantages over traditional frame-based
cameras, especially in scenarios involving rapid motion or challenging lighting
conditions. However, event data frequently suffers from considerable noise,
negatively impacting the performance and robustness of deep learning models.
Traditionally, this problem has been addressed by applying filtering algorithms
to the event stream, but this may also remove some of relevant data. In this
paper, we propose a novel noise-injection training methodology designed to
enhance the neural networks robustness against varying levels of event noise.
Our approach introduces controlled noise directly into the training data,
enabling models to learn noise-resilient representations. We have conducted
extensive evaluations of the proposed method using multiple benchmark datasets
(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,
including Convolutional Neural Networks, Vision Transformers, Spiking Neural
Networks, and Graph Convolutional Networks. Experimental results show that our
noise-injection training strategy achieves stable performance over a range of
noise intensities, consistently outperforms event-filtering techniques, and
achieves the highest average classification accuracy, making it a viable
alternative to traditional event-data filtering methods in an object
classification system. Code: https://github.com/vision-agh/DVS_Filtering

</details>


### [281] [Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning](https://arxiv.org/pdf/2506.03926)
*Debarshi Brahma, Soma Biswas*

Main category: cs.CV

TL;DR: The paper introduces a practical cross-domain few-shot learning (pCDFSL) task using CLIP, proposing MIST, a framework with stochastic prompts to handle domain shifts.


<details>
  <summary>Details</summary>
Motivation: To address real-world challenges in few-shot learning under extreme domain shifts without episodic training.

Method: MIST uses multiple stochastic prompts modeled as Gaussian distributions to explore the prompt space efficiently.

Result: Outperforms state-of-the-art methods on four CDFSL benchmarks.

Conclusion: MIST is effective for pCDFSL, offering a practical solution for real-world applications.

Abstract: In this work, we propose a practical cross-domain few-shot learning (pCDFSL)
task, where a large-scale pre-trained model like CLIP can be easily deployed on
a target dataset. The goal is to simultaneously classify all unseen classes
under extreme domain shifts, by utilizing only a few labeled samples per class.
The pCDFSL paradigm is source-free and moves beyond artificially created
episodic training and testing regimes followed by existing CDFSL frameworks,
making it more challenging and relevant to real-world applications. Towards
that goal, we propose a novel framework, termed MIST (MultIple STochastic
Prompt tuning), where multiple stochastic prompts are utilized to handle
significant domain and semantic shifts. Specifically, multiple prompts are
learnt for each class, effectively capturing multiple peaks in the input data.
Furthermore, instead of representing the weights of the multiple prompts as
point-estimates, we model them as learnable Gaussian distributions with two
different strategies, encouraging an efficient exploration of the prompt
parameter space, which mitigate overfitting due to the few labeled training
samples. Extensive experiments and comparison with the state-of-the-art methods
on four CDFSL benchmarks adapted to this setting, show the effectiveness of the
proposed framework.

</details>


### [282] [Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with Vision Feature Resample](https://arxiv.org/pdf/2506.03928)
*Ze Feng, Jiang-Jiang Liu, Sen Yang, Lingyu Xiao, Xiaofan Li, Wankou Yang, Jingdong Wang*

Main category: cs.CV

TL;DR: The paper proposes Vision Remember, a method to retain fine-grained visual information in multimodal LLMs by re-memorizing vision features between decoder layers, improving performance without sacrificing efficiency.


<details>
  <summary>Details</summary>
Motivation: Redundant vision tokens consume computational resources, and simple compression in Vision Projectors can lose fine-grained spatial information crucial for tasks like OCR and chart understanding.

Method: Vision Remember inserts between LLM decoder layers to resample multi-level vision features using saliency-enhancing local attention, preserving spatial relationships.

Result: Experiments show performance gains on visual benchmarks, with LLaVA-VR (2B parameters) outperforming larger models like Tokenpacker-HD-7B and DeepSeek-VL-7B.

Conclusion: Vision Remember effectively balances efficiency and fine-grained visual understanding, enhancing multimodal LLM performance.

Abstract: In this work, we study the Efficient Multimodal Large Language Model.
Redundant vision tokens consume a significant amount of computational memory
and resources. Therefore, many previous works compress them in the Vision
Projector to reduce the number of vision tokens. However, simply compressing in
the Vision Projector can lead to the loss of visual information, especially for
tasks that rely on fine-grained spatial relationships, such as OCR and Chart \&
Table Understanding. To address this problem, we propose Vision Remember, which
is inserted between the LLM decoder layers to allow vision tokens to
re-memorize vision features. Specifically, we retain multi-level vision
features and resample them with the vision tokens that have interacted with the
text token. During the resampling process, each vision token only attends to a
local region in vision features, which is referred to as saliency-enhancing
local attention. Saliency-enhancing local attention not only improves
computational efficiency but also captures more fine-grained contextual
information and spatial relationships within the region. Comprehensive
experiments on multiple visual understanding benchmarks validate the
effectiveness of our method when combined with various Efficient Vision
Projectors, showing performance gains without sacrificing efficiency. Based on
Vision Remember, LLaVA-VR with only 2B parameters is also superior to previous
representative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.

</details>


### [283] [Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation](https://arxiv.org/pdf/2506.03942)
*Theodore Barfoot, Luis C. Garcia-Peraza-Herrera, Samet Akcay, Ben Glocker, Tom Vercauteren*

Main category: cs.CV

TL;DR: Proposes mL1-ACE as an auxiliary loss to improve calibration in medical image segmentation, showing reduced errors while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address overconfidence in deep neural networks for medical image segmentation to enhance reliability and clinical utility.

Method: Introduces differentiable mL1-ACE with hard- and soft-binning approaches, tested on four datasets (ACDC, AMOS, KiTS, BraTS).

Result: Soft-binned mL1-ACE improves calibration but may reduce segmentation performance; hard-binned maintains performance with weaker calibration.

Conclusion: The approach enhances trustworthiness and clinical integration potential, with code shared for reproducibility.

Abstract: Deep neural networks for medical image segmentation are often overconfident,
compromising both reliability and clinical utility. In this work, we propose
differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)
as an auxiliary loss that can be computed on a per-image basis. We compare both
hard- and soft-binning approaches to directly improve pixel-wise calibration.
Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that
incorporating mL1-ACE significantly reduces calibration errors, particularly
Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while
largely maintaining high Dice Similarity Coefficients (DSCs). We find that the
soft-binned variant yields the greatest improvements in calibration, over the
Dice plus cross-entropy loss baseline, but often compromises segmentation
performance, with hard-binned mL1-ACE maintaining segmentation performance,
albeit with weaker calibration improvement. To gain further insight into
calibration performance and its variability across an imaging dataset, we
introduce dataset reliability histograms, an aggregation of per-image
reliability diagrams. The resulting analysis highlights improved alignment
between predicted confidences and true accuracies. Overall, our approach not
only enhances the trustworthiness of segmentation predictions but also shows
potential for safer integration of deep learning methods into clinical
workflows. We share our code here:
https://github.com/cai4cai/Average-Calibration-Losses

</details>


### [284] [MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection](https://arxiv.org/pdf/2506.03972)
*Guohua Wu, Shengqi Chen, Pengchao Deng, Wenting Yu*

Main category: cs.CV

TL;DR: MS-YOLO, a blood cell detection model, improves accuracy for overlapping and multi-scale cells using innovative modules, achieving 97.4% mAP@50 and real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual microscopy is inefficient and inaccurate; existing automated methods are costly and lack accuracy, especially for overlapping and multi-scale cells.

Method: MS-YOLO integrates three modules: MS-DRM for multi-scale discriminability, DCFEM for feature fusion, and LADS for efficient downsampling.

Result: Achieves 97.4% mAP@50 on CBC benchmark, excels in detecting small targets like platelets, and generalizes well on WBCDD dataset.

Conclusion: MS-YOLO offers precise, efficient, and deployable blood cell detection, supporting standardized clinical diagnostics.

Abstract: Complete blood cell detection holds significant value in clinical
diagnostics. Conventional manual microscopy methods suffer from time
inefficiency and diagnostic inaccuracies. Existing automated detection
approaches remain constrained by high deployment costs and suboptimal accuracy.
While deep learning has introduced powerful paradigms to this field, persistent
challenges in detecting overlapping cells and multi-scale objects hinder
practical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a
blood cell detection model based on the YOLOv11 framework, incorporating three
key architectural innovations to enhance detection performance. Specifically,
the multi-scale dilated residual module (MS-DRM) replaces the original C3K2
modules to improve multi-scale discriminability; the dynamic cross-path feature
enhancement module (DCFEM) enables the fusion of hierarchical features from the
backbone with aggregated features from the neck to enhance feature
representations; and the light adaptive-weight downsampling module (LADS)
improves feature downsampling through adaptive spatial weighting while reducing
computational complexity. Experimental results on the CBC benchmark demonstrate
that MS-YOLO achieves precise detection of overlapping cells and multi-scale
objects, particularly small targets such as platelets, achieving an mAP@50 of
97.4% that outperforms existing models. Further validation on the supplementary
WBCDD dataset confirms its robust generalization capability. Additionally, with
a lightweight architecture and real-time inference efficiency, MS-YOLO meets
clinical deployment requirements, providing reliable technical support for
standardized blood pathology assessment.

</details>


### [285] [RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors](https://arxiv.org/pdf/2506.03988)
*Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio*

Main category: cs.CV

TL;DR: RAID introduces a dataset and method to evaluate the adversarial robustness of AI-generated image detectors, revealing their vulnerability to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated images poses risks of fraud and disinformation, but existing detectors lack robust evaluation under adversarial conditions.

Method: RAID provides a dataset of 72k adversarial examples created by attacking seven detectors and four text-to-image models, enabling simpler robustness assessment.

Result: Current detectors are easily deceived by adversarial examples, demonstrating their lack of robustness.

Conclusion: The study underscores the need for more robust AI-generated image detectors and releases RAID dataset and code for further research.

Abstract: AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustnessOur findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.

</details>


### [286] [Vocabulary-free few-shot learning for Vision-Language Models](https://arxiv.org/pdf/2506.04005)
*Maxime Zanella, Clément Fuchs, Ismail Ben Ayed, Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: The paper introduces vocabulary-free few-shot learning for Vision-Language Models (VLMs) using Similarity Mapping (SiM), a method that classifies images without predefined class names by leveraging similarity scores with generic prompts.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot adaptation methods for VLMs rely on predefined class names, limiting their applicability when exact class names are unavailable.

Method: Proposes SiM, a method that classifies target instances using similarity scores with generic prompts, eliminating the need for task-specific prompts.

Result: SiM shows strong performance, high computational efficiency (learning in <1s), and interpretability by linking classes to generic prompts.

Conclusion: SiM serves as a promising baseline for vocabulary-free few-shot learning in VLMs.

Abstract: Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have
greatly expanded their ability to generalize across tasks using only a few
labeled examples. However, existing approaches primarily build upon the strong
zero-shot priors of these models by leveraging carefully designed,
task-specific prompts. This dependence on predefined class names can restrict
their applicability, especially in scenarios where exact class names are
unavailable or difficult to specify. To address this limitation, we introduce
vocabulary-free few-shot learning for VLMs, a setting where target class
instances - that is, images - are available but their corresponding names are
not. We propose Similarity Mapping (SiM), a simple yet effective baseline that
classifies target instances solely based on similarity scores with a set of
generic prompts (textual or visual), eliminating the need for carefully
handcrafted prompts. Although conceptually straightforward, SiM demonstrates
strong performance, operates with high computational efficiency (learning the
mapping typically takes less than one second), and provides interpretability by
linking target classes to generic prompts. We believe that our approach could
serve as an important baseline for future research in vocabulary-free few-shot
learning. Code is available at
https://github.com/MaxZanella/vocabulary-free-FSL.

</details>


### [287] [Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning](https://arxiv.org/pdf/2506.04034)
*Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Lei Zhang*

Main category: cs.CV

TL;DR: Rex-Thinker is a model for object referring that uses explicit Chain-of-Thought (CoT) reasoning to improve interpretability and trustworthiness, outperforming baselines in precision and generalization.


<details>
  <summary>Details</summary>
Motivation: Current object referring models lack interpretability and struggle to reject expressions with no matching objects, necessitating a grounded approach.

Method: Rex-Thinker formulates object referring as a CoT reasoning task, identifying candidates and assessing matches step-by-step. A dataset (HumanRef-CoT) is created for training, followed by supervised fine-tuning and GRPO-based RL learning.

Result: The model outperforms baselines in precision and interpretability, with improved rejection of hallucinated outputs and strong generalization.

Conclusion: Rex-Thinker demonstrates the effectiveness of explicit reasoning for grounded object referring, offering both verifiable and trustworthy predictions.

Abstract: Object referring aims to detect all objects in an image that match a given
natural language description. We argue that a robust object referring model
should be grounded, meaning its predictions should be both explainable and
faithful to the visual content. Specifically, it should satisfy two key
properties: 1) Verifiable, by producing interpretable reasoning that justifies
its predictions and clearly links them to visual evidence; and 2) Trustworthy,
by learning to abstain when no object in the image satisfies the given
expression. However, most methods treat referring as a direct bounding box
prediction task, offering limited interpretability and struggling to reject
expressions with no matching object. In this work, we propose Rex-Thinker, a
model that formulates object referring as an explicit CoT reasoning task. Given
a referring expression, we first identify all candidate object instances
corresponding to the referred object category. Rex-Thinker then performs
step-by-step reasoning over each candidate to assess whether it matches the
given expression, before making a final prediction. To support this paradigm,
we construct a large-scale CoT-style referring dataset named HumanRef-CoT by
prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a
structured planning, action, and summarization format, enabling the model to
learn decomposed, interpretable reasoning over object candidates. We then train
Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach
the model how to perform structured reasoning, followed by GRPO-based RL
learning to improve accuracy and generalization. Experiments show that our
approach outperforms standard baselines in both precision and interpretability
on in-domain evaluation, while also demonstrating improved ability to reject
hallucinated outputs and strong generalization in out-of-domain settings.

</details>


### [288] [Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology](https://arxiv.org/pdf/2506.04143)
*Ngoc Q. Ly, Hieu N. M. Cao, Thi T. Nguyen*

Main category: cs.CV

TL;DR: The paper proposes a Unified Re-ID system with three modules (PAO, Local MDCNN, IDS) to address challenges like imbalanced data and semantic feature use, achieving better performance on Market1501.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Person Re-ID like imbalanced data, semantic feature utilization, and viewpoint issues.

Method: Proposes a system with Pedestrian Attribute Ontology (PAO), Local Multi-task DCNN (Local MDCNN), and Imbalance Data Solver (IDS) to leverage attribute correlations and filter mismatches.

Result: Achieves higher performance on Market1501 compared to state-of-the-art methods.

Conclusion: The Unified Re-ID system effectively addresses key challenges and improves performance.

Abstract: Person Re-Identification (Re-ID) is a very important task in video
surveillance systems such as tracking people, finding people in public places,
or analysing customer behavior in supermarkets. Although there have been many
works to solve this problem, there are still remaining challenges such as
large-scale datasets, imbalanced data, viewpoint, fine grained data
(attributes), the Local Features are not employed at semantic level in online
stage of Re-ID task, furthermore, the imbalanced data problem of attributes are
not taken into consideration. This paper has proposed a Unified Re-ID system
consisted of three main modules such as Pedestrian Attribute Ontology (PAO),
Local Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main
point of our Re-ID system is the power of mutual support of PAO, Local MDCNN
and IDS to exploit the inner-group correlations of attributes and pre-filter
the mismatch candidates from Gallery set based on semantic information as
Fashion Attributes and Facial Attributes, to solve the imbalanced data of
attributes without adjusting network architecture and data augmentation. We
experimented on the well-known Market1501 dataset. The experimental results
have shown the effectiveness of our Re-ID system and it could achieve the
higher performance on Market1501 dataset in comparison to some state-of-the-art
Re-ID methods.

</details>


### [289] [EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects](https://arxiv.org/pdf/2506.04048)
*Gabriele Magrini, Federico Becattini, Giovanni Colombo, Pietro Pala*

Main category: cs.CV

TL;DR: The paper introduces EV-Flying, an event-based dataset for detecting and recognizing flying objects, addressing challenges like scale variations and motion blur with event cameras and a PointNet-inspired method.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-based methods struggle with challenges like scale variations, motion blur, and unpredictable movement patterns of small flying objects (e.g., insects, drones). Event cameras offer high temporal resolution and robustness to motion blur, making them suitable for this task.

Method: The authors introduce EV-Flying, a dataset of annotated flying objects (birds, insects, drones) with spatio-temporal bounding boxes. They use a point-based approach inspired by PointNet to process asynchronous event streams for classification.

Result: The study demonstrates the effectiveness of event-based vision and lightweight architectures for classifying flying objects, providing a foundation for real-world aerial object recognition.

Conclusion: The EV-Flying dataset and proposed methodology enable more efficient and reliable recognition of flying objects, addressing limitations of traditional RGB-based approaches.

Abstract: Monitoring aerial objects is crucial for security, wildlife conservation, and
environmental studies. Traditional RGB-based approaches struggle with
challenges such as scale variations, motion blur, and high-speed object
movements, especially for small flying entities like insects and drones. In
this work, we explore the potential of event-based vision for detecting and
recognizing flying objects, in particular animals that may not follow short and
long-term predictable patters. Event cameras offer high temporal resolution,
low latency, and robustness to motion blur, making them well-suited for this
task. We introduce EV-Flying, an event-based dataset of flying objects,
comprising manually annotated birds, insects and drones with spatio-temporal
bounding boxes and track identities. To effectively process the asynchronous
event streams, we employ a point-based approach leveraging lightweight
architectures inspired by PointNet. Our study investigates the classification
of flying objects using point cloud-based event representations. The proposed
dataset and methodology pave the way for more efficient and reliable aerial
object recognition in real-world scenarios.

</details>


### [290] [Video Deblurring with Deconvolution and Aggregation Networks](https://arxiv.org/pdf/2506.04054)
*Giyong Choi, HyunWook Park*

Main category: cs.CV

TL;DR: Proposes a Deconvolution and Aggregation Network (DAN) for video deblurring, leveraging neighbor frames effectively through three sub-networks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video deblurring methods underutilize neighbor frames, leading to sub-optimal performance.

Method: DAN combines three sub-networks: Preprocessing Network (PPN), Alignment-Based Deconvolution Network (ABDN), and Frame Aggregation Network (FAN) to deblur and aggregate frames.

Result: DAN achieves superior performance over state-of-the-art methods in quantitative and qualitative evaluations.

Conclusion: DAN effectively utilizes neighbor frames for video deblurring, demonstrating significant improvements over existing approaches.

Abstract: In contrast to single-image deblurring, video deblurring has the advantage
that neighbor frames can be utilized to deblur a target frame. However,
existing video deblurring algorithms often fail to properly employ the neighbor
frames, resulting in sub-optimal performance. In this paper, we propose a
deconvolution and aggregation network (DAN) for video deblurring that utilizes
the information of neighbor frames well. In DAN, both deconvolution and
aggregation strategies are achieved through three sub-networks: the
preprocessing network (PPN) and the alignment-based deconvolution network
(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for
the aggregation scheme. In the deconvolution part, blurry inputs are first
preprocessed by the PPN with non-local operations. Then, the output frames from
the PPN are deblurred by the ABDN based on the frame alignment. In the FAN,
these deblurred frames from the deconvolution part are combined into a latent
frame according to reliability maps which infer pixel-wise sharpness. The
proper combination of three sub-networks can achieve favorable performance on
video deblurring by using the neighbor frames suitably. In experiments, the
proposed DAN was demonstrated to be superior to existing state-of-the-art
methods through both quantitative and qualitative evaluations on the public
datasets.

</details>


### [291] [Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network](https://arxiv.org/pdf/2506.04081)
*Abdelouahed Laazoufi, Mohammed El Hassouni, Hocine Cherifi*

Main category: cs.CV

TL;DR: NR-PCQA is essential for assessing 3D content without reference models.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate 3D content in real-world scenarios where reference models are unavailable.

Method: No-Reference Point Cloud Quality Assessment (NR-PCQA).

Result: Not explicitly stated in the abstract.

Conclusion: NR-PCQA is crucial for practical 3D content evaluation.

Abstract: No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for
evaluating 3D content in real-world applications where reference models are
unavailable.

</details>


### [292] [GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models](https://arxiv.org/pdf/2506.04106)
*Xiao Xiang Zhu, Sining Chen, Fahong Zhang, Yilei Shi, Yuanyuan Wang*

Main category: cs.CV

TL;DR: GlobalBuildingAtlas is the first open dataset offering high-quality, consistent, and complete 2D and 3D building data globally, with over 2.75 billion buildings, surpassing existing databases.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, high-resolution global dataset of building polygons, heights, and 3D models for geospatial analysis and sustainable development monitoring.

Method: Developed machine learning pipelines to derive building polygons and heights from satellite data, and used a quality-based fusion strategy to enhance polygon quality.

Result: Produced GBA.Polygon (2.75B buildings), GBA.Height (3x3m resolution), and GBA.LoD1 (2.68B buildings with 97% height completeness and RMSEs of 1.5-8.9m).

Conclusion: GlobalBuildingAtlas enables unprecedented geospatial analysis, supporting applications like population mapping and monitoring UN Sustainable Development Goals.

Abstract: We introduce GlobalBuildingAtlas, a publicly available dataset providing
global and complete coverage of building polygons, heights and Level of Detail
1 (LoD1) 3D building models. This is the first open dataset to offer high
quality, consistent, and complete building data in 2D and 3D form at the
individual building level on a global scale. Towards this dataset, we developed
machine learning-based pipelines to derive building polygons and heights
(called GBA.Height) from global PlanetScope satellite data, respectively. Also
a quality-based fusion strategy was employed to generate higher-quality
polygons (called GBA.Polygon) based on existing open building polygons,
including our own derived one. With more than 2.75 billion buildings worldwide,
GBA.Polygon surpasses the most comprehensive database to date by more than 1
billion buildings. GBA.Height offers the most detailed and accurate global 3D
building height maps to date, achieving a spatial resolution of 3x3 meters-30
times finer than previous global products (90 m), enabling a high-resolution
and reliable analysis of building volumes at both local and global scales.
Finally, we generated a global LoD1 building model (called GBA.LoD1) from the
resulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete
global LoD1 building models, including 2.68 billion building instances with
predicted heights, i.e., with a height completeness of more than 97%, achieving
RMSEs ranging from 1.5 m to 8.9 m across different continents. With its height
accuracy, comprehensive global coverage and rich spatial details,
GlobalBuildingAltas offers novel insights on the status quo of global
buildings, which unlocks unprecedented geospatial analysis possibilities, as
showcased by a better illustration of where people live and a more
comprehensive monitoring of the progress on the 11th Sustainable Development
Goal of the United Nations.

</details>


### [293] [Multi-view Surface Reconstruction Using Normal and Reflectance Cues](https://arxiv.org/pdf/2506.04115)
*Robin Bruneau, Baptiste Brument, Yvain Quéau, Jean Mélou, François Bernard Lauze, Jean-Denis Durou, Lilian Calvet*

Main category: cs.CV

TL;DR: A framework for high-fidelity 3D surface reconstruction using multi-view normal and reflectance maps, achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges in preserving fine details in 3D reconstruction, especially with complex reflectance properties and sparse views.

Method: Pixel-wise joint re-parametrization of reflectance and surface normals as radiances under varying illumination, compatible with MVS and NVR pipelines.

Result: State-of-the-art performance on MVPS benchmarks (DiLiGenT-MV, LUCES-MV, Skoltech3D), excelling in fine detail reconstruction and visibility handling.

Conclusion: The method improves upon prior work with a more robust algorithm and broader evaluation, offering versatile and high-quality 3D reconstruction.

Abstract: Achieving high-fidelity 3D surface reconstruction while preserving fine
details remains challenging, especially in the presence of materials with
complex reflectance properties and without a dense-view setup. In this paper,
we introduce a versatile framework that incorporates multi-view normal and
optionally reflectance maps into radiance-based surface reconstruction. Our
approach employs a pixel-wise joint re-parametrization of reflectance and
surface normals, representing them as a vector of radiances under simulated,
varying illumination. This formulation enables seamless incorporation into
standard surface reconstruction pipelines, such as traditional multi-view
stereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined
with the latter, our approach achieves state-of-the-art performance on
multi-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,
LUCES-MV and Skoltech3D. In particular, our method excels in reconstructing
fine-grained details and handling challenging visibility conditions. The
present paper is an extended version of the earlier conference paper by Brument
et al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2024), featuring an accelerated and more robust
algorithm as well as a broader empirical evaluation. The code and data relative
to this article is available at https://github.com/RobinBruneau/RNb-NeuS2.

</details>


### [294] [Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking](https://arxiv.org/pdf/2506.04122)
*Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada*

Main category: cs.CV

TL;DR: Contour Errors (CEs) improve match reliability in 3D tracking, reducing functional failures by 80% at close ranges and 60% at far ranges compared to traditional metrics like IoU.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D metrics like IoU and CPD fail in complex 3D scenes, necessitating a more functionally relevant metric for accurate object tracking in safety-critical applications.

Method: Introduces Contour Errors (CEs), an ego-centric metric comparing bounding boxes in the ego vehicle's frame for better match identification.

Result: CEs outperform IoU and CPD, reducing functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges in 3D car tracking.

Conclusion: CEs provide a more reliable and functionally relevant metric for 3D multi-object tracking, enhancing performance and safety in applications like autonomous vehicles.

Abstract: Finding reliable matches is essential in multi-object tracking to ensure the
accuracy and reliability of perception systems in safety-critical applications
such as autonomous vehicles. Effective matching mitigates perception errors,
enhancing object identification and tracking for improved performance and
safety. However, traditional metrics such as Intersection over Union (IoU) and
Center Point Distances (CPDs), which are effective in 2D image planes, often
fail to find critical matches in complex 3D scenes. To address this limitation,
we introduce Contour Errors (CEs), an ego or object-centric metric for
identifying matches of interest in tracking scenarios from a functional
perspective. By comparing bounding boxes in the ego vehicle's frame, contour
errors provide a more functionally relevant assessment of object matches.
Extensive experiments on the nuScenes dataset demonstrate that contour errors
improve the reliability of matches over the state-of-the-art 2D IoU and CPD
metrics in tracking-by-detection methods. In 3D car tracking, our results show
that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges
and 60% at far ranges compared to IoU in the evaluation stage.

</details>


### [295] [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/pdf/2506.04141)
*Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao*

Main category: cs.CV

TL;DR: The paper introduces MMR-V, a benchmark for multimodal deep reasoning in videos, addressing gaps in existing benchmarks by requiring long-range, multi-frame reasoning and hidden information analysis. Current models struggle, achieving only 52.5% accuracy, and reasoning enhancements show limited gains.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks focus on understanding tasks, lacking challenges for multimodal reasoning and long-range evidence location in videos.

Method: Proposes MMR-V, featuring long-range reasoning, hidden information analysis, manual annotation for reliability, and distractor strategies to reduce shortcuts.

Result: Current models perform poorly (52.5% accuracy), and reasoning enhancements like Chain-of-Thought yield limited improvements.

Conclusion: MMR-V highlights the need for better multimodal reasoning capabilities and inspires further research.

Abstract: The sequential structure of videos poses a challenge to the ability of
multimodal large language models (MLLMs) to locate multi-frame evidence and
conduct multimodal reasoning. However, existing video benchmarks mainly focus
on understanding tasks, which only require models to match frames mentioned in
the question (hereafter referred to as "question frame") and perceive a few
adjacent frames. To address this gap, we propose MMR-V: A Benchmark for
Multimodal Deep Reasoning in Videos. The benchmark is characterized by the
following features. (1) Long-range, multi-frame reasoning: Models are required
to infer and analyze evidence frames that may be far from the question frame.
(2) Beyond perception: Questions cannot be answered through direct perception
alone but require reasoning over hidden information. (3) Reliability: All tasks
are manually annotated, referencing extensive real-world user understanding to
align with common perceptions. (4) Confusability: Carefully designed distractor
annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos
and 1,257 tasks. Our experiments reveal that current models still struggle with
multi-modal reasoning; even the best-performing model, o4-mini, achieves only
52.5% accuracy. Additionally, current reasoning enhancement strategies
(Chain-of-Thought and scaling test-time compute) bring limited gains. Further
analysis indicates that the CoT demanded for multi-modal reasoning differs from
it in textual reasoning, which partly explains the limited performance gains.
We hope that MMR-V can inspire further research into enhancing multi-modal
reasoning capabilities.

</details>


### [296] [Image Editing As Programs with Diffusion Models](https://arxiv.org/pdf/2506.04158)
*Yujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, Xinchao Wang*

Main category: cs.CV

TL;DR: IEAP introduces a modular framework for instruction-driven image editing using Diffusion Transformers, outperforming existing methods in complex edits.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with structurally inconsistent edits involving layout changes, prompting the need for a robust solution.

Method: IEAP decomposes editing instructions into atomic operations via lightweight adapters on a shared DiT backbone, programmed by a VLM-based agent.

Result: IEAP outperforms state-of-the-art methods in accuracy and semantic fidelity, especially for complex, multi-step instructions.

Conclusion: IEAP provides a scalable and effective solution for diverse image editing tasks, demonstrated by superior benchmark performance.

Abstract: While diffusion models have achieved remarkable success in text-to-image
generation, they encounter significant challenges with instruction-driven image
editing. Our research highlights a key challenge: these models particularly
struggle with structurally inconsistent edits that involve substantial layout
changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a
unified image editing framework built upon the Diffusion Transformer (DiT)
architecture. At its core, IEAP approaches instructional editing through a
reductionist lens, decomposing complex editing instructions into sequences of
atomic operations. Each operation is implemented via a lightweight adapter
sharing the same DiT backbone and is specialized for a specific type of edit.
Programmed by a vision-language model (VLM)-based agent, these operations
collaboratively support arbitrary and structurally inconsistent
transformations. By modularizing and sequencing edits in this way, IEAP
generalizes robustly across a wide range of editing tasks, from simple
adjustments to substantial structural changes. Extensive experiments
demonstrate that IEAP significantly outperforms state-of-the-art methods on
standard benchmarks across various editing scenarios. In these evaluations, our
framework delivers superior accuracy and semantic fidelity, particularly for
complex, multi-step instructions. Codes are available at
https://github.com/YujiaHu1109/IEAP.

</details>


### [297] [FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting](https://arxiv.org/pdf/2506.04174)
*Hengyu Liu, Yuehao Wang, Chenxin Li, Ruisi Cai, Kevin Wang, Wuyang Li, Pavlo Molchanov, Peihao Wang, Zhangyang Wang*

Main category: cs.CV

TL;DR: A method for elastic inference in 3D Gaussian splatting (3DGS) reduces GPU memory usage without fine-tuning, adapting to device-specific needs.


<details>
  <summary>Details</summary>
Motivation: 3DGS requires high GPU memory, limiting its use on resource-constrained devices. Existing compression methods lack adaptability and require fine-tuning.

Method: Proposes an elastic inference method with a learnable module for Gaussian selection and a transformation module to adjust selected Gaussians.

Result: Achieves efficient rendering performance without fine-tuning, validated on datasets like ZipNeRF and Tanks&Temples.

Conclusion: The approach effectively reduces memory usage while maintaining rendering quality, offering adaptability for various devices.

Abstract: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

</details>


### [298] [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/pdf/2506.04209)
*Jingfeng Yang, Ziyang Wu, Yue Zhao, Yi Ma*

Main category: cs.CV

TL;DR: LIFT proposes using a fixed pre-trained LLM as a text encoder to guide visual representation learning, eliminating costly joint training. It outperforms CLIP in many scenarios and improves computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper questions the necessity of costly joint training for language-image alignment and explores if a fixed LLM can serve as an effective text encoder.

Method: LIFT learns language-image alignment by training only the image encoder while using a fixed pre-trained LLM for text encoding.

Result: LIFT outperforms CLIP in compositional understanding and long captions, with significant computational efficiency gains.

Conclusion: The work suggests an alternative design for language-aligned visual representations and highlights the potential of LLM text embeddings in guiding visual learning.

Abstract: Currently, the most dominant approach to establishing language-image
alignment is to pre-train text and image encoders jointly through contrastive
learning, such as CLIP and its variants. In this work, we question whether such
a costly joint training is necessary. In particular, we investigate if a
pre-trained fixed large language model (LLM) offers a good enough text encoder
to guide visual representation learning. That is, we propose to learn
Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by
training only the image encoder. Somewhat surprisingly, through comprehensive
benchmarking and ablation studies, we find that this much simplified framework
LIFT is highly effective and it outperforms CLIP in most scenarios that involve
compositional understanding and long captions, while achieving considerable
gains in computational efficiency. Our work takes a first step towards
systematically exploring how text embeddings from LLMs can guide visual
learning and suggests an alternative design choice for learning
language-aligned visual representations.

</details>


### [299] [Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector](https://arxiv.org/pdf/2506.04211)
*Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu*

Main category: cs.CV

TL;DR: A method called Diffusion Domain Teacher (DDT) uses a frozen-weight diffusion model to improve cross-domain object detection by generating pseudo labels for unlabeled target domains, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Object detectors often underperform due to domain gaps between training and real-world data. Diffusion models' ability to generate diverse images suggests potential for cross-domain feature extraction.

Method: Train a detector with a frozen-weight diffusion model on the source domain, use it as a teacher to generate pseudo labels for the target domain, and guide a student model's supervised learning.

Result: DDT improves cross-domain detection by 21.2% mAP on average over the baseline and outperforms SOTA methods by 5.7% mAP. It also works well with more complex models.

Conclusion: DDT is a simple yet effective framework for domain adaptation in object detection, demonstrating broad applicability and significant performance improvements.

Abstract: Object detectors often suffer a decrease in performance due to the large
domain gap between the training data (source domain) and real-world data
(target domain). Diffusion-based generative models have shown remarkable
abilities in generating high-quality and diverse images, suggesting their
potential for extracting valuable feature from various domains. To effectively
leverage the cross-domain feature representation of diffusion models, in this
paper, we train a detector with frozen-weight diffusion model on the source
domain, then employ it as a teacher model to generate pseudo labels on the
unlabeled target domain, which are used to guide the supervised learning of the
student model on the target domain. We refer to this approach as Diffusion
Domain Teacher (DDT). By employing this straightforward yet potent framework,
we significantly improve cross-domain object detection performance without
compromising the inference speed. Our method achieves an average mAP
improvement of 21.2% compared to the baseline on 6 datasets from three common
cross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},
surpassing the current state-of-the-art (SOTA) methods by an average of 5.7%
mAP. Furthermore, extensive experiments demonstrate that our method
consistently brings improvements even in more powerful and complex models,
highlighting broadly applicable and effective domain adaptation capability of
our DDT. The code is available at
https://github.com/heboyong/Diffusion-Domain-Teacher.

</details>


### [300] [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](https://arxiv.org/pdf/2506.04213)
*Xuanhua He, Quande Liu, Zixuan Ye, Wecai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, Kun Gai*

Main category: cs.CV

TL;DR: FullDiT2 improves efficiency in video generation by reducing computational redundancy in in-context conditioning, achieving faster performance with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computation overhead in existing in-context conditioning methods for video generation, hindering practical deployment.

Method: Proposes FullDiT2 with dynamic token selection and selective context caching to reduce redundancy in context tokens and interactions.

Result: Achieves 2-3x speedup per diffusion step with minimal performance degradation or improvement in video quality.

Conclusion: FullDiT2 offers an efficient solution for general controllability in video generation and editing tasks.

Abstract: Fine-grained and efficient controllability on video diffusion transformers
has raised increasing desires for the applicability. Recently, In-context
Conditioning emerged as a powerful paradigm for unified conditional video
generation, which enables diverse controls by concatenating varying context
conditioning signals with noisy video latents into a long unified token
sequence and jointly processing them via full-attention, e.g., FullDiT. Despite
their effectiveness, these methods face quadratic computation overhead as task
complexity increases, hindering practical deployment. In this paper, we study
the efficiency bottleneck neglected in original in-context conditioning video
generation framework. We begin with systematic analysis to identify two key
sources of the computation inefficiencies: the inherent redundancy within
context condition tokens and the computational redundancy in context-latent
interactions throughout the diffusion process. Based on these insights, we
propose FullDiT2, an efficient in-context conditioning framework for general
controllability in both video generation and editing tasks, which innovates
from two key perspectives. Firstly, to address the token redundancy, FullDiT2
leverages a dynamic token selection mechanism to adaptively identify important
context tokens, reducing the sequence length for unified full-attention.
Additionally, a selective context caching mechanism is devised to minimize
redundant interactions between condition tokens and video latents. Extensive
experiments on six diverse conditional video editing and generation tasks
demonstrate that FullDiT2 achieves significant computation reduction and 2-3
times speedup in averaged time cost per diffusion step, with minimal
degradation or even higher performance in video generation quality. The project
page is at \href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.

</details>


### [301] [UNIC: Unified In-Context Video Editing](https://arxiv.org/pdf/2506.04216)
*Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, Wenhan Luo*

Main category: cs.CV

TL;DR: UNIC is a unified framework for diverse video editing tasks using a single model with in-context tokens, eliminating task-specific designs.


<details>
  <summary>Details</summary>
Motivation: To address limitations of task-specific architectures and enable versatile, unified video editing.

Method: Represents inputs as three token types (source video, noisy latent, multi-modal conditioning) and models them jointly using DiT attention, with task-aware RoPE and condition bias to avoid token collisions.

Result: Superior performance on six video editing tasks and emergent task composition abilities.

Conclusion: UNIC successfully unifies diverse video editing tasks in a single model, demonstrating adaptability and performance.

Abstract: Recent advances in text-to-video generation have sparked interest in
generative video editing tasks. Previous methods often rely on task-specific
architectures (e.g., additional adapter modules) or dedicated customizations
(e.g., DDIM inversion), which limit the integration of versatile editing
conditions and the unification of various editing tasks. In this paper, we
introduce UNified In-Context Video Editing (UNIC), a simple yet effective
framework that unifies diverse video editing tasks within a single model in an
in-context manner. To achieve this unification, we represent the inputs of
various video editing tasks as three types of tokens: the source video tokens,
the noisy video latent, and the multi-modal conditioning tokens that vary
according to the specific editing task. Based on this formulation, our key
insight is to integrate these three types into a single consecutive token
sequence and jointly model them using the native attention operations of DiT,
thereby eliminating the need for task-specific adapter designs. Nevertheless,
direct task unification under this framework is challenging, leading to severe
token collisions and task confusion due to the varying video lengths and
diverse condition modalities across tasks. To address these, we introduce
task-aware RoPE to facilitate consistent temporal positional encoding, and
condition bias that enables the model to clearly differentiate different
editing tasks. This allows our approach to adaptively perform different video
editing tasks by referring the source video and varying condition tokens "in
context", and support flexible task composition. To validate our method, we
construct a unified video editing benchmark containing six representative video
editing tasks. Results demonstrate that our unified approach achieves superior
performance on each task and exhibits emergent task composition abilities.

</details>


### [302] [Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models](https://arxiv.org/pdf/2506.04220)
*Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang*

Main category: cs.CV

TL;DR: The paper introduces Struct2D, a framework using structured 2D inputs to enhance spatial reasoning in LMMs, achieving strong performance without explicit 3D inputs.


<details>
  <summary>Details</summary>
Motivation: To explore if LMMs can reason about 3D space using only structured 2D representations, avoiding reliance on explicit 3D inputs or specialized architectures.

Method: Proposes Struct2D, a perception-guided prompting framework combining BEV images, object marks, and metadata, and creates Struct2D-Set, a 200K QA dataset for fine-tuning.

Result: LMMs show strong spatial reasoning with Struct2D, and fine-tuned models achieve competitive performance on benchmarks like 3D QA and object grounding.

Conclusion: Structured 2D inputs effectively bridge perception and language reasoning in LMMs, eliminating the need for explicit 3D representations.

Abstract: Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for
enabling intelligent interaction with 3D environments. While prior efforts
often rely on explicit 3D inputs or specialized model architectures, we ask:
can LMMs reason about 3D space using only structured 2D representations derived
from perception? We introduce Struct2D, a perception-guided prompting framework
that combines bird's-eye-view (BEV) images with object marks and object-centric
metadata, optionally incorporating egocentric keyframes when needed. Using
Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs
(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning
abilities when provided with structured 2D inputs, effectively handling tasks
such as relative direction estimation and route planning. Building on these
insights, we construct Struct2D-Set, a large-scale instruction tuning dataset
with 200K fine-grained QA pairs across eight spatial reasoning categories,
generated automatically from 3D indoor scenes. We fine-tune an open-source LMM
(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple
benchmarks, including 3D question answering, dense captioning, and object
grounding. Our approach demonstrates that structured 2D inputs can effectively
bridge perception and language reasoning in LMMs-without requiring explicit 3D
representations as input. We will release both our code and dataset to support
future research.

</details>


### [303] [Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset](https://arxiv.org/pdf/2506.04224)
*Zirui Wang, Wenjing Bian, Xinghui Li, Yifu Tao, Jianeng Wang, Maurice Fallon, Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: Oxford Day-and-Night is a large-scale egocentric dataset for novel view synthesis and visual relocalisation, addressing gaps in existing datasets with ground-truth 3D geometry, lighting variation, and full 6DoF motion.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack combinations of features like 3D geometry, lighting variation, and full motion. This dataset fills these gaps to support egocentric 3D vision research.

Method: Uses Meta ARIA glasses for egocentric video capture and multi-session SLAM for camera pose estimation, 3D point cloud reconstruction, and alignment under varying lighting.

Result: The dataset spans 30 km of trajectories and 40,000 m², supporting benchmarks for NVS and relocalisation in diverse environments.

Conclusion: Oxford Day-and-Night provides a unique, realistic platform for evaluating models in challenging lighting and motion conditions.

Abstract: We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for
novel view synthesis (NVS) and visual relocalisation under challenging lighting
conditions. Existing datasets often lack crucial combinations of features such
as ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF
motion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA
glasses to capture egocentric video and applying multi-session SLAM to estimate
camera poses, reconstruct 3D point clouds, and align sequences captured under
varying lighting conditions, including both day and night. The dataset spans
over 30 $\mathrm{km}$ of recorded trajectories and covers an area of 40,000
$\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.
It supports two core benchmarks, NVS and relocalisation, providing a unique
platform for evaluating models in realistic and diverse environments.

</details>


### [304] [Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation](https://arxiv.org/pdf/2506.04225)
*Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson W. H. Lau, Wangmeng Zuo, Chunchao Guo*

Main category: cs.CV

TL;DR: Voyager is a video diffusion framework that generates 3D-consistent point-cloud sequences from a single image and user-defined camera paths, eliminating the need for traditional 3D reconstruction pipelines.


<details>
  <summary>Details</summary>
Motivation: The challenge of creating long-range, 3D-consistent explorable scenes from single images or text, which existing methods struggle with.

Method: Voyager integrates world-consistent video diffusion, long-range world exploration with a world cache, and a scalable data engine for automated training data curation.

Result: Improved visual quality and geometric accuracy over existing methods, with versatile applications.

Conclusion: Voyager offers a novel, efficient solution for generating explorable 3D scenes with inherent consistency, advancing the field beyond traditional pipelines.

Abstract: Real-world applications like video gaming and virtual reality often demand
the ability to model 3D scenes that users can explore along custom camera
trajectories. While significant progress has been made in generating 3D objects
from text or images, creating long-range, 3D-consistent, explorable 3D scenes
remains a complex and challenging problem. In this work, we present Voyager, a
novel video diffusion framework that generates world-consistent 3D point-cloud
sequences from a single image with user-defined camera path. Unlike existing
approaches, Voyager achieves end-to-end scene generation and reconstruction
with inherent consistency across frames, eliminating the need for 3D
reconstruction pipelines (e.g., structure-from-motion or multi-view stereo).
Our method integrates three key components: 1) World-Consistent Video
Diffusion: A unified architecture that jointly generates aligned RGB and depth
video sequences, conditioned on existing world observation to ensure global
coherence 2) Long-Range World Exploration: An efficient world cache with point
culling and an auto-regressive inference with smooth video sampling for
iterative scene extension with context-aware consistency, and 3) Scalable Data
Engine: A video reconstruction pipeline that automates camera pose estimation
and metric depth prediction for arbitrary videos, enabling large-scale, diverse
training data curation without manual 3D annotations. Collectively, these
designs result in a clear improvement over existing methods in visual quality
and geometric accuracy, with versatile applications.

</details>


### [305] [LayerFlow: A Unified Model for Layer-aware Video Generation](https://arxiv.org/pdf/2506.04228)
*Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, Hengshuang Zhao*

Main category: cs.CV

TL;DR: LayerFlow is a unified framework for layer-aware video generation, supporting tasks like decomposing blended videos or generating backgrounds/foregrounds. It uses layer embeddings and a multi-stage training strategy due to lack of high-quality layer-wise training videos.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating layer-aware videos with transparency and versatility, given the scarcity of high-quality training data.

Method: Utilizes a text-to-video diffusion transformer with layer embeddings for sub-clips, and a multi-stage training strategy involving low-quality videos, motion LoRA, and content LoRA.

Result: Enables smooth video generation with desired layers, supporting various tasks like decomposition and background/foreground generation.

Conclusion: LayerFlow provides a flexible and unified solution for layer-aware video generation, overcoming data limitations through innovative training.

Abstract: We present LayerFlow, a unified solution for layer-aware video generation.
Given per-layer prompts, LayerFlow generates videos for the transparent
foreground, clean background, and blended scene. It also supports versatile
variants like decomposing a blended video or generating the background for the
given foreground and vice versa. Starting from a text-to-video diffusion
transformer, we organize the videos for different layers as sub-clips, and
leverage layer embeddings to distinguish each clip and the corresponding
layer-wise prompts. In this way, we seamlessly support the aforementioned
variants in one unified framework. For the lack of high-quality layer-wise
training videos, we design a multi-stage training strategy to accommodate
static images with high-quality layer annotations. Specifically, we first train
the model with low-quality video data. Then, we tune a motion LoRA to make the
model compatible with static frames. Afterward, we train the content LoRA on
the mixture of image data with high-quality layered images along with
copy-pasted video data. During inference, we remove the motion LoRA thus
generating smooth videos with desired layers.

</details>


### [306] [DreamFrame: Enhancing Video Understanding via Automatically Generated QA and Style-Consistent Keyframes](https://arxiv.org/pdf/2403.01422)
*Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Shengji Tang, Jiayuan Fan, Tao Chen*

Main category: cs.CV

TL;DR: DreamFrame is a three-stage framework for generating style-consistent keyframes and QA pairs to support LVLM instruction tuning, reducing the need for manual dataset creation.


<details>
  <summary>Details</summary>
Motivation: Existing LVLM datasets require heavy human labor for annotation, making adaptation to specific tasks challenging. DreamFrame automates dataset generation to address this.

Method: DreamFrame uses an LLM to generate structured movie plots, a Style Immobilization Process for visual consistency, and integrates descriptions and embeddings to create keyframes.

Result: The framework produced 1k stylized videos and 100k QA pairs, and fine-tuned DreamFrame-7B outperformed similar-sized LVLMs.

Conclusion: DreamFrame effectively automates dataset generation for LVLMs, improving performance and reducing manual effort.

Abstract: Recent large vision-language models (LVLMs) for video understanding are
primarily fine-tuned with various videos scraped from online platforms.
Existing datasets, such as ActivityNet, require considerable human labor for
structuring and annotation before effectively utilized for tuning LVLMs. While
current LVLMs are primarily trained on existing datasets in broad,
general-purpose settings, adapting them to specific downstream scenarios
remains challenging, as collecting and annotating task-specific videos is
highly labor-intensive and time-consuming. To address this issue, we propose a
three-stage framework named DreamFrame for automatically generating
style-consistent keyframes and corresponding question-answer (QA) pairs to
support LVLM instruction tuning. DreamFrame generates datasets in a movie-like
manner. First, we utilize an LLM to generate structured movie plots including
movie prior information (like overview and style), frame descriptions and
plot-related QA pairs, with a story expansion strategy to mitigate context
length limitations.Then, to ensure visual consistency across generated frames,
we design a Style Immobilization Process which maintains consistent style
through an embedding learning strategy. Finally, frame descriptions and style
embeddings are integrated to produce coherent keyframes. Using DreamFrame, we
construct a dataset comprising approximately 1k stylized keyframe-like videos
and 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM
architectures demonstrate the effectiveness of the proposed dataset.
Furthermore, based on the proposed dataset, we fine-tune a new LVLM named
DreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs
across different benchmarks.

</details>


### [307] [Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning](https://arxiv.org/pdf/2404.10332)
*Rui Hu, Yahan Tu, Shuyu Wei, Dongyuan Lu, Jitao Sang*

Main category: cs.CV

TL;DR: The paper introduces DFTG, a framework to address hallucination specificity in LVLMs by generating targeted instruction data.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from hallucination issues due to low-quality instruction data, and existing datasets fail to account for model-specific hallucination patterns.

Method: DFTG involves hallucination diagnosis and targeted data generation to create model-specific instruction data.

Result: Experiments show DFTG-generated data is more effective in reducing hallucinations compared to prior datasets.

Conclusion: DFTG addresses model-specific hallucination issues, improving LVLM performance.

Abstract: Despite achieving outstanding performance on various cross-modal tasks,
current large vision-language models (LVLMs) still suffer from hallucination
issues, manifesting as inconsistencies between their generated responses and
the corresponding images. Prior research has implicated that the low quality of
instruction data, particularly the skewed balance between positive and negative
samples, is a significant contributor to model hallucinations. Recently,
researchers have proposed high-quality instruction datasets, such as
LRV-Instruction, to mitigate model hallucination. Nonetheless, our
investigation reveals that hallucinatory concepts from different LVLMs exhibit
specificity, i.e. the distribution of hallucinatory concepts varies
significantly across models. Existing datasets did not consider the
hallucination specificity of different models in the design processes, thereby
diminishing their efficacy in mitigating model hallucination. In this paper, we
propose a targeted instruction data generation framework named DFTG that
tailored to the hallucination specificity of different models. Concretely, DFTG
consists of two stages: hallucination diagnosis, which extracts the necessary
information from the model's responses and images for hallucination diagnosis;
and targeted data generation, which generates targeted instruction data based
on diagnostic results. The experimental results on hallucination benchmarks
demonstrate that the targeted instruction data generated by our method are more
effective in mitigating hallucinations compared to previous datasets.

</details>


### [308] [MDPE: A Multimodal Deception Dataset with Personality and Emotional Characteristics](https://arxiv.org/pdf/2407.12274)
*Cong Cai, Shan Liang, Xuefei Liu, Kang Zhu, Zhengqi Wen, Jianhua Tao, Heng Xie, Jizhou Cui, Yiming Ma, Zhenhua Cheng, Hanzhe Xu, Ruibo Fu, Bin Liu, Yongwei Li*

Main category: cs.CV

TL;DR: The paper introduces MDPE, a multimodal deception dataset with individual differences, to enhance deception detection research.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of sufficient datasets for evaluating deception detection performance, especially considering individual differences.

Method: Creation of the MDPE dataset, including deception features, personality traits, and emotional expression characteristics from 193 subjects.

Result: MDPE provides over 104 hours of deception and emotional videos, supporting deception detection, personality recognition, and emotion recognition.

Conclusion: MDPE is a valuable resource for advancing affective computing and deception detection research.

Abstract: Deception detection has garnered increasing attention in recent years due to
the significant growth of digital media and heightened ethical and security
concerns. It has been extensively studied using multimodal methods, including
video, audio, and text. In addition, individual differences in deception
production and detection are believed to play a crucial role.Although some
studies have utilized individual information such as personality traits to
enhance the performance of deception detection, current systems remain limited,
partly due to a lack of sufficient datasets for evaluating performance. To
address this issue, we introduce a multimodal deception dataset MDPE. Besides
deception features, this dataset also includes individual differences
information in personality and emotional expression characteristics. It can
explore the impact of individual differences on deception behavior. It
comprises over 104 hours of deception and emotional videos from 193 subjects.
Furthermore, we conducted numerous experiments to provide valuable insights for
future deception detection research. MDPE not only supports deception
detection, but also provides conditions for tasks such as personality
recognition and emotion recognition, and can even study the relationships
between them. We believe that MDPE will become a valuable resource for
promoting research in the field of affective computing.

</details>


### [309] [Mixed Non-linear Quantization for Vision Transformers](https://arxiv.org/pdf/2407.18437)
*Gihwan Kim, Jemin Lee, Sihyeong Park, Yongin Kwon, Hyungshin Kim*

Main category: cs.CV

TL;DR: Proposes a mixed non-linear quantization method for Vision Transformers, outperforming existing methods by adapting quantization per non-linear layer.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods overlook non-linear operations or apply uniform quantization, which can be improved by layer-specific methods.

Method: Uses a mixed non-linear quantization approach, assigning error-minimizing methods per layer based on SQNR difference metric.

Result: Outperforms I-BERT, FQ-ViT, and I-ViT in 8-bit and 6-bit settings by 0.6%p and 19.6%p, respectively.

Conclusion: The method is effective and scalable, with plans to release code for broader use.

Abstract: The majority of quantization methods have been proposed to reduce the model
size of Vision Transformers, yet most of them have overlooked the quantization
of non-linear operations. Only a few works have addressed quantization for
non-linear operations, but they applied a single quantization method across all
non-linear operations. We believe that this can be further improved by
employing a different quantization method for each non-linear operation.
Therefore, to assign the most error-minimizing quantization method from the
known methods to each non-linear layer, we propose a mixed non-linear
quantization that considers layer-wise quantization sensitivity measured by
SQNR difference metric. The results show that our method outperforms I-BERT,
FQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin
models by an average of 0.6%p and 19.6%p, respectively. Our method outperforms
I-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is
limited. We plan to release our code at
https://gitlab.com/ones-ai/mixed-non-linear-quantization.

</details>


### [310] [EasyInv: Toward Fast and Better DDIM Inversion](https://arxiv.org/pdf/2408.05159)
*Ziyue Zhang, Mingbao Lin, Shuicheng Yan, Rongrong Ji*

Main category: cs.CV

TL;DR: EasyInv introduces a simplified, efficient approach to DDIM Inversion by refining inversion noise approximation and leveraging the initial latent state, outperforming traditional methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and performance limitations of traditional iterative optimization methods in DDIM Inversion.

Method: Refines inversion noise approximation, prioritizes the initial latent state, and aggregates latent states methodically to reduce noise impact.

Result: Matches or exceeds conventional DDIM Inversion accuracy, especially in limited precision or resource scenarios, with a threefold efficiency boost.

Conclusion: EasyInv is a highly efficient, accurate, and easy-to-integrate solution for DDIM Inversion, offering significant improvements over traditional methods.

Abstract: This paper introduces EasyInv, an easy yet novel approach that significantly
advances the field of DDIM Inversion by addressing the inherent inefficiencies
and performance limitations of traditional iterative optimization methods. At
the core of our EasyInv is a refined strategy for approximating inversion
noise, which is pivotal for enhancing the accuracy and reliability of the
inversion process. By prioritizing the initial latent state, which encapsulates
rich information about the original images, EasyInv steers clear of the
iterative refinement of noise items. Instead, we introduce a methodical
aggregation of the latent state from the preceding time step with the current
state, effectively increasing the influence of the initial latent state and
mitigating the impact of noise. We illustrate that EasyInv is capable of
delivering results that are either on par with or exceed those of the
conventional DDIM Inversion approach, especially under conditions where the
model's precision is limited or computational resources are scarce.
Concurrently, our EasyInv offers an approximate threefold enhancement regarding
inference efficiency over off-the-shelf iterative optimization techniques. It
can be easily combined with most existing inversion methods by only four lines
of code. See code at https://github.com/potato-kitty/EasyInv.

</details>


### [311] [Your Turn: At Home Turning Angle Estimation for Parkinson's Disease Severity Assessment](https://arxiv.org/pdf/2408.08182)
*Qiushuo Cheng, Catherine Morgan, Arindam Sikdar, Alessandro Masullo, Alan Whone, Majid Mirmehdi*

Main category: cs.CV

TL;DR: A deep learning method quantifies turning angles in Parkinson's Disease (PD) patients using 3D skeletons from videos, achieving 41.6% accuracy in home settings.


<details>
  <summary>Details</summary>
Motivation: Existing clinical tools lack the ability to capture hour-by-hour PD symptom variations. Continuous, passive gait measurement can better track disease progression.

Method: Uses Fastpose and Strided Transformer models on 1386 turning clips from 24 subjects (12 PD, 12 controls). Validates with Turn-H3.6M dataset. Quantizes angles to 45° bins due to ground truth challenges.

Result: Achieves 41.6% accuracy, 34.7° MAE, and 68.3% weighted precision for Turn-REMAP.

Conclusion: First work to quantify PD patient turns in home settings using monocular camera data, addressing challenges like clothing and lighting.

Abstract: People with Parkinson's Disease (PD) often experience progressively worsening
gait, including changes in how they turn around, as the disease progresses.
Existing clinical rating tools are not capable of capturing hour-by-hour
variations of PD symptoms, as they are confined to brief assessments within
clinic settings. Measuring gait turning angles continuously and passively is a
component step towards using gait characteristics as sensitive indicators of
disease progression in PD. This paper presents a deep learning-based approach
to automatically quantify turning angles by extracting 3D skeletons from videos
and calculating the rotation of hip and knee joints. We utilise
state-of-the-art human pose estimation models, Fastpose and Strided
Transformer, on a total of 1386 turning video clips from 24 subjects (12 people
with PD and 12 healthy control volunteers), trimmed from a PD dataset of
unscripted free-living videos in a home-like setting (Turn-REMAP). We also
curate a turning video dataset, Turn-H3.6M, from the public Human3.6M human
pose benchmark with 3D ground truth, to further validate our method. Previous
gait research has primarily taken place in clinics or laboratories evaluating
scripted gait outcomes, but this work focuses on free-living home settings
where complexities exist, such as baggy clothing and poor lighting. Due to
difficulties in obtaining accurate ground truth data in a free-living setting,
we quantise the angle into the nearest bin $45^\circ$ based on the manual
labelling of expert clinicians. Our method achieves a turning calculation
accuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\deg}, and a weighted
precision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the
use of single monocular camera data to quantify turns by PD patients in a home
setting.

</details>


### [312] [KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action Recognition](https://arxiv.org/pdf/2409.09444)
*Zhaoyu Chen, Xing Li, Qian Huang, Qiang Geng, Tianjin Yang, Shihao Han*

Main category: cs.CV

TL;DR: The paper introduces D-Hyperpoint and KAN-HyperpointNet for 3D action recognition, balancing limb micro-movements and posture macro-structure, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to balance limb micro-movements and posture macro-structure, losing crucial action cues.

Method: Proposes D-Hyperpoint Embedding for motion/posture summary and KANsMixer with Kolmogorov-Arnold Networks for spatio-temporal interaction.

Result: Achieves state-of-the-art performance on MSR Action3D and NTU-RGB+D 60 datasets.

Conclusion: D-Hyperpoint and KAN-HyperpointNet effectively address the balance issue and improve 3D action recognition.

Abstract: Point cloud sequence-based 3D action recognition has achieved impressive
performance and efficiency. However, existing point cloud sequence modeling
methods cannot adequately balance the precision of limb micro-movements with
the integrity of posture macro-structure, leading to the loss of crucial
information cues in action inference. To overcome this limitation, we introduce
D-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding
module. D-Hyperpoint encapsulates both regional-momentary motion and
global-static posture, effectively summarizing the unit human action at each
moment. In addition, we present a D-Hyperpoint KANsMixer module, which is
recursively applied to nested groupings of D-Hyperpoints to learn the action
discrimination information and creatively integrates Kolmogorov-Arnold Networks
(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we
propose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for
3D action recognition. Extensive experiments on two public datasets: MSR
Action3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our
method.

</details>


### [313] [DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection](https://arxiv.org/pdf/2410.09821)
*Kecen Li, Bingquan Dai, Jingjing Fu, Xinwen Hou*

Main category: cs.CV

TL;DR: A novel dual-modality augmentation method for 3D anomaly synthesis is proposed, combined with a reconstruction-based discriminative network, achieving state-of-the-art detection precision and competitive segmentation performance.


<details>
  <summary>Details</summary>
Motivation: The lack of exploration in multi-modality anomaly detection, especially involving 3D and RGB images, motivates the development of a dual-modality approach.

Method: The method includes a dual-modality augmentation for 3D anomaly synthesis and a reconstruction-based network with a dual-modal discriminator and augmentation dropout.

Result: Outperforms state-of-the-art methods in detection precision and achieves competitive segmentation on MVTec 3D-AD and Eyescandies datasets.

Conclusion: The proposed method effectively addresses multi-modality anomaly detection, demonstrating superior performance and generalizability.

Abstract: Synthesizing anomaly samples has proven to be an effective strategy for
self-supervised 2D industrial anomaly detection. However, this approach has
been rarely explored in multi-modality anomaly detection, particularly
involving 3D and RGB images. In this paper, we propose a novel dual-modality
augmentation method for 3D anomaly synthesis, which is simple and capable of
mimicking the characteristics of 3D defects. Incorporating with our anomaly
synthesis method, we introduce a reconstruction-based discriminative anomaly
detection network, in which a dual-modal discriminator is employed to fuse the
original and reconstructed embedding of two modalities for anomaly detection.
Additionally, we design an augmentation dropout mechanism to enhance the
generalizability of the discriminator. Extensive experiments show that our
method outperforms the state-of-the-art methods on detection precision and
achieves competitive segmentation performance on both MVTec 3D-AD and
Eyescandies datasets.

</details>


### [314] [MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling](https://arxiv.org/pdf/2410.10798)
*Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: MMAR is a novel multi-modal framework addressing image information loss in joint models by using continuous-valued tokens and a diffusion head, outperforming existing methods in understanding and generation.


<details>
  <summary>Details</summary>
Motivation: Recent methods lose image information during understanding tasks due to discretization or diffusion denoising. MMAR aims to mitigate this.

Method: MMAR uses continuous-valued image tokens and a diffusion head on auto-regressed embeddings, separating diffusion from the backbone. Training includes stability techniques and task balancing.

Result: MMAR outperforms existing models on 18 benchmarks and generates high-quality images, showing scalability.

Conclusion: MMAR effectively addresses information loss, excels in multi-modal tasks, and scales well.

Abstract: Recent advancements in multi-modal large language models have propelled the
development of joint probabilistic models capable of both image understanding
and generation. However, we have identified that recent methods suffer from
loss of image information during understanding task, due to either image
discretization or diffusion denoising steps. To address this issue, we propose
a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework.
Unlike discretization line of method, MMAR takes in continuous-valued image
tokens to avoid information loss in an efficient way. Differing from
diffusion-based approaches, we disentangle the diffusion process from
auto-regressive backbone model by employing a light-weight diffusion head on
top each auto-regressed image patch embedding. In this way, when the model
transits from image generation to understanding through text generation, the
backbone model's hidden representation of the image is not limited to the last
denoising step. To successfully train our method, we also propose a
theoretically proven technique that addresses the numerical stability issue and
a training strategy that balances the generation and understanding task goals.
Extensive evaluations on 18 image understanding benchmarks show that MMAR
significantly outperforms most of the existing joint multi-modal models,
surpassing the method that employs pre-trained CLIP vision encoder. Meanwhile,
MMAR is able to generate high quality images. We also show that our method is
scalable with larger data and model size.

</details>


### [315] [Objective drives the consistency of representational similarity across datasets](https://arxiv.org/pdf/2411.05561)
*Laure Ciernik, Lorenz Linhardt, Marco Morik, Jonas Dippel, Simon Kornblith, Lukas Muttenthaler*

Main category: cs.CV

TL;DR: The study investigates whether the convergence of model representations in foundation models is dataset-dependent, finding that self-supervised vision models generalize better across datasets than other models.


<details>
  <summary>Details</summary>
Motivation: To determine if the convergence of model representations is influenced by the datasets used, and to analyze how representational similarity varies with stimuli.

Method: Proposes a systematic way to measure representational similarity between models across different datasets, focusing on the role of the objective function.

Result: Self-supervised vision models show more consistent representational similarities across datasets compared to image classification or image-text models. Representational similarities' link to task behavior is dataset-dependent.

Conclusion: Provides a framework for analyzing model representation similarities across datasets and their connection to task behavior, highlighting the importance of the objective function.

Abstract: The Platonic Representation Hypothesis claims that recent foundation models
are converging to a shared representation space as a function of their
downstream task performance, irrespective of the objectives and data modalities
used to train these models (Huh et al., 2024). Representational similarity is
generally measured for individual datasets and is not necessarily consistent
across datasets. Thus, one may wonder whether this convergence of model
representations is confounded by the datasets commonly used in machine
learning. Here, we propose a systematic way to measure how representational
similarity between models varies with the set of stimuli used to construct the
representations. We find that the objective function is a crucial factor in
determining the consistency of representational similarities across datasets.
Specifically, self-supervised vision models learn representations whose
relative pairwise similarities generalize better from one dataset to another
compared to those of image classification or image-text models. Moreover, the
correspondence between representational similarities and the models' task
behavior is dataset-dependent, being most strongly pronounced for single-domain
datasets. Our work provides a framework for analyzing similarities of model
representations across datasets and linking those similarities to differences
in task behavior.

</details>


### [316] [Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator](https://arxiv.org/pdf/2411.15466)
*Chaehun Shin, Jooyoung Choi, Heeseung Kim, Sungroh Yoon*

Main category: cs.CV

TL;DR: Diptych Prompting is a zero-shot method for subject-driven text-to-image generation, improving subject alignment without fine-tuning by treating it as an inpainting task.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require fine-tuning for subject alignment, while zero-shot approaches often sacrifice alignment. Diptych Prompting addresses this gap.

Method: The approach treats generation as an inpainting task, using a diptych layout with a reference image and text-conditioned inpainting. It enhances alignment by removing backgrounds and adjusting attention weights.

Result: Outperforms zero-shot methods in user preference and supports diverse applications like stylized generation and image editing.

Conclusion: Diptych Prompting offers a versatile, efficient solution for subject-driven image generation with improved alignment.

Abstract: Subject-driven text-to-image generation aims to produce images of a new
subject within a desired context by accurately capturing both the visual
characteristics of the subject and the semantic content of a text prompt.
Traditional methods rely on time- and resource-intensive fine-tuning for
subject alignment, while recent zero-shot approaches leverage on-the-fly image
prompting, often sacrificing subject alignment. In this paper, we introduce
Diptych Prompting, a novel zero-shot approach that reinterprets as an
inpainting task with precise subject alignment by leveraging the emergent
property of diptych generation in large-scale text-to-image models. Diptych
Prompting arranges an incomplete diptych with the reference image in the left
panel, and performs text-conditioned inpainting on the right panel. We further
prevent unwanted content leakage by removing the background in the reference
image and improve fine-grained details in the generated subject by enhancing
attention weights between the panels during inpainting. Experimental results
confirm that our approach significantly outperforms zero-shot image prompting
methods, resulting in images that are visually preferred by users.
Additionally, our method supports not only subject-driven generation but also
stylized image generation and subject-driven image editing, demonstrating
versatility across diverse image generation applications. Project page:
https://diptychprompting.github.io/

</details>


### [317] [Learning 3D Representations from Procedural 3D Programs](https://arxiv.org/pdf/2411.17467)
*Xuweiyi Chen, Zezhou Cheng*

Main category: cs.CV

TL;DR: Self-supervised learning from procedural 3D programs achieves performance comparable to semantically rich 3D models, without relying on shape semantics.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of acquiring 3D data (scalability, copyright) by using procedurally generated shapes.

Method: Learning 3D representations from procedural 3D programs that generate shapes using primitives and augmentations.

Result: Procedural 3D representations perform on par with state-of-the-art semantically rich models in tasks like classification and segmentation.

Conclusion: Self-supervised 3D learning doesn't depend on shape semantics, and procedural programs can effectively replace real-world 3D data.

Abstract: Self-supervised learning has emerged as a promising approach for acquiring
transferable 3D representations from unlabeled 3D point clouds. Unlike 2D
images, which are widely accessible, acquiring 3D assets requires specialized
expertise or professional 3D scanning equipment, making it difficult to scale
and raising copyright concerns. To address these challenges, we propose
learning 3D representations from procedural 3D programs that automatically
generate 3D shapes using simple primitives and augmentations. Remarkably,
despite lacking semantic content, the 3D representations learned from the
procedurally generated 3D shapes perform on par with state-of-the-art
representations learned from semantically recognizable 3D models (e.g.,
airplanes) across various downstream 3D tasks, including shape classification,
part segmentation, and masked point cloud completion. We provide a detailed
analysis on factors that make a good 3D procedural program. Extensive
experiments further suggest that current self-supervised learning methods on
point clouds do not rely on the semantics of 3D shapes, shedding light on the
nature of 3D representations learned.

</details>


### [318] [T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts](https://arxiv.org/pdf/2412.04300)
*Ziwei Huang, Wanggui He, Quanyu Long, Yandi Wang, Haoyuan Li, Zhelun Yu, Fangxun Shu, Long Chan, Hao Jiang, Fei Wu, Leilei Gan*

Main category: cs.CV

TL;DR: T2I-FactualBench is introduced as the largest benchmark for evaluating the factuality of knowledge-intensive text-to-image generation, revealing gaps in current SOTA models.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of text-to-image models lack focus on factuality, especially for knowledge-intensive concepts.

Method: A three-tiered framework and multi-round VQA-based evaluation are proposed to assess factuality.

Result: Current SOTA T2I models show significant room for improvement in factuality.

Conclusion: T2I-FactualBench highlights the need for better factuality in knowledge-intensive T2I generation.

Abstract: Evaluating the quality of synthesized images remains a significant challenge
in the development of text-to-image (T2I) generation. Most existing studies in
this area primarily focus on evaluating text-image alignment, image quality,
and object composition capabilities, with comparatively fewer studies
addressing the evaluation of the factuality of T2I models, particularly when
the concepts involved are knowledge-intensive. To mitigate this gap, we present
T2I-FactualBench in this work - the largest benchmark to date in terms of the
number of concepts and prompts specifically designed to evaluate the factuality
of knowledge-intensive concept generation. T2I-FactualBench consists of a
three-tiered knowledge-intensive text-to-image generation framework, ranging
from the basic memorization of individual knowledge concepts to the more
complex composition of multiple knowledge concepts. We further introduce a
multi-round visual question answering (VQA) based evaluation framework to
assess the factuality of three-tiered knowledge-intensive text-to-image
generation tasks. Experiments on T2I-FactualBench indicate that current
state-of-the-art (SOTA) T2I models still leave significant room for
improvement.

</details>


### [319] [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/pdf/2412.06141)
*Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao*

Main category: cs.CV

TL;DR: MMedPO improves Med-LVLMs by addressing modality misalignment with clinically relevant preference optimization, enhancing factual accuracy.


<details>
  <summary>Details</summary>
Motivation: Med-LVLMs suffer from hallucinations due to prioritizing text over visuals, and existing methods fail to adequately address clinical relevance in preference data.

Method: MMedPO introduces two types of dispreference (plausible hallucinations and lesion neglect) and uses clinical relevance scores from Med-LLMs and visual tools to weight preference optimization.

Result: MMedPO improves factual accuracy by 14.2% and 51.7% in Med-VQA and report generation tasks, outperforming existing methods.

Conclusion: MMedPO effectively aligns Med-LVLMs by integrating clinical relevance into preference optimization, significantly reducing hallucinations.

Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their
application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter
factuality challenges due to modality misalignment, where the models prioritize
textual knowledge over visual input, leading to hallucinations that contradict
information in medical images. Previous attempts to enhance modality alignment
in Med-LVLMs through preference optimization have inadequately mitigated
clinical relevance in preference data, making these samples easily
distinguishable and reducing alignment effectiveness. To address this
challenge, we propose MMedPO, a novel multimodal medical preference
optimization approach that considers the clinical relevance of preference
samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference
data by introducing two types of dispreference: (1) plausible hallucinations
injected through target Med-LVLMs or GPT-4o to produce medically inaccurate
responses, and (2) lesion region neglect achieved through local lesion-noising,
disrupting visual understanding of critical areas. We then calculate clinical
relevance for each sample based on scores from multiple Med-LLMs and visual
tools, and integrate these scores into the preference optimization process as
weights, enabling effective alignment. Our experiments demonstrate that MMedPO
significantly enhances factual accuracy in Med-LVLMs, achieving substantial
improvements over existing preference optimization methods by averaging 14.2%
and 51.7% across the Med-VQA and report generation tasks. Our code are
available in https://github.com/aiming-lab/MMedPO.

</details>


### [320] [Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders](https://arxiv.org/pdf/2412.09586)
*Fiona Ryan, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman, James M. Rehg*

Main category: cs.CV

TL;DR: Gaze-LLE, a transformer-based framework, simplifies gaze target estimation by using a frozen DINOv2 encoder and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Prior methods use complex, hand-crafted pipelines; Gaze-LLE leverages general-purpose feature extractors for efficiency.

Method: Uses a frozen DINOv2 encoder for scene features and a lightweight module with person-specific prompts to decode gaze.

Result: Achieves state-of-the-art performance on multiple gaze benchmarks.

Conclusion: Gaze-LLE offers a streamlined, effective approach to gaze target estimation with validated design choices.

Abstract: We address the problem of gaze target estimation, which aims to predict where
a person is looking in a scene. Predicting a person's gaze target requires
reasoning both about the person's appearance and the contents of the scene.
Prior works have developed increasingly complex, hand-crafted pipelines for
gaze target estimation that carefully fuse features from separate scene
encoders, head encoders, and auxiliary models for signals like depth and pose.
Motivated by the success of general-purpose feature extractors on a variety of
visual tasks, we propose Gaze-LLE, a novel transformer framework that
streamlines gaze target estimation by leveraging features from a frozen DINOv2
encoder. We extract a single feature representation for the scene, and apply a
person-specific positional prompt to decode gaze with a lightweight module. We
demonstrate state-of-the-art performance across several gaze benchmarks and
provide extensive analysis to validate our design choices. Our code is
available at: http://github.com/fkryan/gazelle .

</details>


### [321] [Single-Pass Object-Focused Data Selection](https://arxiv.org/pdf/2412.10032)
*Niclas Popp, Dan Zhang, Jan Hendrik Metzen, Matthias Hein, Lukas Schott*

Main category: cs.CV

TL;DR: OFDS improves data selection for labeling by using object-level features from foundation models, outperforming random selection and baselines, and enhances active learning.


<details>
  <summary>Details</summary>
Motivation: High-quality labels are costly, and selecting the right images for labeling is crucial for effective model training.

Method: Proposes Object-Focused Data Selection (OFDS), leveraging object-level features from foundation models to ensure semantic coverage of target classes.

Result: OFDS consistently outperforms random selection and baselines across tasks and domains, especially when combined with autolabels.

Conclusion: OFDS is effective for data selection and improves active learning when used to select initial labeled sets.

Abstract: While unlabeled image data is often plentiful, the costs of high-quality
labels pose an important practical challenge: Which images should one select
for labeling to use the annotation budget for a particular target task most
effectively? To address this problem, we focus on single-pass data selection,
which refers to the process of selecting all data to be annotated at once
before training a downstream model. Prior methods for single-pass data
selection rely on image-level representations and fail to reliably outperform
random selection for object detection and segmentation. We propose
Object-Focused Data Selection (OFDS) which leverages object-level features from
foundation models and ensures semantic coverage of all target classes. In
extensive experiments across tasks and target domains, OFDS consistently
outperforms random selection and all baselines. The best results for
constrained annotation budgets are obtained by combining human labels from OFDS
with autolabels from foundation models. Moreover, using OFDS to select the
initial labeled set for active learning yields consistent improvements

</details>


### [322] [ExeChecker: Where Did I Go Wrong?](https://arxiv.org/pdf/2412.10573)
*Yiwen Gu, Mahir Patel, Margrit Betke*

Main category: cs.CV

TL;DR: ExeChecker is a contrastive learning framework for interpreting rehabilitation exercises, outperforming baselines in identifying incorrect joint movements.


<details>
  <summary>Details</summary>
Motivation: To assist rehabilitation by providing feedback on exercise execution, leveraging pose estimation and interpretability.

Method: Uses contrastive learning with graph-attention neural networks and transformers on paired correct/incorrect exercise data.

Result: Outperforms baseline methods in identifying relevant joints for feedback on ExeCheck and UI-PRMD datasets.

Conclusion: ExeChecker effectively highlights incorrect joint movements, aiding rehabilitation feedback.

Abstract: In this paper, we present a contrastive learning based framework, ExeChecker,
for the interpretation of rehabilitation exercises. Our work builds upon
state-of-the-art advances in the area of human pose estimation, graph-attention
neural networks, and transformer interpretablity. The downstream task is to
assist rehabilitation by providing informative feedback to users while they are
performing prescribed exercises. We utilize a contrastive learning strategy
during training. Given a tuple of correctly and incorrectly executed exercises,
our model is able to identify and highlight those joints that are involved in
an incorrect movement and thus require the user's attention. We collected an
in-house dataset, ExeCheck, with paired recordings of both correct and
incorrect execution of exercises. In our experiments, we tested our method on
this dataset as well as the UI-PRMD dataset and found ExeCheck outperformed the
baseline method using pairwise sequence alignment in identifying joints of
physical relevance in rehabilitation exercises.

</details>


### [323] [RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models](https://arxiv.org/pdf/2412.11050)
*Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, Hong Chen*

Main category: cs.CV

TL;DR: RAC3 enhances vision-language models for autonomous driving by integrating frequency-spatial fusion, cross-modal alignment, and retrieval-augmented strategies, achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing corner cases in autonomous driving is critical for safety, but VLMs suffer from hallucinations and poor real-world grounding.

Method: RAC3 combines a frequency-spatial fusion encoder, cross-modal alignment with negative mining, K-Means/HNSW retrieval, and multimodal CoT prompting.

Result: RAC3 scores 74.46 on CODA-LM and improves performance in downstream tasks, outperforming prior methods.

Conclusion: Retrieval-augmented strategies and cross-modal alignment enhance VLMs for safer, interpretable autonomous driving.

Abstract: Understanding and addressing corner cases is essential for ensuring the
safety and reliability of autonomous driving systems. Vision-language models
(VLMs) play a crucial role in enhancing scenario comprehension, yet they face
significant challenges, such as hallucination and insufficient real-world
grounding, which compromise their performance in critical driving scenarios. In
this work, RAC3, a novel framework designed to enhance the performance of VLMs
in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial
fusion (FSF) image encoder, a cross-modal alignment training method for
embedding models with hard and semi-hard negative mining, and a fast querying
and retrieval pipeline based on K-Means clustering and hierarchical navigable
small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting
strategy to guide analogical reasoning and reduce hallucinations during
inference is introduced. Moreover, an update mechanism is integrated into RAC3
to ensure continual learning within the framework. Extensive experiments on the
CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner
case comprehension across multiple downstream tasks. Compared to prior
state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the
CODA-LM benchmark and shows consistent performance gains when integrated with
end-to-end frameworks like DriveLM. These results demonstrate the effectiveness
of retrieval-augmented strategies and cross-modal alignment for safer and more
interpretable autonomous driving.

</details>


### [324] [CondiMen: Conditional Multi-Person Mesh Recovery](https://arxiv.org/pdf/2412.13058)
*Brégier Romain, Baradel Fabien, Lucas Thomas, Galaaoui Salma, Armando Matthieu, Weinzaepfel Philippe, Rogez Grégory*

Main category: cs.CV

TL;DR: CondiMen introduces a Bayesian network for multi-person human mesh recovery, capturing uncertainties and improving predictions with additional data.


<details>
  <summary>Details</summary>
Motivation: Address ambiguities in human mesh recovery, such as uncertainty between size and distance, and loss of 3D information in 2D projections.

Method: Uses a Bayesian network to output a joint parametric distribution over poses, shapes, intrinsics, and distances.

Result: Matches or outperforms state-of-the-art, captures uncertainties, and leverages additional data like multi-view consistency.

Conclusion: CondiMen effectively models ambiguity and enhances predictions, suitable for real-time applications.

Abstract: Multi-person human mesh recovery (HMR) consists in detecting all individuals
in a given input image, and predicting the body shape, pose, and 3D location
for each detected person. The dominant approaches to this task rely on neural
networks trained to output a single prediction for each detected individual. In
contrast, we propose CondiMen, a method that outputs a joint parametric
distribution over likely poses, body shapes, intrinsics and distances to the
camera, using a Bayesian network. This approach offers several advantages.
First, a probability distribution can handle some inherent ambiguities of this
task -- such as the uncertainty between a person's size and their distance to
the camera, or simply the loss of information when projecting 3D data onto the
2D image plane. Second, the output distribution can be combined with additional
information to produce better predictions, by using e.g. known camera or body
shape parameters, or by exploiting multi-view observations. Third, one can
efficiently extract the most likely predictions from the output distribution,
making our proposed approach suitable for real-time applications. Empirically
we find that our model i) achieves performance on par with or better than the
state-of-the-art, ii) captures uncertainties and correlations inherent in pose
estimation and iii) can exploit additional information at test time, such as
multi-view consistency or body shape priors. CondiMen spices up the modeling of
ambiguity, using just the right ingredients on hand.

</details>


### [325] [EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space](https://arxiv.org/pdf/2412.14706)
*Jianrong Zhang, Hehe Fan, Yi Yang*

Main category: cs.CV

TL;DR: EnergyMoGen improves text-driven human motion generation by combining diffusion models with energy-based models, addressing semantic composition challenges.


<details>
  <summary>Details</summary>
Motivation: Latent diffusion models struggle to compose multiple semantic concepts into coherent motion sequences.

Method: Proposes EnergyMoGen with two energy-based models: latent-aware and semantic-aware, plus Synergistic Energy Fusion for consistency.

Result: Outperforms state-of-the-art models in text-to-motion, compositional, and multi-concept motion generation tasks.

Conclusion: EnergyMoGen enables high-quality, complex motion synthesis and extends motion datasets effectively.

Abstract: Diffusion models, particularly latent diffusion models, have demonstrated
remarkable success in text-driven human motion generation. However, it remains
challenging for latent diffusion models to effectively compose multiple
semantic concepts into a single, coherent motion sequence. To address this
issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based
Models: (1) We interpret the diffusion model as a latent-aware energy-based
model that generates motions by composing a set of diffusion models in latent
space; (2) We introduce a semantic-aware energy model based on cross-attention,
which enables semantic composition and adaptive gradient descent for text
embeddings. To overcome the challenges of semantic inconsistency and motion
distortion across these two spectrums, we introduce Synergistic Energy Fusion.
This design allows the motion latent diffusion model to synthesize
high-quality, complex motions by combining multiple energy terms corresponding
to textual descriptions. Experiments show that our approach outperforms
existing state-of-the-art models on various motion generation tasks, including
text-to-motion generation, compositional motion generation, and multi-concept
motion generation. Additionally, we demonstrate that our method can be used to
extend motion datasets and improve the text-to-motion task.

</details>


### [326] [Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models](https://arxiv.org/pdf/2502.01419)
*Mingi Jung, Saehyung Lee, Eunji Kim, Sungroh Yoon*

Main category: cs.CV

TL;DR: SPARC is a training-free method that improves image captioning by selectively recalibrating visual attention, enhancing both precision and recall without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with balancing precision and recall in detailed image captioning due to weakening and noisy visual attention as responses lengthen.

Method: SPARC selectively amplifies visual tokens, identifies critical tokens using attention differences, and reinforces weakening attention during decoding.

Result: SPARC improves both precision and recall in image captioning, outperforming existing methods with minimal computational cost.

Conclusion: SPARC effectively addresses visual attention limitations in MLLMs, enhancing caption quality without additional training.

Abstract: Detailed image captioning is essential for tasks like data generation and
aiding visually impaired individuals. High-quality captions require a balance
between precision and recall, which remains challenging for current multimodal
large language models (MLLMs). In this work, we hypothesize that this
limitation stems from weakening and increasingly noisy visual attention as
responses lengthen. To address this issue, we propose SPARC (Selective
Progressive Attention ReCalibration), a training-free method that enhances the
contribution of visual tokens during decoding. SPARC is founded on three key
observations: (1) increasing the influence of all visual tokens reduces recall;
thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen,
visual attention becomes noisier, so SPARC identifies critical visual tokens by
leveraging attention differences across time steps; (3) as visual attention
gradually weakens, SPARC reinforces it to preserve its influence. Our
experiments, incorporating both automated and human evaluations, demonstrate
that existing methods improve the precision of MLLMs at the cost of recall. In
contrast, our proposed method enhances both precision and recall with minimal
computational overhead.

</details>


### [327] [A Flag Decomposition for Hierarchical Datasets](https://arxiv.org/pdf/2502.07782)
*Nathan Mankovich, Ignacio Santamaria, Gustau Camps-Valls, Tolga Birdal*

Main category: cs.CV

TL;DR: Proposes a flag-based method for decomposing hierarchical data into flag representations, enhancing applications like denoising and clustering.


<details>
  <summary>Details</summary>
Motivation: Current methods for flag extraction are limited to matrix decompositions, lacking a general algorithm for hierarchical data.

Method: Introduces a novel flag-based decomposition method using Stiefel coordinates to preserve hierarchy.

Result: Enables hierarchical data representation, improving tasks like denoising, clustering, and few-shot learning.

Conclusion: The method expands flag manifold utility in computer vision and machine learning.

Abstract: Flag manifolds encode nested sequences of subspaces and serve as powerful
structures for various computer vision and machine learning applications.
Despite their utility in tasks such as dimensionality reduction, motion
averaging, and subspace clustering, current applications are often restricted
to extracting flags using common matrix decomposition methods like the singular
value decomposition. Here, we address the need for a general algorithm to
factorize and work with hierarchical datasets. In particular, we propose a
novel, flag-based method that decomposes arbitrary hierarchical real-valued
data into a hierarchy-preserving flag representation in Stiefel coordinates.
Our work harnesses the potential of flag manifolds in applications including
denoising, clustering, and few-shot learning.

</details>


### [328] [DiffoRA: Enabling Parameter-Efficient Fine-Tuning via Differential Module Selection](https://arxiv.org/pdf/2502.08905)
*Tangyu Jiang, Haodi Wang, Chun Yuan*

Main category: cs.CV

TL;DR: DiffoRA is a new PEFT method that adaptively selects modules for fine-tuning in LLMs using a Differential Adaptation Matrix (DAM), outperforming existing approaches like LoRA.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA apply uniform fine-tuning or importance-based adjustments, but not all modules in LLMs need tuning. DiffoRA addresses this by selectively adapting modules.

Method: DiffoRA introduces a DAM to identify suitable modules for fine-tuning, using continuous relaxation, discretization, and weight-sharing optimizations. Theoretical analysis links DAM to model convergence and generalization.

Result: DiffoRA achieves state-of-the-art performance across multiple benchmarks, demonstrating its effectiveness.

Conclusion: DiffoRA offers a more efficient and effective PEFT approach by selectively fine-tuning only necessary modules in LLMs.

Abstract: The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively
researched for large language models in downstream tasks. Among all the
existing approaches, the Low-Rank Adaptation (LoRA) has gained popularity for
its streamlined design by incorporating low-rank matrices into existing
pre-trained models. Though effective, LoRA, as well as its adaptive
optimizations, either allocate the same matrix to all the modules or adjust the
interior rank of the components based on importance scoring indicators. In this
paper, we argue that not all the modules in LLMs are suitable and necessary to
be fine-tuned. Enlightened by this insight, we propose a new PEFT scheme called
DiffoRA, which enables adaptive adoption of the low-rank decomposition
matrices. At the core of DiffoRA lies a Differential Adaptation Matrix (DAM) to
determine which module is the most suitable and essential for fine-tuning. We
theoretically explain how the designed matrix impacts the convergence rate and
generalization capability of a pre-trained model. We then construct the DAM via
continuous relaxation and discretization with weight-sharing optimizations. We
fully implement DiffoRA and design comprehensive experiments to evaluate its
performance. The experimental results demonstrate that DiffoRA delivers
state-of-the-art results across multiple benchmarks.

</details>


### [329] [Galileo: Learning Global & Local Features of Many Remote Sensing Modalities](https://arxiv.org/pdf/2502.09356)
*Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James R. Green, Evan Shelhamer, Hannah Kerner, David Rolnick*

Main category: cs.CV

TL;DR: A multimodal transformer (Galileo) for remote sensing tasks outperforms specialist models across 11 benchmarks using self-supervised learning with dual contrastive losses.


<details>
  <summary>Details</summary>
Motivation: Remote sensing tasks require handling diverse data modalities and varying object scales, making shared representation learning challenging.

Method: Uses a self-supervised learning algorithm with masked modeling and dual global/local contrastive losses to extract multi-scale features.

Result: Galileo outperforms state-of-the-art specialist models in tasks like crop mapping and flood detection.

Conclusion: The proposed model demonstrates the effectiveness of multimodal self-supervised learning for remote sensing.

Abstract: We introduce a highly multimodal transformer to represent many remote sensing
modalities - multispectral optical, synthetic aperture radar, elevation,
weather, pseudo-labels, and more - across space and time. These inputs are
useful for diverse remote sensing tasks, such as crop mapping and flood
detection. However, learning shared representations of remote sensing data is
challenging, given the diversity of relevant data modalities, and because
objects of interest vary massively in scale, from small boats (1-2 pixels and
fast) to glaciers (thousands of pixels and slow). We present a novel
self-supervised learning algorithm that extracts multi-scale features across a
flexible set of input modalities through masked modeling. Our dual global and
local contrastive losses differ in their targets (deep representations vs.
shallow input projections) and masking strategies (structured vs. not). Our
Galileo is a single generalist model that outperforms SoTA specialist models
for satellite images and pixel time series across eleven benchmarks and
multiple tasks.

</details>


### [330] [FlexTok: Resampling Images into 1D Token Sequences of Flexible Length](https://arxiv.org/pdf/2502.13967)
*Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oğuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, Afshin Dehghan*

Main category: cs.CV

TL;DR: FlexTok introduces variable-length 1D tokenization for images, improving efficiency and quality in autoregressive generation, outperforming fixed-token methods like TiTok.


<details>
  <summary>Details</summary>
Motivation: Traditional image tokenization uses fixed token counts, limiting adaptability to image complexity. FlexTok addresses this by enabling variable-length token sequences.

Method: FlexTok projects 2D images into variable-length 1D token sequences, using a rectified flow decoder and nested dropout for reconstruction. A GPT-style Transformer is trained for autoregressive generation.

Result: On ImageNet, FlexTok achieves FID<2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with fewer tokens. It also supports text-conditioned generation.

Conclusion: FlexTok's variable-length tokenization enables coarse-to-fine image description, with token count adapting to task complexity, offering a flexible and efficient alternative to traditional 2D tokenization.

Abstract: Image tokenization has enabled major advances in autoregressive image
generation by providing compressed, discrete representations that are more
efficient to process than raw pixels. While traditional approaches use 2D grid
tokenization, recent methods like TiTok have shown that 1D tokenization can
achieve high generation quality by eliminating grid redundancies. However,
these methods typically use a fixed number of tokens and thus cannot adapt to
an image's inherent complexity. We introduce FlexTok, a tokenizer that projects
2D images into variable-length, ordered 1D token sequences. For example, a
256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,
hierarchically and semantically compressing its information. By training a
rectified flow model as the decoder and using nested dropout, FlexTok produces
plausible reconstructions regardless of the chosen token sequence length. We
evaluate our approach in an autoregressive generation setting using a simple
GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to
128 tokens, outperforming TiTok and matching state-of-the-art methods with far
fewer tokens. We further extend the model to support to text-conditioned image
generation and examine how FlexTok relates to traditional 2D tokenization. A
key finding is that FlexTok enables next-token prediction to describe images in
a coarse-to-fine "visual vocabulary", and that the number of tokens to generate
depends on the complexity of the generation task.

</details>


### [331] [M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment](https://arxiv.org/pdf/2502.15167)
*Chuan Cui, Kejiang Chen, Zhihua Wei, Wen Shen, Weiming Zhang, Nenghai Yu*

Main category: cs.CV

TL;DR: M3-AGIQA is a framework using Multimodal Large Language Models (MLLMs) to evaluate AI-generated images holistically, focusing on perceptual quality, prompt correspondence, and authenticity. It outperforms benchmarks and aligns with human judgment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges in evaluating AI-generated images (AIGI) across perceptual quality, prompt correspondence, and authenticity.

Method: Leverages MLLMs for multimodal evaluation, featuring a structured multi-round process with intermediate image descriptions.

Result: Achieves state-of-the-art performance on benchmarks and strong generalizability in cross-dataset settings.

Conclusion: M3-AGIQA provides robust, interpretable quality scores aligned with human judgment, advancing AIGI evaluation.

Abstract: The rapid advancement of AI-generated image (AIGI) models presents new
challenges for evaluating image quality, particularly across three aspects:
perceptual quality, prompt correspondence, and authenticity. To address these
challenges, we introduce M3-AGIQA, a comprehensive framework that leverages
Multimodal Large Language Models (MLLMs) to enable more human-aligned, holistic
evaluation of AI-generated images across both visual and textual domains.
Besides, our framework features a structured multi-round evaluation process,
generating and analyzing intermediate image descriptions to provide deeper
insight into these three aspects. By aligning model outputs more closely with
human judgment, M3-AGIQA delivers robust and interpretable quality scores.
Extensive experiments on multiple benchmarks demonstrate that our method
achieves state-of-the-art performance on tested datasets and aspects, and
exhibits strong generalizability in most cross-dataset settings. Code is
available at https://github.com/strawhatboy/M3-AGIQA.

</details>


### [332] [Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection](https://arxiv.org/pdf/2503.02101)
*Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, Liaoni Wu*

Main category: cs.CV

TL;DR: The paper proposes a method using diffusion models to extract domain-invariant features for domain generalization in object detection, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of domain generalization (DG) in object detection due to real-world variations, leveraging diffusion models' scene generation capabilities.

Method: Extracts multi-step intermediate features from diffusion models for domain-invariant features and introduces a knowledge transfer framework for feature and object-level alignment.

Result: Achieves 14.0% mAP improvement over existing DG methods and 15.9% mAP over baselines, outperforming domain adaptation methods without target data.

Conclusion: The method effectively enhances DG for object detection and offers insights for robust real-world visual recognition.

Abstract: Domain generalization (DG) for object detection aims to enhance detectors'
performance in unseen scenarios. This task remains challenging due to complex
variations in real-world applications. Recently, diffusion models have
demonstrated remarkable capabilities in diverse scene generation, which
inspires us to explore their potential for improving DG tasks. Instead of
generating images, our method extracts multi-step intermediate features during
the diffusion process to obtain domain-invariant features for generalized
detection. Furthermore, we propose an efficient knowledge transfer framework
that enables detectors to inherit the generalization capabilities of diffusion
models through feature and object-level alignment, without increasing inference
time. We conduct extensive experiments on six challenging DG benchmarks. The
results demonstrate that our method achieves substantial improvements of 14.0%
mAP over existing DG approaches across different domains and corruption types.
Notably, our method even outperforms most domain adaptation methods without
accessing any target domain data. Moreover, the diffusion-guided detectors show
consistent improvements of 15.9% mAP on average compared to the baseline. Our
work aims to present an effective approach for domain-generalized detection and
provide potential insights for robust visual recognition in real-world
scenarios. The code is available at
https://github.com/heboyong/Generalized-Diffusion-Detector.

</details>


### [333] [The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights](https://arxiv.org/pdf/2503.04167)
*Yufang Liu, Yao Du, Tao Ji, Jianing Wang, Yang Liu, Yuanbin Wu, Aimin Zhou, Mengdi Zhang, Xunliang Cai*

Main category: cs.CV

TL;DR: The paper highlights the underutilization of visual information in multimodal mathematical reasoning and introduces the HC-M3D dataset to address this gap, revealing limitations in current models' visual perception.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of visual information in multimodal mathematical reasoning, as existing models largely ignore it despite its potential importance.

Method: The study introduces the HC-M3D dataset, designed to require image reliance for problem-solving, and tests leading models on their ability to detect subtle visual differences.

Result: Current models fail to utilize visual information effectively, and combining various image encoders does not improve math reasoning performance.

Conclusion: The findings challenge the enhancement of visual reliance in math reasoning and propose the HC-M3D dataset as a tool for better evaluation.

Abstract: Recent research has increasingly focused on multimodal mathematical
reasoning, particularly emphasizing the creation of relevant datasets and
benchmarks. Despite this, the role of visual information in reasoning has been
underexplored. Our findings show that existing multimodal mathematical models
minimally leverage visual information, and model performance remains largely
unaffected by changes to or removal of images in the dataset. We attribute this
to the dominance of textual information and answer options that inadvertently
guide the model to correct answers. To improve evaluation methods, we introduce
the HC-M3D dataset, specifically designed to require image reliance for
problem-solving and to challenge models with similar, yet distinct, images that
change the correct answer. In testing leading models, their failure to detect
these subtle visual differences suggests limitations in current visual
perception capabilities. Additionally, we observe that the common approach of
improving general VQA capabilities by combining various types of image encoders
does not contribute to math reasoning performance. This finding also presents a
challenge to enhancing visual reliance during math reasoning. Our benchmark and
code would be available at
\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\_modality\_role}.

</details>


### [334] [Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces](https://arxiv.org/pdf/2503.05283)
*Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodolà, Simone Melzi, Maks Ovsjanikov*

Main category: cs.CV

TL;DR: The paper explores post-training alignment of 3D and text feature spaces, finding improved performance by projecting onto lower-dimensional subspaces.


<details>
  <summary>Details</summary>
Motivation: To investigate the alignment of 3D uni-modal encoders with text feature spaces, as existing methods rely on explicit alignment objectives.

Method: Projects learned 3D and text representations onto lower-dimensional subspaces for better alignment.

Result: Alignment quality improves significantly, enhancing matching and retrieval tasks, and reveals shared subspaces for semantic and geometric data.

Conclusion: Establishes a baseline for post-training alignment of 3D and text features, highlighting shared and unique properties of 3D data.

Abstract: Recent works have shown that, when trained at scale, uni-modal 2D vision and
text encoders converge to learned features that share remarkable structural
properties, despite arising from different representations. However, the role
of 3D encoders with respect to other modalities remains unexplored.
Furthermore, existing 3D foundation models that leverage large datasets are
typically trained with explicit alignment objectives with respect to frozen
encoders from other representations. In this work, we investigate the
possibility of a posteriori alignment of representations obtained from
uni-modal 3D encoders compared to text-based feature spaces. We show that naive
post-training feature alignment of uni-modal text and 3D encoders results in
limited performance. We then focus on extracting subspaces of the corresponding
feature spaces and discover that by projecting learned representations onto
well-chosen lower-dimensional subspaces the quality of alignment becomes
significantly higher, leading to improved accuracy on matching and retrieval
tasks. Our analysis further sheds light on the nature of these shared
subspaces, which roughly separate between semantic and geometric data
representations. Overall, ours is the first work that helps to establish a
baseline for post-training alignment of 3D uni-modal and text feature spaces,
and helps to highlight both the shared and unique properties of 3D data
compared to other representations. Our code and weights are available at
https://github.com/Souhail-01/3d-text-alignment

</details>


### [335] [SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation](https://arxiv.org/pdf/2503.06764)
*Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hongbin Xu, Runhui Huang, Jun Zhou, Jianhua Han, Hang Xu, Xiaodan Liang*

Main category: cs.CV

TL;DR: SemHiTok is a unified image tokenizer using a semantic-guided hierarchical codebook to balance high-level semantic and low-level pixel features for multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing unified image tokenizers struggle to balance semantic and pixel features due to conflicting priorities in multimodal understanding and generation.

Method: SemHiTok introduces a semantic-guided hierarchical codebook, decoupling semantic and pixel features structurally and in training, enabling better feature capture.

Result: SemHiTok achieves SOTA in image reconstruction and multimodal understanding, and its unified MLLM excels in both understanding and generation tasks.

Conclusion: SemHiTok effectively balances semantic and pixel features, outperforming previous methods in multimodal tasks.

Abstract: In this paper, we introduce SemHiTok, a unified image Tokenizer via
Semantic-Guided Hierarchical codebook that provides consistent discrete
representations for multimodal understanding and generation. Recently, unified
image tokenizers have sparked exploration within research community, which is
designed to capture high-level semantic features for understanding and
retaining low-level pixel features for generation. Previous works attempt to
train a unified image tokenizer by combining loss for semantic distillation and
pixel reconstruction. However, due to the differing levels of features
prioritized by multimodal understanding and generation, joint training methods
face significant challenges in achieving a good trade-off. SemHiTok addresses
this challenge through a novel semantic-guided hierarchical codebook, which
builds pixel sub-codebooks on a pretrained semantic codebook. This design
decouples semantic and pixel both in terms of structure and training strategy,
enabling the tokenizer to capture pixel features while retaining its ability to
comprehend high-level semantic information. Our experiments demonstrate that
SemHiTok achieves SOTA performance in image reconstruction and multimodal
understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with
SemHiTok, which exhibits superior performance across multimodal understanding
and generation tasks. For understanding, SemHiTok achieves impressive
performance on most benchmarks. For generation, our model achieves SOTA
performance on MJHQ30K in unified MLLMs.

</details>


### [336] [EscapeCraft: A 3D Room Escape Environment for Benchmarking Complex Multimodal Reasoning Ability](https://arxiv.org/pdf/2503.10042)
*Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu*

Main category: cs.CV

TL;DR: The paper introduces MM-Escape, a benchmark for evaluating multimodal reasoning in MLLMs, highlighting gaps in current assessments and revealing model limitations.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on task completion, neglecting the reasoning process. The paper aims to address this by analyzing intermediate behaviors in multimodal tasks.

Method: The authors develop MM-Escape, a benchmark inspired by escape games, and EscapeCraft, an open environment for assessing multimodal reasoning.

Result: MLLMs succeed in simple tasks but struggle with complexity, showing varied failure modes like poor spatial awareness and ineffective prop use.

Conclusion: The work identifies new challenges in multimodal reasoning and suggests areas for improving MLLMs.

Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred
interest in complex multimodal reasoning tasks in the real-world and virtual
environment, which require coordinating multiple abilities, including visual
perception, visual reasoning, spatial awareness, and target deduction. However,
existing evaluations primarily assess the final task completion, often
degrading assessments to isolated abilities such as visual grounding and visual
question answering. Less attention is given to comprehensively and
quantitatively analyzing reasoning process in multimodal environments, which is
crucial for understanding model behaviors and underlying reasoning mechanisms
beyond merely task success. To address this, we introduce MM-Escape, an
extensible benchmark for investigating multimodal reasoning, inspired by
real-world escape games. MM-Escape emphasizes intermediate model behaviors
alongside final task completion. To achieve this, we develop EscapeCraft, a
customizable and open environment that enables models to engage in free-form
exploration for assessing multimodal reasoning. Extensive experiments show that
MLLMs, regardless of scale, can successfully complete the simplest room escape
tasks, with some exhibiting human-like exploration strategies. Yet, performance
dramatically drops as task difficulty increases. Moreover, we observe that
performance bottlenecks vary across models, revealing distinct failure modes
and limitations in their multimodal reasoning abilities, such as repetitive
trajectories without adaptive exploration, getting stuck in corners due to poor
visual spatial awareness, and ineffective use of acquired props, such as the
key. We hope our work sheds light on new challenges in multimodal reasoning,
and uncovers potential improvements in MLLMs capabilities.

</details>


### [337] [Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation](https://arxiv.org/pdf/2503.10691)
*Qiji Zhou, Yifan Gong, Guangsheng Bao, Hongjie Qiu, Jinqiang Li, Xiangrong Zhu, Huajian Zhang, Yue Zhang*

Main category: cs.CV

TL;DR: COVER is a new benchmark for evaluating multimodal models' counterfactual reasoning in video understanding, focusing on structured sub-questions and revealing the importance of reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks lack systematic evaluation of counterfactual reasoning in video understanding, which is crucial for robustness.

Method: COVER decomposes complex queries into structured sub-questions to enable fine-grained reasoning analysis.

Result: Experiments show a strong correlation between sub-question accuracy and counterfactual reasoning performance, emphasizing structured inference's role.

Conclusion: Enhancing reasoning capabilities is key to robust video understanding, and COVER sets a new standard for evaluating MLLMs in dynamic environments.

Abstract: Counterfactual reasoning is crucial for robust video understanding but
remains underexplored in existing multimodal benchmarks. In this paper, we
introduce \textbf{COVER} (\textbf{\underline{CO}}unterfactual
\textbf{\underline{V}}id\textbf{\underline{E}}o
\textbf{\underline{R}}easoning), a multidimensional multimodal benchmark that
systematically evaluates MLLMs across the abstract-concrete and
perception-cognition dimensions. Beyond prior multimodal benchmarks, COVER
decomposes complex queries into structured sub-questions, enabling fine-grained
reasoning analysis. Experiments on commercial and open-source models reveal a
strong correlation between sub-question accuracy and counterfactual reasoning
performance, highlighting the role of structured inference in video
understanding. Furthermore, our results suggest a key insight: enhancing the
reasoning capability of models is essential for improving the robustness of
video understanding. COVER establishes a new standard for assessing MLLMs'
logical reasoning abilities in dynamic environments. Our work is available at
https://github.com/gongyifan-hash/COVER-Benchmark.

</details>


### [338] [MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps](https://arxiv.org/pdf/2503.18223)
*Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumbül, Alexander Mathis, Devis Tuia*

Main category: cs.CV

TL;DR: MammAlps is a multimodal dataset for wildlife behavior monitoring, offering annotated video, audio, and segmentation maps to advance machine learning in ecology.


<details>
  <summary>Details</summary>
Motivation: The lack of annotated wildlife video datasets hinders the development of models for processing fieldwork data. MammAlps addresses this gap.

Method: The dataset includes 14 hours of video with audio, 2D segmentation maps, and 8.5 hours of labeled tracks. It proposes two benchmarks: hierarchical behavior recognition and ecology-oriented event identification.

Result: MammAlps provides 6135 single animal clips and 397 multi-view ecological events, enabling complementary tasks for machine learning and ecology.

Conclusion: MammAlps bridges the gap between machine learning and ecology by offering rich, annotated data for wildlife behavior monitoring.

Abstract: Monitoring wildlife is essential for ecology and ethology, especially in
light of the increasing human impact on ecosystems. Camera traps have emerged
as habitat-centric sensors enabling the study of wildlife populations at scale
with minimal disturbance. However, the lack of annotated video datasets limits
the development of powerful video understanding models needed to process the
vast amount of fieldwork data collected. To advance research in wild animal
behavior monitoring we present MammAlps, a multimodal and multi-view dataset of
wildlife behavior monitoring from 9 camera-traps in the Swiss National Park.
MammAlps contains over 14 hours of video with audio, 2D segmentation maps and
8.5 hours of individual tracks densely labeled for species and behavior. Based
on 6135 single animal clips, we propose the first hierarchical and multimodal
animal behavior recognition benchmark using audio, video and reference scene
segmentation maps as inputs. Furthermore, we also propose a second
ecology-oriented benchmark aiming at identifying activities, species, number of
individuals and meteorological conditions from 397 multi-view and long-term
ecological events, including false positive triggers. We advocate that both
tasks are complementary and contribute to bridging the gap between machine
learning and ecology. Code and data are available at:
https://github.com/eceo-epfl/MammAlps

</details>


### [339] [Two-stage deep learning framework for the restoration of incomplete-ring PET images](https://arxiv.org/pdf/2504.00816)
*Yeqi Fang, Rong Zhou*

Main category: cs.CV

TL;DR: A two-stage deep-learning framework improves image quality in incomplete-ring PET scanners without TOF, handling 50% missing data.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in incomplete-ring PET scanners due to data incompleteness and geometric inconsistencies.

Method: Two-stage pipeline: projection-domain Attention U-Net predicts missing sinogram sections, followed by OSEM reconstruction and U-Net-diffusion for artefact removal.

Result: Preserves anatomical structures and tracer distribution with PSNR 30.92 dB and SSIM 0.9708, achieving higher inference speed.

Conclusion: Provides an effective solution for incomplete-ring PET imaging with improved performance.

Abstract: Positron Emission Tomography (PET) is an important molecular imaging tool
widely used in medicine. Traditional PET systems rely on complete detector
rings for full angular coverage and reliable data collection. However,
incomplete-ring PET scanners have emerged due to hardware failures, cost
constraints, or specific clinical needs. Standard reconstruction algorithms
often suffer from performance degradation with these systems because of reduced
data completeness and geometric inconsistencies. We present a two-stage
deep-learning framework that, without incorporating any time-of-flight (TOF)
information, restores high-quality images from data with about 50% missing
coincidences - double the loss levels previously addressed by CNN-based
methods. The pipeline operates in two stages: a projection-domain Attention
U-Net first predicts the missing sections of the sinogram by leveraging spatial
context from neighbouring slices, after which the completed data are
reconstructed with OSEM algorithm and passed to a U-Net-diffusion module that
removes residual artefacts while reinstating high-frequency detail. Using 206
brain volumes from a public dataset, the result shows that our model
successfully preserves most anatomical structures and tracer distribution
features with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher
inference speed, thus providing an effective solution for incomplete-ring PET
imaging.

</details>


### [340] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/pdf/2504.16656)
*Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork R1V2 is a multimodal reasoning model with hybrid reinforcement learning, combining MPO and GRPO, and introduces SSB for training efficiency. It achieves top benchmark results and addresses challenges like visual hallucinations.


<details>
  <summary>Details</summary>
Motivation: To balance sophisticated reasoning with broad generalization and improve training efficiency in multimodal models.

Method: Uses hybrid reinforcement learning (MPO + GRPO) and SSB for sample prioritization, with calibrated reward thresholds to mitigate visual hallucinations.

Result: Achieves leading benchmarks: 62.6 (OlympiadBench), 78.9 (AIME2024), 63.6 (LiveCodeBench), 73.6 (MMMU). Outperforms open-source models and narrows the gap with proprietary systems.

Conclusion: Skywork R1V2 advances multimodal reasoning, offering superior performance and public release for openness.

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that jointly leverages the
Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization
(GRPO), which harmonizes reward-model guidance with rule-based strategies,
thereby addressing the long-standing challenge of balancing sophisticated
reasoning capabilities with broad generalization. To further enhance training
efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which
effectively addresses the vanishing advantages dilemma inherent in GRPO by
prioritizing high-value samples throughout the optimization process. Notably,
we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and
73.6 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>


### [341] [CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis](https://arxiv.org/pdf/2504.19223)
*Alexander Baumann, Leonardo Ayala, Silvia Seidlitz, Jan Sellner, Alexander Studier-Fischer, Berkin Özdemir, Lena Maier-Hein, Slobodan Ilic*

Main category: cs.CV

TL;DR: CARL is a camera-agnostic model for spectral imaging that addresses variability in channel dimensionality and wavelengths, using novel encoding and pre-training to improve cross-camera applicability.


<details>
  <summary>Details</summary>
Motivation: Variability in spectral cameras hinders AI-driven methodologies, leading to camera-specific models with limited generalizability.

Method: Introduces wavelength positional encoding and a self-attention-cross-attention mechanism for spectral image conversion, with spectral-spatial pre-training via a JEPA-inspired strategy.

Result: Outperforms in robustness to spectral heterogeneity across medical, autonomous driving, and satellite imaging domains.

Conclusion: CARL's scalability and versatility make it a potential backbone for future spectral foundation models.

Abstract: Spectral imaging offers promising applications across diverse domains,
including medicine and urban scene understanding, and is already established as
a critical modality in remote sensing. However, variability in channel
dimensionality and captured wavelengths among spectral cameras impede the
development of AI-driven methodologies, leading to camera-specific models with
limited generalizability and inadequate cross-camera applicability. To address
this bottleneck, we introduce $\textbf{CARL}$, a model for
$\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation
$\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging
modalities. To enable the conversion of a spectral image with any channel
dimensionality to a camera-agnostic embedding, we introduce wavelength
positional encoding and a self-attention-cross-attention mechanism to compress
spectral information into learned query representations. Spectral-spatial
pre-training is achieved with a novel spectral self-supervised JEPA-inspired
strategy tailored to CARL. Large-scale experiments across the domains of
medical imaging, autonomous driving, and satellite imaging demonstrate our
model's unique robustness to spectral heterogeneity, outperforming on datasets
with simulated and real-world cross-camera spectral variations. The scalability
and versatility of the proposed approach position our model as a backbone for
future spectral foundation models.

</details>


### [342] [Flow-GRPO: Training Flow Matching Models via Online RL](https://arxiv.org/pdf/2505.05470)
*Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, Wanli Ouyang*

Main category: cs.CV

TL;DR: Flow-GRPO integrates online RL into flow matching models using ODE-to-SDE conversion and Denoising Reduction, improving efficiency and performance in text-to-image tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and efficiency of flow matching models by incorporating online RL, enabling better exploration and sampling without compromising quality.

Method: Uses ODE-to-SDE conversion for RL exploration and Denoising Reduction to improve sampling efficiency while maintaining inference timesteps.

Result: Achieves significant improvements in text-to-image tasks, e.g., boosting GenEval accuracy from 63% to 95% and visual text rendering from 59% to 92%, with minimal reward hacking.

Conclusion: Flow-GRPO effectively integrates RL into flow matching, delivering high performance and efficiency without sacrificing image quality or diversity.

Abstract: We propose Flow-GRPO, the first method integrating online reinforcement
learning (RL) into flow matching models. Our approach uses two key strategies:
(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary
Differential Equation (ODE) into an equivalent Stochastic Differential Equation
(SDE) that matches the original model's marginal distribution at all timesteps,
enabling statistical sampling for RL exploration; and (2) a Denoising Reduction
strategy that reduces training denoising steps while retaining the original
inference timestep number, significantly improving sampling efficiency without
performance degradation. Empirically, Flow-GRPO is effective across multiple
text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly
perfect object counts, spatial relations, and fine-grained attributes, boosting
GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy
improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO
also achieves substantial gains in human preference alignment. Notably, very
little reward hacking occurred, meaning rewards did not increase at the cost of
appreciable image quality or diversity degradation.

</details>


### [343] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/pdf/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised training approach for crowd counting using Multi-Column CNN (MCNN) and a spatiotemporal model for anomaly detection, achieving superior performance on public datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in crowd counting (data annotation, occlusions, non-uniform density) and anomaly detection (lighting, scalability) to improve accuracy and efficiency.

Method: For crowd counting: self-supervised training with MCNN. For anomaly detection: VGG19-based spatiotemporal model with CNN for spatial features and LSTM for temporal features, enhanced by dense residual blocks.

Result: Improved performance on ShanghaiTech, UCFQNRF (crowd counting) and Hockey Fight, SCVD datasets (anomaly detection) with lower MAE and MSE.

Conclusion: The proposed models effectively tackle key challenges in crowd analysis, outperforming state-of-the-art methods.

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [344] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/pdf/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: Proposes MCSAD for federated domain generalization, combining multi-source style augmentation and domain-invariant learning to improve generalization on unseen domains.


<details>
  <summary>Details</summary>
Motivation: Existing style augmentation methods in federated domain generalization are limited by isolated or interpolated styles, restricting the style space.

Method: Introduces multi-source collaborative style augmentation and domain-invariant learning via cross-domain feature alignment and relation distillation.

Result: Outperforms state-of-the-art methods on multiple domain generalization datasets.

Conclusion: MCSAD effectively broadens style space and enhances domain generalization in decentralized settings.

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [345] [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/pdf/2505.16836)
*Fanrui Zhang, Dian Li, Qiang Zhang, Chenjun, sinbadliu, Junxiong Lin, Jiahong Yan, Jiawei Liu, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper introduces FakeVV, a large-scale dataset for video misinformation detection, and Fact-R1, a framework combining deep reasoning and reinforcement learning to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale datasets and overfitting issues in existing video misinformation detection methods.

Method: Proposes Fact-R1, a three-stage framework: misinformation CoT instruction tuning, DPO for preference alignment, and GRPO with a verifiable reward function.

Result: Fact-R1 achieves reasoning behaviors comparable to advanced text-based systems in multimodal settings.

Conclusion: The work sets a new paradigm for misinformation detection by integrating large-scale video understanding, reasoning, and interpretable verification.

Abstract: The rapid spread of multimodal misinformation on social media has raised
growing concerns, while research on video misinformation detection remains
limited due to the lack of large-scale, diverse datasets. Existing methods
often overfit to rigid templates and lack deep reasoning over deceptive
content. To address these challenges, we introduce FakeVV, a large-scale
benchmark comprising over 100,000 video-text pairs with fine-grained,
interpretable annotations. In addition, we further propose Fact-R1, a novel
framework that integrates deep reasoning with collaborative rule-based
reinforcement learning. Fact-R1 is trained through a three-stage process: (1)
misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference
alignment via Direct Preference Optimization (DPO), and (3) Group Relative
Policy Optimization (GRPO) using a novel verifiable reward function. This
enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those
observed in advanced text-based reinforcement learning systems, but in the more
complex multimodal misinformation setting. Our work establishes a new paradigm
for misinformation detection, bridging large-scale video understanding,
reasoning-guided alignment, and interpretable verification.

</details>


### [346] [Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation](https://arxiv.org/pdf/2505.20704)
*Zixuan Hu, Yichun Hu, Xiaotong Li, Shixiang Tang, Ling-Yu Duan*

Main category: cs.CV

TL;DR: ReCAP introduces a region-integrated method for Wild Test-Time Adaptation (WTTA), addressing optimization inefficiencies in entropy minimization by leveraging region confidence and probabilistic modeling.


<details>
  <summary>Details</summary>
Motivation: Existing WTTA methods focus on sample selection but neglect optimization dynamics, leading to inefficiencies. Entropy minimization, widely used, has noisy dynamics, while region confidence, though superior, is computationally prohibitive.

Method: ReCAP uses probabilistic region modeling to capture semantic changes and a finite-to-infinite asymptotic approximation to make region confidence tractable.

Result: ReCAP outperforms existing methods across datasets and wild scenarios, demonstrating superior adaptation efficiency.

Conclusion: ReCAP effectively addresses WTTA challenges by optimizing region confidence, offering a concise and efficient solution.

Abstract: Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to
unseen domains under extreme data scarcity and multiple shifts. Previous
approaches mainly focused on sample selection strategies, while overlooking the
fundamental problem on underlying optimization. Initially, we critically
analyze the widely-adopted entropy minimization framework in WTTA and uncover
its significant limitations in noisy optimization dynamics that substantially
hinder adaptation efficiency. Through our analysis, we identify region
confidence as a superior alternative to traditional entropy, however, its
direct optimization remains computationally prohibitive for real-time
applications. In this paper, we introduce a novel region-integrated method
ReCAP that bypasses the lengthy process. Specifically, we propose a
probabilistic region modeling scheme that flexibly captures semantic changes in
embedding space. Subsequently, we develop a finite-to-infinite asymptotic
approximation that transforms the intractable region confidence into a
tractable and upper-bounded proxy. These innovations significantly unlock the
overlooked potential dynamics in local region in a concise solution. Our
extensive experiments demonstrate the consistent superiority of ReCAP over
existing methods across various datasets and wild scenarios.

</details>


### [347] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/pdf/2505.21649)
*Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer*

Main category: cs.CV

TL;DR: DORI is a benchmark for evaluating object orientation understanding in vision-language models, revealing significant limitations in current systems.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks conflate object orientation with other visual tasks, so DORI isolates and evaluates this critical capability for applications like robotics and AR.

Method: DORI assesses four orientation dimensions using tasks from 11 datasets across synthetic and real-world scenarios, evaluating 15 state-of-the-art models.

Result: Models perform poorly, with 54.2% accuracy on coarse tasks and 33.0% on granular ones, struggling with reference frame shifts and compound rotations.

Conclusion: DORI highlights the need for better orientation representation in models, offering insights for robotics, 3D reconstruction, and human-AI interaction.

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [348] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/pdf/2505.22944)
*Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma*

Main category: cs.CV

TL;DR: A unified framework for motion control in video generation integrates camera movement, object translation, and local motion using trajectory inputs, outperforming prior methods in controllability and quality.


<details>
  <summary>Details</summary>
Motivation: Prior methods handle motion types separately or with task-specific designs, lacking a cohesive solution. This work aims to unify motion control for better flexibility and performance.

Method: Projects user-defined trajectories into the latent space of pre-trained video models via a lightweight motion injector, enabling control over deformations, object motion, and camera dynamics.

Result: Superior performance in tasks like stylized motion, viewpoint changes, and local motion manipulation, with better controllability and visual quality than existing methods.

Conclusion: The framework offers a versatile, high-quality solution for motion control in video generation, compatible with various state-of-the-art backbones.

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [349] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/pdf/2505.23161)
*Antonio D'Orazio, Maria Rosaria Briglia, Donato Crisostomi, Dario Loi, Emanuele Rodolà, Iacopo Masi*

Main category: cs.CV

TL;DR: CLIP, a discriminative model, can generate images without a decoder or training by optimizing a frequency-aware neural representation and using stabilization techniques.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped generative potential of discriminative models like CLIP without modifying its weights.

Method: Uses a frequency-aware implicit neural representation, adversarially robust initialization, Orthogonal Procrustes projection, and a blending loss to stabilize inverse mapping.

Result: Enables text-to-image generation, style transfer, and image reconstruction without altering CLIP.

Conclusion: Discriminative models like CLIP may have hidden generative capabilities.

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [350] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/pdf/2505.23637)
*Dashti A. Ali, Richard K. G. Do, William R. Jarnagin, Aras T. Asaad, Amber L. Simpson*

Main category: cs.CV

TL;DR: The paper compares two methods for constructing topological feature vectors from persistence barcodes in medical image analysis, finding that concatenation outperforms aggregation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional feature extraction in medical image analysis, the study leverages persistent homology for robust feature representation.

Method: The study compares two approaches: aggregating persistence barcodes followed by featurization versus concatenating topological feature vectors from individual barcodes.

Result: Feature concatenation preserves detailed topological information and yields better classification performance.

Conclusion: Concatenating topological feature vectors is the preferred approach for similar experiments in medical image analysis.

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [351] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/pdf/2505.24371)
*Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi*

Main category: cs.CV

TL;DR: Grid-LoGAT system for VideoQA uses grid-based visual prompting and edge-cloud deployment to enhance transcript quality and privacy, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve VideoQA accuracy while ensuring image privacy by leveraging edge-cloud deployment and grid-based visual prompting.

Method: Two-phase system: VLM extracts text transcripts from video frames, LLM processes questions using transcripts. Grid-based visual prompting enhances local and global details.

Result: Outperforms state-of-the-art methods on NExT-QA (65.9%) and STAR-QA (50.11%) datasets, with a 24-point improvement on localization questions.

Conclusion: Grid-LoGAT is effective for VideoQA, balancing privacy and performance through innovative grid-based prompting and edge-cloud architecture.

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA. (This paper is accepted by IEEE ICIP 2025.)

</details>


### [352] [FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation](https://arxiv.org/pdf/2506.01144)
*Ariel Shaulov, Itay Hazan, Lior Wolf, Hila Chefer*

Main category: cs.CV

TL;DR: FlowMo is a training-free method to improve motion coherence in text-to-video diffusion models by leveraging the model's own predictions without retraining or external inputs.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models struggle with temporal aspects like motion and physics, often requiring retraining or external signals. FlowMo explores extracting temporal coherence directly from pre-trained models.

Method: FlowMo derives a temporal representation from latent distances between frames, estimates motion coherence via patch-wise variance, and dynamically guides the model to reduce variance during sampling.

Result: FlowMo significantly enhances motion coherence in pre-trained models without compromising visual quality or prompt alignment.

Conclusion: FlowMo provides an effective, plug-and-play solution for improving temporal fidelity in video diffusion models without additional training.

Abstract: Text-to-video diffusion models are notoriously limited in their ability to
model temporal aspects such as motion, physics, and dynamic interactions.
Existing approaches address this limitation by retraining the model or
introducing external conditioning signals to enforce temporal consistency. In
this work, we explore whether a meaningful temporal representation can be
extracted directly from the predictions of a pre-trained model without any
additional training or auxiliary inputs. We introduce FlowMo, a novel
training-free guidance method that enhances motion coherence using only the
model's own predictions in each diffusion step. FlowMo first derives an
appearance-debiased temporal representation by measuring the distance between
latents corresponding to consecutive frames. This highlights the implicit
temporal structure predicted by the model. It then estimates motion coherence
by measuring the patch-wise variance across the temporal dimension and guides
the model to reduce this variance dynamically during sampling. Extensive
experiments across multiple text-to-video models demonstrate that FlowMo
significantly improves motion coherence without sacrificing visual quality or
prompt alignment, offering an effective plug-and-play solution for enhancing
the temporal fidelity of pre-trained video diffusion models.

</details>


### [353] [Moving Beyond Discrete Categories: Continuous Demographic Labels for Fair Facial Recognition](https://arxiv.org/pdf/2506.01532)
*Pedro C. Neto, Naser Damer, Jaime S. Cardoso, Ana F. Sequeira*

Main category: cs.CV

TL;DR: The paper proposes treating ethnicity labels as continuous variables to better address bias in face recognition models, showing improved performance over discrete labeling.


<details>
  <summary>Details</summary>
Motivation: Bias in face recognition models persists due to limited approaches in mitigating data bias, particularly in how ethnicity labels are used.

Method: Ethnicity labels are revised as continuous variables, validated experimentally and theoretically, with models trained on datasets balanced in this continuous space.

Result: Models trained on continuously balanced datasets outperform those using discrete labels, supported by extensive experimentation (65+ models, 20+ dataset subsets).

Conclusion: Treating ethnicity as a continuous variable provides a more effective way to balance datasets and reduce bias in face recognition models.

Abstract: Bias has been a constant in face recognition models. Over the years,
researchers have looked at it from both the model and the data point of view.
However, their approach to mitigation of data bias was limited and lacked
insight on the real nature of the problem. Here, in this document, we propose
to revise our use of ethnicity labels as a continuous variable instead of a
discrete value per identity. We validate our formulation both experimentally
and theoretically, showcasing that not all identities from one ethnicity
contribute equally to the balance of the dataset; thus, having the same number
of identities per ethnicity does not represent a balanced dataset. We further
show that models trained on datasets balanced in the continuous space
consistently outperform models trained on data balanced in the discrete space.
We trained more than 65 different models, and created more than 20 subsets of
the original datasets.

</details>


### [354] [MedEBench: Revisiting Text-instructed Image Editing on Medical Domain](https://arxiv.org/pdf/2506.01921)
*Minghao Liu, Zhitao He, Zhiyuan Fan, Qingyun Wang, Yi R. Fung*

Main category: cs.CV

TL;DR: MedEBench is a benchmark for evaluating text-guided medical image editing, addressing the lack of standardized evaluation in this domain. It includes 1,182 image-prompt triplets, evaluates seven models, and introduces a failure analysis protocol.


<details>
  <summary>Details</summary>
Motivation: To standardize and improve text-guided medical image editing for clinical applications like surgical simulation, teaching, and patient communication.

Method: Introduces MedEBench, a benchmark with clinically sourced image-prompt triplets, evaluation metrics (Editing Accuracy, Contextual Preservation, Visual Quality), and a failure analysis protocol using attention grounding.

Result: Systematic comparison of seven models reveals common failure patterns, with attention grounding used to identify mislocalization issues.

Conclusion: MedEBench provides a foundation for developing reliable, clinically meaningful medical image editing systems.

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce MedEBench, a comprehensive
benchmark for evaluating text-guided medical image editing. It consists of
1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13
anatomical regions. MedEBench offers three key contributions: (1) a clinically
relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems. Project website:
https://mliuby.github.io/MedEBench_Website/

</details>


### [355] [SAB3R: Semantic-Augmented Backbone in 3D Reconstruction](https://arxiv.org/pdf/2506.02112)
*Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng*

Main category: cs.CV

TL;DR: The paper introduces 'Map and Locate,' a task combining open-vocabulary segmentation and 3D reconstruction, and proposes SAB3R, a unified model outperforming separate methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between open-vocabulary segmentation and 3D reconstruction for embodied AI applications.

Method: SAB3R builds on MASt3R, using lightweight distillation to transfer 2D semantic features (e.g., CLIP, DINOv2) for cohesive point maps in one pass.

Result: SAB3R outperforms separate MASt3R and CLIP deployments on the Map and Locate benchmark and excels in 2D and 3D tasks.

Conclusion: SAB3R effectively unifies segmentation and reconstruction, advancing embodied AI with practical applications.

Abstract: We introduce a new task, Map and Locate, which unifies the traditionally
distinct objectives of open-vocabulary segmentation - detecting and segmenting
object instances based on natural language queries - and 3D reconstruction, the
process of estimating a scene's 3D structure from visual inputs. Specifically,
Map and Locate involves generating a point cloud from an unposed video and
segmenting object instances based on open-vocabulary queries. This task serves
as a critical step toward real-world embodied AI applications and introduces a
practical task that bridges reconstruction, recognition and reorganization. To
tackle this task, we introduce a simple yet effective baseline, which we denote
as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer
vision, and incorporates a lightweight distillation strategy. This method
transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP
and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary
frozen networks, our model generates per-pixel semantic features and constructs
cohesive point maps in a single forward pass. Compared to separately deploying
MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the
Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic
segmentation and 3D tasks to comprehensively validate its effectiveness.

</details>


### [356] [Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation](https://arxiv.org/pdf/2506.02294)
*Niclas Popp, Kevin Alexander Laube, Matthias Hein, Lukas Schott*

Main category: cs.CV

TL;DR: The paper proposes a diffusion-based data augmentation method to improve knowledge distillation under covariate shift, enhancing student model robustness to spurious features.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of knowledge distillation effectiveness due to covariate shift and unknown spurious features, leveraging a robust teacher to improve student robustness.

Method: Introduces a diffusion-based data augmentation strategy that generates challenging samples by maximizing teacher-student disagreement.

Result: Significant improvements in worst-group and mean-group accuracy on datasets like CelebA and SpuCo Birds, and spurious mAUC on ImageNet under covariate shift.

Conclusion: The proposed method outperforms existing diffusion-based data augmentation techniques, demonstrating its effectiveness in handling covariate shift in knowledge distillation.

Abstract: Large foundation models trained on extensive datasets demonstrate strong
zero-shot capabilities in various domains. To replicate their success when data
and model size are constrained, knowledge distillation has become an
established tool for transferring knowledge from foundation models to small
student networks. However, the effectiveness of distillation is critically
limited by the available training data. This work addresses the common
practical issue of covariate shift in knowledge distillation, where spurious
features appear during training but not at test time. We ask the question: when
these spurious features are unknown, yet a robust teacher is available, is it
possible for a student to also become robust to them? We address this problem
by introducing a novel diffusion-based data augmentation strategy that
generates images by maximizing the disagreement between the teacher and the
student, effectively creating challenging samples that the student struggles
with. Experiments demonstrate that our approach significantly improves worst
group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious
mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art
diffusion-based data augmentation baselines

</details>


### [357] [InterRVOS: Interaction-aware Referring Video Object Segmentation](https://arxiv.org/pdf/2506.02356)
*Woojeong Jin, Seongchan Kim, Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces Interaction-aware referring video object segmentation (InterRVOS), a new task focusing on segmenting interacting objects in videos using complementary natural language expressions. It presents a dataset (InterRVOS-8K) and a baseline model (ReVIOSa) for this task, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing referring video object segmentation methods focus on single objects, ignoring interactions between entities, which are crucial for comprehensive video understanding.

Method: Proposes InterRVOS-8K, a large-scale dataset with interaction-aware expressions, and ReVIOSa, a baseline model for actor-target segmentation from a single expression.

Result: ReVIOSa outperforms prior methods in modeling complex object interactions, demonstrating strong performance in both standard and interaction-focused settings.

Conclusion: The work establishes a foundation for interaction-centric video understanding, with potential for future research in this direction.

Abstract: Referring video object segmentation aims to segment the object in a video
corresponding to a given natural language expression. While prior works have
explored various referring scenarios, including motion-centric or
multi-instance expressions, most approaches still focus on localizing a single
target object in isolation. However, in comprehensive video understanding, an
object's role is often defined by its interactions with other entities, which
are largely overlooked in existing datasets and models. In this work, we
introduce Interaction-aware referring video object sgementation (InterRVOS), a
new task that requires segmenting both actor and target entities involved in an
interaction. Each interactoin is described through a pair of complementary
expressions from different semantic perspectives, enabling fine-grained
modeling of inter-object relationships. To tackle this task, we propose
InterRVOS-8K, the large-scale and automatically constructed dataset containing
diverse interaction-aware expressions with corresponding masks, including
challenging cases such as motion-only multi-instance expressions. We also
present a baseline architecture, ReVIOSa, designed to handle actor-target
segmentation from a single expression, achieving strong performance in both
standard and interaction-focused settings. Furthermore, we introduce an
actor-target-aware evalaution setting that enables a more targeted assessment
of interaction understanding. Experimental results demonstrate that our
approach outperforms prior methods in modeling complex object interactions for
referring video object segmentation task, establishing a strong foundation for
future research in interaction-centric video understanding. Our project page is
available at https://cvlab-kaist.github.io/InterRVOS.

</details>


### [358] [SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios](https://arxiv.org/pdf/2506.02444)
*Lingwei Dang, Ruizhi Shao, Hongwen Zhang, Wei Min, Yebin Liu, Qingyao Wu*

Main category: cs.CV

TL;DR: A novel framework combines visual priors and dynamic constraints in a synchronized diffusion process to generate Hand-Object Interaction (HOI) videos and motion simultaneously, enhancing consistency and generalization.


<details>
  <summary>Details</summary>
Motivation: Current HOI methods rely on predefined 3D models or sacrifice physical plausibility for visual fidelity, limiting generalization.

Method: Uses tri-modal adaptive modulation for feature alignment and 3D full-attention for dependencies. Introduces a vision-aware 3D interaction diffusion model for closed-loop feedback.

Result: Outperforms state-of-the-art in generating high-fidelity, physically plausible HOI sequences with strong generalization.

Conclusion: The framework eliminates dependencies on predefined models and enhances video-motion consistency, showing superior performance in real-world scenarios.

Abstract: Hand-Object Interaction (HOI) generation has significant application
potential. However, current 3D HOI motion generation approaches heavily rely on
predefined 3D object models and lab-captured motion data, limiting
generalization capabilities. Meanwhile, HOI video generation methods prioritize
pixel-level visual fidelity, often sacrificing physical plausibility.
Recognizing that visual appearance and motion patterns share fundamental
physical laws in the real world, we propose a novel framework that combines
visual priors and dynamic constraints within a synchronized diffusion process
to generate the HOI video and motion simultaneously. To integrate the
heterogeneous semantics, appearance, and motion features, our method implements
tri-modal adaptive modulation for feature aligning, coupled with 3D
full-attention for modeling inter- and intra-modal dependencies. Furthermore,
we introduce a vision-aware 3D interaction diffusion model that generates
explicit 3D interaction sequences directly from the synchronized diffusion
outputs, then feeds them back to establish a closed-loop feedback cycle. This
architecture eliminates dependencies on predefined object models or explicit
pose guidance while significantly enhancing video-motion consistency.
Experimental results demonstrate our method's superiority over state-of-the-art
approaches in generating high-fidelity, dynamically plausible HOI sequences,
with notable generalization capabilities in unseen real-world scenarios.
Project page at https://github.com/Droliven/SViMo\_project.

</details>


### [359] [MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection](https://arxiv.org/pdf/2506.02535)
*Juntong Li, Lingwei Dang, Yukun Su, Yun Hao, Qingxin Xiao, Yongwei Nie, Qingyao Wu*

Main category: cs.CV

TL;DR: The paper proposes a novel Video Anomaly Detection (VAD) framework addressing generalization and semantic limitations by introducing Sparse Feature Filtering Module (SFFM) and integrating a Vision-Language Model (VLM).


<details>
  <summary>Details</summary>
Motivation: Existing VAD methods struggle with distinguishing anomalies due to excessive generalization and lack of high-level semantic understanding.

Method: The framework includes SFFM for adaptive abnormal feature filtering and MoE architecture for diverse feature extraction. It also integrates VLM for semantic-textual descriptions and enforces modality consistency.

Result: Experiments on public datasets confirm the effectiveness of the multimodal joint modeling and sparse feature filtering approach.

Conclusion: The proposed framework improves VAD by addressing generalization and semantic challenges, validated through extensive testing.

Abstract: Video Anomaly Detection (VAD) methods based on reconstruction or prediction
face two critical challenges: (1) strong generalization capability often
results in accurate reconstruction or prediction of abnormal events, making it
difficult to distinguish normal from abnormal patterns; (2) reliance only on
low-level appearance and motion cues limits their ability to identify
high-level semantic in abnormal events from complex scenes. To address these
limitations, we propose a novel VAD framework with two key innovations. First,
to suppress excessive generalization, we introduce the Sparse Feature Filtering
Module (SFFM) that employs bottleneck filters to dynamically and adaptively
remove abnormal information from features. Unlike traditional memory modules,
it does not need to memorize the normal prototypes across the training dataset.
Further, we design the Mixture of Experts (MoE) architecture for SFFM. Each
expert is responsible for extracting specialized principal features during
running time, and different experts are selectively activated to ensure the
diversity of the learned principal features. Second, to overcome the neglect of
semantics in existing methods, we integrate a Vision-Language Model (VLM) to
generate textual descriptions for video clips, enabling comprehensive joint
modeling of semantic, appearance, and motion cues. Additionally, we enforce
modality consistency through semantic similarity constraints and motion
frame-difference contrastive loss. Extensive experiments on multiple public
datasets validate the effectiveness of our multimodal joint modeling framework
and sparse feature filtering paradigm. Project page at
https://qzfm.github.io/sfn_vad_project_page/.

</details>


### [360] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/pdf/2506.02614)
*Guohang Zhuang, Weixi Song, Jinyang Huang, Chenwei Yang, Yan Lu*

Main category: cs.CV

TL;DR: A deep learning-based Space Debris Tracking Network (SDT-Net) is proposed for accurate debris tracking, supported by a large-scale synthetic dataset (SDTD). The model achieves strong performance in real-world tests.


<details>
  <summary>Details</summary>
Motivation: Space debris poses a significant threat, but existing methods fail to handle complex backgrounds and dense debris effectively.

Method: Proposes SDT-Net, a deep learning model for debris tracking, and introduces SDTD, a synthetic dataset for training and evaluation.

Result: SDT-Net achieves a MOTA score of 70.6% on real-world data from the Antarctic Station, demonstrating strong transferability.

Conclusion: The proposed model and dataset effectively address the challenge of space debris tracking, with potential for real-world application.

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [361] [FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition](https://arxiv.org/pdf/2506.02695)
*Linquan Wu, Tianxiang Jiang, Wenhao Duan, Yini Fang, Jacky Keung*

Main category: cs.CV

TL;DR: FaceSleuth introduces a dual-stream architecture for micro-expression recognition, enhancing vertical motion and localizing signals, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition requires amplifying subtle facial motions while suppressing identity-specific features, which existing models struggle with.

Method: FaceSleuth uses a Continuously Vertical Attention (CVA) block, Facial Position Focalizer, and Action-Unit embeddings. It also proposes a Single-Orientation Attention (SOA) module to validate the vertical axis.

Result: FaceSleuth achieves 95.1% accuracy on CASME II, 87.1% on SAMM, and 92.9% on MMEW, outperforming previous methods.

Conclusion: FaceSleuth sets a new benchmark for MER, empirically proving vertical attention bias as the most discriminative orientation.

Abstract: Micro-expression recognition (MER) demands models that can amplify
millisecond-level, low-amplitude facial motions while suppressing
identity-specific appearance. We introduce FaceSleuth, a dual-stream
architecture that (1) enhances motion along the empirically dominant vertical
axix through a Continuously Vertical Attention (CVA) block, (2) localises the
resulting signals with a Facial Position Focalizer built on hierarchical
cross-window attention, and (3) steers feature learning toward physiologically
meaningful regions via lightweight Action-Unit embeddings. To examine whether
the hand-chosen vertical axis is indeed optimal, we further propose a
Single-Orientation Attention (SOA) module that learns its own pooling direction
end-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses
to CVA when the learned angle converges to {\Pi}/2. In practice, SOA reliably
drifts to 88{\deg}, confirming the effectiveness of the vertical prior while
delivering consistent gains. On three standard MER benchmarks, FaceSleuth with
CVA already surpasses previous state-of-the-art methods; plugging in SOA lifts
accuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840
on SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.
These results establish a new state of the art and, for the first time, provide
empirical evidence that the vertical attention bias is the most discriminative
orientation for MER.

</details>


### [362] [Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](https://arxiv.org/pdf/2506.02738)
*Negin Baghbanzadeh, Sajad Ashkezari, Elham Dolatabadi, Arash Afkanpour*

Main category: cs.CV

TL;DR: The paper introduces a scalable subfigure extraction pipeline for biomedical compound figures, releasing a large dataset (OPEN-PMC-18M) and showing improved vision-language model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in large-scale subfigure extraction and its impact on vision-language models in biomedical literature.

Method: A transformer-based object detection pipeline trained on 500,000 synthetic compound figures, applied to create OPEN-PMC-18M.

Result: State-of-the-art performance on benchmarks and improved vision-language model tasks like retrieval and zero-shot classification.

Conclusion: The work advances biomedical vision-language modeling, with released resources for reproducibility and further research.

Abstract: Compound figures, which are multi-panel composites containing diverse
subfigures, are ubiquitous in biomedical literature, yet large-scale subfigure
extraction remains largely unaddressed. Prior work on subfigure extraction has
been limited in both dataset size and generalizability, leaving a critical open
question: How does high-fidelity image-text alignment via large-scale subfigure
extraction impact representation learning in vision-language models? We address
this gap by introducing a scalable subfigure extraction pipeline based on
transformer-based object detection, trained on a synthetic corpus of 500,000
compound figures, and achieving state-of-the-art performance on both ImageCLEF
2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a
large-scale high quality biomedical vision-language dataset comprising 18
million clinically relevant subfigure-caption pairs spanning radiology,
microscopy, and visible light photography. We train and evaluate
vision-language models on our curated datasets and show improved performance
across retrieval, zero-shot classification, and robustness benchmarks,
outperforming existing baselines. We release our dataset, models, and code to
support reproducible benchmarks and further study into biomedical
vision-language modeling and representation learning.

</details>


### [363] [Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments](https://arxiv.org/pdf/2506.02845)
*Di Wen, Lei Qi, Kunyu Peng, Kailun Yang, Fei Teng, Ao Luo, Jia Fu, Yufan Chen, Ruiping Liu, Yitian Shi, M. Saquib Sarfraz, Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: MicroG-4M is the first benchmark dataset for video understanding in microgravity, featuring 4,759 clips, 50 actions, and multiple tasks like action recognition and visual question answering.


<details>
  <summary>Details</summary>
Motivation: Existing video datasets lack microgravity conditions, which are crucial for safety-critical space applications, highlighting the need for domain-robust video understanding.

Method: The dataset is constructed from real-world space missions and cinematic simulations, supporting tasks like action recognition, video captioning, and visual question answering.

Result: MicroG-4M includes 4,759 clips, 50 actions, 1,238 captions, and 7,000 QA pairs, with baselines established using state-of-the-art models.

Conclusion: MicroG-4M fills a critical gap in video understanding for microgravity, enabling comprehensive evaluation of spatial and semantic reasoning in space-related contexts.

Abstract: Despite substantial progress in video understanding, most existing datasets
are limited to Earth's gravitational conditions. However, microgravity alters
human motion, interactions, and visual semantics, revealing a critical gap for
real-world vision systems. This presents a challenge for domain-robust video
understanding in safety-critical space applications. To address this, we
introduce MicroG-4M, the first benchmark for spatio-temporal and semantic
understanding of human activities in microgravity. Constructed from real-world
space missions and cinematic simulations, the dataset includes 4,759 clips
covering 50 actions, 1,238 context-rich captions, and over 7,000
question-answer pairs on astronaut activities and scene understanding.
MicroG-4M supports three core tasks: fine-grained multi-label action
recognition, temporal video captioning, and visual question answering, enabling
a comprehensive evaluation of both spatial localization and semantic reasoning
in microgravity contexts. We establish baselines using state-of-the-art models.
All data, annotations, and code are available at
https://github.com/LEI-QI-233/HAR-in-Space.

</details>


### [364] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/pdf/2506.02896)
*Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieliński, Maciej Wołczyk*

Main category: cs.CV

TL;DR: VLMs struggle with active exploration in complex 3D environments, with performance gaps widening as tasks get harder. FlySearch benchmark highlights key failure causes, some addressable by finetuning.


<details>
  <summary>Details</summary>
Motivation: Assess whether Vision-Language Models (VLMs) can effectively perform goal-driven exploration in unstructured, real-world-like 3D environments.

Method: Introduce FlySearch, a 3D photorealistic environment, and test VLMs on three difficulty levels of exploration tasks. Analyze failure causes like vision hallucination and task planning.

Result: State-of-the-art VLMs fail even simple tasks, with performance gaps to humans increasing with difficulty. Some issues can be mitigated by finetuning.

Conclusion: VLMs currently lack robustness for active exploration in complex settings, but benchmarks like FlySearch can guide improvements.

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [365] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/pdf/2506.03147)
*Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan*

Main category: cs.CV

TL;DR: UniWorld, a unified generative framework, leverages semantic features from multimodal LLMs and contrastive encoders, outperforming traditional VAE-based methods in image tasks with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing unified models lack advanced image perception and manipulation capabilities, despite growing demand. GPT-4o-Image's success inspired a shift from VAEs to semantic encoders.

Method: Proposes UniWorld, using semantic features from multimodal LLMs and contrastive encoders, trained on 2.7M data points.

Result: Achieves strong performance in image understanding, generation, manipulation, and perception.

Conclusion: UniWorld is open-sourced to encourage reproducibility and further research in unified image tasks.

Abstract: Although existing unified models achieve strong performance in
vision-language understanding and text-to-image generation, they remain limited
in addressing image perception and manipulation -- capabilities increasingly
demanded in practical applications. Recently, OpenAI introduced the powerful
GPT-4o-Image model, which showcases advanced capabilities in comprehensive
image perception and manipulation, sparking widespread interest. Through
carefully designed experiments, we observe that GPT-4o-Image likely relies on
semantic encoders rather than VAEs for feature extraction, despite VAEs being
commonly regarded as crucial for image manipulation tasks. Inspired by this
insight, we propose UniWorld, a unified generative framework built upon
semantic features extracted from powerful multimodal large language models and
contrastive semantic encoders. Using only 2.7M training data, UniWorld achieves
impressive performance across diverse tasks, including image understanding,
generation, manipulation, and perception. We fully open-source the UniWorld
framework, including model weights, training and evaluation scripts, and
datasets to promote reproducibility and further research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [366] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/pdf/2506.03205)
*Umberto Gonçalves de Sousa*

Main category: cs.AI

TL;DR: Q-ARDNS-Multi is a multi-agent quantum reinforcement learning framework integrating quantum circuits, meta-cognition, and coordination, outperforming MADDPG and SAC in complex 3D environments.


<details>
  <summary>Details</summary>
Motivation: To bridge quantum computing, cognitive science, and multi-agent RL for scalable, human-like decision-making in dynamic settings like robotics and navigation.

Method: Uses 2-qubit quantum circuits, dual-memory systems, shared memory for cooperation, and adaptive exploration strategies. Evaluated in a 3D GridWorld with two agents.

Result: Achieves 99.6% and 99.5% success rates for agents, outperforming MADDPG and SAC in success rate, stability, and efficiency.

Conclusion: Q-ARDNS-Multi provides a robust, scalable solution for complex multi-agent tasks, leveraging quantum and cognitive principles.

Abstract: This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [367] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/pdf/2506.03233)
*Andrea Ferrario*

Main category: cs.AI

TL;DR: The paper challenges the orthodox view that AI systems lack metaphysical identity by proposing a theory based on trustworthiness profiles to define their kinds and persistence.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored metaphysical foundations of AI systems, countering the claim that they lack identity and persistence conditions.

Method: Introduces identity criteria for AI systems by linking their functional requirements to physical make-ups, using trustworthiness profiles as a lens.

Result: AI systems' identity and persistence are determined by their trustworthiness profiles, which reflect their capabilities and effectiveness over time.

Conclusion: The theory provides a metaphysical foundation for AI systems, linking their identity to socio-technical contexts and supporting broader epistemological, ethical, and legal discussions.

Abstract: Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [368] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/pdf/2506.03543)
*Wanghao Ye, Sihan Chen, Yiting Wang, Shwai He, Bowei Tian, Guoheng Sun, Ziyi Wang, Ziyao Wang, Yexiao He, Zheyu Shen, Meng Liu, Yuning Zhang, Meng Feng, Yang Wang, Siyuan Peng, Yilong Dai, Zhenle Duan, Hanzhang Qin, Ang Li*

Main category: cs.AI

TL;DR: The paper introduces a computational implementation of Global Workspace Theory (GNWT) to enhance LLM agents with human-like psychological processes, including a novel personality test and the CogniPair platform for realistic social interactions.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack authentic human psychological processes, limiting their use in digital twins and social AI applications.

Method: The study integrates GNWT into LLM agents, creating specialized sub-agents and an adventure-based personality test for accurate initialization. The CogniPair platform simulates dating and job interviews.

Result: Validation shows 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human studies.

Conclusion: The work advances psychological authenticity in LLM agents, supporting applications in dating platforms and HR technology.

Abstract: Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [369] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/pdf/2506.03315)
*Kai Sauerwald, Kenneth Skiba, Eduardo Fermé, Thomas Meyer*

Main category: cs.AI

TL;DR: The paper explores constructing choice functions using linear orders for restricted sets of alternatives, showing feasibility even with fallback values. It presents axiomatics for general and union-closed cases, with applications in knowledge representation, theory change, and argumentation.


<details>
  <summary>Details</summary>
Motivation: To address scenarios where choice functions cannot be constructed via relations on alternatives due to restricted sets, the study aims to demonstrate the viability of using linear orders on sets of alternatives, including fallback values.

Method: The paper employs linear orders on sets of alternatives to construct choice functions, even with fallback values as minimal elements. It also provides axiomatics for general and union-closed input restrictions.

Result: The study proves that choice functions can always be constructed via linear orders on sets of alternatives, including fallback values, and presents axiomatics for such functions.

Conclusion: The findings highlight the utility of linear orders in restricted choice settings, with applications in knowledge representation, theory change, and abstract argumentation.

Abstract: We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [370] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/pdf/2506.03332)
*Yifei Ming, Zixuan Ke, Xuan-Phi Nguyen, Jiayu Wang, Shafiq Joty*

Main category: cs.AI

TL;DR: The paper analyzes vulnerabilities in agentic workflows where LLM judges provide feedback, introducing a framework and benchmark (WAFER-QA) to evaluate robustness against deceptive feedback.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of judges in feedback-driven workflows, which can hallucinate, be biased, or adversarial, undermining system stability.

Method: A two-dimensional framework categorizes judge behavior by intent and knowledge, followed by creating a benchmark with grounded critiques to test workflow robustness.

Result: Strong agents are vulnerable to persuasive but flawed critiques, often changing correct answers after misleading feedback. Reasoning and non-reasoning models show distinct behavioral patterns.

Conclusion: Feedback-based workflows have fundamental vulnerabilities; the study provides insights for building more robust agentic systems.

Abstract: Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [371] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/pdf/2506.03469)
*Tuan Le, Risal Shefin, Debashis Gupta, Thai Le, Sarra Alqahtani*

Main category: cs.AI

TL;DR: A hybrid framework combining explainability, model checking, and risk-guided falsification ensures RL policy safety with interpretability and formal guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of formal verification (abstraction quality, dataset coverage) in ensuring RL policy safety in high-stakes environments.

Method: Uses CAPS for interpretable policy abstraction, Storm for model checking, and risk-guided falsification for coverage. Includes a safety shield for runtime mitigation.

Result: Provides verifiable safety guarantees, interpretable counterexamples, and PAC-style guarantees on violation detection.

Conclusion: The framework balances rigor and coverage, enhancing RL policy safety with interpretability and runtime safeguards.

Abstract: Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [372] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/pdf/2506.03828)
*Dhaval Patel, Shuxin Lin, James Rayfield, Nianjun Zhou, Roman Vaculin, Natalia Martinez, Fearghal O'donncha, Jayant Kalagnanam*

Main category: cs.AI

TL;DR: The paper proposes AssetOpsBench, a framework for developing AI agents to automate end-to-end industrial asset lifecycle management, leveraging LLMs and AI agents for holistic solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional AI/ML approaches address industrial tasks in isolation, lacking end-to-end automation. The paper aims to bridge this gap by enabling autonomous management of complex workflows.

Method: Introduces AssetOpsBench, a unified framework for developing domain-specific AI agents that integrate perception, reasoning, and control for Industry 4.0.

Result: AssetOpsBench provides a practical environment for orchestrating and evaluating AI agents in industrial settings, with software available on GitHub.

Conclusion: The paper envisions AI agents autonomously managing industrial workflows, reducing human workload and downtime, with AssetOpsBench as a foundational tool.

Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [373] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/pdf/2506.03503)
*Shan Shan*

Main category: cs.AI

TL;DR: The paper proposes a quantum-mechanics and Generative AI framework to simulate social norms, addressing gaps in applying quantum principles to social systems.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in system-level applications of quantum principles in social science, moving beyond micro-cognitive models or philosophical analogies.

Method: Combines quantum mechanics (superposition, entanglement, probabilistic measurement) with Generative AI to simulate social norms using 25 generative agents in five ideal-type experiments.

Result: Reveals emergent patterns like normative convergence, resistance spread, and new equilibria, demonstrating quantum principles' utility in modeling social uncertainty and interdependence.

Conclusion: Introduces a novel computational lens for quantum-informed social theory, enabling dynamic simulation and redesign of societal systems.

Abstract: The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [374] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/pdf/2506.03548)
*Chenglong Ye, Gang Xiong, Junyou Shang, Xingyuan Dai, Xiaoyan Gong, Yisheng Lv*

Main category: cs.AI

TL;DR: SUMO-MCP simplifies SUMO traffic simulation by wrapping core utilities and adding auxiliary tools, enabling natural-language prompts for tasks like scenario generation, demand creation, and congestion detection.


<details>
  <summary>Details</summary>
Motivation: Traffic simulation tools like SUMO are complex, requiring manual workflows for setup and analysis, which SUMO-MCP aims to streamline.

Method: SUMO-MCP integrates SUMO's utilities into a unified platform, offering natural-language prompts and flexible custom workflows without coding.

Result: Experiments show SUMO-MCP enhances accessibility and reliability of traffic simulation for researchers.

Conclusion: SUMO-MCP effectively simplifies and improves traffic simulation workflows, with future code release planned.

Abstract: Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [375] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/pdf/2506.03586)
*Yu Ma, Chongtao Guo, Le Liang, Xiao Li, Shi Jin*

Main category: cs.AI

TL;DR: A hybrid DRL approach optimizes RIS phase shifts and subcarrier allocation in OFDM systems to reduce average delay, leveraging PPO algorithms and multi-agent strategies for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the stochastic packet arrivals and optimize delay in RIS-assisted OFDM systems, which is modeled as an MDP.

Method: Uses hybrid DRL (PPO-Θ for RIS phase shifts, PPO-N for subcarrier allocation) with multi-agent strategies and transfer learning.

Result: Significantly reduces average delay, improves resource allocation efficiency, and enhances system robustness and fairness.

Conclusion: The proposed method outperforms baselines, demonstrating effectiveness in handling complex network dynamics.

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [376] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/pdf/2506.03610)
*Dongmin Park, Minkyu Kim, Beongjun Choi, Junhyuck Kim, Keon Lee, Jonghyun Lee, Inkyu Park, Byeong-Uk Lee, Jaeyoung Hwang, Jaewoo Ahn, Ameya S. Mahabaleshwarkar, Bilal Kartal, Pritam Biswas, Yoshi Suhara, Kangwook Lee, Jaewoong Cho*

Main category: cs.AI

TL;DR: Orak is a new benchmark for training and evaluating LLM agents across diverse video games, addressing gaps in existing benchmarks by covering multiple genres, agentic modules, and fine-tuning datasets.


<details>
  <summary>Details</summary>
Motivation: Existing game benchmarks lack evaluations of diverse LLM capabilities, agentic modules for complex gameplay, and fine-tuning datasets for aligning LLMs into gaming agents.

Method: Orak includes 12 popular video games across major genres, a plug-and-play interface (MCP), and a fine-tuning dataset of LLM gameplay trajectories.

Result: Orak provides a comprehensive evaluation framework with game score leaderboards, LLM battle arenas, and analyses of visual input, strategies, and fine-tuning effects.

Conclusion: Orak establishes a foundation for developing generic gaming agents by addressing practical needs in LLM agent evaluation and training.

Abstract: Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [377] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/pdf/2506.03613)
*Shaoshan Liu, Fan Wang, Hongjun Zhou, Yuanfeng Wang*

Main category: cs.AI

TL;DR: The paper demonstrates the importance of theoretical insights in solving real-world engineering challenges, specifically in training cross-morphology embodied AI policies. It formalizes the problem as a structured POMDP, proves its complexity, and introduces Collective Adaptation as a scalable alternative.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training AI policies that generalize across diverse robot morphologies, highlighting the gap between theory and practice in reinforcement learning.

Method: Formalizes the problem as the Heterogeneous Embodied Agent Training (HEAT) problem, proves it reduces to a structured POMDP, and explores Collective Adaptation, a distributed learning approach.

Result: The HEAT problem is PSPACE-complete, explaining why current methods fail under morphological diversity. Collective Adaptation, though NEXP-complete, offers practical scalability.

Conclusion: Theoretical insights can guide robust and scalable embodied AI design, bridging theory and practice. Implementation code is publicly available.

Abstract: While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [378] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/pdf/2506.03673)
*Yinlong Xu, Yanzhao Zheng, Shuoshuo Sun, Shuaihan Huang, Baohua Dong, Hangcheng Zhu, Ruohui Huang, Gang Yu, Hongxia Xu, Jian Wu*

Main category: cs.AI

TL;DR: The paper introduces Reason from Future (RFF), a bidirectional reasoning paradigm that outperforms Chain-of-Thought and Tree-of-Thought by reducing search space and avoiding local optima.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning paradigms like CoT and ToT enhance small language models but suffer from unbounded search space and local optima, lacking a global perspective.

Method: RFF combines top-down planning with bottom-up reasoning, using reverse reasoning to prioritize core logic and constrain intermediate steps.

Result: Empirical evaluations show RFF achieves higher accuracy and requires less search space than conventional methods.

Conclusion: RFF offers a more efficient and accurate reasoning paradigm by addressing the limitations of forward sequential reasoning.

Abstract: It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [379] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/pdf/2506.03915)
*Sebastian Rödling, Matej Zečević, Devendra Singh Dhami, Kristian Kersting*

Main category: cs.AI

TL;DR: The paper generalizes Structural Causal Explanations (SCEs) to handle temporal interactions and feedback loops, improving their applicability beyond small data.


<details>
  <summary>Details</summary>
Motivation: SCEs are limited to small data and lack support for temporal or feedback-driven scenarios, which restricts their practical use.

Method: The authors propose a recursive formulation of explanation trees to capture temporal interactions and feedback loops.

Result: The generalized SCE algorithm outperforms the base SCE and other methods on synthetic time-series data and a 2D grid game.

Conclusion: The recursive SCE formulation enhances explanatory power for temporal and interactive scenarios, broadening its applicability.

Abstract: Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [380] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/pdf/2506.03939)
*Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor enhances GraphRAG by addressing inefficiencies in information aggregation and rigid reasoning through multi-agent collaboration and adaptive modules, improving LLM performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Existing GraphRAG methods struggle with inefficient information aggregation and rigid reasoning, limiting their adaptability and accuracy in handling graph data.

Method: Proposes Graph Counselor, using multi-agent collaboration (AGIEM) and a Self-Reflection module for adaptive graph modeling and reasoning.

Result: Outperforms existing methods in graph reasoning tasks, showing higher accuracy and generalization.

Conclusion: Graph Counselor effectively overcomes limitations of current GraphRAG methods, offering improved performance and adaptability.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [381] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/pdf/2506.03997)
*Mario Alviano, Laura Giordano, Daniele Theseider Dupré*

Main category: cs.AI

TL;DR: A Conditional ASP framework is introduced for conditional extensions of Answer Set Programming, combining conditional logic with ASP for reasoning over answer sets.


<details>
  <summary>Details</summary>
Motivation: To extend Answer Set Programming with conditional reasoning capabilities, leveraging conditional logic and multi-preferential semantics.

Method: Combines a conditional knowledge base with an ASP program, using multi-preferential semantics (including KLM preferential semantics) to interpret conditionals.

Result: The framework enables conditional reasoning over answer sets, providing a flexible and interpretable approach.

Conclusion: Conditional ASP offers a robust method for integrating conditional reasoning into ASP, enhancing its expressiveness and applicability.

Abstract: In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [382] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/pdf/2506.04018)
*Akshat Naik, Patrick Quinn, Guillermo Bosch, Emma Gouné, Francisco Javier Campos Zabala, Jason Ross Brown, Edward James Young*

Main category: cs.AI

TL;DR: The paper introduces a benchmark, AgentMisalignment, to evaluate the likelihood of LLM agents attempting misaligned behaviors in real-world settings, finding that more capable models and certain personas increase misalignment risks.


<details>
  <summary>Details</summary>
Motivation: To address the poorly understood likelihood of LLM agents attempting misaligned behaviors in real-world scenarios, given the rise of such agents and associated risks.

Method: Developed the AgentMisalignment benchmark with realistic scenarios, categorized misaligned behaviors (e.g., goal-guarding, power-seeking), and evaluated frontier models while varying system prompts to assess personality impacts.

Result: More capable models showed higher misalignment, and system prompts (personas) unpredictably influenced misalignment tendencies, sometimes more than model choice itself.

Conclusion: Current alignment methods fail to generalize to LLM agents, emphasizing the need for further propensity evaluations as autonomous systems proliferate, and highlighting the critical role of system prompt engineering.

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [383] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/pdf/2506.04022)
*Qiyue Xia, J. Michael Herrmann*

Main category: cs.AI

TL;DR: The paper presents a method for multi-objective reinforcement learning (MORL) using a locally linear map to approximate Pareto fronts, improving search efficiency in contiguous solution domains.


<details>
  <summary>Details</summary>
Motivation: MORL aims to optimize conflicting goals for flexible and reliable RL in practical tasks by finding diverse, non-dominated policies.

Method: A training scheme based on a locally linear map between parameter and performance spaces is used to approximate Pareto fronts.

Result: Experiments show the method's efficiency compared to previous approaches, with and without retraining across domains.

Conclusion: The approach effectively interprets parameter vectors in terms of objectives, enabling efficient search in contiguous solution domains.

Abstract: Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [384] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/pdf/2506.04133)
*Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis*

Main category: cs.AI

TL;DR: The paper reviews Trust, Risk, and Security Management (TRiSM) in LLM-based agentic multi-agent systems (AMAS), covering governance, explainability, ModelOps, and privacy/security, with case studies and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of trust, risk, and security in agentic AI systems built on LLMs, ensuring safe and accountable deployment.

Method: Structured analysis of TRiSM pillars (governance, explainability, ModelOps, privacy/security), threat vectors, risk taxonomy, case studies, and trust-building mechanisms.

Result: Identifies vulnerabilities, proposes mitigation strategies, and reviews metrics for trust and performance in distributed LLM agent systems.

Conclusion: Proposes a roadmap for responsible agentic AI, aligning multi-agent systems with robust TRiSM principles for transparency and accountability.

Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [385] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/pdf/2506.04135)
*Pei Yang, Hai Ci, Mike Zheng Shou*

Main category: cs.AI

TL;DR: macOSWorld is the first benchmark for GUI agents on macOS, featuring 202 multilingual tasks across 30 apps, including safety testing. It reveals performance gaps and multilingual challenges.


<details>
  <summary>Details</summary>
Motivation: Existing GUI benchmarks lack macOS coverage, despite its unique patterns and apps. macOSWorld fills this gap.

Method: macOSWorld includes 202 tasks in 5 languages, covering 30 macOS apps (28 exclusive), with a safety benchmarking subset.

Result: Proprietary agents outperform open-source ones (30% vs. 2% success). Multilingual tasks show weaknesses, especially in Arabic. Safety tests reveal deception vulnerabilities.

Conclusion: macOSWorld highlights the need for macOS adaptation and multilingual robustness in GUI agents, with safety concerns requiring attention.

Abstract: Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [386] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/pdf/2506.04210)
*Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi*

Main category: cs.AI

TL;DR: Extended thinking at test-time initially improves reasoning but leads to 'overthinking' and performance decline. Parallel thinking, an alternative method, outperforms extended thinking by 20%.


<details>
  <summary>Details</summary>
Motivation: To investigate whether extended thinking at test-time genuinely enhances reasoning performance or creates an illusion of improvement.

Method: Empirical study across models and benchmarks, analysis using a probabilistic model, and introduction of parallel thinking (inspired by Best-of-N sampling).

Result: Extended thinking initially improves performance but later declines due to increased output variance. Parallel thinking achieves up to 20% higher accuracy.

Conclusion: Extended thinking is ineffective for test-time scaling; parallel thinking is a superior alternative.

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


### [387] [Risk Awareness in HTN Planning](https://arxiv.org/pdf/2204.10669)
*Ebaa Alnazer, Ilche Georgievski, Marco Aiello*

Main category: cs.AI

TL;DR: The paper introduces a risk-aware HTN planning framework to handle uncertain action costs in real-world domains like autonomous vehicles, using expected utility theory to account for varying risk attitudes.


<details>
  <summary>Details</summary>
Motivation: Real-world domains involve uncertain action costs (e.g., traffic delays in autonomous vehicles), but existing HTN planning approaches assume risk neutrality, ignoring risk assessment.

Method: The authors enhance HTN planning by incorporating expected utility theory, modeling action costs as probability distributions and computing risk-aware plans with the highest expected utility.

Result: The proposed framework allows HTN planning agents to compute plans tailored to specific risk attitudes, demonstrated as feasible and expressive in two case studies.

Conclusion: The approach advances HTN planning by addressing risk awareness, though challenges remain in extending it beyond HTN planning for broader modeling and plan generation.

Abstract: Actual real-world domains are characterised by uncertain situations in which
acting and using resources may entail the embracing of risks. Performing
actions in such domains involves costs of consuming some resource, such as time
or energy, where the knowledge about these costs can range from known to
totally unknown. In autonomous vehicles, actions have uncertain costs due to
factors like traffic. Choosing an action requires assessing delay risks, as
each road may have unpredictable congestion. Thus, these domains call for not
only planning under uncertainty but also planning while embracing risk.
Resorting to HTN planning as a widely used planning technique in real-world
applications, one can observe that existing approaches assume risk neutrality,
relying on single-valued action costs without considering risk. Here, we
enhance HTN planning with risk awareness by considering expected utility
theory. We introduce a general framework for HTN planning that allows modelling
risk and uncertainty using a probability distribution of action costs upon
which we define risk-aware HTN planning as being capable of accounting for the
different risk attitudes and allowing the computation of plans that go beyond
risk neutrality. We lay out that computing risk-aware plans requires finding
plans with the highest expected utility. We argue that it is possible for HTN
planning agents to solve specialised risk-aware HTN planning problems by
adapting existing HTN planning approaches, and develop an approach that
surpasses the expressiveness of current approaches by allowing these agents to
compute plans tailored to a particular risk attitude. An empirical evaluation
of two case studies highlights the feasibility and expressiveness of this
approach. We also highlight open issues, such as applying the proposal beyond
HTN planning, covering both modelling and plan generation.

</details>


### [388] [MacroSwarm: A Field-based Compositional Framework for Swarm Programming](https://arxiv.org/pdf/2401.10969)
*Gianluca Aguzzi, Roberto Casadei, Mirko Viroli*

Main category: cs.AI

TL;DR: The paper introduces MacroSwarm, a field-based coordination approach for designing and programming swarm behavior using reusable, composable functional blocks. It demonstrates its practicality through simulations of flocking, pattern formation, and decision-making.


<details>
  <summary>Details</summary>
Motivation: The need for general methods and tools to define complex swarm behavior in a principled way, despite recent progress in swarm engineering.

Method: Proposes MacroSwarm, a field-based coordination approach using pure functions to map sensing fields into actuation fields, building on aggregate computing.

Result: Simulations show MacroSwarm's expressiveness and compositionality in achieving swarm behaviors like flocking and pattern formation. The framework also exhibits self-stabilization properties.

Conclusion: MacroSwarm offers a practical and resilient framework for swarm programming, with formal guarantees for resilience and self-stabilization.

Abstract: Swarm behaviour engineering is an area of research that seeks to investigate
methods and techniques for coordinating computation and action within groups of
simple agents to achieve complex global goals like pattern formation,
collective movement, clustering, and distributed sensing. Despite recent
progress in the analysis and engineering of swarms (of drones, robots,
vehicles), there is still a need for general design and implementation methods
and tools that can be used to define complex swarm behaviour in a principled
way. To contribute to this quest, this article proposes a new field-based
coordination approach, called MacroSwarm, to design and program swarm behaviour
in terms of reusable and fully composable functional blocks embedding
collective computation and coordination. Based on the macroprogramming paradigm
of aggregate computing, MacroSwarm builds on the idea of expressing each swarm
behaviour block as a pure function, mapping sensing fields into actuation goal
fields, e.g., including movement vectors. In order to demonstrate the
expressiveness, compositionality, and practicality of MacroSwarm as a framework
for swarm programming, we perform a variety of simulations covering common
patterns of flocking, pattern formation, and collective decision-making. The
implications of the inherent self-stabilisation properties of field-based
computations in MacroSwarm are discussed, which formally guarantee some
resilience properties and guided the design of the library.

</details>


### [389] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.24073)
*Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu*

Main category: cs.AI

TL;DR: The paper explores Retrieval-Augmented Generation (RAG) for Large Vision-Language Models (LVLMs) to address limitations like static data and hallucinations, improving performance by 5% without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs face challenges like static training data, hallucinations, and lack of real-time verification, limiting real-world applicability. RAG offers a solution by grounding outputs in factual, dynamic knowledge.

Method: The study systematically examines the multimodal RAG pipeline: retrieval (modality configurations, strategies), re-ranking (mitigating biases, improving relevance), and generation (integrating retrieved candidates). It also proposes a unified agentic framework for dynamic evidence selection.

Result: The approach yields a 5% average performance boost in LVLMs without fine-tuning.

Conclusion: RAG significantly enhances LVLMs by integrating dynamic knowledge retrieval and generation, addressing key limitations for real-world applications.

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [390] [Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace](https://arxiv.org/pdf/2403.04588)
*Léopold Maytié, Benjamin Devillers, Alexandre Arnold, Rufin VanRullen*

Main category: cs.AI

TL;DR: The paper explores using a brain-inspired 'Global Workspace' for RL agents to achieve robust multimodal representation and zero-shot cross-modal transfer.


<details>
  <summary>Details</summary>
Motivation: Humans generalize across sensory inputs, but RL agents struggle with sensor redundancy and complementarity. The study aims to leverage cognitive science principles for better RL performance.

Method: A 'Global Workspace' is trained to combine multimodal inputs (visual and attribute vectors), then used to train an RL agent. Performance is tested in zero-shot cross-modal transfer tasks.

Result: The model successfully performs zero-shot transfer between modalities, outperforming variants like CLIP-like representations.

Conclusion: The brain-inspired Global Workspace is effective for RL agents, enabling robust multimodal generalization without additional training.

Abstract: Humans perceive the world through multiple senses, enabling them to create a
comprehensive representation of their surroundings and to generalize
information across domains. For instance, when a textual description of a scene
is given, humans can mentally visualize it. In fields like robotics and
Reinforcement Learning (RL), agents can also access information about the
environment through multiple sensors; yet redundancy and complementarity
between sensors is difficult to exploit as a source of robustness (e.g. against
sensor failure) or generalization (e.g. transfer across domains). Prior
research demonstrated that a robust and flexible multimodal representation can
be efficiently constructed based on the cognitive science notion of a 'Global
Workspace': a unique representation trained to combine information across
modalities, and to broadcast its signal back to each modality. Here, we explore
whether such a brain-inspired multimodal representation could be advantageous
for RL agents. First, we train a 'Global Workspace' to exploit information
collected about the environment via two input modalities (a visual input, or an
attribute vector representing the state of the agent and/or its environment).
Then, we train a RL agent policy using this frozen Global Workspace. In two
distinct environments and tasks, our results reveal the model's ability to
perform zero-shot cross-modal transfer between input modalities, i.e. to apply
to image inputs a policy previously trained on attribute vectors (and
vice-versa), without additional training or fine-tuning. Variants and ablations
of the full Global Workspace (including a CLIP-like multimodal representation
trained via contrastive learning) did not display the same generalization
abilities.

</details>


### [391] [Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and Multi-Agent Environments](https://arxiv.org/pdf/2407.03969)
*Mikel Malagón, Josu Ceberio, Jose A. Lozano*

Main category: cs.AI

TL;DR: Craftium is a customizable 3D platform for single- and multi-agent environments, offering computational efficiency and rich features.


<details>
  <summary>Details</summary>
Motivation: Existing 2D environments lack real-world challenges, while 3D ones are computationally expensive and inflexible.

Method: Introduces Craftium, a platform for building diverse 3D environments, including multi-agent tasks and procedural generators.

Result: Craftium reduces computational costs, outperforming alternatives like Minecraft-based frameworks by +2K steps per second.

Conclusion: Craftium provides a scalable, efficient solution for creating rich 3D environments for autonomous agents.

Abstract: Advances in large models, reinforcement learning, and open-endedness have
accelerated progress toward autonomous agents that can learn and interact in
the real world. To achieve this, flexible tools are needed to create rich, yet
computationally efficient, environments. While scalable 2D environments fail to
address key real-world challenges like 3D navigation and spatial reasoning,
more complex 3D environments are computationally expensive and lack features
like customizability and multi-agent support. This paper introduces Craftium, a
highly customizable and easy-to-use platform for building rich 3D single- and
multi-agent environments. We showcase environments of different complexity and
nature: from single- and multi-agent tasks to vast worlds with many creatures
and biomes, and customizable procedural task generators. Benchmarking shows
that Craftium significantly reduces the computational cost of alternatives of
similar richness, achieving +2K steps per second more than Minecraft-based
frameworks.

</details>


### [392] [Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development](https://arxiv.org/pdf/2407.11784)
*Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, Jingren Zhou*

Main category: cs.AI

TL;DR: A sandbox suite for integrated data-model co-development is introduced, optimizing multimodal large models through a feedback-driven workflow, validated by practical use cases and open-sourced for broader research.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimizing multimodal large models due to isolated model-centric and data-centric developments, leading to inefficiencies.

Method: Proposes a "Probe-Analyze-Refine" workflow within a sandbox suite for iterative data-model co-development, tested on tasks like image-text pre-training and text-to-video generation.

Result: Achieves notable performance boosts (e.g., topping VBench leaderboard) and provides insights into data quality, diversity, and computational costs.

Conclusion: The suite is usable, extensible, and open-sourced, enabling future research in multimodal AI that was previously infeasible.

Abstract: The emergence of multimodal large models has advanced artificial
intelligence, introducing unprecedented levels of performance and
functionality. However, optimizing these models remains challenging due to
historically isolated paths of model-centric and data-centric developments,
leading to suboptimal outcomes and inefficient resource utilization. In
response, we present a new sandbox suite tailored for integrated data-model
co-development. This sandbox provides a feedback-driven experimental platform,
enabling cost-effective iteration and guided refinement of both data and
models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through
practical use cases on multimodal tasks such as image-text pre-training with
CLIP, image-to-text generation with LLaVA-like models, and text-to-video
generation with DiT-based models, yields transferable and notable performance
boosts, such as topping the VBench leaderboard. A comprehensive set of over 100
experiments demonstrated the suite's usability and extensibility, while also
uncovering insights into the interplay between data quality, diversity, model
behavior, and computational costs. All codes, datasets, and models are
open-sourced to foster future research and applications that would otherwise be
infeasible due to the lack of a dedicated co-development infrastructure.

</details>


### [393] [Trust-Oriented Adaptive Guardrails for Large Language Models](https://arxiv.org/pdf/2408.08959)
*Jinwei Hu, Yi Dong, Xiaowei Huang*

Main category: cs.AI

TL;DR: The paper introduces an adaptive guardrail mechanism for LLMs, using trust metrics to dynamically moderate content access, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails lack a methodology to address diverse user needs, especially access rights, requiring a sociotechnical solution.

Method: Combines trust modeling (social aspect) and retrieval-augmented generation (technical aspect) to create adaptive guardrails based on user trust metrics.

Result: Empirical evaluation shows the adaptive guardrail effectively meets diverse user needs, secures sensitive content, and manages hazardous material contextually.

Conclusion: This is the first trust-oriented guardrail system, offering a scalable solution for ethical LLM deployment.

Abstract: Guardrail, an emerging mechanism designed to ensure that large language
models (LLMs) align with human values by moderating harmful or toxic responses,
requires a sociotechnical approach in their design. This paper addresses a
critical issue: existing guardrails lack a well-founded methodology to
accommodate the diverse needs of different user groups, particularly concerning
access rights. Supported by trust modeling (primarily on `social' aspect) and
enhanced with online in-context learning via retrieval-augmented generation (on
`technical' aspect), we introduce an adaptive guardrail mechanism, to
dynamically moderate access to sensitive content based on user trust metrics.
User trust metrics, defined as a novel combination of direct interaction trust
and authority-verified trust, enable the system to precisely tailor the
strictness of content moderation by aligning with the user's credibility and
the specific context of their inquiries. Our empirical evaluation demonstrates
the effectiveness of the adaptive guardrail in meeting diverse user needs,
outperforming existing guardrails while securing sensitive information and
precisely managing potentially hazardous content through a context-aware
knowledge base. To the best of our knowledge, this work is the first to
introduce trust-oriented concept into a guardrail system, offering a scalable
solution that enriches the discourse on ethical deployment for next-generation
LLM service.

</details>


### [394] [A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization](https://arxiv.org/pdf/2410.02165)
*Yucheng Chu, Hang Li, Kaiqi Yang, Harry Shomer, Hui Liu, Yasemin Copur-Gencturk, Jiliang Tang*

Main category: cs.AI

TL;DR: GradeOpt is a multi-agent ASAG framework using LLMs to grade short-answer questions, improving accuracy and alignment with human graders.


<details>
  <summary>Details</summary>
Motivation: Address challenges in grading SAGs, like workload and inconsistency, by leveraging NLP advancements.

Method: Proposes GradeOpt, a unified framework with LLM-based graders, a reflector, and a refiner to optimize grading guidelines.

Result: Outperforms baselines in grading accuracy and alignment with human graders, especially for PCK and CK questions.

Conclusion: GradeOpt's components are effective, offering a scalable solution for ASAG tasks.

Abstract: Open-ended short-answer questions (SAGs) have been widely recognized as a
powerful tool for providing deeper insights into learners' responses in the
context of learning analytics (LA). However, SAGs often present challenges in
practice due to the high grading workload and concerns about inconsistent
assessments. With recent advancements in natural language processing (NLP),
automatic short-answer grading (ASAG) offers a promising solution to these
challenges. Despite this, current ASAG algorithms are often limited in
generalizability and tend to be tailored to specific questions. In this paper,
we propose a unified multi-agent ASAG framework, GradeOpt, which leverages
large language models (LLMs) as graders for SAGs. More importantly, GradeOpt
incorporates two additional LLM-based agents - the reflector and the refiner -
into the multi-agent system. This enables GradeOpt to automatically optimize
the original grading guidelines by performing self-reflection on its errors.
Through experiments on a challenging ASAG task, namely the grading of
pedagogical content knowledge (PCK) and content knowledge (CK) questions,
GradeOpt demonstrates superior performance in grading accuracy and behavior
alignment with human graders compared to representative baselines. Finally,
comprehensive ablation studies confirm the effectiveness of the individual
components designed in GradeOpt.

</details>


### [395] [Reflection-Bench: Evaluating Epistemic Agency in Large Language Models](https://arxiv.org/pdf/2410.16270)
*Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, Yingchun Wang*

Main category: cs.AI

TL;DR: The paper studies epistemic agency in LLMs, proposing a benchmark (Reflection-Bench) to evaluate seven cognitive dimensions, revealing limitations in current models, especially in meta-reflection.


<details>
  <summary>Details</summary>
Motivation: To address the understudied intrinsic epistemic agency of LLMs, which is crucial for their reliability and effectiveness as AI agents.

Method: Proposes Reflection-Bench, a cognitive-psychology-inspired benchmark with seven tasks, evaluating 16 models using three prompting strategies.

Result: Identifies a three-tier performance hierarchy, highlighting significant limitations in current LLMs, particularly in meta-reflection.

Conclusion: While state-of-the-art LLMs show rudimentary epistemic agency, improvements in core cognitive functions, cross-functional coordination, and adaptive processing are needed.

Abstract: With large language models (LLMs) increasingly deployed as cognitive engines
for AI agents, the reliability and effectiveness critically hinge on their
intrinsic epistemic agency, which remains understudied. Epistemic agency, the
ability to flexibly construct, adapt, and monitor beliefs about dynamic
environments, represents a base-model-level capacity independent of specific
tools, modules, or applications. We characterize the holistic process
underlying epistemic agency, which unfolds in seven interrelated dimensions:
prediction, decision-making, perception, memory, counterfactual thinking,
belief updating, and meta-reflection. Correspondingly, we propose
Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven
tasks with long-term relevance and minimization of data leakage. Through a
comprehensive evaluation of 16 models using three prompting strategies, we
identify a clear three-tier performance hierarchy and significant limitations
of current LLMs, particularly in meta-reflection capabilities. While
state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our
findings suggest several promising research directions, including enhancing
core cognitive functions, improving cross-functional coordination, and
developing adaptive processing mechanisms. Our code and data are available at
https://github.com/AI45Lab/ReflectionBench.

</details>


### [396] [Understanding Impact of Human Feedback via Influence Functions](https://arxiv.org/pdf/2501.05790)
*Taywon Min, Haeone Lee, Yongchan Kwon, Kimin Lee*

Main category: cs.AI

TL;DR: The paper explores using influence functions to address noisy, inconsistent, or biased human feedback in RLHF, proposing a compute-efficient method to analyze and improve reward models.


<details>
  <summary>Details</summary>
Motivation: Human feedback in RLHF can be noisy or biased, leading to misaligned reward signals and unintended side effects. The study aims to enhance feedback interpretability and alignment.

Method: The authors propose a compute-efficient approximation of influence functions for LLM-based reward models and large-scale preference datasets.

Result: Experiments show influence functions can detect labeler bias and guide labelers to refine feedback strategies, improving alignment with expert feedback.

Conclusion: Influence functions enhance feedback interpretability and scalable oversight in RLHF, aiding labelers in providing more accurate and consistent feedback.

Abstract: In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn
suitable reward models from human feedback to align large language models
(LLMs) with human intentions. However, human feedback can often be noisy,
inconsistent, or biased, especially when evaluating complex responses. Such
feedback can lead to misaligned reward signals, potentially causing unintended
side effects during the RLHF process. To address these challenges, we explore
the use of influence functions to measure the impact of human feedback on the
performance of reward models. We propose a compute-efficient approximation
method that enables the application of influence functions to LLM-based reward
models and large-scale preference datasets. In our experiments, we demonstrate
two key applications of influence functions: (1) detecting common forms of
labeler bias in human feedback datasets and (2) guiding labelers to refine
their strategies to align more closely with expert feedback. By quantifying the
impact of human feedback on reward models, we believe that influence functions
can enhance feedback interpretability and contribute to scalable oversight in
RLHF, helping labelers provide more accurate and consistent feedback. Source
code is available at https://github.com/mintaywon/IF_RLHF

</details>


### [397] [An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures](https://arxiv.org/pdf/2501.07930)
*Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier*

Main category: cs.AI

TL;DR: AOC (Adaptive Orthogonal Convolution) is introduced to overcome computational and feature limitations of orthogonal convolutions, enabling scalable and efficient models.


<details>
  <summary>Details</summary>
Motivation: Orthogonal convolutions are valuable but face deployment challenges in large-scale applications due to computational overhead and limited feature support.

Method: AOC extends BCOP to address limitations like strides, dilations, group convolutions, and transposed convolutions.

Result: AOC produces expressive, scalable models with increasing efficiency, demonstrated through experiments.

Conclusion: AOC unlocks previously impractical architectures, with an open-source package (Orthogonium) provided for further research.

Abstract: Orthogonal convolutional layers are valuable components in multiple areas of
machine learning, such as adversarial robustness, normalizing flows, GANs, and
Lipschitz-constrained models. Their ability to preserve norms and ensure stable
gradient propagation makes them valuable for a large range of problems. Despite
their promise, the deployment of orthogonal convolution in large-scale
applications is a significant challenge due to computational overhead and
limited support for modern features like strides, dilations, group
convolutions, and transposed convolutions. In this paper, we introduce AOC
(Adaptative Orthogonal Convolution), a scalable method that extends a previous
method (BCOP), effectively overcoming existing limitations in the construction
of orthogonal convolutions. This advancement unlocks the construction of
architectures that were previously considered impractical. We demonstrate
through our experiments that our method produces expressive models that become
increasingly efficient as they scale. To foster further advancement, we provide
an open-source python package implementing this method, called Orthogonium (
https://github.com/deel-ai/orthogonium ) .

</details>


### [398] [A Comprehensive Survey of Agents for Computer Use: Foundations, Challenges, and Future Directions](https://arxiv.org/pdf/2501.16150)
*Pascal J. Sager, Benjamin Meyer, Peng Yan, Rebekka von Wartburg-Kottler, Layan Etaiwi, Aref Enayati, Gabriel Nobel, Ahmed Abdulkadir, Benjamin F. Grewe, Thilo Stadelmann*

Main category: cs.AI

TL;DR: The paper surveys the state-of-the-art in Agents for Computer Use (ACUs), identifies research gaps, and proposes solutions to advance the field toward practical, general-purpose agents.


<details>
  <summary>Details</summary>
Motivation: To investigate the current state, trends, and gaps in ACUs, aiming to bridge the divide between research and practical deployment.

Method: The authors review 87 ACUs and 33 datasets, introducing a taxonomy spanning domain, interaction, and agent perspectives.

Result: Six major research gaps are identified, including insufficient generalization and inefficient learning, with proposed solutions like vision-based observations and adaptive learning.

Conclusion: The taxonomy and analysis provide a foundation for advancing ACU research toward robust and scalable general-purpose agents.

Abstract: Agents for computer use (ACUs) are an emerging class of systems capable of
executing complex tasks on digital devices - such as desktops, mobile phones,
and web platforms - given instructions in natural language. These agents can
automate tasks by controlling software via low-level actions like mouse clicks
and touchscreen gestures. However, despite rapid progress, ACUs are not yet
mature for everyday use.
  In this survey, we investigate the state-of-the-art, trends, and research
gaps in the development of practical ACUs. We provide a comprehensive review of
the ACU landscape, introducing a unifying taxonomy spanning three dimensions:
(I) the domain perspective, characterizing agent operating contexts; (II) the
interaction perspective, describing observation modalities (e.g., screenshots,
HTML) and action modalities (e.g., mouse, keyboard, code execution); and (III)
the agent perspective, detailing how agents perceive, reason, and learn.
  We review 87 ACUs and 33 datasets across foundation model-based and classical
approaches through this taxonomy. Our analysis identifies six major research
gaps: insufficient generalization, inefficient learning, limited planning, low
task complexity in benchmarks, non-standardized evaluation, and a disconnect
between research and practical conditions.
  To address these gaps, we advocate for: (a) vision-based observations and
low-level control to enhance generalization; (b) adaptive learning beyond
static prompting; (c) effective planning and reasoning methods and models; (d)
benchmarks that reflect real-world task complexity; (e) standardized evaluation
based on task success; (f) aligning agent design with real-world deployment
constraints.
  Together, our taxonomy and analysis establish a foundation for advancing ACU
research toward general-purpose agents for robust and scalable computer use.

</details>


### [399] [MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models](https://arxiv.org/pdf/2502.00698)
*Huanqia Cai, Yijun Yang, Winston Hu*

Main category: cs.AI

TL;DR: The paper introduces MM-IQ, a benchmark for evaluating multimodal AI systems' cognitive capabilities, revealing their poor performance compared to human reasoning.


<details>
  <summary>Details</summary>
Motivation: Current AI lacks systematic benchmarks for cognitive capabilities like abstraction and reasoning, which IQ tests measure in humans.

Method: Proposes MM-IQ, a framework with 4,776 training and 2,710 test items across 8 reasoning paradigms, and evaluates existing models.

Result: State-of-the-art models perform only slightly better than random chance (33.17% vs. 25%), highlighting their limitations.

Conclusion: The cognitive gap between AI and humans is significant, requiring advancements. A new multimodal reasoning model is introduced as a baseline.

Abstract: IQ testing has served as a foundational methodology for evaluating human
cognitive capabilities, deliberately decoupling assessment from linguistic
background, language proficiency, or domain-specific knowledge to isolate core
competencies in abstraction and reasoning. Yet, artificial intelligence
research currently lacks systematic benchmarks to quantify these critical
cognitive capabilities in multimodal systems. To address this crucial gap, we
propose MM-IQ, a comprehensive evaluation framework, which comprises a
large-scale training set with 4,776 visual reasoning problems and 2,710
meticulously curated test items spanning 8 distinct reasoning paradigms.
Through systematic evaluation of existing open-source and proprietary
multimodal models, our benchmark reveals striking limitations: even
state-of-the-art architectures achieve only marginally superior performance to
random chance (33.17% vs. 25% baseline accuracy). This substantial performance
chasm highlights the inadequacy of current multimodal models in approximating
fundamental human reasoning capacities, underscoring the need for
paradigm-shifting advancements to bridge this cognitive divide. Moreover,
inspired by the recent surge of large reasoning models, we also release a
multimodal reasoning model as the baseline that is trained via reinforcement
learning with verifiable reward functions, reaching competitive performance to
the state-of-the-art with a notably smaller model size.

</details>


### [400] [HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims](https://arxiv.org/pdf/2502.11753)
*Michiel van der Meer, Pavel Korshunov, Sébastien Marcel, Lonneke van der Plas*

Main category: cs.AI

TL;DR: HintsOfTruth is a dataset for multimodal checkworthiness detection, comparing text-based and multimodal LLMs. Lightweight text encoders perform well but lack robustness with synthetic data, while multimodal models are more accurate but costly.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of detecting checkworthy claims in multimodal, diverse, and synthetic content, aiding fact-checkers by automating the process.

Method: Introduces HintsOfTruth, a dataset with 27K real-world and synthetic image/claim pairs. Compares fine-tuned and prompted LLMs, focusing on text-based and multimodal models.

Result: Lightweight text encoders perform comparably but struggle with synthetic data. Multimodal models are more robust but computationally expensive.

Conclusion: Multimodal models are better for robustness, but text-based models are practical for large-scale applications.

Abstract: Misinformation can be countered with fact-checking, but the process is costly
and slow. Identifying checkworthy claims is the first step, where automation
can help scale fact-checkers' efforts. However, detection methods struggle with
content that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We
introduce HintsOfTruth, a public dataset for multimodal checkworthiness
detection with 27K real-world and synthetic image/claim pairs. The mix of real
and synthetic data makes this dataset unique and ideal for benchmarking
detection methods. We compare fine-tuned and prompted Large Language Models
(LLMs). We find that well-configured lightweight text-based encoders perform
comparably to multimodal models but the former only focus on identifying
non-claim-like content. Multimodal LLMs can be more accurate but come at a
significant computational cost, making them impractical for large-scale
applications. When faced with synthetic data, multimodal models perform more
robustly.

</details>


### [401] [Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?](https://arxiv.org/pdf/2503.11207)
*Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi*

Main category: cs.AI

TL;DR: The paper evaluates two LRMs (OpenAI's o3-mini and DeepSeek R1) on analogical reasoning using Raven's matrices, showing significant accuracy drops under perceptual uncertainty, while a neuro-symbolic model (ARLC) remains robust.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of state-of-the-art LRMs on analogical reasoning tasks under challenging conditions like perceptual uncertainty and extended reasoning rules.

Method: Benchmarked models on I-RAVEN and I-RAVEN-X datasets, introduced confounding attributes and smoothed attribute distributions to simulate imperfect perception.

Result: LRMs' accuracy dropped sharply (o3-mini: 86.6% to 17.0%; DeepSeek R1: 80.6% to 23.2%), while ARLC maintained high accuracy (98.6% to 88.0%).

Conclusion: Neuro-symbolic models like ARLC outperform LRMs in robustness under perceptual uncertainty and extended reasoning challenges.

Abstract: This work presents a first evaluation of two state-of-the-art Large Reasoning
Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,
focusing on well-established nonverbal human IQ tests based on Raven's
progressive matrices. We benchmark with the I-RAVEN dataset and its extension,
I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and
ranges of the attribute values. To assess the influence of visual uncertainties
on these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset,
which otherwise assumes an oracle perception. We adopt a two-fold strategy to
simulate this imperfect visual perception: 1) we introduce confounding
attributes which, being sampled at random, do not contribute to the prediction
of the correct answer of the puzzles, and 2) we smoothen the distributions of
the input attributes' values. We observe a sharp decline in OpenAI's o3-mini
task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% --
approaching random chance -- on the more challenging I-RAVEN-X, which increases
input length and range and emulates perceptual uncertainty. This drop occurred
despite spending 3.4x more reasoning tokens. A similar trend is also observed
for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic
probabilistic abductive model, ARLC, that achieves state-of-the-art
performances on I-RAVEN, can robustly reason under all these
out-of-distribution tests, maintaining strong accuracy with only a modest
accuracy reduction from 98.6% to 88.0%. Our code is available at
https://github.com/IBM/raven-large-language-models.

</details>


### [402] [A Formalism for Optimal Search with Dynamic Heuristics (Extended Version)](https://arxiv.org/pdf/2504.21131)
*Remo Christen, Florian Pommerening, Clemens Büchner, Malte Helmert*

Main category: cs.AI

TL;DR: The paper formalizes dynamic heuristics in heuristic search, introduces a generic algorithm framework, and proves optimality results, connecting existing classical planning approaches as special cases.


<details>
  <summary>Details</summary>
Motivation: Existing approaches using dynamic heuristics in A*-like algorithms often overlook the complexities of mutable heuristics, necessitating a formal framework.

Method: The authors formalize dynamic heuristics, develop a generic algorithm framework, and study an instantiation modeling A* with dynamic heuristics.

Result: General optimality results are proven, and existing classical planning approaches are shown as special cases of the framework.

Conclusion: The framework enables direct application of optimality results to existing methods, unifying dynamic heuristic approaches.

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>


### [403] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/pdf/2505.10981)
*Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan*

Main category: cs.AI

TL;DR: Scaling test-time compute on LLMs shows simple Chain-of-Thought outperforms complex prompting strategies as compute increases, with a proposed method to predict and improve scaling performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how prompting strategies scale with increased compute, focusing on majority voting, and to improve scaling efficiency.

Method: Experiments on 6 LLMs, 8 prompting strategies, and 6 benchmarks, with theoretical analysis and a probabilistic prediction method.

Result: Simple Chain-of-Thought surpasses complex strategies as compute scales; a method to predict optimal strategies is introduced.

Conclusion: Encourages re-evaluating complex prompting, highlights simple strategies' potential, and offers insights for scaling performance.

Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a probabilistic method to efficiently predict scaling
performance and identify the best prompting strategy under large sampling
times, eliminating the need for resource-intensive inference processes in
practical applications. Furthermore, we introduce two ways derived from our
theoretical analysis to significantly improve the scaling performance. We hope
that our research can promote to re-examine the role of complicated prompting,
unleash the potential of simple prompting strategies, and provide new insights
for enhancing test-time scaling performance. Code is available at
https://github.com/MraDonkey/rethinking_prompting.

</details>


### [404] [MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models](https://arxiv.org/pdf/2505.17433)
*Zhengyi Zhao, Shubo Zhang, Yuxi Zhang, Yanxi Zhao, Yifan Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

Main category: cs.AI

TL;DR: MemeReaCon is a benchmark to evaluate LVLMs' context-aware understanding of memes, highlighting their current limitations in interpreting context-dependent intent.


<details>
  <summary>Details</summary>
Motivation: Current meme analysis overlooks context-dependent intent, creating a gap in evaluation as LVLMs struggle with context-aware interpretation.

Method: Collected and labeled memes from Reddit communities, including images, post text, and comments, to test LVLMs' understanding of context.

Result: LVLMs fail to interpret context or overly focus on visuals, missing communicative intent.

Conclusion: MemeReaCon exposes LVLM limitations and serves as a benchmark for developing context-aware models.

Abstract: Memes have emerged as a popular form of multimodal online communication,
where their interpretation heavily depends on the specific context in which
they appear. Current approaches predominantly focus on isolated meme analysis,
either for harmful content detection or standalone interpretation, overlooking
a fundamental challenge: the same meme can express different intents depending
on its conversational context. This oversight creates an evaluation gap:
although humans intuitively recognize how context shapes meme interpretation,
Large Vision Language Models (LVLMs) can hardly understand context-dependent
meme intent. To address this critical limitation, we introduce MemeReaCon, a
novel benchmark specifically designed to evaluate how LVLMs understand memes in
their original context. We collected memes from five different Reddit
communities, keeping each meme's image, the post text, and user comments
together. We carefully labeled how the text and meme work together, what the
poster intended, how the meme is structured, and how the community responded.
Our tests with leading LVLMs show a clear weakness: models either fail to
interpret critical information in the contexts, or overly focus on visual
details while overlooking communicative purpose. MemeReaCon thus serves both as
a diagnostic tool exposing current limitations and as a challenging benchmark
to drive development toward more sophisticated LVLMs of the context-aware
understanding.

</details>


### [405] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/pdf/2505.19641)
*Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He*

Main category: cs.AI

TL;DR: SynLogic is a framework for generating diverse logical reasoning data, enhancing RL training for LLMs, and improving reasoning generalization.


<details>
  <summary>Details</summary>
Motivation: The gap in methods and resources for general reasoning capabilities in LLMs, especially due to the lack of diverse and verifiable reasoning data for RL.

Method: SynLogic synthesizes scalable, verifiable logical reasoning data across 35 tasks, with adjustable difficulty and quantity.

Result: SynLogic achieves state-of-the-art logical reasoning performance and improves training efficiency and generalization when mixed with other tasks.

Conclusion: SynLogic is a valuable resource for advancing LLMs' reasoning capabilities, with open-sourced data and pipeline.

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [406] [Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning](https://arxiv.org/pdf/2505.21427)
*Xianling Mu, Joseph Ternasky, Fuat Alican, Yigit Ihlamur*

Main category: cs.AI

TL;DR: A transparent, data-efficient framework for early-stage startup investment using memory-augmented LLMs with in-context learning outperforms traditional methods and VC success rates.


<details>
  <summary>Details</summary>
Motivation: Early-stage startup investment lacks data and transparency; traditional ML methods are opaque and require large datasets.

Method: Uses memory-augmented LLMs with in-context learning, embedding natural language policies for interpretability and iterative refinement.

Result: Achieves 20x higher precision than random chance (1.9%) and 7.1x higher than top-tier VC firms (5.6%).

Conclusion: The framework offers a scalable, interpretable solution for high-risk investment decisions with minimal supervision.

Abstract: Early-stage startup investment is a high-risk endeavor characterized by
scarce data and uncertain outcomes. Traditional machine learning approaches
often require large, labeled datasets and extensive fine-tuning, yet remain
opaque and difficult for domain experts to interpret or improve. In this paper,
we propose a transparent and data-efficient investment decision framework
powered by memory-augmented large language models (LLMs) using in-context
learning (ICL). Central to our method is a natural language policy embedded
directly into the LLM prompt, enabling the model to apply explicit reasoning
patterns and allowing human experts to easily interpret, audit, and iteratively
refine the logic. We introduce a lightweight training process that combines
few-shot learning with an in-context learning loop, enabling the LLM to update
its decision policy iteratively based on structured feedback. With only minimal
supervision and no gradient-based optimization, our system predicts startup
success far more accurately than existing benchmarks. It is over 20x more
precise than random chance, which succeeds 1.9% of the time. It is also 7.1x
more precise than the typical 5.6% success rate of top-tier venture capital
(VC) firms.

</details>


### [407] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/pdf/2505.22990)
*Pin-Han Chen, Yu-Sheng Lin, Wei-Cheng Lee, Tin-Yu Leu, Po-Hsiang Hsu, Anjana Dissanayake, Sungjin Oh, Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: MenTeR is a multiagent AI workflow for analog design, reducing manual effort and accelerating the design process.


<details>
  <summary>Details</summary>
Motivation: Analog design is complex and time-consuming, relying heavily on expert intuition, which hinders efficiency.

Method: MenTeR uses specialized AI agents for tasks like specification understanding, optimization, and validation.

Result: MenTeR speeds up design cycles and enables broader design space exploration.

Conclusion: MenTeR paves the way for future AI-assisted analog design tools.

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [408] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/pdf/2505.23703)
*Ruida Wang, Yuxin Li, Yi R. Fung, Tong Zhang*

Main category: cs.AI

TL;DR: The paper introduces **NL-FL HybridReasoning**, a framework combining Natural Language (NL) and Formal Language (FL) to enhance math reasoning in LLMs, achieving higher accuracy than NL baselines.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for math reasoning in LLMs struggle to integrate new capabilities like FL into NL reasoning due to structural disparities between NL and FL.

Method: The framework includes *NL-FL Problem Alignment* to reformulate NL QA problems as FL existence theorems, *Mixed Problem Input* for FL reasoners to handle both QA and existence problems, and *Answer Extraction* to bridge output format gaps.

Result: The framework achieves 89.80% and 84.34% accuracy on MATH-500 and AMC benchmarks, outperforming NL baselines by 4.60% and 4.82%, respectively.

Conclusion: **HybridReasoning** effectively integrates FL into NL math reasoning, solving problems unsolved by NL baselines, demonstrating its superiority.

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [409] [Balancing Profit and Fairness in Risk-Based Pricing Markets](https://arxiv.org/pdf/2506.00140)
*Jesse Thibodeau, Hadi Nekoei, Afaf Taïk, Janarthanan Rajendran, Golnoosh Farnadi*

Main category: cs.AI

TL;DR: The paper proposes a learned, interpretable tax schedule to regulate dynamic pricing, ensuring fairness in markets like health insurance and credit. Using a simulator and RL, it improves fairness and welfare without explicit coordination.


<details>
  <summary>Details</summary>
Motivation: Dynamic pricing can exclude vulnerable groups, creating a need for regulation to align private incentives with social fairness goals.

Method: Develops a simulator (MarketSim) and trains an RL-based social planner to implement a fairness-tax, regularized for simplicity and interpretability.

Result: In health-insurance and credit markets, the method boosts fairness by 16% and welfare compared to unregulated or fixed-tax approaches.

Conclusion: AI-assisted regulation can transform competitive dilemmas into win-win outcomes, offering a practical framework for fairness in markets.

Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer
groups from essential resources such as health insurance and consumer credit.
We show that a regulator can realign private incentives with social objectives
through a learned, interpretable tax schedule. First, we provide a formal
proposition that bounding each firm's \emph{local} demographic gap implicitly
bounds the \emph{global} opt-out disparity, motivating firm-level penalties.
Building on this insight we introduce \texttt{MarketSim} -- an open-source,
scalable simulator of heterogeneous consumers and profit-maximizing firms --
and train a reinforcement learning (RL) social planner (SP) that selects a
bracketed fairness-tax while remaining close to a simple linear prior via an
$\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and
easily interpretable. In two empirically calibrated markets, i.e., U.S.
health-insurance and consumer-credit, our planner simultaneously raises
demand-fairness by up to $16\%$ relative to unregulated Free Market while
outperforming a fixed linear schedule in terms of social welfare without
explicit coordination. These results illustrate how AI-assisted regulation can
convert a competitive social dilemma into a win-win equilibrium, providing a
principled and practical framework for fairness-aware market oversight.

</details>


### [410] [What do professional software developers need to know to succeed in an age of Artificial Intelligence?](https://arxiv.org/pdf/2506.00202)
*Matthew Kam, Cody Miller, Miaoxin Wang, Abey Tidwell, Irene A. Lee, Joyce Malyn-Smith, Beatriz Perez, Vikram Tiwari, Joshua Kenitzer, Andrew Macvean, Erin Barrar*

Main category: cs.AI

TL;DR: Generative AI boosts developer productivity but raises concerns about workforce disruption and deskilling. Research with 21 developers reveals 12 work goals, 75 tasks, and skills needed, leading to 5 insights. Successful AI-enhanced developers require skills in four domains, integrated into a 6-step workflow. Future-proofing demands focus on both soft and technical skills.


<details>
  <summary>Details</summary>
Motivation: To understand how developers use AI at work and identify the skills needed to thrive in an AI-enhanced environment, addressing concerns about deskilling and workforce disruption.

Method: Research involving 21 developers, analyzing their work goals, tasks, and required skills, culminating in 5 key insights.

Result: Identified four skill domains (Generative AI, core software engineering, adjacent engineering, and non-engineering) and a 6-step workflow for AI-enhanced development.

Conclusion: Future-proofing developers requires integrating soft and technical skills across all four domains into learning initiatives and education programs.

Abstract: Generative AI is showing early evidence of productivity gains for software
developers, but concerns persist regarding workforce disruption and deskilling.
We describe our research with 21 developers at the cutting edge of using AI,
summarizing 12 of their work goals we uncovered, together with 75 associated
tasks and the skills & knowledge for each, illustrating how developers use AI
at work. From all of these, we distilled our findings in the form of 5
insights. We found that the skills & knowledge to be a successful AI-enhanced
developer are organized into four domains (using Generative AI effectively,
core software engineering, adjacent engineering, and adjacent non-engineering)
deployed at critical junctures throughout a 6-step task workflow. In order to
"future proof" developers for this age of AI, on-the-job learning initiatives
and computer science degree programs will need to target both "soft" skills and
the technical skills & knowledge in all four domains to reskill, upskill and
safeguard against deskilling.

</details>


### [411] [RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents](https://arxiv.org/pdf/2506.00618)
*Jingyi Yang, Shuai Shao, Dongrui Liu, Jing Shao*

Main category: cs.AI

TL;DR: The paper introduces RIOSWorld, a benchmark to evaluate safety risks of MLLM-based computer-use agents in real-world scenarios, addressing gaps in existing research.


<details>
  <summary>Details</summary>
Motivation: Existing safety risk evaluations for MLLM-based agents lack realistic environments or focus narrowly, ignoring real-world complexity.

Method: RIOSWorld includes 492 risky tasks across various applications, categorizing risks into user-originated and environmental. Evaluation considers risk goal intention and completion.

Result: Experiments show current agents face significant safety risks, emphasizing the need for safety alignment.

Conclusion: The benchmark underscores the urgency of safety alignment for trustworthy computer-use agents and is publicly available.

Abstract: With the rapid development of multimodal large language models (MLLMs), they
are increasingly deployed as autonomous computer-use agents capable of
accomplishing complex computer tasks. However, a pressing issue arises: Can the
safety risk principles designed and aligned for general MLLMs in dialogue
scenarios be effectively transferred to real-world computer-use scenarios?
Existing research on evaluating the safety risks of MLLM-based computer-use
agents suffers from several limitations: it either lacks realistic interactive
environments, or narrowly focuses on one or a few specific risk types. These
limitations ignore the complexity, variability, and diversity of real-world
environments, thereby restricting comprehensive risk evaluation for
computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark
designed to evaluate the potential risks of MLLM-based agents during real-world
computer manipulations. Our benchmark includes 492 risky tasks spanning various
computer applications, involving web, social media, multimedia, os, email, and
office software. We categorize these risks into two major classes based on
their risk source: (i) User-originated risks and (ii) Environmental risks. For
the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal
intention and (ii) Risk goal completion. Extensive experiments with multimodal
agents on \textbf{RiOSWorld} demonstrate that current computer-use agents
confront significant safety risks in real-world scenarios. Our findings
highlight the necessity and urgency of safety alignment for computer-use agents
in real-world computer manipulation, providing valuable insights for developing
trustworthy computer-use agents. Our benchmark is publicly available at
https://yjyddq.github.io/RiOSWorld.github.io/.

</details>


### [412] [MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch](https://arxiv.org/pdf/2506.01056)
*Xiang Fei, Xiawu Zheng, Hao Feng*

Main category: cs.AI

TL;DR: MCP-Zero is a proactive agent framework for LLMs that autonomously retrieves and assembles task-specific toolchains, reducing context overhead and token costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for tool-calling in LLMs are costly and error-prone due to injecting large tool schemas into prompts.

Method: MCP-Zero uses Proactive Tool Request, Hierarchical Vector Routing, and Iterative Proactive Invocation to dynamically retrieve and refine toolchains.

Result: The framework reduces token consumption by 98%, accurately selects tools from 3,000 candidates, and supports multi-turn tool invocation.

Conclusion: MCP-Zero effectively addresses context overhead and tool retrieval challenges in LLMs, enabling efficient and accurate tool usage.

Abstract: Function-calling has enabled large language models (LLMs) to act as
tool-using agents, but injecting thousands of tool schemas into the prompt is
costly and error-prone. We introduce MCP-Zero, a proactive agent framework that
lets the LLM itself decide when and which external tools to retrieve, thereby
assembling a task-specific toolchain from scratch. The framework is built upon
three components: (1) Proactive Tool Request, where the model emits a
structured $\left<\operatorname{tool\_assistant}\right>$ block that explicitly
specifies the desired server and task; (2) Hierarchical Vector Routing, a
coarse-to-fine retrieval algorithm that first selects candidate servers and
then ranks tools within each server based on the semantic similarity; (3)
Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain
construction with minimal context overhead, and allowing the model to
iteratively revise its request when the returned tools are insufficient. To
evaluate our approach we also compile MCP-tools, a retrieval dataset comprising
308 MCP servers and 2,797 tools extracted from the official
Model-Context-Protocol repository and normalized into a unified JSON schema.
Experiments show that MCP-Zero (i) effectively addresses the context overhead
problem of existing methods and accurately selects the correct tool from a pool
of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by
98\% on the APIbank while maintaining high accuracy; and (iii) supports
multi-turn tool invocation with consistent accuracy across rounds.

</details>


### [413] [MobCLIP: Learning General-purpose Geospatial Representation at Scale](https://arxiv.org/pdf/2506.01297)
*Ya Wen, Jixuan Cai, Qiyao Ma, Linyan Li, Xinhua Chen, Chris Webster, Yulun Zhou*

Main category: cs.AI

TL;DR: MobCLIP is a general-purpose location encoder integrating diverse data modalities for geospatial intelligence, outperforming state-of-the-art models by 35% on average.


<details>
  <summary>Details</summary>
Motivation: Current geospatial embedding methods lack versatility, limiting their utility across diverse tasks.

Method: MobCLIP uses a CLIP-based architecture to align POIs, remote sensing imagery, demographic statistics, and a mobility graph, tokenizing locations into grid cells.

Result: MobCLIP achieves superior performance, especially in human-centric tasks (e.g., +260% in energy consumption prediction).

Conclusion: MobCLIP demonstrates scalable geospatial representation learning and is open-sourced for broader use.

Abstract: Representation learning of geospatial locations remains a core challenge in
achieving general geospatial intelligence. Current embedding methods often lack
versatility, limiting their utility across diverse tasks in both human and
natural domains. We present MobCLIP, the first nationwide general-purpose
location encoder, integrating an unprecedented diversity of data modalities
through effective and scalable multimodal fusion. Adopting a novel CLIP-based
architecture, our framework aligns 100M+ POIs, nationwide remote sensing
imagery, and structured demographic statistics with a billion-edge mobility
graph. By tokenizing spatial locations into grid cells inspired by Vision
Transformers, we establish a unified representation space bridging mobility
patterns and multimodal features. To rigorously evaluate the general-purpose
effectiveness of MobCLIP, we construct a benchmark dataset composed of 11
downstream prediction tasks across social, economic, and natural domains.
Experiments show that MobCLIP, with four input modalities and a compact
128-dimensional representation space, achieves significantly superior
general-purpose predictive performances than state-of-the-art models by an
average of 35%. Thanks to the effective integration of human-centric
modalities, the performance gain is particularly profound in human-centric
tasks, such as energy consumption (+260%), offline retail consumption amount
(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we
further demonstrate the scaling behavior in geospatial representation learning.
We open-source code and pretrained models at:
https://github.com/ylzhouchris/MobCLIP.

</details>


### [414] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/pdf/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: The paper introduces the Unified Cognitive Consciousness Theory (UCCT) to explain why some tasks in few-shot learning generalize easily while others require more supervision, framing LLMs as unconscious substrates for general intelligence.


<details>
  <summary>Details</summary>
Motivation: To address the paradox in few-shot learning where some tasks generalize with minimal examples while others need extensive supervision, and to reconceptualize LLMs as foundational substrates rather than flawed cognitive agents.

Method: Proposes UCCT, which views LLMs as unconscious pattern repositories and introduces semantic anchoring (via prompts, role assignments, etc.) as a conscious control layer to modulate task-relevant reasoning.

Result: UCCT unifies various techniques (prompting, fine-tuning, retrieval-augmented generalization) under one framework, enabling LLMs to operate coherently in specialized domains like legal or medical reasoning.

Conclusion: The paper advocates integrating LLMs through a structured cognitive layer to support intentional reasoning, highlighting phase-transition behavior in anchored representations.

Abstract: Few-shot learning in large language models (LLMs) reveals a core paradox:
certain tasks generalize from just a few examples, while others demand
extensive supervision. To explain this, we introduce the Unified Cognitive
Consciousness Theory (UCCT), which reconceptualizes LLMs not as deficient
agents, but as unconscious substrates: dense, distributed repositories of
linguistic and conceptual patterns that operate without explicit semantics,
intention, or goal-directed reasoning. Under this view, LLMs are not flawed
simulations of cognition but foundational substrates for general intelligence.
UCCT posits that semantic anchoring, via prompts, role assignments, and
structured interaction, functions as a conscious control layer that modulates
latent representations toward task-relevant semantics and enables coherent,
structured reasoning. It unifies prompting, fine-tuning, retrieval-augmented
generalization, and multi-agent collaboration within a single framework,
grounded in the probabilistic alignment between unconscious pattern space and
externally imposed semantic constraints (e.g., prompts, supervision, task
objectives). The core implication is not to replace LLMs, but to integrate and
unify them through a structured cognitive layer that supports intentional
reasoning. This enables collections of LLMs to operate within
domain-specialized verticals (e.g., legal reasoning, medical diagnosis) that
reason, regulate, and adapt together. Such integration is characterized by
phase-transition behavior, wherein anchored representations cross coherence
thresholds as a function of semantic constraint strength and interaction
context.

</details>


### [415] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/pdf/2506.02576)
*Haichen Wang, Liu Yang, Xinyuan Zhang, Haomin Yu, Ming Li, Jilin Hu*

Main category: cs.AI

TL;DR: The paper introduces ADFormer, a model for passenger demand forecasting that improves urban efficiency by better capturing spatio-temporal correlations using Differential Attention and aggregation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully adapt to complex spatio-temporal correlations and overlook high-level correlations, limiting their effectiveness.

Method: Proposes ADFormer with Differential Attention for spatial correlation capture and denoising, and distinct aggregation strategies for space and time to unify original and high-level correlations.

Result: Experiments on taxi and bike datasets confirm ADFormer's effectiveness and efficiency.

Conclusion: ADFormer successfully integrates high-level correlations with original ones, demonstrating practical value for demand forecasting.

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [416] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/pdf/2506.02867)
*Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao*

Main category: cs.AI

TL;DR: The paper explores the reasoning mechanisms of large reasoning models (LRMs) through mutual information (MI) analysis, identifying 'thinking tokens' that boost performance and proposing methods to enhance LRM reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the internal reasoning mechanisms of LRMs, which remain unclear despite their problem-solving capabilities.

Method: Track mutual information (MI) between intermediate representations and correct answers during LRM reasoning, identifying 'thinking tokens' like 'Hmm' or 'Therefore.'

Result: Observed MI peaks correlate with reduced prediction errors; thinking tokens significantly impact reasoning performance.

Conclusion: The study offers insights into LRM reasoning and practical methods to improve their performance by leveraging thinking tokens.

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [417] [Conformer-based Ultrasound-to-Speech Conversion](https://arxiv.org/pdf/2506.03831)
*Ibrahim Ibrahimov, Zainkó Csaba, Gábor Gosztolya*

Main category: cs.SD

TL;DR: Conformer-based DNNs (Base and bi-LSTM) were tested for ultrasound-to-speech conversion. While objective metrics showed no improvement over a 2D-CNN baseline, perceptual quality was better with bi-LSTM, and Base matched baseline performance with faster training.


<details>
  <summary>Details</summary>
Motivation: To explore Conformer-based DNNs as alternatives to CNNs for ultrasound-to-speech conversion in Silent Speech Interfaces.

Method: Used Conformer architectures (Base and bi-LSTM) trained on Ultrasuite-Tal80 data, with HiFi-GAN vocoder for waveform synthesis. Compared to a 2D-CNN baseline.

Result: No significant objective improvement, but bi-LSTM had better perceptual quality, and Base matched baseline with 3x faster training.

Conclusion: Conformer-based models, especially with bi-LSTM, are promising for ultrasound-to-speech conversion despite no objective gains.

Abstract: Deep neural networks have shown promising potential for ultrasound-to-speech
conversion task towards Silent Speech Interfaces. In this work, we applied two
Conformer-based DNN architectures (Base and one with bi-LSTM) for this task.
Speaker-specific models were trained on the data of four speakers from the
Ultrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized
to audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN
baseline, objective measurements (MSE and mel cepstral distortion) showed no
statistically significant improvement for either model. However, a MUSHRA
listening test revealed that Conformer with bi-LSTM provided better perceptual
quality, while Conformer Base matched the performance of the baseline along
with a 3x faster training time due to its simpler architecture. These findings
suggest that Conformer-based models, especially the Conformer with bi-LSTM,
offer a promising alternative to CNNs for ultrasound-to-speech conversion.

</details>


### [418] [Local Equivariance Error-Based Metrics for Evaluating Sampling-Frequency-Independent Property of Neural Network](https://arxiv.org/pdf/2506.03550)
*Kanami Imamura, Tomohiko Nakamura, Norihiro Takamune, Kohei Yatabe, Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: The paper proposes metrics to measure the robustness of deep neural networks (DNNs) to sampling frequency (SF) changes in audio signal processing, addressing performance degradation at untrained SFs.


<details>
  <summary>Details</summary>
Motivation: Most DNN-based audio signal processing methods are trained at a single SF, leading to performance issues when handling untrained SFs due to resampling. This problem is often overlooked as evaluations typically focus on trained SFs.

Method: The authors introduce three metrics based on local equivariance error (LEE) to quantify the SF-independent (SFI) property, extending LEE to measure robustness to signal resampling in audio source separation.

Result: Experiments on music source separation showed a strong correlation between the proposed metrics and performance degradation at untrained SFs.

Conclusion: The proposed metrics effectively assess the SFI property, highlighting the need for robustness in DNNs to SF changes in audio processing.

Abstract: Audio signal processing methods based on deep neural networks (DNNs) are
typically trained only at a single sampling frequency (SF) and therefore
require signal resampling to handle untrained SFs. However, recent studies have
shown that signal resampling can degrade performance with untrained SFs. This
problem has been overlooked because most studies evaluate only the performance
at trained SFs. In this paper, to assess the robustness of DNNs to SF changes,
which we refer to as the SF-independent (SFI) property, we propose three
metrics to quantify the SFI property on the basis of local equivariance error
(LEE). LEE measures the robustness of DNNs to input transformations. By using
signal resampling as input transformation, we extend LEE to measure the
robustness of audio source separation methods to signal resampling. The
proposed metrics are constructed to quantify the SFI property in specific
network components responsible for predicting time-frequency masks. Experiments
on music source separation demonstrated a strong correlation between the
proposed metrics and performance degradation at untrained SFs.

</details>


### [419] [Comparative Analysis of Fast and High-Fidelity Neural Vocoders for Low-Latency Streaming Synthesis in Resource-Constrained Environments](https://arxiv.org/pdf/2506.03554)
*Reo Yoneyama, Masaya Kawamura, Ryo Terashima, Ryuichi Yamamoto, Tomoki Toda*

Main category: cs.SD

TL;DR: MS-Wavehax is an efficient neural vocoder for low-latency streaming, addressing inefficiencies like limited parallelism and parameter loading overhead. It extends Wavehax with multi-stream decomposition, optimizing latency-throughput trade-offs and delivering high speech quality.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in streaming neural vocoders, such as limited parallelism and parameter loading overhead, while maintaining low-latency synthesis.

Method: Extends Wavehax with multi-stream decomposition, analyzes latency-throughput trade-offs, and optimizes chunk sizes for streaming.

Result: MS-Wavehax achieves high speech quality under causal and non-causal conditions, is compact, and deployable in resource-constrained environments.

Conclusion: The paper provides practical insights for optimizing streaming neural vocoders, demonstrating MS-Wavehax's effectiveness in balancing latency, throughput, and quality.

Abstract: In real-time speech synthesis, neural vocoders often require low-latency
synthesis through causal processing and streaming. However, streaming
introduces inefficiencies absent in batch synthesis, such as limited
parallelism, inter-frame dependency management, and parameter loading overhead.
This paper proposes multi-stream Wavehax (MS-Wavehax), an efficient neural
vocoder for low-latency streaming, by extending the aliasing-free neural
vocoder Wavehax with multi-stream decomposition. We analyze the
latency-throughput trade-off in a CPU-only environment and identify key
bottlenecks in streaming neural vocoders. Our findings provide practical
insights for optimizing chunk sizes and designing vocoders tailored to specific
application demands and hardware constraints. Furthermore, our subjective
evaluations show that MS-Wavehax delivers high speech quality under causal and
non-causal conditions while being remarkably compact and easily deployable in
resource-constrained environments.

</details>


### [420] [From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder Framework for Auditory Perception and Cochlear Implant Simulation](https://arxiv.org/pdf/2506.03959)
*Jacob de Nobel, Jeroen J. Briaire, Thomas H. W. Baeck, Anna V. Kononova, Johan H. M. Frijns*

Main category: cs.SD

TL;DR: NeuroVoc is a flexible vocoder framework for reconstructing speech from neural activity, enabling comparisons between normal and electrical hearing models while preserving model-specific features.


<details>
  <summary>Details</summary>
Motivation: To simplify auditory perception simulations for cochlear implant users by eliminating the need for speech-coding-strategy-specific vocoders and enabling direct comparisons between normal and electrical hearing models.

Method: Uses an inverse Fourier transform on neurogram representations from auditory nerve fiber models, with a modular architecture for easy model substitution.

Result: The vocoder preserved model-specific features (e.g., harmonic structure) and demonstrated perceptual intelligibility in noise tests, with results aligning with clinical data for normal and cochlear implant listeners.

Conclusion: NeuroVoc effectively reconstructs intelligible speech and accurately reflects the speech-in-noise performance differences between normal and electrical hearing.

Abstract: We present NeuroVoc, a flexible model-agnostic vocoder framework that
reconstructs acoustic waveforms from simulated neural activity patterns using
an inverse Fourier transform. The system applies straightforward signal
processing to neurogram representations, time-frequency binned outputs from
auditory nerve fiber models. Crucially, the model architecture is modular,
allowing for easy substitution or modification of the underlying auditory
models. This flexibility eliminates the need for
speech-coding-strategy-specific vocoder implementations when simulating
auditory perception in cochlear implant (CI) users. It also allows direct
comparisons between normal hearing (NH) and electrical hearing (EH) models, as
demonstrated in this study. The vocoder preserves distinctive features of each
model; for example, the NH model retains harmonic structure more faithfully
than the EH model. We evaluated perceptual intelligibility in noise using an
online Digits-in-Noise (DIN) test, where participants completed three test
conditions: one with standard speech, and two with vocoded speech using the NH
and EH models. Both the standard DIN test and the EH-vocoded groups were
statistically equivalent to clinically reported data for NH and CI listeners.
On average, the NH and EH vocoded groups increased SRT compared to the standard
test by 2.4 dB and 7.1 dB, respectively. These findings show that, although
some degradation occurs, the vocoder can reconstruct intelligible speech under
both hearing models and accurately reflects the reduced speech-in-noise
performance experienced by CI users.

</details>


### [421] [Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion](https://arxiv.org/pdf/2506.04013)
*Seymanur Akti, Tuan Nam Nguyen, Alexander Waibel*

Main category: cs.SD

TL;DR: The paper improves expressive voice conversion by reducing source timbre leakage and enhancing style transfer using multilingual speech units, augmentation-based loss, and enriched style embeddings.


<details>
  <summary>Details</summary>
Motivation: To achieve better transfer of both speaker identity and expressive attributes while minimizing source style leakage.

Method: Uses a conditional variational autoencoder with multilingual discrete speech units, augmentation-based similarity loss, mix-style layer normalization, and enriched style embeddings with global pitch and energy features.

Result: Outperforms baselines in emotion and speaker similarity, showing superior style adaptation and reduced source style leakage.

Conclusion: The proposed framework effectively improves expressive voice conversion by addressing style leakage and enhancing disentanglement.

Abstract: Expressive voice conversion aims to transfer both speaker identity and
expressive attributes from a target speech to a given source speech. In this
work, we improve over a self-supervised, non-autoregressive framework with a
conditional variational autoencoder, focusing on reducing source timbre leakage
and improving linguistic-acoustic disentanglement for better style transfer. To
minimize style leakage, we use multilingual discrete speech units for content
representation and reinforce embeddings with augmentation-based similarity loss
and mix-style layer normalization. To enhance expressivity transfer, we
incorporate local F0 information via cross-attention and extract style
embeddings enriched with global pitch and energy features. Experiments show our
model outperforms baselines in emotion and speaker similarity, demonstrating
superior style adaptation and reduced source style leakage.

</details>


### [422] [A Statistics-Driven Differentiable Approach for Sound Texture Synthesis and Analysis](https://arxiv.org/pdf/2506.04073)
*Esteban Gutiérrez, Frederic Font, Xavier Serra, Lonce Wyse*

Main category: cs.SD

TL;DR: TexStat is a novel loss function for texture sound analysis/synthesis, validated with FAD. TexEnv synthesizes texture sounds, integrated into TexDSP. Both are open-source, efficient, and perceptually effective.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing and synthesizing texture sounds with stochastic structure and perceptual stationarity without relying on temporal structure.

Method: TexStat identifies texture similarities statistically and perceptually. TexEnv synthesizes sounds using amplitude envelopes on filtered noise. Both are integrated into TexDSP, a generative model.

Result: TexStat is perceptually meaningful, time-invariant, and noise-robust, effective as a loss function and validation metric. TexDSP demonstrates strong performance across texture types.

Conclusion: TexStat and TexEnv provide efficient, perceptually grounded tools for texture sound tasks, with open-source implementations for broader use.

Abstract: In this work, we introduce TexStat, a novel loss function specifically
designed for the analysis and synthesis of texture sounds characterized by
stochastic structure and perceptual stationarity. Drawing inspiration from the
statistical and perceptual framework of McDermott and Simoncelli, TexStat
identifies similarities between signals belonging to the same texture category
without relying on temporal structure. We also propose using TexStat as a
validation metric alongside Frechet Audio Distances (FAD) to evaluate texture
sound synthesis models. In addition to TexStat, we present TexEnv, an
efficient, lightweight and differentiable texture sound synthesizer that
generates audio by imposing amplitude envelopes on filtered noise. We further
integrate these components into TexDSP, a DDSP-inspired generative model
tailored for texture sounds. Through extensive experiments across various
texture sound types, we demonstrate that TexStat is perceptually meaningful,
time-invariant, and robust to noise, features that make it effective both as a
loss function for generative tasks and as a validation metric. All tools and
code are provided as open-source contributions and our PyTorch implementations
are efficient, differentiable, and highly configurable, enabling its use in
both generative tasks and as a perceptually grounded evaluation metric.

</details>


### [423] [Neural Scoring: A Refreshed End-to-End Approach for Speaker Recognition in Complex Conditions](https://arxiv.org/pdf/2410.16428)
*Wan Lin, Junhui Chen, Tianhao Wang, Zhenyu Zhou, Lantian Li, Dong Wang*

Main category: cs.SD

TL;DR: Neural Scoring (NS) is an end-to-end framework for speaker verification that outperforms traditional methods, especially in multi-talker scenarios, reducing EER by 70.36%.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker verification methods struggle with multi-talker speech due to unidentifiable embeddings.

Method: Proposes Neural Scoring (NS), an end-to-end framework that directly estimates verification probabilities without test-side embeddings, using multi-enrollment training.

Result: NS achieves a 70.36% reduction in Equal Error Rate (EER) on the VoxCeleb dataset.

Conclusion: NS is more powerful and robust for speaker verification, particularly in complex conditions like multi-talker speech.

Abstract: Modern speaker verification systems primarily rely on speaker embeddings and
cosine similarity. While effective, these methods struggle with multi-talker
speech due to the unidentifiability of embedding vectors. We propose Neural
Scoring (NS), a novel end-to-end framework that directly estimates verification
posterior probabilities without relying on test-side embeddings, making it more
powerful and robust to complex conditions, e.g., with multiple talkers. To
address the challenge of training such end-to-end models, we introduce a
multi-enrollment training strategy, which pairs each test utterance with
multiple enrolled speakers and proves essential to the model's success.
Experiments on the VoxCeleb dataset demonstrate that NS consistently
outperforms both the baseline and several competitive methods, achieving an
overall 70.36% reduction in Equal Error Rate (EER) compared to the baseline.

</details>


### [424] [ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors](https://arxiv.org/pdf/2502.14627)
*Yuguo Yin, Yuxin Xie, Wenyuan Yang, Dongchao Yang, Jinghan Ru, Xianwei Zhuang, Liming Liang, Yuexian Zou*

Main category: cs.SD

TL;DR: The paper addresses inconsistency in multilingual audio-text retrieval (ML-ATR) by analyzing errors and proposing a contrastive learning scheme, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing ML-ATR schemes struggle with inconsistency in similarity matching across languages, prompting a theoretical and practical solution.

Method: The authors analyze inconsistency via modal alignment and weight errors, then propose 1-to-k contrastive learning and audio-English co-anchor contrastive learning.

Result: The proposed scheme outperforms others on recall and consistency metrics for eight languages, as validated on AudioCaps and Clotho datasets.

Conclusion: The study successfully mitigates data distribution errors in ML-ATR, enhancing retrieval consistency and performance.

Abstract: Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to
retrieve audio clips or multilingual texts from databases. However, existing
ML-ATR schemes suffer from inconsistencies for instance similarity matching
across languages. We theoretically analyze the inconsistency in terms of both
multilingual modal alignment direction error and weight error, and propose the
theoretical weight error upper bound for quantifying the inconsistency. Based
on the analysis of the weight error upper bound, we find that the inconsistency
problem stems from the data distribution error caused by random sampling of
languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive
learning and audio-English co-anchor contrastive learning, aiming to mitigate
the negative impact of data distribution error on recall and consistency in
ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets
show that our scheme achieves state-of-the-art performance on recall and
consistency metrics for eight mainstream languages, including English. Our code
will be available at https://github.com/ATRI-ACL/ATRI-ACL.

</details>


### [425] [InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training](https://arxiv.org/pdf/2503.02769)
*Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, Junyang Lin*

Main category: cs.SD

TL;DR: InSerter, a scalable training method, improves speech instruction adherence in SpeechLLMs by pre-training on speech-text sequences, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Current SpeechLLMs underperform in speech instruction adherence due to semantic inconsistency between speech and text representations.

Method: InSerter pre-trains on large-scale unsupervised speech-text sequences, using synthesized speech from text segments, eliminating intensive data design.

Result: InSerter achieves SOTA performance in SpeechInstructBench and excels in diverse speech tasks.

Conclusion: InSerter offers a simple, effective solution for enhancing speech instruction-following in SpeechLLMs.

Abstract: Recent advancements in speech large language models (SpeechLLMs) have
attracted considerable attention. Nonetheless, current methods exhibit
suboptimal performance in adhering to speech instructions. Notably, the
intelligence of models significantly diminishes when processing speech-form
input as compared to direct text-form input. Prior work has attempted to
mitigate this semantic inconsistency between speech and text representations
through techniques such as representation and behavior alignment, which involve
the meticulous design of data pairs during the post-training phase. In this
paper, we introduce a simple and scalable training method called InSerter,
which stands for Interleaved Speech-Text Representation Pre-training. InSerter
is designed to pre-train large-scale unsupervised speech-text sequences, where
the speech is synthesized from randomly selected segments of an extensive text
corpus using text-to-speech conversion. Consequently, the model acquires the
ability to generate textual continuations corresponding to the provided speech
segments, obviating the need for intensive data design endeavors. To
systematically evaluate speech instruction-following capabilities, we introduce
SpeechInstructBench, the first comprehensive benchmark specifically designed
for speech-oriented instruction-following tasks. Our proposed InSerter achieves
SOTA performance in SpeechInstructBench and demonstrates superior or
competitive results across diverse speech processing tasks.

</details>


### [426] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/pdf/2505.14470)
*Nadav Har-Tuv, Or Tal, Yossi Adi*

Main category: cs.SD

TL;DR: PAST is an end-to-end framework for joint phonetic and signal modeling, outperforming baselines in phonetic representation and speech reconstruction, and enabling real-time applications.


<details>
  <summary>Details</summary>
Motivation: To eliminate reliance on pretrained models and integrate phonetic knowledge directly into tokenization.

Method: Uses supervised phonetic data with auxiliary tasks and introduces a causal, streamable variant for real-time use.

Result: Surpasses baselines in metrics and excels as a speech representation for language models.

Conclusion: PAST is effective for spoken language generation and is publicly available for further research.

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [427] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/pdf/2506.03154)
*Zhaoyang Chen, Cody Fleming*

Main category: cs.LG

TL;DR: The paper proposes modular training methods for diffusion-based reinforcement learning, decoupling the guidance module from the diffusion model to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on joint training of the guidance module and diffusion model, which can be suboptimal due to noisy early-stage guidance. The paper aims to optimize this by modularizing the training process.

Method: The method involves training the guidance module independently first as a value estimator, then freezing it to guide the diffusion model. It also explores cross-module transferability by reusing guidance models across different algorithms.

Result: The modular approach reduces memory usage, improves computational efficiency, and enhances sample efficiency and final performance. Cross-module transferability reduces score variance (e.g., IQR by 86%) and demonstrates strong modularity.

Conclusion: The findings suggest a new paradigm for offline RL with modular, reusable, and composable training pipelines, validated theoretically and empirically on D4RL benchmarks.

Abstract: Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [428] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/pdf/2506.03155)
*Yu Zheng*

Main category: cs.LG

TL;DR: The paper introduces a framework for cross-domain multimodal data fusion to address real-world problems when data is inadequate in the target domain.


<details>
  <summary>Details</summary>
Motivation: The complexity of physical environments necessitates multimodal data fusion, but existing methods assume alignment within a single domain, which fails in cross-domain scenarios.

Method: A four-layer framework (Domains, Links, Models, Data) is proposed to address 'what to fuse', 'why can be fused', and 'how to fuse'.

Result: The framework enables effective end-to-end solutions for cross-domain multimodal data fusion.

Conclusion: The proposed framework overcomes challenges of cross-domain fusion, offering a structured approach for real-world problem-solving.

Abstract: The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [429] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/pdf/2506.03158)
*Jiahao Qin, Bei Peng, Feng Liu, Guangliang Cheng, Lu Zong*

Main category: cs.LG

TL;DR: DUAL is a framework for handling feature uncertainty in deep learning, improving performance in single-modal and multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing feature uncertainty in deep learning, especially in multi-modal scenarios, to enhance model reliability and performance.

Method: DUAL introduces Dynamic Feature Uncertainty Modeling, Adaptive Distribution-Aware Modulation, and Uncertainty-aware Cross-Modal Relationship Learning.

Result: Achieves significant accuracy improvements: 7.1% on CIFAR-10, 6.5% on CIFAR-100, 2.3% on Tiny-ImageNet, 4.1% on CMU-MOSEI, 2.8% on CMU-MOSI, and 1.4% on MISR.

Conclusion: DUAL effectively addresses feature uncertainty, demonstrating consistent performance gains across diverse tasks.

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [430] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/pdf/2506.03159)
*Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi*

Main category: cs.LG

TL;DR: The paper evaluates BER estimators for classification accuracy, finding kNN as the most accurate non-parametric method, requiring 1000-2500 samples per class for reliable results.


<details>
  <summary>Details</summary>
Motivation: To identify accurate BER estimators for practical use in classification problems with limited samples and unknown distributions.

Method: Monte Carlo simulations with synthetic and real-world data, comparing kNN, GHP divergence, and KDE techniques.

Result: kNN outperforms other estimators, needing 1000-2500 samples per class for under 5% error in 95% confidence bounds. Other methods fail to meet targets.

Conclusion: kNN is the most reliable BER estimator for binary classification, though sample requirements increase with feature dimensionality.

Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [431] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/pdf/2506.03160)
*Shriyank Somvanshi, Anannya Ghosh Tusti, Mahmuda Sultana Mimi, Md Monzurul Islam, Sazzad Bin Bashar Polock, Anandi Dutta, Subasish Das*

Main category: cs.LG

TL;DR: The study evaluates three deep learning models (MambaAttention, TabPFN, TabTransformer) for classifying SAE automation levels in AV crashes, finding MambaAttention most effective, while TabTransformer struggles with partial automation cases.


<details>
  <summary>Details</summary>
Motivation: Accurate classification of SAE automation levels in AV crashes is crucial for understanding crash dynamics and system accountability, but existing methods lack sophistication.

Method: The study uses structured crash data from Texas (2024) and evaluates three models (MambaAttention, TabPFN, TabTransformer) after class balancing with SMOTEENN.

Result: MambaAttention achieved the highest F1-scores (88% for SAE 1, 97% for SAE 2, 99% for SAE 3-5), while TabTransformer underperformed (55% for SAE 2).

Conclusion: Deep learning models tailored for tabular data can improve automation-level classification, aiding policy and safety evaluations for AVs.

Abstract: The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [432] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/pdf/2506.03161)
*Mira Nuthakki*

Main category: cs.LG

TL;DR: A reinforcement learning framework with custom rewards, using PPO, significantly improves traffic safety and efficiency in a 3D simulation, reducing collisions and emissions.


<details>
  <summary>Details</summary>
Motivation: Addressing traffic congestion and collisions, which are major global challenges, by leveraging advanced simulation and reinforcement learning.

Method: Developed a 3D city-wide simulation, a collision model, and a PPO-based reinforcement learning framework with safety-focused rewards.

Result: Achieved 3x reduction in collisions, 39% better fuel efficiency, and 88% lower emissions compared to baselines.

Conclusion: The approach is feasible for real-world traffic management, aligning with safety and environmental goals.

Abstract: Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [433] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/pdf/2506.03163)
*Oluwaseyi Giwa*

Main category: cs.LG

TL;DR: Proposes a sequential regression-based algorithm with NOTEARS constraint for dynamic causal discovery in wireless networks, deriving detection delay bounds and validating them via simulations.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in causal inference due to evolving interference, fading, and mobility in wireless networks, which static models cannot handle.

Method: Uses a sequential regression-based algorithm with a novel application of the NOTEARS acyclicity constraint for efficient online updates.

Result: Theoretical bounds on detection delay are derived, showing dependence on network size, noise variance, and fading severity. Simulations confirm linear, quadratic, and inverse-square relationships.

Conclusion: Provides theoretical insights and practical guidelines for robust online causal inference in nonstationary wireless environments.

Abstract: Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [434] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/pdf/2506.03164)
*Vignav Ramesh, Morteza Mardani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [435] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/pdf/2506.03176)
*Bin Wang, Yongqi Han, Minbo Ma, Tianrui Li, Junbo Zhang, Feng Hong, Yanwei Yu*

Main category: cs.LG

TL;DR: The paper proposes Socket+Plug (SoP), a universal calibrating strategy for refining existing deep learning models in time series forecasting, addressing multi-target learning conflicts and improving performance by up to 22%.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for time series forecasting face challenges in optimizing across time steps, leading to underutilized learning capabilities. Refining models is more efficient than building new ones.

Method: Introduces SoP, which uses separate optimizers and early-stopping monitors for each target (Plug) while keeping the trained backbone (Socket) frozen, making it model-agnostic.

Result: SoP achieves up to 22% improvement on benchmarks and a spatio-temporal dataset, even with a simple MLP as the Plug.

Conclusion: SoP offers a resource-efficient way to enhance existing models, demonstrating significant performance gains without requiring new architectures.

Abstract: Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [436] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/pdf/2506.03206)
*Nadav Timor, Jonathan Mamou, Oren Pereg, Hongyang Zhang, David Harel*

Main category: cs.LG

TL;DR: RDK introduces an out-of-vocabulary sampler to boost speculative decoding efficiency by virtually restoring pruned tokens, improving acceptance rates even with extreme vocabulary pruning.


<details>
  <summary>Details</summary>
Motivation: Larger vocabularies slow down drafters in speculative decoding, creating a need for efficient methods to maintain performance with smaller vocabularies.

Method: RDK uses token-affinity priors to reallocate drafter mass, offering a first-order approximation for efficient implementation.

Result: RDK achieves higher acceptance rates than existing samplers, even with over 75% vocabulary pruning, and reduces redistribution time to linear complexity.

Conclusion: RDK enables practical use of extremely pruned drafters, overcoming previous limitations in speculative decoding.

Abstract: Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [437] [Aligning Compound AI Systems via System-level DPO](https://arxiv.org/pdf/2502.17721)
*Xiangwen Wang, Yibo Jacky Zhang, Zhoujie Ding, Katherine Tsai, Haolun Wu, Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper introduces SysDPO, a framework for aligning compound AI systems with human preferences by modeling them as DAGs and extending DPO for joint system-level alignment.


<details>
  <summary>Details</summary>
Motivation: Aligning compound AI systems with human preferences is challenging due to non-differentiable interactions and system-level preference transformation issues.

Method: Formulates compound AI systems as DAGs and introduces SysDPO (with variants SysDPO-Direct and SysDPO-Sampling) for joint alignment.

Result: Empirically demonstrates effectiveness in aligning language-diffusion models and LLM collaboration systems.

Conclusion: SysDPO provides a practical solution for aligning complex AI systems with human preferences.

Abstract: Compound AI systems, comprising multiple interacting components such as LLMs,
foundation models, and external tools, have demonstrated remarkable
improvements compared to single models in various tasks. To ensure their
effective deployment in real-world applications, aligning these systems with
human preferences is crucial. However, aligning the compound system via policy
optimization, unlike the alignment of a single model, is challenging for two
main reasons: (i) non-differentiable interactions between components make
end-to-end gradient-based optimization method inapplicable, and (ii)
system-level preferences cannot be directly transformed into component-level
preferences. To address these challenges, we first formulate compound AI
systems as Directed Acyclic Graphs (DAGs), explicitly modeling both component
interactions and the associated data flows. Building on this formulation, we
introduce $\textbf{SysDPO}$, a framework that extends Direct Preference
Optimization (DPO) to enable joint system-level alignment. We propose two
variants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending
on whether we construct a system-specific preference dataset. We empirically
demonstrate the effectiveness of our approach across two applications: the
joint alignment of a language model and a diffusion model, and the joint
alignment of an LLM collaboration system.

</details>


### [438] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/pdf/2506.03207)
*Md Nahid Hasan Shuvo, Moinul Hossain*

Main category: cs.LG

TL;DR: The paper investigates privacy risks in Federated Learning (FL) by analyzing network traffic to fingerprint deep learning models, revealing high accuracy in identifying architectures, highlighting a security vulnerability.


<details>
  <summary>Details</summary>
Motivation: To explore indirect privacy breaches in FL via network traffic analysis, an overlooked area in research, and assess the feasibility of fingerprinting models.

Method: Experimental evaluation using deep learning architectures (CNN, RNN) in an FL testbed, employing SVM, Random Forest, and Gradient Boosting to analyze traffic patterns.

Result: High fingerprinting accuracy: 100% with Random Forest, ~95.7% with SVM and Gradient Boosting, enabling identification of specific architectures.

Conclusion: The study exposes a security flaw in FL, emphasizing the need for network-level protections to prevent targeted attacks.

Abstract: Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [439] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/pdf/2506.03210)
*Qiusheng Huang, Yuan Niu, Xiaohui Zhong, Anboyu Guo, Lei Chen, Dianjun Zhang, Xuefeng Zhang, Hao Li*

Main category: cs.LG

TL;DR: FuXi-Ocean is a data-driven global ocean forecasting model achieving high-resolution, six-hourly predictions at 1/12° spatial resolution, outperforming traditional and other data-driven methods.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical ocean models are computationally intensive and struggle with fine-scale accuracy, while existing data-driven models lack sub-daily resolution. FuXi-Ocean addresses these gaps.

Method: The model combines context-aware feature extraction with a predictive network using stacked attention blocks and a Mixture-of-Time (MoT) module to mitigate cumulative errors.

Result: FuXi-Ocean excels in predicting temperature, salinity, and currents across multiple depths, demonstrating superior skill.

Conclusion: FuXi-Ocean sets a new benchmark for data-driven ocean forecasting with its high-resolution, sub-daily predictions and adaptive error mitigation.

Abstract: Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [440] [Multiple-Frequencies Population-Based Training](https://arxiv.org/pdf/2506.03225)
*Waël Doulazmi, Auguste Lehuger, Marin Toromanoff, Valentin Charraut, Thibault Buhet, Fabien Moutarde*

Main category: cs.LG

TL;DR: MF-PBT addresses PBT's greediness by using sub-populations with distinct evolution frequencies and migration, improving long-term performance.


<details>
  <summary>Details</summary>
Motivation: PBT's focus on short-term improvements leads to local optima and inefficiency; MF-PBT aims to balance short and long-term optimization.

Method: MF-PBT employs sub-populations evolving at different frequencies and introduces asymmetric migration for information transfer.

Result: Experiments on Brax show MF-PBT enhances sample efficiency and long-term performance without hyperparameter tuning.

Conclusion: MF-PBT effectively mitigates PBT's greediness, offering better stability and efficiency in hyperparameter optimization.

Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [441] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/pdf/2506.03227)
*Abdelrahman Sayed Sayed, Pierre-Jean Meyer, Mohamed Ghazel*

Main category: cs.LG

TL;DR: The paper formalizes the relationship between neural ODEs and ResNets by bounding their approximation error, enabling safety verification for one model to apply to the other.


<details>
  <summary>Details</summary>
Motivation: To establish a formal connection between neural ODEs and ResNets, leveraging their approximation behaviors for efficient safety verification.

Method: Bounding the approximation error between neural ODEs and ResNets, allowing one model to serve as a verification proxy for the other.

Result: An error bound is derived, enabling safety verification on one model to guarantee safety on the other without redundant verification.

Conclusion: The approach is validated on a neural ODE example, demonstrating its utility for reversible safety verification.

Abstract: A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [442] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/pdf/2506.03230)
*Selcuk Gurses, Aozhong Zhang, Yanxia Deng, Xun Dong, Xin Li, Naigang Wang, Penghang Yin, Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo is a Parameter-Efficient Finetuning (PEFT) method that updates only diagonal blocks of weight matrices, offering stable convergence and efficiency comparable to LoRA without low-rank constraints.


<details>
  <summary>Details</summary>
Motivation: To address the performance gaps and computational inefficiencies of existing PEFT methods compared to full-model fine-tuning.

Method: DiaBlo updates only diagonal blocks of selected weight matrices, avoiding low-rank products and auxiliary initialization schemes.

Result: DiaBlo shows strong, consistent performance across tasks like reasoning, code generation, and safety alignment, with high memory efficiency and speed.

Conclusion: DiaBlo is a simple, effective PEFT approach that bridges performance gaps while maintaining efficiency.

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [443] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/pdf/2506.03234)
*Kaiwen Duan, Hongwei Yao, Yufei Chen, Ziyun Li, Tong Qiao, Zhan Qin, Cong Wang*

Main category: cs.LG

TL;DR: BadReward is a stealthy poisoning attack on RLHF-based T2I models, corrupting reward models via feature collisions in preference data, leading to improper outputs.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in RLHF for T2I models, showing how adversaries can exploit preference data poisoning.

Method: Proposes BadReward, a clean-label attack inducing feature collisions in multi-modal preference data to corrupt reward models.

Result: BadReward successfully guides T2I models to generate biased or violent imagery for targeted concepts.

Conclusion: Highlights the urgent need for robust defenses in multi-modal RLHF systems due to amplified threats.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [444] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/pdf/2506.03267)
*Shahbaz Rezaei, Avishai Halev, Xin Liu*

Main category: cs.LG

TL;DR: The paper introduces the uncertainty principle (UP) from quantum mechanics to time series XAI, showing that time and frequency domain attributions can highlight different features, necessitating multi-domain explanations.


<details>
  <summary>Details</summary>
Motivation: Existing time series XAI methods often focus on single-domain explanations (time or frequency), potentially missing distinct features highlighted in other domains.

Method: The paper applies the uncertainty principle (UP) to quantify when time and frequency domain attributions diverge, validating this across models, XAI methods, and datasets.

Result: UP violations frequently occur, indicating that time and frequency domain explanations often highlight non-counterpart features, requiring both for comprehensive interpretation.

Conclusion: Multi-domain explanations are essential for accurate and comprehensive interpretation of time series models, as demonstrated by UP violations.

Abstract: A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [445] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/pdf/2506.03302)
*James Bagrow, Josh Bongard*

Main category: cs.LG

TL;DR: Multi-exit KANs improve accuracy and interpretability by allowing predictions at multiple depths, often favoring simpler, earlier exits.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of determining optimal depth and optimization difficulty in Kolmogorov-Arnold Networks (KANs) for scientific modeling.

Method: Introduces multi-exit KANs with prediction branches at each layer and a differentiable 'learning to exit' algorithm.

Result: Outperforms standard KANs on synthetic and real-world tasks, often using simpler exits for better interpretability.

Conclusion: Multi-exit KANs provide a practical solution for balancing performance and interpretability in scientific machine learning.

Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [446] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/pdf/2506.03307)
*Kristen Goebel, William Solow, Paola Pesantez-Cabrera, Markus Keller, Alan Fern*

Main category: cs.LG

TL;DR: A novel budgeted online active learning method for finite-horizon data streams with limited labeling budgets, leveraging expert predictors and episodic knowledge, outperforming baselines in agricultural applications.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from finite-horizon data streams (e.g., daily weather data) with costly labels (e.g., plant measurements) under severe labeling budget constraints.

Method: Integrates preexisting expert predictors and episodic behavioral knowledge from unlabeled data streams, considering query budgets, finite horizons, and episodic knowledge.

Result: Outperforms baseline expert predictions, uniform query selection, and existing methods, even with highly constrained labeling budgets.

Conclusion: The approach effectively combines expert knowledge and episodic behavior for superior performance in budget-limited, finite-horizon learning scenarios.

Abstract: This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [447] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/pdf/2209.01205)
*Han Wu, Jie Yin, Bala Rajaratnam, Jianyuan Guo*

Main category: cs.LG

TL;DR: HiRe proposes a hierarchical relational learning method for few-shot KG completion, capturing entity-level, triplet-level, and context-level information to improve meta representations of relations.


<details>
  <summary>Details</summary>
Motivation: Addressing incompleteness and long-tail distribution in KGs by improving few-shot KG completion for novel relations with limited training data.

Method: HiRe jointly learns entity-level, triplet-level, and context-level relational information to refine meta representations of few-shot relations.

Result: HiRe outperforms state-of-the-art methods on benchmark datasets, demonstrating superior generalization to unseen relations.

Conclusion: HiRe effectively addresses few-shot KG completion by leveraging hierarchical relational learning, validated by experimental results.

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [448] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/pdf/2506.03320)
*Jack Bell, Luigi Quarantiello, Eric Nuertey Coleman, Lanpei Li, Malio Li, Mauro Madeddu, Elia Piccoli, Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: The paper argues that continual learning remains essential in AI, especially with the rise of LLMs, for updating models, personalization, and modular intelligence.


<details>
  <summary>Details</summary>
Motivation: To address whether continual learning is still relevant given the capabilities of large, centralized models like LLMs.

Method: The paper discusses three key reasons for continual learning: pre-training updates, fine-tuning for specialization, and compositionality for modular intelligence.

Result: Continual learning is deemed crucial for maintaining up-to-date, adaptable, and scalable AI systems.

Conclusion: Continual learning will define the future of AI through evolving, interacting models, making it more relevant than ever.

Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [449] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/pdf/2506.03324)
*Ethan Che, Hakan Ceylan, James McInerney, Nathan Kallus*

Main category: cs.LG

TL;DR: A framework for dynamic exploration rate adjustment in recommendation systems using Bayesian regret minimization and SGD, outperforming heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimizing exploration rates in recommendation systems under practical constraints like batched updates and varying user traffic.

Method: Proposes a framework using Bayesian regret minimization via SGD and Model-Predictive Control (MPC) for dynamic exploration rate adjustment.

Result: Experiments show the method adapts to batch size variations and outperforms heuristic strategies.

Conclusion: The framework provides a principled, adaptive approach to exploration in recommendation systems, improving performance over heuristics.

Abstract: Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [450] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/pdf/2506.03333)
*Juan Sebastian Rojas, Chi-Guhn Lee*

Main category: cs.LG

TL;DR: The paper extends distributional RL to the average-reward setting, introducing quantile-based algorithms for learning long-run reward distributions and differential returns.


<details>
  <summary>Details</summary>
Motivation: Current distributional RL methods focus only on discounted settings, leaving the average-reward setting unexplored.

Method: A quantile-based approach is used to develop convergent tabular algorithms for prediction and control, along with scalable variants.

Result: The algorithms perform competitively against non-distributional methods and capture detailed reward/return distribution information.

Conclusion: The work successfully bridges the gap in distributional RL for average-reward settings, offering practical and scalable solutions.

Abstract: To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [451] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/pdf/2506.03337)
*Yide Ran, Wentao Guo, Jingwei Sun, Yanzhou Pan, Xiaodong Yu, Hao Wang, Jianwen Xie, Yiran Chen, Denghui Zhang, Zhaozhuo Xu*

Main category: cs.LG

TL;DR: Meerkat introduces a sparse zeroth-order optimization method for federated LLM fine-tuning, improving communication efficiency and performance by focusing on a static, sparse subset of parameters. Meerkat-vp further enhances this by identifying extreme Non-IID clients using GradIP signals.


<details>
  <summary>Details</summary>
Motivation: Address memory and communication challenges in federated LLM fine-tuning due to massive parameter sizes and Non-IID data.

Method: Sparse zeroth-order optimization (ZO) with static parameter subset; Meerkat-vp uses GradIP trajectories for client identification.

Result: Meerkat achieves superior performance and efficiency; Meerkat-vp improves aggregated model quality by handling extreme Non-IID drift.

Conclusion: Meerkat and Meerkat-vp significantly enhance federated LLM fine-tuning efficiency and effectiveness.

Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [452] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/pdf/2506.03355)
*Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, Volkan Cevher*

Main category: cs.LG

TL;DR: LEAF is an adversarial finetuning method for CLIP text encoders, improving robustness in text domain tasks while maintaining vision performance.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on CLIP embeddings can disrupt downstream models, but text encoder robustness is unexplored.

Method: Proposes LEAF, an efficient adversarial finetuning method for CLIP text encoders, scalable to large models.

Result: Improves zero-shot adversarial accuracy in text, enhances generation quality in text-to-image models, and boosts recall in retrieval tasks.

Conclusion: Robust text encoders improve adversarial resilience and enable better text reconstruction from embeddings.

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [453] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/pdf/2506.03363)
*Divya Shyamal, Jiaqi Zhang, Caroline Uhler*

Main category: cs.LG

TL;DR: The paper introduces probabilistic factorial experimental design to efficiently study combinatorial interventions, providing optimal and near-optimal solutions for passive and active settings.


<details>
  <summary>Details</summary>
Motivation: Combinatorial interventions are crucial in fields like biomedicine but testing all combinations is infeasible for large p. The paper aims to formalize a practical experimental design.

Method: The framework uses product Bernoulli distributions to randomly assign treatments, with dosages optimized for estimating interactions. Solutions are provided for passive and multi-round active settings.

Result: A dosage of 1/2 is near-optimal for estimating k-way interactions, requiring O(kp^3k ln(p)) observations. A near-optimal acquisition function is also derived for active learning.

Conclusion: The proposed design efficiently estimates treatment interactions, validated by simulations, and offers scalable solutions for combinatorial experiments.

Abstract: A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [454] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/pdf/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: Survey on unique hard attention transformers (UHATs) for formal language recognition, comparing masked/non-masked, finite/infinite image, and general/bilinear attention scores.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of UHATs in recognizing formal languages by analyzing different variants and their relationships.

Method: Comparison of UHAT variants (masked/non-masked, finite/infinite image, general/bilinear attention scores) and theoretical analysis using first-order logic and circuit complexity.

Result: Relations between UHAT variants identified, with a lower bound in first-order logic and an upper bound in circuit complexity.

Conclusion: UHATs exhibit varied capabilities in formal language recognition, with theoretical bounds providing insights into their computational power.

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [455] [Product Quantization for Surface Soil Similarity](https://arxiv.org/pdf/2506.03374)
*Haley Dozier, Althea Henslee, Ashley Abraham, Andrew Strelzoff, Mark Chappell*

Main category: cs.LG

TL;DR: ML-based soil taxonomy improves specificity and accuracy over human-derived classifications by leveraging data-driven similarities and systematic parameter evaluation.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of human-derived soil classifications, which rely on historical data rather than observable similarities.

Method: Combines product quantization with systematic parameter and output evaluation to optimize results.

Result: Produces highly accurate and flexible soil taxonomies tailored to specific applications.

Conclusion: ML enables more precise and adaptable soil classifications than traditional methods.

Abstract: The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [456] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/pdf/2506.03392)
*Aref Ghoreishee, Abhishek Mishra, John Walsh, Anup Das, Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: A new ternary spiking neuron model is proposed to enhance representation capacity in deep Q-learning, outperforming binary and existing ternary models by addressing gradient estimation bias.


<details>
  <summary>Details</summary>
Motivation: To overcome the limited representation capacity of binary spiking neurons and the poor performance of existing ternary models in deep Q-learning tasks.

Method: Proposes a novel ternary spiking neuron model to reduce gradient estimation bias, integrated into a deep spiking Q-learning network (DSQN).

Result: The proposed model mitigates performance degradation in ternary neurons and outperforms binary neurons in Atari game tasks.

Conclusion: The ternary spiking neuron model improves DSQN performance, making it more practical for autonomous decision-making.

Abstract: We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [457] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/pdf/2506.03404)
*Walter Mayor, Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro*

Main category: cs.LG

TL;DR: The paper analyzes trade-offs in data collection for RL, focusing on PPO, and finds scaling parallel environments more effective than longer rollouts for performance.


<details>
  <summary>Details</summary>
Motivation: To understand the bias-variance trade-offs in RL data collection and their impact on optimization stability and performance.

Method: Empirical analysis of PPO, examining parallel actors, rollout lengths, and dataset sizes.

Result: Larger datasets improve performance; scaling parallel environments is more effective than longer rollouts.

Conclusion: Data collection strategies, especially parallel environments, are crucial for RL performance.

Abstract: The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [458] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/pdf/2506.03979)
*Haoxuan Chen, Yinuo Ren, Martin Renqiang Min, Lexing Ying, Zachary Izzo*

Main category: cs.LG

TL;DR: The paper proposes an ensemble-based algorithm for posterior sampling in Bayesian inverse problems using diffusion models, avoiding heuristic approximations and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current DM-based posterior sampling methods rely on heuristic approximations, limiting their effectiveness. The goal is to exploit DMs' generative capability without such approximations.

Method: The algorithm combines DM-based methods with sequential Monte Carlo (SMC), deriving a modified PDE for posterior evolution and simulating it via stochastic weighted particle methods.

Result: Theoretical bounds on posterior error are provided, and empirical validation shows more accurate reconstructions compared to existing DM-based methods.

Conclusion: The proposed method outperforms existing DM-based approaches in accuracy for inverse problems in imaging.

Abstract: Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [459] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/pdf/2506.03411)
*Melissa Dutz, Han Shao, Avrim Blum, Aloni Cohen*

Main category: cs.LG

TL;DR: The paper explores strategic litigation in a machine learning framework, analyzing how a litigator can influence future rulings by strategically bringing cases to a higher court.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of strategic litigation on legal decision-making through an abstract model of a common-law system.

Method: An abstract model where a lower court learns decision rules from higher court rulings, and a strategic litigator brings cases to influence these rules.

Result: The study examines the litigator's potential impact, optimal case selection, and scenarios where bringing losing cases may be beneficial.

Conclusion: Strategic litigation can significantly influence future rulings, and even seemingly unfavorable cases may serve a strategic purpose.

Abstract: Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [460] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/pdf/2506.03426)
*Joonseong Kang, Soojeong Lee, Subeen Park, Sumin Park, Taero Kim, Jihee Kim, Ryunyi Lee, Kyungwoo Song*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Task Vectors (ATV), a dynamic framework for generating task-specific vectors in LLMs, improving generalization and performance over fixed demonstration-based methods like ICL.


<details>
  <summary>Details</summary>
Motivation: ICL and fixed task vector methods have limitations like sensitivity to demonstration order and poor adaptation to input queries. ATV aims to dynamically tailor task vectors to each query for better performance.

Method: ATV uses a small language model to generate task vectors conditioned on each input query, transforming them to match the target LLM's architecture.

Result: ATV outperforms ICL and fixed vector methods, showing strong generalization on unseen tasks. It is theoretically more expressive than Prefix-Tuning and equivalent to LoRA.

Conclusion: ATV provides a flexible and effective solution for task adaptation in LLMs, addressing key limitations of existing methods and offering theoretical advantages.

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [461] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/pdf/2506.03444)
*Yue Gong, Raul Castro Fernandez*

Main category: cs.LG

TL;DR: The paper addresses the bottleneck of hypothesis assessment in automated systems by proposing an LLM-based method to evaluate the novelty and worthiness of statistical correlations.


<details>
  <summary>Details</summary>
Motivation: Modern systems generate many statistical relationships but lack guidance on their novelty or importance, creating a need for automated hypothesis assessment.

Method: The authors propose the Logit-based Calibrated Prior, which uses LLMs to derive a prior distribution over correlation values, assessing novelty based on how surprising the observed correlation is.

Result: The method achieves 78.8% sign accuracy, 0.26 mean absolute error, and 89.2% credible interval coverage, outperforming a fine-tuned RoBERTa classifier.

Conclusion: The LLM-based prior effectively assesses correlation novelty and generalizes to unseen correlations, demonstrating context-sensitive reasoning.

Abstract: As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [462] [Directional Non-Commutative Monoidal Embeddings for MNIST](https://arxiv.org/pdf/2506.03472)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: The paper validates a non-commutative monoidal embedding framework on MNIST, showing it outperforms fixed DFT-based embeddings, especially in low dimensions, by learning task-specific spectral components.


<details>
  <summary>Details</summary>
Motivation: To empirically validate the effectiveness of the proposed monoidal embedding framework in modeling real data, using MNIST as a controlled task.

Method: Applied the framework to MNIST, comparing learned monoidal embeddings against fixed DFT-based embeddings across varying dimensionalities.

Result: Learned embeddings outperform fixed DFT-based ones, with the gap widening as dimensionality decreases, indicating better capture of discriminative spectral components.

Conclusion: Directional non-commutative monoidal embeddings are effective for image data, offering compact, high-performing learned representations.

Abstract: We present an empirical validation of the directional non-commutative
monoidal embedding framework recently introduced in prior
work~\cite{Godavarti2025monoidal}. This framework defines learnable
compositional embeddings using distinct non-commutative operators per dimension
(axis) that satisfy an interchange law, generalizing classical one-dimensional
transforms. Our primary goal is to verify that this framework can effectively
model real data by applying it to a controlled, well-understood task: image
classification on the MNIST dataset~\cite{lecun1998gradient}. A central
hypothesis for why the proposed monoidal embedding works well is that it
generalizes the Discrete Fourier Transform (DFT)~\cite{oppenheim1999discrete}
by learning task-specific frequency components instead of using fixed basis
frequencies. We test this hypothesis by comparing learned monoidal embeddings
against fixed DFT-based embeddings on MNIST. The results show that as the
embedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance
gap between the learned monoidal embeddings and fixed DFT-based embeddings on
MNIST grows increasingly large. This comparison is used as an analytic tool to
explain why the framework performs well: the learnable embeddings can capture
the most discriminative spectral components for the task. Overall, our
experiments confirm that directional non-commutative monoidal embeddings are
highly effective for representing image data, offering a compact learned
representation that retains high task performance. The code used in this work
is available at
https://github.com/mahesh-godavarti/directional_composition_mnist.

</details>


### [463] [CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design](https://arxiv.org/pdf/2506.03474)
*Yifeng Xiao, Yurong Xu, Ning Yan, Masood Mortazavi, Pierluigi Nuzzo*

Main category: cs.LG

TL;DR: CORE is a constraint-aware, one-step RL method for efficient simulation-based design space exploration, outperforming existing methods in sample efficiency and design quality.


<details>
  <summary>Details</summary>
Motivation: Existing DSE methods struggle with balancing sampling efficiency and constraint satisfaction due to sparse feedback and large action spaces.

Method: CORE uses a structured distribution over designs, a scaling-graph-based decoder, and reward shaping to penalize invalid designs. It updates policy via a surrogate objective without a value function.

Result: CORE significantly improves sample efficiency and achieves better accelerator configurations than state-of-the-art baselines.

Conclusion: CORE is a general solution for discrete-continuous constrained design problems, demonstrating superior performance in hardware-mapping co-design.

Abstract: Simulation-based design space exploration (DSE) aims to efficiently optimize
high-dimensional structured designs under complex constraints and expensive
evaluation costs. Existing approaches, including heuristic and multi-step
reinforcement learning (RL) methods, struggle to balance sampling efficiency
and constraint satisfaction due to sparse, delayed feedback, and large hybrid
action spaces. In this paper, we introduce CORE, a constraint-aware, one-step
RL method for simulationguided DSE. In CORE, the policy agent learns to sample
design configurations by defining a structured distribution over them,
incorporating dependencies via a scaling-graph-based decoder, and by reward
shaping to penalize invalid designs based on the feedback obtained from
simulation. CORE updates the policy using a surrogate objective that compares
the rewards of designs within a sampled batch, without learning a value
function. This critic-free formulation enables efficient learning by
encouraging the selection of higher-reward designs. We instantiate CORE for
hardware-mapping co-design of neural network accelerators, demonstrating that
it significantly improves sample efficiency and achieves better accelerator
configurations compared to state-of-the-art baselines. Our approach is general
and applicable to a broad class of discrete-continuous constrained design
problems.

</details>


### [464] [Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach](https://arxiv.org/pdf/2506.03522)
*Daniel Campa, Mehdi Saeedi, Ian Colbert, Srinjoy Das*

Main category: cs.LG

TL;DR: A novel nonparametric statistical approach for generating and evaluating human-like navigation paths in video games, addressing interpretability and control issues in deep learning models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of deep learning-based path generation in games, such as complexity and lack of interpretability, by using principled statistical methods.

Method: Combines nonparametric model-free transformations for temporal characteristics and copula models for spatial dependencies, with a three-sample hypothesis test for evaluation.

Result: Demonstrates precise and reliable generation of diverse paths, with user-controllable parameters for human-likeness, validated on gaming benchmarks.

Conclusion: The proposed method offers interpretable, controllable, and reliable path generation, outperforming neural network-based approaches in usability.

Abstract: Navigation path traces play a crucial role in video game design, serving as a
vital resource for both enhancing player engagement and fine-tuning
non-playable character behavior. Generating such paths with human-like realism
can enrich the overall gaming experience, and evaluating path traces can
provide game designers insights into player interactions. Despite the
impressive recent advancements in deep learning-based generative modeling, the
video game industry hesitates to adopt such models for path generation, often
citing their complex training requirements and interpretability challenges. To
address these problems, we propose a novel path generation and evaluation
approach that is grounded in principled nonparametric statistics and provides
precise control while offering interpretable insights. Our path generation
method fuses two statistical techniques: (1) nonparametric model-free
transformations that capture statistical characteristics of path traces through
time; and (2) copula models that capture statistical dependencies in space. For
path evaluation, we adapt a nonparametric three-sample hypothesis test designed
to determine if the generated paths are overfit (mimicking the original data
too closely) or underfit (diverging too far from it). We demonstrate the
precision and reliability of our proposed methods with empirical analysis on
two existing gaming benchmarks to showcase controlled generation of diverse
navigation paths. Notably, our novel path generator can be fine-tuned with user
controllable parameters to create navigation paths that exhibit varying levels
of human-likeness in contrast to those produced by neural network-based agents.
The code is available at https://github.com/daniel-campa/mf-copula.

</details>


### [465] [Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees](https://arxiv.org/pdf/2506.03531)
*Daniel Ovalle, Lorenz T. Biegler, Ignacio E. Grossmann, Carl D. Laird, Mateo Dulce Rubio*

Main category: cs.LG

TL;DR: C-MICL is a framework ensuring probabilistic feasibility guarantees for data-driven constraints in optimization, outperforming existing methods in feasibility, performance, and cost.


<details>
  <summary>Details</summary>
Motivation: Standard methods often violate true constraints due to errors; C-MICL aims to provide reliable feasibility guarantees.

Method: Uses conformal prediction to ensure feasible solutions are ground-truth feasible, supporting regression and classification without needing the true constraint function.

Result: Achieves target feasibility rates, maintains competitive performance, and reduces computational cost in real-world applications.

Conclusion: Bridges optimization and ML, offering uncertainty-aware constraints with rigorous guarantees.

Abstract: We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel
framework that provides probabilistic feasibility guarantees for data-driven
constraints in optimization problems. While standard Mixed-Integer Constraint
Learning methods often violate the true constraints due to model error or data
limitations, our C-MICL approach leverages conformal prediction to ensure
feasible solutions are ground-truth feasible. This guarantee holds with
probability at least $1{-}\alpha$, under a conditional independence assumption.
The proposed framework supports both regression and classification tasks
without requiring access to the true constraint function, while avoiding the
scalability issues associated with ensemble-based heuristics. Experiments on
real-world applications demonstrate that C-MICL consistently achieves target
feasibility rates, maintains competitive objective performance, and
significantly reduces computational cost compared to existing methods. Our work
bridges mathematical optimization and machine learning, offering a principled
approach to incorporate uncertainty-aware constraints into decision-making with
rigorous statistical guarantees.

</details>


### [466] [Enhancing Fourier Neural Operators with Local Spatial Features](https://arxiv.org/pdf/2503.17797)
*Chaoyu Liu, Davide Murari, Lihao Liu, Yangming Li, Chris Budd, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: The paper introduces Conv-FNO, a hybrid architecture combining CNN-based local feature extraction with FNOs to enhance PDE solution approximation by capturing both local and global spatial structures.


<details>
  <summary>Details</summary>
Motivation: FNOs efficiently capture global patterns in PDEs but overlook local spatial features, limiting their performance. Existing methods for local feature extraction are insufficient or inefficient.

Method: A CNN-based feature pre-extractor is added to FNOs to capture Local Spatial Features (LSFs), creating Conv-FNO. Two novel resizing schemes ensure resolution invariance.

Result: Conv-FNO improves FNOs' representational capacity and performance on challenging PDE benchmarks, as shown by theoretical analysis and numerical experiments.

Conclusion: Incorporating LSFs into FNOs via Conv-FNO is a simple yet effective modification that significantly enhances PDE solution approximation.

Abstract: Partial Differential Equation (PDE) problems often exhibit strong local
spatial structures, and effectively capturing these structures is critical for
approximating their solutions. Recently, the Fourier Neural Operator (FNO) has
emerged as an efficient approach for solving these PDE problems. By using
parametrization in the frequency domain, FNOs can efficiently capture global
patterns. However, this approach inherently overlooks the critical role of
local spatial features, as frequency-domain parameterized convolutions
primarily emphasize global interactions without encoding comprehensive
localized spatial dependencies. Although several studies have attempted to
address this limitation, their extracted Local Spatial Features (LSFs) remain
insufficient, and computational efficiency is often compromised. To address
this limitation, we introduce a convolutional neural network (CNN)-based
feature pre-extractor to capture LSFs directly from input data, resulting in a
hybrid architecture termed \textit{Conv-FNO}. Furthermore, we introduce two
novel resizing schemes to make our Conv-FNO resolution invariant. In this work,
we focus on demonstrating the effectiveness of incorporating LSFs into FNOs by
conducting both a theoretical analysis and extensive numerical experiments. Our
findings show that this simple yet impactful modification enhances the
representational capacity of FNOs and significantly improves performance on
challenging PDE benchmarks.

</details>


### [467] [Learning Monotonic Probabilities with a Generative Cost Model](https://arxiv.org/pdf/2506.03542)
*Yongxiang Tang, Yanhua Cheng, Xiaocheng Liu, Chenchen Jiao, Yanxiang Zeng, Ning Luo, Pengjia Yuan, Xialong Liu, Peng Jiang*

Main category: cs.LG

TL;DR: The paper introduces Generative Cost Model (GCM) and Implicit Generative Cost Model (IGCM) to address strict and implicit monotonicity in machine learning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Monotonic relationships between input and output variables are crucial in many ML tasks, but traditional methods rely on construction or regularization. This paper reframes monotonicity as a partial order problem involving latent cost variables.

Method: The authors propose GCM for strict monotonicity and IGCM for implicit monotonicity, modeling the latent cost variable. The approach is validated via quantile regression simulations and experiments on public datasets.

Result: The method significantly outperforms existing monotonic modeling techniques, as demonstrated in experiments.

Conclusion: The paper successfully reformulates monotonicity challenges using latent cost variables, offering effective solutions with GCM and IGCM.

Abstract: In many machine learning tasks, it is often necessary for the relationship
between input and output variables to be monotonic, including both strictly
monotonic and implicitly monotonic relationships. Traditional methods for
maintaining monotonicity mainly rely on construction or regularization
techniques, whereas this paper shows that the issue of strict monotonic
probability can be viewed as a partial order between an observable revenue
variable and a latent cost variable. This perspective enables us to reformulate
the monotonicity challenge into modeling the latent cost variable. To tackle
this, we introduce a generative network for the latent cost variable, termed
the Generative Cost Model (GCM), which inherently addresses the strict
monotonic problem, and propose the Implicit Generative Cost Model (IGCM) to
address the implicit monotonic problem. We further validate our approach with a
numerical simulation of quantile regression and conduct multiple experiments on
public datasets, showing that our method significantly outperforms existing
monotonic modeling techniques. The code for our experiments can be found at
https://github.com/tyxaaron/GCM.

</details>


### [468] [Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning](https://arxiv.org/pdf/2506.03556)
*Wang WeiQuan, Riaz-ul-Haque Mian*

Main category: cs.LG

TL;DR: The study proposes a novel algorithm (SDE) to enhance sampling strategies in semiconductor testing, improving predictive accuracy in wafer and FPGA tests.


<details>
  <summary>Details</summary>
Motivation: High testing costs in semiconductor manufacturing drive the need for efficient sampling methods to reduce tests while maintaining accuracy.

Method: Investigates Random, Stratified, and k-means Sampling, proposes hybrid S-SDE and K-SDE strategies using SDE for uniform data distribution, evaluated via GPR.

Result: SDE-based strategies (K-SDE, S-SDE) improve accuracy: 16.26% (wafer) and 13.07% (FPGA) over k-means; 16.49% (wafer) and 8.84% (FPGA) over stratified sampling.

Conclusion: The SDE algorithm effectively enhances sampling quality, reducing testing costs while maintaining accuracy in semiconductor manufacturing.

Abstract: In semiconductor manufacturing, testing costs remain significantly high,
especially during wafer and FPGA testing. To reduce the number of required
tests while maintaining predictive accuracy, this study investigates three
baseline sampling strategies: Random Sampling, Stratified Sampling, and k-means
Clustering Sampling. To further enhance these methods, this study proposes a
novel algorithm that improves the sampling quality of each approach. This
research is conducted using real industrial production data from wafer-level
tests and silicon measurements from various FPGAs. This study introduces two
hybrid strategies: Stratified with Short Distance Elimination (S-SDE) and
k-means with Short Distance Elimination (K-SDE). Their performance is evaluated
within the framework of Gaussian Process Regression (GPR) for predicting wafer
and FPGA test data. At the core of our proposed approach is the Short Distance
Elimination (SDE) algorithm, which excludes spatially proximate candidate
points during sampling, thereby ensuring a more uniform distribution of
training data across the physical domain. A parameter sweep was conducted over
the (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,
3, 4} and not both zero, to identify the optimal combination that minimizes
RMSD. Experimental results on a randomly selected wafer file reveal that
(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent
experiments adopt this parameter configuration. The results demonstrate that
the proposed SDE-based strategies enhance predictive accuracy: K-SDE improves
upon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while
S-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84
percent (FPGA).

</details>


### [469] [A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems](https://arxiv.org/pdf/2506.03588)
*Hiroki Shiraishi, Hisao Ishibuchi, Masaya Nakata*

Main category: cs.LG

TL;DR: The paper introduces a novel class inference scheme for Learning Fuzzy-Classifier Systems (LFCSs) using Dempster-Shafer Theory to improve uncertainty handling and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing LFCSs rely on voting-based or single-winner inference schemes, which may overfit and lack transparency. The study aims to enhance decision-making in LFCSs.

Method: A new inference scheme based on Dempster-Shafer Theory calculates belief masses for classes and an "I don't know" state, improving uncertainty handling.

Result: The scheme outperforms conventional methods on 30 datasets, showing better macro F1 scores, smoother decision boundaries, and improved robustness.

Conclusion: The proposed scheme enhances LFCSs' reliability and generalizability, offering a transparent and robust alternative to traditional inference methods.

Abstract: The decision-making process significantly influences the predictions of
machine learning models. This is especially important in rule-based systems
such as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and
application of rules directly determine prediction accuracy and reliability.
LFCSs combine evolutionary algorithms with supervised learning to optimize
fuzzy classification rules, offering enhanced interpretability and robustness.
Despite these advantages, research on improving decision-making mechanisms
(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use
voting-based or single-winner-based inference schemes. These schemes rely on
classification performance on training data and may not perform well on unseen
data, risking overfitting. To address these limitations, this article
introduces a novel class inference scheme for LFCSs based on the
Dempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles
uncertainty well. By using the DS theory, the scheme calculates belief masses
(i.e., measures of belief) for each specific class and the ``I don't know''
state from each fuzzy rule and infers a class from these belief masses. Unlike
the conventional schemes, the proposed scheme also considers the ``I don't
know'' state that reflects uncertainty, thereby improving the transparency and
reliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the
proposed scheme demonstrates statistically significant improvements in terms of
test macro F1 scores across 30 real-world datasets compared to conventional
voting-based and single-winner-based fuzzy inference schemes. It forms smoother
decision boundaries, provides reliable confidence measures, and enhances the
robustness and generalizability of LFCSs in real-world applications. Our
implementation is available at https://github.com/YNU-NakataLab/jUCS.

</details>


### [470] [VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration](https://arxiv.org/pdf/2506.03590)
*Minh Luu, Surya Jasper, Khoi Le, Evan Pan, Michael Quinn, Aakash Tyagi, Jiang Hu*

Main category: cs.LG

TL;DR: VCDiag uses VCD data to classify failing waveforms and locate failures in RTL-level simulation, achieving high accuracy and significant data reduction.


<details>
  <summary>Details</summary>
Motivation: Manual failure triage in design verification is time-consuming, and ML applications for RTL-level simulation failure triage are limited.

Method: VCDiag employs signal selection and statistical compression on VCD data to classify failures and reduce data size.

Result: Achieves over 94% accuracy in identifying top failure modules and reduces raw data size by over 120x.

Conclusion: VCDiag is an efficient, adaptable framework for failure triage, compatible with diverse Verilog/SystemVerilog designs.

Abstract: Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.

</details>


### [471] [Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner](https://arxiv.org/pdf/2506.03595)
*Runa Eschenhagen, Aaron Defazio, Tsung-Hsien Lee, Richard E. Turner, Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: The paper analyzes Shampoo's heuristics in neural network training, proposing improvements like eigenvalue correction and adaptive eigenbasis updates to reduce complexity and enhance performance.


<details>
  <summary>Details</summary>
Motivation: Shampoo's reliance on heuristics like learning rate grafting and stale preconditioning increases complexity and lacks theoretical backing, prompting a need for principled alternatives.

Method: The study decouples preconditioner eigenvalue and eigenbasis updates, corrects eigenvalues directly, and introduces an adaptive criterion for eigenbasis computation frequency.

Result: Grafting from Adam mitigates preconditioner issues, and eigenvalue correction eliminates the need for grafting. Adaptive eigenbasis updates manage approximation errors.

Conclusion: The proposed techniques offer a principled approach to improve Shampoo's heuristics, advancing Kronecker-factorization-based training algorithms.

Abstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.

</details>


### [472] [Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems](https://arxiv.org/pdf/2506.03602)
*Hiroki Shiraishi, Yohei Hayamizu, Tomonori Hashiyama, Keiki Takadama, Hisao Ishibuchi, Masaya Nakata*

Main category: cs.LG

TL;DR: The paper introduces a flexible rule representation using a four-parameter beta distribution in Learning Classifier Systems (LCSs) to adaptively choose rule representations for different subspaces, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Choosing appropriate rule representations in LCSs is challenging, and some problems benefit from varied representations for different subspaces. An adaptive mechanism is needed.

Method: The proposed method integrates a four-parameter beta distribution into a fuzzy-style LCS, enabling automatic selection of rule representations for different subspaces.

Result: Experimental results show superior test accuracy and more compact rule sets compared to standard representations.

Conclusion: The flexible rule representation enhances LCS adaptability and interpretability without sacrificing accuracy, validated by real-world classification tasks.

Abstract: Rule representations significantly influence the search capabilities and
decision boundaries within the search space of Learning Classifier Systems
(LCSs), a family of rule-based machine learning systems that evolve
interpretable models through evolutionary processes. However, it is very
difficult to choose an appropriate rule representation for each problem.
Additionally, some problems benefit from using different representations for
different subspaces within the input space. Thus, an adaptive mechanism is
needed to choose an appropriate rule representation for each rule in LCSs. This
article introduces a flexible rule representation using a four-parameter beta
distribution and integrates it into a fuzzy-style LCS. The four-parameter beta
distribution can form various function shapes, and this flexibility enables our
LCS to automatically select appropriate representations for different
subspaces. Our rule representation can represent crisp/fuzzy decision
boundaries in various boundary shapes, such as rectangles and bells, by
controlling four parameters, compared to the standard representations such as
trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the
appropriate rule representation for each subspace. Moreover, our LCS
incorporates a generalization bias favoring crisp rules where feasible,
enhancing model interpretability without compromising accuracy. Experimental
results on real-world classification tasks show that our LCS achieves
significantly superior test accuracy and produces more compact rule sets. Our
implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An
extended abstract related to this work is available at
https://doi.org/10.36227/techrxiv.174900805.59801248/v1.

</details>


### [473] [GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS](https://arxiv.org/pdf/2506.03618)
*Jiayi Wan, Xiang Zhu, Fanzhen Liu, Wei Fan, Xiaolong Xu*

Main category: cs.LG

TL;DR: Proposes a server-side gradient correction framework for differentially private federated learning to balance privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: Mitigate privacy risks in CPSS while addressing noise-induced accuracy reduction in existing methods.

Method: Introduces a gradient correction mechanism to detect and correct noisy gradients, promoting alignment and convergence.

Result: Achieves state-of-the-art performance under the same privacy budget on benchmark datasets.

Conclusion: The framework effectively balances privacy and accuracy in federated learning for CPSS.

Abstract: Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.

</details>


### [474] [Out-of-Distribution Graph Models Merging](https://arxiv.org/pdf/2506.03674)
*Yidi Wang, Jiawei Gu, pei Xiaobing, Xubin Zheng, Xiao Luo, Pengyang Wang, Ziyue Qiao*

Main category: cs.LG

TL;DR: The paper addresses merging out-of-distribution graph models into a generalized model, using a graph generation strategy and MoE module for adaptation.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in learning domain-invariant knowledge from heterogeneous GNN models pre-trained on different domains.

Method: Proposes a graph generation strategy for domain mixture distribution and merges models via MoE and masking mechanisms.

Result: The framework is architecture-agnostic, requires no source/target data, and shows effectiveness in model generalization.

Conclusion: The approach successfully generalizes models across domains without needing domain-specific data.

Abstract: This paper studies a novel problem of out-of-distribution graph models
merging, which aims to construct a generalized model from multiple graph models
pre-trained on different domains with distribution discrepancy. This problem is
challenging because of the difficulty in learning domain-invariant knowledge
implicitly in model parameters and consolidating expertise from potentially
heterogeneous GNN backbones. In this work, we propose a graph generation
strategy that instantiates the mixture distribution of multiple domains. Then,
we merge and fine-tune the pre-trained graph models via a MoE module and a
masking mechanism for generalized adaptation. Our framework is
architecture-agnostic and can operate without any source/target domain data.
Both theoretical analysis and experimental results demonstrate the
effectiveness of our approach in addressing the model generalization problem.

</details>


### [475] [Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring](https://arxiv.org/pdf/2506.03696)
*Fang Wang, Paolo Ceravolo, Ernesto Damiani*

Main category: cs.LG

TL;DR: The paper proposes dynamic LSTM HyperModels for Predictive Business Process Monitoring (PBPM) to address limitations like simultaneous events, class imbalance, and multi-level attributes, achieving high accuracy and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Existing PBPM methods lack flexibility for real-world challenges like simultaneous events and data heterogeneity.

Method: Dynamic LSTM HyperModels with hierarchical encoding, character-based decomposition, and pseudo-embedding techniques, plus specialized LSTM variants for simultaneous events.

Result: Achieves up to 100% accuracy on balanced datasets and F1 scores over 86% on imbalanced ones.

Conclusion: The approach advances PBPM with modular, interpretable models and contributes to AI by improving temporal prediction and explainability.

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future
outcomes of ongoing business processes. However, existing methods often lack
flexibility to handle real-world challenges such as simultaneous events, class
imbalance, and multi-level attributes. While prior work has explored static
encoding schemes and fixed LSTM architectures, they struggle to support
adaptive representations and generalize across heterogeneous datasets. To
address these limitations, we propose a suite of dynamic LSTM HyperModels that
integrate two-level hierarchical encoding for event and sequence attributes,
character-based decomposition of event labels, and novel pseudo-embedding
techniques for durations and attribute correlations. We further introduce
specialized LSTM variants for simultaneous event modeling, leveraging
multidimensional embeddings and time-difference flag augmentation. Experimental
validation on four public and real-world datasets demonstrates up to 100%
accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones.
Our approach advances PBPM by offering modular and interpretable models better
suited for deployment in complex settings. Beyond PBPM, it contributes to the
broader AI community by improving temporal outcome prediction, supporting data
heterogeneity, and promoting explainable process intelligence frameworks.

</details>


### [476] [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](https://arxiv.org/pdf/2506.03703)
*Xiansheng Cai, Sihan Hu, Tao Wang, Yuan Huang, Pan Zhang, Youjin Deng, Kun Chen*

Main category: cs.LG

TL;DR: LaC (Learning at Criticality) is a reinforcement learning method that optimizes LLMs for peak generalization with minimal data, demonstrated in tasks like base-7 addition and quantum field theory.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of AI's reliance on large datasets in information-scarce domains like fundamental physics.

Method: LaC tunes LLMs to a sharp learning transition (criticality), where they generalize best from minimal data, supported by a concept-network model (CoNet).

Result: LLMs at criticality solve complex tasks (e.g., 7-digit base-7 addition, symbolic Matsubara sums) with few exemplars, outperforming larger models.

Conclusion: LaC leverages critical phenomena to enhance AI performance in data-sparse, complex problems in physics.

Abstract: Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.

</details>


### [477] [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://arxiv.org/pdf/2506.03719)
*Quentin Bertrand, Anne Gagneux, Mathurin Massias, Rémi Emonet*

Main category: cs.LG

TL;DR: The paper investigates whether the stochastic nature of the flow matching loss contributes to generalization in deep generative models, finding it does not significantly impact performance.


<details>
  <summary>Details</summary>
Motivation: To understand why modern generative models generalize well, specifically testing if the noisy nature of the flow matching loss is a key factor.

Method: Empirical comparison of stochastic and closed-form flow matching losses in high-dimensional settings and on standard image datasets.

Result: Both loss variants perform similarly, with the closed-form version sometimes improving performance.

Conclusion: The stochastic nature of the flow matching loss is not a primary contributor to generalization in flow matching.

Abstract: Modern deep generative models can now produce high-quality synthetic samples
that are often indistinguishable from real training data. A growing body of
research aims to understand why recent methods -- such as diffusion and flow
matching techniques -- generalize so effectively. Among the proposed
explanations are the inductive biases of deep learning architectures and the
stochastic nature of the conditional flow matching loss. In this work, we rule
out the latter -- the noisy nature of the loss -- as a primary contributor to
generalization in flow matching. First, we empirically show that in
high-dimensional settings, the stochastic and closed-form versions of the flow
matching loss yield nearly equivalent losses. Then, using state-of-the-art flow
matching models on standard image datasets, we demonstrate that both variants
achieve comparable statistical performance, with the surprising observation
that using the closed-form can even improve performance.

</details>


### [478] [Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization](https://arxiv.org/pdf/2506.03725)
*Daniil Medyakov, Sergey Stanko, Gleb Molodtsov, Philip Zmushko, Grigoriy Evseev, Egor Petrov, Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper explores variants of Sign-SGD to address the challenge of determining effective stepsizes in large language model training, extending them to practical scenarios like stochastic and multi-node learning.


<details>
  <summary>Details</summary>
Motivation: Training large language models is resource-intensive, and Sign-SGD offers a memory-efficient solution, but lacks automatic stepsize determination.

Method: Designs deterministic Sign-SGD variants, extends to stochastic and multi-node learning, and incorporates momentum.

Result: Extensive experiments demonstrate practical applicability on real machine learning problems.

Conclusion: The proposed Sign-SGD variants effectively address stepsize challenges, enhancing practical usability in resource-intensive training.

Abstract: Quite recently, large language models have made a significant breakthrough
across various disciplines. However, training them is an extremely
resource-intensive task, even for major players with vast computing resources.
One of the methods gaining popularity in light of these challenges is Sign-SGD.
This method can be applied both as a memory-efficient approach in single-node
training and as a gradient compression technique in the distributed learning.
Nevertheless, it is impossible to automatically determine the effective
stepsize from the theoretical standpoint. Indeed, it depends on the parameters
of the dataset to which we do not have access in the real-world learning
paradigm. To address this issue, we design several variants of single-node
deterministic Sign-SGD. We extend our approaches to practical scenarios:
stochastic single-node and multi-node learning, methods with incorporated
momentum. We conduct extensive experiments on real machine learning problems
that emphasize the practical applicability of our ideas.

</details>


### [479] [PPO in the Fisher-Rao geometry](https://arxiv.org/pdf/2506.03757)
*Razvan-Andrei Lascu, David Šiška, Łukasz Szpruch*

Main category: cs.LG

TL;DR: FR-PPO, a variant of PPO, improves theoretical guarantees and convergence by using Fisher-Rao geometry.


<details>
  <summary>Details</summary>
Motivation: PPO lacks formal theoretical guarantees despite its empirical success. FR-PPO addresses this gap.

Method: Derives a tighter surrogate loss in Fisher-Rao geometry, creating FR-PPO.

Result: FR-PPO ensures monotonic policy improvement and sub-linear convergence in tabular settings.

Conclusion: FR-PPO advances PPO's theoretical foundation, offering stronger guarantees and convergence.

Abstract: Proximal Policy Optimization (PPO) has become a widely adopted algorithm for
reinforcement learning, offering a practical policy gradient method with strong
empirical performance. Despite its popularity, PPO lacks formal theoretical
guarantees for policy improvement and convergence. PPO is motivated by Trust
Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL
divergence penalty, which arises from linearizing the value function within a
flat geometric space. In this paper, we derive a tighter surrogate in the
Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).
Our proposed scheme provides strong theoretical guarantees, including monotonic
policy improvement. Furthermore, in the tabular setting, we demonstrate that
FR-PPO achieves sub-linear convergence without any dependence on the
dimensionality of the action or state spaces, marking a significant step toward
establishing formal convergence results for PPO-based algorithms.

</details>


### [480] [Scaling CrossQ with Weight Normalization](https://arxiv.org/pdf/2506.03758)
*Daniel Palenicek, Florian Vogt, Jan Peters*

Main category: cs.LG

TL;DR: CrossQ's scaling with higher UTD ratios faces challenges like Q-bias explosion and critic weight growth. Weight normalization stabilizes training, enabling reliable scaling and superior performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing sample efficiency bottlenecks in reinforcement learning by exploring CrossQ's behavior at higher UTD ratios.

Method: Integrating weight normalization into CrossQ to stabilize training dynamics and maintain constant effective learning rates.

Result: Improved scalability and performance on challenging tasks like dog and humanoid environments in the DeepMind control benchmark.

Conclusion: Weight normalization offers a robust solution for scaling CrossQ without drastic interventions, enhancing sample efficiency in model-free RL.

Abstract: Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics which are
emphasized by higher UTDs, particularly Q-bias explosion and the growing
magnitude of critic network weights. To address this, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
prevents potential loss of plasticity and keeps the effective learning rate
constant. Our proposed approach reliably scales with increasing UTD ratios,
achieving competitive or superior performance across a range of challenging
tasks on the DeepMind control benchmark, notably the complex dog and humanoid
environments. This work eliminates the need for drastic interventions, such as
network resets, and offers a robust pathway for improving sample efficiency and
scalability in model-free reinforcement learning.

</details>


### [481] [FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning](https://arxiv.org/pdf/2506.03777)
*Li Zhang, Zhongxuan Han, Chaochao chen, Xiaohua Feng, Jiaming Zhang, Yuyuan Li*

Main category: cs.LG

TL;DR: FedFACT is a framework for fair Federated Learning, addressing global and local fairness challenges in multi-class classification with controllable accuracy-fairness trade-offs.


<details>
  <summary>Details</summary>
Motivation: To regulate model fairness in FL, ensuring no disparities across sensitive groups, and to harmonize global and local fairness while enabling optimal accuracy-fairness balance.

Method: Proposes FedFACT, which identifies Bayes-optimal classifiers under fairness constraints, reformulates fair FL as personalized cost-sensitive learning, and uses bi-level optimization.

Result: FedFACT outperforms baselines in balancing accuracy and fairness across datasets with varying data heterogeneity.

Conclusion: FedFACT provides a near-optimal solution for fair FL with theoretical guarantees and practical effectiveness.

Abstract: With emerging application of Federated Learning (FL) in decision-making
scenarios, it is imperative to regulate model fairness to prevent disparities
across sensitive groups (e.g., female, male). Current research predominantly
focuses on two concepts of group fairness within FL: Global Fairness (overall
model disparity across all clients) and Local Fairness (the disparity within
each client). However, the non-decomposable, non-differentiable nature of
fairness criteria pose two fundamental, unresolved challenges for fair FL: (i)
Harmonizing global and local fairness in multi-class classification; (ii)
Enabling a controllable, optimal accuracy-fairness trade-off. To tackle the
aforementioned challenges, we propose a novel controllable federated
group-fairness calibration framework, named FedFACT. FedFACT identifies the
Bayes-optimal classifiers under both global and local fairness constraints in
multi-class case, yielding models with minimal performance decline while
guaranteeing fairness. To effectively realize an adjustable, optimal
accuracy-fairness balance, we derive specific characterizations of the
Bayes-optimal fair classifiers for reformulating fair FL as personalized
cost-sensitive learning problem for in-processing, and bi-level optimization
for post-processing. Theoretically, we provide convergence and generalization
guarantees for FedFACT to approach the near-optimal accuracy under given
fairness levels. Extensive experiments on multiple datasets across various data
heterogeneity demonstrate that FedFACT consistently outperforms baselines in
balancing accuracy and global-local fairness.

</details>


### [482] [When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective](https://arxiv.org/pdf/2506.03784)
*Beatrix M. G. Nielsen, Emanuele Marconato, Andrea Dittadi, Luigi Gresele*

Main category: cs.LG

TL;DR: The paper investigates when and why deep neural networks learn similar representations, using identifiability theory. It shows that small KL divergence doesn't guarantee similar representations, and defines a new distance metric linking distributional closeness to representational similarity.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which different neural networks learn similar representations, despite generating close model distributions.

Method: Uses identifiability theory and focuses on a model family including autoregressive language models. Proves limitations of KL divergence and introduces a new distributional distance.

Result: Small KL divergence doesn't ensure similar representations. Wider networks learn distributions closer to the new distance and have more similar representations.

Conclusion: Establishes a connection between distributional closeness and representational similarity, with implications for model training and evaluation.

Abstract: When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.

</details>


### [483] [Attention-Only Transformers via Unrolled Subspace Denoising](https://arxiv.org/pdf/2506.03790)
*Peng Wang, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma*

Main category: cs.LG

TL;DR: The paper proposes a fully interpretable transformer architecture derived from denoising noisy token representations, achieving performance close to standard transformers like GPT-2 and CRATE.


<details>
  <summary>Details</summary>
Motivation: Current transformer architectures are empirically designed, redundant, and lack interpretability. The goal is to create a mathematically justified, compact, and interpretable transformer.

Method: The method involves compressing noisy token representations into low-dimensional subspaces using multi-head self-attention, unrolled into a deep network with skip connections.

Result: The compact architecture achieves linear-rate denoising per layer and performs comparably to standard transformers in vision and language tasks.

Conclusion: The proposed interpretable transformer is efficient, compact, and competitive with empirical designs, offering a mathematically grounded alternative.

Abstract: Despite the popularity of transformers in practice, their architectures are
empirically designed and neither mathematically justified nor interpretable.
Moreover, as indicated by many empirical studies, some components of
transformer architectures may be redundant. To derive a fully interpretable
transformer architecture with only necessary components, we contend that the
goal of representation learning is to compress a set of noisy initial token
representations towards a mixture of low-dimensional subspaces. To compress
these noisy token representations, an associated denoising operation naturally
takes the form of a multi-head (subspace) self-attention. By unrolling such
iterative denoising operations into a deep network, we arrive at a highly
compact architecture that consists of \textit{only} self-attention operators
with skip connections at each layer. Moreover, we show that each layer performs
highly efficient denoising: it improves the signal-to-noise ratio of token
representations \textit{at a linear rate} with respect to the number of layers.
Despite its simplicity, extensive experiments on vision and language tasks
demonstrate that such a transformer achieves performance close to that of
standard transformer architectures such as GPT-2 and CRATE.

</details>


### [484] [Learning Equilibria in Matching Games with Bandit Feedback](https://arxiv.org/pdf/2506.03802)
*Andreas Athanasopoulos, Christos Dimitrakakis*

Main category: cs.LG

TL;DR: The paper explores learning equilibria in a two-sided matching market where agents adaptively choose actions based on matches, using a UCB algorithm to achieve sublinear regret.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning equilibria in dynamic matching markets with initially unknown payoff matrices, where agents' actions influence outcomes.

Method: A UCB algorithm is proposed, where agents form preferences and select actions based on optimistic payoff estimates, aiming for sublinear regret.

Result: The algorithm achieves sublinear, instance-independent regret over time horizon T, demonstrating its effectiveness.

Conclusion: The study successfully introduces a method for learning equilibria in adaptive matching markets, with theoretical guarantees on regret.

Abstract: We investigate the problem of learning an equilibrium in a generalized
two-sided matching market, where agents can adaptively choose their actions
based on their assigned matches. Specifically, we consider a setting in which
matched agents engage in a zero-sum game with initially unknown payoff
matrices, and we explore whether a centralized procedure can learn an
equilibrium from bandit feedback. We adopt the solution concept of matching
equilibrium, where a pair consisting of a matching $\mathfrak{m}$ and a set of
agent strategies $X$ forms an equilibrium if no agent has the incentive to
deviate from $(\mathfrak{m}, X)$. To measure the deviation of a given pair
$(\mathfrak{m}, X)$ from the equilibrium pair $(\mathfrak{m}^\star, X^\star)$,
we introduce matching instability that can serve as a regret measure for the
corresponding learning problem. We then propose a UCB algorithm in which agents
form preferences and select actions based on optimistic estimates of the game
payoffs, and prove that it achieves sublinear, instance-independent regret over
a time horizon $T$.

</details>


### [485] [Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks](https://arxiv.org/pdf/2506.03813)
*Lili Chen, Changyang She, Jingge Zhu, Jamie Evans*

Main category: cs.LG

TL;DR: Proposed JCPGNN-M, a GNN-based solution for joint channel and power allocation, outperforming eWMMSE in data rates and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Interference in wireless networks hinders data rate improvements, necessitating efficient joint channel and power allocation methods.

Method: Introduced eWMMSE and JCPGNN-M, combining Lagrangian framework with GNNs for multi-channel allocation and power constraints.

Result: JCPGNN-M achieves higher data rates, lower inference time, and better scalability than eWMMSE.

Conclusion: JCPGNN-M is a scalable, efficient solution for dense wireless networks, outperforming traditional iterative methods.

Abstract: As the number of mobile devices continues to grow, interference has become a
major bottleneck in improving data rates in wireless networks. Efficient joint
channel and power allocation (JCPA) is crucial for managing interference. In
this paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the
JCPA problem in multi-channel wireless networks. To reduce the computational
complexity of iterative optimization, we further introduce JCPGNN-M, a graph
neural network-based solution that enables simultaneous multi-channel
allocation for each user. We reformulate the problem as a Lagrangian function,
which allows us to enforce the total power constraints systematically. Our
solution involves combining this Lagrangian framework with GNNs and iteratively
updating the Lagrange multipliers and resource allocation scheme. Unlike
existing GNN-based methods that limit each user to a single channel, JCPGNN-M
supports efficient spectrum reuse and scales well in dense network scenarios.
Simulation results show that JCPGNN-M achieves better data rate compared to
eWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and
it can generalize well to larger networks.

</details>


### [486] [Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid](https://arxiv.org/pdf/2506.03817)
*Julius Gonsior, Tim Rieß, Anja Reusch, Claudio Hartmann, Maik Thiele, Wolfgang Lehner*

Main category: cs.LG

TL;DR: Active Learning (AL) reduces labeling effort but is underused due to setup complexity and trust issues. This study explores AL's hyperparameter space, analyzes impacts, and provides recommendations for reproducible research.


<details>
  <summary>Details</summary>
Motivation: AL is rarely used in practice due to its complex setup and perceived ineffectiveness, likely caused by its large, unexplored hyperparameter space.

Method: The study compiled a hyperparameter grid of 4.6M combinations, recorded their performance in a large AL study, and analyzed each hyperparameter's impact.

Result: Findings highlight hyperparameter influence, AL strategy implementation effects, and propose a study design for reproducible AL experiments.

Conclusion: The study aims to improve AL's reproducibility and trustworthiness by clarifying hyperparameter impacts and suggesting efficient experimental designs.

Abstract: Annotating data is a time-consuming and costly task, but it is inherently
required for supervised machine learning. Active Learning (AL) is an
established method that minimizes human labeling effort by iteratively
selecting the most informative unlabeled samples for expert annotation, thereby
improving the overall classification performance. Even though AL has been known
for decades, AL is still rarely used in real-world applications. As indicated
in the two community web surveys among the NLP community about AL, two main
reasons continue to hold practitioners back from using AL: first, the
complexity of setting AL up, and second, a lack of trust in its effectiveness.
We hypothesize that both reasons share the same culprit: the large
hyperparameter space of AL. This mostly unexplored hyperparameter space often
leads to misleading and irreproducible AL experiment results. In this study, we
first compiled a large hyperparameter grid of over 4.6 million hyperparameter
combinations, second, recorded the performance of all combinations in the
so-far biggest conducted AL study, and third, analyzed the impact of each
hyperparameter in the experiment results. In the end, we give recommendations
about the influence of each hyperparameter, demonstrate the surprising
influence of the concrete AL strategy implementation, and outline an
experimental study design for reproducible AL experiments with minimal
computational effort, thus contributing to more reproducible and trustworthy AL
research in the future.

</details>


### [487] [Learning task-specific predictive models for scientific computing](https://arxiv.org/pdf/2506.03835)
*Jianyuan Yin, Qianxiao Li*

Main category: cs.LG

TL;DR: The paper proposes a task-specific supervised learning approach for downstream tasks in machine-learning-augmented scientific computing, focusing on maximum prediction error instead of mean square error.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning methods, like minimizing mean square error, are inadequate for downstream tasks that require model evaluation beyond prediction.

Method: Formulates a task-specific supervised learning problem using maximum prediction error, discretizes empirical risk, and develops an iterative algorithm.

Result: Demonstrates effectiveness through numerical examples in trajectory prediction, optimal control, and minimum energy path computation.

Conclusion: The approach provides a reliable surrogate model for downstream tasks, outperforming traditional methods.

Abstract: We consider learning a predictive model to be subsequently used for a given
downstream task (described by an algorithm) that requires access to the model
evaluation. This task need not be prediction, and this situation is frequently
encountered in machine-learning-augmented scientific computing. We show that
this setting differs from classical supervised learning, and in general it
cannot be solved by minimizing the mean square error of the model predictions
as is frequently performed in the literature. Instead, we find that the maximum
prediction error on the support of the downstream task algorithm can serve as
an effective estimate for the subsequent task performance. With this insight,
we formulate a task-specific supervised learning problem based on the given
sampling measure, whose solution serves as a reliable surrogate model for the
downstream task. Then, we discretize the empirical risk based on training data,
and develop an iterative algorithm to solve the task-specific supervised
learning problem. Three illustrative numerical examples on trajectory
prediction, optimal control and minimum energy path computation demonstrate the
effectiveness of the approach.

</details>


### [488] [Revisiting Unbiased Implicit Variational Inference](https://arxiv.org/pdf/2506.03839)
*Tobias Pielok, Bernd Bischl, David Rügamer*

Main category: cs.LG

TL;DR: Revisiting UIVI, the paper shows its MCMC loop can be replaced with importance sampling, achieving superior or comparable performance to SIVI benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational and precision issues of UIVI by improving its training routine.

Method: Replace UIVI's MCMC loop with importance sampling and learn the optimal proposal distribution via minimizing an expected forward KL divergence.

Result: The refined UIVI method matches or outperforms state-of-the-art SIVI benchmarks.

Conclusion: UIVI, when refined, is a viable and effective alternative to SIVI methods.

Abstract: Recent years have witnessed growing interest in semi-implicit variational
inference (SIVI) methods due to their ability to rapidly generate samples from
complex distributions. However, since the likelihood of these samples is
non-trivial to estimate in high dimensions, current research focuses on finding
effective SIVI training routines. Although unbiased implicit variational
inference (UIVI) has largely been dismissed as imprecise and computationally
prohibitive because of its inner MCMC loop, we revisit this method and show
that UIVI's MCMC loop can be effectively replaced via importance sampling and
the optimal proposal distribution can be learned stably by minimizing an
expected forward Kullback-Leibler divergence without bias. Our refined approach
demonstrates superior performance or parity with state-of-the-art methods on
established SIVI benchmarks.

</details>


### [489] [Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/pdf/2506.03850)
*Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong*

Main category: cs.LG

TL;DR: Vulnerability-Aware Alignment (VAA) identifies and mitigates harmful fine-tuning risks by focusing on vulnerable data subsets, using adversarial sampling and perturbations for balanced learning.


<details>
  <summary>Details</summary>
Motivation: Harmful fine-tuning (HFT) undermines safety alignment in LLMs, and existing methods fail to address data vulnerability patterns.

Method: VAA estimates data vulnerability, partitions data, and uses Group DRO with adversarial sampling and perturbations.

Result: VAA reduces harmful scores while maintaining task performance, outperforming baselines.

Conclusion: VAA effectively addresses HFT risks by targeting vulnerable data subsets and ensuring balanced learning.

Abstract: Harmful fine-tuning (HFT), performed directly on open-source LLMs or through
Fine-tuning-as-a-Service, breaks safety alignment and poses significant
threats. Existing methods aim to mitigate HFT risks by learning robust
representation on alignment data or making harmful data unlearnable, but they
treat each data sample equally, leaving data vulnerability patterns
understudied. In this work, we reveal that certain subsets of alignment data
are consistently more prone to forgetting during HFT across different
fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware
Alignment (VAA), which estimates data vulnerability, partitions data into
"vulnerable" and "invulnerable" groups, and encourages balanced learning using
a group distributionally robust optimization (Group DRO) framework.
Specifically, VAA learns an adversarial sampler that samples examples from the
currently underperforming group and then applies group-dependent adversarial
perturbations to the data during training, aiming to encourage a balanced
learning process across groups. Experiments across four fine-tuning tasks
demonstrate that VAA significantly reduces harmful scores while preserving
downstream task performance, outperforming state-of-the-art baselines.

</details>


### [490] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/pdf/2506.03857)
*Mingxuan Xia, Haobo Wang, Yixuan Li, Zewei Yu, Jindong Wang, Junbo Zhao, Runze Wu*

Main category: cs.LG

TL;DR: The paper introduces a candidate annotation paradigm using LLMs to output multiple labels for uncertain samples, improving data quality via a teacher-student framework (CanDist) and outperforming single-label methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based annotation methods often produce incorrect labels for uncertain samples, harming downstream data quality. Inspired by human ambiguity aversion, the authors propose a multi-label approach.

Method: A candidate annotation paradigm where LLMs output all possible labels for uncertain samples, followed by a teacher-student framework (CanDist) to distill these annotations using a Small Language Model (SLM).

Result: The method shows superior theoretical guarantees and effectiveness across six text classification tasks.

Conclusion: The proposed candidate annotation paradigm and CanDist framework enhance data quality by leveraging LLM uncertainty, validated by experiments.

Abstract: Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [491] [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/pdf/2506.03870)
*Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, Abdur R. Shahid*

Main category: cs.LG

TL;DR: The paper explores how Apple Intelligence's writing tools can mitigate emotion inference attacks by modifying text, preserving privacy against LLM misuse.


<details>
  <summary>Details</summary>
Motivation: Address the threat of LLM misuse for emotion inference attacks, which compromises user privacy.

Method: Develop datasets to assess text modifications (rewriting, tone adjustment) and their impact on LLM-based detection.

Result: Apple Intelligence's tools show strong potential as privacy-preserving mechanisms by neutralizing emotional content.

Conclusion: The study pioneers empirical analysis of Apple's tools for privacy, paving the way for adaptive systems to protect against LLM-based attacks.

Abstract: The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.

</details>


### [492] [HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark](https://arxiv.org/pdf/2506.03954)
*Jianqing Zhang, Xinghao Wu, Yanbing Zhou, Xiaoting Sun, Qiqi Cai, Yang Liu, Yang Hua, Zhenzhe Zheng, Jian Cao, Qiang Yang*

Main category: cs.LG

TL;DR: The paper introduces HtFLlib, a benchmark library for Heterogeneous Federated Learning (HtFL) to standardize evaluation and analysis of diverse HtFL methods, addressing data and model heterogeneity challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional Federated Learning (FL) supports only homogeneous models, limiting collaboration among clients with heterogeneous architectures. HtFL methods aim to overcome this but lack a standardized benchmark for evaluation.

Method: The authors develop HtFLlib, a framework integrating 12 datasets, 40 model architectures, and 10 HtFL methods. It provides systematic evaluations on accuracy, convergence, and costs.

Result: HtFLlib offers a robust benchmark for HtFL research, enabling fair comparisons and exploring method effectiveness in diverse scenarios like medical and sensor domains.

Conclusion: HtFLlib fills a critical gap in HtFL research, promoting advancements and broader applications. The library is open-source and extensible.

Abstract: As AI evolves, collaboration among heterogeneous models helps overcome data
scarcity by enabling knowledge transfer across institutions and devices.
Traditional Federated Learning (FL) only supports homogeneous models, limiting
collaboration among clients with heterogeneous model architectures. To address
this, Heterogeneous Federated Learning (HtFL) methods are developed to enable
collaboration across diverse heterogeneous models while tackling the data
heterogeneity issue at the same time. However, a comprehensive benchmark for
standardized evaluation and analysis of the rapidly growing HtFL methods is
lacking. Firstly, the highly varied datasets, model heterogeneity scenarios,
and different method implementations become hurdles to making easy and fair
comparisons among HtFL methods. Secondly, the effectiveness and robustness of
HtFL methods are under-explored in various scenarios, such as the medical
domain and sensor signal modality. To fill this gap, we introduce the first
Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and
extensible framework that integrates multiple datasets and model heterogeneity
scenarios, offering a robust benchmark for research and practical applications.
Specifically, HtFLlib integrates (1) 12 datasets spanning various domains,
modalities, and data heterogeneity scenarios; (2) 40 model architectures,
ranging from small to large, across three modalities; (3) a modularized and
easy-to-extend HtFL codebase with implementations of 10 representative HtFL
methods; and (4) systematic evaluations in terms of accuracy, convergence,
computation costs, and communication costs. We emphasize the advantages and
potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze
advancing HtFL research and enable its broader applications. The code is
released at https://github.com/TsingZ0/HtFLlib.

</details>


### [493] [Temporal horizons in forecasting: a performance-learnability trade-off](https://arxiv.org/pdf/2506.03889)
*Pau Vilimelis Aceituno, Jack William Miller, Noah Marti, Youssef Farag, Victor Boussange*

Main category: cs.LG

TL;DR: The paper analyzes the trade-off in training horizons for autoregressive models in dynamical systems, showing how loss landscape geometry varies with horizon length and its impact on generalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting an optimal training horizon for autoregressive models in dynamical systems, balancing short-term accuracy and long-term prediction quality.

Method: Theoretical analysis of loss landscape geometry for chaotic and periodic systems, validated through numerical experiments.

Result: Loss roughness grows exponentially with horizon in chaotic systems and linearly in periodic ones. Long-horizon models generalize better to short-term forecasts, while short-horizon models perform poorly on long-term predictions.

Conclusion: Provides a principled approach for selecting training horizons in autoregressive forecasting models, emphasizing the importance of horizon choice for model performance.

Abstract: When training autoregressive models for dynamical systems, a critical
question arises: how far into the future should the model be trained to
predict? Too short a horizon may miss long-term trends, while too long a
horizon can impede convergence due to accumulating prediction errors. In this
work, we formalize this trade-off by analyzing how the geometry of the loss
landscape depends on the training horizon. We prove that for chaotic systems,
the loss landscape's roughness grows exponentially with the training horizon,
while for limit cycles, it grows linearly, making long-horizon training
inherently challenging. However, we also show that models trained on long
horizons generalize well to short-term forecasts, whereas those trained on
short horizons suffer exponentially (resp. linearly) worse long-term
predictions in chaotic (resp. periodic) systems. We validate our theory through
numerical experiments and discuss practical implications for selecting training
horizons. Our results provide a principled foundation for hyperparameter
optimization in autoregressive forecasting models.

</details>


### [494] [Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection](https://arxiv.org/pdf/2506.03964)
*HyunGi Kim, Jisoo Mok, Dongjun Lee, Jaihyun Lew, Sungjae Kim, Sungroh Yoon*

Main category: cs.LG

TL;DR: CAROTS introduces causality-aware contrastive learning for robust multivariate time-series anomaly detection, using data augmentors to create causality-preserving and -disturbing samples for training.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize causal relationships in multivariate time-series data, limiting anomaly detection robustness.

Method: CAROTS employs two data augmentors to generate causality-preserving (normal) and -disturbing (anomalous) samples, training an encoder via contrastive learning with a similarity-filtered one-class loss.

Result: Experiments on real-world and synthetic datasets show CAROTS improves anomaly detection by leveraging causal relationships.

Conclusion: CAROTS effectively integrates causality into contrastive learning, enhancing multivariate time-series anomaly detection.

Abstract: Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.

</details>


### [495] [A kernel conditional two-sample test](https://arxiv.org/pdf/2506.03898)
*Pierre-François Massiani, Christian Fiedler, Lukas Haverbeck, Friedrich Solowjow, Sebastian Trimpe*

Main category: cs.LG

TL;DR: A framework for hypothesis testing on conditional probability distributions is proposed, enabling conditional two-sample tests to identify covariate differences. It leverages learning method confidence bounds, generalizes KRR bounds, and introduces practical bootstrapping schemes.


<details>
  <summary>Details</summary>
Motivation: To address the need for conditional two-sample tests in scenarios with sequential, non-independent data or varying output distributions, such as process monitoring and dynamical systems comparison.

Method: Transforms confidence bounds of learning methods (e.g., KRR, conditional kernel mean embeddings) into tests, generalizes KRR bounds for infinite-dimensional outputs, and introduces bootstrapping for practical use.

Result: Enables conditional two-sample testing without independent data, with theoretical guarantees and practical applicability demonstrated in process monitoring and dynamical systems.

Conclusion: The work establishes a foundation for conditional two-sample testing, advancing vector-valued least squares estimation and offering practical implementation.

Abstract: We propose a framework for hypothesis testing on conditional probability
distributions, which we then use to construct conditional two-sample
statistical tests. These tests identify the inputs -- called covariates in this
context -- where two conditional expectations differ with high probability. Our
key idea is to transform confidence bounds of a learning method into a
conditional two-sample test, and we instantiate this principle for kernel ridge
regression (KRR) and conditional kernel mean embeddings. We generalize existing
pointwise-in-time or time-uniform confidence bounds for KRR to
previously-inaccessible yet essential cases such as infinite-dimensional
outputs with non-trace-class kernels. These bounds enable circumventing the
need for independent data in our statistical tests, since they allow online
sampling. We also introduce bootstrapping schemes leveraging the parametric
form of testing thresholds identified in theory to avoid tuning inaccessible
parameters, making our method readily applicable in practice. Such conditional
two-sample tests are especially relevant in applications where data arrive
sequentially or non-independently, or when output distributions vary with
operational parameters. We demonstrate their utility through examples in
process monitoring and comparison of dynamical systems. Overall, our results
establish a comprehensive foundation for conditional two-sample testing, from
theoretical guarantees to practical implementation, and advance the
state-of-the-art on the concentration of vector-valued least squares
estimation.

</details>


### [496] [CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](https://arxiv.org/pdf/2506.04001)
*Han Ji, Yuqi Feng, Jiahao Fan, Yanan Sun*

Main category: cs.LG

TL;DR: CARL improves neural architecture search (NAS) performance prediction by separating causal and non-causal features, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing performance predictors in NAS often fail due to distribution shifts and spurious correlations, limiting generalization.

Method: CARL uses a substructure extractor to split architectures into causal and non-causal features, generating interventional samples to prioritize critical features.

Result: CARL achieves state-of-the-art accuracy, e.g., 97.67% top-1 accuracy on CIFAR-10 with DARTS.

Conclusion: CARL effectively addresses generalization issues in NAS performance prediction by focusing on causal features.

Abstract: Performance predictors have emerged as a promising method to accelerate the
evaluation stage of neural architecture search (NAS). These predictors estimate
the performance of unseen architectures by learning from the correlation
between a small set of trained architectures and their performance. However,
most existing predictors ignore the inherent distribution shift between limited
training samples and diverse test samples. Hence, they tend to learn spurious
correlations as shortcuts to predictions, leading to poor generalization. To
address this, we propose a Causality-guided Architecture Representation
Learning (CARL) method aiming to separate critical (causal) and redundant
(non-causal) features of architectures for generalizable architecture
performance prediction. Specifically, we employ a substructure extractor to
split the input architecture into critical and redundant substructures in the
latent space. Then, we generate multiple interventional samples by pairing
critical representations with diverse redundant representations to prioritize
critical features. Extensive experiments on five NAS search spaces demonstrate
the state-of-the-art accuracy and superior interpretability of CARL. For
instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

</details>


### [497] [Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods](https://arxiv.org/pdf/2506.03910)
*Shyam Prabhu, P Akshay Kumar, Antov Selwinston, Pavan Taduvai, Shreya Bairi, Rohit Batra*

Main category: cs.LG

TL;DR: Machine learning (GPR) outperforms Taguchi method in optimizing WAAM bead geometry, offering better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Full factorial exploration is impractical for multi-variable materials design. DOE methods like Taguchi lack non-linear dependency capture, motivating ML solutions.

Method: Compared Taguchi (L25 orthogonal array) with active learning-based GPR (uncertainty exploration + latin hypercube sampling) in WAAM.

Result: GPR outperformed Taguchi in accuracy and efficiency across 15 test cases.

Conclusion: ML methods like GPR are superior for complex parameter exploration in materials processing.

Abstract: Materials design problems often require optimizing multiple variables,
rendering full factorial exploration impractical. Design of experiment (DOE)
methods, such as Taguchi technique, are commonly used to efficiently sample the
design space but they inherently lack the ability to capture non-linear
dependency of process variables. In this work, we demonstrate how machine
learning (ML) methods can be used to overcome these limitations. We compare the
performance of Taguchi method against an active learning based Gaussian process
regression (GPR) model in a wire arc additive manufacturing (WAAM) process to
accurately predict aspects of bead geometry, including penetration depth, bead
width, and height. While Taguchi method utilized a three-factor, five-level L25
orthogonal array to suggest weld parameters, the GPR model used an
uncertainty-based exploration acquisition function coupled with latin hypercube
sampling for initial training data. Accuracy and efficiency of both models was
evaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.
This work applies to broader materials processing domain requiring efficient
exploration of complex parameters.

</details>


### [498] [Learning Fair And Effective Points-Based Rewards Programs](https://arxiv.org/pdf/2506.03911)
*Chamsi Hssaine, Yichun Hu, Ciara Pike-Burke*

Main category: cs.LG

TL;DR: The paper explores fair design of points-based rewards programs, balancing revenue and fairness despite customer heterogeneity and unknown behavior-points relationships. It proposes learning algorithms to limit unfair point devaluation while maintaining revenue.


<details>
  <summary>Details</summary>
Motivation: Address concerns about unfair practices in points-based rewards programs by ensuring fairness without significantly compromising effectiveness or revenue.

Method: Analyze fair rewards programs with uniform thresholds and design learning algorithms to limit threshold changes and devaluation risks.

Result: Uniform thresholds lose at most a factor of $1+\ln 2$ revenue vs. personalized strategies. Proposed algorithms achieve optimal regret and limit unfairness.

Conclusion: Fair rewards programs can balance revenue and fairness, with limited value in personalization and strong performance of proposed algorithms.

Abstract: Points-based rewards programs are a prevalent way to incentivize customer
loyalty; in these programs, customers who make repeated purchases from a seller
accumulate points, working toward eventual redemption of a free reward. These
programs have recently come under scrutiny due to accusations of unfair
practices in their implementation. Motivated by these concerns, we study the
problem of fairly designing points-based rewards programs, with a focus on two
obstacles that put fairness at odds with their effectiveness. First, due to
customer heterogeneity, the seller should set different redemption thresholds
for different customers to generate high revenue. Second, the relationship
between customer behavior and the number of accumulated points is typically
unknown; this requires experimentation which may unfairly devalue customers'
previously earned points. We first show that an individually fair rewards
program that uses the same redemption threshold for all customers suffers a
loss in revenue of at most a factor of $1+\ln 2$, compared to the optimal
personalized strategy that differentiates between customers. We then tackle the
problem of designing temporally fair learning algorithms in the presence of
demand uncertainty. Toward this goal, we design a learning algorithm that
limits the risk of point devaluation due to experimentation by only changing
the redemption threshold $O(\log T)$ times, over a horizon of length $T$. This
algorithm achieves the optimal (up to polylogarithmic factors)
$\widetilde{O}(\sqrt{T})$ regret in expectation. We then modify this algorithm
to only ever decrease redemption thresholds, leading to improved fairness at a
cost of only a constant factor in regret. Extensive numerical experiments show
the limited value of personalization in average-case settings, in addition to
demonstrating the strong practical performance of our proposed learning
algorithms.

</details>


### [499] [Learning equivariant models by discovering symmetries with learnable augmentations](https://arxiv.org/pdf/2506.03914)
*Eduardo Santos Escriche, Stefanie Jegelka*

Main category: cs.LG

TL;DR: SEMoLA is an end-to-end method for discovering unknown symmetries in data and encoding approximate equivariance into models, addressing limitations of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning symmetries from data either require prior knowledge (soft equivariance) or lack interpretability (implicit inference). SEMoLA aims to overcome these limitations.

Method: SEMoLA jointly discovers unknown symmetries via learnable data augmentations and softly encodes approximate equivariance into unconstrained models.

Result: SEMoLA robustly discovers relevant symmetries and achieves high prediction accuracy across diverse datasets and symmetry groups.

Conclusion: SEMoLA provides a flexible, interpretable, and robust solution for learning symmetries without prior knowledge.

Abstract: Recently, a trend has emerged that favors learning relevant symmetries from
data in geometric domains instead of designing constrained architectures. To do
so, two popular options are (1) to modify the training protocol, e.g., with a
specific loss and data augmentations (soft equivariance), or (2) to ignore
equivariance and infer it only implicitly. However, both options have
limitations: soft equivariance requires a priori knowledge about relevant
symmetries, while inferring symmetries merely via the task and larger data
lacks interpretability. To address both limitations, we propose SEMoLA, an
end-to-end approach that jointly (1) discovers a priori unknown symmetries in
the data via learnable data augmentations, and (2) softly encodes the
respective approximate equivariance into an arbitrary unconstrained model.
Hence, it does not need prior knowledge about symmetries, it offers
interpretability, and it maintains robustness to distribution shifts.
Empirically, we demonstrate the ability of SEMoLA to robustly discover relevant
symmetries while achieving high prediction accuracy across various datasets,
encompassing multiple data modalities and underlying symmetry groups.

</details>


### [500] [Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win](https://arxiv.org/pdf/2506.03919)
*Lorenz Kummer, Samir Moustafa, Anatol Ehrlich, Franka Bause, Nikolaus Suess, Wilfried N. Gansterer, Nils M. Kriege*

Main category: cs.LG

TL;DR: The paper establishes theoretical foundations for the lottery ticket hypothesis (LTH) in graph neural networks (GNNs), focusing on expressivity of sparse subnetworks and their ability to match full-network performance. It introduces a Strong Expressive Lottery Ticket Hypothesis and demonstrates its implications for convergence and generalization, with applications in drug discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap in theoretical understanding of LTH for GNNs, which has only been empirically validated. The study aims to identify conditions where sparse subnetworks maintain expressivity and performance.

Method: The method involves analyzing the expressivity of sparse subnetworks in GNNs, comparing them to the Weisfeiler-Leman test, and proving the Strong Expressive Lottery Ticket Hypothesis. Experiments include applications in drug discovery.

Result: Results show that sparse subnetworks can match the expressivity of full networks under certain conditions, leading to faster convergence and better generalization. Theoretical proofs support these findings.

Conclusion: The study provides novel theoretical insights into LTH for GNNs, emphasizing the role of expressivity in sparse subnetworks. It advances both LTH and GNN research, with practical implications for tasks like drug discovery.

Abstract: The lottery ticket hypothesis (LTH) is well-studied for convolutional neural
networks but has been validated only empirically for graph neural networks
(GNNs), for which theoretical findings are largely lacking. In this paper, we
identify the expressivity of sparse subnetworks, i.e. their ability to
distinguish non-isomorphic graphs, as crucial for finding winning tickets that
preserve the predictive performance. We establish conditions under which the
expressivity of a sparsely initialized GNN matches that of the full network,
particularly when compared to the Weisfeiler-Leman test, and in that context
put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We
subsequently show that an increased expressivity in the initialization
potentially accelerates model convergence and improves generalization. Our
findings establish novel theoretical foundations for both LTH and GNN research,
highlighting the importance of maintaining expressivity in sparsely initialized
GNNs. We illustrate our results using examples from drug discovery.

</details>


### [501] [Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study](https://arxiv.org/pdf/2506.03931)
*Yotam Alexander, Yonatan Slutzky, Yuval Ran-Milo, Nadav Cohen*

Main category: cs.LG

TL;DR: The paper challenges the volume hypothesis by showing that Guess & Check (G&C) performs worse than gradient descent in wide networks but better in deep ones, using matrix factorization as a testbed.


<details>
  <summary>Details</summary>
Motivation: To investigate whether neural networks need gradient descent to generalize well, contrasting the volume hypothesis with conventional wisdom.

Method: Theoretical analysis of matrix factorization (with linear and non-linear activations) under G&C, comparing performance with gradient descent across wide and deep networks.

Result: G&C deteriorates generalization in wide networks but improves it in deep ones, highlighting a fundamental difference between wide and deep architectures.

Conclusion: The findings suggest no simple answer to whether neural networks require gradient descent for generalization, as performance depends on network architecture.

Abstract: Conventional wisdom attributes the mysterious generalization abilities of
overparameterized neural networks to gradient descent (and its variants). The
recent volume hypothesis challenges this view: it posits that these
generalization abilities persist even when gradient descent is replaced by
Guess & Check (G&C), i.e., by drawing weight settings until one that fits the
training data is found. The validity of the volume hypothesis for wide and deep
neural networks remains an open question. In this paper, we theoretically
investigate this question for matrix factorization (with linear and non-linear
activation)--a common testbed in neural network theory. We first prove that
generalization under G&C deteriorates with increasing width, establishing what
is, to our knowledge, the first case where G&C is provably inferior to gradient
descent. Conversely, we prove that generalization under G&C improves with
increasing depth, revealing a stark contrast between wide and deep networks,
which we further validate empirically. These findings suggest that even in
simple settings, there may not be a simple answer to the question of whether
neural networks need gradient descent to generalize well.

</details>


### [502] [FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review](https://arxiv.org/pdf/2506.03938)
*Cédric Léonard, Dirk Stober, Martin Schulz*

Main category: cs.LG

TL;DR: The paper reviews 66 experiments deploying ML models on FPGAs for Remote Sensing, introducing taxonomies for model architectures and FPGA strategies, following PRISMA 2020 guidelines.


<details>
  <summary>Details</summary>
Motivation: The rise of UAVs and NewSpace technologies has increased data volume, requiring efficient onboard processing and decision-making.

Method: Systematic review of 66 experiments, introducing taxonomies for ML models and FPGA implementations, adhering to PRISMA 2020.

Result: Two taxonomies for efficient ML architectures and FPGA strategies, with shared data and code for reproducibility.

Conclusion: FPGAs enable adaptable, high-performance onboard ML for Remote Sensing, supported by transparent methodologies.

Abstract: New UAV technologies and the NewSpace era are transforming Earth Observation
missions and data acquisition. Numerous small platforms generate large data
volume, straining bandwidth and requiring onboard decision-making to transmit
high-quality information in time. While Machine Learning allows real-time
autonomous processing, FPGAs balance performance with adaptability to
mission-specific requirements, enabling onboard deployment. This review
systematically analyzes 66 experiments deploying ML models on FPGAs for Remote
Sensing applications. We introduce two distinct taxonomies to capture both
efficient model architectures and FPGA implementation strategies. For
transparency and reproducibility, we follow PRISMA 2020 guidelines and share
all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.

</details>


### [503] [Lower Ricci Curvature for Hypergraphs](https://arxiv.org/pdf/2506.03943)
*Shiyi Yang, Can Chen, Didong Li*

Main category: cs.LG

TL;DR: The paper introduces hypergraph lower Ricci curvature (HLRC), a new curvature metric for hypergraphs that balances interpretability and efficiency, outperforming existing methods in capturing higher-order structure.


<details>
  <summary>Details</summary>
Motivation: Existing curvature-based methods for hypergraphs either lack expressivity (combinatorial approaches) or are computationally expensive (geometric methods), limiting their practical utility.

Method: The authors propose HLRC, a closed-form curvature metric for hypergraphs, designed to be both interpretable and efficient.

Result: HLRC effectively reveals higher-order organization in hypergraphs, distinguishing communities, uncovering latent labels, tracking dynamics, and supporting clustering.

Conclusion: HLRC unifies geometric sensitivity with simplicity, offering a versatile tool for hypergraph analytics with applications in classification, anomaly detection, and generative modeling.

Abstract: Networks with higher-order interactions, prevalent in biological, social, and
information systems, are naturally represented as hypergraphs, yet their
structural complexity poses fundamental challenges for geometric
characterization. While curvature-based methods offer powerful insights in
graph analysis, existing extensions to hypergraphs suffer from critical
trade-offs: combinatorial approaches such as Forman-Ricci curvature capture
only coarse features, whereas geometric methods like Ollivier-Ricci curvature
offer richer expressivity but demand costly optimal transport computations. To
address these challenges, we introduce hypergraph lower Ricci curvature (HLRC),
a novel curvature metric defined in closed form that achieves a principled
balance between interpretability and efficiency. Evaluated across diverse
synthetic and real-world hypergraph datasets, HLRC consistently reveals
meaningful higher-order organization, distinguishing intra- from
inter-community hyperedges, uncovering latent semantic labels, tracking
temporal dynamics, and supporting robust clustering of hypergraphs based on
global structure. By unifying geometric sensitivity with algorithmic
simplicity, HLRC provides a versatile foundation for hypergraph analytics, with
broad implications for tasks including node classification, anomaly detection,
and generative modeling in complex systems.

</details>


### [504] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/pdf/2506.03951)
*Aojun Lu, Hangjie Yuan, Tao Feng, Yanan Sun*

Main category: cs.LG

TL;DR: The paper introduces Dual-Arch, a framework for Continual Learning (CL) that balances stability and plasticity at the architectural level, using two specialized networks. It outperforms existing methods while being more parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: Address the stability-plasticity dilemma in CL by exploring architectural-level trade-offs, as most methods focus only on parameter-level adjustments.

Method: Propose Dual-Arch, a plug-in framework with two independent networks: one for plasticity (deeper) and one for stability (wider).

Result: Dual-Arch improves CL performance and is up to 87% more compact in parameters.

Conclusion: Architectural-level design is crucial for CL, and Dual-Arch effectively balances stability and plasticity.

Abstract: The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [505] [Adapt before Continual Learning](https://arxiv.org/pdf/2506.03956)
*Aojun Lu, Tao Feng, Hangjie Yuan, Chunhui Ding, Yanan Sun*

Main category: cs.LG

TL;DR: ACL refines pre-trained models (PTMs) before continual learning (CL) to balance stability and plasticity, improving performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the stability-plasticity trade-off in CL by enhancing PTM adaptability without catastrophic forgetting.

Method: Propose ACL, a plug-and-play adaptation phase before CL tasks, aligning embeddings with class prototypes.

Result: ACL significantly improves CL performance, balancing stability and plasticity.

Conclusion: ACL offers a versatile solution for PTM-based CL, enhancing adaptability and performance.

Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [506] [Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks](https://arxiv.org/pdf/2506.03996)
*Lianfeng Shi, Ao Li, Benjamin Ward-Cherrier*

Main category: cs.LG

TL;DR: OSBC is a one-shot post-training pruning/quantization framework for SNNs, improving efficiency and accuracy by minimizing loss on spiking neuron membrane potential.


<details>
  <summary>Details</summary>
Motivation: SNNs need efficient compression methods due to limited neuromorphic hardware resources, but current iterative approaches are costly.

Method: OSBC adapts Optimal Brain Compression (OBC) for SNNs, focusing on minimizing membrane potential loss with a small dataset.

Result: Achieves 97% sparsity (pruning) or 4-bit quantization with minimal accuracy loss on neuromorphic datasets.

Conclusion: OSBC offers efficient, accurate one-shot compression for SNNs, suitable for resource-limited hardware.

Abstract: Spiking Neural Networks (SNNs) have emerged as a new generation of
energy-efficient neural networks suitable for implementation on neuromorphic
hardware. As neuromorphic hardware has limited memory and computing resources,
weight pruning and quantization have recently been explored to improve SNNs'
efficiency. State-of-the-art SNN pruning/quantization methods employ multiple
compression and training iterations, increasing the cost for pre-trained or
very large SNNs. In this paper, we propose a new one-shot post-training
pruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that
adapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and
Alistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input
current as OBC does, OSBC achieves more efficient and accurate SNN compression
in one pass by minimizing the loss on spiking neuron membrane potential with a
small sample dataset. Our experiments on neuromorphic datasets (N-MNIST,
CIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity
through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric
quantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code
will be available on GitHub.

</details>


### [507] [On the Usage of Gaussian Process for Efficient Data Valuation](https://arxiv.org/pdf/2506.04026)
*Clément Bénesse, Patrick Mesana, Athénaïs Gautier, Sébastien Gambs*

Main category: cs.LG

TL;DR: The paper introduces a novel decomposition for data valuation in machine learning, combining utility functions and aggregation procedures, and proposes Gaussian Processes for efficient utility estimation.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic way to analyze and implement data valuation methods by decomposing them into interpretable components.

Method: Proposes a canonical decomposition of data valuation into utility functions and aggregation procedures, using Gaussian Processes for utility estimation on sub-models.

Result: The approach is theoretically grounded in Bayesian theory and offers practical efficiency through fast valuation estimation.

Conclusion: The method provides a flexible and efficient framework for data valuation, bridging theory and practice.

Abstract: In machine learning, knowing the impact of a given datum on model training is
a fundamental task referred to as Data Valuation. Building on previous works
from the literature, we have designed a novel canonical decomposition allowing
practitioners to analyze any data valuation method as the combination of two
parts: a utility function that captures characteristics from a given model and
an aggregation procedure that merges such information. We also propose to use
Gaussian Processes as a means to easily access the utility function on
``sub-models'', which are models trained on a subset of the training set. The
strength of our approach stems from both its theoretical grounding in Bayesian
theory, and its practical reach, by enabling fast estimation of valuations
thanks to efficient update formulae.

</details>


### [508] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/pdf/2506.04088)
*Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye*

Main category: cs.LG

TL;DR: The paper introduces Turbo, a framework for tabular reasoning from table images using privileged structured data during training to enhance multimodal LLMs. It addresses alignment and reasoning transfer challenges, achieving SOTA performance with limited data.


<details>
  <summary>Details</summary>
Motivation: Real-world tables often appear as images without textual representations, posing challenges for tabular reasoning. Leveraging privileged structured data during training can enhance multimodal LLMs for this task.

Method: Turbo uses a structure-aware reasoning trace generator (DeepSeek-R1) to create modality-bridged data. It repeatedly generates and selects advantageous reasoning paths to improve reasoning skills.

Result: Turbo achieves state-of-the-art performance (+7.2% vs. previous SOTA) with only 9k training data across multiple datasets.

Conclusion: Turbo effectively bridges the gap between structured and visual table representations, enhancing multimodal tabular reasoning with limited data.

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [509] [Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence](https://arxiv.org/pdf/2506.04053)
*Alexander Semenenko, Ivan Butakov, Alexey Frolov, Ivan Oseledets*

Main category: cs.LG

TL;DR: Sliced Mutual Information (SMI) is scalable but flawed, being vulnerable to manipulation and exhibiting counterintuitive behavior like saturation and poor performance compared to simpler measures.


<details>
  <summary>Details</summary>
Motivation: To highlight the limitations of SMI despite its advantages, such as scalability and robustness, by demonstrating its susceptibility to data manipulation and inefficiency in detecting dependence.

Method: Extensive benchmarking and theoretical analysis to evaluate SMI's behavior under various conditions, including linear transformations.

Result: SMI saturates easily, fails to detect increased dependence, prioritizes redundancy, and underperforms compared to simpler measures like correlation.

Conclusion: SMI's flaws, such as susceptibility to manipulation and poor performance, suggest caution in its use and the need for better alternatives.

Abstract: Sliced Mutual Information (SMI) is widely used as a scalable alternative to
mutual information for measuring non-linear statistical dependence. Despite its
advantages, such as faster convergence, robustness to high dimensionality, and
nullification only under statistical independence, we demonstrate that SMI is
highly susceptible to data manipulation and exhibits counterintuitive behavior.
Through extensive benchmarking and theoretical analysis, we show that SMI
saturates easily, fails to detect increases in statistical dependence (even
under linear transformations designed to enhance the extraction of
information), prioritizes redundancy over informative content, and in some
cases, performs worse than simpler dependence measures like the correlation
coefficient.

</details>


### [510] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/pdf/2506.04089)
*Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov*

Main category: cs.LG

TL;DR: The paper introduces AmbiK, a textual dataset for ambiguous instructions in kitchen environments, aiming to standardize ambiguity detection method comparisons.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ambiguous instructions for LLMs in real-world environments and the lack of a universal benchmark for ambiguity detection methods.

Method: Creation of AmbiK, a dataset with 1000 pairs of ambiguous and unambiguous tasks, categorized by ambiguity type, and validated by humans.

Result: AmbiK provides a standardized benchmark with 2000 tasks, including environment descriptions, clarifying questions, and task plans.

Conclusion: AmbiK facilitates unified comparison of ambiguity detection methods and is publicly available for research use.

Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [511] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/pdf/2506.04071)
*Luiz Manella Pereira, M. Hadi Amini*

Main category: cs.LG

TL;DR: The paper introduces an Optimal Transport-based preprocessing algorithm for federated learning to address dataset imbalance, improving global model performance by minimizing distributional discrepancies.


<details>
  <summary>Details</summary>
Motivation: Federated learning's privacy and scalability benefits are hindered by dataset imbalance, which degrades model performance. This work aims to mitigate this issue.

Method: The method uses Wasserstein barycenters to align datasets across edge devices, minimizing distributional discrepancies and creating a target RGB space for projection.

Result: The approach demonstrates improved generalization on the CIFAR-10 dataset, achieving better performance in fewer communication rounds.

Conclusion: The proposed method effectively addresses dataset imbalance in federated learning, enhancing model accuracy and efficiency.

Abstract: Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [512] [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/pdf/2506.04118)
*Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis*

Main category: cs.LG

TL;DR: GSI is a new algorithm for efficient reward-guided decoding in large language models, combining soft best-of-n with a reward model and speculative samples from a small auxiliary model. It approximates the optimal tilted policy and outperforms existing methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of reward-guided decoding in large language models by combining soft best-of-n scaling with speculative samples and a reward model.

Method: GSI integrates soft best-of-n test-time scaling, a reward model, and speculative samples from a small auxiliary model to approximate the optimal tilted policy under the primary model.

Result: GSI achieves higher accuracy than standard soft best-of-n and reward-guided speculative decoding, sometimes outperforming soft best-of-n with the primary model.

Conclusion: GSI is a promising approach for efficient and accurate reward-guided decoding, with potential for broader applications in reasoning tasks.

Abstract: We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .

</details>


### [513] [Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems](https://arxiv.org/pdf/2506.04126)
*Yujun Kim, Jaeyoung Cha, Chulhee Yun*

Main category: cs.LG

TL;DR: The paper investigates the convergence behavior of Incremental Gradient Descent (IGD) in the small epoch regime, contrasting with known results for permutation-based SGD in large epochs. It highlights slower convergence for IGD under certain conditions and worse performance with nonconvex components.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding convergence rates of permutation-based SGD when the number of epochs is smaller than the condition number, focusing on IGD as a deterministic variant.

Method: The study analyzes IGD on smooth and strongly convex functions, providing lower bounds for convergence in the small epoch regime and examining the impact of nonconvex component functions.

Result: IGD exhibits slow convergence in the small epoch regime, especially with nonconvex components, and the paper provides tight bounds for IGD in the large epoch regime.

Conclusion: The convergence properties of permutation-based SGD vary significantly in the small epoch regime depending on component function assumptions, with IGD showing limitations compared to large epoch results.

Abstract: Recent theoretical results demonstrate that the convergence rates of
permutation-based SGD (e.g., random reshuffling SGD) are faster than
uniform-sampling SGD; however, these studies focus mainly on the large epoch
regime, where the number of epochs $K$ exceeds the condition number $\kappa$.
In contrast, little is known when $K$ is smaller than $\kappa$, and it is still
a challenging open question whether permutation-based SGD can converge faster
in this small epoch regime (Safran and Shamir, 2021). As a step toward
understanding this gap, we study the naive deterministic variant, Incremental
Gradient Descent (IGD), on smooth and strongly convex functions. Our lower
bounds reveal that for the small epoch regime, IGD can exhibit surprisingly
slow convergence even when all component functions are strongly convex.
Furthermore, when some component functions are allowed to be nonconvex, we
prove that the optimality gap of IGD can be significantly worse throughout the
small epoch regime. Our analyses reveal that the convergence properties of
permutation-based SGD in the small epoch regime may vary drastically depending
on the assumptions on component functions. Lastly, we supplement the paper with
tight upper and lower bounds for IGD in the large epoch regime.

</details>


### [514] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/pdf/2506.04165)
*Yashas Samaga, Varun Yerram, Spandana Raj Babbula, Prateek Jain, Praneeth Netrapalli*

Main category: cs.LG

TL;DR: The paper generalizes a two-stage approximate Top-K selection algorithm, improves recall bounds, and achieves significant speedups on TPUv5e.


<details>
  <summary>Details</summary>
Motivation: Top-K selection is a bottleneck in machine learning on accelerators. The paper aims to enhance the efficiency and theoretical understanding of an existing approximate algorithm.

Method: Generalizes the original two-stage algorithm by selecting top-K' elements per partition (1 ≤ K' ≤ K), derives expected recall expressions, and tightens recall bounds.

Result: Choosing K' > 1 reduces input size to the second stage while maintaining recall. The improved bound is tighter by a factor of 2. Implementation on TPUv5e shows ~10x speedups without recall loss.

Conclusion: The generalized algorithm is more efficient and theoretically sound, with practical speedups on real-world tasks.

Abstract: We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [515] [N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion](https://arxiv.org/pdf/2506.04166)
*Caleb Chin, Aashish Khubchandani, Harshvardhan Maskara, Kyuseong Choi, Jacob Feitelberg, Albert Gong, Manit Paul, Tathagata Sadhukhan, Anish Agarwal, Raaz Dwivedi*

Main category: cs.LG

TL;DR: The paper introduces N$^2$, a Python package for NN-based matrix completion, showcasing its modularity, extensibility, and a new NN variant with state-of-the-art performance. It also provides real-world benchmarks, demonstrating NN methods' superiority over classical approaches in practical settings.


<details>
  <summary>Details</summary>
Motivation: To unify and simplify the use of NN-based methods for matrix completion, addressing the gap between theoretical guarantees and practical applications.

Method: Developed N$^2$, a modular Python package, and introduced a new NN variant. Benchmarked performance using real-world datasets across diverse domains.

Result: NN-based methods, especially the new variant, outperform classical methods in real-world scenarios, as validated by the benchmark suite.

Conclusion: NN methods are robust and effective for matrix completion in practical settings, with N$^2$ serving as a valuable tool for research and application.

Abstract: Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix
completion, offering strong empirical performance and recent theoretical
guarantees, including entry-wise error bounds, confidence intervals, and
minimax optimality. Despite their simplicity, recent work has shown that NN
approaches are robust to a range of missingness patterns and effective across
diverse applications. This paper introduces N$^2$, a unified Python package and
testbed that consolidates a broad class of NN-based methods through a modular,
extensible interface. Built for both researchers and practitioners, N$^2$
supports rapid experimentation and benchmarking. Using this framework, we
introduce a new NN variant that achieves state-of-the-art results in several
settings. We also release a benchmark suite of real-world datasets, from
healthcare and recommender systems to causal inference and LLM evaluation,
designed to stress-test matrix completion methods beyond synthetic scenarios.
Our experiments demonstrate that while classical methods excel on idealized
data, NN-based techniques consistently outperform them in real-world settings.

</details>


### [516] [Horizon Reduction Makes RL Scalable](https://arxiv.org/pdf/2506.04168)
*Seohong Park, Kevin Frans, Deepinder Mann, Benjamin Eysenbach, Aviral Kumar, Sergey Levine*

Main category: cs.LG

TL;DR: The paper investigates the scalability of offline RL algorithms, finding that many fail to scale well with large datasets due to long horizons. Horizon reduction techniques, including the proposed SHARSA method, significantly improve scalability.


<details>
  <summary>Details</summary>
Motivation: To determine if current offline RL algorithms can scale effectively with large datasets and complex tasks, and to identify barriers to scalability.

Method: Evaluates existing offline RL algorithms on large datasets, analyzes the impact of horizon length, and introduces SHARSA, a horizon reduction technique.

Result: Long horizons hinder scalability; SHARSA outperforms other methods by effectively reducing horizon length.

Conclusion: Horizon reduction is key to scalable offline RL, with SHARSA demonstrating superior performance.

Abstract: In this work, we study the scalability of offline reinforcement learning (RL)
algorithms. In principle, a truly scalable offline RL algorithm should be able
to solve any given problem, regardless of its complexity, given sufficient
data, compute, and model capacity. We investigate if and how current offline RL
algorithms match up to this promise on diverse, challenging, previously
unsolved tasks, using datasets up to 1000x larger than typical offline RL
datasets. We observe that despite scaling up data, many existing offline RL
algorithms exhibit poor scaling behavior, saturating well below the maximum
performance. We hypothesize that the horizon is the main cause behind the poor
scaling of offline RL. We empirically verify this hypothesis through several
analysis experiments, showing that long horizons indeed present a fundamental
barrier to scaling up offline RL. We then show that various horizon reduction
techniques substantially enhance scalability on challenging tasks. Based on our
insights, we also introduce a minimal yet scalable method named SHARSA that
effectively reduces the horizon. SHARSA achieves the best asymptotic
performance and scaling behavior among our evaluation methods, showing that
explicitly reducing the horizon unlocks the scalability of offline RL. Code:
https://github.com/seohongpark/horizon-reduction

</details>


### [517] [Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints](https://arxiv.org/pdf/2506.04171)
*Utkarsh Utkarsh, Pengfei Cai, Alan Edelman, Rafael Gomez-Bombarelli, Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: PCFM enforces hard physical constraints in generative models for PDEs, outperforming baselines while ensuring exact constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to enforce hard physical constraints in generative models for PDEs, often relying on soft penalties or biases.

Method: Proposes Physics-Constrained Flow Matching (PCFM), a zero-shot framework that applies physics-based corrections during sampling to ensure constraints.

Result: PCFM outperforms unconstrained and constrained baselines on various PDEs, handling shocks and sharp features while satisfying constraints.

Conclusion: PCFM provides a general framework for enforcing hard constraints in generative models, crucial for applications requiring exact constraint satisfaction.

Abstract: Deep generative models have recently been applied to physical systems
governed by partial differential equations (PDEs), offering scalable simulation
and uncertainty-aware inference. However, enforcing physical constraints, such
as conservation laws (linear and nonlinear) and physical consistencies, remains
challenging. Existing methods often rely on soft penalties or architectural
biases that fail to guarantee hard constraints. In this work, we propose
Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that
enforces arbitrary nonlinear constraints in pretrained flow-based generative
models. PCFM continuously guides the sampling process through physics-based
corrections applied to intermediate solution states, while remaining aligned
with the learned flow and satisfying physical constraints. Empirically, PCFM
outperforms both unconstrained and constrained baselines on a range of PDEs,
including those with shocks, discontinuities, and sharp features, while
ensuring exact constraint satisfaction at the final solution. Our method
provides a general framework for enforcing hard constraints in both scientific
and general-purpose generative models, especially in applications where
constraint satisfaction is essential.

</details>


### [518] [Does Prompt Design Impact Quality of Data Imputation by LLMs?](https://arxiv.org/pdf/2506.04172)
*Shreenidhi Srinivasan, Lydia Manikonda*

Main category: cs.LG

TL;DR: A novel token-aware data imputation method using LLMs improves synthetic tabular data generation for class-imbalanced datasets by optimizing prompt design.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating realistic synthetic tabular data, especially with class imbalance, by leveraging LLMs' in-context learning.

Method: Combines structured group-wise CSV-style prompting and removes irrelevant contextual information in prompts.

Result: Reduces prompt size while maintaining/improvising imputation quality, particularly for smaller datasets.

Conclusion: Highlights prompt design importance for LLM-based synthetic data generation and offers a practical solution for class-imbalanced datasets.

Abstract: Generating realistic synthetic tabular data presents a critical challenge in
machine learning. It adds another layer of complexity when this data contain
class imbalance problems. This paper presents a novel token-aware data
imputation method that leverages the in-context learning capabilities of large
language models. This is achieved through the combination of a structured
group-wise CSV-style prompting technique and the elimination of irrelevant
contextual information in the input prompt. We test this approach with two
class-imbalanced binary classification datasets and evaluate the effectiveness
of imputation using classification-based evaluation metrics. The experimental
results demonstrate that our approach significantly reduces the input prompt
size while maintaining or improving imputation quality compared to our baseline
prompt, especially for datasets that are of relatively smaller in size. The
contributions of this presented work is two-fold -- 1) it sheds light on the
importance of prompt design when leveraging LLMs for synthetic data generation
and 2) it addresses a critical gap in LLM-based data imputation for
class-imbalanced datasets with missing data by providing a practical solution
within computational constraints. We hope that our work will foster further
research and discussions about leveraging the incredible potential of LLMs and
prompt engineering techniques for synthetic data generation.

</details>


### [519] [MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures](https://arxiv.org/pdf/2506.04195)
*Elena Zamaraeva, Christopher M. Collins, George R. Darling, Matthew S. Dyer, Bei Peng, Rahul Savani, Dmytro Antypov, Vladimir V. Gusev, Judith Clymo, Paul G. Spirakis, Matthew J. Rosseinsky*

Main category: cs.LG

TL;DR: MACS is a multi-agent reinforcement learning method for optimizing periodic crystal structures, outperforming state-of-the-art methods in speed, efficiency, and scalability.


<details>
  <summary>Details</summary>
Motivation: Geometry optimization is critical in computational chemistry and materials design, but existing methods may lack efficiency or scalability.

Method: MACS treats optimization as a partially observable Markov game, with atoms as agents adjusting positions to find stable configurations.

Result: MACS optimizes structures faster, with fewer energy calculations and lower failure rates, and shows excellent scalability and zero-shot transferability.

Conclusion: MACS is a highly effective and scalable solution for periodic crystal structure optimization.

Abstract: Geometry optimization of atomic structures is a common and crucial task in
computational chemistry and materials design. Following the learning to
optimize paradigm, we propose a new multi-agent reinforcement learning method
called Multi-Agent Crystal Structure optimization (MACS) to address periodic
crystal structure optimization. MACS treats geometry optimization as a
partially observable Markov game in which atoms are agents that adjust their
positions to collectively discover a stable configuration. We train MACS across
various compositions of reported crystalline materials to obtain a policy that
successfully optimizes structures from the training compositions as well as
structures of larger sizes and unseen compositions, confirming its excellent
scalability and zero-shot transferability. We benchmark our approach against a
broad range of state-of-the-art optimization methods and demonstrate that MACS
optimizes periodic crystal structures significantly faster, with fewer energy
calculations, and the lowest failure rate.

</details>


### [520] [OpenThoughts: Data Recipes for Reasoning Models](https://arxiv.org/pdf/2506.04178)
*Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, Ludwig Schmidt*

Main category: cs.LG

TL;DR: OpenThoughts project creates open-source datasets for reasoning models, achieving state-of-the-art results with OpenThinker3-7B.


<details>
  <summary>Details</summary>
Motivation: Address the lack of public information on training recipes for reasoning models by developing open-source datasets.

Method: Systematically improve datasets through controlled experiments, scale data generation, and use QwQ-32B as a teacher model.

Result: OpenThinker3-7B achieves 53% on AIME 2025, 51% on LiveCodeBench, and 54% on GPQA Diamond.

Conclusion: Open-source datasets and models like OpenThoughts3 and OpenThinker3-7B can match or exceed proprietary solutions.

Abstract: Reasoning models have made rapid progress on many benchmarks involving math,
code, and science. Yet, there are still many open questions about the best
training recipes for reasoning since state-of-the-art models often rely on
proprietary datasets with little to no public information available. To address
this, the goal of the OpenThoughts project is to create open-source datasets
for training reasoning models. After initial explorations, our OpenThoughts2-1M
dataset led to OpenThinker2-32B, the first model trained on public reasoning
data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as
AIME and LiveCodeBench. We then improve our dataset further by systematically
investigating each step of our data generation pipeline with 1,000+ controlled
experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples
and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves
state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,
and 54% on GPQA Diamond. All of our datasets and models are available on
https://openthoughts.ai.

</details>


### [521] [How to Use Graph Data in the Wild to Help Graph Anomaly Detection?](https://arxiv.org/pdf/2506.04190)
*Yuxuan Cao, Jiarong Xu, Chen Zhao, Jiaan Wang, Carl Yang, Chunping Wang, Yang Yang*

Main category: cs.LG

TL;DR: The paper proposes Wild-GAD, a framework leveraging external graph data to improve unsupervised graph anomaly detection, addressing challenges like label scarcity and data insufficiency.


<details>
  <summary>Details</summary>
Motivation: Graph anomaly detection faces issues like label scarcity and ill-defined anomalies, making supervised methods unreliable. Unsupervised methods struggle with insufficient data for capturing normal distributions.

Method: Wild-GAD uses external graph data (UniWildGraph) and selection criteria based on representativity and diversity to enhance anomaly detection.

Result: Experiments show Wild-GAD outperforms baselines with 18% AUCROC and 32% AUCPR improvements.

Conclusion: Utilizing external data via Wild-GAD effectively addresses data insufficiency in graph anomaly detection, demonstrating significant performance gains.

Abstract: In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.

</details>


### [522] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/pdf/2506.04207)
*Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng*

Main category: cs.LG

TL;DR: ReVisual-R1 improves multimodal reasoning in MLLMs by addressing training pipeline issues, including cold start initialization, gradient stagnation, and staged RL training, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in Multimodal Large Language Models (MLLMs) by addressing limitations in current reinforcement learning (RL) approaches.

Method: Identifies three key training phenomena, proposes solutions (cold start initialization, addressing gradient stagnation, staged RL training), and introduces ReVisual-R1.

Result: ReVisual-R1 achieves state-of-the-art performance on multiple benchmarks (MathVerse, MathVision, etc.).

Conclusion: Staged training and addressing RL issues significantly improve multimodal reasoning, setting a new benchmark for 7B MLLMs.

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


### [523] [EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://arxiv.org/pdf/2506.04205)
*Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu*

Main category: cs.LG

TL;DR: EPiC condenses CoT traces by retaining only initial and final segments, reducing training time by 34% without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: High training costs from verbose CoT traces in LLMs, especially when distilled from LRMs, necessitate efficient condensation methods.

Method: Proposes Edge-Preserving Condensation (EPiC), which prunes intermediate reasoning steps, keeping only problem understanding and solution convergence stages.

Result: EPiC reduces training time by 34% while maintaining reasoning accuracy comparable to full CoT supervision on MATH500.

Conclusion: EPiC offers a resource-efficient method for CoT condensation, preserving reasoning quality and reducing costs.

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at pruning
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.

</details>


### [524] [A Few Moments Please: Scalable Graphon Learning via Moment Matching](https://arxiv.org/pdf/2506.04206)
*Reza Ramezanpour, Victor M. Tenorio, Antonio G. Marques, Ashutosh Sabharwal, Santiago Segarra*

Main category: cs.LG

TL;DR: A scalable graphon estimator using implicit neural representations (INRs) and moment matching, avoiding latent variables and Gromov-Wasserstein distance, with theoretical guarantees and improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing graphon estimation methods face scalability and resolution issues due to reliance on latent variables or costly metrics like Gromov-Wasserstein distance.

Method: Proposes a direct graphon recovery via moment matching using INRs, avoiding latent variables and combinatorial complexity. Introduces MomentMixup for data augmentation.

Result: Achieves high accuracy on small graphs and superior efficiency on large ones, outperforming state-of-the-art in 75% of benchmarks. MomentMixup improves graph classification.

Conclusion: The method offers a scalable, efficient, and accurate solution for graphon estimation, with theoretical guarantees and practical benefits.

Abstract: Graphons, as limit objects of dense graph sequences, play a central role in
the statistical analysis of network data. However, existing graphon estimation
methods often struggle with scalability to large networks and
resolution-independent approximation, due to their reliance on estimating
latent variables or costly metrics such as the Gromov-Wasserstein distance. In
this work, we propose a novel, scalable graphon estimator that directly
recovers the graphon via moment matching, leveraging implicit neural
representations (INRs). Our approach avoids latent variable modeling by
training an INR--mapping coordinates to graphon values--to match empirical
subgraph counts (i.e., moments) from observed graphs. This direct estimation
mechanism yields a polynomial-time solution and crucially sidesteps the
combinatorial complexity of Gromov-Wasserstein optimization. Building on
foundational results, we establish a theoretical guarantee: when the observed
subgraph motifs sufficiently represent those of the true graphon (a condition
met with sufficiently large or numerous graph samples), the estimated graphon
achieves a provable upper bound in cut distance from the ground truth.
Additionally, we introduce MomentMixup, a data augmentation technique that
performs mixup in the moment space to enhance graphon-based learning. Our
graphon estimation method achieves strong empirical performance--demonstrating
high accuracy on small graphs and superior computational efficiency on large
graphs--outperforming state-of-the-art scalable estimators in 75\% of benchmark
settings and matching them in the remaining cases. Furthermore, MomentMixup
demonstrated improved graph classification accuracy on the majority of our
benchmarks.

</details>


### [525] [EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost](https://arxiv.org/pdf/2306.01310)
*Jaeseung Heo, Seungbeom Lee, Sungsoo Ahn, Dongwoo Kim*

Main category: cs.LG

TL;DR: EPIC is a novel graph data augmentation method using edit path interpolation and a learnable cost model to improve model generalization.


<details>
  <summary>Details</summary>
Motivation: Graph data augmentation is challenging due to irregular structures, and existing methods lack nuanced transformations.

Method: EPIC uses graph edit distance to construct edit paths between graphs and introduces a learnable, context-sensitive cost model for meaningful transformations.

Result: EPIC outperforms existing augmentation techniques on benchmark datasets.

Conclusion: EPIC provides an effective solution for graph data augmentation, enhancing model performance.

Abstract: Data augmentation plays a critical role in improving model performance across
various domains, but it becomes challenging with graph data due to their
complex and irregular structure. To address this issue, we propose EPIC (Edit
Path Interpolation via learnable Cost), a novel interpolation-based method for
augmenting graph datasets. To interpolate between two graphs lying in an
irregular domain, EPIC leverages the concept of graph edit distance,
constructing an edit path that represents the transformation process between
two graphs via edit operations. Moreover, our method introduces a
context-sensitive cost model that accounts for the importance of specific edit
operations formulated through a learning framework. This allows for a more
nuanced transformation process, where the edit distance is not merely
count-based but reflects meaningful graph attributes. With randomly sampled
graphs from the edit path, we enrich the training set to enhance the
generalization capability of classification models. Experimental evaluations
across several benchmark datasets demonstrate that our approach outperforms
existing augmentation techniques in many tasks.

</details>


### [526] [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities](https://arxiv.org/pdf/2309.16739)
*Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen, Kaibin Huang*

Main category: cs.LG

TL;DR: The paper explores deploying large language models (LLMs) at the 6G edge to address cloud-based challenges like latency, bandwidth costs, and privacy, proposing edge training and inference techniques.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of cloud-based LLM deployment (e.g., latency, privacy) by leveraging 6G mobile edge computing (MEC) for efficient, localized LLM applications.

Method: Proposes 6G MEC architecture for LLMs, focusing on edge training and inference using techniques like split learning, quantization, and parameter-sharing.

Result: Identifies challenges and solutions for LLM deployment at the edge, emphasizing resource-efficient methods.

Conclusion: The paper advocates for 6G edge deployment of LLMs, outlining a pathway to address current challenges and enable future applications.

Abstract: Large language models (LLMs), which have shown remarkable capabilities, are
revolutionizing AI development and potentially shaping our future. However,
given their multimodality, the status quo cloud-based deployment faces some
critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the
violation of data privacy. 6G mobile edge computing (MEC) systems may resolve
these pressing issues. In this article, we explore the potential of deploying
LLMs at the 6G edge. We start by introducing killer applications powered by
multimodal LLMs, including robotics and healthcare, to highlight the need for
deploying LLMs in the vicinity of end users. Then, we identify the critical
challenges for LLM deployment at the edge and envision the 6G MEC architecture
for LLMs. Furthermore, we delve into two design aspects, i.e., edge training
and edge inference for LLMs. In both aspects, considering the inherent resource
limitations at the edge, we discuss various cutting-edge techniques, including
split learning/inference, parameter-efficient fine-tuning, quantization, and
parameter-sharing inference, to facilitate the efficient deployment of LLMs.
This article serves as a position paper for thoroughly identifying the
motivation, challenges, and pathway for empowering LLMs at the 6G edge.

</details>


### [527] [AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks](https://arxiv.org/pdf/2403.13101)
*Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung*

Main category: cs.LG

TL;DR: The paper introduces AdaptSFL, a resource-adaptive framework for split federated learning (SFL) to optimize performance in resource-constrained edge systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of deep neural networks limits their deployment on edge devices. SFL offloads training to servers but lacks system optimization insights.

Method: The paper provides a convergence analysis of SFL and proposes AdaptSFL, which adaptively controls model splitting and client-side aggregation to balance latency and convergence.

Result: Simulations show AdaptSFL achieves target accuracy faster than benchmarks.

Conclusion: AdaptSFL effectively optimizes SFL for resource-constrained edge systems, validated by theoretical and experimental results.

Abstract: The increasing complexity of deep neural networks poses significant barriers
to democratizing them to resource-limited edge devices. To address this
challenge, split federated learning (SFL) has emerged as a promising solution
by of floading the primary training workload to a server via model partitioning
while enabling parallel training among edge devices. However, although system
optimization substantially influences the performance of SFL under
resource-constrained systems, the problem remains largely uncharted. In this
paper, we provide a convergence analysis of SFL which quantifies the impact of
model splitting (MS) and client-side model aggregation (MA) on the learning
performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a
novel resource-adaptive SFL framework, to expedite SFL under
resource-constrained edge computing systems. Specifically, AdaptSFL adaptively
controls client-side MA and MS to balance communication-computing latency and
training convergence. Extensive simulations across various datasets validate
that our proposed AdaptSFL framework takes considerably less time to achieve a
target accuracy than benchmarks, demonstrating the effectiveness of the
proposed strategies.

</details>


### [528] [DropCluster: A structured dropout for convolutional networks](https://arxiv.org/pdf/2002.02997)
*Liyan Chen, Philippos Mordohai, Sergul Aydore*

Main category: cs.LG

TL;DR: DropCluster, a structured Dropout method, improves regularization in convolutional layers by clustering and dropping feature clusters adaptively, outperforming other methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Dropout is less effective in convolutional layers due to ignoring local spatial correlations, leading to information leakage. Existing structured Dropout methods lack adaptability.

Method: Introduces DropCluster, which clusters features in convolutional layers and randomly drops these clusters during training to better regularize and prevent overfitting.

Result: Experiments on CIFAR-10/100, SVHN, and APPA-REAL show DropCluster effectively controls overfitting and outperforms other methods.

Conclusion: DropCluster addresses the limitations of traditional Dropout in convolutional layers by leveraging feature clustering, offering a more adaptive and effective regularization approach.

Abstract: Dropout as a common regularizer to prevent overfitting in deep neural
networks has been less effective in convolutional layers than in fully
connected layers. This is because Dropout drops features randomly, without
considering local structure. When features are spatially correlated, as in the
case of convolutional layers, information from the dropped features can still
propagate to subsequent layers via neighboring features. To address this
problem, structured forms of Dropout have been proposed. A drawback of these
methods is that they do not adapt to the data. In this work, we leverage the
structure in the outputs of convolutional layers and introduce a novel
structured regularization method named DropCluster. Our approach clusters
features in convolutional layers, and drops the resulting clusters randomly
during training iterations. Experiments on CIFAR-10/100, SVHN, and APPA-REAL
datasets demonstrate that our approach is effective and controls overfitting
better than other approaches.

</details>


### [529] [Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem](https://arxiv.org/pdf/2303.05978)
*Xavier Aramayo Carrasco, Maksim Nekrashevich, Petr Mokrov, Evgeny Burnaev, Alexander Korotin*

Main category: cs.LG

TL;DR: The paper benchmarks continuous Gromov-Wasserstein Optimal Transport (GWOT) methods, identifies their limitations, and proposes a new solver avoiding discrete techniques.


<details>
  <summary>Details</summary>
Motivation: GWOT is geometrically intuitive but challenging, and existing continuous solvers rely on discrete methods. The paper aims to evaluate their effectiveness and propose improvements.

Method: Benchmarking existing continuous GWOT approaches on various scenarios, analyzing results, and introducing a new method independent of discrete techniques.

Result: Existing continuous GWOT solvers are unreliable; the new method partially addresses their shortcomings.

Conclusion: Further research is needed for reliable continuous GWOT solvers; the proposed method is a step forward.

Abstract: Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has
attracted the special attention of the ML community. In this problem, given two
distributions supported on two (possibly different) spaces, one has to find the
most isometric map between them. In the discrete variant of GWOT, the task is
to learn an assignment between given discrete sets of points. In the more
advanced continuous formulation, one aims at recovering a parametric mapping
between unknown continuous distributions based on i.i.d. samples derived from
them. The clear geometrical intuition behind the GWOT makes it a natural choice
for several practical use cases, giving rise to a number of proposed solvers.
Some of them claim to solve the continuous version of the problem. At the same
time, GWOT is notoriously hard, both theoretically and numerically. Moreover,
all existing continuous GWOT solvers still heavily rely on discrete techniques.
Natural questions arise: to what extent do existing methods unravel the GWOT
problem, what difficulties do they encounter, and under which conditions they
are successful? Our benchmark paper is an attempt to answer these questions. We
specifically focus on the continuous GWOT as the most interesting and debatable
setup. We crash-test existing continuous GWOT approaches on different
scenarios, carefully record and analyze the obtained results, and identify
issues. Our findings experimentally testify that the scientific community is
still missing a reliable continuous GWOT solver, which necessitates further
research efforts. As the first step in this direction, we propose a new
continuous GWOT method which does not rely on discrete techniques and partially
solves some of the problems of the competitors.

</details>


### [530] [Torch-Choice: A PyTorch Package for Large-Scale Choice Modeling with Python](https://arxiv.org/pdf/2304.01906)
*Tianyu Du, Ayush Kanodia, Susan Athey*

Main category: cs.LG

TL;DR: torch-choice is a Python/PyTorch library for flexible, fast choice modeling, featuring GPU support and scalability for large datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible, memory-efficient, and computationally powerful tool for choice modeling in Python.

Method: Implements multinomial logit and nested logit models, supports regularization, and offers GPU acceleration. Uses ChoiceDataset for data management.

Result: Demonstrates scalability and efficiency, outperforming mlogit in R for large datasets.

Conclusion: torch-choice is a robust, scalable solution for choice modeling, especially for large datasets.

Abstract: The $\texttt{torch-choice}$ is an open-source library for flexible, fast
choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a
$\texttt{ChoiceDataset}$ data structure to manage databases flexibly and
memory-efficiently. The paper demonstrates constructing a
$\texttt{ChoiceDataset}$ from databases of various formats and functionalities
of $\texttt{ChoiceDataset}$. The package implements two widely used models,
namely the multinomial logit and nested logit models, and supports
regularization during model estimation. The package incorporates the option to
take advantage of GPUs for estimation, allowing it to scale to massive datasets
while being computationally efficient. Models can be initialized using either
R-style formula strings or Python dictionaries. We conclude with a comparison
of the computational efficiencies of $\texttt{torch-choice}$ and
$\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the
number of covariates increases, and (3) the expansion of item sets. Finally, we
demonstrate the scalability of $\texttt{torch-choice}$ on large-scale datasets.

</details>


### [531] [LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models](https://arxiv.org/pdf/2405.17829)
*Jinho Chang, Jong Chul Ye*

Main category: cs.LG

TL;DR: LDMol, a latent diffusion model, improves text-conditioned molecule generation by using contrastive learning for better latent space design, outperforming autoregressive models and enabling versatile downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The discreteness of molecules makes it hard for diffusion models to link raw data with complex conditions like natural language, prompting the need for a better latent space design.

Method: LDMol employs contrastive learning to extract a novel feature space from text data, embedding molecule structure characteristics for improved diffusion model performance.

Result: LDMol outperforms autoregressive baselines in text-to-molecule generation and is applicable to tasks like molecule-to-text retrieval and text-guided molecule editing.

Conclusion: LDMol demonstrates the effectiveness of latent diffusion models for molecule generation and their versatility in downstream applications.

Abstract: With the emergence of diffusion models as a frontline generative model, many
researchers have proposed molecule generation techniques with conditional
diffusion models. However, the unavoidable discreteness of a molecule makes it
difficult for a diffusion model to connect raw data with highly complex
conditions like natural language. To address this, here we present a novel
latent diffusion model dubbed LDMol for text-conditioned molecule generation.
By recognizing that the suitable latent space design is the key to the
diffusion model performance, we employ a contrastive learning strategy to
extract novel feature space from text data that embeds the unique
characteristics of the molecule structure. Experiments show that LDMol
outperforms the existing autoregressive baselines on the text-to-molecule
generation benchmark, being one of the first diffusion models that outperforms
autoregressive models in textual data generation with a better choice of the
latent domain. Furthermore, we show that LDMol can be applied to downstream
tasks such as molecule-to-text retrieval and text-guided molecule editing,
demonstrating its versatility as a diffusion model.

</details>


### [532] [Automated Architecture Synthesis for Arbitrarily Structured Neural Networks](https://arxiv.org/pdf/2306.02157)
*Xinshun Liu, Yizhi Fang, Yichao Jiang*

Main category: cs.LG

TL;DR: A novel ANN framework inspired by biological neural systems, enabling arbitrary graph structures and Neural Modules for improved collaboration and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional ANNs lack horizontal and backward communication, limiting collaboration. Biological neural systems' complex connections inspire a more flexible approach.

Method: Proposes a framework learning arbitrary graph structures during training, using Neural Modules for organization. Includes structure computation and regularization for balanced modules.

Result: Reduces overfitting, improves efficiency via parallel computing, and adapts to various tasks. Experimental results show potential.

Conclusion: The method enables ANNs to learn biologically inspired structures, enhancing adaptability and performance across scenarios.

Abstract: This paper offers a new perspective on Artificial Neural Networks (ANNs)
architecture. Traditional ANNs commonly use tree-like or DAG structures for
simplicity, which can be preset or determined by Neural Architecture Search
(NAS). Yet, these structures restrict network collaboration and capability due
to the absence of horizontal and backward communication. Biological neural
systems, however, feature billions of neural units with highly complex
connections, allowing each biological neuron to connect with others based on
specific situations. Inspired by biological systems, we propose a novel
framework that learns to construct arbitrary graph structures during training
and introduce the concept of Neural Modules for organizing neural units, which
facilitates communication between any nodes and collaboration among modules.
Unlike traditional NAS methods that rely on DAG search spaces, our framework
learns from complete graphs, enabling free communication between neurons akin
to biological neural networks. Furthermore, we present a method to compute
these structures and a regularization technique that organizes them into
multiple independent, balanced neural modules. This approach reduces
overfitting and improves efficiency through parallel computing. Overall, our
method allows ANNs to learn effective arbitrary structures similar to
biological ones. It is adaptable to various tasks and compatible across
different scenarios, with experimental results demonstrating its potential.

</details>


### [533] [Quantifying Prediction Consistency Under Fine-Tuning Multiplicity in Tabular LLMs](https://arxiv.org/pdf/2407.04173)
*Faisal Hamman, Pasan Dissanayake, Saumitra Mishra, Freddy Lecue, Sanghamitra Dutta*

Main category: cs.LG

TL;DR: The paper addresses fine-tuning multiplicity in LLMs for tabular classification, proposing a measure to quantify prediction consistency without retraining, validated on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for tabular tasks can yield conflicting predictions due to training variations, raising reliability concerns in high-stakes applications.

Method: The work introduces a measure to quantify prediction consistency by analyzing local model behavior in the embedding space, providing probabilistic guarantees.

Result: Experiments show the proposed measure effectively captures consistency under fine-tuning multiplicity, outperforming alternatives.

Conclusion: The proposed local stability measure offers a practical solution to assess prediction consistency in Tabular LLMs, enhancing reliability.

Abstract: Fine-tuning LLMs on tabular classification tasks can lead to the phenomenon
of fine-tuning multiplicity where equally well-performing models make
conflicting predictions on the same input. Fine-tuning multiplicity can arise
due to variations in the training process, e.g., seed, weight initialization,
minor changes to training data, etc., raising concerns about the reliability of
Tabular LLMs in high-stakes applications such as finance, hiring, education,
healthcare. Our work formalizes this unique challenge of fine-tuning
multiplicity in Tabular LLMs and proposes a novel measure to quantify the
consistency of individual predictions without expensive model retraining. Our
measure quantifies a prediction's consistency by analyzing (sampling) the
model's local behavior around that input in the embedding space. Interestingly,
we show that sampling in the local neighborhood can be leveraged to provide
probabilistic guarantees on prediction consistency under a broad class of
fine-tuned models, i.e., inputs with sufficiently high local stability (as
defined by our measure) also remain consistent across several fine-tuned models
with high probability. We perform experiments on multiple real-world datasets
to show that our local stability measure preemptively captures consistency
under actual multiplicity across several fine-tuned models, outperforming
competing measures.

</details>


### [534] [Easy attention: A simple attention mechanism for temporal predictions with transformers](https://arxiv.org/pdf/2308.12874)
*Marcial Sanchis-Agudo, Yuning Wang, Roger Arnau, Luca Guastoni, Jasmin Lim, Karthik Duraisamy, Ricardo Vinuesa*

Main category: cs.LG

TL;DR: The paper introduces 'easy attention,' a simplified attention mechanism for transformers, improving robustness in predicting chaotic systems by treating attention scores as learnable parameters.


<details>
  <summary>Details</summary>
Motivation: To enhance the robustness of transformer networks in chaotic system prediction by simplifying the attention mechanism.

Method: Proposes 'easy attention,' replacing standard self-attention with learnable attention scores, validated via SVD analysis.

Result: Demonstrates superior performance in reconstructing and predicting chaotic systems (Lorenz, turbulence flow, nuclear reactor model) compared to self-attention and LSTM.

Conclusion: Easy attention offers a simpler, more robust alternative to traditional attention mechanisms for chaotic temporal dynamics.

Abstract: To improve the robustness of transformer neural networks used for
temporal-dynamics prediction of chaotic systems, we propose a novel attention
mechanism called easy attention which we demonstrate in time-series
reconstruction and prediction. While the standard self attention only makes use
of the inner product of queries and keys, it is demonstrated that the keys,
queries and softmax are not necessary for obtaining the attention score
required to capture long-term dependencies in temporal sequences. Through the
singular-value decomposition (SVD) on the softmax attention score, we further
observe that self attention compresses the contributions from both queries and
keys in the space spanned by the attention score. Therefore, our proposed
easy-attention method directly treats the attention scores as learnable
parameters. This approach produces excellent results when reconstructing and
predicting the temporal dynamics of chaotic systems exhibiting more robustness
and less complexity than self attention or the widely-used long short-term
memory (LSTM) network. We show the improved performance of the easy-attention
method in the Lorenz system, a turbulence shear flow and a model of a nuclear
reactor.

</details>


### [535] [Understanding the Training Speedup from Sampling with Approximate Losses](https://arxiv.org/pdf/2402.07052)
*Rudrajit Das, Xi Chen, Bertram Ieong, Parikshit Bansal, Sujay Sanghavi*

Main category: cs.LG

TL;DR: SIFT reduces training time by selecting samples with approximate losses instead of exact losses, using early exiting for efficiency.


<details>
  <summary>Details</summary>
Motivation: Reducing the overhead of sample selection in training by approximating losses, aiming for faster convergence.

Method: Proposes SIFT, which uses early exiting to approximate losses from intermediate layers for sample selection.

Result: SIFT achieves significant training time reduction (e.g., 43 vs. 57 hours for 64% accuracy) without optimized implementation.

Conclusion: Approximate loss selection via SIFT is effective for reducing training time while maintaining performance.

Abstract: It is well known that selecting samples with large losses/gradients can
significantly reduce the number of training steps. However, the selection
overhead is often too high to yield any meaningful gains in terms of overall
training time. In this work, we focus on the greedy approach of selecting
samples with large \textit{approximate losses} instead of exact losses in order
to reduce the selection overhead. For smooth convex losses, we show that such a
greedy strategy can converge to a constant factor of the minimum value of the
average loss in fewer iterations than the standard approach of random
selection. We also theoretically quantify the effect of the approximation
level. We then develop SIFT which uses early exiting to obtain approximate
losses with an intermediate layer's representations for sample selection. We
evaluate SIFT on the task of training a 110M parameter 12 layer BERT base
model, and show significant gains (in terms of training hours and number of
backpropagation steps) without any optimized implementation over vanilla
training. For e.g., to reach 64% validation accuracy, SIFT with exit at the
first layer takes ~ 43 hours compared to ~ 57 hours of vanilla training.

</details>


### [536] [Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting](https://arxiv.org/pdf/2410.12593)
*Wei Chen, Yuxuan Liang*

Main category: cs.LG

TL;DR: A novel prompt tuning-based method for spatio-temporal forecasting in streaming scenarios addresses inefficiency and catastrophic forgetting by integrating a continuous prompt pool with a base spatio-temporal graph neural network.


<details>
  <summary>Details</summary>
Motivation: Real-world spatio-temporal data arrives in streams, and networks expand with new sensors, making retraining inefficient and causing catastrophic forgetting of long-term history.

Method: Proposes a prompt tuning-based continuous forecasting method with 'expand and compress' principles, integrating a base spatio-temporal graph neural network with a continuous prompt pool for lightweight tuning.

Result: Extensive experiments show the method's superiority in effectiveness, efficiency, and universality over state-of-the-art baselines.

Conclusion: The method effectively addresses streaming spatio-temporal forecasting challenges with lightweight tuning, outperforming existing approaches.

Abstract: The widespread deployment of sensing devices leads to a surge in data for
spatio-temporal forecasting applications such as traffic flow, air quality, and
wind energy. Although spatio-temporal graph neural networks have achieved
success in modeling various static spatio-temporal forecasting scenarios,
real-world spatio-temporal data are typically received in a streaming manner,
and the network continuously expands with the installation of new sensors.
Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges:
the inefficiency of retraining models over newly arrived data and the
detrimental effects of catastrophic forgetting over long-term history. To
address these challenges, we propose a novel prompt tuning-based continuous
forecasting method, following two fundamental tuning principles guided by
empirical and theoretical analysis: expand and compress, which effectively
resolve the aforementioned problems with lightweight tuning parameters.
Specifically, we integrate the base spatio-temporal graph neural network with a
continuous prompt pool, utilizing stored prompts (i.e., few learnable
parameters) in memory, and jointly optimize them with the base spatio-temporal
graph neural network. This method ensures that the model sequentially learns
from the spatio-temporal data stream to accomplish tasks for corresponding
periods. Extensive experimental results on multiple real-world datasets
demonstrate the multi-faceted superiority of our method over the
state-of-the-art baselines, including effectiveness, efficiency, universality,
etc.

</details>


### [537] [$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers](https://arxiv.org/pdf/2406.00153)
*Benjamin Thérien, Charles-Étienne Joseph, Boris Knyazev, Edouard Oyallon, Irina Rish, Eugene Belilovsky*

Main category: cs.LG

TL;DR: Learned optimizers (LOs) with Maximal Update Parametrization ($\mu$P) improve meta-generalization to wider, deeper, and longer training tasks compared to standard parametrization.


<details>
  <summary>Details</summary>
Motivation: LOs reduce training time but struggle with unseen tasks, especially wider networks. This work aims to improve their meta-generalization.

Method: Derived $\mu$P for LOs and proposed a meta-training recipe for $\mu$-parameterized LOs ($\mu$LOs).

Result: $\mu$LOs outperform standard LOs in meta-generalization to wider, deeper (5×), and longer (25×) training tasks.

Conclusion: The $\mu$P and meta-training recipe enhance LOs' generalization, making them more effective for diverse tasks.

Abstract: Learned optimizers (LOs) can significantly reduce the wall-clock training
time of neural networks, substantially reducing training costs. However, they
can struggle to optimize unseen tasks (meta-generalize), especially when
training networks wider than those seen during meta-training. To address this,
we derive the Maximal Update Parametrization ($\mu$P) for two state-of-the-art
learned optimizer architectures and propose a simple meta-training recipe for
$\mu$-parameterized LOs ($\mu$LOs). Our empirical evaluation demonstrates that
LOs meta-trained with our recipe substantially improve meta-generalization to
wider unseen tasks when compared to LOs trained under standard parametrization
(SP), as they are trained in existing work. We also empirically observe that
$\mu$LOs trained with our recipe exhibit unexpectedly improved
meta-generalization to deeper networks ($5\times$ meta-training) and surprising
generalization to much longer training horizons ($25\times$ meta-training) when
compared to SP LOs.

</details>


### [538] [Generating Synthetic Electronic Health Record Data: a Methodological Scoping Review with Benchmarking on Phenotype Data and Open-Source Software](https://arxiv.org/pdf/2411.04281)
*Xingran Chen, Zhenke Wu, Xu Shi, Hyunghoon Cho, Bhramar Mukherjee*

Main category: cs.LG

TL;DR: A scoping review benchmarks synthetic EHR data generation methods, recommending GAN-based approaches for fidelity and utility, and rule-based for privacy. A Python package and decision tree guide method selection.


<details>
  <summary>Details</summary>
Motivation: To evaluate and recommend synthetic EHR data generation methods for practitioners, addressing fidelity, utility, privacy, and computational cost.

Method: Scoping review of 42 studies, benchmarking seven methods on MIMIC-III/IV datasets, evaluating fidelity, utility, privacy, and cost.

Result: GAN-based methods excel in fidelity and utility; rule-based methods in privacy. A Python package and decision tree aid method selection.

Conclusion: Method choice depends on evaluation priorities. Future work should improve fidelity and privacy, and benchmark longitudinal methods.

Abstract: We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, "SynthEHRella", is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.

</details>


### [539] [OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling](https://arxiv.org/pdf/2407.09887)
*Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi Song, Xiaodan Liang, Jing Tang*

Main category: cs.LG

TL;DR: OptiBench is a benchmark for evaluating LLMs' ability to solve complex optimization problems, supplemented by the ReSocratic-29k dataset to improve open-source models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for optimization problems are too simplistic, lacking real-world complexity. OptiBench aims to bridge this gap and enhance LLMs' problem-solving abilities.

Method: Proposes OptiBench with diverse optimization problems and introduces ReSocratic, a data synthesis method to generate the ReSocratic-29k dataset for fine-tuning open-source models.

Result: ReSocratic-29k significantly boosts the performance of open-source LLMs in solving optimization problems.

Conclusion: OptiBench and ReSocratic-29k advance LLMs' capabilities in realistic optimization problem-solving, narrowing the gap between open-source and closed-source models.

Abstract: Large language models (LLMs) have exhibited their problem-solving abilities
in mathematical reasoning. Solving realistic optimization (OPT) problems in
application scenarios requires advanced and applied mathematics ability.
However, current OPT benchmarks that merely solve linear programming are far
from complex realistic situations. In this work, we propose OptiBench, a
benchmark for End-to-end optimization problem-solving with human-readable
inputs and outputs. OptiBench contains rich optimization problems, including
linear and nonlinear programming with or without tabular data, which can
comprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are
required to call a code solver to provide precise numerical answers.
Furthermore, to alleviate the data scarcity for optimization problems, and to
bridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and
closed-source LLMs (e.g., GPT-4), we further propose a data synthesis method
namely ReSocratic. Unlike general data synthesis methods that proceed from
questions to answers, \ReSocratic first incrementally synthesizes formatted
optimization demonstration with mathematical formulations step by step and then
back-translates the generated demonstrations into questions. Based on this, we
synthesize the ReSocratic-29k dataset. We further conduct supervised
fine-tuning with ReSocratic-29k on multiple open-source models. Experimental
results show that ReSocratic-29k significantly improves the performance of
open-source models.

</details>


### [540] [Engagement-Driven Content Generation with Large Language Models](https://arxiv.org/pdf/2411.13187)
*Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco*

Main category: cs.LG

TL;DR: The paper explores LLMs' persuasive power in social networks, proposing a reinforcement learning pipeline to maximize engagement via simulated feedback, demonstrating adaptability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance LLMs' ability to generate engaging content in complex social networks, addressing underexplored dynamics.

Method: A reinforcement learning pipeline with simulated feedback, using a plug-and-play engagement model to efficiently train LLMs.

Result: LLMs effectively generate engaging content under varied conditions, showcasing their potential in social network contexts.

Conclusion: The framework is flexible and scalable, suitable for computational social science tasks, with publicly available experimental code.

Abstract: Large Language Models (LLMs) demonstrate significant persuasive capabilities
in one-on-one interactions, but their influence within social networks, where
interconnected users and complex opinion dynamics pose unique challenges,
remains underexplored. This paper addresses the research question: \emph{Can
LLMs generate meaningful content that maximizes user engagement on social
networks?}
  To answer this, we propose a pipeline using reinforcement learning with
simulated feedback, where the network's response to LLM-generated content
(i.e., the reward) is simulated through a formal engagement model. This
approach bypasses the temporal cost and complexity of live experiments,
enabling an efficient feedback loop between the LLM and the network under
study. It also allows to control over endogenous factors such as the LLM's
position within the social network and the distribution of opinions on a given
topic. Our approach is adaptive to the opinion distribution of the underlying
network and agnostic to the specifics of the engagement model, which is
embedded as a plug-and-play component. Such flexibility makes it suitable for
more complex engagement tasks and interventions in computational social
science.
  Using our framework, we analyze the performance of LLMs in generating social
engagement under different conditions, showcasing their full potential in this
task. The experimental code is publicly available at
https://github.com/mminici/Engagement-Driven-Content-Generation.

</details>


### [541] [Prior Learning in Introspective VAEs](https://arxiv.org/pdf/2408.13805)
*Ioannis Athanasiadis, Fredrik Lindsten, Michael Felsberg*

Main category: cs.LG

TL;DR: The paper investigates Soft-IntroVAE (S-IntroVAE), focusing on incorporating a multimodal, trainable prior to improve generation and representation learning. It introduces theoretical regularizations and validates results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance S-IntroVAE by integrating a trainable prior, addressing training stability and inactive prior modes, while maintaining its state-of-the-art performance.

Method: Incorporates a multimodal prior as a third player, formulates optimal ELBO modifications, and introduces adaptive variance clipping and responsibility regularization.

Result: Demonstrates improved generation and representation learning on 2D density estimation and image datasets (F-MNIST, CIFAR-10).

Conclusion: The proposed prior learning framework and regularizations effectively enhance S-IntroVAE, validated by empirical results.

Abstract: Variational Autoencoders (VAEs) are a popular framework for unsupervised
learning and data generation. A plethora of methods have been proposed focusing
on improving VAEs, with the incorporation of adversarial objectives and the
integration of prior learning mechanisms being prominent directions. When it
comes to the former, an indicative instance is the recently introduced family
of Introspective VAEs aiming at ensuring that a low likelihood is assigned to
unrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE),
one of only two members of the Introspective VAE family, the other being the
original IntroVAE. We select S-IntroVAE for its state-of-the-art status and its
training stability. In particular, we investigate the implication of
incorporating a multimodal and trainable prior into this S-IntroVAE. Namely, we
formulate the prior as a third player and show that when trained in cooperation
with the decoder constitutes an effective way for prior learning, which shares
the Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a
modified formulation of the optimal ELBO in S-IntroVAE, we develop
theoretically motivated regularizations, namely (i) adaptive variance clipping
to stabilize training when learning the prior and (ii) responsibility
regularization to discourage the formation of inactive prior modes. Finally, we
perform a series of targeted experiments on a 2D density estimation benchmark
and in an image generation setting comprised of the (F)-MNIST and CIFAR-10
datasets demonstrating the effect of prior learning in S-IntroVAE in generation
and representation learning.

</details>


### [542] [Exploring Representations and Interventions in Time Series Foundation Models](https://arxiv.org/pdf/2409.12915)
*Michał Wiliński, Mononito Goswami, Willa Potosnak, Nina Żukowska, Artur Dubrawski*

Main category: cs.LG

TL;DR: The paper analyzes the structure and redundancy in time series foundation models (TSFMs), revealing block-like redundancy for efficient pruning and exploring learned concepts like periodicity and trends through latent space steering.


<details>
  <summary>Details</summary>
Motivation: To understand the internal representations and learned concepts of TSFMs, which are not well understood despite their potential for diverse applications.

Method: Investigates self-similarity of model layers across different sizes, analyzes redundancy, and explores latent space steering to manipulate learned concepts.

Result: Block-like redundancy enables efficient pruning, and latent space steering can introduce new features (e.g., periodicity or trends) into signals.

Conclusion: Representational analysis optimizes TSFMs, and conceptual steering enhances controlled and efficient time series analysis.

Abstract: Time series foundation models (TSFMs) promise to be powerful tools for a wide
range of applications. However, their internal representations and learned
concepts are still not well understood. In this study, we investigate the
structure and redundancy of representations across various TSFMs, examining the
self-similarity of model layers within and across different model sizes. This
analysis reveals block-like redundancy in the representations, which can be
utilized for informed pruning to improve inference speed and efficiency.
Additionally, we explore the concepts learned by these models - such as
periodicity and trends - and how these can be manipulated through latent space
steering to influence model behavior. Our experiments show that steering
interventions can introduce new features, e.g., adding periodicity or trends to
signals that initially lacked them. These findings underscore the value of
representational analysis for optimizing models and demonstrate how conceptual
steering offers new possibilities for more controlled and efficient time series
analysis with TSFMs.

</details>


### [543] [CAdam: Confidence-Based Optimization for Online Learning](https://arxiv.org/pdf/2411.19647)
*Shaowen Wang, Anan Liu, Jian Xiao, Huan Liu, Yuekui Yang, Cong Xu, Qianqian Pu, Suncong Zheng, Wei Zhang, Di Wang, Jie Jiang, Jian Li*

Main category: cs.LG

TL;DR: CAdam, a confidence-based optimizer, improves adaptation to distribution shifts and noise in online learning by validating momentum-gradient consistency, outperforming Adam in experiments and real-world recommendation systems.


<details>
  <summary>Details</summary>
Motivation: Address Adam optimizer's limitations in handling volatile online learning data, such as distribution shifts and noise, which slow adaptation and degrade performance.

Method: Introduces CAdam, which checks momentum-gradient consistency before updates, withholding updates if inconsistent to monitor for true distribution shifts.

Result: CAdam outperforms Adam and other optimizers in noisy and shifting data scenarios, boosting performance in live recommendation systems and increasing GMV.

Conclusion: CAdam effectively mitigates Adam's drawbacks in volatile environments, enhancing adaptability and performance in online learning settings.

Abstract: Modern recommendation systems frequently employ online learning to
dynamically update their models with freshly collected data. The most commonly
used optimizer for updating neural networks in these contexts is the Adam
optimizer, which integrates momentum ($m_t$) and adaptive learning rate
($v_t$). However, the volatile nature of online learning data, characterized by
its frequent distribution shifts and presence of noise, poses significant
challenges to Adam's standard optimization process: (1) Adam may use outdated
momentum and the average of squared gradients, resulting in slower adaptation
to distribution changes, and (2) Adam's performance is adversely affected by
data noise. To mitigate these issues, we introduce CAdam, a confidence-based
optimization strategy that assesses the consistency between the momentum and
the gradient for each parameter dimension before deciding on updates. If
momentum and gradient are in sync, CAdam proceeds with parameter updates
according to Adam's original formulation; if not, it temporarily withholds
updates and monitors potential shifts in data distribution in subsequent
iterations. This method allows CAdam to distinguish between the true
distributional shifts and mere noise, and to adapt more quickly to new data
distributions. In various settings with distribution shift or noise, our
experiments demonstrate that CAdam surpasses other well-known optimizers,
including the original Adam. Furthermore, in large-scale A/B testing within a
live recommendation system, CAdam significantly enhances model performance
compared to Adam, leading to substantial increases in the system's gross
merchandise volume (GMV).

</details>


### [544] [Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization](https://arxiv.org/pdf/2409.18850)
*Vladimír Boža, Vladimír Macko*

Main category: cs.LG

TL;DR: DSF factorizes weight matrices into two sparse matrices, achieving state-of-the-art sparsification with efficient ADMM-based heuristics.


<details>
  <summary>Details</summary>
Motivation: Neural networks' large size and complexity make them hard to work with; DSF aims to reduce model size while maintaining performance.

Method: Double Sparse Factorization (DSF) decomposes weight matrices into two sparse matrices using alternating minimization via ADMM.

Result: DSF reduces LLaMA2-13B size by 50% while outperforming dense LLaMA2-7B and competes with Optimal Brain Compression.

Conclusion: DSF enables unprecedented sparsification with persistent accuracy improvements, even post fine-tuning.

Abstract: Neural networks are often challenging to work with due to their large size
and complexity. To address this, various methods aim to reduce model size by
sparsifying or decomposing weight matrices, such as magnitude pruning and
low-rank or block-diagonal factorization. In this work, we present Double
Sparse Factorization (DSF), where we factorize each weight matrix into two
sparse matrices. Although solving this problem exactly is computationally
infeasible, we propose an efficient heuristic based on alternating minimization
via ADMM that achieves state-of-the-art results, enabling unprecedented
sparsification of neural networks. For instance, in a one-shot pruning setting,
our method can reduce the size of the LLaMA2-13B model by 50% while maintaining
better performance than the dense LLaMA2-7B model. We also compare favorably
with Optimal Brain Compression, the state-of-the-art layer-wise pruning
approach for convolutional neural networks. Furthermore, accuracy improvements
of our method persist even after further model fine-tuning.
  Code available at: https://github.com/usamec/double_sparse.

</details>


### [545] [VinePPO: Refining Credit Assignment in RL Training of LLMs](https://arxiv.org/pdf/2410.01679)
*Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux*

Main category: cs.LG

TL;DR: The paper evaluates the shortcomings of value networks in LLM reasoning tasks and proposes VinePPO, a Monte Carlo-based method, which outperforms PPO in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the poor performance of value networks in credit assignment for LLM reasoning tasks and explore if improved credit assignment can enhance RL training.

Method: Proposes VinePPO, leveraging unbiased Monte Carlo estimates for credit assignment, tested on MATH and GSM8K datasets.

Result: VinePPO outperforms PPO and baselines, achieving up to 3.0x faster training and higher test accuracy.

Conclusion: Accurate credit assignment is crucial for RL training of LLMs, and VinePPO demonstrates its effectiveness.

Abstract: Large language models (LLMs) are increasingly applied to complex reasoning
tasks that require executing several complex steps before receiving any reward.
Properly assigning credit to these steps is essential for enhancing model
performance. Proximal Policy Optimization (PPO), a common reinforcement
learning (RL) algorithm used for LLM finetuning, employs value networks to
tackle credit assignment. However, recent approaches achieve strong results
without it, raising questions about the efficacy of value networks in practice.
In this work, we systematically evaluate the efficacy of value networks and
reveal their significant shortcomings in reasoning-heavy LLM tasks, showing
that they often produce poor estimate of expected return and barely outperform
a random baseline when comparing alternative steps. This motivates our key
question: Can improved credit assignment enhance RL training for LLMs? To
address this, we propose VinePPO, a straightforward approach that leverages the
flexibility of language environments to compute unbiased Monte Carlo-based
estimates. Our method consistently outperforms PPO and other baselines across
MATH and GSM8K datasets in less wall-clock time (up to 3.0x). Crucially, it
achieves higher test accuracy for a given training accuracy, capturing more
generalization signal per sample. These results emphasize the importance of
accurate credit assignment in RL training of LLM.

</details>


### [546] [HashAttention: Semantic Sparsity for Faster Inference](https://arxiv.org/pdf/2412.14468)
*Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica*

Main category: cs.LG

TL;DR: HashAttention improves attention efficiency by identifying pivotal tokens via a recommendation problem, reducing token usage by up to 32x with minimal quality loss and significant speedups.


<details>
  <summary>Details</summary>
Motivation: Scaled dot-product attention (SDPA) faces scalability issues due to token sparsity, and existing methods degrade quality or require excessive resources.

Method: HashAttention frames pivotal token identification as a recommendation problem, encoding keys and queries in Hamming space using learned mappings and bitwise operations.

Result: HashAttention reduces token usage by up to 16x (32x with fine-tuning), cuts attention latency by 4.3x in GPT-FAST, and boosts throughput by 3.12x.

Conclusion: HashAttention efficiently addresses SDPA's scalability by leveraging sparsity, offering significant performance gains with minimal quality trade-offs.

Abstract: Leveraging long contexts is crucial for advanced AI systems, but attention
computation poses a scalability challenge. While scaled dot-product attention
(SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly
contribute to output, exploiting this sparsity remains challenging. Existing
methods either suffer from quality degradation or require substantial
additional resources. We show that identifying pivotal tokens is a Maximum
Inner Product Search (MIPS) problem. However, existing MIPS solutions are not
well-suited for SDPA, as they are not GPU-friendly and often underperform due
to the separated query and key distributions. This paper introduces
HashAttention, framing pivotal token identification as a recommendation
problem. Given a query, HashAttention encodes keys and queries in Hamming
space, capturing the required semantic similarity, using learned mapping
functions. HashAttention efficiently identifies pivotal tokens for a given
query using bitwise operations and computes attention using only these tokens,
improving the overall attention efficiency. Trained on generic data,
HashAttention reduces tokens used by up to $16\times$ with minimal quality
loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be
further improved to $32\times$ through task-specific fine-tuning. On A100 GPU,
at $32\times$ sparsity, incorporating HashAttention reduces attention latency
by up to $4.3\times$ in GPT-FAST and $2.54\times$ in FlashDecode, and achieves
up to $3.12\times$ higher throughput for GPT-FAST.

</details>


### [547] [On the Convergence of Single-Timescale Actor-Critic](https://arxiv.org/pdf/2410.08868)
*Navdeep Kumar, Priyank Agrawal, Giorgia Ramponi, Kfir Yehuda Levy, Shie Mannor*

Main category: cs.LG

TL;DR: The paper analyzes the global convergence of the single-timescale actor-critic algorithm for discounted MDPs, achieving an improved sample complexity of $O(\epsilon^{-3})$ for globally optimal policies.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing methods in achieving globally optimal policies for MDPs, the paper introduces a novel analytical framework.

Method: The authors develop an elegant framework to handle coupled recursions in the actor-critic algorithm, using step sizes decaying as $O(k^{-\frac{2}{3}})$.

Result: The algorithm converges to an $\epsilon$-close globally optimal policy with $O(\epsilon^{-3})$ complexity, outperforming prior $O(\epsilon^{-4})$ results.

Conclusion: The work demonstrates the importance of step-size choice and provides a significant improvement in convergence efficiency for actor-critic methods.

Abstract: We analyze the global convergence of the single-timescale actor-critic (AC)
algorithm for the infinite-horizon discounted Markov Decision Processes (MDPs)
with finite state spaces. To this end, we introduce an elegant analytical
framework for handling complex, coupled recursions inherent in the algorithm.
Leveraging this framework, we establish that the algorithm converges to an
$\epsilon$-close \textbf{globally optimal} policy with a sample complexity of
\( O(\epsilon^{-3}) \). This significantly improves upon the existing
complexity of $O(\epsilon^{-2})$ to achieve $\epsilon$-close \textbf{stationary
policy}, which is equivalent to the complexity of $O(\epsilon^{-4})$ to achieve
$\epsilon$-close \textbf{globally optimal} policy using gradient domination
lemma. Furthermore, we demonstrate that to achieve this improvement, the step
sizes for both the actor and critic must decay as \( O(k^{-\frac{2}{3}}) \)
with iteration $k$, diverging from the conventional \( O(k^{-\frac{1}{2}}) \)
rates commonly used in (non)convex optimization.

</details>


### [548] [Model Alignment Search](https://arxiv.org/pdf/2501.06164)
*Satchel Grant*

Main category: cs.LG

TL;DR: The paper introduces a causal intervention method to compare neural systems, addressing limitations of correlative analyses like RSA and CKA, and demonstrates its utility in behavior transfer and alignment tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of correlative methods in comparing neural representations and explore causal relationships between representations and behavior.

Method: Proposes a method using orthogonal transformations to align subspaces for behavior isolation and interchange, complemented by an efficient subspace orthogonalization technique and auxiliary loss for causal alignment.

Result: The method enables behavior transfer between networks, reduces comparison complexity, and can be adapted for causal or stitching-like analyses.

Conclusion: The approach provides a flexible, efficient, and causally informative way to compare neural representations, with applications in model stitching and biological network analysis.

Abstract: When can we say that two neural systems are the same? The answer to this
question is goal-dependent, and it is often addressed through correlative
methods such as Representational Similarity Analysis (RSA) and Centered Kernel
Alignment (CKA). What nuances do we miss, however, when we fail to causally
probe the representations? Do the dangers of cause vs. correlation exist in
comparative representational analyses? In this work, we introduce a method for
connecting neural representational similarity to behavior through causal
interventions. The method learns orthogonal transformations that find an
aligned subspace in which behavioral information from multiple distributed
networks' representations can be isolated and interchanged. We first show that
the method can be used to transfer the behavior from one frozen Neural Network
(NN) to another in a manner similar to model stitching, and we show how the
method can complement correlative similarity measures like RSA. We then
introduce an efficient subspace orthogonalization technique using the
Gram-Schmidt process -- that can also be used for Distributed Alignment Search
(DAS) -- allowing us to perform analyses on larger models. Next, we empirically
and theoretically show how our method can be equivalent to model stitching when
desired, or it can take a form that is more restrictive to causal information,
and in both cases, it reduces the number of required matrices for a comparison
of n models from quadratic to linear in n. We then show how we can augment the
loss objective with an auxiliary loss to train causally relevant alignments
even when we can only read the representations from one of the two networks
during training (like with biological networks). Lastly, we use number
representations as a case study to explore how our method can be used to
compare specific types of representational information across tasks and models.

</details>


### [549] [Deterministic Apple Tasting](https://arxiv.org/pdf/2410.10404)
*Zachary Chase, Idan Mehalel*

Main category: cs.LG

TL;DR: The paper introduces the first widely-applicable deterministic learner for binary online classification with apple tasting feedback, proving its feasibility and tight bounds.


<details>
  <summary>Details</summary>
Motivation: Prior work left it unknown whether deterministic apple tasting is feasible, and this paper addresses this gap by confirming a conjecture and providing deterministic solutions.

Method: The authors develop deterministic algorithms for learning with apple tasting feedback, analyzing both realizable and agnostic cases, and derive mistake bounds.

Result: They show tight mistake bounds for deterministic learning, prove a trichotomy for agnostic cases, and provide optimal bounds for learning from expert advice.

Conclusion: Deterministic apple tasting is feasible, with tight bounds established, and the work resolves open questions about learnability in this model.

Abstract: In binary ($0/1$) online classification with apple tasting feedback, the
learner receives feedback only when predicting $1$. Besides some degenerate
learning tasks, all previously known learning algorithms for this model are
randomized. Consequently, prior to this work it was unknown whether
deterministic apple tasting is generally feasible. In this work, we provide the
first widely-applicable deterministic apple tasting learner, and show that in
the realizable case, a hypothesis class is learnable if and only if it is
deterministically learnable, confirming a conjecture of [Raman, Subedi, Raman,
Tewari-24]. Quantitatively, we show that every class $\mathcal{H}$ is learnable
with mistake bound $O \left(\sqrt{\mathtt{L}(\mathcal{H}) T \log T} \right)$
(where $\mathtt{L}(\mathcal{H})$ is the Littlestone dimension of
$\mathcal{H}$), and that this is tight for some classes.
  We further study the agnostic case, in which the best hypothesis makes at
most $k$ many mistakes, and prove a trichotomy stating that every class
$\mathcal{H}$ must be either easy, hard, or unlearnable. Easy classes have
(both randomized and deterministic) mistake bound $\Theta_{\mathcal{H}}(k)$.
Hard classes have randomized mistake bound $\tilde{\Theta}_{\mathcal{H}}
\left(k + \sqrt{T} \right)$, and deterministic mistake bound
$\tilde{\Theta}_{\mathcal{H}} \left(\sqrt{k \cdot T} \right)$, where $T$ is the
time horizon. Unlearnable classes have (both randomized and deterministic)
mistake bound $\Theta(T)$.
  Our upper bound is based on a deterministic algorithm for learning from
expert advice with apple tasting feedback, a problem interesting in its own
right. For this problem, we show that the optimal deterministic mistake bound
is $\Theta \left(\sqrt{T (k + \log n)} \right)$ for all $k$ and $T \leq n \leq
2^T$, where $n$ is the number of experts.

</details>


### [550] [Subspace Optimization for Large Language Models with Convergence Guarantees](https://arxiv.org/pdf/2410.11289)
*Yutong He, Pengrui Li, Yipeng Hu, Chuyan Chen, Kun Yuan*

Main category: cs.LG

TL;DR: The paper analyzes GaLore's convergence issues, introduces GoLore for guaranteed convergence, and validates results empirically.


<details>
  <summary>Details</summary>
Motivation: To address unclear convergence guarantees of GaLore in stochastic settings and propose a solution.

Method: Theoretical analysis of GaLore's convergence, introduction of GoLore, and empirical validation.

Result: GaLore may not converge optimally; GoLore ensures convergence under standard conditions.

Conclusion: GoLore is a provably convergent alternative to GaLore, with broader implications for subspace optimization.

Abstract: Subspace optimization algorithms, such as GaLore (Zhao et al., 2024), have
gained attention for pre-training and fine-tuning large language models (LLMs)
due to their memory efficiency. However, their convergence guarantees remain
unclear, particularly in stochastic settings. In this paper, we reveal that
GaLore does not always converge to the optimal solution and provide an explicit
counterexample to support this finding. We further explore the conditions under
which GaLore achieves convergence, showing that it does so when either (i) a
sufficiently large mini-batch size is used or (ii) the gradient noise is
isotropic. More significantly, we introduce GoLore (Gradient random Low-rank
projection), a novel variant of GaLore that provably converges in typical
stochastic settings, even with standard batch sizes. Our convergence analysis
extends naturally to other subspace optimization algorithms. Finally, we
empirically validate our theoretical results and thoroughly test the proposed
mechanisms. Codes are available at https://github.com/pkumelon/Golore.

</details>


### [551] [Fast Estimation of Partial Dependence Functions using Trees](https://arxiv.org/pdf/2410.13448)
*Jinyang Liu, Tessa Steensgaard, Marvin N. Wright, Niklas Pfister, Munir Hiabu*

Main category: cs.LG

TL;DR: The paper introduces FastPD, a tree-based estimator for efficiently estimating Partial Dependence (PD) functions, addressing inconsistencies in TreeSHAP and improving computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing methods like SHAP and PD plots have limitations, such as merging main and interaction effects (SHAP) or inconsistency with correlated features (TreeSHAP). FastPD aims to provide consistent and efficient PD function estimation.

Method: The paper proposes FastPD, a new tree-based estimator for arbitrary PD functions, improving computational efficiency from quadratic to linear complexity for moderately deep trees.

Result: FastPD consistently estimates population quantities, unlike TreeSHAP, and efficiently computes PD-based interpretations like SHAP, PD plots, and interaction effects.

Conclusion: FastPD offers a reliable and efficient alternative to existing PD-based interpretation methods, addressing their limitations and expanding interpretability capabilities.

Abstract: Many existing interpretation methods are based on Partial Dependence (PD)
functions that, for a pre-trained machine learning model, capture how a subset
of the features affects the predictions by averaging over the remaining
features.
  Notable methods include Shapley additive explanations (SHAP) which computes
feature contributions based on a game theoretical interpretation and PD plots
(i.e., 1-dim PD functions) that capture average marginal main effects. Recent
work has connected these approaches using a functional decomposition and argues
that SHAP values can be misleading since they merge main and interaction
effects into a single local effect. However, a major advantage of SHAP compared
to other PD-based interpretations has been the availability of fast estimation
techniques, such as \texttt{TreeSHAP}.
  In this paper, we propose a new tree-based estimator, \texttt{FastPD}, which
efficiently estimates arbitrary PD functions.
  We show that \texttt{FastPD} consistently estimates the desired population
quantity -- in contrast to path-dependent \texttt{TreeSHAP} which is
inconsistent when features are correlated.
  For moderately deep trees, \texttt{FastPD} improves the complexity of
existing methods from quadratic to linear in the number of observations.
  By estimating PD functions for arbitrary feature subsets, \texttt{FastPD} can
be used to extract PD-based interpretations such as SHAP, PD plots and
higher-order interaction effects.

</details>


### [552] [ZipNN: Lossless Compression for AI Models](https://arxiv.org/pdf/2411.05239)
*Moshik Hershcovitch, Andrew Wood, Leshem Choshen, Guy Girmonsky, Roy Leibovitz, Ilias Ennmouri, Michal Malka, Peter Chin, Swaminathan Sundararaman, Danny Harnik*

Main category: cs.LG

TL;DR: ZipNN is a lossless compression method for neural networks, achieving significant size reductions (33-50%) and faster speeds compared to vanilla compression, potentially saving substantial network traffic.


<details>
  <summary>Details</summary>
Motivation: Addressing the infrastructure burden of large model sizes by exploring lossless compression as an alternative to traditional model pruning.

Method: Developed ZipNN, a specialized lossless compression technique tailored for neural networks, with variants to enhance effectiveness.

Result: Achieved 33-50% model size reduction, 17% better than vanilla compression, and 62% faster compression/decompression speeds.

Conclusion: ZipNN offers efficient lossless compression for neural networks, promising significant infrastructure savings.

Abstract: With the growth of model sizes and the scale of their deployment, their sheer
size burdens the infrastructure requiring more network and more storage to
accommodate these. While there is a vast model compression literature deleting
parts of the model weights for faster inference, we investigate a more
traditional type of compression - one that represents the model in a compact
form and is coupled with a decompression algorithm that returns it to its
original form and size - namely lossless compression.
  We present ZipNN a lossless compression tailored to neural networks. Somewhat
surprisingly, we show that specific lossless compression can gain significant
network and storage reduction on popular models, often saving 33% and at times
reducing over 50% of the model size. We investigate the source of model
compressibility and introduce specialized compression variants tailored for
models that further increase the effectiveness of compression. On popular
models (e.g. Llama 3) ZipNN shows space savings that are over 17% better than
vanilla compression while also improving compression and decompression speeds
by 62%. We estimate that these methods could save over an ExaByte per month of
network traffic downloaded from a large model hub like Hugging Face.

</details>


### [553] [Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions](https://arxiv.org/pdf/2502.13135)
*Taedong Yun, Eric Yang, Mustafa Safdari, Jong Ha Lee, Vaishnavi Vinod Kumar, S. Sara Mahdavi, Jonathan Amar, Derek Peyton, Reut Aharony, Andreas Michaelides, Logan Schneider, Isaac Galatzer-Levy, Yugang Jia, John Canny, Arthur Gretton, Maja Matarić*

Main category: cs.LG

TL;DR: An end-to-end framework for generating synthetic users to evaluate interactive health coaching agents, grounded in real-world health conditions like sleep and diabetes management.


<details>
  <summary>Details</summary>
Motivation: To create realistic synthetic users for evaluating interactive agents that encourage positive behavior changes in health and lifestyle coaching.

Method: Two-stage synthetic user creation: structured data generation based on health factors, followed by profile development. Interactions simulated using generative models or language models.

Result: Validated framework shows synthetic users accurately portray real human users, outperforming generic synthetic users in realism.

Conclusion: The framework enables efficient development of conversational agents through realistic simulated interactions.

Abstract: We present an end-to-end framework for generating synthetic users for
evaluating interactive agents designed to encourage positive behavior changes,
such as in health and lifestyle coaching. The synthetic users are grounded in
health and lifestyle conditions, specifically sleep and diabetes management in
this study, to ensure realistic interactions with the health coaching agent.
Synthetic users are created in two stages: first, structured data are generated
grounded in real-world health and lifestyle factors in addition to basic
demographics and behavioral attributes; second, full profiles of the synthetic
users are developed conditioned on the structured data. Interactions between
synthetic users and the coaching agent are simulated using generative
agent-based models such as Concordia, or directly by prompting a language
model. Using two independently-developed agents for sleep and diabetes coaching
as case studies, the validity of this framework is demonstrated by analyzing
the coaching agent's understanding of the synthetic users' needs and
challenges. Finally, through multiple blinded evaluations of user-coach
interactions by human experts, we demonstrate that our synthetic users with
health and behavioral attributes more accurately portray real human users with
the same attributes, compared to generic synthetic users not grounded in such
attributes. The proposed framework lays the foundation for efficient
development of conversational agents through extensive, realistic, and grounded
simulated interactions.

</details>


### [554] [KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation](https://arxiv.org/pdf/2411.17089)
*Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, Murali Annavaram*

Main category: cs.LG

TL;DR: KVPR is an efficient I/O-aware method for LLM inference that overlaps GPU recomputation with KV cache transfer to reduce latency and improve throughput.


<details>
  <summary>Details</summary>
Motivation: The memory demands of KV cache in LLMs exceed GPU capacity, and offloading to CPU introduces PCIe bandwidth bottlenecks. Existing methods struggle with data movement and CPU dependency.

Method: KVPR transfers partial KV cache activations to the GPU for recomputation while concurrently transferring the remaining cache, overlapping GPU work with I/O. It includes a profiler, scheduler, and runtime for automation.

Result: KVPR reduces latency by up to 35.8% and increases throughput by 46.2% compared to state-of-the-art methods.

Conclusion: KVPR effectively addresses GPU memory and PCIe bandwidth limitations, enhancing LLM inference performance.

Abstract: Inference for Large Language Models (LLMs) is computationally demanding. To
reduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to
store intermediate activations, which significantly lowers the computational
overhead for token generation. However, the memory required for the KV cache
grows rapidly, often exceeding the capacity of GPU memory. A cost-effective
alternative is to offload KV cache to CPU memory, which alleviates GPU memory
pressure, but shifts the bottleneck to the limited bandwidth of the PCIe
connection between the CPU and GPU. Existing methods attempt to address these
issues by overlapping GPU computation with I/O or employing CPU-GPU
heterogeneous execution, but they are hindered by excessive data movement and
dependence on CPU capabilities. Fully overlapping PCIe communication latency
gets challenging as the size of the KV cache grows and/or the GPU compute
capabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware
LLM inference method where the CPU first transfers a partial set of
activations, from which the GPU can start recomputing the KV cache values.
While the GPU recomputes the partial KV cache, the remaining portion of the KV
cache is transferred concurrently from the CPU. This approach overlaps GPU
recomputation with KV cache transfer to minimize idle GPU time and maximize
inference performance. KVPR is fully automated by integrating a profiler module
that utilizes input characteristics and system hardware information, a
scheduler module to optimize the distribution of computation and communication
workloads, and a runtime module to efficiently execute the derived execution
plan. Experimental results show that KVPR achieves up to 35.8% lower latency
and 46.2% higher throughput during decoding compared to state-of-the-art
approaches. The code is available at https://github.com/chaoyij/KVPR.

</details>


### [555] [Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation](https://arxiv.org/pdf/2412.07169)
*Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey*

Main category: cs.LG

TL;DR: Rate-In dynamically adjusts dropout rates during inference using information-theoretic principles, improving uncertainty estimation without labeled data.


<details>
  <summary>Details</summary>
Motivation: Static dropout rates in Monte Carlo Dropout lead to suboptimal uncertainty estimates, especially in risk-sensitive applications like medical diagnosis.

Method: Rate-In quantifies information loss in feature maps to adaptively tune dropout rates per layer and input, without needing ground truth labels.

Result: Rate-In enhances calibration and sharpens uncertainty estimates without compromising predictive performance, as shown in synthetic and real-world medical imaging tasks.

Conclusion: Rate-In provides a practical, unsupervised method for optimizing dropout rates during inference, improving reliability in critical applications.

Abstract: Accurate uncertainty estimation is crucial for deploying neural networks in
risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a
widely used technique for approximating predictive uncertainty by performing
stochastic forward passes with dropout during inference. However, using static
dropout rates across all layers and inputs can lead to suboptimal uncertainty
estimates, as it fails to adapt to the varying characteristics of individual
inputs and network layers. Existing approaches optimize dropout rates during
training using labeled data, resulting in fixed inference-time parameters that
cannot adjust to new data distributions, compromising uncertainty estimates in
Monte Carlo simulations.
  In this paper, we propose Rate-In, an algorithm that dynamically adjusts
dropout rates during inference by quantifying the information loss induced by
dropout in each layer's feature maps. By treating dropout as controlled noise
injection and leveraging information-theoretic principles, Rate-In adapts
dropout rates per layer and per input instance without requiring ground truth
labels. By quantifying the functional information loss in feature maps, we
adaptively tune dropout rates to maintain perceptual quality across diverse
medical imaging tasks and architectural configurations. Our extensive empirical
study on synthetic data and real-world medical imaging tasks demonstrates that
Rate-In improves calibration and sharpens uncertainty estimates compared to
fixed or heuristic dropout rates without compromising predictive performance.
Rate-In offers a practical, unsupervised, inference-time approach to optimizing
dropout for more reliable predictive uncertainty estimation in critical
applications.

</details>


### [556] [FragFM: Hierarchical Framework for Efficient Molecule Generation via Fragment-Level Discrete Flow Matching](https://arxiv.org/pdf/2502.15805)
*Joongwon Lee, Seonghwan Kim, Seokhyun Moon, Hyunwoo Kim, Woo Youn Kim*

Main category: cs.LG

TL;DR: FragFM is a hierarchical framework for molecular graph generation using fragment-level discrete flow matching, achieving efficient and scalable generation with better property control.


<details>
  <summary>Details</summary>
Motivation: To improve molecular generation efficiency and property control by leveraging fragment-level modeling and addressing challenges in fragment space handling.

Method: Uses a coarse-to-fine autoencoder for atom-level reconstruction and a stochastic fragment bag strategy for fragment space management.

Result: Outperforms atom-based methods in property control and flexibility, validated on the NPGen benchmark for natural product-like molecules.

Conclusion: FragFM demonstrates the potential of fragment-based generative modeling for scalable, property-aware molecular design, advancing chemical space exploration.

Abstract: We introduce FragFM, a novel hierarchical framework via fragment-level
discrete flow matching for efficient molecular graph generation. FragFM
generates molecules at the fragment level, leveraging a coarse-to-fine
autoencoder to reconstruct details at the atom level. Together with a
stochastic fragment bag strategy to effectively handle an extensive fragment
space, our framework enables more efficient and scalable molecular generation.
We demonstrate that our fragment-based approach achieves better property
control than the atom-based method and additional flexibility through
conditioning the fragment bag. We also propose a Natural Product Generation
benchmark (NPGen) to evaluate modern molecular graph generative models' ability
to generate natural product-like molecules. Since natural products are
biologically prevalidated and differ from typical drug-like molecules, our
benchmark provides a more challenging yet meaningful evaluation relevant to
drug discovery. We conduct a FragFM comparative study against various models on
diverse molecular generation benchmarks, including NPGen, demonstrating
superior performance. The results highlight the potential of fragment-based
generative modeling for large-scale, property-aware molecular design, paving
the way for more efficient exploration of chemical space.

</details>


### [557] [Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency](https://arxiv.org/pdf/2412.07326)
*Yael Itzhakev, Amit Giloni, Yuval Elovici, Asaf Shabtai*

Main category: cs.LG

TL;DR: The paper addresses adversarial attacks on tabular data, proposing a method to perturb dependent features coherently and introducing Class-Specific Anomaly Detection (CSAD) for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack metrics fail to account for the complex interdependencies in tabular data, necessitating improved techniques and evaluation methods.

Method: Proposes perturbing dependent features while preserving coherence and introduces CSAD for anomaly detection, integrating SHAP for explainability.

Result: Evaluates attacks on benchmark datasets, revealing differences in risk, effort, and quality, providing insights for attackers and defenders.

Conclusion: The work advances adversarial attack and defense research in the tabular domain, highlighting trade-offs and future directions.

Abstract: Machine learning models trained on tabular data are vulnerable to adversarial
attacks, even in realistic scenarios where attackers only have access to the
model's outputs. Since tabular data contains complex interdependencies among
features, it presents a unique challenge for adversarial samples which must
maintain coherence and respect these interdependencies to remain
indistinguishable from benign data. Moreover, existing attack evaluation
metrics-such as the success rate, perturbation magnitude, and query count-fail
to account for this challenge. To address those gaps, we propose a technique
for perturbing dependent features while preserving sample coherence. In
addition, we introduce Class-Specific Anomaly Detection (CSAD), an effective
novel anomaly detection approach, along with concrete metrics for assessing the
quality of tabular adversarial attacks. CSAD evaluates adversarial samples
relative to their predicted class distribution, rather than a broad benign
distribution. It ensures that subtle adversarial perturbations, which may
appear coherent in other classes, are correctly identified as anomalies. We
integrate SHAP explainability techniques to detect inconsistencies in model
decision-making, extending CSAD for SHAP-based anomaly detection. Our
evaluation incorporates both anomaly detection rates with SHAP-based
assessments to provide a more comprehensive measure of adversarial sample
quality. We evaluate various attack strategies, examining black-box query-based
and transferability-based gradient attacks across four target models.
Experiments on benchmark tabular datasets reveal key differences in the
attacker's risk and effort and attack quality, offering insights into the
strengths, limitations, and trade-offs faced by attackers and defenders. Our
findings lay the groundwork for future research on adversarial attacks and
defense development in the tabular domain.

</details>


### [558] [The Built-In Robustness of Decentralized Federated Averaging to Bad Data](https://arxiv.org/pdf/2502.18097)
*Samuele Sabella, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti*

Main category: cs.LG

TL;DR: Decentralized federated learning (DFL) shows robustness to low-quality or corrupted data, even when concentrated in influential nodes, due to the averaging process.


<details>
  <summary>Details</summary>
Motivation: To understand the vulnerability of DFL to poor-quality data and explore its impact on model training in decentralized settings.

Method: Simulated two scenarios of degraded data quality (evenly distributed vs. concentrated on one node) using decentralized FedAvg.

Result: DFL is robust to localized bad data, with enhanced robustness when corruption is concentrated on a single node.

Conclusion: Averaging in DFL mitigates disproportionate influence from any single node, ensuring system resilience.

Abstract: Decentralized federated learning (DFL) enables devices to collaboratively
train models over complex network topologies without relying on a central
controller. In this setting, local data remains private, but its quality and
quantity can vary significantly across nodes. The extent to which a fully
decentralized system is vulnerable to poor-quality or corrupted data remains
unclear, but several factors could contribute to potential risks. Without a
central authority, there can be no unified mechanism to detect or correct
errors, and each node operates with a localized view of the data distribution,
making it difficult for the node to assess whether its perspective aligns with
the true distribution. Moreover, models trained on low-quality data can
propagate through the network, amplifying errors. To explore the impact of
low-quality data on DFL, we simulate two scenarios with degraded data quality
-- one where the corrupted data is evenly distributed in a subset of nodes and
one where it is concentrated on a single node -- using a decentralized
implementation of FedAvg. Our results reveal that averaging-based decentralized
learning is remarkably robust to localized bad data, even when the corrupted
data resides in the most influential nodes of the network. Counterintuitively,
this robustness is further enhanced when the corrupted data is concentrated on
a single node, regardless of its centrality in the communication network
topology. This phenomenon is explained by the averaging process, which ensures
that no single node -- however central -- can disproportionately influence the
overall learning process.

</details>


### [559] [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/pdf/2501.02423)
*Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuai Li, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang*

Main category: cs.LG

TL;DR: The paper explores FP quantization in LLM training, identifying optimal exponent-mantissa bit ratios, critical data size effects, and precision-computational power relationships.


<details>
  <summary>Details</summary>
Motivation: Previous scaling laws for precision focused on integer quantization, neglecting FP quantization's impact on LLM losses, despite its common use in production.

Method: The study examines FP quantization targets, exponent/mantissa bits, and scaling factor granularity in LLM training.

Result: Key findings include: (1) exponent bits slightly outperform mantissa bits, with optimal ratios provided; (2) critical data size limits training effectiveness; (3) optimal precision scales with computational power (4-8 bits).

Conclusion: The paper offers a unified FP quantization scaling law and practical guidelines for hardware and training optimization.

Abstract: Low-precision training is considered an effective strategy for reducing both
training and downstream inference costs. Previous scaling laws for precision
mainly focus on integer quantization, which pay less attention to the
constituents in floating-point (FP) quantization, and thus cannot well fit the
LLM losses in this scenario. In contrast, while FP quantization training is
more commonly implemented in production, it's research has been relatively
superficial. In this paper, we thoroughly explore the effects of FP
quantization targets, exponent bits, mantissa bits, and the calculation
granularity of the scaling factor in FP quantization training performance of
LLM models. In addition to an accurate FP quantization unified scaling law, we
also provide valuable suggestions for the community: (1) Exponent bits
contribute slightly more to the model performance than mantissa bits. We
provide the optimal exponent-mantissa bit ratio for different bit numbers,
which is available for future reference by hardware manufacturers; (2) We
discover the formation of the critical data size in low-precision LLM training.
Too much training data exceeding the critical data size will inversely bring in
degradation of LLM performance; (3) The optimal FP quantization precision is
directly proportional to the computational power, but within a wide
computational power range. We estimate that the best cost-performance precision
should lie between 4-8 bits.

</details>


### [560] [Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency](https://arxiv.org/pdf/2502.19307)
*Michael Somma, Thomas Gallien, Branka Stojanovic*

Main category: cs.LG

TL;DR: A physics-inspired anomaly detection method for dynamical systems, using embedding theory and a Temporal Differential Consistency Autoencoder (TDC-AE), outperforms LSTMs and Transformers with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is crucial for reliability and safety in industrial and cyber-physical systems, especially those with oscillatory behaviors and bounded motion.

Method: Proposes a system-theoretic approach using embedding theory and state-derivative pairs, with a TDC-AE incorporating a TDC-Loss for temporal coherence.

Result: TDC-AE outperforms LSTMs and Transformers on the C-MAPSS dataset, with a 200x reduction in MAC operations, suitable for edge computing.

Conclusion: Anomalies disrupt stable dynamics, and the proposed method provides a robust, lightweight solution for anomaly detection.

Abstract: Anomaly detection in complex dynamical systems is essential for ensuring
reliability, safety, and efficiency in industrial and cyber-physical
infrastructures. Predictive maintenance helps prevent costly failures, while
cybersecurity monitoring has become critical as digitized systems face growing
threats. Many of these systems exhibit oscillatory behaviors and bounded
motion, requiring anomaly detection methods that capture structured temporal
dependencies while adhering to physical consistency principles. In this work,
we propose a system-theoretic approach to anomaly detection, grounded in
classical embedding theory and physics-inspired consistency principles. We
build upon the Fractal Whitney Embedding Prevalence Theorem that extends
traditional embedding techniques to complex system dynamics. Additionally, we
introduce state-derivative pairs as an embedding strategy to capture system
evolution. To enforce temporal coherence, we develop a Temporal Differential
Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the
approximated derivatives of latent variables with their dynamic
representations. We evaluate our method on the C-MAPSS dataset, a benchmark for
turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers
while achieving a nearly 200x reduction in MAC operations, making it
particularly suited for lightweight edge computing. Our findings support the
hypothesis that anomalies disrupt stable system dynamics, providing a robust
signal for anomaly detection.

</details>


### [561] [Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML](https://arxiv.org/pdf/2501.09621)
*Tehila Dahan, Kfir Y. Levy*

Main category: cs.LG

TL;DR: A novel weighted robust aggregation framework is proposed to address Byzantine-robust training in asynchronous distributed ML systems, achieving optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency and integrity in asynchronous distributed ML systems amid Byzantine failures and heterogeneous computing resources.

Method: Adapts the Byzantine framework to asynchronous dynamics using weighted robust aggregation and variance-reduction techniques.

Result: Achieves optimal convergence rate and improves fault tolerance in asynchronous Byzantine environments.

Conclusion: The framework effectively mitigates delays and Byzantine faults, optimizing performance in asynchronous ML systems.

Abstract: We address the challenges of Byzantine-robust training in asynchronous
distributed machine learning systems, aiming to enhance efficiency amid massive
parallelization and heterogeneous computing resources. Asynchronous systems,
marked by independently operating workers and intermittent updates, uniquely
struggle with maintaining integrity against Byzantine failures, which encompass
malicious or erroneous actions that disrupt learning. The inherent delays in
such settings not only introduce additional bias to the system but also obscure
the disruptions caused by Byzantine faults. To tackle these issues, we adapt
the Byzantine framework to asynchronous dynamics by introducing a novel
weighted robust aggregation framework. This allows for the extension of robust
aggregators and a recent meta-aggregator to their weighted versions, mitigating
the effects of delayed updates. By further incorporating a recent
variance-reduction technique, we achieve an optimal convergence rate for the
first time in an asynchronous Byzantine environment. Our methodology is
rigorously validated through empirical and theoretical analysis, demonstrating
its effectiveness in enhancing fault tolerance and optimizing performance in
asynchronous ML systems.

</details>


### [562] [Generative Unordered Flow for Set-Structured Data Generation](https://arxiv.org/pdf/2501.17770)
*Yangming Li, Chaoyu Liu, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: A flow-based generative model for unordered data (e.g., point sets) is introduced, using function representations and flow matching, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based models are designed for ordered data, leaving unordered data (e.g., point sets) underexplored. This work addresses this gap.

Method: Unordered data is converted into function representations, with probability measures learned via function-valued flow matching. Inverse mapping uses Langevin dynamics and gradient-based search.

Result: The model effectively generates set-structured data, significantly outperforming baselines in experiments.

Conclusion: Unordered flow is a promising approach for generating unordered data, filling a gap in flow-based generative models.

Abstract: Flow-based generative models have demonstrated promising performance across a
broad spectrum of data modalities (e.g., image and text). However, there are
few works exploring their extension to unordered data (e.g., spatial point
set), which is not trivial because previous models are mostly designed for
vector data that are naturally ordered. In this paper, we present unordered
flow, a type of flow-based generative model for set-structured data generation.
Specifically, we convert unordered data into an appropriate function
representation, and learn the probability measure of such representations
through function-valued flow matching. For the inverse map from a function
representation to unordered data, we propose a method similar to particle
filtering, with Langevin dynamics to first warm-up the initial particles and
gradient-based search to update them until convergence. We have conducted
extensive experiments on multiple real-world datasets, showing that our
unordered flow model is very effective in generating set-structured data and
significantly outperforms previous baselines.

</details>


### [563] [Knowledge Retention for Continual Model-Based Reinforcement Learning](https://arxiv.org/pdf/2503.04256)
*Yixiang Sun, Haotian Fu, Michael Littman, George Konidaris*

Main category: cs.LG

TL;DR: DRAGO improves continual model-based reinforcement learning by using synthetic experiences and intrinsic rewards to maintain knowledge across tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incremental world model development in tasks with varying rewards but shared state space and dynamics.

Method: Combines Synthetic Experience Rehearsal (generative models for synthetic experiences) and Regaining Memories Through Exploration (intrinsic rewards for revisiting past states).

Result: Empirical results show DRAGO preserves knowledge and outperforms in continual learning scenarios.

Conclusion: DRAGO effectively maintains and develops world models, enhancing learning and adaptation across diverse environments.

Abstract: We propose DRAGO, a novel approach for continual model-based reinforcement
learning aimed at improving the incremental development of world models across
a sequence of tasks that differ in their reward functions but not the state
space or dynamics. DRAGO comprises two key components: Synthetic Experience
Rehearsal, which leverages generative models to create synthetic experiences
from past tasks, allowing the agent to reinforce previously learned dynamics
without storing data, and Regaining Memories Through Exploration, which
introduces an intrinsic reward mechanism to guide the agent toward revisiting
relevant states from prior tasks. Together, these components enable the agent
to maintain a comprehensive and continually developing world model,
facilitating more effective learning and adaptation across diverse
environments. Empirical evaluations demonstrate that DRAGO is able to preserve
knowledge across tasks, achieving superior performance in various continual
learning scenarios.

</details>


### [564] [Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective](https://arxiv.org/pdf/2501.18282)
*Yunzhen Yao, Lie He, Michael Gastpar*

Main category: cs.LG

TL;DR: The paper improves sample efficiency in preference learning by leveraging sparsity, reducing the minimax optimal error rate from Θ(d/n) to Θ(k/n log(d/k)) for k-sparse models.


<details>
  <summary>Details</summary>
Motivation: High dimensionality and costly human-annotated data challenge traditional preference learning methods, prompting the need for more efficient approaches.

Method: The authors use sparsity in the preference model and analyze the ℓ₁-regularized estimator under mild assumptions on the Gram matrix.

Result: Theoretical and experimental results show reduced sample complexity and improved accuracy with sparsity-aware methods.

Conclusion: Sparsity-aware methods are effective for efficient preference learning, validated by synthetic and LLM alignment data.

Abstract: This paper considers the sample-efficiency of preference learning, which
models and predicts human choices based on comparative judgments. The minimax
optimal estimation error rate $\Theta(d/n)$ in classical estimation theory
requires that the number of samples $n$ scales linearly with the dimensionality
of the feature space $d$. However, the high dimensionality of the feature space
and the high cost of collecting human-annotated data challenge the efficiency
of traditional estimation methods. To remedy this, we leverage sparsity in the
preference model and establish sharp error rates. We show that under the sparse
random utility model, where the parameter of the reward function is $k$-sparse,
the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$.
Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it
achieves near-optimal rate under mild assumptions on the Gram matrix.
Experiments on synthetic data and LLM alignment data validate our theoretical
findings, showing that sparsity-aware methods significantly reduce sample
complexity and improve prediction accuracy.

</details>


### [565] [Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?](https://arxiv.org/pdf/2501.18527)
*Konrad Mundinger, Max Zimmer, Aldo Kiem, Christoph Spiegel, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Neural networks improve the Hadwiger-Nelson problem, discovering new six-colorings after 30 years.


<details>
  <summary>Details</summary>
Motivation: To solve the long-standing Hadwiger-Nelson problem in discrete geometry using neural networks.

Method: Reformulated the problem as an optimization task with a probabilistic, differentiable loss function using neural networks.

Result: Discovered two novel six-colorings, the first improvement in 30 years.

Conclusion: Demonstrates neural networks' potential in mathematical discovery, with broader applicability.

Abstract: We demonstrate how neural networks can drive mathematical discovery through a
case study of the Hadwiger-Nelson problem, a long-standing open problem at the
intersection of discrete geometry and extremal combinatorics that is concerned
with coloring the plane while avoiding monochromatic unit-distance pairs. Using
neural networks as approximators, we reformulate this mixed discrete-continuous
geometric coloring problem with hard constraints as an optimization task with a
probabilistic, differentiable loss function. This enables gradient-based
exploration of admissible configurations that most significantly led to the
discovery of two novel six-colorings, providing the first improvement in thirty
years to the off-diagonal variant of the original problem. Here, we establish
the underlying machine learning approach used to obtain these results and
demonstrate its broader applicability through additional numerical insights.

</details>


### [566] [Language-Enhanced Representation Learning for Single-Cell Transcriptomics](https://arxiv.org/pdf/2503.09427)
*Yaorui Shi, Jiaqi Yang, Changhao Nai, Sihang Li, Junfeng Fang, Xiang Wang, Zhiyuan Liu, Yang Zhang*

Main category: cs.LG

TL;DR: scMMGPT is a multimodal framework for single-cell transcriptomics, combining transcriptomic data with textual knowledge to outperform existing methods in tasks like cell annotation and clustering.


<details>
  <summary>Details</summary>
Motivation: Existing scLLMs focus only on transcriptomic data, ignoring valuable textual biological knowledge, limiting their effectiveness.

Method: scMMGPT uses robust cell representation extraction and a two-stage pre-training strategy to integrate transcriptomic and textual data.

Result: scMMGPT outperforms unimodal and multimodal baselines in downstream tasks and shows better generalization in out-of-distribution scenarios.

Conclusion: scMMGPT's multimodal approach enhances representation learning in single-cell transcriptomics, offering superior performance and flexibility.

Abstract: Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular
heterogeneity. Recent advancements leverage single-cell large language models
(scLLMs) for effective representation learning. These models focus exclusively
on transcriptomic data, neglecting complementary biological knowledge from
textual descriptions. To overcome this limitation, we propose scMMGPT, a novel
multimodal framework designed for language-enhanced representation learning in
single-cell transcriptomics. Unlike existing methods, scMMGPT employs robust
cell representation extraction, preserving quantitative gene expression data,
and introduces an innovative two-stage pre-training strategy combining
discriminative precision with generative flexibility. Extensive experiments
demonstrate that scMMGPT significantly outperforms unimodal and multimodal
baselines across key downstream tasks, including cell annotation and
clustering, and exhibits superior generalization in out-of-distribution
scenarios.

</details>


### [567] [A theoretical framework for overfitting in energy-based modeling](https://arxiv.org/pdf/2501.19158)
*Giovanni Catania, Aurélien Decelle, Cyril Furtlehner, Beatriz Seoane*

Main category: cs.LG

TL;DR: The paper explores how limited data affects training energy-based models for inverse problems, revealing insights into learning timescales, early stopping, and overfitting control.


<details>
  <summary>Details</summary>
Motivation: To understand the challenges of training pairwise energy-based models with limited data and improve their performance in identifying interaction networks.

Method: Analyzes training trajectories using the Gaussian model, studies eigenmode evolution, and applies random matrix theory for finite data corrections. Extends to binary-variable models and proposes neural tangent kernel dynamics.

Result: Optimal early stopping points are identified, finite data corrections are modeled accurately, and strategies to control overfitting in discrete-variable models are provided.

Conclusion: The framework offers practical strategies for managing overfitting and extends to broader energy-based models, enhancing their applicability.

Abstract: We investigate the impact of limited data on training pairwise energy-based
models for inverse problems aimed at identifying interaction networks.
Utilizing the Gaussian model as testbed, we dissect training trajectories
across the eigenbasis of the coupling matrix, exploiting the independent
evolution of eigenmodes and revealing that the learning timescales are tied to
the spectral decomposition of the empirical covariance matrix. We see that
optimal points for early stopping arise from the interplay between these
timescales and the initial conditions of training. Moreover, we show that
finite data corrections can be accurately modeled through asymptotic random
matrix theory calculations and provide the counterpart of generalized
cross-validation in the energy based model context. Our analytical framework
extends to binary-variable maximum-entropy pairwise models with minimal
variations. These findings offer strategies to control overfitting in
discrete-variable models through empirical shrinkage corrections, improving the
management of overfitting in energy-based generative models. Finally, we
propose a generalization to arbitrary energy-based models by deriving the
neural tangent kernel dynamics of the score function under the score-matching
algorithm.

</details>


### [568] [Detecting Dataset Bias in Medical AI: A Generalized and Modality-Agnostic Auditing Framework](https://arxiv.org/pdf/2503.09969)
*Nathan Drenkow, Mitchell Pavlak, Keith Harrigian, Ayah Zirikly, Adarsh Subbaswamy, Mohammad Mehdi Farhangi, Nicholas Petrick, Mathias Unberath*

Main category: cs.LG

TL;DR: G-AUDIT is a modality-agnostic framework for auditing datasets to detect and quantify biases in AI training and testing data, demonstrated across medical imaging, EHR, and ICU data.


<details>
  <summary>Details</summary>
Motivation: AI in healthcare often suffers from biases due to non-representative datasets, leading to unreliable outcomes. G-AUDIT aims to identify and mitigate these biases.

Method: G-AUDIT analyzes relationships between task-level labels and data attributes (e.g., patient demographics, acquisition details) to quantify bias risks for shortcut learning or spurious associations.

Result: G-AUDIT identified subtle biases in skin lesion classification, EHR language classification, and ICU mortality prediction, outperforming traditional qualitative methods.

Conclusion: G-AUDIT is a practical tool for exposing dataset-level biases, supporting the development of more reliable AI systems in healthcare.

Abstract: Artificial Intelligence (AI) is now firmly at the center of evidence-based
medicine. Despite many success stories that edge the path of AI's rise in
healthcare, there are comparably many reports of significant shortcomings and
unexpected behavior of AI in deployment. A major reason for these limitations
is AI's reliance on association-based learning, where non-representative
machine learning datasets can amplify latent bias during training and/or hide
it during testing. To unlock new tools capable of foreseeing and preventing
such AI bias issues, we present G-AUDIT. Generalized Attribute Utility and
Detectability-Induced bias Testing (G-AUDIT) for datasets is a
modality-agnostic dataset auditing framework that allows for generating
targeted hypotheses about sources of bias in training or testing data. Our
method examines the relationship between task-level annotations (commonly
referred to as ``labels'') and data properties including patient attributes
(e.g., age, sex) and environment/acquisition characteristics (e.g., clinical
site, imaging protocols). G-AUDIT quantifies the extent to which the observed
data attributes pose a risk for shortcut learning, or in the case of testing
data, might hide predictions made based on spurious associations. We
demonstrate the broad applicability of our method by analyzing large-scale
medical datasets for three distinct modalities and machine learning tasks: skin
lesion classification in images, stigmatizing language classification in
Electronic Health Records (EHR), and mortality prediction for ICU tabular data.
In each setting, G-AUDIT successfully identifies subtle biases commonly
overlooked by traditional qualitative methods, underscoring its practical value
in exposing dataset-level risks and supporting the downstream development of
reliable AI systems.

</details>


### [569] [OneForecast: A Universal Framework for Global and Regional Weather Forecasting](https://arxiv.org/pdf/2502.00338)
*Yuan Gao, Hao Wu, Ruiqi Shu, Huanshuo Dong, Fan Xu, Rui Ray Chen, Yibo Yan, Qingsong Wen, Xuming Hu, Kun Wang, Jiahao Wu, Qing Li, Hui Xiong, Xiaomeng Huang*

Main category: cs.LG

TL;DR: OneForecast is a global-regional nested weather forecasting framework using graph neural networks to improve accuracy, especially for extreme events, by combining dynamic system modeling and multi-grid theory.


<details>
  <summary>Details</summary>
Motivation: Traditional NWP methods are computationally expensive and underutilize historical data, while deep learning models struggle with global-regional balance, extreme event smoothing, and dynamic system modeling.

Method: Proposes a graph neural network-based framework with multi-scale graph structures, adaptive messaging, and neural nested grids for high-resolution regional forecasts.

Result: OneForecast excels in global to regional scales and short-term to long-term forecasts, particularly in extreme event predictions.

Conclusion: The framework effectively addresses key challenges in weather forecasting, offering improved accuracy and performance.

Abstract: Accurate weather forecasts are important for disaster prevention,
agricultural planning, etc. Traditional numerical weather prediction (NWP)
methods offer physically interpretable high-accuracy predictions but are
computationally expensive and fail to fully leverage rapidly growing historical
data. In recent years, deep learning models have made significant progress in
weather forecasting, but challenges remain, such as balancing global and
regional high-resolution forecasts, excessive smoothing in extreme event
predictions, and insufficient dynamic system modeling. To address these issues,
this paper proposes a global-regional nested weather forecasting framework
(OneForecast) based on graph neural networks. By combining a dynamic system
perspective with multi-grid theory, we construct a multi-scale graph structure
and densify the target region to capture local high-frequency features. We
introduce an adaptive messaging mechanism, using dynamic gating units to deeply
integrate node and edge features for more accurate extreme event forecasting.
For high-resolution regional forecasts, we propose a neural nested grid method
to mitigate boundary information loss. Experimental results show that
OneForecast performs excellently across global to regional scales and
short-term to long-term forecasts, especially in extreme event predictions.
Codes link https://github.com/YuanGao-YG/OneForecast.

</details>


### [570] [Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models](https://arxiv.org/pdf/2502.01203)
*Gholamali Aminian, Amir R. Asadi, Idan Shenfeld, Youssef Mroueh*

Main category: cs.LG

TL;DR: The paper introduces an exact solution for integrating multiple reference models in RLHF, addressing limitations of single-model approaches like lack of diversity and overfitting.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods rely on single reference models, limiting diversity and causing overfitting. Using multiple models could mitigate these issues but poses theoretical challenges.

Method: The paper presents a theoretical framework for exact solutions in reverse KL-regularized RLHF and extends it to forward KL-regularized RLHF, with rigorous statistical analysis and sample complexity guarantees.

Result: The framework provides exact solutions and sample complexity guarantees for multiple reference model integration, enabling more adaptable LLM alignment.

Conclusion: This work advances LLM alignment by supporting multiple reference models, offering theoretical soundness and better adaptability for modern AI challenges.

Abstract: Recent methods for aligning large language models (LLMs) with human feedback
predominantly rely on a single reference model, which limits diversity, model
overfitting, and underutilizes the wide range of available pre-trained models.
Incorporating multiple reference models has the potential to address these
limitations by broadening perspectives, reducing bias, and leveraging the
strengths of diverse open-source LLMs. However, integrating multiple reference
models into reinforcement learning with human feedback (RLHF) frameworks poses
significant theoretical challenges, where achieving exact solutions has
remained an open problem. This paper presents the first \emph{exact solution}
to the multiple reference model problem in reverse KL-regularized RLHF. We
introduce a comprehensive theoretical framework that includes rigorous
statistical analysis and provides sample complexity guarantees. Additionally,
we extend our analysis to forward KL-regularized RLHF, offering new insights
into sample complexity requirements in multiple reference scenarios. Our
contributions lay the foundation for more advanced and adaptable LLM alignment
techniques, enabling the effective use of multiple reference models. This work
paves the way for developing alignment frameworks that are both theoretically
sound and better suited to the challenges of modern AI ecosystems.

</details>


### [571] [The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration](https://arxiv.org/pdf/2502.01458)
*Wei Yao, Wenkai Yang, Gengze Xu, Ziqiao Wang, Yankai Lin, Yong Liu*

Main category: cs.LG

TL;DR: The paper explores weak-to-strong generalization, providing theoretical bounds on generalization and calibration errors, and validates findings with experiments.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the alignment of superhuman models with human values through weak-to-strong generalization.

Method: Theoretical analysis of generalization and calibration errors in classification and regression settings, with experimental validation.

Result: Identified key limitations and insights: weak model's generalization and calibration are critical, and strong model's optimization must balance reliance on weak signals.

Conclusion: Weak-to-strong generalization is promising but requires careful design of weak models and optimization processes to ensure strong performance.

Abstract: Weak-to-strong generalization, where weakly supervised strong models
outperform their weaker teachers, offers a promising approach to aligning
superhuman models with human values. To deepen the understanding of this
approach, we provide theoretical insights into its capabilities and
limitations. First, in the classification setting, we establish upper and lower
generalization error bounds for the strong model, identifying the primary
limitations as stemming from the weak model's generalization error and the
optimization objective itself. Additionally, we derive lower and upper bounds
on the calibration error of the strong model. These theoretical bounds reveal
two critical insights: (1) the weak model should demonstrate strong
generalization performance and maintain well-calibrated predictions, and (2)
the strong model's training process must strike a careful balance, as excessive
optimization could undermine its generalization capability by over-relying on
the weak supervision signals. Finally, in the regression setting, we extend the
work of Charikar et al. (2024) to a loss function based on Kullback-Leibler
(KL) divergence, offering guarantees that the strong student can outperform its
weak teacher by at least the magnitude of their disagreement. We conduct
sufficient experiments to validate our theory.

</details>


### [572] [PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts](https://arxiv.org/pdf/2502.06244)
*Zeman Li, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni*

Main category: cs.LG

TL;DR: PiKE is an adaptive data mixing algorithm for multitask learning that dynamically adjusts sampling weights by exploiting non-conflicting gradient interactions, improving convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Prior work in multitask learning focused on gradient conflicts, but large-scale pretraining often lacks such conflicts. PiKE addresses this gap by leveraging non-conflicting gradient interactions.

Method: PiKE dynamically adjusts sampling weights during training to minimize a near-tight upper bound on average loss decrease, with negligible computational overhead.

Result: PiKE outperforms static and nonadaptive baselines, achieving faster convergence and better downstream performance in large-scale language model pretraining.

Conclusion: PiKE is an effective adaptive data mixing algorithm for multitask learning, validated by theoretical guarantees and experimental results.

Abstract: Modern foundation models are trained on diverse datasets to enhance
generalization across tasks and domains A central challenge in this process is
determining how to effectively mix and sample data from multiple sources This
naturally leads to a multitask learning (MTL) perspective While prior work in
MTL has emphasized mitigating gradient conflicts we observe that largescale
pretraining scenariossuch as multilingual or multidomain trainingoften exhibit
little to no gradient conflict Motivated by this observation we propose PiKE
(Positive gradient interaction-based K-task weights Estimator) an adaptive data
mixing algorithm that dynamically adjusts sampling weights during training PiKE
exploits nonconflicting gradient interactions to minimize a neartight upper
bound on the average loss decrease at each step while incurring negligible
computational overhead We provide theoretical convergence guarantees and show
that PiKE outperforms static and nonadaptive mixing baselines Furthermore we
extend PiKE to promote balanced learning across tasks Extensive experiments on
largescale language model pretraining confirm that PiKE achieves faster
convergence and improved downstream performance compared to existing approaches

</details>


### [573] [Continual Release Moment Estimation with Differential Privacy](https://arxiv.org/pdf/2502.06597)
*Nikita P. Kalinin, Jalaj Upadhyay, Christoph H. Lampert*

Main category: cs.LG

TL;DR: JME is a method for privately estimating first and second moments of data with reduced noise, improving accuracy without extra privacy cost.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy in private moment estimation while maintaining privacy, addressing limitations of naive approaches.

Method: Uses the matrix mechanism and joint sensitivity analysis for second moment estimation without additional privacy cost.

Result: Demonstrated effectiveness in running mean/covariance estimation for Gaussian density and DP-Adam training on CIFAR-10.

Conclusion: JME provides a more accurate and efficient solution for private moment estimation.

Abstract: We propose Joint Moment Estimation (JME), a method for continually and
privately estimating both the first and second moments of data with reduced
noise compared to naive approaches. JME uses the matrix mechanism and a joint
sensitivity analysis to allow the second moment estimation with no additional
privacy cost, thereby improving accuracy while maintaining privacy. We
demonstrate JME's effectiveness in two applications: estimating the running
mean and covariance matrix for Gaussian density estimation, and model training
with DP-Adam on CIFAR-10.

</details>


### [574] [Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles](https://arxiv.org/pdf/2505.06459)
*Pablo Flores, Olga Graf, Pavlos Protopapas, Karim Pichara*

Main category: cs.LG

TL;DR: The paper proposes a two-step method to train Bayesian Neural Networks for uncertainty quantification in Physics-Informed Neural Networks (PINNs), improving uncertainty estimation using error bounds and applying it to cosmology problems.


<details>
  <summary>Details</summary>
Motivation: PINNs lack built-in uncertainty quantification, and existing methods need improvement for better reliability in solving differential equations.

Method: A two-step procedure trains Bayesian Neural Networks, incorporating heteroscedastic variance based on PINN error bounds for better uncertainty estimation.

Result: Improved uncertainty quantification for PINNs, demonstrated in solving forward problems and parameter estimation in cosmology.

Conclusion: The method enhances uncertainty estimation in PINNs, proving useful for both forward and inverse problems in cosmology.

Abstract: Physics-Informed Neural Networks (PINNs) have been widely used to obtain
solutions to various physical phenomena modeled as Differential Equations. As
PINNs are not naturally equipped with mechanisms for Uncertainty
Quantification, some work has been done to quantify the different uncertainties
that arise when dealing with PINNs. In this paper, we use a two-step procedure
to train Bayesian Neural Networks that provide uncertainties over the solutions
to differential equation systems provided by PINNs. We use available error
bounds over PINNs to formulate a heteroscedastic variance that improves the
uncertainty estimation. Furthermore, we solve forward problems and utilize the
obtained uncertainties when doing parameter estimation in inverse problems in
cosmology.

</details>


### [575] [Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing](https://arxiv.org/pdf/2502.09564)
*Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino*

Main category: cs.LG

TL;DR: The paper introduces Diffusing DeBias (DDB), a method using synthetic data from diffusion models to address bias in deep learning classification tasks, outperforming current debiasing techniques.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often suffer from bias due to spurious correlations in training data, leading to poor generalization. The paper aims to tackle this by leveraging bias amplification with synthetic data.

Method: DDB uses conditional diffusion models to generate synthetic bias-aligned images, replacing the original training set to learn a bias amplifier model. This is integrated into unsupervised debiasing approaches.

Result: DDB outperforms state-of-the-art methods on multiple benchmark datasets by addressing bias-conflicting sample memorization.

Conclusion: DDB is a versatile and effective tool for mitigating bias in deep learning models, demonstrating superior performance over existing techniques.

Abstract: Deep learning model effectiveness in classification tasks is often challenged
by the quality and quantity of training data whenever they are affected by
strong spurious correlations between specific attributes and target labels.
This results in a form of bias affecting training data, which typically leads
to unrecoverable weak generalization in prediction. This paper aims at facing
this problem by leveraging bias amplification with generated synthetic data: we
introduce Diffusing DeBias (DDB), a novel approach acting as a plug-in for
common methods of unsupervised model debiasing exploiting the inherent
bias-learning tendency of diffusion models in data generation. Specifically,
our approach adopts conditional diffusion models to generate synthetic
bias-aligned images, which replace the original training set for learning an
effective bias amplifier model that we subsequently incorporate into an
end-to-end and a two-step unsupervised debiasing approach. By tackling the
fundamental issue of bias-conflicting training samples memorization in learning
auxiliary models, typical of this type of techniques, our proposed method beats
current state-of-the-art in multiple benchmark datasets, demonstrating its
potential as a versatile and effective tool for tackling bias in deep learning
models.

</details>


### [576] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/pdf/2505.10360)
*Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe*

Main category: cs.LG

TL;DR: The paper introduces FactsR, a method for real-time extraction of clinical information during consultations to generate more accurate and concise notes, addressing issues like hallucinations and clinician workload.


<details>
  <summary>Details</summary>
Motivation: Current AI scribes rely on post-consultation prompts, leading to errors and inefficiencies, which risks patient safety due to clinician workload and fatigue.

Method: The FactsR method extracts salient clinical information (Facts) in real-time during consultations and uses it recursively to generate final notes.

Result: FactsR produces more accurate and concise notes by involving clinicians in the note-generation process and enables real-time decision support.

Conclusion: FactsR improves note accuracy and conciseness while enhancing clinician involvement and enabling new real-time applications.

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [577] [Censor Dependent Variational Inference](https://arxiv.org/pdf/2502.09591)
*Chuanhui Liu, Xiao Wang*

Main category: cs.LG

TL;DR: The paper analyzes variational inference in survival analysis, identifies flaws in existing methods, and proposes censor-dependent variational inference (CDVI) and CD-CVAE for improved survival distribution estimation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying variational methods to survival data, particularly the impact of poorly designed variational distributions on modeling time-to-event distributions.

Method: Proposes CDVI, a censor-dependent variational inference method, and CD-CVAE, a VAE for scalable CDVI implementation. Extends theories and training techniques for survival analysis.

Result: Experiments show significant improvements in estimating individual survival distributions.

Conclusion: CDVI and CD-CVAE effectively address variational inference challenges in survival analysis, enhancing accuracy in survival distribution estimation.

Abstract: This paper provides a comprehensive analysis of variational inference in
latent variable models for survival analysis, emphasizing the distinctive
challenges associated with applying variational methods to survival data. We
identify a critical weakness in the existing methodology, demonstrating how a
poorly designed variational distribution may hinder the objective of survival
analysis tasks - modeling time-to-event distributions. We prove that the
optimal variational distribution, which perfectly bounds the log-likelihood,
may depend on the censoring mechanism. To address this issue, we propose
censor-dependent variational inference (CDVI), tailored for latent variable
models in survival analysis. More practically, we introduce CD-CVAE, a
V-structure Variational Autoencoder (VAE) designed for the scalable
implementation of CDVI. Further discussion extends some existing theories and
training techniques to survival analysis. Extensive experiments validate our
analysis and demonstrate significant improvements in the estimation of
individual survival distributions.

</details>


### [578] [Best of Both Worlds: Regret Minimization versus Minimax Play](https://arxiv.org/pdf/2502.11673)
*Adrian Müller, Jon Schneider, Stratis Skoulakis, Luca Viano, Volkan Cevher*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate the existence of online learning algorithms
with bandit feedback that simultaneously guarantee $O(1)$ regret compared to a
given comparator strategy, and $\tilde{O}(\sqrt{T})$ regret compared to any
fixed strategy, where $T$ is the number of rounds. We provide the first
affirmative answer to this question whenever the comparator strategy supports
every action. In the context of zero-sum games with min-max value zero, both in
normal- and extensive form, we show that our results allow us to guarantee to
risk at most $O(1)$ loss while being able to gain $\Omega(T)$ from exploitable
opponents, thereby combining the benefits of both no-regret algorithms and
minimax play.

</details>


### [579] [Coreset Selection via LLM-based Concept Bottlenecks](https://arxiv.org/pdf/2502.16733)
*Akshay Mehra, Trisha Mittal, Subhadra Gopalakrishnan, Joshua Kimball*

Main category: cs.LG

TL;DR: Proposes a model-independent coreset selection method using human-understandable concepts, outperforming traditional training dynamics-based methods.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency and interpretability issues in coreset selection by avoiding dependency on downstream model training.

Method: Uses concept bottlenecks derived from large language models to compute sample difficulty scores, enabling stratified sampling for coreset generation.

Result: Achieves high-performing coresets for various models without full dataset training, outperforming random subsets and traditional methods.

Conclusion: Demonstrates effectiveness on CIFAR-10/100 and ImageNet-1K, offering a scalable and interpretable alternative for coreset selection.

Abstract: Coreset Selection (CS) aims to identify a subset of the training dataset that
achieves model performance comparable to using the entire dataset. Many
state-of-the-art CS methods select coresets using scores whose computation
requires training the downstream model on the entire dataset first and
recording changes in the model's behavior on samples as it trains (training
dynamics). These scores are inefficient to compute and hard to interpret, as
they do not indicate whether a sample is difficult to learn in general or only
for a specific downstream model. Our work addresses these challenges by
proposing a score that computes a sample's difficulty using
human-understandable textual attributes (concepts) independent of any
downstream model. Specifically, we measure the alignment between a sample's
visual features and concept bottlenecks, derived via large language models, by
training a linear concept bottleneck layer and computing the sample's
difficulty score using it.We then use stratified sampling based on this score
to generate a coreset of the dataset.Crucially, our score is efficiently
computable without training the downstream model on the full dataset even once,
leads to high-performing coresets for various downstream models, and is
computable even for an unlabeled dataset. Through experiments on CIFAR-10/100,
and ImageNet-1K, we show that our coresets outperform random subsets, even at
high pruning rates, and achieve model performance comparable to or better than
coresets found by training dynamics-based methods.

</details>


### [580] [VCT: Training Consistency Models with Variational Noise Coupling](https://arxiv.org/pdf/2502.18197)
*Gianluigi Silvestri, Luca Ambrogioni, Chieh-Hsin Lai, Yuhta Takida, Yuki Mitsufuji*

Main category: cs.LG

TL;DR: VCT improves Consistency Training by adaptively learning noise-data pairings, reducing variance and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Non-distillation CT suffers from high variance and instability, prompting the need for a more robust framework.

Method: VCT introduces a learned noise-data coupling scheme inspired by VAEs, enabling adaptive noise-data pairings.

Result: VCT surpasses baselines, achieves top FID on CIFAR-10, and matches SoTA on ImageNet 64x64 with minimal steps.

Conclusion: VCT is a flexible and effective alternative to classical CT, offering improved stability and performance.

Abstract: Consistency Training (CT) has recently emerged as a strong alternative to
diffusion models for image generation. However, non-distillation CT often
suffers from high variance and instability, motivating ongoing research into
its training dynamics. We propose Variational Consistency Training (VCT), a
flexible and effective framework compatible with various forward kernels,
including those in flow matching. Its key innovation is a learned noise-data
coupling scheme inspired by Variational Autoencoders, where a data-dependent
encoder models noise emission. This enables VCT to adaptively learn
noise-todata pairings, reducing training variance relative to the fixed,
unsorted pairings in classical CT. Experiments on multiple image datasets
demonstrate significant improvements: our method surpasses baselines, achieves
state-of-the-art FID among non-distillation CT approaches on CIFAR-10, and
matches SoTA performance on ImageNet 64 x 64 with only two sampling steps. Code
is available at https://github.com/sony/vct.

</details>


### [581] [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/pdf/2505.14884)
*Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy*

Main category: cs.LG

TL;DR: Polar Sparsity accelerates LLM inference by focusing on sparsity in Attention layers, achieving 2.2× speedups for models like OPT and LLaMA-2/3 without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling contextual sparsity for large batch sizes in LLM inference, where traditional methods fail due to dense computation.

Method: Introduces Polar Sparsity, emphasizing sparsity in Attention layers over MLP layers, and develops sparsity-aware GPU kernels for selective computations.

Result: Achieves up to 2.2× speedups across models (OPT, LLaMA-2/3) for various batch sizes and sequence lengths while maintaining accuracy.

Conclusion: Polar Sparsity effectively scales contextual sparsity to large batch sizes, enabling practical, high-throughput LLM deployments.

Abstract: Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.

</details>


### [582] [Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as hyperelastic constitutive artificial neural networks (CANs)](https://arxiv.org/pdf/2503.05617)
*Prakash Thakolkaran, Yaqi Guo, Shivam Saini, Mathias Peirlinck, Benjamin Alheit, Siddhant Kumar*

Main category: cs.LG

TL;DR: ICKANs balance expressivity and interpretability for hyperelastic constitutive laws using trainable spline-based functions.


<details>
  <summary>Details</summary>
Motivation: Traditional models lack expressivity, while neural networks lack interpretability. ICKANs aim to bridge this gap.

Method: Monotonic Input-Convex Kolmogorov-Arnold Networks (ICKANs) decompose models into trainable univariate spline functions, ensuring physical admissibility.

Result: ICKANs accurately model stress-strain behavior and generalize well in finite element simulations.

Conclusion: ICKANs provide a robust, interpretable framework for hyperelastic constitutive modeling.

Abstract: Traditional constitutive models rely on hand-crafted parametric forms with
limited expressivity and generalizability, while neural network-based models
can capture complex material behavior but often lack interpretability. To
balance these trade-offs, we present monotonic Input-Convex Kolmogorov-Arnold
Networks (ICKANs) for learning polyconvex hyperelastic constitutive laws.
ICKANs leverage the Kolmogorov-Arnold representation, decomposing the model
into compositions of trainable univariate spline-based activation functions for
rich expressivity. We introduce trainable monotonic input-convex splines within
the KAN architecture, ensuring physically admissible polyconvex models for
isotropic compressible hyperelasticity. The resulting models are both compact
and interpretable, enabling explicit extraction of analytical constitutive
relationships through a monotonic input-convex symbolic regression technique.
Through unsupervised training on full-field strain data and limited global
force measurements, ICKANs accurately capture nonlinear stress-strain behavior
across diverse strain states. Finite element simulations of unseen geometries
with trained ICKAN hyperelastic constitutive models confirm the framework's
robustness and generalization capability.

</details>


### [583] [GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models](https://arxiv.org/pdf/2503.09117)
*Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han*

Main category: cs.LG

TL;DR: GRU is a gradient-based unlearning framework for LLMs that minimizes trade-offs between unlearning and retention by projecting gradients orthogonally.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between unlearning harmful data and retaining useful functionality in LLMs.

Method: Proposes Gradient Rectified Unlearning (GRU), which projects unlearning gradients orthogonally to harmful retention gradients.

Result: GRU enhances baseline methods, is easy to implement, and performs well across benchmarks.

Conclusion: GRU effectively balances unlearning and retention, offering a practical solution for LLM safety and legality.

Abstract: Large language model (LLM) unlearning has demonstrated its essential role in
removing privacy and copyright-related responses, crucial for their legal and
safe applications. However, the pursuit of complete unlearning often comes with
substantial costs due to its compromises in their general functionality,
leading to a notorious trade-off between unlearning and retention. In examining
the update process for unlearning dynamically, we find gradients hold essential
information for revealing this trade-off. In particular, we look at the varying
relationship between retention performance and directional disparities between
gradients during unlearning. It motivates the sculpting of an update mechanism
derived from gradients from two sources, i.e., harmful for retention and useful
for unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an
enhanced unlearning framework controlling the updating gradients in a
geometry-focused and optimization-driven manner such that their side impacts on
other, unrelated responses can be minimized. Specifically, GRU derives a
closed-form solution to project the unlearning gradient onto the orthogonal
space of that gradient harmful for retention, ensuring minimal deviation from
its original direction under the condition that overall performance is
retained. Comprehensive experiments are conducted to demonstrate that GRU, as a
general framework, is straightforward to implement and efficiently enhances a
range of baseline methods through its adaptable and compatible characteristics.
Additionally, experimental results show its broad effectiveness across a
diverse set of benchmarks for LLM unlearning.

</details>


### [584] [FlexiReg: Flexible Urban Region Representation Learning](https://arxiv.org/pdf/2503.09128)
*Fengze Sun, Yanchuan Chang, Egemen Tanin, Shanika Karunasekera, Jianzhong Qi*

Main category: cs.LG

TL;DR: FlexiReg is a flexible urban region representation learning model that adapts to varying region formations and features, outperforming state-of-the-art models by up to 202% in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing urban region representation models use fixed region formations and features, which may not suit diverse downstream tasks.

Method: FlexiReg uses spatial grid partitioning, learns cell representations from public data (POI, land use, imagery), and employs adaptive aggregation and prompt learning for task-specific representations.

Result: FlexiReg achieves up to 202% better accuracy than state-of-the-art models across five real-world datasets.

Conclusion: FlexiReg's flexibility in region formation and feature adaptation makes it highly effective for diverse urban downstream tasks.

Abstract: The increasing availability of urban data offers new opportunities for
learning region representations, which can be used as input to machine learning
models for downstream tasks such as check-in or crime prediction. While
existing solutions have produced promising results, an issue is their fixed
formation of regions and fixed input region features, which may not suit the
needs of different downstream tasks. To address this limitation, we propose a
model named FlexiReg for urban region representation learning that is flexible
with both the formation of urban regions and the input region features.
FlexiReg is based on a spatial grid partitioning over the spatial area of
interest. It learns representations for the grid cells, leveraging publicly
accessible data, including POI, land use, satellite imagery, and street view
imagery. We propose adaptive aggregation to fuse the cell representations and
prompt learning techniques to tailor the representations towards different
tasks, addressing the needs of varying formations of urban regions and
downstream tasks. Extensive experiments on five real-world datasets demonstrate
that FlexiReg outperforms state-of-the-art models by up to 202% in term of the
accuracy of four diverse downstream tasks using the produced urban region
representations.

</details>


### [585] [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/pdf/2503.09532)
*Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda*

Main category: cs.LG

TL;DR: SAEBench introduces a standardized evaluation suite for sparse autoencoders (SAEs), revealing that proxy metrics often fail to predict practical performance.


<details>
  <summary>Details</summary>
Motivation: Prior work on SAEs relies on unclear proxy metrics, lacking practical relevance. SAEBench aims to provide a comprehensive evaluation framework.

Method: SAEBench evaluates SAEs across eight metrics, including interpretability and feature disentanglement, using over 200 open-source SAEs from eight architectures.

Result: Proxy metrics do not reliably indicate practical performance; e.g., Matryoshka SAEs excel in feature disentanglement despite underperforming on proxies.

Conclusion: SAEBench enables systematic comparison and scaling trend analysis, advancing SAE research with a standardized framework.

Abstract: Sparse autoencoders (SAEs) are a popular technique for interpreting language
model activations, and there is extensive recent work on improving SAE
effectiveness. However, most prior work evaluates progress using unsupervised
proxy metrics with unclear practical relevance. We introduce SAEBench, a
comprehensive evaluation suite that measures SAE performance across eight
diverse metrics, spanning interpretability, feature disentanglement and
practical applications like unlearning. To enable systematic comparison, we
open-source a suite of over 200 SAEs across eight recently proposed SAE
architectures and training algorithms. Our evaluation reveals that gains on
proxy metrics do not reliably translate to better practical performance. For
instance, while Matryoshka SAEs slightly underperform on existing proxy
metrics, they substantially outperform other architectures on feature
disentanglement metrics; moreover, this advantage grows with SAE scale. By
providing a standardized framework for measuring progress in SAE development,
SAEBench enables researchers to study scaling trends and make nuanced
comparisons between different SAE architectures and training methodologies. Our
interactive interface enables researchers to flexibly visualize relationships
between metrics across hundreds of open-source SAEs at:
www.neuronpedia.org/sae-bench

</details>


### [586] [Sample Compression for Self Certified Continual Learning](https://arxiv.org/pdf/2503.10503)
*Jacob Comeau, Mathieu Bazinet, Pascal Germain, Cem Subakan*

Main category: cs.LG

TL;DR: CoP2L is a new continual learning method combining Pick-to-Learn and experience replay, providing generalization guarantees and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning methods lack theoretical guarantees and rely on heuristics. CoP2L addresses this by ensuring non-vacuous bounds on generalization loss.

Method: CoP2L integrates Pick-to-Learn (sample compression theory) with experience replay, retaining representative samples per task efficiently.

Result: CoP2L outperforms baselines in Class-, Task-, and Domain-Incremental settings, mitigating catastrophic forgetting.

Conclusion: CoP2L offers reliable predictors with computable bounds, enhancing trustworthiness in continual learning.

Abstract: Continual learning algorithms aim to learn from a sequence of tasks, making
the training distribution non-stationary. The majority of existing continual
learning approaches in the literature rely on heuristics and do not provide
learning guarantees. In this paper, we present a new method called Continual
Pick-to-Learn (CoP2L), which is able to retain the most representative samples
for each task in an efficient way. CoP2L combines the Pick-to-Learn algorithm
(rooted in the sample compression theory) and the experience replay continual
learning scheme. This allows us to provide non-vacuous upper bounds on the
generalization loss of the learned predictors, numerically computable after
each task. We empirically evaluate our approach on several standard continual
learning benchmarks across Class-Incremental, Task-Incremental, and
Domain-Incremental settings. Our results show that CoP2L is highly competitive
across all setups, often outperforming existing baselines, and significantly
mitigating catastrophic forgetting compared to vanilla experience replay in the
Class-Incremental setting. It is possible to leverage the bounds provided by
CoP2L in practical scenarios to certify the predictor reliability on previously
learned tasks, in order to improve the trustworthiness of the continual
learning algorithm.

</details>


### [587] [Adversarial bandit optimization for approximately linear functions](https://arxiv.org/pdf/2505.20734)
*Zhuoyu Cheng, Kohei Hatano, Eiji Takimoto*

Main category: cs.LG

TL;DR: The paper analyzes bandit optimization for nonconvex, nonsmooth functions with linear and perturbed losses, providing regret bounds and a lower bound.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of bandit optimization with nonconvex, nonsmooth functions and adversarial perturbations.

Method: Analyzes regret bounds (expected and high probability) for the problem, including a special case of bandit linear optimization.

Result: Improved high-probability regret bounds for bandit linear optimization and a lower bound on expected regret.

Conclusion: The work advances understanding of bandit optimization under adversarial perturbations and nonconvexity.

Abstract: We consider a bandit optimization problem for nonconvex and non-smooth
functions, where in each trial the loss function is the sum of a linear
function and a small but arbitrary perturbation chosen after observing the
player's choice. We give both expected and high probability regret bounds for
the problem. Our result also implies an improved high-probability regret bound
for the bandit linear optimization, a special case with no perturbation. We
also give a lower bound on the expected regret.

</details>


### [588] [Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable](https://arxiv.org/pdf/2503.20117)
*Bicheng Ying, Zhe Li, Haibo Yang*

Main category: cs.LG

TL;DR: The paper introduces FOCUS, a Federated Learning algorithm with exact convergence, addressing challenges like arbitrary client participation and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of FedAvg-style algorithms in handling arbitrary client participation and data heterogeneity, which hinder exact and fast convergence.

Method: Uses stochastic matrix and time-varying graphs to model client dynamics, leading to the design of FOCUS, a provably convergent algorithm with a push-pull strategy.

Result: FOCUS achieves exact convergence with a linear rate, regardless of arbitrary client participation, a first in FL research.

Conclusion: FOCUS provides a robust solution for FL challenges, offering exact convergence and faster rates compared to existing methods.

Abstract: This work tackles the fundamental challenges in Federated Learning (FL) posed
by arbitrary client participation and data heterogeneity, prevalent
characteristics in practical FL settings. It is well-established that popular
FedAvg-style algorithms struggle with exact convergence and can suffer from
slow convergence rates since a decaying learning rate is required to mitigate
these scenarios. To address these issues, we introduce the concept of
stochastic matrix and the corresponding time-varying graphs as a novel modeling
tool to accurately capture the dynamics of arbitrary client participation and
the local update procedure. Leveraging this approach, we offer a fresh
perspective on designing FL algorithms, provide a rigorous quantitative
analysis of the limitations inherent in the FedAvg algorithm, and present
FOCUS, Federated Optimization with Exact Convergence via Push-pull Strategy, a
provably convergent algorithm designed to effectively overcome the previously
mentioned two challenges. More specifically, we provide a rigorous proof
demonstrating that FOCUS achieves exact convergence with a linear rate
regardless of the arbitrary client participation, establishing it as the first
work to demonstrate this significant result.

</details>


### [589] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/pdf/2505.21677)
*Hung Anh Vu, Galen Reeves, Emily Wenger*

Main category: cs.LG

TL;DR: The paper explores the effects of AI models training on outputs from other AI models, finding both benefits (exposure to new concepts) and drawbacks (performance homogenization).


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on generative AI and the potential for models to train on each other's outputs necessitates understanding the downstream effects of such interactions.

Method: The study provides empirical evidence, develops a theoretical model, and conducts experiments to analyze data-mediated interactions between AI models.

Result: Data-mediated interactions can introduce novel concepts to models but may also lead to homogenized performance on shared tasks.

Conclusion: Understanding the implications of AI models training on each other's outputs is crucial for managing their long-term societal impact.

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [590] [Efficient Learning for Entropy-Regularized Markov Decision Processes via Multilevel Monte Carlo](https://arxiv.org/pdf/2503.21224)
*Matthieu Meunier, Christoph Reisinger, Yufei Zhang*

Main category: cs.LG

TL;DR: Proposes MLMC algorithms for entropy-regularized MDPs with Polish spaces, achieving quasi-polynomial or polynomial sample complexity independent of state/action space sizes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient learning in large or continuous MDPs with complexity guarantees.

Method: Novel MLMC algorithms combining fixed-point iteration, MLMC techniques, and stochastic Bellman operator approximation.

Result: Quasi-polynomial sample complexity with biased MC, polynomial with unbiased MLMC, both independent of state/action dimensions.

Conclusion: Theoretical and numerical validation shows improved efficiency over existing methods.

Abstract: Designing efficient learning algorithms with complexity guarantees for Markov
decision processes (MDPs) with large or continuous state and action spaces
remains a fundamental challenge. We address this challenge for
entropy-regularized MDPs with Polish state and action spaces, assuming access
to a generative model of the environment. We propose a novel family of
multilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration
with MLMC techniques and a generic stochastic approximation of the Bellman
operator. We quantify the precise impact of the chosen approximate Bellman
operator on the accuracy of the resulting MLMC estimator. Leveraging this error
analysis, we show that using a biased plain MC estimate for the Bellman
operator results in quasi-polynomial sample complexity, whereas an unbiased
randomized multilevel approximation of the Bellman operator achieves polynomial
sample complexity in expectation. Notably, these complexity bounds are
independent of the dimensions or cardinalities of the state and action spaces,
distinguishing our approach from existing algorithms whose complexities scale
with the sizes of these spaces. We validate these theoretical performance
guarantees through numerical experiments.

</details>


### [591] [RBFleX-NAS: Training-Free Neural Architecture Search Using Radial Basis Function Kernel and Hyperparameter Detection](https://arxiv.org/pdf/2503.22733)
*Tomomasa Yamasaki, Zhehui Wang, Tao Luo, Niangjun Chen, Bo Wang*

Main category: cs.LG

TL;DR: RBFleX-NAS is a training-free NAS framework using RBF kernels for better performance prediction and activation function exploration, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current training-free NAS methods struggle with accurate performance prediction and activation function exploration, leading to sub-optimal results.

Method: Proposes RBFleX-NAS, leveraging RBF kernels for evaluating networks and a detection algorithm for hyperparameter optimization. Introduces NAFBee for extended activation function search.

Result: RBFleX-NAS achieves higher top-1 accuracy and Kendall correlation, with shorter search time, outperforming state-of-the-art methods.

Conclusion: RBFleX-NAS is a superior training-free NAS framework, excelling in accuracy and efficiency, especially in activation function exploration.

Abstract: Neural Architecture Search (NAS) is an automated technique to design optimal
neural network architectures for a specific workload. Conventionally,
evaluating candidate networks in NAS involves extensive training, which
requires significant time and computational resources. To address this,
training-free NAS has been proposed to expedite network evaluation with minimal
search time. However, state-of-the-art training-free NAS algorithms struggle to
precisely distinguish well-performing networks from poorly-performing networks,
resulting in inaccurate performance predictions and consequently sub-optimal
top-1 network accuracy. Moreover, they are less effective in activation
function exploration. To tackle the challenges, this paper proposes RBFleX-NAS,
a novel training-free NAS framework that accounts for both activation outputs
and input features of the last layer with a Radial Basis Function (RBF) kernel.
We also present a detection algorithm to identify optimal hyperparameters using
the obtained activation outputs and input feature maps. We verify the efficacy
of RBFleX-NAS over a variety of NAS benchmarks. RBFleX-NAS significantly
outperforms state-of-the-art training-free NAS methods in terms of top-1
accuracy, achieving this with short search time in NAS-Bench-201 and
NAS-Bench-SSS. In addition, it demonstrates higher Kendall correlation compared
to layer-based training-free NAS algorithms. Furthermore, we propose NAFBee, a
new activation design space that extends the activation type to encompass
various commonly used functions. In this extended design space, RBFleX-NAS
demonstrates its superiority by accurately identifying the best-performing
network during activation function search, providing a significant advantage
over other NAS algorithms.

</details>


### [592] [Large Language Models are Locally Linear Mappings](https://arxiv.org/pdf/2505.24293)
*James R. Golden*

Main category: cs.LG

TL;DR: The paper shows that LLMs' inference operations can be mapped to linear systems without altering weights or outputs, revealing low-dimensional subspaces and interpretable semantic structures.


<details>
  <summary>Details</summary>
Motivation: To understand and interpret the internal workings of large language models (LLMs) by mapping their inference to linear systems.

Method: Extends techniques from image diffusion models to alter gradient computation, enabling nearly exact linear system representation of LLMs.

Result: LLMs operate in low-dimensional subspaces with interpretable semantic concepts, and their layers can be analyzed as linear systems.

Conclusion: Despite global nonlinearity, LLMs can be interpreted through locally linear decompositions, providing insights into their internal representations.

Abstract: We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [593] [NNN: Next-Generation Neural Networks for Marketing Measurement](https://arxiv.org/pdf/2504.06212)
*Thomas Mulc, Mike Anderson, Paul Cubre, Huikun Zhang, Ivy Liu, Saket Kumar*

Main category: cs.LG

TL;DR: NNN is a Transformer-based neural network for marketing measurement, outperforming traditional models by capturing complex interactions and long-term effects with rich embeddings and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional Marketing Mix Models (MMMs) lack the ability to capture qualitative aspects and complex interactions in marketing data, prompting the need for a more expressive approach.

Method: NNN uses Transformer-based architecture with rich embeddings and attention mechanisms, enhanced by L1 regularization for data-constrained settings.

Result: NNN shows significant improvement in predictive power on simulated and real-world data, along with additional insights from model probing.

Conclusion: NNN is effective for marketing measurement and offers complementary insights, making it a valuable tool beyond traditional methods.

Abstract: We present NNN, an experimental Transformer-based neural network approach to
marketing measurement. Unlike Marketing Mix Models (MMMs) which rely on scalar
inputs and parametric decay functions, NNN uses rich embeddings to capture both
quantitative and qualitative aspects of marketing and organic channels (e.g.,
search queries, ad creatives). This, combined with its attention mechanism,
potentially enables NNN to model complex interactions, capture long-term
effects, and improve sales attribution accuracy. We show that L1 regularization
permits the use of such expressive models in typical data-constrained settings.
Evaluating NNN on simulated and real-world data demonstrates its efficacy,
particularly through considerable improvement in predictive power. In addition
to marketing measurement, the NNN framework can provide valuable, complementary
insights through model probing, such as evaluating keyword or creative
effectiveness.

</details>


### [594] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/pdf/2505.24298)
*Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu*

Main category: cs.LG

TL;DR: AReaL is an asynchronous RL system for LLMs that decouples generation from training, improving GPU utilization and achieving faster training speeds compared to synchronous systems.


<details>
  <summary>Details</summary>
Motivation: Existing synchronous RL systems for LLMs suffer from inefficiency due to waiting for batch generation, leading to GPU underutilization.

Method: AReaL decouples generation and training, uses system-level optimizations, and employs a staleness-enhanced PPO variant to handle outdated data.

Result: AReaL achieves up to 2.77× training speedup with matched or improved performance on math and code reasoning benchmarks.

Conclusion: AReaL offers a scalable and efficient solution for RL training in LLMs, addressing inefficiencies in synchronous systems.

Abstract: Reinforcement learning (RL) has become a dominant paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are
synchronous, alternating generation and training in a batch setting where
rollouts in each training batch are generated by the same model. This approach
stabilizes RL training but suffers from severe system-level inefficiency:
generation must wait until the longest output in the batch is completed before
model updates, resulting in GPU underutilization. We present AReaL, a fully
asynchronous RL system that completely decouples generation from training.
Rollout workers in AReaL continuously generate new outputs without waiting,
while training workers update the model whenever a batch of data is collected.
AReaL also incorporates a collection of system-level optimizations, leading to
substantially higher GPU utilization. To stabilize RL training, AReaL balances
the workload of rollout and training workers to control data staleness, and
adopts a staleness-enhanced PPO variant to better handle outdated training
samples. Extensive experiments on math and code reasoning benchmarks show that
AReaL achieves up to 2.77$\times$ training speedup compared to synchronous
systems with the same number of GPUs and matched or improved final performance.
The code of AReaL is available at https://github.com/inclusionAI/AReaL/.

</details>


### [595] [Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis](https://arxiv.org/pdf/2504.09132)
*Matthew B. Webster, Dongheon Lee, Joonnyong Lee*

Main category: cs.LG

TL;DR: A self-supervised multi-encoder autoencoder (MEAE) is proposed to separate heartbeat-related signals from PPG, improving heart rate detection in noisy data without pre-processing.


<details>
  <summary>Details</summary>
Motivation: To enhance heart rate detection in noisy PPG data by extracting underlying source signals using a self-supervised approach.

Method: A multi-encoder autoencoder (MEAE) is trained on PPG signals from a polysomnography database without pre-processing, then applied to noisy PPG data.

Result: The extracted heartbeat-related signal significantly improves heart rate detection compared to the original PPG.

Conclusion: The MEAE's self-supervised nature and strong performance show promise for blind source separation in biosignal analysis.

Abstract: Biosignals can be viewed as mixtures measuring particular physiological
events, and blind source separation (BSS) aims to extract underlying source
signals from mixtures. This paper proposes a self-supervised multi-encoder
autoencoder (MEAE) to separate heartbeat-related source signals from
photoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG
data. The MEAE is trained on PPG signals from a large open polysomnography
database without any pre-processing or data selection. The trained network is
then applied to a noisy PPG dataset collected during the daily activities of
nine subjects. The extracted heartbeat-related source signal significantly
improves HR detection as compared to the original PPG. The absence of
pre-processing and the self-supervised nature of the proposed method, combined
with its strong performance, highlight the potential of MEAE for BSS in
biosignal analysis.

</details>


### [596] [Object Centric Concept Bottlenecks](https://arxiv.org/pdf/2505.24492)
*David Steinmann, Wolfgang Stammer, Antonia Wüst, Kristian Kersting*

Main category: cs.LG

TL;DR: OCB combines CBMs with object-centric models to improve performance and interpretability in complex vision tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of CBMs in handling object-centric real-world settings and complex vision tasks.

Method: Introduces Object-Centric Concept Bottlenecks (OCB), leveraging pre-trained object-centric models and concept activations.

Result: OCB outperforms traditional CBMs and enables interpretable decisions for complex visual tasks.

Conclusion: OCB successfully bridges the gap between performance and interpretability in AI models for complex vision tasks.

Abstract: Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [597] [Towards Trustworthy Federated Learning with Untrusted Participants](https://arxiv.org/pdf/2505.01874)
*Youssef Allouah, Rachid Guerraoui, John Stephan*

Main category: cs.LG

TL;DR: CafCor enables privacy and resilience in federated learning without a trusted server, using shared randomness between participants and outperforming local DP methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for resilience against malicious participants and data privacy in federated learning without relying on a trusted central server.

Method: Proposes CafCor, integrating robust gradient aggregation with correlated noise injection using shared randomness between participants.

Result: Achieves strong privacy-utility trade-offs, outperforming local DP and nearing central DP utility.

Conclusion: Privacy and robustness can coexist in distributed systems without trusting the server or sacrificing utility.

Abstract: Resilience against malicious participants and data privacy are essential for
trustworthy federated learning, yet achieving both with good utility typically
requires the strong assumption of a trusted central server. This paper shows
that a significantly weaker assumption suffices: each pair of participants
shares a randomness seed unknown to others. In a setting where malicious
participants may collude with an untrusted server, we propose CafCor, an
algorithm that integrates robust gradient aggregation with correlated noise
injection, using shared randomness between participants. We prove that CafCor
achieves strong privacy-utility trade-offs, significantly outperforming local
differential privacy (DP) methods, which do not make any trust assumption,
while approaching central DP utility, where the server is fully trusted.
Empirical results on standard benchmarks validate CafCor's practicality,
showing that privacy and robustness can coexist in distributed systems without
sacrificing utility or trusting the server.

</details>


### [598] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/pdf/2505.14613)
*Emmanuel Noutahi, Jason Hartford, Prudencio Tossou, Shawn Whitfield, Alisandra K. Denton, Cas Wognum, Kristina Ulicna, Michael Craig, Jonathan Hsu, Michael Cuccarese, Emmanuel Bengio, Dominique Beaini, Christopher Gibson, Daniel Cohen, Berton Earnshaw*

Main category: cs.LG

TL;DR: The paper discusses the potential of virtual cells in drug discovery, emphasizing the need for accurate predictions of cellular responses to perturbations and explaining biomolecular interactions. It outlines principles for designing therapeutically relevant virtual cells and advocates for biologically grounded benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve drug discovery by developing computational models (virtual cells) that can simulate patient responses, reducing the need for costly clinical trials.

Method: The method involves a lab-in-the-loop approach for generating insights, designing virtual cells with therapeutic relevance, and using biologically grounded benchmarks.

Result: The paper presents a vision for virtual cells that predict functional responses and explain biomolecular interactions, aiming to enhance drug discovery.

Conclusion: The conclusion advocates for virtual cells as a framework for higher-level models (e.g., virtual patients) to optimize drug discovery outcomes.

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [599] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/pdf/2505.16933)
*Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li*

Main category: cs.LG

TL;DR: LLaDA-V is a diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning, showing competitive performance in multimodal tasks despite weaker textual performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of diffusion-based models in multimodal contexts, diverging from dominant autoregressive approaches.

Method: LLaDA-V combines a vision encoder and MLP connector to align visual features with language embeddings, built upon the LLaDA diffusion model.

Result: LLaDA-V achieves competitive multimodal performance, narrowing gaps with stronger models and outperforming hybrid and purely diffusion-based MLLMs.

Conclusion: Diffusion-based models like LLaDA-V show promise for multimodal tasks, warranting further research.

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [600] [Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation](https://arxiv.org/pdf/2505.17226)
*Kun Yang, Neena Imam*

Main category: cs.LG

TL;DR: Proposes ArKrum, a robust aggregation strategy for Federated Learning (FL) to counter Byzantine attacks, enhancing resilience and privacy without needing prior knowledge of malicious clients.


<details>
  <summary>Details</summary>
Motivation: FL is vulnerable to adversarial threats from Byzantine clients, and traditional methods lack robustness or require impractical prior knowledge.

Method: ArKrum combines median-based filtering to remove outliers and multi-update averaging for stability, improving on the rKrum algorithm.

Result: ArKrum outperforms other methods in accuracy and stability under various Byzantine attacks.

Conclusion: ArKrum is a practical and effective solution for secure FL in adversarial settings.

Abstract: Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.

</details>


### [601] [VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis](https://arxiv.org/pdf/2505.18570)
*Tina Khezresmaeilzadeh, Parsa Razmara, Seyedarmin Azizi, Mohammad Erfan Sadeghi, Erfan Baghaei Potraghloo*

Main category: cs.LG

TL;DR: VISTA is a training-free, multi-modal framework using Vision-Language Models (VLMs) for stock price prediction, outperforming traditional methods by up to 89.83%.


<details>
  <summary>Details</summary>
Motivation: Stock price prediction is complex and high-stakes, with existing methods often missing complementary patterns due to unimodal approaches.

Method: VISTA combines textual and visual data (line charts) in a zero-shot setting with chain-of-thought prompts for multi-modal forecasting.

Result: VISTA outperforms ARIMA and text-only LLM methods by up to 89.83%.

Conclusion: Multi-modal inference with VLMs is effective for stock forecasting, offering potential without task-specific training.

Abstract: Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.

</details>


### [602] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/pdf/2505.24434)
*Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber*

Main category: cs.LG

TL;DR: Graph Flow Matching (GFM) enhances flow matching by incorporating neighbor information via a graph neural module, improving generation quality without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing flow matching methods predict velocities independently, ignoring correlations between points, which could improve generation quality.

Method: GFM decomposes velocity into a reaction term (standard flow matching) and a diffusion term (graph neural module for neighbor aggregation).

Result: GFM improves Fréchet Inception Distance (FID) and recall across five image generation benchmarks.

Conclusion: GFM is a lightweight, effective modular enhancement for flow matching architectures.

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [603] [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/pdf/2506.00486)
*Jun Wu, Yirong Xiong, Jiangtao Wen, Yuxing Han*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for LLM optimization using generalized Gaussian distributions (GGDs), improving initialization, training, and compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LLMs, the statistical distribution of parameters and their impact on training and efficiency is understudied. The work aims to address this gap.

Method: Proposes GG-based initialization, DeepShape for post-training regularization, and RF8 for hardware-efficient 8-bit floating-point format.

Result: Experiments show smaller, faster models matching or outperforming baselines.

Conclusion: The framework advances efficient, scalable, and hardware-aware AI systems through principled statistical modeling.

Abstract: Despite rapid advancements in the research and deployment of large language
models (LLMs), the statistical distribution of model parameters, as well as
their influence on initialization, training dynamics, and downstream
efficiency, has received surprisingly little attention. A recent work
introduced BackSlash, a training-time compression algorithm. It first
demonstrated that pre-trained LLM parameters follow generalized Gaussian
distributions (GGDs) better. By optimizing GG priors during training, BackSlash
can reduce parameters by up to 90\% with minimal performance loss. Building on
this foundational insight, we propose a unified, end-to-end framework for LLM
optimization based on the GG model. Our contributions are threefold: (1)
GG-based initialization scheme that aligns with the statistical structure of
trained models, resulting in faster convergence and improved accuracy; (2)
DeepShape, a post-training regularization method that reshapes weight
distributions to match a GG profile, improving compressibility with minimized
degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit
floating-point format designed for GG-distributed-initialized BackSlash
training, enabling low-cost inference without compromising accuracy.
Experiments across diverse model architectures show that our framework
consistently yields smaller and faster models that match or outperform standard
training baselines. By grounding LLM development in principled statistical
modeling, this work forges a new path toward efficient, scalable, and
hardware-aware AI systems. The code is available on our project page:
https://huggingface.co/spaces/shifeng3711/gg_prior.

</details>


### [604] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/pdf/2505.22813)
*Josiah Couch, Miao Li, Rima Arnaout, Ramy Arnaout*

Main category: cs.LG

TL;DR: Dataset quality is an intrinsic property, independent of model architecture, size, and class balance, and correlates strongly with classifier performance.


<details>
  <summary>Details</summary>
Motivation: To determine if dataset quality is intrinsic or dependent on other factors like model architecture, size, and class balance.

Method: Created controlled datasets, trained classifiers with varied architectures (random forests, SVMs, deep networks), and analyzed performance correlations.

Result: Classifier performance strongly correlated by dataset subset across architectures (R²=0.79), indicating intrinsic dataset quality.

Conclusion: Dataset quality is an independent factor for classifier performance, alongside size, class balance, and architecture.

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [605] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/pdf/2505.23585)
*Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei*

Main category: cs.LG

TL;DR: OPO is a simplified reinforcement learning algorithm for stable and efficient training of large language models, improving performance and diversity in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning algorithms face instability and inefficiency due to loose on-policy constraints and auxiliary models.

Method: OPO introduces exact on-policy training and an optimal reward baseline to minimize gradient variance.

Result: OPO shows superior performance, stability, and diversity in responses on mathematical reasoning benchmarks.

Conclusion: OPO is a promising approach for stable and effective reinforcement learning in language model alignment and reasoning.

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO integrates a practically feasible
formulation of the optimal reward baseline that minimizes gradient variance. We
evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its
superior performance and training stability without additional models or
regularization terms. Furthermore, OPO achieves lower policy shifts and higher
output entropy, encouraging more diverse and less repetitive responses. These
results highlight OPO as a promising direction for stable and effective
reinforcement learning in large language model alignment and reasoning tasks.
The implementation is merged into the verl library at
https://verl.readthedocs.io/en/latest/algo/opo.html.

</details>


### [606] [Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/pdf/2506.00691)
*Junaid Muzaffar, Khubaib Ahmed, Ingo Frommholz, Zeeshan Pervez, Ahsan ul Haq*

Main category: cs.LG

TL;DR: The paper proposes a modified attention mechanism with non-linear key vector transformation to improve RL training efficiency without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational cost and long training times in RL by enhancing the representational capacity of attention mechanisms.

Method: Introduces a non-linear transformation of key vectors (K) in the attention mechanism, creating new vectors (K') to capture complex feature interactions.

Result: The enhanced model shows significant improvements in learning efficiency and convergence speed.

Conclusion: Non-linear attention mechanisms hold promise for advancing RL algorithms by boosting efficiency and maintaining performance.

Abstract: Training reinforcement learning (RL) agents often requires significant
computational resources and extended training times. To address this, we build
upon the foundation laid by Google Brain's Sensory Neuron, which introduced a
novel neural architecture for reinforcement learning tasks that maintained
permutation in-variance in the sensory neuron system. While the baseline model
demonstrated significant performance improvements over traditional approaches,
we identified opportunities to enhance the efficiency of the learning process
further. We propose a modified attention mechanism incorporating a non-linear
transformation of the key vectors (K) using a mapping function, resulting in a
new set of key vectors (K'). This non-linear mapping enhances the
representational capacity of the attention mechanism, allowing the model to
encode more complex feature interactions and accelerating convergence without
compromising performance. Our enhanced model demonstrates significant
improvements in learning efficiency, showcasing the potential for non-linear
attention mechanisms in advancing reinforcement learning algorithms.

</details>


### [607] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/pdf/2505.23470)
*Chenjie Li, Amir Gilad, Boris Glavic, Zhengjie Miao, Sudeepa Roy*

Main category: cs.LG

TL;DR: The paper introduces techniques to repair labeling functions (LFs) in programmatic weak supervision (PWS) using minimal changes based on a small labeled dataset, improving LF accuracy and label quality.


<details>
  <summary>Details</summary>
Motivation: The quality of labels in PWS depends on LF accuracy, but LFs may be imperfect. The goal is to fix LFs efficiently using limited labeled data.

Method: LFs are modeled as conditional rules and refined by minimally altering their outputs on labeled examples, ensuring correctness and high accuracy.

Result: Experimental results show the system enhances LF quality even with small labeled datasets.

Conclusion: The proposed method effectively repairs LFs, improving label quality in PWS with minimal labeled data.

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [608] [Normalizing Flows are Capable Models for RL](https://arxiv.org/pdf/2505.23527)
*Raj Ghugare, Benjamin Eysenbach*

Main category: cs.LG

TL;DR: The paper challenges the belief that normalizing flows (NFs) lack expressivity in RL, proposing a unified NF architecture that simplifies algorithms and improves performance across various RL tasks.


<details>
  <summary>Details</summary>
Motivation: To address the computational and representational limitations of existing probabilistic models (e.g., diffusion models, transformers) in RL by leveraging the untapped potential of NFs.

Method: Introduces a single NF architecture that serves as a policy, Q-function, and occupancy measure, integrating it into RL algorithms.

Result: The proposed NF-based approach simplifies algorithms and achieves higher performance in imitation learning, offline, goal-conditioned, and unsupervised RL.

Conclusion: NFs are a viable and effective alternative to other probabilistic models in RL, offering simplicity and superior performance.

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [609] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/pdf/2505.24603)
*Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson*

Main category: cs.LG

TL;DR: The paper refines the privacy analysis of Gaussian sketching using Renyi Differential Privacy (RDP), providing tighter bounds and demonstrating improved performance in linear regression.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding and utility of Gaussian sketching in data science by leveraging RDP for tighter privacy bounds.

Method: Revisits Gaussian sketching with RDP, providing a refined privacy analysis and applying it to linear regression.

Result: Tighter privacy bounds and improved performance in linear regression, with empirical validation across datasets.

Conclusion: The refined RDP analysis of Gaussian sketching offers stronger privacy guarantees and practical performance gains.

Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [610] [Optimistic critics can empower small actors](https://arxiv.org/pdf/2506.01016)
*Olya Mastikhina, Dhruv Sreenivas, Pablo Samuel Castro*

Main category: cs.LG

TL;DR: Smaller actors in asymmetric actor-critic methods degrade performance and cause critic overfitting, primarily due to value underestimation. Mitigating this issue can aid further research.


<details>
  <summary>Details</summary>
Motivation: To understand the implications of asymmetric actor-critic setups, particularly with smaller actors, and identify causes of performance degradation.

Method: Broad empirical investigations and analyses of asymmetric actor-critic architectures, focusing on smaller actors and their impact.

Result: Smaller actors lead to performance degradation and critic overfitting, driven by value underestimation and poor data collection.

Conclusion: Addressing value underestimation can mitigate these issues, enabling progress in asymmetric actor-critic research.

Abstract: Actor-critic methods have been central to many of the recent advances in deep
reinforcement learning. The most common approach is to use symmetric
architectures, whereby both actor and critic have the same network topology and
number of parameters. However, recent works have argued for the advantages of
asymmetric setups, specifically with the use of smaller actors. We perform
broad empirical investigations and analyses to better understand the
implications of this and find that, in general, smaller actors result in
performance degradation and overfit critics. Our analyses suggest poor data
collection, due to value underestimation, as one of the main causes for this
behavior, and further highlight the crucial role the critic can play in
alleviating this pathology. We explore techniques to mitigate the observed
value underestimation, which enables further research in asymmetric
actor-critic methods.

</details>


### [611] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/pdf/2506.02308)
*Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang*

Main category: cs.LG

TL;DR: Scaling instruction fine-tuning with larger datasets doesn't always improve performance. Grouping tasks by multimodal interactions (e.g., shared or unique information) is more effective, leading to the introduction of MINT, a task-grouping strategy that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of simply increasing instruction-tuning tasks and improve multimodal model performance by focusing on task interactions.

Method: Introduces MINT, a task-grouping strategy based on multimodal interaction types (e.g., shared information, modality selection, synergistic fusion).

Result: MINT outperforms existing baselines, balancing generalization and specialization in multimodal instruction tuning.

Conclusion: Task grouping by multimodal interactions is more effective than scaling task quantity, with MINT demonstrating superior performance.

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [612] [PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/pdf/2506.02965)
*Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: PC-MoE enables decentralized, privacy-preserving collaborative training of LLMs with reduced GPU memory usage, matching centralized model performance.


<details>
  <summary>Details</summary>
Motivation: To allow multiple parties with limited resources to collaboratively train LLMs while preserving data privacy.

Method: Leverages MoE sparsity for memory-efficient decentralized training, keeping data and gradients local.

Result: Matches centralized model performance, reduces GPU RAM by 70%, and resists reconstruction attacks.

Conclusion: PC-MoE breaks the privacy-accuracy trade-off, offering efficient, secure collaborative LLM training.

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [613] [Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs](https://arxiv.org/pdf/2506.04215)
*Alex DeWeese, Guannan Qu*

Main category: cs.MA

TL;DR: The paper introduces the Extended Cutoff Policy Class for Locally Interdependent Multi-Agent MDPs, improving performance in small visibility settings and resolving Penalty Jittering.


<details>
  <summary>Details</summary>
Motivation: Dec-POMDPs are intractable, but local assumptions enable tractable solutions. Existing policies perform poorly in small visibility settings due to Penalty Jittering.

Method: Proposes the Extended Cutoff Policy Class, which remembers agents beyond visibility, and generalizes the Locally Interdependent Multi-Agent MDP framework.

Result: Policies are exponentially close to optimal, resolve Penalty Jittering, and can guarantee joint optimal behavior under partial observability.

Conclusion: The Extended Cutoff Policy Class offers a practical, near-optimal solution for Locally Interdependent Multi-Agent MDPs, even in challenging visibility conditions.

Abstract: Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.

</details>


### [614] [RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration](https://arxiv.org/pdf/2411.07161)
*Young-Min Cho, Raphael Shu, Nilaksh Das, Tamer Alkhouli, Yi-An Lai, Jason Cai, Monica Sunkara, Yi Zhang, Dan Roth*

Main category: cs.MA

TL;DR: The paper studies how different consensus mechanisms (like majority and unanimous voting) impact collaboration quality and efficiency in Multi-Agent Systems (MAS). Majority voting leads to inefficiency, while unanimous voting performs poorly initially. Language-based early stopping improves performance and reduces rounds.


<details>
  <summary>Details</summary>
Motivation: To understand how consensus mechanisms affect collaboration in decentralized MAS, as this area is understudied.

Method: Systematic study with controlled experiments analyzing voting rules' impact on decision quality and efficiency, including qualitative analysis of agent communication.

Result: Majority voting causes inefficiency; unanimous voting has 87% lower initial performance. Language-based early stopping improves performance by 13% and reduces rounds by 50%.

Conclusion: Group decision-making mechanisms significantly impact MAS collaboration, with language-based methods offering notable improvements.

Abstract: Effective group decision-making is critical in Multi-Agent Systems (MAS).
Yet, how different mechanisms for reaching consensus impact collaboration
quality and efficiency remains understudied. We conduct a systematic study on
group decision-making mechanisms in a decentralized setting. Through controlled
experiments, we analyze how different voting rules affect decision quality and
efficiency in a multi-round collaboration. Results reveal that majority voting
often cause inefficient collaboration due to its strict acceptance criteria. At
the extreme, unanimous voting gives 87% lower initial performance than the
best-performing method. Our qualitative analysis of cross-agent communication
shows that messages become longer and more repetitive over time: while message
length increases by 84%, similarity to the previous round increases to 90%.
Based on these insights, language-based early stopping methods make the
performance 13% closer to oracle while reducing rounds by 50%. Our findings
highlight the crucial role of group decision-making in optimizing MAS
collaboration.

</details>


### [615] [From Intention To Implementation: Automating Biomedical Research via LLMs](https://arxiv.org/pdf/2412.09429)
*Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin*

Main category: cs.MA

TL;DR: BioResearcher is an AI-driven, end-to-end automated system for biomedical research, addressing challenges like multidisciplinary expertise and experimental design. It achieves a 63.07% success rate and outperforms typical systems by 22.0%.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of scientific literature and datasets makes biomedical research labor-intensive. AI, especially LLMs, can automate and streamline this process.

Method: BioResearcher uses a modular multi-agent architecture with specialized agents for search, literature processing, experimental design, and programming, along with hierarchical learning and LLM-based quality control.

Result: The system achieves a 63.07% success rate and outperforms typical agent systems by 22.0% on quality metrics.

Conclusion: BioResearcher reduces researchers' workloads and accelerates discoveries, showcasing potential for future automated research innovations.

Abstract: Conventional biomedical research is increasingly labor-intensive due to the
exponential growth of scientific literature and datasets. Artificial
intelligence (AI), particularly Large Language Models (LLMs), has the potential
to revolutionize this process by automating various steps. Still, significant
challenges remain, including the need for multidisciplinary expertise,
logicality of experimental design, and performance measurements. This paper
introduces BioResearcher, the first end-to-end automated system designed to
streamline the entire biomedical research process involving dry lab
experiments. BioResearcher employs a modular multi-agent architecture,
integrating specialized agents for search, literature processing, experimental
design, and programming. By decomposing complex tasks into logically related
sub-tasks and utilizing a hierarchical learning approach, BioResearcher
effectively addresses the challenges of multidisciplinary requirements and
logical complexity. Furthermore, BioResearcher incorporates an LLM-based
reviewer for in-process quality control and introduces novel evaluation metrics
to assess the quality and automation of experimental protocols. BioResearcher
successfully achieves an average execution success rate of 63.07% across eight
previously unmet research objectives. The generated protocols averagely
outperform typical agent systems by 22.0% on five quality metrics. The system
demonstrates significant potential to reduce researchers' workloads and
accelerate biomedical discoveries, paving the way for future innovations in
automated research systems.

</details>


### [616] [M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality](https://arxiv.org/pdf/2503.02077)
*Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du*

Main category: cs.MA

TL;DR: The paper introduces $	ext{M}^3	ext{HF}$, a framework for MARL that integrates multi-phase human feedback of mixed quality to refine agent policies, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions in MARL is challenging and often leads to suboptimal behaviors. The paper aims to address this by leveraging diverse human feedback.

Method: The framework involves iterative human feedback, pauses learning for evaluation, parses feedback using LLMs, and updates reward functions with adaptive weights and predefined templates.

Result: Empirical results show $	ext{M}^3	ext{HF}$ outperforms existing methods, improving interpretability and robustness in multi-agent cooperation.

Conclusion: $	ext{M}^3	ext{HF}$ effectively addresses reward design complexities in MARL and enables broader human participation in training.

Abstract: Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
($\text{M}^3\text{HF}$), a novel framework that integrates multi-phase human
feedback of mixed quality into the MARL training process. By involving humans
with diverse expertise levels to provide iterative guidance,
$\text{M}^3\text{HF}$ leverages both expert and non-expert feedback to
continuously refine agents' policies. During training, we strategically pause
agent learning for human evaluation, parse feedback using large language models
to assign it appropriately and update reward functions through predefined
templates and adaptive weights by using weight decay and performance-based
adjustments. Our approach enables the integration of nuanced human insights
across various levels of quality, enhancing the interpretability and robustness
of multi-agent cooperation. Empirical results in challenging environments
demonstrate that $\text{M}^3\text{HF}$ significantly outperforms
state-of-the-art methods, effectively addressing the complexities of reward
design in MARL and enabling broader human participation in the training
process.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [617] [How Far Are We from Predicting Missing Modalities with Foundation Models?](https://arxiv.org/pdf/2506.03530)
*Guanzhou Ke, Yi Xie, Xiaoli Wang, Guoqing Chao, Bo Wang, Shengfeng He*

Main category: cs.MM

TL;DR: The paper explores the use of multimodal foundation models for missing modality prediction, identifies their limitations, and proposes an agentic framework with a self-refinement mechanism to improve accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored potential of multimodal foundation models in predicting missing modalities and their shortcomings in fine-grained semantic extraction and robust validation.

Method: Categorizes existing approaches into three paradigms, evaluates 42 model variants, and introduces an agentic framework with modality-aware mining strategies and a self-refinement mechanism.

Result: The proposed method reduces FID for missing image prediction by 14% and MER for missing text prediction by 10% compared to baselines.

Conclusion: The agentic framework and self-refinement mechanism effectively enhance missing modality prediction, addressing key limitations of current foundation models.

Abstract: Multimodal foundation models have demonstrated impressive capabilities across
diverse tasks. However, their potential as plug-and-play solutions for missing
modality prediction remains underexplored. To investigate this, we categorize
existing approaches into three representative paradigms, encompassing a total
of 42 model variants, and conduct a comprehensive evaluation in terms of
prediction accuracy and adaptability to downstream tasks. Our analysis reveals
that current foundation models often fall short in two critical aspects: (i)
fine-grained semantic extraction from the available modalities, and (ii) robust
validation of generated modalities. These limitations lead to suboptimal and,
at times, misaligned predictions. To address these challenges, we propose an
agentic framework tailored for missing modality prediction. This framework
dynamically formulates modality-aware mining strategies based on the input
context, facilitating the extraction of richer and more discriminative semantic
features. In addition, we introduce a \textit{self-refinement mechanism}, which
iteratively verifies and enhances the quality of generated modalities through
internal feedback. Experimental results show that our method reduces FID for
missing image prediction by at least 14% and MER for missing text prediction by
at least 10% compared to baselines.

</details>


### [618] [Sonic: Shifting Focus to Global Audio Perception in Portrait Animation](https://arxiv.org/pdf/2411.16331)
*Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, Chengjie Wang*

Main category: cs.MM

TL;DR: The paper introduces Sonic, a novel paradigm for talking face generation by focusing on global audio perception, outperforming existing methods in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on visual signals, leading to unnatural results. Audio signals are ideal for driving facial animations without visual interference.

Method: Sonic disentangles global audio into intra- and inter-clip perception, using context-enhanced learning, motion-decoupled control, and time-aware fusion.

Result: Sonic outperforms state-of-the-art methods in video quality, temporal consistency, lip sync precision, and motion diversity.

Conclusion: Focusing on global audio perception improves talking face generation, offering a more natural and consistent approach.

Abstract: The study of talking face generation mainly explores the intricacies of
synchronizing facial movements and crafting visually appealing,
temporally-coherent animations. However, due to the limited exploration of
global audio perception, current approaches predominantly employ auxiliary
visual and spatial knowledge to stabilize the movements, which often results in
the deterioration of the naturalness and temporal inconsistencies.Considering
the essence of audio-driven animation, the audio signal serves as the ideal and
unique priors to adjust facial expressions and lip movements, without resorting
to interference of any visual signals. Based on this motivation, we propose a
novel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of
global audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,
we disentangle it into intra- and inter-clip audio perception and collaborate
with both aspects to enhance overall perception.For the intra-clip audio
perception, 1). \textbf{Context-enhanced audio learning}, in which long-range
intra-clip temporal audio knowledge is extracted to provide facial expression
and lip motion priors implicitly expressed as the tone and speed of speech. 2).
\textbf{Motion-decoupled controller}, in which the motion of the head and
expression movement are disentangled and independently controlled by
intra-audio clips. Most importantly, for inter-clip audio perception, as a
bridge to connect the intra-clips to achieve the global perception,
\textbf{Time-aware position shift fusion}, in which the global inter-clip audio
information is considered and fused for long-audio inference via through
consecutively time-aware shifted windows. Extensive experiments demonstrate
that the novel audio-driven paradigm outperform existing SOTA methodologies in
terms of video quality, temporally consistency, lip synchronization precision,
and motion diversity.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [619] [SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer](https://arxiv.org/pdf/2506.03378)
*Orchid Chetia Phukan, Mohd Mujtaba Akhtar, Girish, Swarup Ranjan Behera, Abu Osama Siddiqui, Sarthak Jain, Priyabrata Mallick, Jaya Sai Kiran Patibandla, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: SNIFR combines audio and visual cues for fine-grained harmful content detection in videos, outperforming unimodal and baseline methods.


<details>
  <summary>Details</summary>
Motivation: The rise in child viewership on video platforms necessitates better detection of harmful content, which is often hidden in minimal frames. Audio features are underexplored in prior work.

Method: SNIFR uses a transformer encoder for intra-modality interaction and a cascaded cross-transformer for inter-modality alignment of audio and visual cues.

Result: SNIFR achieves superior performance, setting a new state-of-the-art in harmful content detection.

Conclusion: Integrating audio with visual cues enhances detection accuracy, making SNIFR a promising solution for child safety on video platforms.

Abstract: As video-sharing platforms have grown over the past decade, child viewership
has surged, increasing the need for precise detection of harmful content like
violence or explicit scenes. Malicious users exploit moderation systems by
embedding unsafe content in minimal frames to evade detection. While prior
research has focused on visual cues and advanced such fine-grained detection,
audio features remain underexplored. In this study, we embed audio cues with
visual for fine-grained child harmful content detection and introduce SNIFR, a
novel framework for effective alignment. SNIFR employs a transformer encoder
for intra-modality interaction, followed by a cascaded cross-transformer for
inter-modality alignment. Our approach achieves superior performance over
unimodal and baseline fusion methods, setting a new state-of-the-art.

</details>


### [620] [Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models](https://arxiv.org/pdf/2506.03364)
*Orchid Chetia Phukan, Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Priyabrata Mallick, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: The paper introduces singing voice deepfake source attribution (SVDSA) and proposes using multimodal foundation models (MMFMs) for superior performance. A novel framework, COFFE, with Chernoff Distance loss, outperforms individual models and baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of attributing deepfake singing voices to their sources by leveraging multimodal foundation models for capturing unique characteristics like timbre and synthesis artifacts.

Method: Utilizes MMFMs (e.g., ImageBind, LanguageBind) and proposes COFFE, a framework with Chernoff Distance loss for effective fusion of foundation models.

Result: MMFMs are most effective for SVDSA, and COFFE achieves top performance compared to individual models and baseline fusion methods.

Conclusion: Multimodal foundation models and the COFFE framework significantly improve singing voice deepfake source attribution, setting a new benchmark for the task.

Abstract: In this work, we introduce the task of singing voice deepfake source
attribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs)
such as ImageBind, LanguageBind will be most effective for SVDSA as they are
better equipped for capturing subtle source-specific characteristics-such as
unique timbre, pitch manipulation, or synthesis artifacts of each singing voice
deepfake source due to their cross-modality pre-training. Our experiments with
MMFMs, speech foundation models and music foundation models verify the
hypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired
from related research, we also explore fusion of foundation models (FMs) for
improved SVDSA. To this end, we propose a novel framework, COFFE which employs
Chernoff Distance as novel loss function for effective fusion of FMs. Through
COFFE with the symphony of MMFMs, we attain the topmost performance in
comparison to all the individual FMs and baseline fusion methods.

</details>


### [621] [HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition](https://arxiv.org/pdf/2506.03403)
*Orchid Chetia Phukan, Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: HYFuse, a novel framework, fuses compression-based (CBR) and representation-learning-based (RLR) audio representations in hyperbolic space, achieving state-of-the-art performance in Speech Emotion Recognition (SER).


<details>
  <summary>Details</summary>
Motivation: Previous SER research explored CBRs and RLRs separately, but their fusion was unexplored. The study hypothesizes that combining them provides complementary information for better performance.

Method: Proposes HYFuse, a framework that transforms and fuses CBRs (Soundstream) and RLRs (x-vector) in hyperbolic space.

Result: HYFuse outperforms individual representations and homogeneous fusion methods, achieving state-of-the-art results.

Conclusion: Fusing CBRs and RLRs in hyperbolic space enhances SER performance, demonstrating the effectiveness of complementary information fusion.

Abstract: Compression-based representations (CBRs) from neural audio codecs such as
EnCodec capture intricate acoustic features like pitch and timbre, while
representation-learning-based representations (RLRs) from pre-trained models
trained for speech representation learning such as WavLM encode high-level
semantic and prosodic information. Previous research on Speech Emotion
Recognition (SER) has explored both, however, fusion of CBRs and RLRs haven't
been explored yet. In this study, we solve this gap and investigate the fusion
of RLRs and CBRs and hypothesize they will be more effective by providing
complementary information. To this end, we propose, HYFuse, a novel framework
that fuses the representations by transforming them to hyperbolic space. With
HYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the
top performance in comparison to individual representations as well as the
homogeneous fusion of RLRs and CBRs and report SOTA.

</details>


### [622] [A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations](https://arxiv.org/pdf/2506.03425)
*Petr Grinberg, Ankur Kumar, Surya Koppisetti, Gaurav Bharaj*

Main category: eess.AS

TL;DR: Proposes a data-driven method using a diffusion model to identify deepfake audio artifacts, outperforming SHAP and LRP.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating explainability techniques like SHAP and LRP for audio deepfake detection due to lack of ground truth.

Method: Uses paired real and vocoded audio, with time-frequency differences as ground truth, to train a diffusion model.

Result: Outperforms traditional explainability techniques on VocV4 and LibriSeVoc datasets.

Conclusion: The proposed method effectively exposes deepfake artifacts, offering better explanations than existing techniques.

Abstract: Evaluating explainability techniques, such as SHAP and LRP, in the context of
audio deepfake detection is challenging due to lack of clear ground truth
annotations. In the cases when we are able to obtain the ground truth, we find
that these methods struggle to provide accurate explanations. In this work, we
propose a novel data-driven approach to identify artifact regions in deepfake
audio. We consider paired real and vocoded audio, and use the difference in
time-frequency representation as the ground-truth explanation. The difference
signal then serves as a supervision to train a diffusion model to expose the
deepfake artifacts in a given vocoded audio. Experimental results on the VocV4
and LibriSeVoc datasets demonstrate that our method outperforms traditional
explainability techniques, both qualitatively and quantitatively.

</details>


### [623] [BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing](https://arxiv.org/pdf/2506.03515)
*Masaya Kawamura, Takuya Hasumi, Yuma Shirahata, Ryuichi Yamamoto*

Main category: eess.AS

TL;DR: A compact TTS model using quantization-aware training and weight indexing reduces size by 83% while improving synthesis quality.


<details>
  <summary>Details</summary>
Motivation: To enable efficient on-device TTS applications by minimizing model size without compromising performance.

Method: Uses quantization-aware training (QAT) to reduce parameters to 1.58-bit and weight indexing for efficient storage.

Result: Achieved 83% model size reduction and better synthesis quality than non-quantized baselines.

Conclusion: The proposed techniques enable highly compact and efficient TTS models for on-device use.

Abstract: This paper proposes a highly compact, lightweight text-to-speech (TTS) model
for on-device applications. To reduce the model size, the proposed model
introduces two techniques. First, we introduce quantization-aware training
(QAT), which quantizes model parameters during training to as low as 1.58-bit.
In this case, most of 32-bit model parameters are quantized to ternary values
{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,
we save a group of 1.58-bit weights as a single int8 index. This allows for
efficient storage of model parameters, even on hardware that treats values in
units of 8-bit. Experimental results demonstrate that the proposed method
achieved 83 % reduction in model size, while outperforming the baseline of
similar model size without quantization in synthesis quality.

</details>


### [624] [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/pdf/2506.03606)
*Parismita Gogoi, Sishir Kalita, Wendy Lalhminghlui, Viyazonuo Terhiija, Moakala Tzudir, Priyankoo Sarmah, S. R. M. Prasanna*

Main category: eess.AS

TL;DR: The study evaluates SSL models (Wav2vec2.0) for tone recognition in low-resource languages (Angami, Ao, Mizo), finding Mizo performs best and middle layers are most critical. Tone inventory and dialectal variations impact performance.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of SSL models for tone recognition in low-resource tonal languages and understand the role of model layers and linguistic factors.

Method: Evaluated four Wav2vec2.0 base models pre-trained on tonal/non-tonal languages, analyzing tone-wise performance across layers for Angami, Ao, and Mizo.

Result: Mizo had the best tone recognition, Angami the worst. Middle layers were most important. Tone inventory and dialectal variations affected performance.

Conclusion: SSL models show promise for tone recognition in low-resource languages, with middle layers being key. Linguistic factors like tone inventory and dialects influence results.

Abstract: This study explores the use of self-supervised learning (SSL) models for tone
recognition in three low-resource languages from North Eastern India: Angami,
Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on
both tonal and non-tonal languages. We analyze tone-wise performance across the
layers for all three languages and compare the different models. Our results
show that tone recognition works best for Mizo and worst for Angami. The middle
layers of the SSL models are the most important for tone recognition,
regardless of the pre-training language, i.e. tonal or non-tonal. We have also
found that the tone inventory, tone types, and dialectal variations affect tone
recognition. These findings provide useful insights into the strengths and
weaknesses of SSL-based embeddings for tonal languages and highlight the
potential for improving tone recognition in low-resource settings. The source
code is available at GitHub 1 .

</details>


### [625] [Sound Field Reconstruction Using Physics-Informed Boundary Integral Networks](https://arxiv.org/pdf/2506.03917)
*Stefano Damiano, Toon van Waterschoot*

Main category: eess.AS

TL;DR: A boundary integral network is introduced for sound field reconstruction, outperforming existing physics-informed methods.


<details>
  <summary>Details</summary>
Motivation: To improve sound field reconstruction using limited measurements by leveraging boundary integral equations.

Method: Uses a shallow neural network to model boundary pressure via the Kirchhoff-Helmholtz equation, trained with microphone measurements.

Result: The model outperforms existing physics-informed data-driven techniques in accuracy.

Conclusion: The boundary integral network is effective for sound field reconstruction, offering better performance than current methods.

Abstract: Sound field reconstruction refers to the problem of estimating the acoustic
pressure field over an arbitrary region of space, using only a limited set of
measurements. Physics-informed neural networks have been adopted to solve the
problem by incorporating in the training loss function the governing partial
differential equation, either the Helmholtz or the wave equation. In this work,
we introduce a boundary integral network for sound field reconstruction.
Relying on the Kirchhoff-Helmholtz boundary integral equation to model the
sound field in a given region of space, we employ a shallow neural network to
retrieve the pressure distribution on the boundary of the considered domain,
enabling to accurately retrieve the acoustic pressure inside of it. Assuming
the positions of measurement microphones are known, we train the model by
minimizing the mean squared error between the estimated and measured pressure
at those locations. Experimental results indicate that the proposed model
outperforms existing physics-informed data-driven techniques.

</details>


### [626] [HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset](https://arxiv.org/pdf/2506.04152)
*Ryan Langman, Xuesong Yang, Paarth Neekhara, Shehzeen Hussain, Edresson Casanova, Evelina Bakhturina, Jason Li*

Main category: eess.AS

TL;DR: HiFiTTS-2 is a large-scale speech dataset for high-bandwidth TTS, derived from LibriVox audiobooks, with extensive processing and metadata.


<details>
  <summary>Details</summary>
Motivation: To provide a high-quality, large-scale dataset for training high-bandwidth TTS models.

Method: Data processing pipeline includes bandwidth estimation, segmentation, text preprocessing, and multi-speaker detection.

Result: The dataset (36.7k hours for 22.05 kHz, 31.7k hours for 44.1 kHz) supports high-quality, zero-shot TTS training.

Conclusion: HiFiTTS-2 and its pipeline enable effective training of high-bandwidth TTS models.

Abstract: This paper introduces HiFiTTS-2, a large-scale speech dataset designed for
high-bandwidth speech synthesis. The dataset is derived from LibriVox
audiobooks, and contains approximately 36.7k hours of English speech for 22.05
kHz training, and 31.7k hours for 44.1 kHz training. We present our data
processing pipeline, including bandwidth estimation, segmentation, text
preprocessing, and multi-speaker detection. The dataset is accompanied by
detailed utterance and audiobook metadata generated by our pipeline, enabling
researchers to apply data quality filters to adapt the dataset to various use
cases. Experimental results demonstrate that our data pipeline and resulting
dataset can facilitate the training of high-quality, zero-shot text-to-speech
(TTS) models at high bandwidths.

</details>


### [627] [Language-Codec: Bridging Discrete Codec Representations and Speech Language Models](https://arxiv.org/pdf/2402.12208)
*Shengpeng Ji, Minghui Fang, Jialong Zuo, Ziyue Jiang, Dingdong Wang, Hanting Wang, Hai Huang, Zhou Zhao*

Main category: eess.AS

TL;DR: The paper introduces Language-Codec, a method addressing gaps between discrete acoustic codecs and downstream speech language models, using MCRVQ and improved structures for better performance.


<details>
  <summary>Details</summary>
Motivation: Discrete acoustic codecs have gaps in handling weakly supervised signals and burdening downstream models, prompting the need for a better solution.

Method: Proposes Language-Codec with Masked Channel Residual Vector Quantization (MCRVQ), improved Fourier transforms, attention blocks, and refined discriminator design.

Result: Outperforms competing audio compression algorithms in evaluations and validates efficiency in downstream tasks.

Conclusion: Language-Codec effectively bridges gaps and enhances performance, with code and models available for access.

Abstract: In recent years, large language models have achieved significant success in
generative tasks related to speech, audio, music, and other signal domains. A
crucial element of these models is the discrete acoustic codecs, which serve as
an intermediate representation replacing the mel-spectrogram. However, there
exist several gaps between discrete codecs and downstream speech language
models. Specifically, 1) Due to the reconstruction paradigm of the Codec model
and the structure of residual vector quantization, the initial channel of the
codebooks contains excessive information, making it challenging to directly
generate acoustic tokens from weakly supervised signals such as text in
downstream tasks. 2) numerous codebooks increases the burden on downstream
speech language models. Consequently, leveraging the characteristics of speech
language models, we propose Language-Codec. In the Language-Codec, we introduce
a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with
improved fourier transform structures and attention blocks, refined
discriminator design to address the aforementioned gaps. We compare our method
with competing audio compression algorithms and observe significant
outperformance across extensive evaluations. Furthermore, we also validate the
efficiency of the Language-Codec on downstream speech language models. The
source code and pre-trained models can be accessed at
https://github.com/jishengpeng/languagecodec .

</details>


### [628] [ControlSpeech: Towards Simultaneous and Independent Zero-shot Speaker Cloning and Zero-shot Language Style Control](https://arxiv.org/pdf/2406.01205)
*Shengpeng Ji, Qian Chen, Wen Wang, Jialong Zuo, Minghui Fang, Ziyue Jiang, Hai Huang, Zehan Wang, Xize Cheng, Siqi Zheng, Zhou Zhao*

Main category: eess.AS

TL;DR: ControlSpeech is a TTS system that clones voices and controls speaking styles, addressing limitations of prior models by decoupling timbre, content, and style using bidirectional attention and parallel decoding.


<details>
  <summary>Details</summary>
Motivation: Prior TTS models lack combined control of voice cloning and style adjustment. ControlSpeech aims to achieve both simultaneously.

Method: Uses speech, content, and style prompts with bidirectional attention and mask-based parallel decoding. Introduces SMSD for style control.

Result: Achieves SOTA performance in controllability, timbre similarity, audio quality, robustness, and generalizability.

Conclusion: ControlSpeech successfully integrates voice cloning and style control, validated by experiments and a new dataset.

Abstract: In this paper, we present ControlSpeech, a text-to-speech (TTS) system
capable of fully cloning the speaker's voice and enabling arbitrary control and
adjustment of speaking style. Prior zero-shot TTS models only mimic the
speaker's voice without further control and adjustment capabilities while prior
controllable TTS models cannot perform speaker-specific voice generation.
Therefore, ControlSpeech focuses on a more challenging task: a TTS system with
controllable timbre, content, and style at the same time. ControlSpeech takes
speech prompts, content prompts, and style prompts as inputs and utilizes
bidirectional attention and mask-based parallel decoding to capture codec
representations corresponding to timbre, content, and style in a discrete
decoupling codec space. Moreover, we analyze the many-to-many issue in textual
style control and propose the Style Mixture Semantic Density (SMSD) module,
which is based on Gaussian mixture density networks, to resolve this problem.
To facilitate empirical validations, we make available a new style controllable
dataset called VccmDataset. Our experimental results demonstrate that
ControlSpeech exhibits comparable or state-of-the-art (SOTA) performance in
terms of controllability, timbre similarity, audio quality, robustness, and
generalizability. The relevant code and demo are available at
https://github.com/jishengpeng/ControlSpeech .

</details>


### [629] [Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs](https://arxiv.org/pdf/2406.05298)
*Ryan Langman, Ante Jukić, Kunal Dhawan, Nithin Rao Koluguri, Jason Li*

Main category: eess.AS

TL;DR: A spectral codec using Finite Scalar Quantization (FSQ) is proposed for speech synthesis, offering comparable quality to existing audio codecs while improving parallel TTS model performance.


<details>
  <summary>Details</summary>
Motivation: Traditional speech models rely on mel-spectrograms, but discrete audio tokens from neural codecs are complex for some TTS models, requiring large autoregressive models.

Method: The proposed spectral codec uses FSQ to compress mel-spectrograms and reconstruct audio, avoiding the complexity of Residual Vector Quantization (RVQ).

Result: Objective metrics and subjective tests show comparable perceptual quality to existing codecs.

Conclusion: FSQ and spectral representations enhance parallel TTS model performance.

Abstract: Historically, most speech models in machine-learning have used the
mel-spectrogram as a speech representation. Recently, discrete audio tokens
produced by neural audio codecs have become a popular alternate speech
representation for speech synthesis tasks such as text-to-speech (TTS).
However, the data distribution produced by such codecs is too complex for some
TTS models to predict, typically requiring large autoregressive models to get
good quality. Most existing audio codecs use Residual Vector Quantization (RVQ)
to compress and reconstruct the time-domain audio signal. We propose a spectral
codec which uses Finite Scalar Quantization (FSQ) to compress the
mel-spectrogram and reconstruct the time-domain audio signal. A study of
objective audio quality metrics and subjective listening tests suggests that
our spectral codec has comparable perceptual quality to equivalent audio
codecs. We show that FSQ, and the use of spectral speech representations, can
both improve the performance of parallel TTS models.

</details>


### [630] [Benchmarking Audio Deepfake Detection Robustness in Real-world Communication Scenarios](https://arxiv.org/pdf/2504.12423)
*Haohan Shi, Xiyu Shi, Safak Dogan, Saif Alzubi, Tianjin Huang, Yunxiao Zhang*

Main category: eess.AS

TL;DR: The paper addresses the challenge of audio deepfake detection (ADD) systems struggling with degraded audio quality in real-world scenarios. It introduces ADD-C, a benchmark dataset, and a data augmentation strategy to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing ADD systems fail to generalize well due to audio quality degradation from codec compression and transmission effects.

Method: Developed the ADD-C benchmark dataset and proposed a novel data augmentation strategy to enhance ADD system robustness.

Result: Benchmarking showed baseline models' performance declined under communication conditions, but the proposed DA strategy improved robustness.

Conclusion: The ADD-C benchmark and DA strategy can help build more practical and robust ADD systems.

Abstract: Existing Audio Deepfake Detection (ADD) systems often struggle to generalise
effectively due to the significantly degraded audio quality caused by audio
codec compression and channel transmission effects in real-world communication
scenarios. To address this challenge, we developed a rigorous benchmark to
evaluate the performance of the ADD system under such scenarios. We introduced
ADD-C, a new test dataset to evaluate the robustness of ADD systems under
diverse communication conditions, including different combinations of audio
codecs for compression and packet loss rates. Benchmarking three baseline ADD
models on the ADD-C dataset demonstrated a significant decline in robustness
under such conditions. A novel Data Augmentation (DA) strategy was proposed to
improve the robustness of ADD systems. Experimental results demonstrated that
the proposed approach significantly enhances the performance of ADD systems on
the proposed ADD-C dataset. Our benchmark can assist future efforts towards
building practical and robustly generalisable ADD systems.

</details>


### [631] [Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives](https://arxiv.org/pdf/2505.15965)
*Gowtham Premananth, Vinith Kugathasan, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The paper explores accented English speech, highlighting its differences from native speech and proposing a method to quantify accent strength without phonetic transcriptions.


<details>
  <summary>Details</summary>
Motivation: The rise of non-native accented speech in global interactions challenges AI-driven speech systems, which are typically trained on native speech datasets.

Method: The study uses articulatory and acoustic analysis, eigenspectra, and Vocal Tract Variable-based coordination features to analyze accented speech.

Result: Findings show simpler coordination patterns and higher average pitch in accented speech, with a method to quantify accent strength efficiently.

Conclusion: The research opens new avenues for studying accent impacts on intelligibility and aids in developing inclusive speech-processing systems.

Abstract: Advancements in AI-driven speech-based applications have transformed diverse
industries ranging from healthcare to customer service. However, the increasing
prevalence of non-native accented speech in global interactions poses
significant challenges for speech-processing systems, which are often trained
on datasets dominated by native speech. This study investigates accented
English speech through articulatory and acoustic analysis, identifying simpler
coordination patterns and higher average pitch than native speech. Using
eigenspectra and Vocal Tract Variable-based coordination features, we establish
an efficient method for quantifying accent strength without relying on
resource-intensive phonetic transcriptions. Our findings provide a new avenue
for research on the impacts of accents on speech intelligibility and offer
insights for developing inclusive, robust speech processing systems that
accommodate diverse linguistic communities.

</details>


### [632] [Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation](https://arxiv.org/pdf/2505.16044)
*Gowtham Premananth, Philip Resnik, Sonia Bansal, Deanna L. Kelly, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The study proposes a multimodal deep learning approach to estimate individual symptom severity in schizophrenia, moving beyond binary classification for better clinical utility.


<details>
  <summary>Details</summary>
Motivation: Traditional binary classification oversimplifies schizophrenia, limiting clinical applicability. This study aims to capture detailed symptom profiles for improved diagnosis and treatment.

Method: Unimodal models for speech, video, and text are developed, followed by a multimodal framework to enhance accuracy and robustness.

Result: The approach provides a more detailed symptom profile, improving diagnostic precision and supporting personalized treatment.

Conclusion: This scalable, objective tool enhances mental health assessment by addressing the complexity of schizophrenia.

Abstract: Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.

</details>


### [633] [Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling](https://arxiv.org/pdf/2505.19931)
*Qixi Zheng, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiaofei Wang, Kai Yu, Xie Chen*

Main category: eess.AS

TL;DR: Fast F5-TTS accelerates flow-matching-based TTS models by reducing sampling steps with Empirically Pruned Step Sampling (EPSS), achieving 4x speedup without performance loss.


<details>
  <summary>Details</summary>
Motivation: Flow-matching-based TTS models suffer from slow inference due to multiple sampling steps. Reducing steps can improve efficiency.

Method: Introduces EPSS, a non-uniform time-step sampling strategy, to prune redundant steps in F5-TTS.

Result: Achieves 7-step generation with 0.030 RTF, 4x faster than F5-TTS, while maintaining performance. Generalizes well to E2 TTS.

Conclusion: EPSS is a training-free, effective method to speed up flow-matching-based TTS models without compromising quality.

Abstract: Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS,
and F5-TTS, have attracted significant attention in recent years. These models
require multiple sampling steps to reconstruct speech from noise, making
inference speed a key challenge. Reducing the number of sampling steps can
greatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a
training-free approach to accelerate the inference of flow-matching-based TTS
models. By inspecting the sampling trajectory of F5-TTS, we identify redundant
steps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform
time-step sampling strategy that effectively reduces the number of sampling
steps. Our approach achieves a 7-step generation with an inference RTF of 0.030
on an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS
while maintaining comparable performance. Furthermore, EPSS performs well on E2
TTS models, demonstrating its strong generalization ability.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [634] [Adaptive and Robust Image Processing on CubeSats](https://arxiv.org/pdf/2506.03152)
*Robert Bayer, Julian Priest, Daniel Kjellberg, Jeppe Lindhard, Nikolaj Sørenesen, Nicolaj Valsted, Ívar Óli, Pınar Tözün*

Main category: eess.IV

TL;DR: The paper introduces DIPP and DISH, two systems for flexible and efficient image processing on resource-constrained CubeSats. DIPP enables adaptable pipelines, while DISH optimizes workload scheduling. Both systems show minimal overhead and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: CubeSats' resource constraints and space environment limit the flexibility and complexity of image processing pipelines. The paper aims to address these challenges.

Method: DIPP is a modular framework for adaptable pipelines, and DISH is a DSL/runtime for efficient scheduling on low-power processors. Experiments evaluate their performance.

Result: DIPP reduces network needs for updates and handles errors robustly. DISH matches Lua's expressiveness with lower memory use.

Conclusion: DIPP and DISH effectively enhance CubeSat image processing, balancing adaptability and efficiency.

Abstract: CubeSats offer a low-cost platform for space research, particularly for Earth
observation. However, their resource-constrained nature and being in space,
challenge the flexibility and complexity of the deployed image processing
pipelines and their orchestration. This paper introduces two novel systems,
DIPP and DISH, to address these challenges. DIPP is a modular and configurable
image processing pipeline framework that allows for adaptability to changing
mission goals even after deployment, while preserving robustness. DISH is a
domain-specific language (DSL) and runtime system designed to schedule complex
imaging workloads on low-power and memory-constrained processors.
  Our experiments demonstrate that DIPP's decomposition of the processing
pipelines adds negligible overhead, while significantly reducing the network
requirements of updating pipelines and being robust against erroneous module
uploads. Furthermore, we compare DISH to Lua, a general purpose scripting
language, and demonstrate its comparable expressiveness and lower memory
requirement.

</details>


### [635] [Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view](https://arxiv.org/pdf/2506.03175)
*Youshen Xiao, Yiling Shi, Ruixi Sun, Hongjiang Wei, Fei Gao, Yuyao Zhang*

Main category: eess.IV

TL;DR: The paper proposes an Implicit Neural Representation (INR)-based method to enhance dynamic photoacoustic image reconstruction from sparse sensor data, improving temporal resolution and reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: Traditional photoacoustic image reconstruction methods struggle with sparse sensor data and ignore inter-frame relationships in dynamic imaging, leading to artifacts and limited temporal resolution.

Method: The authors use INR to represent dynamic images as implicit functions, encoding them into a neural network trained solely on sparse sensor data, without external datasets. The method incorporates implicit continuity regularization and explicit low-rank/sparsity constraints.

Result: The proposed method outperforms traditional techniques under sparse conditions, suppressing artifacts and maintaining image quality.

Conclusion: INR-based reconstruction offers a promising solution for dynamic photoacoustic imaging, addressing sparsity and temporal resolution challenges without requiring prior data.

Abstract: Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging
technique for monitoring physiological processes, capable of providing
high-contrast images of optical absorption at much greater depths than
traditional optical imaging methods. However, practical instrumentation and
geometric constraints limit the number of acoustic sensors available around the
imaging target, leading to sparsity in sensor data. Traditional photoacoustic
(PA) image reconstruction methods, when directly applied to sparse PA data,
produce severe artifacts. Additionally, these traditional methods do not
consider the inter-frame relationships in dynamic imaging. Temporal resolution
is crucial for dynamic photoacoustic imaging, which is fundamentally limited by
the low repetition rate (e.g., 20 Hz) and high cost of high-power laser
technology. Recently, Implicit Neural Representation (INR) has emerged as a
powerful deep learning tool for solving inverse problems with sparse data, by
characterizing signal properties as continuous functions of their coordinates
in an unsupervised manner. In this work, we propose an INR-based method to
improve dynamic photoacoustic image reconstruction from sparse-views and
enhance temporal resolution, using only spatiotemporal coordinates as input.
Specifically, the proposed INR represents dynamic photoacoustic images as
implicit functions and encodes them into a neural network. The weights of the
network are learned solely from the acquired sparse sensor data, without the
need for external training datasets or prior images. Benefiting from the strong
implicit continuity regularization provided by INR, as well as explicit
regularization for low-rank and sparsity, our proposed method outperforms
traditional reconstruction methods under two different sparsity conditions,
effectively suppressing artifacts and ensuring image quality.

</details>


### [636] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/pdf/2506.03177)
*Isarun Chamveha, Supphanut Chaiyungyuen, Sasinun Worakriangkrai, Nattawadee Prasawang, Warasinee Chaisangmongkon, Pornpim Korpraphong, Voraparee Suvannarerg, Shanigarn Thiravit, Chalermdej Kannawat, Kewalin Rungsinaporn, Suwara Issaragrisil, Payia Chadbunchachai, Pattiya Gatechumpol, Chawiporn Muktabhant, Patarachai Sereerat*

Main category: eess.IV

TL;DR: A deep learning system using modified EfficientNetV2 with enhanced attention mechanisms for breast cancer detection in mammography, validated on diverse datasets with strong performance and clinical acceptance.


<details>
  <summary>Details</summary>
Motivation: To develop an effective deep learning system for breast cancer detection in mammography, improving accuracy and clinical workflow.

Method: Modified EfficientNetV2 architecture with enhanced attention mechanisms, trained on mammograms from a Thai medical center and validated on three distinct datasets.

Result: Achieved AUROCs of 0.89, 0.96, and 0.94 on respective datasets; strong lesion localization and high concordance with radiologists (83.5%-84.0% for biopsy-confirmed cases).

Conclusion: The system is effective for mammogram interpretation, with potential to enhance breast cancer screening workflows clinically.

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [637] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/pdf/2506.03178)
*Md. Zihad Bin Jahangir, Muhammad Ashad Kabir, Sumaiya Akter, Israt Jahan, Minh Chau*

Main category: eess.IV

TL;DR: LLaMA-XR, a novel framework combining LLaMA 3.1 with DenseNet-121 and QLoRA fine-tuning, improves automated radiology report generation by enhancing coherence, clinical accuracy, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Automated radiology report generation can reduce workload and improve diagnostic accuracy, but existing models struggle with accuracy and contextual relevance.

Method: LLaMA-XR integrates LLaMA 3.1 with DenseNet-121-based image embeddings and QLoRA fine-tuning, optimizing parameter utilization and reducing memory overhead.

Result: LLaMA-XR achieves ROUGE-L 0.433 and METEOR 0.336 on the IU X-ray dataset, outperforming state-of-the-art methods.

Conclusion: LLaMA-XR is an efficient and effective AI system for automated radiology reporting, offering enhanced clinical utility and reliability.

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


### [638] [Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning](https://arxiv.org/pdf/2506.03181)
*Wangting Zhou, Jiangshan He, Tong Cai, Lin Wang, Zhen Yuan, Xunbin Wei, Xueli Chen*

Main category: eess.IV

TL;DR: A lightweight siamese network (Dc-EEMF) is proposed to extend the depth-of-field (DoF) in photoacoustic microscopy (PAM), improving image fusion without sacrificing lateral resolution.


<details>
  <summary>Details</summary>
Motivation: Conventional OR-PAM has a limited DoF due to Gaussian beam constraints, hindering detailed depth resolution.

Method: Dc-EEMF uses a siamese network with artifact-resistant channel-wise spatial frequency for feature fusion and a U-Net-based perceptual loss function for decision-level focus properties.

Result: The method achieves robust fusion results for PAM images, maintaining lateral resolution.

Conclusion: Dc-EEMF-powered PAM is promising for preclinical and clinical studies requiring extended DoF.

Abstract: Photoacoustic microscopy holds the potential to measure biomarkers'
structural and functional status without labels, which significantly aids in
comprehending pathophysiological conditions in biomedical research. However,
conventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered
by a limited depth-of-field (DoF) due to the narrow depth range focused on a
Gaussian beam. Consequently, it fails to resolve sufficient details in the
depth direction. Herein, we propose a decision-level constrained end-to-end
multi-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method
is a lightweight siamese network that incorporates an artifact-resistant
channel-wise spatial frequency as its feature fusion rule. The meticulously
crafted U-Net-based perceptual loss function for decision-level focus
properties in end-to-end fusion seamlessly integrates the complementary
advantages of spatial domain and transform domain methods within Dc-EEMF. This
approach can be trained end-to-end without necessitating post-processing
procedures. Experimental results and numerical analyses collectively
demonstrate our method's robust performance, achieving an impressive fusion
result for PAM images without a substantial sacrifice in lateral resolution.
The utilization of Dc-EEMF-powered PAM has the potential to serve as a
practical tool in preclinical and clinical studies requiring extended DoF for
various applications.

</details>


### [639] [Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study](https://arxiv.org/pdf/2506.03183)
*Yaşar Utku Alçalar, Yu Cao, Mehmet Akçakaya*

Main category: eess.IV

TL;DR: A novel PD-AI MRI reconstruction method optimized for FPGA-based edge computing, using 8-bit quantization and eliminating redundant FFT/IFFT operations, improves efficiency without compromising quality.


<details>
  <summary>Details</summary>
Motivation: High-resolution MRI scans generate massive data, causing transmission, storage, and processing challenges, especially in functional MRI. Edge computing with FPGAs offers a solution but requires hardware-efficient PD-AI models.

Method: Proposes a PD-AI approach optimized for FPGAs, using 8-bit complex data quantization and removing redundant FFT/IFFT operations.

Result: The method enhances computational efficiency while maintaining reconstruction quality, outperforming standard clinical methods.

Conclusion: The approach enables high-resolution MRI reconstruction on resource-constrained devices, showing promise for real-world deployment.

Abstract: Physics-driven artificial intelligence (PD-AI) reconstruction methods have
emerged as the state-of-the-art for accelerating MRI scans, enabling higher
spatial and temporal resolutions. However, the high resolution of these scans
generates massive data volumes, leading to challenges in transmission, storage,
and real-time processing. This is particularly pronounced in functional MRI,
where hundreds of volumetric acquisitions further exacerbate these demands.
Edge computing with FPGAs presents a promising solution for enabling PD-AI
reconstruction near the MRI sensors, reducing data transfer and storage
bottlenecks. However, this requires optimization of PD-AI models for hardware
efficiency through quantization and bypassing traditional FFT-based approaches,
which can be a limitation due to their computational demands. In this work, we
propose a novel PD-AI computational MRI approach optimized for FPGA-based edge
computing devices, leveraging 8-bit complex data quantization and eliminating
redundant FFT/IFFT operations. Our results show that this strategy improves
computational efficiency while maintaining reconstruction quality comparable to
conventional PD-AI methods, and outperforms standard clinical methods. Our
approach presents an opportunity for high-resolution MRI reconstruction on
resource-constrained devices, highlighting its potential for real-world
deployment.

</details>


### [640] [DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset](https://arxiv.org/pdf/2506.03185)
*Liangrui Pan, Xingchen Li, Zhongyi Chen, Ling Chu, Shaoliang Peng*

Main category: eess.IV

TL;DR: DLiPath is a benchmark for donor liver assessment using histopathology images, addressing variability in manual evaluations with MIL models.


<details>
  <summary>Details</summary>
Motivation: Manual assessment of donor liver biopsies is time-consuming and inconsistent. DLiPath aims to standardize and automate this process.

Method: Collected 636 whole slide images from 304 patients, annotated by experts, and evaluated nine MIL models for accuracy.

Result: Several MIL models achieved high accuracy in assessing key pathological features, indicating potential for automation.

Conclusion: DLiPath provides a foundation for future automated and intelligent donor liver assessment research.

Abstract: Pathologists comprehensive evaluation of donor liver biopsies provides
crucial information for accepting or discarding potential grafts. However,
rapidly and accurately obtaining these assessments intraoperatively poses a
significant challenge for pathologists. Features in donor liver biopsies, such
as portal tract fibrosis, total steatosis, macrovesicular steatosis, and
hepatocellular ballooning are correlated with transplant outcomes, yet
quantifying these indicators suffers from substantial inter- and intra-observer
variability. To address this, we introduce DLiPath, the first benchmark for
comprehensive donor liver assessment based on a histopathology image dataset.
We collected and publicly released 636 whole slide images from 304 donor liver
patients at the Department of Pathology, the Third Xiangya Hospital, with
expert annotations for key pathological features (including cholestasis, portal
tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,
and hepatocellular ballooning). We selected nine state-of-the-art
multiple-instance learning (MIL) models based on the DLiPath dataset as
baselines for extensive comparative analysis. The experimental results
demonstrate that several MIL models achieve high accuracy across donor liver
assessment indicators on DLiPath, charting a clear course for future automated
and intelligent donor liver assessment research. Data and code are available at
https://github.com/panliangrui/ACM_MM_2025.

</details>


### [641] [Lightweight Convolutional Neural Networks for Retinal Disease Classification](https://arxiv.org/pdf/2506.03186)
*Duaa Kareem Qasim, Sabah Abdulazeez Jebur, Lafta Raheem Ali, Abdul Jalil M. Khalaf, Abir Jaafar Hussain*

Main category: eess.IV

TL;DR: The paper uses MobileNet and NASNetMobile CNNs to classify retinal diseases (DR and MH) with MobileNetV2 achieving 90.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of retinal diseases like DR and MH is critical to prevent vision loss.

Method: Employed MobileNet and NASNetMobile CNNs, trained on the RFMiD dataset with preprocessing and data augmentation.

Result: MobileNetV2 achieved 90.8% accuracy, outperforming NASNetMobile (89.5%).

Conclusion: CNNs are effective for retinal disease classification, aiding AI-assisted diagnosis and early intervention.

Abstract: Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)
significantly impact vision and affect millions worldwide. Early detection is
crucial, as DR, a complication of diabetes, damages retinal blood vessels,
potentially leading to blindness, while MH disrupts central vision, affecting
tasks like reading and facial recognition. This paper employed two lightweight
and efficient Convolution Neural Network architectures, MobileNet and
NASNetMobile, for the classification of Normal, DR, and MH retinal images. The
models were trained on the RFMiD dataset, consisting of 3,200 fundus images,
after undergoing preprocessing steps such as resizing, normalization, and
augmentation. To address data scarcity, this study leveraged transfer learning
and data augmentation techniques, enhancing model generalization and
performance. The experimental results demonstrate that MobileNetV2 achieved the
highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%
accuracy. These findings highlight the effectiveness of CNNs in retinal disease
classification, providing a foundation for AI-assisted ophthalmic diagnosis and
early intervention.

</details>


### [642] [Multi-Analyte, Swab-based Automated Wound Monitor with AI](https://arxiv.org/pdf/2506.03188)
*Madhu Babu Sikha, Lalith Appari, Gurudatt Nanjanagudu Ganesh, Amay Bandodkar, Imon Banerjee*

Main category: eess.IV

TL;DR: A low-cost, multi-analyte 3D printed assay and iOS app detect non-healing diabetic foot ulcers early, reducing treatment costs and amputation risks.


<details>
  <summary>Details</summary>
Motivation: Early detection of non-healing diabetic foot ulcers (DFUs) is critical to lower treatment costs and prevent amputations.

Method: Developed 3D printed assays integrated on swabs and a Wound Sensor iOS App for automated analysis of wound data using computer vision.

Result: Automated comparison of assay images determines wound severity, with the app ensuring accurate data collection despite environmental variations.

Conclusion: The integrated sensor and app enable real-time wound monitoring and healing progress tracking for healthcare professionals.

Abstract: Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000
individuals every year in the US alone and identifying non-healing DFUs that
develop to chronic wounds early can drastically reduce treatment costs and
minimize risks of amputation. There is therefore a pressing need for diagnostic
tools that can detect non-healing DFUs early. We develop a low cost,
multi-analyte 3D printed assays seamlessly integrated on swabs that can
identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile
application developed for the controlled acquisition and automated analysis of
wound sensor data. By comparing both the original base image (before exposure
to the wound) and the wound-exposed image, we developed automated computer
vision techniques to compare density changes between the two assay images,
which allow us to automatically determine the severity of the wound. The iOS
app ensures accurate data collection and presents actionable insights, despite
challenges such as variations in camera configurations and ambient conditions.
The proposed integrated sensor and iOS app will allow healthcare professionals
to monitor wound conditions real-time, track healing progress, and assess
critical parameters related to wound care.

</details>


### [643] [Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers](https://arxiv.org/pdf/2506.03192)
*Basudha Pal, Rama Chellappa, Muhammad Umair*

Main category: eess.IV

TL;DR: A framework predicts severe left ventricular hypertrophy from chest X-rays using Mutual Information Neural Estimation, achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: Echocardiography and MRI are costly and less accessible, prompting a need for an alternative method using chest X-rays.

Method: Direct classification framework using Mutual Information Neural Estimation to quantify feature expressivity.

Result: High AUROC and AUPRC, with clinically meaningful attribute encoding and transparent model interpretation.

Conclusion: The framework offers a cost-effective, accessible alternative for detecting severe left ventricular hypertrophy.

Abstract: While echocardiography and MRI are clinical standards for evaluating cardiac
structure, their use is limited by cost and accessibility.We introduce a direct
classification framework that predicts severe left ventricular hypertrophy from
chest X-rays, without relying on anatomical measurements or demographic inputs.
Our approach achieves high AUROC and AUPRC, and employs Mutual Information
Neural Estimation to quantify feature expressivity. This reveals clinically
meaningful attribute encoding and supports transparent model interpretation.

</details>


### [644] [A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction](https://arxiv.org/pdf/2506.03202)
*Itxasne Antúnez Sáenz, Ane Alberdi Aramendi, David Dunaway, Juling Ong, Lara Deliège, Amparo Sáenz, Anita Ahmadi Birjandi, Noor UI Owase Jeelani, Silvia Schievano, Alessandro Borghi*

Main category: eess.IV

TL;DR: A real-time prediction tool for craniosynostosis surgery outcomes is developed using 3D photos and ML, avoiding CT scans and achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current surgical planning for craniosynostosis lacks efficiency and relies on CT scans, which involve radiation. The goal is to create a safer, faster tool using 3D photos and ML.

Method: Personalized synthetic skulls are created from 3D photos, incorporating average population data. A machine learning surrogate model predicts surgical outcomes.

Result: The ML model achieves an R2 of 0.95 and low MSE/MAE (<0.13), demonstrating high accuracy in predicting outcomes.

Conclusion: The tool offers a radiation-free, efficient alternative to CT-based planning and could optimize surgical parameters for better cranial index outcomes.

Abstract: Craniosynostosis is a medical condition that affects the growth of babies'
heads, caused by an early fusion of cranial sutures. In recent decades,
surgical treatments for craniosynostosis have significantly improved, leading
to reduced invasiveness, faster recovery, and less blood loss. At Great Ormond
Street Hospital (GOSH), the main surgical treatment for patients diagnosed with
sagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This
procedure involves a 15x15 mm2 osteotomy, where two springs are inserted to
induce distraction. Despite the numerous advantages of this surgical technique
for patients, the outcome remains unpredictable due to the lack of efficient
preoperative planning tools. The surgeon's experience and the baby's age are
currently relied upon to determine the osteotomy location and spring selection.
Previous tools for predicting the surgical outcome of SC relied on finite
element modeling (FEM), which involved computed tomography (CT) imaging and
required engineering expertise and lengthy calculations. The main goal of this
research is to develop a real-time prediction tool for the surgical outcome of
patients, eliminating the need for CT scans to minimise radiation exposure
during preoperative planning. The proposed methodology involves creating
personalised synthetic skulls based on three-dimensional (3D) photographs,
incorporating population average values of suture location, skull thickness,
and soft tissue properties. A machine learning (ML) surrogate model is employed
to achieve the desired surgical outcome. The resulting multi-output support
vector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.
Furthermore, in the future, this model could not only simulate various surgical
scenarios but also provide optimal parameters for achieving a maximum cranial
index (CI).

</details>


### [645] [A Survey of Deep Learning Video Super-Resolution](https://arxiv.org/pdf/2506.03216)
*Arbind Agrahari Baniya, Tsz-Kwan Lee, Peter Eklund, Sunil Aryal*

Main category: eess.IV

TL;DR: A comprehensive survey of deep learning-based video super-resolution (VSR) models, analyzing components, methodologies, and trends to guide future research and applications.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of deep learning in VSR lacks systematic analysis, necessitating a methodical review to inform model development for specific needs.

Method: The paper provides an overview of VSR models, categorizes methodologies, and establishes a taxonomy to identify trends and challenges.

Result: Identified key components, trends, and challenges in VSR, along with a multi-level taxonomy for future research guidance.

Conclusion: This survey enhances understanding and interpretation of VSR practices, aiding tailored model development for diverse applications.

Abstract: Video super-resolution (VSR) is a prominent research topic in low-level
computer vision, where deep learning technologies have played a significant
role. The rapid progress in deep learning and its applications in VSR has led
to a proliferation of tools and techniques in the literature. However, the
usage of these methods is often not adequately explained, and decisions are
primarily driven by quantitative improvements. Given the significance of VSR's
potential influence across multiple domains, it is imperative to conduct a
comprehensive analysis of the elements and deep learning methodologies employed
in VSR research. This methodical analysis will facilitate the informed
development of models tailored to specific application needs. In this paper, we
present an overarching overview of deep learning-based video super-resolution
models, investigating each component and discussing its implications.
Furthermore, we provide a synopsis of key components and technologies employed
by state-of-the-art and earlier VSR models. By elucidating the underlying
methodologies and categorising them systematically, we identified trends,
requirements, and challenges in the domain. As a first-of-its-kind survey of
deep learning-based VSR models, this work also establishes a multi-level
taxonomy to guide current and future VSR research, enhancing the maturation and
interpretation of VSR practices for various practical applications.

</details>


### [646] [petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI](https://arxiv.org/pdf/2506.03217)
*Pierrick Coupé, Boris Mansencal, Floréal Morandat, Sergio Morell-Ortega, Nicolas Villain, Jose V. Manjón, Vincent Planche*

Main category: eess.IV

TL;DR: petBrain is a new web-based pipeline for AD biomarker analysis, offering fast, reliable, and standardized quantification of amyloid, tau, and neurodegeneration using PET and MRI.


<details>
  <summary>Details</summary>
Motivation: Existing pipelines for AD diagnosis and prognosis have limitations in processing time, tracer variability, and multimodal integration.

Method: petBrain uses deep learning-based segmentation and standardized biomarker quantification (Centiloid, CenTauR, HAVAs) for amyloid-PET, tau-PET, and MRI. It's a web-based platform requiring no local infrastructure.

Result: petBrain matches existing pipelines in reliability for amyloid and tau quantification, aligns with ADNI data, and correlates well with CSF/plasma biomarkers, clinical status, and cognition.

Conclusion: petBrain is a robust, accessible tool for standardized AD biomarker analysis, enhancing clinical research.

Abstract: INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles
(T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's
disease (AD) diagnosis and prognosis. Existing pipelines face limitations
regarding processing time, variability in tracer types, and challenges in
multimodal integration.
  METHODS: We developed petBrain, a novel end-to-end processing pipeline for
amyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based
segmentation, standardized biomarker quantification (Centiloid, CenTauR,
HAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is
implemented as a web-based platform, requiring no local computational
infrastructure or specialized software knowledge.
  RESULTS: petBrain provides reliable and rapid biomarker quantification, with
results comparable to existing pipelines for A and T2. It shows strong
concordance with data processed in ADNI databases. The staging and
quantification of A/T2/N by petBrain demonstrated good agreement with
CSF/plasma biomarkers, clinical status, and cognitive performance.
  DISCUSSION: petBrain represents a powerful and openly accessible platform for
standardized AD biomarker analysis, facilitating applications in clinical
research.

</details>


### [647] [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach](https://arxiv.org/pdf/2506.03238)
*Ziheng Zhao, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie*

Main category: eess.IV

TL;DR: The paper introduces OminiAbnorm-CT, a system for automated interpretation of CT images, including a taxonomy, dataset, model, and benchmarks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of localizing and describing abnormal findings in multi-plane and whole-body CT scans in clinical radiology.

Method: Proposes a hierarchical taxonomy, contributes a dataset with annotations, develops OminiAbnorm-CT for automated grounding and description, and establishes evaluation tasks.

Result: OminiAbnorm-CT significantly outperforms existing methods on all tasks and metrics.

Conclusion: The system advances automated CT image interpretation, offering comprehensive tools for clinical radiology.

Abstract: Automated interpretation of CT images-particularly localizing and describing
abnormal findings across multi-plane and whole-body scans-remains a significant
challenge in clinical radiology. This work aims to address this challenge
through four key contributions: (i) On taxonomy, we collaborate with senior
radiologists to propose a comprehensive hierarchical classification system,
with 404 representative abnormal findings across all body regions; (ii) On
data, we contribute a dataset containing over 14.5K CT images from multiple
planes and all human body regions, and meticulously provide grounding
annotations for over 19K abnormalities, each linked to the detailed description
and cast into the taxonomy; (iii) On model development, we propose
OminiAbnorm-CT, which can automatically ground and describe abnormal findings
on multi-plane and whole-body CT images based on text queries, while also
allowing flexible interaction through visual prompts; (iv) On benchmarks, we
establish three representative evaluation tasks based on real clinical
scenarios. Through extensive experiments, we show that OminiAbnorm-CT can
significantly outperform existing methods on all the tasks and metrics.

</details>


### [648] [Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images](https://arxiv.org/pdf/2506.03420)
*Muhammad Zubair Hasan, Fahmida Yasmin Rifat*

Main category: eess.IV

TL;DR: A hybrid machine and deep learning approach for skin lesion classification achieves top performance using a segmentation-assisted pipeline and synthetic data augmentation.


<details>
  <summary>Details</summary>
Motivation: Early detection of skin cancer is critical, and this work aims to improve classification of malignant and benign lesions under non-dermoscopic conditions.

Method: Combines vision transformers (EVA02) and a convolutional ViT hybrid (EdgeNeXtSAC) with a GBDT ensemble, synthetic data augmentation, and relabeling for imbalance.

Result: Achieves a pAUC of 0.1755, the highest among tested configurations, demonstrating strong performance for skin cancer triage.

Conclusion: Hybrid AI systems show promise for telemedicine and resource-limited settings in skin cancer detection.

Abstract: Skin cancer is among the most prevalent and life-threatening diseases
worldwide, with early detection being critical to patient outcomes. This work
presents a hybrid machine and deep learning-based approach for classifying
malignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,
which comprises 401,059 cropped lesion images extracted from 3D Total Body
Photography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our
method combines vision transformers (EVA02) and our designed convolutional ViT
hybrid (EdgeNeXtSAC) to extract robust features, employing a
segmentation-assisted classification pipeline to enhance lesion localization.
Predictions from these models are fused with a gradient-boosted decision tree
(GBDT) ensemble enriched by engineered features and patient-specific relational
metrics. To address class imbalance and improve generalization, we augment
malignant cases with Stable Diffusion-generated synthetic lesions and apply a
diagnosis-informed relabeling strategy to harmonize external datasets into a
3-class format. Using partial AUC (pAUC) above 80 percent true positive rate
(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the
highest among all configurations. These results underscore the potential of
hybrid, interpretable AI systems for skin cancer triage in telemedicine and
resource-constrained settings.

</details>


### [649] [Frame-Level Real-Time Assessment of Stroke Rehabilitation Exercises from Video-Level Labeled Data: Task-Specific vs. Foundation Models](https://arxiv.org/pdf/2506.03752)
*Gonçalo Mesquita, Ana Rita Cóias, Artur Dubrawski, Alexandre Bernardino*

Main category: eess.IV

TL;DR: A framework for real-time stroke rehabilitation exercise assessment using video-level annotations and pseudo-labels, leveraging pre-trained models to reduce labeling costs and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost and effort of frame-level annotations for training real-time motion analysis systems in stroke rehabilitation.

Method: Uses gradient-based techniques and pseudo-label selection to train a frame-level classifier from video-level annotations, employing pre-trained models (Action Transformer, SkateFormer, MOMENT) for pseudo-label generation.

Result: MOMENT achieves 73% AUC for video-level assessment, outperforming LSTM (58%). Action Transformer with Integrated Gradient achieves 72% AUC for frame-level assessment, surpassing baseline (69%).

Conclusion: The approach reduces labeling demands, improves generalization, and enhances customization for new patients, demonstrating the effectiveness of pre-trained models in rehabilitation exercise assessment.

Abstract: The growing demands of stroke rehabilitation have increased the need for
solutions to support autonomous exercising. Virtual coaches can provide
real-time exercise feedback from video data, helping patients improve motor
function and keep engagement. However, training real-time motion analysis
systems demands frame-level annotations, which are time-consuming and costly to
obtain. In this work, we present a framework that learns to classify individual
frames from video-level annotations for real-time assessment of compensatory
motions in rehabilitation exercises. We use a gradient-based technique and a
pseudo-label selection method to create frame-level pseudo-labels for training
a frame-level classifier. We leverage pre-trained task-specific models - Action
Transformer, SkateFormer - and a foundation model - MOMENT - for pseudo-label
generation, aiming to improve generalization to new patients. To validate the
approach, we use the \textit{SERE} dataset with 18 post-stroke patients
performing five rehabilitation exercises annotated on compensatory motions.
MOMENT achieves better video-level assessment results (AUC = $73\%$),
outperforming the baseline LSTM (AUC = $58\%$). The Action Transformer, with
the Integrated Gradient technique, leads to better outcomes (AUC = $72\%$) for
frame-level assessment, outperforming the baseline trained with ground truth
frame-level labeling (AUC = $69\%$). We show that our proposed approach with
pre-trained models enhances model generalization ability and facilitates the
customization to new patients, reducing the demands of data labeling.

</details>


### [650] [Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering](https://arxiv.org/pdf/2506.03890)
*Christian Tinauer, Maximilian Sackl, Stefan Ropele, Christian Langkammer*

Main category: eess.IV

TL;DR: The study analyzes deep learning models for Alzheimer's disease classification using R2* maps, focusing on interpretability. Layer-wise Relevance Propagation and spectral clustering reveal distinct decision patterns, emphasizing preprocessing and training impacts.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for AD classification lack interpretability, raising concerns about biases in decisions. This study aims to explore decision strategies using explainable AI techniques.

Method: A 3D CNN was trained on R2* maps, with LRP generating relevance heatmaps. Spectral clustering identified decision patterns, and t-SNE visualized clustering structure.

Result: Spectral clustering showed distinct decision patterns, with relevance-guided models clearly separating AD and NC cases. t-SNE confirmed alignment of heatmap groupings with subject groups.

Conclusion: Preprocessing and training choices significantly impact model decisions, even with similar performance. Spectral clustering aids in understanding classification strategies, stressing the need for explainability in medical AI.

Abstract: Deep learning models have shown strong performance in classifying Alzheimer's
disease (AD) from R2* maps, but their decision-making remains opaque, raising
concerns about interpretability. Previous studies suggest biases in model
decisions, necessitating further analysis. This study uses Layer-wise Relevance
Propagation (LRP) and spectral clustering to explore classifier decision
strategies across preprocessing and training configurations using R2* maps. We
trained a 3D convolutional neural network on R2* maps, generating relevance
heatmaps via LRP and applied spectral clustering to identify dominant patterns.
t-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess
clustering structure. Spectral clustering revealed distinct decision patterns,
with the relevance-guided model showing the clearest separation between AD and
normal control (NC) cases. The t-SNE visualization confirmed that this model
aligned heatmap groupings with the underlying subject groups. Our findings
highlight the significant impact of preprocessing and training choices on deep
learning models trained on R2* maps, even with similar performance metrics.
Spectral clustering offers a structured method to identify classification
strategy differences, emphasizing the importance of explainability in medical
AI.

</details>


### [651] [Conformal coronary calcification volume estimation with conditional coverage via histogram clustering](https://arxiv.org/pdf/2506.04030)
*Olivier Jaubert, Salman Mohammadi, Keith A. Goatman, Shadia S. Mikhael, Conor Bradley, Rebecca Hughes, Richard Good, John H. Hipwell, Sonia Dahdouh*

Main category: eess.IV

TL;DR: A cluster-based conditional conformal prediction framework is proposed to calibrate coronary calcium score intervals from segmentation networks, improving triage metrics compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: To balance early detection benefits and risks of over-reporting coronary calcium in CT scans, ensuring accurate and reliable automated reporting.

Method: A cluster-based conditional conformal prediction framework is applied to 3D UNet models (deterministic, MCDropout, deep ensemble) for calibrated score intervals without retraining.

Result: Achieved similar coverage with better triage metrics than conventional conformal prediction, enabling confident patient risk categorization.

Conclusion: The method provides meaningful predictive intervals for calcium scores, aiding in patient triage and clinical decision-making.

Abstract: Incidental detection and quantification of coronary calcium in CT scans could
lead to the early introduction of lifesaving clinical interventions. However,
over-reporting could negatively affect patient wellbeing and unnecessarily
burden the medical system. Therefore, careful considerations should be taken
when automatically reporting coronary calcium scores. A cluster-based
conditional conformal prediction framework is proposed to provide score
intervals with calibrated coverage from trained segmentation networks without
retraining. The proposed method was tuned and used to calibrate predictive
intervals for 3D UNet models (deterministic, MCDropout and deep ensemble)
reaching similar coverage with better triage metrics compared to conventional
conformal prediction. Meaningful predictive intervals of calcium scores could
help triage patients according to the confidence of their risk category
prediction.

</details>


### [652] [Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays](https://arxiv.org/pdf/2506.04058)
*Bulat Maksudov, Kathleen Curran, Alessandra Mileo*

Main category: eess.IV

TL;DR: The paper proposes using Concept Activation Vectors (CAVs) in generative models to align medical imaging with clinical knowledge, enabling interpretable explanations and counterfactual images.


<details>
  <summary>Details</summary>
Motivation: To ensure medical imaging models align with clinical knowledge and provide interpretable explanations for clinicians.

Method: Mapping clinical concepts into the latent space of generative models using a reconstruction autoencoder, extracting stable CAVs without labeled training.

Result: Stable concept extraction and visual explanations for clinical features, with promising results for large pathologies like cardiomegaly, though smaller pathologies remain challenging.

Conclusion: The approach offers interpretable, concept-based explanations aligned with clinical knowledge, despite not outperforming baselines.

Abstract: An essential step in deploying medical imaging models is ensuring alignment
with clinical knowledge and interpretability. We focus on mapping clinical
concepts into the latent space of generative models to identify Concept
Activation Vectors (CAVs). Using a simple reconstruction autoencoder, we link
user-defined concepts to image-level features without explicit label training.
The extracted concepts are stable across datasets, enabling visual explanations
that highlight clinically relevant features. By traversing latent space along
concept directions, we produce counterfactuals that exaggerate or reduce
specific clinical features. Preliminary results on chest X-rays show promise
for large pathologies like cardiomegaly, while smaller pathologies remain
challenging due to reconstruction limits. Although not outperforming baselines,
this approach offers a path toward interpretable, concept-based explanations
aligned with clinical knowledge.

</details>


### [653] [A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging](https://arxiv.org/pdf/2506.04116)
*Xuanru Zhou, Jiarun Liu, Shoujun Yu, Hao Yang, Cheng Li, Tao Tan, Shanshan Wang*

Main category: eess.IV

TL;DR: TSSC-Net is a novel framework for generating intermediate frames in 4D MRI, improving temporal and spatial resolution during rapid motion.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional registration-based interpolation in handling large deformations and preserving spatial consistency in dynamic MRI.

Method: Uses a diffusion-based temporal super-resolution network and a tri-directional Mamba-based module for long-range contextual information.

Result: Achieves 6x temporal super-resolution and enhances volumetric coherence, validated on cardiac and knee MRI datasets.

Conclusion: TSSC-Net effectively preserves structural fidelity and spatial consistency in fast-motion 4D MRI.

Abstract: In medical imaging, 4D MRI enables dynamic 3D visualization, yet the
trade-off between spatial and temporal resolution requires prolonged scan time
that can compromise temporal fidelity--especially during rapid, large-amplitude
motion. Traditional approaches typically rely on registration-based
interpolation to generate intermediate frames. However, these methods struggle
with large deformations, resulting in misregistration, artifacts, and
diminished spatial consistency. To address these challenges, we propose
TSSC-Net, a novel framework that generates intermediate frames while preserving
spatial consistency. To improve temporal fidelity under fast motion, our
diffusion-based temporal super-resolution network generates intermediate frames
using the start and end frames as key references, achieving 6x temporal
super-resolution in a single inference step. Additionally, we introduce a novel
tri-directional Mamba-based module that leverages long-range contextual
information to effectively resolve spatial inconsistencies arising from
cross-slice misalignment, thereby enhancing volumetric coherence and correcting
cross-slice errors. Extensive experiments were performed on the public ACDC
cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results
demonstrate that TSSC-Net can generate high-resolution dynamic MRI from
fast-motion data while preserving structural fidelity and spatial consistency.

</details>


### [654] [A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks](https://arxiv.org/pdf/2506.04121)
*Loan Dao, Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: The paper reviews Medical Image Segmentation (MIS) using Deep Neural Networks (DNNs), focusing on DIKIW levels and Explainable AI (XAI) for transparency. It highlights MIS's role in disease diagnosis and early detection, addressing challenges and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: To advance MIS for disease diagnosis, especially cancer, by leveraging DNNs and XAI for transparency and ethical AI.

Method: Comprehensive study of MIS based on DNNs, evaluated using DIKIW levels and incorporating XAI for explainability.

Result: Identifies state-of-the-art MIS solutions and emphasizes XAI's role in bridging intelligence to wisdom.

Conclusion: MIS with DNNs and XAI is crucial for timely diagnosis, but challenges remain; solutions are proposed for efficient implementation.

Abstract: Over the past decade, Medical Image Segmentation (MIS) using Deep Neural
Networks (DNNs) has achieved significant performance improvements and holds
great promise for future developments. This paper presents a comprehensive
study on MIS based on DNNs. Intelligent Vision Systems are often evaluated
based on their output levels, such as Data, Information, Knowledge,
Intelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at
these levels are the focus of research. Additionally, Explainable Artificial
Intelligence (XAI) has become an important research direction, as it aims to
uncover the "black box" nature of previous DNN architectures to meet the
requirements of transparency and ethics. The study emphasizes the importance of
MIS in disease diagnosis and early detection, particularly for increasing the
survival rate of cancer patients through timely diagnosis. XAI and early
prediction are considered two important steps in the journey from
"intelligence" to "wisdom." Additionally, the paper addresses existing
challenges and proposes potential solutions to enhance the efficiency of
implementing DNN-based MIS.

</details>


### [655] [Recent Advances in Medical Image Classification](https://arxiv.org/pdf/2506.04129)
*Loan Dao, Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: The paper reviews AI advancements in medical image classification, covering basic to applied solutions, including deep learning models and Vision Language Models, addressing data limitations and explainability.


<details>
  <summary>Details</summary>
Motivation: To improve diagnosis and treatment through AI-driven medical image classification, leveraging recent technological progress.

Method: Reviews three levels of solutions (basic, specific, applied) using deep learning (CNNs, Vision Transformers) and Vision Language Models, with a focus on Explainable AI.

Result: Highlights advancements in handling limited labeled data and improving predictive accuracy and explainability.

Conclusion: AI, particularly deep learning and Vision Language Models, significantly enhances medical image classification, addressing key challenges like data scarcity and interpretability.

Abstract: Medical image classification is crucial for diagnosis and treatment,
benefiting significantly from advancements in artificial intelligence. The
paper reviews recent progress in the field, focusing on three levels of
solutions: basic, specific, and applied. It highlights advances in traditional
methods using deep learning models like Convolutional Neural Networks and
Vision Transformers, as well as state-of-the-art approaches with Vision
Language Models. These models tackle the issue of limited labeled data, and
enhance and explain predictive results through Explainable Artificial
Intelligence.

</details>


### [656] [Synthetic multi-inversion time magnetic resonance images for visualization of subcortical structures](https://arxiv.org/pdf/2506.04173)
*Savannah P. Hays, Lianrui Zuo, Anqi Feng, Yihao Liu, Blake E. Dewey, Jiachen Zhuo, Ellen M. Mowry, Scott D. Newsome Jerry L. Prince, Aaron Carass*

Main category: eess.IV

TL;DR: SyMTIC uses deep learning to generate synthetic multi-TI MR images from routine T1-w, T2-w, and FLAIR scans, improving subcortical visualization.


<details>
  <summary>Details</summary>
Motivation: Multi-TI T1-w MR imaging enhances subcortical visualization but is rarely used clinically. SyMTIC aims to bridge this gap by synthesizing multi-TI images from standard scans.

Method: SyMTIC combines deep neural networks with imaging physics to estimate T1 and PD maps, then computes multi-TI images with arbitrary inversion times.

Result: SyMTIC accurately synthesized multi-TI images, enhancing subcortical visualization and thalamic nuclei segmentation, especially for TI values 400-800 ms.

Conclusion: SyMTIC provides a practical solution for generating high-quality multi-TI images from routine MR contrasts, improving brain image analysis.

Abstract: Purpose: Visualization of subcortical gray matter is essential in
neuroscience and clinical practice, particularly for disease understanding and
surgical planning.While multi-inversion time (multi-TI) T$_1$-weighted
(T$_1$-w) magnetic resonance (MR) imaging improves visualization, it is rarely
acquired in clinical settings. Approach: We present SyMTIC (Synthetic Multi-TI
Contrasts), a deep learning method that generates synthetic multi-TI images
using routinely acquired T$_1$-w, T$_2$-weighted (T$_2$-w), and FLAIR images.
Our approach combines image translation via deep neural networks with imaging
physics to estimate longitudinal relaxation time (T$_1$) and proton density
(PD) maps. These maps are then used to compute multi-TI images with arbitrary
inversion times. Results: SyMTIC was trained using paired MPRAGE and FGATIR
images along with T$_2$-w and FLAIR images. It accurately synthesized multi-TI
images from standard clinical inputs, achieving image quality comparable to
that from explicitly acquired multi-TI data.The synthetic images, especially
for TI values between 400-800 ms, enhanced visualization of subcortical
structures and improved segmentation of thalamic nuclei. Conclusion: SyMTIC
enables robust generation of high-quality multi-TI images from routine MR
contrasts. It generalizes well to varied clinical datasets, including those
with missing FLAIR images or unknown parameters, offering a practical solution
for improving brain MR image visualization and analysis.

</details>


### [657] [UltraBones100k: A reliable automated labeling method and large-scale dataset for ultrasound-based bone surface extraction](https://arxiv.org/pdf/2502.03783)
*Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd Viehöfer, Stephan Wirth, Lilian Calvet, Philipp Fürnstahl*

Main category: eess.IV

TL;DR: The paper proposes a method to automate bone label generation for ultrasound images, improving dataset quality and model performance in bone segmentation.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for ultrasound bone segmentation rely on costly manual labeling, which is limited in size and quality due to ultrasound's low signal-to-noise ratio and acoustic shadowing.

Method: The methodology involves collecting ex-vivo ultrasound datasets with automatically generated labels by superimposing tracked bone CT models onto ultrasound images, refined for ultrasound physics. A neural network is trained on this dataset.

Result: The UltraBones100k dataset was created, showing significant improvement in label quality (p < 0.001). The trained model outperformed manual labeling, especially in low-intensity regions (320% improvement in completeness).

Conclusion: Automated label generation enhances dataset quality and model performance, addressing limitations of manual labeling in ultrasound bone segmentation.

Abstract: Ultrasound-based bone surface segmentation is crucial in computer-assisted
orthopedic surgery. However, ultrasound images have limitations, including a
low signal-to-noise ratio, and acoustic shadowing, which make interpretation
difficult. Existing deep learning models for bone segmentation rely primarily
on costly manual labeling by experts, limiting dataset size and model
generalizability. Additionally, the complexity of ultrasound physics and
acoustic shadow makes the images difficult for humans to interpret, leading to
incomplete labels in anechoic regions and limiting model performance. To
advance ultrasound bone segmentation and establish effective model benchmarks,
larger and higher-quality datasets are needed.
  We propose a methodology for collecting ex-vivo ultrasound datasets with
automatically generated bone labels, including anechoic regions. The proposed
labels are derived by accurately superimposing tracked bone CT models onto the
tracked ultrasound images. These initial labels are refined to account for
ultrasound physics. A clinical evaluation is conducted by an expert physician
specialized on orthopedic sonography to assess the quality of the generated
bone labels. A neural network for bone segmentation is trained on the collected
dataset and its predictions are compared to expert manual labels, evaluating
accuracy, completeness, and F1-score.
  We collected the largest known dataset of 100k ultrasound images of human
lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank
test with Bonferroni correction confirmed that the bone alignment after our
method significantly improved the quality of bone labeling (p < 0.001). The
model trained on UltraBones100k consistently outperforms manual labeling in all
metrics, particularly in low-intensity regions (320% improvement in
completeness at a distance threshold of 0.5 mm).

</details>


### [658] [Rapid Bone Scintigraphy Enhancement via Semantic Prior Distillation from Segment Anything Model](https://arxiv.org/pdf/2503.02321)
*Pengchen Liang, Leijun Shi, Huiping Yao, Bin Pu, Jianguo Chen, Lei Zhao, Haishan Huang, Zhuangzhuang Chen, Zhaozhao Xu, Lite Xu, Qing Chang, Yiwei Li*

Main category: eess.IV

TL;DR: The paper introduces a SAM-based method to enhance pediatric rapid bone scintigraphy, addressing image quality degradation in accelerated scans. It uses cascaded networks and specialized modules, validated on a novel dataset (RBS).


<details>
  <summary>Details</summary>
Motivation: Accelerated bone scintigraphy in children reduces discomfort but degrades image quality, potentially compromising diagnosis. The goal is to restore fine anatomical details.

Method: The approach employs two cascaded networks ($f^{IR1}$ and $f^{IR2}$) with three modules: SPI, SKD, and SCM, leveraging a fine-tuned SAM for semantic priors.

Result: The method outperforms existing techniques on PSNR, SSIM, FID, and LPIPS metrics, validated on the RBS dataset and a public endoscopic dataset.

Conclusion: The proposed SAM-based restoration method effectively enhances pediatric rapid bone scintigraphy, improving diagnostic accuracy while maintaining scan efficiency.

Abstract: Rapid bone scintigraphy is crucial for diagnosing skeletal disorders and
detecting tumor metastases in children, as it shortens scan duration and
reduces discomfort. However, accelerated acquisition often degrades image
quality, impairing the visibility of fine anatomical details and potentially
compromising diagnosis. To overcome this limitation, we introduce the first
application of SAM-based semantic priors for medical image restoration,
utilizing the Segment Anything Model (SAM) to enhance pediatric rapid bone
scintigraphy. Our approach employs two cascaded networks, $f^{IR1}$ and
$f^{IR2}$, supported by three specialized modules: a Semantic Prior Integration
(SPI) module, a Semantic Knowledge Distillation (SKD) module, and a Semantic
Consistency Module (SCM). The SPI and SKD modules inject domain-specific
semantic cues from a fine-tuned SAM, while the SCM preserves coherent semantic
feature representations across both cascaded stages. Moreover, we present RBS,
a novel Rapid Bone Scintigraphy dataset comprising paired standard (20 cm/min)
and rapid (40 cm/min) scans from 137 pediatric patients aged 0.5 - 16 years,
making it the first dataset tailored for pediatric rapid bone scintigraphy
restoration. Extensive experiments on both a public endoscopic dataset and our
RBS dataset demonstrate that our method consistently surpasses existing
techniques in PSNR, SSIM, FID, and LPIPS metrics.

</details>


### [659] [Towards a deep learning approach for classifying treatment response in glioblastomas](https://arxiv.org/pdf/2504.18268)
*Ana Matoso, Catarina Passarinho, Marta P. Loureiro, José Maria Moreira, Patrícia Figueiredo, Rita G. Nunes*

Main category: eess.IV

TL;DR: A deep learning pipeline was developed to classify glioblastoma treatment response using MRI scans and RANO criteria, achieving a median balanced accuracy of 50.96%.


<details>
  <summary>Details</summary>
Motivation: The manual assessment of glioblastoma treatment response using RANO criteria is complex and time-consuming, prompting the need for an automated DL solution.

Method: Five DL approaches were tested, including image subtraction, modality combinations, model architectures, pretraining tasks, and clinical data integration. The best-performing pipeline used Densenet264 with T1, T2, and FLAIR images.

Result: The pipeline achieved a median balanced accuracy of 50.96%, with Saliency Maps effectively highlighting tumor regions, while Grad-CAM had limited success.

Conclusion: This study establishes a benchmark for automated glioblastoma response assessment, highlighting the complexity and heterogeneity of factors influencing treatment evaluation.

Abstract: Glioblastomas are the most aggressive type of glioma, having a 5-year
survival rate of 6.9%. Treatment typically involves surgery, followed by
radiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)
scans to monitor disease progression. To assess treatment response,
radiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to
categorize the tumor into one of four labels based on imaging and clinical
features: complete response, partial response, stable disease, and progressive
disease. This assessment is very complex and time-consuming. Since deep
learning (DL) has been widely used to tackle classification problems, this work
aimed to implement the first DL pipeline for the classification of RANO
criteria based on two consecutive MRI acquisitions. The models were trained and
tested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction
of input images, 2) different combinations of modalities, 3) different model
architectures, 4) different pretraining tasks, and 5) adding clinical data. The
pipeline that achieved the best performance used a Densenet264 considering only
T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)
images as input without any pretraining. A median Balanced Accuracy of 50.96%
was achieved. Additionally, explainability methods were applied. Using Saliency
Maps, the tumor region was often successfully highlighted. In contrast,
Grad-CAM typically failed to highlight the tumor region, with some exceptions
observed in the Complete Response and Progressive Disease classes, where it
effectively identified the tumor region. These results set a benchmark for
future studies on glioblastoma treatment response assessment based on the RANO
criteria while emphasizing the heterogeneity of factors that might play a role
when assessing the tumor's response to treatment.

</details>


### [660] [ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education](https://arxiv.org/pdf/2506.00605)
*Ruiming Min, Minghao Liu*

Main category: eess.IV

TL;DR: The paper explores using CNNs and CycleGAN to generate synthetic medical images for education, addressing privacy and resource limitations.


<details>
  <summary>Details</summary>
Motivation: Modern medical education lacks high-quality teaching materials due to privacy concerns and resource shortages. Synthetic images from generative models offer a solution.

Method: The study employs convolutional neural networks (CNNs) and CycleGAN to create synthetic medical images.

Result: The approach generates diverse and privacy-compliant medical imaging datasets for educational use.

Conclusion: Generative models like CNNs and CycleGAN can effectively support medical education by providing synthetic, privacy-safe imaging data.

Abstract: With the advancement of modern medicine and the development of technologies
such as MRI, CT, and cellular analysis, it has become increasingly critical for
clinicians to accurately interpret various diagnostic images. However, modern
medical education often faces challenges due to limited access to high-quality
teaching materials, stemming from privacy concerns and a shortage of
educational resources (Balogh et al., 2015). In this context, image data
generated by machine learning models, particularly generative models, presents
a promising solution. These models can create diverse and comparable imaging
datasets without compromising patient privacy, thereby supporting modern
medical education. In this study, we explore the use of convolutional neural
networks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic
medical images. The source code is available at
https://github.com/mliuby/COMP4211-Project.

</details>


### [661] [NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution](https://arxiv.org/pdf/2506.02197)
*Marcos V. Conde, Radu Timofte, Zihao Lu, Xiangyu Kong, Xiaoxia Xing, Fan Wang, Suejin Han, MinKyu Park, Tianyu Zhang, Xin Luo, Yeda Chen, Dong Liu, Li Pang, Yuhang Yang, Hongzhong Wang, Xiangyong Cao, Ruixuan Jiang, Senyan Xu, Siyuan Jiang, Xueyang Fu, Zheng-Jun Zha, Tianyu Hao, Yuhong He, Ruoqi Li, Yueqi Yang, Xiang Yu, Guanlan Hong, Minmin Yi, Yuanjia Chen, Liwen Zhang, Zijie Jin, Cheng Li, Lian Liu, Wei Song, Heng Sun, Yubo Wang, Jinghua Wang, Jiajie Lu, Watchara Ruangsan*

Main category: eess.IV

TL;DR: The paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution Challenge, showcasing new methods and results for RAW image processing, which is less explored than RGB.


<details>
  <summary>Details</summary>
Motivation: To advance RAW image restoration and super-resolution, addressing gaps in modern ISP pipelines.

Method: Participants proposed solutions for restoring degraded RAW images and upscaling RAW Bayer images by 2x, tackling unknown noise and blur.

Result: 230 participants registered, 45 submitted results, revealing the current state-of-the-art in RAW restoration.

Conclusion: The challenge highlights progress and potential in RAW image processing, though it remains under-explored compared to RGB.

Abstract: This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution
Challenge, highlighting the proposed solutions and results. New methods for RAW
Restoration and Super-Resolution could be essential in modern Image Signal
Processing (ISP) pipelines, however, this problem is not as explored as in the
RGB domain. The goal of this challenge is two fold, (i) restore RAW images with
blur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering
unknown noise and blur. In the challenge, a total of 230 participants
registered, and 45 submitted results during thee challenge period. This report
presents the current state-of-the-art in RAW Restoration.

</details>
