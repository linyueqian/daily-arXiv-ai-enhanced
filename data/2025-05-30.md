<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 205]
- [cs.CV](#cs.CV) [Total: 232]
- [cs.AI](#cs.AI) [Total: 68]
- [cs.SD](#cs.SD) [Total: 18]
- [cs.LG](#cs.LG) [Total: 286]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 4]
- [eess.AS](#eess.AS) [Total: 9]
- [eess.IV](#eess.IV) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/pdf/2505.22704)
*Feng Yao, Zilong Wang, Liyuan Liu, Junxia Cui, Li Zhong, Xiaohan Fu, Haohui Mai, Vish Krishnan, Jianfeng Gao, Jingbo Shang*

Main category: cs.CL

TL;DR: REAL is a reinforcement learning framework for LLMs to generate high-quality code using automated feedback from program analysis and unit tests, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code generation lacks quality assurance in security and maintainability, and existing methods are not scalable or effective.

Method: REAL uses reinforcement learning with automated signals from program analysis (for defects) and unit tests (for correctness), without needing manual annotations.

Result: REAL outperforms state-of-the-art methods in functionality and code quality across datasets and model scales.

Conclusion: REAL bridges the gap between rapid prototyping and production-ready code, combining speed and quality in LLM-generated code.

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [2] [Climate Finance Bench](https://arxiv.org/pdf/2505.22752)
*Rafik Mankour, Yassine Chafai, Hamada Saleh, Ghassen Ben Hassine, Thibaud Barreau, Peter Tankov*

Main category: cs.CL

TL;DR: Climate Finance Bench introduces an open benchmark for QA over corporate climate disclosures using LLMs, highlighting retriever performance as a bottleneck and advocating for transparent carbon reporting.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for QA tasks in corporate climate disclosures and promote transparency in AI-for-climate applications.

Method: Curated 33 sustainability reports with 330 expert-validated QA pairs, compared RAG approaches, and emphasized retriever performance.

Result: Identified the retriever's ability to locate relevant passages as the main performance bottleneck.

Conclusion: Advocates for transparent carbon reporting in AI-for-climate applications, suggesting techniques like Weight Quantization.

Abstract: Climate Finance Bench introduces an open benchmark that targets
question-answering over corporate climate disclosures using Large Language
Models. We curate 33 recent sustainability reports in English drawn from
companies across all 11 GICS sectors and annotate 330 expert-validated
question-answer pairs that span pure extraction, numerical reasoning, and
logical reasoning. Building on this dataset, we propose a comparison of RAG
(retrieval-augmented generation) approaches. We show that the retriever's
ability to locate passages that actually contain the answer is the chief
performance bottleneck. We further argue for transparent carbon reporting in
AI-for-climate applications, highlighting advantages of techniques such as
Weight Quantization.

</details>


### [3] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/pdf/2505.22757)
*Ansar Aynetdinov, Alan Akbik*

Main category: cs.CL

TL;DR: A curriculum learning strategy for multi-token prediction (MTP) helps smaller language models (SLMs) leverage MTP benefits, improving downstream performance and generative quality.


<details>
  <summary>Details</summary>
Motivation: Smaller language models struggle with MTP, so a curriculum learning approach is proposed to ease the transition from next-token prediction (NTP) to MTP.

Method: Two curriculum variants: forward (NTP to MTP) and reverse (MTP to NTP).

Result: Forward curriculum improves downstream NTP performance and generative quality while retaining self-speculative decoding benefits. Reverse curriculum boosts NTP performance but lacks decoding benefits.

Conclusion: Curriculum learning, especially the forward variant, effectively enables SLMs to benefit from MTP training.

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [4] [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/pdf/2505.22765)
*Iddo Yosha, Gallil Maimon, Yossi Adi*

Main category: cs.CL

TL;DR: The paper introduces StressTest, a benchmark for evaluating speech-aware language models (SLMs) on sentence stress interpretation, and proposes a synthetic data pipeline (Stress17k) to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Sentence stress is crucial for meaning and intent in speech but is overlooked in SLM evaluation.

Method: Developed StressTest benchmark and Stress17k synthetic dataset to train and evaluate SLMs on stress-based interpretations.

Result: Existing SLMs perform poorly on stress tasks; finetuned model (StresSLM) outperforms others.

Conclusion: Synthetic data improves SLM performance on stress tasks, enabling better real-world application.

Abstract: Sentence stress refers to emphasis, placed on specific words within a spoken
utterance to highlight or contrast an idea, or to introduce new information. It
is often used to imply an underlying intention that is not explicitly stated.
Recent advances in speech-aware language models (SLMs) have enabled direct
processing of audio, allowing models to bypass transcription and access the
full richness of the speech signal and perform audio reasoning tasks such as
spoken question answering. Despite the crucial role of sentence stress in
shaping meaning and speaker intent, it remains largely overlooked in evaluation
and development of such models. In this work, we address this gap by
introducing StressTest, a benchmark specifically designed to evaluate a model's
ability to distinguish between interpretations of spoken sentences based on the
stress pattern. We assess the performance of several leading SLMs and find
that, despite their overall capabilities, they perform poorly on such tasks. To
overcome this limitation, we propose a novel synthetic data generation
pipeline, and create Stress17k, a training set that simulates change of meaning
implied by stress variation. Then, we empirically show that optimizing models
with this synthetic dataset aligns well with real-world recordings and enables
effective finetuning of SLMs. Results suggest, that our finetuned model,
StresSLM, significantly outperforms existing models on both sentence stress
reasoning and detection tasks. Code, models, data, and audio samples -
pages.cs.huji.ac.il/adiyoss-lab/stresstest.

</details>


### [5] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/pdf/2505.22759)
*Sara Papi, Marco Gaido, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri*

Main category: cs.CL

TL;DR: FAMA introduces open science speech foundation models (SFMs) for English and Italian, trained on 150k+ hours of open-source data, achieving competitive performance and faster speeds while promoting transparency.


<details>
  <summary>Details</summary>
Motivation: Closed nature of existing SFMs (e.g., Whisper, SeamlessM4T) hinders reproducibility and fair evaluation. Open science efforts in speech are limited.

Method: Developed FAMA, a family of SFMs, using 150k+ hours of open-source speech data and a new 16k-hour cleaned and pseudo-labeled dataset.

Result: FAMA matches performance of existing SFMs while being up to 8 times faster.

Conclusion: FAMA promotes openness in speech research by releasing all artifacts (code, data, models) under OS-compliant licenses.

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [6] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/pdf/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: Incorporating feedback-driven annotations (spelling/grammar errors and argumentative components) into AES improves scoring accuracy, demonstrated using the PERSUADE corpus and LLMs.


<details>
  <summary>Details</summary>
Motivation: Enhance automated essay scoring accuracy by integrating feedback-oriented annotations.

Method: Use two LLMs: one for spell-correction and another to identify argumentative elements, integrating annotations into the scoring pipeline.

Result: Improved performance in automated essay scoring using fine-tuned encoder-based LLMs.

Conclusion: Feedback-driven annotations enhance AES accuracy, with practical applications demonstrated using LLMs.

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [7] [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/pdf/2505.23494)
*Nicol Visser, Herman Kamper*

Main category: cs.CL

TL;DR: The paper explores how codebook size and unit coarseness affect spoken language models (SLMs), finding coarser units improve performance in sentence resynthesis and language modeling tasks, but not at phone or word levels.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction between codebook size and unit coarseness in SLMs, which remains unexplored despite its impact on performance.

Method: Varies codebook size and unit coarseness using duration-penalized dynamic programming (DPDP) and analyzes performance across linguistic levels.

Result: Coarser units benefit sentence resynthesis and language modeling tasks but not phone/word levels. DPDP efficiently provides coarser units where needed.

Conclusion: Coarser units are task-dependent; DPDP is a simple, effective method to optimize SLM performance for specific tasks.

Abstract: Spoken language models (SLMs) operate on acoustic units obtained by
discretizing self-supervised speech representations. Although the
characteristics of these units directly affect performance, the interaction
between codebook size and unit coarseness (i.e., duration) remains unexplored.
We investigate SLM performance as we vary codebook size and unit coarseness
using the simple duration-penalized dynamic programming (DPDP) method. New
analyses are performed across different linguistic levels. At the phone and
word levels, coarseness provides little benefit, as long as the codebook size
is chosen appropriately. However, when producing whole sentences in a
resynthesis task, SLMs perform better with coarser units. In lexical and
syntactic language modeling tasks, coarser units also give higher accuracies at
lower bitrates. We therefore show that coarser units aren't always better, but
that DPDP is a simple and efficient way to obtain coarser units for the tasks
where they are beneficial.

</details>


### [8] [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/pdf/2505.22774)
*Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: A treebank-driven method compares syntactic structures in speech and writing using dependency-parsed corpora, revealing fewer and less diverse structures in speech with limited overlap between modalities.


<details>
  <summary>Details</summary>
Motivation: To systematically compare syntactic structures in spoken and written language across modalities and languages, identifying modality-specific preferences.

Method: Delexicalized dependency subtrees are extracted from spoken and written Universal Dependencies treebanks in English and Slovenian, analyzing size, diversity, distribution, and overlap.

Result: Spoken corpora have fewer and less diverse structures than written ones, with minimal overlap. Speech-specific structures reflect interactivity and economy of expression.

Conclusion: The framework provides a scalable, language-independent method for studying syntactic variation, advancing data-driven theories of grammar.

Abstract: This paper presents a novel treebank-driven approach to comparing syntactic
structures in speech and writing using dependency-parsed corpora. Adopting a
fully inductive, bottom-up method, we define syntactic structures as
delexicalized dependency (sub)trees and extract them from spoken and written
Universal Dependencies (UD) treebanks in two syntactically distinct languages,
English and Slovenian. For each corpus, we analyze the size, diversity, and
distribution of syntactic inventories, their overlap across modalities, and the
structures most characteristic of speech. Results show that, across both
languages, spoken corpora contain fewer and less diverse syntactic structures
than their written counterparts, with consistent cross-linguistic preferences
for certain structural types across modalities. Strikingly, the overlap between
spoken and written syntactic inventories is very limited: most structures
attested in speech do not occur in writing, pointing to modality-specific
preferences in syntactic organization that reflect the distinct demands of
real-time interaction and elaborated writing. This contrast is further
supported by a keyness analysis of the most frequent speech-specific
structures, which highlights patterns associated with interactivity,
context-grounding, and economy of expression. We argue that this scalable,
language-independent framework offers a useful general method for
systematically studying syntactic variation across corpora, laying the
groundwork for more comprehensive data-driven theories of grammar in use.

</details>


### [9] [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/pdf/2505.22777)
*John Mendonça, Alon Lavie, Isabel Trancoso*

Main category: cs.CL

TL;DR: MEDAL is an automated multi-agent framework for generating and evaluating multilingual dialogue benchmarks, revealing LLMs' limitations in nuanced issue detection.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking datasets for chatbots are static, outdated, and lack multilingual coverage, hindering development.

Method: MEDAL uses LLMs to generate multilingual dialogues, evaluates them with GPT-4.1, and curates a new benchmark with human annotations.

Result: The framework uncovers cross-lingual performance differences and shows LLMs struggle with nuanced issues like empathy and reasoning.

Conclusion: MEDAL provides a more representative benchmark, highlighting the need for improved LLM evaluators in open-domain dialogues.

Abstract: As the capabilities of chatbots and their underlying LLMs continue to
dramatically improve, evaluating their performance has increasingly become a
major blocker to their further development. A major challenge is the available
benchmarking datasets, which are largely static, outdated, and lacking in
multilingual coverage, limiting their ability to capture subtle linguistic and
cultural variations. This paper introduces MEDAL, an automated multi-agent
framework for generating, evaluating, and curating more representative and
diverse open-domain dialogue evaluation benchmarks. Our approach leverages
several state-of-the-art LLMs to generate user-chatbot multilingual dialogues,
conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a
multidimensional analysis of the performance of the chatbots, uncovering
noticeable cross-lingual performance differences. Guided by this large-scale
evaluation, we curate a new meta-evaluation multilingual benchmark and
human-annotate samples with nuanced quality judgments. This benchmark is then
used to assess the ability of several reasoning and non-reasoning LLMs to act
as evaluators of open-domain dialogues. We find that current LLMs struggle to
detect nuanced issues, particularly those involving empathy and reasoning.

</details>


### [10] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/pdf/2505.23187)
*Yilong Li, Chen Qian, Yu Xia, Ruijie Shi, Yufan Dang, Zihao Xie, Ziming You, Weize Chen, Cheng Yang, Weichuan Liu, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: MAEL introduces cross-task experiential learning for LLM-based multi-agent systems, improving efficiency and generalization by leveraging prior task experiences.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat tasks in isolation, leading to redundancy and limited generalization. MAEL addresses this by enabling agents to learn from past experiences.

Method: MAEL uses a graph-structured collaboration network where agents share and store high-reward task-solving steps. During inference, agents retrieve relevant experiences to enhance reasoning.

Result: Experiments show MAEL achieves faster convergence and higher-quality solutions by leveraging prior experiences.

Conclusion: MAEL enhances multi-agent collaboration by enabling cross-task learning, improving both accuracy and efficiency.

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [11] [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/pdf/2505.22787)
*Christopher Polzak, Alejandro Lozano, Min Woo Sun, James Burgess, Yuhui Zhang, Kevin Wu, Serena Yeung-Levy*

Main category: cs.CL

TL;DR: LLMs struggle to match expert-conducted systematic reviews (SRs) in critical assessment and reasoning, despite access to the same studies. Performance varies with model size and fine-tuning, but issues like overconfidence and lack of skepticism persist.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs can replicate the conclusions of expert-conducted SRs when given the same studies, addressing the growing interest in automating SR generation.

Method: Benchmarking 24 LLMs (varying sizes and types) on MedEvidence, a dataset pairing 100 SRs with their underlying studies, to assess reasoning, performance, and behavior.

Result: Reasoning doesn't improve performance, larger models don't guarantee better results, and fine-tuning reduces accuracy. Models show overconfidence and lack skepticism toward low-quality findings.

Conclusion: LLMs are not yet reliable for matching expert SRs, highlighting the need for further research despite current clinical use.

Abstract: Systematic reviews (SR), in which experts summarize and analyze evidence
across individual studies to provide insights on a specialized topic, are a
cornerstone for evidence-based clinical decision-making, research, and policy.
Given the exponential growth of scientific articles, there is growing interest
in using large language models (LLMs) to automate SR generation. However, the
ability of LLMs to critically assess evidence and reason across multiple
documents to provide recommendations at the same proficiency as domain experts
remains poorly characterized. We therefore ask: Can LLMs match the conclusions
of systematic reviews written by clinical experts when given access to the same
studies? To explore this question, we present MedEvidence, a benchmark pairing
findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs
on MedEvidence, including reasoning, non-reasoning, medical specialist, and
models across varying sizes (from 7B-700B). Through our systematic evaluation,
we find that reasoning does not necessarily improve performance, larger models
do not consistently yield greater gains, and knowledge-based fine-tuning
degrades accuracy on MedEvidence. Instead, most models exhibit similar
behavior: performance tends to degrade as token length increases, their
responses show overconfidence, and, contrary to human experts, all models show
a lack of scientific skepticism toward low-quality findings. These results
suggest that more work is still required before LLMs can reliably match the
observations from expert-conducted SRs, even though these systems are already
deployed and being used by clinicians. We release our codebase and benchmark to
the broader research community to further investigate LLM-based SR systems.

</details>


### [12] [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/pdf/2505.22801)
*Qing Wang, Yuepei Li, Qiao Qiao, Kang Zhou, Qi Li*

Main category: cs.CL

TL;DR: MixORE is a two-phase framework for Open Relation Extraction (OpenRE) that handles mixed known and novel relations, outperforming baselines in classification and clustering.


<details>
  <summary>Details</summary>
Motivation: Traditional OpenRE assumes unlabeled data contains only novel relations or is pre-divided, but real-world data mixes known and novel relations arbitrarily.

Method: MixORE integrates relation classification and clustering in two phases to jointly learn known and novel relations.

Result: Experiments on three benchmarks show MixORE outperforms baselines in known relation classification and novel relation clustering.

Conclusion: MixORE advances generalized OpenRE research and is applicable to real-world scenarios.

Abstract: Open Relation Extraction (OpenRE) seeks to identify and extract novel
relational facts between named entities from unlabeled data without pre-defined
relation schemas. Traditional OpenRE methods typically assume that the
unlabeled data consists solely of novel relations or is pre-divided into known
and novel instances. However, in real-world scenarios, novel relations are
arbitrarily distributed. In this paper, we propose a generalized OpenRE setting
that considers unlabeled data as a mixture of both known and novel instances.
To address this, we propose MixORE, a two-phase framework that integrates
relation classification and clustering to jointly learn known and novel
relations. Experiments on three benchmark datasets demonstrate that MixORE
consistently outperforms competitive baselines in known relation classification
and novel relation clustering. Our findings contribute to the advancement of
generalized OpenRE research and real-world applications.

</details>


### [13] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/pdf/2505.22809)
*Andrew Zhu, Evan Osgood, Chris Callison-Burch*

Main category: cs.CL

TL;DR: The paper introduces "overhearing agents," LLM agents that assist users by listening to human conversations without active participation, demonstrated in Dungeons & Dragons gameplay.


<details>
  <summary>Details</summary>
Motivation: To explore an alternative interaction paradigm for LLM agents, focusing on passive assistance through "overhearing" human conversations.

Method: Utilized large multimodal audio-language models as overhearing agents in Dungeons & Dragons, conducting human evaluations to assess their helpfulness.

Result: Found that some models can perform overhearing tasks using implicit audio cues, demonstrating emergent abilities.

Conclusion: The study validates the feasibility of overhearing agents, releasing tools for further research.

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [14] [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/pdf/2505.22823)
*Yingming Wang, Pepa Atanasova*

Main category: cs.CL

TL;DR: SR-NLE is a framework that improves the faithfulness of LLM-generated explanations through iterative self-critique and refinement, reducing unfaithfulness rates by 18.79%.


<details>
  <summary>Details</summary>
Motivation: Existing LLM explanations often lack faithfulness to the model's reasoning, and the potential for self-improvement in this context is unexplored.

Method: SR-NLE uses iterative critique and refinement with feedback mechanisms like natural language self-feedback and feature attribution.

Result: Experiments show SR-NLE reduces unfaithfulness rates to 36.02% from 54.81%, a significant improvement.

Conclusion: LLMs can refine their explanations to better reflect reasoning with guided feedback, without additional training.

Abstract: With the rapid development of large language models (LLMs), natural language
explanations (NLEs) have become increasingly important for understanding model
predictions. However, these explanations often fail to faithfully represent the
model's actual reasoning process. While existing work has demonstrated that
LLMs can self-critique and refine their initial outputs for various tasks, this
capability remains unexplored for improving explanation faithfulness. To
address this gap, we introduce Self-critique and Refinement for Natural
Language Explanations (SR-NLE), a framework that enables models to improve the
faithfulness of their own explanations -- specifically, post-hoc NLEs --
through an iterative critique and refinement process without external
supervision. Our framework leverages different feedback mechanisms to guide the
refinement process, including natural language self-feedback and, notably, a
novel feedback approach based on feature attribution that highlights important
input words. Our experiments across three datasets and four state-of-the-art
LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with
our best method achieving an average unfaithfulness rate of 36.02%, compared to
54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal
that the investigated LLMs can indeed refine their explanations to better
reflect their actual reasoning process, requiring only appropriate guidance
through feedback without additional training or fine-tuning.

</details>


### [15] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/pdf/2505.22830)
*Alexander Gill, Abhilasha Ravichander, Ana Marasović*

Main category: cs.CL

TL;DR: LLMs can generate valid evaluation benchmarks at lower cost but produce less challenging data compared to human-authored benchmarks.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can meet the demands of creating high-quality evaluation benchmarks for reasoning tasks.

Method: Case studies comparing LLM-generated benchmarks (CondaQA and DROP variants) to human-crowdsourced ones, evaluating validity and difficulty.

Result: LLM-generated benchmarks are valid but less challenging for LLMs than human-authored ones.

Conclusion: The approach of using LLMs for benchmark creation needs critical reassessment due to reduced challenge.

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [16] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/pdf/2505.22842)
*Arthur S. Bianchessi, Rodrigo C. Barros, Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: The paper introduces BAM, a Bayesian framework for positional encoding in transformers, improving long-context generalization and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing positional encoding methods lack theoretical clarity and rely on limited evaluation metrics for extrapolation claims.

Method: Proposes BAM, a probabilistic model treating positional encoding as a prior, unifying existing methods and introducing a Generalized Gaussian positional prior.

Result: BAM achieves accurate information retrieval at 500× training context length, outperforming state-of-the-art in long-context accuracy with minimal added parameters.

Conclusion: BAM provides a theoretically grounded and empirically superior approach to positional encoding for transformers.

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [17] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/pdf/2505.22943)
*Jaewoo Ahn, Heeseung Yun, Dayoon Ko, Gunhee Kim*

Main category: cs.CL

TL;DR: MAC benchmark uses LLMs to exploit multimodal representation vulnerabilities, proposing a self-training method to improve zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Address compositional vulnerabilities in pre-trained multimodal representations like CLIP, which lead to counterintuitive judgments.

Method: Introduces MAC benchmark with LLM-generated deceptive text samples, evaluates via attack success rate and entropy-based diversity. Proposes self-training with rejection-sampling fine-tuning and diversity-promoting filtering.

Result: Superior performance in revealing vulnerabilities across modalities (images, videos, audios) using smaller models like Llama-3.1-8B.

Conclusion: MAC and self-training enhance attack success and diversity, effectively exposing multimodal representation weaknesses.

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [18] [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/pdf/2505.22848)
*Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces LITEX, a taxonomy for categorizing free-text explanations in NLI, addressing within-label variation and improving explanation generation.


<details>
  <summary>Details</summary>
Motivation: To address overlooked within-label variation in NLI and understand annotators' reasoning behind labels.

Method: Develop LITEX, a linguistically-informed taxonomy; annotate e-SNLI subset; validate taxonomy reliability; analyze alignment with labels, highlights, and explanations.

Result: LITEX captures within-label variation and improves explanation generation, aligning better with human reasoning than label/highlight-based methods.

Conclusion: LITEX bridges the gap between human and model explanations, offering a systematic approach to understanding NLI reasoning.

Abstract: There is increasing evidence of Human Label Variation (HLV) in Natural
Language Inference (NLI), where annotators assign different labels to the same
premise-hypothesis pair. However, within-label variation--cases where
annotators agree on the same label but provide divergent reasoning--poses an
additional and mostly overlooked challenge. Several NLI datasets contain
highlighted words in the NLI item as explanations, but the same spans on the
NLI item can be highlighted for different reasons, as evidenced by free-text
explanations, which offer a window into annotators' reasoning. To
systematically understand this problem and gain insight into the rationales
behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for
categorizing free-text explanations. Using this taxonomy, we annotate a subset
of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it
aligns with NLI labels, highlights, and explanations. We further assess the
taxonomy's usefulness in explanation generation, demonstrating that
conditioning generation on LITEX yields explanations that are linguistically
closer to human explanations than those generated using only labels or
highlights. Our approach thus not only captures within-label variation but also
shows how taxonomy-guided generation for reasoning can bridge the gap between
human and model explanations more effectively than existing strategies.

</details>


### [19] [Agentic Knowledgeable Self-awareness](https://arxiv.org/pdf/2504.03553)
*Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen*

Main category: cs.CL

TL;DR: KnowSelf introduces agentic knowledgeable self-awareness for LLM-based agents, enabling dynamic knowledge regulation for optimal planning with minimal costs.


<details>
  <summary>Details</summary>
Motivation: Traditional agent planning lacks situational self-awareness, leading to inefficient knowledge use. KnowSelf addresses this gap.

Method: Uses a heuristic situation judgement criterion and two-stage training to mark and utilize self-explored trajectories.

Result: Outperforms baselines with minimal external knowledge use.

Conclusion: KnowSelf enhances LLM-based agent planning by mimicking human-like self-awareness.

Abstract: Large Language Models (LLMs) have achieved considerable performance across
various agentic planning tasks. However, traditional agent planning approaches
adopt a "flood irrigation" methodology that indiscriminately injects gold
trajectories, external feedback, and domain knowledge into agent models. This
practice overlooks the fundamental human cognitive principle of situational
self-awareness during decision-making-the ability to dynamically assess
situational demands and strategically employ resources during decision-making.
We propose agentic knowledgeable self-awareness to address this gap, a novel
paradigm enabling LLM-based agents to autonomously regulate knowledge
utilization. Specifically, we propose KnowSelf, a data-centric approach that
applies agents with knowledgeable self-awareness like humans. Concretely, we
devise a heuristic situation judgement criterion to mark special tokens on the
agent's self-explored trajectories for collecting training data. Through a
two-stage training process, the agent model can switch between different
situations by generating specific special tokens, achieving optimal planning
effects with minimal costs. Our experiments demonstrate that KnowSelf can
outperform various strong baselines on different tasks and models with minimal
use of external knowledge. Code is available at
https://github.com/zjunlp/KnowSelf.

</details>


### [20] [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/pdf/2505.23170)
*Jian Zhu, Farhan Samir, Eleanor Chodroff, David R. Mortensen*

Main category: cs.CL

TL;DR: ZIPA introduces efficient speech models for crosslinguistic phone recognition, leveraging a large multilingual corpus and outperforming existing systems with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Advance state-of-the-art performance in crosslinguistic phone recognition by addressing data and model efficiency challenges.

Method: Utilizes IPAPack++ corpus, Zipformer backbones, and noisy student training for scaling. Includes ZIPA-T (transducer) and ZIPA-CR (CTC-based) variants.

Result: Outperforms existing phone recognition systems with fewer parameters. Error analysis highlights limitations in sociophonetic diversity.

Conclusion: ZIPA advances phone recognition but reveals challenges in modeling sociophonetic variation, pointing to future research directions.

Abstract: We present ZIPA, a family of efficient speech models that advances the
state-of-the-art performance of crosslinguistic phone recognition. We first
curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours
of normalized phone transcriptions and a novel evaluation set capturing unseen
languages and sociophonetic variation. With the large-scale training data,
ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage
the efficient Zipformer backbones and outperform existing phone recognition
systems with much fewer parameters. Further scaling via noisy student training
on 11,000 hours of pseudo-labeled multilingual data yields further improvement.
While ZIPA achieves strong performance on benchmarks, error analysis reveals
persistent limitations in modeling sociophonetic diversity, underscoring
challenges for future research.

</details>


### [21] [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/pdf/2505.22867)
*Iknoor Singh, Carolina Scarton, Kalina Bontcheva*

Main category: cs.CL

TL;DR: The paper introduces Hierarchical Three-Step Prompting (H3Prompt) for multilingual narrative classification, achieving top performance in SemEval 2025 Task 10 Subtask 2.


<details>
  <summary>Details</summary>
Motivation: The need for robust methods to classify online news narratives due to misinformation proliferation.

Method: A three-step LLM prompting strategy: domain classification, main narrative identification, and sub-narrative assignment.

Result: Top performance on the English test set among 28 teams.

Conclusion: H3Prompt is effective for multilingual narrative classification, with code publicly available.

Abstract: The proliferation of online news and the increasing spread of misinformation
necessitate robust methods for automatic data analysis. Narrative
classification is emerging as a important task, since identifying what is being
said online is critical for fact-checkers, policy markers and other
professionals working on information studies. This paper presents our approach
to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a
pre-defined two-level taxonomy of main narratives and sub-narratives across
multiple languages.
  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual
narrative classification. Our methodology follows a three-step Large Language
Model (LLM) prompting strategy, where the model first categorises an article
into one of two domains (Ukraine-Russia War or Climate Change), then identifies
the most relevant main narratives, and finally assigns sub-narratives. Our
approach secured the top position on the English test set among 28 competing
teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.

</details>


### [22] [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/pdf/2505.22888)
*Jirui Qi, Shan Chen, Zidi Xiong, Raquel Fernández, Danielle S. Bitterman, Arianna Bisazza*

Main category: cs.CL

TL;DR: Current Large Reasoning Models (LRMs) struggle with multilingual reasoning, often defaulting to English or producing fragmented traces. Prompt interventions improve readability but reduce accuracy, while targeted training mitigates some issues.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the multilingual reasoning capabilities of LRMs, as reasoning traces in users' languages are crucial for oversight in real-world applications.

Method: Evaluation of two leading LRM families on the XReasoning benchmark, testing reasoning in non-English languages. Interventions include prompt-based forcing and targeted post-training.

Result: Advanced models often revert to English or produce fragmented reasoning in other languages. Prompt interventions improve readability but reduce accuracy, while targeted training helps but doesn't fully resolve the issue.

Conclusion: Current LRMs have limited multilingual reasoning capabilities, highlighting a need for future improvements. Targeted training shows promise but doesn't eliminate accuracy trade-offs.

Abstract: Recent Large Reasoning Models (LRMs) with thinking traces have shown strong
performance on English reasoning tasks. However, their ability to think in
other languages is less studied. This capability is as important as answer
accuracy for real world applications because users may find the reasoning trace
useful for oversight only when it is expressed in their own language. We
comprehensively evaluate two leading families of LRMs on our XReasoning
benchmark and find that even the most advanced models often revert to English
or produce fragmented reasoning in other languages, revealing a substantial gap
in multilingual reasoning. Prompt based interventions that force models to
reason in the users language improve readability and oversight but reduce
answer accuracy, exposing an important trade off. We further show that targeted
post training on just 100 examples mitigates this mismatch, though some
accuracy loss remains. Our results highlight the limited multilingual reasoning
capabilities of current LRMs and outline directions for future work. Code and
data are available at https://github.com/Betswish/mCoT-XReasoning.

</details>


### [23] [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/pdf/2505.22897)
*Chahat Raj, Bowen Wei, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu*

Main category: cs.CL

TL;DR: A benchmark called VIGNETTE is introduced to evaluate bias in vision-language models (VLMs) across four areas, revealing complex stereotypes and social hierarchies.


<details>
  <summary>Details</summary>
Motivation: Bias in VLMs is understudied compared to LLMs, with existing work focusing narrowly on gender-occupation associations.

Method: VIGNETTE, a large-scale VQA benchmark with 30M+ images, assesses bias through question-answering in four directions: factuality, perception, stereotyping, and decision making.

Result: The study uncovers subtle, multifaceted stereotypical patterns in VLMs, showing how they encode social hierarchies and make biased assumptions.

Conclusion: The work highlights the need for broader bias evaluation in VLMs, revealing their construction of social meaning from inputs.

Abstract: While bias in large language models (LLMs) is well-studied, similar concerns
in vision-language models (VLMs) have received comparatively less attention.
Existing VLM bias studies often focus on portrait-style images and
gender-occupation associations, overlooking broader and more complex social
stereotypes and their implied harm. This work introduces VIGNETTE, a
large-scale VQA benchmark with 30M+ images for evaluating bias in VLMs through
a question-answering framework spanning four directions: factuality,
perception, stereotyping, and decision making. Beyond narrowly-centered
studies, we assess how VLMs interpret identities in contextualized settings,
revealing how models make trait and capability assumptions and exhibit patterns
of discrimination. Drawing from social psychology, we examine how VLMs connect
visual identity cues to trait and role-based inferences, encoding social
hierarchies, through biased selections. Our findings uncover subtle,
multifaceted, and surprising stereotypical patterns, offering insights into how
VLMs construct social meaning from inputs.

</details>


### [24] [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/pdf/2505.22910)
*Chahat Raj, Mahika Banerjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu*

Main category: cs.CL

TL;DR: The paper explores how LLMs attribute event outcomes (e.g., exam failure) to internal (effort) or external (test difficulty) factors, with fairness implications. It proposes a cognitively grounded framework to evaluate biases in LLMs' reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs assign responsibility for events is crucial for fairness, as biases in attribution can reinforce stereotypes and influence decisions.

Method: The work introduces a cognitively grounded bias evaluation framework to analyze LLMs' reasoning disparities and biases toward demographic groups.

Result: The framework identifies how LLMs' attribution patterns channelize biases, moving beyond surface-level associations to deeper reasoning disparities.

Conclusion: The proposed framework offers a nuanced approach to evaluating biases in LLMs, addressing fairness implications of attribution patterns.

Abstract: When a student fails an exam, do we tend to blame their effort or the test's
difficulty? Attribution, defined as how reasons are assigned to event outcomes,
shapes perceptions, reinforces stereotypes, and influences decisions.
Attribution Theory in social psychology explains how humans assign
responsibility for events using implicit cognition, attributing causes to
internal (e.g., effort, ability) or external (e.g., task difficulty, luck)
factors. LLMs' attribution of event outcomes based on demographics carries
important fairness implications. Most works exploring social biases in LLMs
focus on surface-level associations or isolated stereotypes. This work proposes
a cognitively grounded bias evaluation framework to identify how models'
reasoning disparities channelize biases toward demographic groups.

</details>


### [25] [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/pdf/2505.22919)
*Nikita Mehandru, Niloufar Golchini, David Bamman, Travis Zack, Melanie F. Molina, Ahmed Alaa*

Main category: cs.CL

TL;DR: ER-Reason is a benchmark for evaluating LLMs in emergency room (ER) clinical reasoning, highlighting gaps between AI and clinician decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world clinical workflow and reasoning, relying on costly human annotators and isolated tasks.

Method: ER-Reason uses 3,984 patient records and 25,174 clinical notes, structured around ER workflow stages and clinical reasoning processes like differential diagnosis.

Result: State-of-the-art LLMs show a gap in clinical reasoning compared to clinician-authored rationales.

Conclusion: Future research is needed to improve LLMs' clinical reasoning for ER decisions.

Abstract: Large language models (LLMs) have been extensively evaluated on medical
question answering tasks based on licensing exams. However, real-world
evaluations often depend on costly human annotators, and existing benchmarks
tend to focus on isolated tasks that rarely capture the clinical reasoning or
full workflow underlying medical decisions. In this paper, we introduce
ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and
decision-making in the emergency room (ER)--a high-stakes setting where
clinicians make rapid, consequential decisions across diverse patient
presentations and medical specialties under time pressure. ER-Reason includes
data from 3,984 patients, encompassing 25,174 de-identified longitudinal
clinical notes spanning discharge summaries, progress notes, history and
physical exams, consults, echocardiography reports, imaging notes, and ER
provider documentation. The benchmark includes evaluation tasks that span key
stages of the ER workflow: triage intake, initial assessment, treatment
selection, disposition planning, and final diagnosis--each structured to
reflect core clinical reasoning processes such as differential diagnosis via
rule-out reasoning. We also collected 72 full physician-authored rationales
explaining reasoning processes that mimic the teaching process used in
residency training, and are typically absent from ER documentation. Evaluations
of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and
clinician-authored clinical reasoning for ER decisions, highlighting the need
for future research to bridge this divide.

</details>


### [26] [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/pdf/2505.22921)
*Yue Xing, Tao Yang, Yijiashun Qi, Minggu Wei, Yu Cheng, Honghui Xin*

Main category: cs.CL

TL;DR: The paper proposes a model with long-term memory mechanisms to improve context retention in language models, showing better performance in text generation, question answering, and reasoning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of large language models in understanding long-term context, particularly issues like context loss and semantic drift.

Method: Introduces a model with explicit memory units, gated writing, attention-based reading, and a forgetting function, trained with a joint objective combining task loss and memory constraints.

Result: The model outperforms in text generation consistency, multi-turn QA stability, and cross-context reasoning, mitigating context loss and semantic drift.

Conclusion: Memory mechanisms are crucial for language understanding; the proposed approach is feasible and effective in design and performance.

Abstract: This paper addresses the limitations of large language models in
understanding long-term context. It proposes a model architecture equipped with
a long-term memory mechanism to improve the retention and retrieval of semantic
information across paragraphs and dialogue turns. The model integrates explicit
memory units, gated writing mechanisms, and attention-based reading modules. A
forgetting function is introduced to enable dynamic updates of memory content,
enhancing the model's ability to manage historical information. To further
improve the effectiveness of memory operations, the study designs a joint
training objective. This combines the main task loss with constraints on memory
writing and forgetting. It guides the model to learn better memory strategies
during task execution. Systematic evaluation across multiple subtasks shows
that the model achieves clear advantages in text generation consistency,
stability in multi-turn question answering, and accuracy in cross-context
reasoning. In particular, the model demonstrates strong semantic retention and
contextual coherence in long-text tasks and complex question answering
scenarios. It effectively mitigates the context loss and semantic drift
problems commonly faced by traditional language models when handling long-term
dependencies. The experiments also include analysis of different memory
structures, capacity sizes, and control strategies. These results further
confirm the critical role of memory mechanisms in language understanding. They
demonstrate the feasibility and effectiveness of the proposed approach in both
architectural design and performance outcomes.

</details>


### [27] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/pdf/2505.22934)
*Haobo Zhang, Jiayu Zhou*

Main category: cs.CL

TL;DR: Proposes OSRM, a method to improve merging of LoRA-fine-tuned models by constraining subspaces to avoid task interference, enhancing performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Addresses performance degradation in merging LoRA-fine-tuned models due to overlooked parameter-data interplay.

Method: Uses Orthogonal Subspaces for Robust model Merging (OSRM) to constrain LoRA subspaces before fine-tuning, reducing task interference.

Result: Boosts merging performance, preserves single-task accuracy, and shows robustness to hyperparameters across eight datasets and multiple LMs.

Conclusion: Highlights data-parameter interaction's role in merging and offers a plug-and-play solution for LoRA model merging.

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [28] [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/pdf/2505.22937)
*Ngeyen Yinkfu*

Main category: cs.CL

TL;DR: An efficient transformer-based QA model optimized for Intel CPUs achieves a balance between accuracy and speed, outperforming rule-based and full BERT models.


<details>
  <summary>Details</summary>
Motivation: To develop a QA model suitable for real-time applications on resource-constrained systems like Intel CPUs.

Method: Uses DistilBERT architecture with data augmentation, fine-tuning, and systematic hyperparameter optimization.

Result: Achieves F1 score of 0.6536 with 0.1208s inference time per question, outperforming baselines.

Conclusion: The model offers a practical solution for CPU-based inference, balancing accuracy and efficiency.

Abstract: This study presents an efficient transformer-based question-answering (QA)
model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the
Stanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data
analysis, data augmentation, and fine-tuning of a DistilBERT architecture, the
model achieves a validation F1 score of 0.6536 with an average inference time
of 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124)
and full BERT-based models, our approach offers a favorable trade-off between
accuracy and computational efficiency. This makes it well-suited for real-time
applications on resource-constrained systems. The study includes systematic
evaluation of data augmentation strategies and hyperparameter configurations,
providing practical insights into optimizing transformer models for CPU-based
inference.

</details>


### [29] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/pdf/2505.22942)
*Yuchen Zhuang, Di Jin, Jiaao Chen, Wenqi Shi, Hanrui Wang, Chao Zhang*

Main category: cs.CL

TL;DR: WorkForceAgent-R1, an LLM-based web agent using rule-based R1-style reinforcement learning, outperforms SFT baselines by 10.26-16.59% in business-oriented web navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing web agents struggle with generalization and robustness due to insufficient reasoning in dynamic web interactions.

Method: Rule-based R1-style reinforcement learning with a structured reward function for single-step reasoning and planning.

Result: Substantially outperforms SFT baselines by 10.26-16.59% and achieves competitive performance with proprietary LLM-based agents.

Conclusion: WorkForceAgent-R1 enhances robustness and reasoning in business-oriented web navigation tasks.

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [30] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/pdf/2505.22945)
*Alisha Srivastava, Emir Korukluoglu, Minh Nhat Le, Duyen Tran, Chau Minh Pham, Marzena Karpinska, Mohit Iyyer*

Main category: cs.CL

TL;DR: The paper explores multilingual and cross-lingual memorization in LLMs, finding they recall content across languages, even without direct translation in pretraining data.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs' memorization ability generalizes to non-English languages or transfers across languages.

Method: Uses the OWL dataset (31.5K aligned excerpts in 10 languages) and evaluates memorization via direct probing, name cloze, and prefix probing tasks.

Result: LLMs recall content across languages; GPT-4o identifies authors/titles 69% of the time and masked entities 6% of the time in new translations.

Conclusion: LLMs exhibit significant cross-lingual memorization, with modest reductions from perturbations, revealing differences between models.

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [31] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/pdf/2505.22946)
*Yuhui Zhang, Yuchang Su, Yiming Liu, Serena Yeung-Levy*

Main category: cs.CL

TL;DR: NegVQA is a benchmark for evaluating vision-language models' (VLMs) understanding of negation, revealing significant gaps in their performance.


<details>
  <summary>Details</summary>
Motivation: Assessing VLMs' comprehension of negation is crucial as they advance in high-stakes applications.

Method: NegVQA was created by generating negated questions from existing VQA datasets using large language models, and 20 VLMs were evaluated.

Result: VLMs struggle with negation, showing a performance drop and a U-shaped scaling trend with model size.

Conclusion: NegVQA highlights critical gaps in VLMs' negation understanding and provides insights for future development.

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [32] [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/pdf/2505.22950)
*Haohan Yuan, Sukhwa Hong, Haopeng Zhang*

Main category: cs.CL

TL;DR: StrucSum is a training-free prompting framework that improves LLM summarization by incorporating document structure through graph-based strategies, outperforming baselines in quality and factual consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with document structure and salient information in long texts for zero-shot summarization.

Method: StrucSum uses three strategies: Neighbor-Aware Prompting (NAP), Centrality-Aware Prompting (CAP), and Centrality-Guided Masking (CGM) to inject structural signals into prompts.

Result: Experiments show StrucSum improves summary quality and factual consistency, with notable gains on ArXiv (19.2 and 9.7 points for FactCC and SummaC).

Conclusion: Structure-aware prompting is effective for zero-shot extractive summarization with LLMs, requiring no training or task-specific tuning.

Abstract: Large language models (LLMs) have shown strong performance in zero-shot
summarization, but often struggle to model document structure and identify
salient information in long texts. In this work, we introduce StrucSum, a
training-free prompting framework that enhances LLM reasoning through
sentence-level graph structures. StrucSum injects structural signals into
prompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local
context, Centrality-Aware Prompting (CAP) for importance estimation, and
Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on
ArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves
both summary quality and factual consistency over unsupervised baselines and
vanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and
9.7 points, indicating stronger alignment between summaries and source content.
These findings suggest that structure-aware prompting is a simple yet effective
approach for zero-shot extractive summarization with LLMs, without any training
or task-specific tuning.

</details>


### [33] [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/pdf/2505.22956)
*Matteo Guida, Yulia Otmakhova, Eduard Hovy, Lea Frermann*

Main category: cs.CL

TL;DR: The paper evaluates four LLMs for argument mining in online comments on polarizing topics, showing strong performance but limitations in handling nuanced or emotional language.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' effectiveness in detecting and understanding topic-specific arguments in public discussions, given their underexplored performance in this context.

Method: Evaluation of four state-of-the-art LLMs on three argument mining tasks using datasets of over 2,000 opinion comments across six polarizing topics.

Result: LLMs performed well, especially large and fine-tuned models, but struggled with long, nuanced, or emotionally charged comments.

Conclusion: LLMs show promise for automated argument analysis but have limitations for nuanced or emotional content, impacting applications like content moderation.

Abstract: Automated large-scale analysis of public discussions around contested issues
like abortion requires detecting and understanding the use of arguments. While
Large Language Models (LLMs) have shown promise in language processing tasks,
their performance in mining topic-specific, pre-defined arguments in online
comments remains underexplored. We evaluate four state-of-the-art LLMs on three
argument mining tasks using datasets comprising over 2,000 opinion comments
across six polarizing topics. Quantitative evaluation suggests an overall
strong performance across the three tasks, especially for large and fine-tuned
LLMs, albeit at a significant environmental cost. However, a detailed error
analysis revealed systematic shortcomings on long and nuanced comments and
emotionally charged language, raising concerns for downstream applications like
content moderation or opinion analysis. Our results highlight both the promise
and current limitations of LLMs for automated argument analysis in online
comments.

</details>


### [34] [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/pdf/2505.22959)
*Jianwei Wang, Mengqi Wang, Yinsi Zhou, Zhenchang Xing, Qing Liu, Xiwei Xu, Wenjie Zhang, Liming Zhu*

Main category: cs.CL

TL;DR: HSE-Bench is introduced as the first benchmark to evaluate LLMs' HSE compliance assessment, revealing reliance on semantic matching over principled reasoning. A new prompting technique, RoE, is proposed to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capacity of LLMs in domain-specific HSE knowledge and structured legal reasoning for compliance assessment.

Method: HSE-Bench, a dataset of 1,000+ curated questions, evaluates LLMs using IRAC reasoning flow. RoE prompting is introduced to simulate expert reasoning.

Result: LLMs perform well but rely on semantic matching, lacking systematic legal reasoning. RoE improves decision accuracy.

Conclusion: The study highlights reasoning gaps in LLMs for HSE compliance and encourages further research.

Abstract: Health, Safety, and Environment (HSE) compliance assessment demands dynamic
real-time decision-making under complicated regulations and complex
human-machine-environment interactions. While large language models (LLMs) hold
significant potential for decision intelligence and contextual dialogue, their
capacity for domain-specific knowledge in HSE and structured legal reasoning
remains underexplored. We introduce HSE-Bench, the first benchmark dataset
designed to evaluate the HSE compliance assessment capabilities of LLM.
HSE-Bench comprises over 1,000 manually curated questions drawn from
regulations, court cases, safety exams, and fieldwork videos, and integrates a
reasoning flow based on Issue spotting, rule Recall, rule Application, and rule
Conclusion (IRAC) to assess the holistic reasoning pipeline. We conduct
extensive evaluations on different prompting strategies and more than 10 LLMs,
including foundation models, reasoning models and multimodal vision models. The
results show that, although current LLMs achieve good performance, their
capabilities largely rely on semantic matching rather than principled reasoning
grounded in the underlying HSE compliance context. Moreover, their native
reasoning trace lacks the systematic legal reasoning required for rigorous HSE
compliance assessment. To alleviate these, we propose a new prompting
technique, Reasoning of Expert (RoE), which guides LLMs to simulate the
reasoning process of different experts for compliance assessment and reach a
more accurate unified decision. We hope our study highlights reasoning gaps in
LLMs for HSE compliance and inspires further research on related tasks.

</details>


### [35] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/pdf/2505.22961)
*Peixuan Han, Zijia Liu, Jiaxuan You*

Main category: cs.CL

TL;DR: ToMAP enhances LLM persuaders by incorporating Theory of Mind modules, improving opponent awareness and argument diversity, outperforming larger models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack dynamic Theory of Mind reasoning, limiting persuasion diversity and opponent awareness.

Method: ToMAP uses two ToM modules: prompting for objections and predicting opponent stance via MLP. Reinforcement learning refines argument generation.

Result: ToMAP (3B params) outperforms GPT-4o by 39.4%, showing diverse, logical arguments and reduced repetition.

Conclusion: ToMAP's opponent-aware design improves persuasion effectiveness, suitable for long conversations and diverse strategies.

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [36] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/pdf/2505.22964)
*Sheng Zhang, Qin Liu, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon*

Main category: cs.CL

TL;DR: The paper explores scaling laws in EHR foundation models, showing they behave similarly to LLMs, with predictable performance gains from scaling model size, data, and compute.


<details>
  <summary>Details</summary>
Motivation: Scaling laws are well-studied in LLMs but unexplored in EHRs, despite their rich, sequential data. This work aims to bridge that gap.

Method: Transformer models were trained on MIMIC-IV patient timeline data, varying model sizes and compute budgets to analyze scaling patterns.

Result: Consistent scaling patterns emerged, including parabolic IsoFLOPs curves and power-law relationships, similar to LLMs.

Conclusion: The findings support resource-efficient training of EHR models, enabling advancements in clinical prediction and personalized healthcare.

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [37] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/pdf/2505.22993)
*Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: VeGraph is a novel framework using LLMs for claim verification, addressing entity disambiguation and structured representation to improve accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Traditional claim verification struggles with complex claims and ambiguous entities, lacking effective disambiguation strategies.

Method: VeGraph decomposes claims into graph-based triplets, resolves ambiguous entities iteratively, and verifies remaining triplets.

Result: VeGraph outperforms baselines on HoVer and FEVEROUS benchmarks using Meta-Llama-3-70B.

Conclusion: VeGraph effectively tackles claim verification challenges, offering a robust and explainable solution.

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [38] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/pdf/2505.23001)
*Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi*

Main category: cs.CL

TL;DR: DyePack is a framework using backdoor attacks to detect if models trained on benchmark test sets, ensuring low false positive rates and strong evidence for contamination.


<details>
  <summary>Details</summary>
Motivation: Open benchmarks are prone to test set contamination, necessitating a method to detect misuse without accessing model internals.

Method: DyePack introduces backdoor samples into test data, using stochastic targets for precise false positive rate control.

Result: It detects contamination with extremely low false positive rates (e.g., 0.000073% on MMLU-Pro) across multiple datasets and tasks.

Conclusion: DyePack effectively identifies benchmark misuse with provable guarantees, enhancing reproducibility in model evaluation.

Abstract: Open benchmarks are essential for evaluating and advancing large language
models, offering reproducibility and transparency. However, their accessibility
makes them likely targets of test set contamination. In this work, we introduce
DyePack, a framework that leverages backdoor attacks to identify models that
used benchmark test sets during training, without requiring access to the loss,
logits, or any internal details of the model. Like how banks mix dye packs with
their money to mark robbers, DyePack mixes backdoor samples with the test data
to flag models that trained on it. We propose a principled design incorporating
multiple backdoors with stochastic targets, enabling exact false positive rate
(FPR) computation when flagging every model. This provably prevents false
accusations while providing strong evidence for every detected case of
contamination. We evaluate DyePack on five models across three datasets,
covering both multiple-choice and open-ended generation tasks. For
multiple-choice questions, it successfully detects all contaminated models with
guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard
using eight backdoors. For open-ended generation tasks, it generalizes well and
identifies all contaminated models on Alpaca with a guaranteed false positive
rate of just 0.127% using six backdoors.

</details>


### [39] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/pdf/2505.23006)
*Chiwan Park, Wonjun Jang, Daeryong Kim, Aelim Ahn, Kichang Yang, Woosung Hwang, Jihyeon Roh, Hyerin Park, Hyosun Wang, Min Seok Kim, Jihoon Kang*

Main category: cs.CL

TL;DR: The paper addresses the challenge of applying SOTA LLMs in industrial settings by balancing conversational flexibility with service-specific constraints, demonstrated through an e-commerce chatbot case study.


<details>
  <summary>Details</summary>
Motivation: The need to reconcile the probabilistic nature of LLMs with strict industrial requirements in applications like search, recommendation, and chatbots.

Method: Proposes a framework for developing scalable, controllable, and reliable AI-driven agents, detailed via an e-commerce conversational agent case study.

Result: Provides insights and strategies for bridging the gap between academic research and real-world LLM applications.

Conclusion: Introduces a practical framework for deploying LLMs in industrial settings, ensuring both flexibility and compliance.

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [40] [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/pdf/2505.23015)
*Jinwen Chen, Hainan Zhang, Fei Sun, Qinnan Zhang, Sijia Wen, Ziwei Wang, Zhiming Zheng*

Main category: cs.CL

TL;DR: The paper proposes RFTC, a method to detect stealthy backdoor samples in LLMs using Reference-Filtration and TF-IDF clustering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods for poisoned samples in LLMs are either ineffective for generation tasks or degrade performance, necessitating a better solution.

Method: RFTC combines Reference-Filtration (comparing responses to a reference model) and TF-IDF clustering (analyzing intra-class distances) to identify poisoned samples.

Result: RFTC shows superior performance in backdoor detection and model performance on translation and QA datasets compared to baselines.

Conclusion: RFTC effectively addresses the challenge of detecting stealthy backdoor samples in LLMs without compromising generation quality.

Abstract: Fine-tuning LLMs with datasets containing stealthy backdoors from publishers
poses security risks to downstream applications. Mainstream detection methods
either identify poisoned samples by analyzing the prediction probability of
poisoned classification models or rely on the rewriting model to eliminate the
stealthy triggers. However, the former cannot be applied to generation tasks,
while the latter may degrade generation performance and introduce new triggers.
Therefore, efficiently eliminating stealthy poisoned samples for LLMs remains
an urgent problem. We observe that after applying TF-IDF clustering to the
sample response, there are notable differences in the intra-class distances
between clean and poisoned samples. Poisoned samples tend to cluster closely
because of their specific malicious outputs, whereas clean samples are more
scattered due to their more varied responses. Thus, in this paper, we propose a
stealthy backdoor sample detection method based on Reference-Filtration and
Tfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample
response with the reference model's outputs and consider the sample suspicious
if there's a significant discrepancy. And then we perform TF-IDF clustering on
these suspicious samples to identify the true poisoned samples based on the
intra-class distance. Experiments on two machine translation datasets and one
QA dataset demonstrate that RFTC outperforms baselines in backdoor detection
and model performance. Further analysis of different reference models also
confirms the effectiveness of our Reference-Filtration.

</details>


### [41] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/pdf/2505.23026)
*Haewon Park, Gyubin Choi, Minjun Kim, Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces CHED, a benchmark for evaluating context robustness in knowledge editing (KE) methods, and CoRE, a KE method to improve robustness by reducing context-sensitive variance in hidden states.


<details>
  <summary>Details</summary>
Motivation: Current KE evaluations ignore preceding contexts, which can trigger original knowledge retrieval and undermine edits. The paper aims to address this gap.

Method: Develop CHED benchmark for context robustness evaluation and propose CoRE, a KE method minimizing context-sensitive variance in hidden states.

Result: KE methods often fail with preceding contexts. CoRE improves editing success while preserving model capabilities.

Conclusion: CoRE enhances context robustness in KE, and CHED provides a valuable benchmark for future evaluations.

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [42] [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/pdf/2505.23029)
*Si Wu, Sebastian Bruch*

Main category: cs.CL

TL;DR: The paper proposes an unsupervised measure, Neighborhood Stability Measure (NSM), to estimate imageability and concreteness of text using semantic embedding space, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To leverage text signals in image-caption datasets for estimating psycholinguistic properties like imageability and concreteness without relying on parallel visual-semantic spaces.

Method: Introduces NSM, which quantifies the peakedness of a word's neighborhood in semantic embedding space to reflect imageability and concreteness.

Result: NSM correlates more strongly with ground-truth ratings than existing unsupervised methods and is effective for classification.

Conclusion: Text in image-caption datasets provides sufficient signals for accurate estimation of imageability and concreteness using NSM.

Abstract: Imageability (potential of text to evoke a mental image) and concreteness
(perceptibility of text) are two psycholinguistic properties that link visual
and semantic spaces. It is little surprise that computational methods that
estimate them do so using parallel visual and semantic spaces, such as
collections of image-caption pairs or multi-modal models. In this paper, we
work on the supposition that text itself in an image-caption dataset offers
sufficient signals to accurately estimate these properties. We hypothesize, in
particular, that the peakedness of the neighborhood of a word in the semantic
embedding space reflects its degree of imageability and concreteness. We then
propose an unsupervised, distribution-free measure, which we call Neighborhood
Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive
experiments show that NSM correlates more strongly with ground-truth ratings
than existing unsupervised methods, and is a strong predictor of these
properties for classification. Our code and data are available on GitHub
(https://github.com/Artificial-Memory-Lab/imageability).

</details>


### [43] [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/pdf/2505.23030)
*Shruti Hegde, Mabon Manoj Ninan, Jonathan R. Dillman, Shireen Hayatghaibi, Lynn Babcock, Elanchezhian Somasundaram*

Main category: cs.CL

TL;DR: The study evaluates four commercial NLP systems and two dedicated tools for labeling pediatric CXR reports, finding significant variability in performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of general-purpose clinical NLP tools for specific tasks like pediatric CXR report labeling, given limited independent evaluations.

Method: Compared four NLP systems (AWS, GC, AZ, SP) and two dedicated tools (CheXpert, CheXbert) on 95,008 pediatric CXR reports, analyzing entity extraction and assertion detection.

Result: Significant differences in entity extraction and assertion accuracy (62% average, SP highest at 76%, AWS lowest at 50%). CheXpert and CheXbert achieved 56% accuracy.

Conclusion: Variability in NLP tool performance underscores the need for validation before clinical deployment.

Abstract: General-purpose clinical natural language processing (NLP) tools are
increasingly used for the automatic labeling of clinical reports. However,
independent evaluations for specific tasks, such as pediatric chest radiograph
(CXR) report labeling, are limited. This study compares four commercial
clinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP
(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and
assertion detection in pediatric CXR reports. Additionally, CheXpert and
CheXbert, two dedicated chest radiograph report labelers, were evaluated on the
same task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR
reports from a large academic pediatric hospital. Entities and assertion
statuses (positive, negative, uncertain) from the findings and impression
sections were extracted by the NLP systems, with impression section entities
mapped to 12 disease categories and a No Findings category. CheXpert and
CheXbert extracted the same 13 categories. Outputs were compared using Fleiss
Kappa and accuracy against a consensus pseudo-ground truth. Significant
differences were found in the number of extracted entities and assertion
distributions across NLP systems. SP extracted 49,688 unique entities, GC
16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged
around 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert
achieved 56% accuracy. Considerable variability in performance highlights the
need for careful validation and review before deploying NLP tools for clinical
report labeling.

</details>


### [44] [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/pdf/2505.23035)
*Hyunwoo Kim, Hanau Yi*

Main category: cs.CL

TL;DR: The paper explores Machine-Facing English (MFE), a language register adapted for AI interaction, highlighting its syntactic and pragmatic simplifications for machine parseability.


<details>
  <summary>Details</summary>
Motivation: To understand how human-AI interaction shapes language, focusing on features that enhance machine understanding but reduce natural fluency.

Method: Qualitative analysis of bilingual (Korean/English) voice and text interactions, using Natural Language Declarative Prompting (NLD-P) and thematic analysis.

Result: Identified five traits (redundant clarity, directive syntax, controlled vocabulary, flattened prosody, single-intent structuring) that improve AI execution but limit expressiveness.

Conclusion: MFE reveals a trade-off between efficiency and richness, posing challenges for conversational AI design and multilingual users, calling for further research.

Abstract: Machine-Facing English (MFE) is an emergent register shaped by the adaptation
of everyday language to the expanding presence of AI interlocutors. Drawing on
register theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience
design (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this
study traces how sustained human-AI interaction normalizes syntactic rigidity,
pragmatic simplification, and hyper-explicit phrasing - features that enhance
machine parseability at the expense of natural fluency. Our analysis is
grounded in qualitative observations from bilingual (Korean/English) voice- and
text-based product testing sessions, with reflexive drafting conducted using
Natural Language Declarative Prompting (NLD-P) under human curation. Thematic
analysis identifies five recurrent traits - redundant clarity, directive
syntax, controlled vocabulary, flattened prosody, and single-intent structuring
- that improve execution accuracy but compress expressive range. MFE's
evolution highlights a persistent tension between communicative efficiency and
linguistic richness, raising design challenges for conversational interfaces
and pedagogical considerations for multilingual users. We conclude by
underscoring the need for comprehensive methodological exposition and future
empirical validation.

</details>


### [45] [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/pdf/2505.23037)
*Longyin Zhang, Bowei Zou, Ai Ti Aw*

Main category: cs.CL

TL;DR: The paper proposes a method (CAT-G) using multilingual LLMs to generate aspect terms from social media comments, improving NLP tasks like clustering and summarization. It also introduces a multilingual test set for comparative analysis.


<details>
  <summary>Details</summary>
Motivation: Social media posts' unstructured and diverse nature complicates NLP tasks. The paper aims to enhance comprehension by focusing on aspect terms.

Method: Uses multilingual LLMs with supervised fine-tuning for aspect term generation (CAT-G) and aligns predictions with human expectations via DPO.

Result: The method improves social media discourse comprehension in NLP tasks and introduces a multilingual test set for performance comparison.

Conclusion: CAT-G effectively addresses challenges in social media NLP tasks and provides a multilingual benchmark for future research.

Abstract: The inherent nature of social media posts, characterized by the freedom of
language use with a disjointed array of diverse opinions and topics, poses
significant challenges to downstream NLP tasks such as comment clustering,
comment summarization, and social media opinion analysis. To address this, we
propose a granular level of identifying and generating aspect terms from
individual comments to guide model attention. Specifically, we leverage
multilingual large language models with supervised fine-tuning for comment
aspect term generation (CAT-G), further aligning the model's predictions with
human expectations through DPO. We demonstrate the effectiveness of our method
in enhancing the comprehension of social media discourse on two NLP tasks.
Moreover, this paper contributes the first multilingual CAT-G test set on
English, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among
languages, this test set allows for a comparative analysis of performance
across languages with varying levels of LLM proficiency.

</details>


### [46] [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/pdf/2505.23038)
*Yuzhen Xiao, Jiahe Song, Yongxin Xu, Ruizhe Zhang, Yiqi Xiao, Xin Lu, Runchuan Zhu, Bowen Jiang, Junfeng Zhao*

Main category: cs.CL

TL;DR: EL4NER is an ensemble learning method for NER tasks that aggregates outputs of small-parameter LLMs, reducing costs and improving performance compared to large-parameter LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the high computational and API costs, data-privacy concerns, and limited collaboration in ICL-based NER methods relying on large-parameter LLMs.

Method: EL4NER uses a task decomposition pipeline, span-level sentence similarity for ICL demonstration retrieval, and a self-validation mechanism to reduce noise.

Result: EL4NER outperforms large-parameter LLM methods at lower cost and achieves SOTA performance on some datasets.

Conclusion: EL4NER demonstrates the feasibility of using small-parameter LLMs in ICL for NER tasks, offering parameter efficiency and cost-effectiveness.

Abstract: In-Context Learning (ICL) technique based on Large Language Models (LLMs) has
gained prominence in Named Entity Recognition (NER) tasks for its lower
computing resource consumption, less manual labeling overhead, and stronger
generalizability. Nevertheless, most ICL-based NER methods depend on
large-parameter LLMs: the open-source models demand substantial computational
resources for deployment and inference, while the closed-source ones incur high
API costs, raise data-privacy concerns, and hinder community collaboration. To
address this question, we propose an Ensemble Learning Method for Named Entity
Recognition (EL4NER), which aims at aggregating the ICL outputs of multiple
open-source, small-parameter LLMs to enhance overall performance in NER tasks
at less deployment and inference cost. Specifically, our method comprises three
key components. First, we design a task decomposition-based pipeline that
facilitates deep, multi-stage ensemble learning. Second, we introduce a novel
span-level sentence similarity algorithm to establish an ICL demonstration
retrieval mechanism better suited for NER tasks. Third, we incorporate a
self-validation mechanism to mitigate the noise introduced during the ensemble
process. We evaluated EL4NER on multiple widely adopted NER datasets from
diverse domains. Our experimental results indicate that EL4NER surpasses most
closed-source, large-parameter LLM-based methods at a lower parameter cost and
even attains state-of-the-art (SOTA) performance among ICL-based methods on
certain datasets. These results show the parameter efficiency of EL4NER and
underscore the feasibility of employing open-source, small-parameter LLMs
within the ICL paradigm for NER tasks.

</details>


### [47] [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/pdf/2505.23052)
*Jiarui Zhang, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Guihai Chen*

Main category: cs.CL

TL;DR: RAGRouter improves LLM routing in RAG scenarios by dynamically incorporating document influence, outperforming existing methods by 3.29%-9.33%.


<details>
  <summary>Details</summary>
Motivation: Existing routing methods for retrieval-augmented LLMs rely on static knowledge representations, leading to suboptimal performance due to dynamic document influence.

Method: Proposes RAGRouter, which uses document embeddings and RAG capability embeddings with contrastive learning to capture knowledge shifts for informed routing.

Result: RAGRouter outperforms the best individual LLM by 3.61% on average and existing routing methods by 3.29%-9.33%.

Conclusion: RAGRouter effectively addresses the retrieval-augmented LLM routing problem, achieving strong performance-efficiency trade-offs.

Abstract: Retrieval-Augmented Generation (RAG) significantly improves the performance
of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying
response quality across LLMs under RAG necessitates intelligent routing
mechanisms, which select the most suitable model for each query from multiple
retrieval-augmented LLMs via a dedicated router model. We observe that external
documents dynamically affect LLMs' ability to answer queries, while existing
routing methods, which rely on static parametric knowledge representations,
exhibit suboptimal performance in RAG scenarios. To address this, we formally
define the new retrieval-augmented LLM routing problem, incorporating the
influence of retrieved documents into the routing framework. We propose
RAGRouter, a RAG-aware routing design, which leverages document embeddings and
RAG capability embeddings with contrastive learning to capture knowledge
representation shifts and enable informed routing decisions. Extensive
experiments on diverse knowledge-intensive tasks and retrieval settings show
that RAGRouter outperforms the best individual LLM by 3.61% on average and
existing routing methods by 3.29%-9.33%. With an extended score-threshold-based
mechanism, it also achieves strong performance-efficiency trade-offs under
low-latency constraints.

</details>


### [48] [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/pdf/2505.23060)
*Jeonghun Cho, Deokhyung Kang, Hyounghun Kim, Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Smaller models struggle with self-correction in code generation. CoCoS, a reinforcement learning approach, improves their multi-turn correction ability, achieving significant gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Exploring whether smaller models can effectively self-correct code outputs, given their limited capacity compared to larger models.

Method: Introduces CoCoS, an online reinforcement learning approach with an accumulated reward function and fine-grained rewards for multi-turn correction.

Result: CoCoS improves performance by 35.8% on MBPP and 27.7% on HumanEval with 1B-scale models.

Conclusion: CoCoS enhances smaller models' self-correction capabilities, demonstrating its effectiveness in code generation tasks.

Abstract: Self-correction has demonstrated potential in code generation by allowing
language models to revise and improve their outputs through successive
refinement. Recent studies have explored prompting-based strategies that
incorporate verification or feedback loops using proprietary models, as well as
training-based methods that leverage their strong reasoning capabilities.
However, whether smaller models possess the capacity to effectively guide their
outputs through self-reflection remains unexplored. Our findings reveal that
smaller models struggle to exhibit reflective revision behavior across both
self-correction paradigms. In response, we introduce CoCoS, an approach
designed to enhance the ability of small language models for multi-turn code
correction. Specifically, we propose an online reinforcement learning objective
that trains the model to confidently maintain correct outputs while
progressively correcting incorrect outputs as turns proceed. Our approach
features an accumulated reward function that aggregates rewards across the
entire trajectory and a fine-grained reward better suited to multi-turn
correction scenarios. This facilitates the model in enhancing initial response
quality while achieving substantial improvements through self-correction. With
1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on
HumanEval compared to the baselines.

</details>


### [49] [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/pdf/2505.23065)
*Hongcheng Guo, Zheyong Xie, Shaosheng Cao, Boyang Wang, Weiting Liu, Anjie Le, Lei Li, Zhoujun Li*

Main category: cs.CL

TL;DR: SNS-Bench-VL is a new multimodal benchmark for evaluating Vision-Language LLMs in social media contexts, covering 8 tasks with 4,001 QA pairs. It reveals challenges in multimodal comprehension.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack coverage of multimodal contexts in SNS, necessitating a tool to assess LLMs' performance in real-world social media scenarios.

Method: SNS-Bench-VL includes 8 multimodal tasks (e.g., note comprehension, user engagement) with 4,001 QA pairs, evaluated on 25 state-of-the-art LLMs.

Result: The benchmark uncovers persistent challenges in multimodal social context comprehension among LLMs.

Conclusion: SNS-Bench-VL aims to drive research towards robust, context-aware multimodal intelligence for future SNS.

Abstract: With the increasing integration of visual and textual content in Social
Networking Services (SNS), evaluating the multimodal capabilities of Large
Language Models (LLMs) is crucial for enhancing user experience, content
understanding, and platform intelligence. Existing benchmarks primarily focus
on text-centric tasks, lacking coverage of the multimodal contexts prevalent in
modern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a
comprehensive multimodal benchmark designed to assess the performance of
Vision-Language LLMs in real-world social media scenarios. SNS-Bench-VL
incorporates images and text across 8 multimodal tasks, including note
comprehension, user engagement analysis, information retrieval, and
personalized recommendation. It comprises 4,001 carefully curated multimodal
question-answer pairs, covering single-choice, multiple-choice, and open-ended
tasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their
performance across tasks. Our findings highlight persistent challenges in
multimodal social context comprehension. We hope SNS-Bench-VL will inspire
future research towards robust, context-aware, and human-aligned multimodal
intelligence for next-generation social networking services.

</details>


### [50] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/pdf/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: MBR-OT, a variant of MBR decoding using Wasserstein distance, improves document-level text generation tasks by leveraging sentence-level utility functions.


<details>
  <summary>Details</summary>
Motivation: Document-level text generation is challenging due to longer context requirements, and existing MBR decoding methods are limited by sentence-focused utility functions.

Method: Proposes MBR-OT, which uses Wasserstein distance to compute document utility from sentence-level functions.

Result: MBR-OT outperforms standard MBR in document-level machine translation, text simplification, and dense image captioning.

Conclusion: MBR-OT effectively adapts MBR decoding for document-level tasks, demonstrating superior performance.

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [51] [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/pdf/2505.23108)
*Zexuan Li, Hongliang Dai, Piji Li*

Main category: cs.CL

TL;DR: The paper explores improving diversity in LLM-generated training data for Relation Extraction (RE) while ensuring correctness, using ICL prompts and DPO fine-tuning. Results show better data quality and performance when training non-LLM models with this data.


<details>
  <summary>Details</summary>
Motivation: LLM-generated RE training samples often lack diversity due to structural similarities, limiting their effectiveness for zero or few-shot tasks.

Method: 1. Use ICL prompts to encourage diverse samples. 2. Fine-tune LLMs with DPO for diversity.

Result: Both methods improve training data quality. Training non-LLM RE models with this data outperforms direct LLM-based RE.

Conclusion: Enhancing diversity in LLM-generated RE data via ICL and DPO improves downstream task performance, suggesting a viable alternative to direct LLM use.

Abstract: Using Large Language Models (LLMs) to generate training data can potentially
be a preferable way to improve zero or few-shot NLP tasks. However, many
problems remain to be investigated for this direction. For the task of Relation
Extraction (RE), we find that samples generated by directly prompting LLMs may
easily have high structural similarities with each other. They tend to use a
limited variety of phrasing while expressing the relation between a pair of
entities. Therefore, in this paper, we study how to effectively improve the
diversity of the training samples generated with LLMs for RE, while also
maintaining their correctness. We first try to make the LLMs produce dissimilar
samples by directly giving instructions in In-Context Learning (ICL) prompts.
Then, we propose an approach to fine-tune LLMs for diversity training sample
generation through Direct Preference Optimization (DPO). Our experiments on
commonly used RE datasets show that both attempts can improve the quality of
the generated training data. We also find that comparing with directly
performing RE with an LLM, training a non-LLM RE model with its generated
samples may lead to better performance.

</details>


### [52] [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/pdf/2505.23114)
*Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang*

Main category: cs.CL

TL;DR: Alignment Data Map, a GPT-4o-assisted tool, improves efficiency in collecting human preference data for LLM alignment by identifying high-quality samples without explicit annotations.


<details>
  <summary>Details</summary>
Motivation: Human preference data collection for LLM alignment is costly and inefficient, necessitating a scalable solution.

Method: Uses GPT-4o to compute alignment scores for LLM responses, constructing an Alignment Data Map based on mean and variance of scores.

Result: Using 33% of high-quality data achieves comparable or better performance than full datasets.

Conclusion: The Alignment Data Map enhances data efficiency and diagnoses dataset quality, reducing reliance on explicit annotations.

Abstract: Human preference data plays a critical role in aligning large language models
(LLMs) with human values. However, collecting such data is often expensive and
inefficient, posing a significant scalability challenge. To address this, we
introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and
diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we
compute alignment scores for LLM-generated responses to instructions from
existing preference datasets. These scores are then used to construct an
Alignment Data Map based on their mean and variance. Our experiments show that
using only 33 percent of the data, specifically samples in the high-mean,
low-variance region, achieves performance comparable to or better than using
the entire dataset. This finding suggests that the Alignment Data Map can
significantly improve data collection efficiency by identifying high-quality
samples for LLM alignment without requiring explicit annotations. Moreover, the
Alignment Data Map can diagnose existing preference datasets. Our analysis
shows that it effectively detects low-impact or potentially misannotated
samples. Source code is available online.

</details>


### [53] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/pdf/2505.23118)
*Linjie Mu, Zhongzhen Huang, Yakun Zhu, Xiangyu Zhao, Shaoting Zhang, Xiaofan Zhang*

Main category: cs.CL

TL;DR: The paper introduces MedE², a two-stage post-training pipeline to enhance multimodal reasoning in medical domains, outperforming baselines in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making relies on multimodal reasoning, yet existing models are underexplored in medical applications.

Method: Stage-I fine-tunes models with text-only reasoning demonstrations; Stage-II enhances reasoning using curated multimodal medical cases.

Result: MedE² improves reasoning performance, consistently outperforming baselines in medical benchmarks.

Conclusion: The approach is robust, scalable, and practical for enhancing medical multimodal reasoning.

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [54] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/pdf/2505.23121)
*Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CL

TL;DR: The paper introduces ContextQFormer, a module to improve multi-turn interaction in multi-modal models, and a new dataset (TMDialog) for research. ContextQFormer outperforms baselines by 2%-4%.


<details>
  <summary>Details</summary>
Motivation: Existing open-source multi-modal models lack strong multi-turn interaction capabilities, especially for long contexts.

Method: Introduces ContextQFormer, a context modeling module with a memory block, and builds TMDialog, a multi-turn multi-modal dialogue dataset.

Result: ContextQFormer improves performance by 2%-4% over baselines on TMDialog.

Conclusion: The proposed ContextQFormer and TMDialog dataset address limitations in multi-turn multi-modal dialogue research.

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [55] [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/pdf/2505.23126)
*Atharva Naik, Darsh Agrawal, Manav Kapadnis, Yuwei An, Yash Mathur, Carolyn Rose, David Mortensen*

Main category: cs.CL

TL;DR: The paper evaluates the reasoning abilities of LCoT LLMs on historical linguistics tasks, showing they struggle despite strong performance in other domains.


<details>
  <summary>Details</summary>
Motivation: To assess if LCoT LLMs' reasoning is general enough for practical problems like historical linguistics, beyond math and coding.

Method: Developed an automated pipeline to generate a scalable, contamination-free benchmark with controllable difficulty for Programming by Examples.

Result: Generated a 1k-instance test set; best model (Claude-3.7-Sonnet) achieved only 54% pass rate, indicating poor performance.

Conclusion: LCoT LLMs still struggle with reasoning tasks common in historical linguistics and other domains.

Abstract: Recently, long chain of thought (LCoT), Large Language Models (LLMs), have
taken the machine learning world by storm with their breathtaking reasoning
capabilities. However, are the abstract reasoning abilities of these models
general enough for problems of practical importance? Unlike past work, which
has focused mainly on math, coding, and data wrangling, we focus on a
historical linguistics-inspired inductive reasoning problem, formulated as
Programming by Examples. We develop a fully automated pipeline for dynamically
generating a benchmark for this task with controllable difficulty in order to
tackle scalability and contamination issues to which many reasoning benchmarks
are subject. Using our pipeline, we generate a test set with nearly 1k
instances that is challenging for all state-of-the-art reasoning LLMs, with the
best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating
that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in
historical linguistics as well as many other domains.

</details>


### [56] [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/pdf/2505.23140)
*Qiuyu Ding, Zhiqiang Cao, Hailong Cao, Tiejun Zhao*

Main category: cs.CL

TL;DR: A method to improve large language models' machine translation by addressing context-sensitive units (CSUs) like polysemous words, enhancing accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Persistent challenges in translating CSUs affect LLMs' performance, leading to mistranslations and misunderstandings.

Method: Dynamically analyze and identify CSUs, apply semantic focus, and incorporate them into LLMs to mitigate errors.

Result: Achieves competitive performance on MT benchmarks across diverse language pairs without extra training.

Conclusion: The method is effective, resource-efficient, and enhances LLMs' translation and broader NLP task performance.

Abstract: Large language models have demonstrated exceptional performance across
multiple crosslingual NLP tasks, including machine translation (MT). However,
persistent challenges remain in addressing context-sensitive units (CSUs), such
as polysemous words. These CSUs not only affect the local translation accuracy
of LLMs, but also affect LLMs' understanding capability for sentences and
tasks, and even lead to translation failure. To address this problem, we
propose a simple but effective method to enhance LLMs' MT capabilities by
acquiring CSUs and applying semantic focus. Specifically, we dynamically
analyze and identify translation challenges, then incorporate them into LLMs in
a structured manner to mitigate mistranslations or misunderstandings of CSUs
caused by information flattening. Efficiently activate LLMs to identify and
apply relevant knowledge from its vast data pool in this way, ensuring more
accurate translations for translating difficult terms. On a benchmark dataset
of MT, our proposed method achieved competitive performance compared to
multiple existing open-sourced MT baseline models. It demonstrates
effectiveness and robustness across multiple language pairs, including both
similar language pairs and distant language pairs. Notably, the proposed method
requires no additional model training and enhances LLMs' performance across
multiple NLP tasks with minimal resource consumption.

</details>


### [57] [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/pdf/2505.23146)
*Qiuyu Ding, Zhiqiang Cao, Hailong Cao, Tiejun Zhao*

Main category: cs.CL

TL;DR: The paper introduces a new task in Bilingual Lexicon Induction (BLI) for domain-specific bilingual dictionaries, leveraging pre-trained models and code-switching to improve performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional BLI methods in specialized domains, where smaller datasets and context-dependent word meanings reduce translation quality.

Method: The method involves using monolingual corpora from general and target domains, incorporating pre-trained models and code-switching techniques to enhance word embeddings.

Result: The method improves performance over robust BLI baselines by an average of 0.78 points across three specific domains.

Conclusion: The approach is effective for domain-specific BLI, addressing challenges like data scarcity and contextual word meanings.

Abstract: Bilingual Lexicon Induction (BLI) is generally based on common domain data to
obtain monolingual word embedding, and by aligning the monolingual word
embeddings to obtain the cross-lingual embeddings which are used to get the
word translation pairs. In this paper, we propose a new task of BLI, which is
to use the monolingual corpus of the general domain and target domain to
extract domain-specific bilingual dictionaries. Motivated by the ability of
Pre-trained models, we propose a method to get better word embeddings that
build on the recent work on BLI. This way, we introduce the Code Switch(Qin et
al., 2020) firstly in the cross-domain BLI task, which can match differit is
yet to be seen whether these methods are suitable for bilingual lexicon
extraction in professional fields. As we can see in table 1, the classic and
efficient BLI approach, Muse and Vecmap, perform much worse on the Medical
dataset than on the Wiki dataset. On one hand, the specialized domain data set
is relatively smaller compared to the generic domain data set generally, and
specialized words have a lower frequency, which will directly affect the
translation quality of bilingual dictionaries. On the other hand, static word
embeddings are widely used for BLI, however, in some specific fields, the
meaning of words is greatly influenced by context, in this case, using only
static word embeddings may lead to greater bias. ent strategies in different
contexts, making the model more suitable for this task. Experimental results
show that our method can improve performances over robust BLI baselines on
three specific domains by averagely improving 0.78 points.

</details>


### [58] [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/pdf/2505.23166)
*Li Lucy, Camilla Griffiths, Sarah Levine, Jennifer L. Eberhardt, Dorottya Demszky, David Bamman*

Main category: cs.CL

TL;DR: Retell improves topic modeling for literature by using generative LMs to translate narrative details into higher-level concepts, enhancing LDA's precision.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like LDA struggle with literary texts due to their focus on sensory details over abstract themes.

Method: Prompt generative LMs to retell passages, then apply LDA to these retellings for better topic extraction.

Result: Retell produces more precise and informative topics than LDA alone or direct LM topic listing.

Conclusion: Retell shows promise for cultural analytics, as demonstrated in a case study on racial/cultural identity in literature.

Abstract: Conventional bag-of-words approaches for topic modeling, like latent
Dirichlet allocation (LDA), struggle with literary text. Literature challenges
lexical methods because narrative language focuses on immersive sensory details
instead of abstractive description or exposition: writers are advised to "show,
don't tell." We propose Retell, a simple, accessible topic modeling approach
for literature. Here, we prompt resource-efficient, generative language models
(LMs) to tell what passages show, thereby translating narratives' surface forms
into higher-level concepts and themes. By running LDA on LMs' retellings of
passages, we can obtain more precise and informative topics than by running LDA
alone or by directly asking LMs to list topics. To investigate the potential of
our method for cultural analytics, we compare our method's outputs to
expert-guided annotations in a case study on racial/cultural identity in high
school English language arts books.

</details>


### [59] [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/pdf/2505.23174)
*Naman Ahuja, Fenil Bardoliya, Chitta Baral, Vivek Gupta*

Main category: cs.CL

TL;DR: The paper introduces Map&Make, a method for Text-to-Table generation that decomposes text into atomic statements to extract latent schemas, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-Table methods fail to handle complex information extraction and lack inference capabilities, necessitating a more versatile approach.

Method: Map&Make dissects text into propositional atomic statements to extract latent schemas, which populate tables capturing qualitative and quantitative details.

Result: The method shows significant improvements on Rotowire and Livesum datasets, with better interpretability and reduced hallucination errors.

Conclusion: Map&Make validates its practicality and superior performance in structured summarization tasks through rigorous evaluation and ablation studies.

Abstract: Transforming dense, detailed, unstructured text into an interpretable and
summarised table, also colloquially known as Text-to-Table generation, is an
essential task for information retrieval. Current methods, however, miss out on
how and what complex information to extract; they also lack the ability to
infer data from the text. In this paper, we introduce a versatile approach,
Map&Make, which "dissects" text into propositional atomic statements. This
facilitates granular decomposition to extract the latent schema. The schema is
then used to populate the tables that capture the qualitative nuances and the
quantitative facts in the original text. Our approach is tested against two
challenging datasets, Rotowire, renowned for its complex and multi-table
schema, and Livesum, which demands numerical aggregation. By carefully
identifying and correcting hallucination errors in Rotowire, we aim to achieve
a cleaner and more reliable benchmark. We evaluate our method rigorously on a
comprehensive suite of comparative and referenceless metrics. Our findings
demonstrate significant improvement results across both datasets with better
interpretability in Text-to-Table generation. Moreover, through detailed
ablation studies and analyses, we investigate the factors contributing to
superior performance and validate the practicality of our framework in
structured summarization tasks.

</details>


### [60] [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/pdf/2505.23177)
*Wenjing Xing, Wenke Lu, Yeheng Duan, Bing Zhao, Zhenghui kang, Yaolong Wang, Kai Gao, Lei Qiao*

Main category: cs.CL

TL;DR: Infinite-Instruct automates high-quality question-answer pair synthesis for LLM code generation, improving diversity, logic, and quality via reverse and backfeeding construction, plus static analysis.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional code instruction synthesis (low diversity, poor logic) to enhance LLM code generation.

Method: Uses 'Reverse Construction' to turn code into problems, 'Backfeeding Construction' to structure keywords into a knowledge graph, and static analysis for filtering.

Result: Fine-tuned models show 21.70% (7B) and 36.95% (32B) performance gains, matching Qwen-2.5-Coder-Instruct with less data.

Conclusion: Infinite-Instruct offers a scalable solution for LLM programming training, with open-sourced datasets.

Abstract: Traditional code instruction data synthesis methods suffer from limited
diversity and poor logic. We introduce Infinite-Instruct, an automated
framework for synthesizing high-quality question-answer pairs, designed to
enhance the code generation capabilities of large language models (LLMs). The
framework focuses on improving the internal logic of synthesized problems and
the quality of synthesized code. First, "Reverse Construction" transforms code
snippets into diverse programming problems. Then, through "Backfeeding
Construction," keywords in programming problems are structured into a knowledge
graph to reconstruct them into programming problems with stronger internal
logic. Finally, a cross-lingual static code analysis pipeline filters invalid
samples to ensure data quality. Experiments show that on mainstream code
generation benchmarks, our fine-tuned models achieve an average performance
improvement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter
models. Using less than one-tenth of the instruction fine-tuning data, we
achieved performance comparable to the Qwen-2.5-Coder-Instruct.
Infinite-Instruct provides a scalable solution for LLM training in programming.
We open-source the datasets used in the experiments, including both unfiltered
versions and filtered versions via static analysis. The data are available at
https://github.com/xingwenjing417/Infinite-Instruct-dataset

</details>


### [61] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/pdf/2505.23183)
*Gabriele Sarti, Vilém Zouhar, Malvina Nissim, Arianna Bisazza*

Main category: cs.CL

TL;DR: The paper explores efficient alternatives to costly word-level quality estimation (WQE) methods by leveraging language model interpretability and uncertainty quantification, evaluating their performance across diverse metrics and translation directions.


<details>
  <summary>Details</summary>
Motivation: Modern WQE techniques are expensive and rely on large language models or extensive labeled data. The study aims to find efficient alternatives using model interpretability and uncertainty.

Method: The approach uses language model interpretability and uncertainty quantification to identify translation errors from translation models' inner workings. Evaluation involves 14 metrics across 12 translation directions, using multiple human label sets.

Result: Results show the potential of unsupervised metrics, the limitations of supervised methods under label uncertainty, and the fragility of single-annotator evaluations.

Conclusion: The study highlights the promise of unsupervised WQE methods and the need to address label uncertainty in supervised approaches.

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [62] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/pdf/2505.23191)
*Jinglong Gao, Xiao Ding, Lingxiao Zou, Bibo Cai, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: The paper introduces an autonomous experience transfer framework for LLMs to improve performance by transferring knowledge from source to target tasks, reducing human labor and time costs.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of gathering task-solving experiences manually for diverse LLM tasks, the paper explores autonomous experience transfer.

Method: An autonomous experience transfer framework is designed to mimic human cognitive intelligence for transferring knowledge between tasks.

Result: Experiments on 13 datasets show the framework effectively enhances LLM performance.

Conclusion: The framework offers a cost-effective and novel approach for LLM generalization, with detailed module analysis provided.

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [63] [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/pdf/2505.23224)
*Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R., Fung*

Main category: cs.CL

TL;DR: MMBoundary improves multimodal reasoning in MLLMs by calibrating confidence at each reasoning step, reducing errors and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of multimodal reasoning in MLLMs, particularly the lack of step-wise confidence assessment leading to hallucination.

Method: Proposes MMBoundary, a framework using textual and cross-modal self-rewarding signals for step-wise confidence calibration, combined with supervised fine-tuning and reinforcement learning.

Result: Achieves 7.5% reduction in calibration errors and up to 8.3% improvement in task performance.

Conclusion: MMBoundary effectively enhances MLLM reasoning by improving confidence awareness and self-correction.

Abstract: In recent years, multimodal large language models (MLLMs) have made
significant progress but continue to face inherent challenges in multimodal
reasoning, which requires multi-level (e.g., perception, reasoning) and
multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior
work on estimating model confidence tends to focus on the overall response for
training and calibration, but fails to assess confidence in each reasoning
step, leading to undesirable hallucination snowballing. In this work, we
present MMBoundary, a novel framework that advances the knowledge boundary
awareness of MLLMs through reasoning step confidence calibration. To achieve
this, we propose to incorporate complementary textual and cross-modal
self-rewarding signals to estimate confidence at each step of the MLLM
reasoning process. In addition to supervised fine-tuning MLLM on this set of
self-rewarded confidence estimation signal for initial confidence expression
warm-up, we introduce a reinforcement learning stage with multiple reward
functions for further aligning model knowledge and calibrating confidence at
each reasoning step, enhancing reasoning chain self-correction. Empirical
results show that MMBoundary significantly outperforms existing methods across
diverse domain datasets and metrics, achieving an average of 7.5% reduction in
multimodal confidence calibration errors and up to 8.3% improvement in task
performance.

</details>


### [64] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/pdf/2505.23229)
*Hao Lu, Yanchi Gu, Haoyuan Huang, Yulin Zhou, Ningxin Zhu, Chen Li*

Main category: cs.CL

TL;DR: MCTSr-Zero, an MCTS framework for open-ended dialogues, improves alignment with human-centric principles like empathy in psychological counseling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MCTS methods struggle in open-ended dialogues (e.g., counseling) due to ill-defined correctness criteria and misalignment with subjective factors like empathy.

Method: MCTSr-Zero introduces 'domain alignment' to focus on conversational trajectories, plus 'Regeneration' and 'Meta-Prompt Adaptation' for broader exploration.

Result: MCTSr-Zero generates high-quality dialogue data, fine-tuning PsyLLM, which achieves state-of-the-art performance on the PsyEval benchmark.

Conclusion: MCTSr-Zero effectively addresses the challenge of aligning LLMs with complex psychological standards in human-centric dialogues.

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [65] [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/pdf/2505.23242)
*Jingxuan Wei, Nan Xu, Junnan Zhu, Yanni Hao, Gaowei Wu, Bihui Yu, Lei Wang*

Main category: cs.CL

TL;DR: ChartMind is a new benchmark for complex chart question answering (CQA), addressing real-world demands. ChartLLM, a context-aware framework, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CQA evaluations lack flexibility for real-world chart analysis, prompting the need for a more robust benchmark and framework.

Method: Introduces ChartMind (7 task categories, multilingual, open-domain) and ChartLLM (context-aware, noise-reducing framework).

Result: ChartLLM outperforms three common CQA paradigms on ChartMind and public benchmarks.

Conclusion: Flexible chart understanding is crucial for real-world CQA, guiding future research toward robust chart reasoning.

Abstract: Chart question answering (CQA) has become a critical multimodal task for
evaluating the reasoning capabilities of vision-language models. While early
approaches have shown promising performance by focusing on visual features or
leveraging large-scale pre-training, most existing evaluations rely on rigid
output formats and objective metrics, thus ignoring the complex, real-world
demands of practical chart analysis. In this paper, we introduce ChartMind, a
new benchmark designed for complex CQA tasks in real-world settings. ChartMind
covers seven task categories, incorporates multilingual contexts, supports
open-domain textual outputs, and accommodates diverse chart formats, bridging
the gap between real-world applications and traditional academic benchmarks.
Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,
that focuses on extracting key contextual elements, reducing noise, and
enhancing the reasoning accuracy of multimodal large language models. Extensive
evaluations on ChartMind and three representative public benchmarks with 14
mainstream multimodal models show our framework significantly outperforms the
previous three common CQA paradigms: instruction-following, OCR-enhanced, and
chain-of-thought, highlighting the importance of flexible chart understanding
for real-world CQA. These findings suggest new directions for developing more
robust chart reasoning in future research.

</details>


### [66] [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/pdf/2505.23252)
*Bing Ma, Hai Zhuge*

Main category: cs.CL

TL;DR: The paper proposes a multi-dimensional framework for efficiently querying and managing scientific research approaches by identifying patterns, categorizing approaches, and using tree structures for similarity and clustering.


<details>
  <summary>Details</summary>
Motivation: The challenge of querying and managing approaches in scientific papers is time-consuming and inefficient without a structured framework.

Method: Identifies approach patterns across linguistic levels, extracts approaches, categorizes them into five dimensions, and uses tree structures for similarity and clustering.

Result: A multi-dimensional approach space is created, enabling efficient and relevant querying with reduced search space.

Conclusion: The proposed framework improves approach querying and management in scientific research by leveraging multi-dimensional categorization and similarity measures.

Abstract: Approaches form the foundation for conducting scientific research. Querying
approaches from a vast body of scientific papers is extremely time-consuming,
and without a well-organized management framework, researchers may face
significant challenges in querying and utilizing relevant approaches.
Constructing multiple dimensions on approaches and managing them from these
dimensions can provide an efficient solution. Firstly, this paper identifies
approach patterns using a top-down way, refining the patterns through four
distinct linguistic levels: semantic level, discourse level, syntactic level,
and lexical level. Approaches in scientific papers are extracted based on
approach patterns. Additionally, five dimensions for categorizing approaches
are identified using these patterns. This paper proposes using tree structure
to represent step and measuring the similarity between different steps with a
tree-structure-based similarity measure that focuses on syntactic-level
similarities. A collection similarity measure is proposed to compute the
similarity between approaches. A bottom-up clustering algorithm is proposed to
construct class trees for approach components within each dimension by merging
each approach component or class with its most similar approach component or
class in each iteration. The class labels generated during the clustering
process indicate the common semantics of the step components within the
approach components in each class and are used to manage the approaches within
the class. The class trees of the five dimensions collectively form a
multi-dimensional approach space. The application of approach queries on the
multi-dimensional approach space demonstrates that querying within this space
ensures strong relevance between user queries and results and rapidly reduces
search space through a class-based query mechanism.

</details>


### [67] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/pdf/2505.23276)
*Maged S. Al-Shaibani, Moataz Ahmed*

Main category: cs.CL

TL;DR: The paper investigates machine-generated Arabic text, revealing detectable linguistic patterns and developing high-performance BERT-based detection models to combat misinformation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLM-generated misinformation in low-resource languages like Arabic, particularly in critical domains like education and healthcare.

Method: Examined Arabic text generation strategies (title-only, content-aware, refinement) across models (ALLaM, Jais, Llama, GPT-4) using stylometric analysis and developed BERT-based detectors.

Result: Detected distinctive linguistic signatures in machine-generated Arabic text, achieving up to 99.9% F1-score in formal contexts, though cross-domain generalization remains challenging.

Conclusion: The study provides a foundation for linguistically-informed detection systems to preserve information integrity in Arabic-language contexts.

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [68] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/pdf/2505.23277)
*Yong Zhang, Yanwen Huang, Ning Cheng, Yang Guo, Yun Zhu, Yanmeng Wang, Shaojun Wang, Jing Xiao*

Main category: cs.CL

TL;DR: Sentinel is a lightweight framework for compressing retrieved passages in RAG systems by probing decoder attention from a small proxy LLM, achieving efficient and question-aware compression without supervised training.


<details>
  <summary>Details</summary>
Motivation: Retrieved passages in RAG systems are often lengthy or noisy, and existing compression methods require costly supervised training, reducing portability.

Method: Sentinel reframes context filtering as an attention-based task, using a lightweight classifier to probe decoder attention from a 0.5B proxy LLM for sentence relevance.

Result: On LongBench, Sentinel achieves up to 5× compression while matching QA performance of larger 7B-scale systems.

Conclusion: Probing native attention signals enables fast, effective, and question-aware context compression without dedicated training.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [69] [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/pdf/2505.23291)
*Xinye Li, Zunwen Zheng, Qian Zhang, Dekai Zhuang, Jiabao Kang, Liyan Xu, Qingbin Liu, Xi Chen, Zhiying Tu, Dianhui Chu, Dianbo Sui*

Main category: cs.CL

TL;DR: The paper introduces ScEdit, a script-based benchmark for Knowledge Editing (KE), extending evaluation to action-based questions and revealing performance drops in existing KE methods.


<details>
  <summary>Details</summary>
Motivation: Current KE tasks are too simple, with high scores in evaluations, but lack integration into real-world applications like LLM-as-agent scenarios.

Method: A novel script-based benchmark (ScEdit) is introduced, covering counterfactual and temporal edits, and combining token-level and text-level evaluations.

Result: Existing KE methods show performance drops on established metrics and struggle with text-level evaluations, highlighting the challenge.

Conclusion: ScEdit provides a comprehensive benchmark for KE, revealing limitations in current methods and paving the way for more realistic evaluations.

Abstract: Knowledge Editing (KE) has gained increasing attention, yet current KE tasks
remain relatively simple. Under current evaluation frameworks, many editing
methods achieve exceptionally high scores, sometimes nearing perfection.
However, few studies integrate KE into real-world application scenarios (e.g.,
recent interest in LLM-as-agent). To support our analysis, we introduce a novel
script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --
which encompasses both counterfactual and temporal edits. We integrate
token-level and text-level evaluation methods, comprehensively analyzing
existing KE techniques. The benchmark extends traditional fact-based
("What"-type question) evaluation to action-based ("How"-type question)
evaluation. We observe that all KE methods exhibit a drop in performance on
established metrics and face challenges on text-level metrics, indicating a
challenging task. Our benchmark is available at
https://github.com/asdfo123/ScEdit.

</details>


### [70] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/pdf/2505.23295)
*James Xu Zhao, Jimmy Z. J. Liu, Bryan Hooi, See-Kiong Ng*

Main category: cs.CL

TL;DR: Longer responses from LLMs tend to have lower factual precision due to facts exhaustion, not error propagation or long context.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored relationship between response length and factuality in LLMs, addressing reliability concerns.

Method: Introduces an automatic, bi-level long-form factuality evaluation framework and conducts controlled experiments to test three hypotheses.

Result: Longer responses show lower factual precision, with facts exhaustion identified as the primary cause.

Conclusion: Facts exhaustion, not error propagation or long context, drives factual degradation in longer LLM responses.

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [71] [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/pdf/2505.23297)
*Daryna Dementieva, Nikolay Babakov, Alexander Fraser*

Main category: cs.CL

TL;DR: EmoBench-UA is the first annotated dataset for emotion detection in Ukrainian, addressing a gap in Ukrainian NLP. It uses crowdsourcing for annotation and evaluates various methods, revealing challenges in non-mainstream languages.


<details>
  <summary>Details</summary>
Motivation: Emotion classification in Ukrainian is underexplored, lacking public benchmarks. This work fills the gap by introducing EmoBench-UA.

Method: The dataset is annotated via crowdsourcing (Toloka.ai), adapting English-centric guidelines. Evaluated methods include linguistic baselines, synthetic data, and LLMs.

Result: Challenges in emotion classification for Ukrainian are highlighted, showing the need for Ukrainian-specific models and resources.

Conclusion: EmoBench-UA is a foundational step for Ukrainian emotion detection, calling for further development in this area.

Abstract: While Ukrainian NLP has seen progress in many texts processing tasks, emotion
classification remains an underexplored area with no publicly available
benchmark to date. In this work, we introduce EmoBench-UA, the first annotated
dataset for emotion detection in Ukrainian texts. Our annotation schema is
adapted from the previous English-centric works on emotion detection (Mohammad
et al., 2018; Mohammad, 2022) guidelines. The dataset was created through
crowdsourcing using the Toloka.ai platform ensuring high-quality of the
annotation process. Then, we evaluate a range of approaches on the collected
dataset, starting from linguistic-based baselines, synthetic data translated
from English, to large language models (LLMs). Our findings highlight the
challenges of emotion classification in non-mainstream languages like Ukrainian
and emphasize the need for further development of Ukrainian-specific models and
training resources.

</details>


### [72] [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/pdf/2505.23299)
*Julia Belikova, Konstantin Polev, Rauf Parchiev, Dmitry Simakov*

Main category: cs.CL

TL;DR: The paper proposes a lightweight method to reduce training data requirements for hallucination detection in LLMs and RAG systems, achieving competitive performance with only 250 samples.


<details>
  <summary>Details</summary>
Motivation: The reliability of LLMs and RAG systems is limited by hallucination detection challenges, and existing SOTA methods rely on extensive annotated data, hindering scalability.

Method: Combines efficient classification algorithms with dimensionality reduction to minimize sample size needs while maintaining performance for hallucination detection frameworks like Lookback Lens and probing-based approaches.

Result: The approach achieves performance comparable to proprietary LLM baselines with just 250 training samples on standardized benchmarks.

Conclusion: Lightweight, data-efficient methods are viable for industrial deployment, especially in annotation-constrained scenarios.

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems
are increasingly deployed in industry applications, yet their reliability
remains hampered by challenges in detecting hallucinations. While supervised
state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as
activation tracing and representation analysis -- show promise, their
dependence on extensively annotated datasets limits scalability in real-world
applications. This paper addresses the critical bottleneck of data annotation
by investigating the feasibility of reducing training data requirements for two
SOTA hallucination detection frameworks: Lookback Lens, which analyzes
attention head dynamics, and probing-based approaches, which decode internal
model representations. We propose a methodology combining efficient
classification algorithms with dimensionality reduction techniques to minimize
sample size demands while maintaining competitive performance. Evaluations on
standardized question-answering RAG benchmarks show that our approach achieves
performance comparable to strong proprietary LLM-based baselines with only 250
training samples. These results highlight the potential of lightweight,
data-efficient paradigms for industrial deployment, particularly in
annotation-constrained scenarios.

</details>


### [73] [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/pdf/2505.23304)
*Yi Luo, Qiwen Wang, Junqi Yang, Luyao Tang, Zhenghao Lin, Zhenzhe Ying, Weiqiang Wang, Chen Lin*

Main category: cs.CL

TL;DR: PaMA improves GCD by addressing clustering-classification divergence and minority class unfairness using LLMs and a ranking-filtering-mining pipeline, achieving up to 12.58% H-score gains.


<details>
  <summary>Details</summary>
Motivation: Current GCD methods lack validation in realistic settings with long narratives and imbalanced classes, leading to unfair alignment and divergent groupings.

Method: Proposes PaMA, leveraging LLMs to refine event patterns and a pipeline for balanced prototype representation.

Result: Outperforms prior methods with up to 12.58% H-score gains on EC-GCD benchmarks, including a new Scam Report dataset.

Conclusion: PaMA effectively addresses GCD challenges in realistic settings while maintaining strong generalization.

Abstract: Generalized Category Discovery (GCD) aims to classify both known and novel
categories using partially labeled data that contains only known classes.
Despite achieving strong performance on existing benchmarks, current textual
GCD methods lack sufficient validation in realistic settings. We introduce
Event-Centric GCD (EC-GCD), characterized by long, complex narratives and
highly imbalanced class distributions, posing two main challenges: (1)
divergent clustering versus classification groupings caused by subjective
criteria, and (2) Unfair alignment for minority classes. To tackle these, we
propose PaMA, a framework leveraging LLMs to extract and refine event patterns
for improved cluster-class alignment. Additionally, a ranking-filtering-mining
pipeline ensures balanced representation of prototypes across imbalanced
categories. Evaluations on two EC-GCD benchmarks, including a newly constructed
Scam Report dataset, demonstrate that PaMA outperforms prior methods with up to
12.58% H-score gains, while maintaining strong generalization on base GCD
datasets.

</details>


### [74] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/pdf/2505.23315)
*Abhirup Chakravarty, Mark Brenchley, Trevor Breakspear, Ian Lewin, Yan Huang*

Main category: cs.CL

TL;DR: The paper proposes a confidence estimation method for AES by framing it as a classification task, achieving high reliability in score releases.


<details>
  <summary>Details</summary>
Motivation: To ensure AES scores meet high reliability standards by predicting whether scores correctly align with CEFR levels.

Method: Frames confidence estimation as an n-ary classification task using score binning and introduces KWOCCE loss functions to incorporate CEFR's ordinal structure.

Result: Best model achieves F1 score of 0.97, enabling release of 47% scores with 100% CEFR agreement and 99% with at least 95% agreement.

Conclusion: The method significantly improves reliability of AES score releases compared to standalone models.

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [75] [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/pdf/2505.23316)
*Kaiyang Guo, Yinchuan Li, Zhitang Chen*

Main category: cs.CL

TL;DR: The paper identifies a limitation in direct alignment methods for LLMs, called likelihood underdetermination, and introduces PRO, a method to address it by reformulating the DPO loss and incorporating a complete regularizer.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of likelihood underdetermination in direct alignment methods, which causes aligned models to deviate from expected patterns and exhibit reward-hacking effects.

Method: The authors revisit DPO, reformulate its loss to decompose it, and introduce PRO, which includes a complete regularizer to resolve underdetermination.

Result: PRO outperforms existing methods in scenarios with pairwise, binary, and scalar feedback, effectively eliminating likelihood underdetermination.

Conclusion: The paper demonstrates that addressing the oversimplified regularizer in DPO through PRO resolves underdetermination and improves alignment with diverse feedback types.

Abstract: Direct alignment methods typically optimize large language models (LLMs) by
contrasting the likelihoods of preferred versus dispreferred responses. While
effective in steering LLMs to match relative preference, these methods are
frequently noted for decreasing the absolute likelihoods of example responses.
As a result, aligned models tend to generate outputs that deviate from the
expected patterns, exhibiting reward-hacking effect even without a reward
model. This undesired consequence exposes a fundamental limitation in
contrastive alignment, which we characterize as likelihood underdetermination.
In this work, we revisit direct preference optimization (DPO) -- the seminal
direct alignment method -- and demonstrate that its loss theoretically admits a
decomposed reformulation. The reformulated loss not only broadens applicability
to a wider range of feedback types, but also provides novel insights into the
underlying cause of likelihood underdetermination. Specifically, the standard
DPO implementation implicitly oversimplifies a regularizer in the reformulated
loss, and reinstating its complete version effectively resolves the
underdetermination issue. Leveraging these findings, we introduce PRoximalized
PReference Optimization (PRO), a unified method to align with diverse feeback
types, eliminating likelihood underdetermination through an efficient
approximation of the complete regularizer. Comprehensive experiments show the
superiority of PRO over existing methods in scenarios involving pairwise,
binary and scalar feedback.

</details>


### [76] [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/pdf/2505.23323)
*Harish Tayyar Madabushi, Melissa Torgbi, Claire Bonial*

Main category: cs.CL

TL;DR: The paper argues for a balanced view of LLM capabilities, rejecting extreme views of them as either 'stochastic parrots' or possessing unpredictable advanced reasoning. It introduces 'context-directed extrapolation' as a predictable, controllable mechanism.


<details>
  <summary>Details</summary>
Motivation: To address polarized views on LLM capabilities and propose a realistic middle-ground perspective.

Method: Analyzes existing literature to support the concept of 'context-directed extrapolation' as the mechanism behind LLM reasoning.

Result: LLMs' reasoning is predictable and controllable, not indicative of human-like cognition, and not infinitely scalable.

Conclusion: Future research should focus on context-directed extrapolation and alternative techniques, avoiding reliance on assumed advanced reasoning in LLMs.

Abstract: In this position paper we raise critical awareness of a realistic view of LLM
capabilities that eschews extreme alternative views that LLMs are either
"stochastic parrots" or in possession of "emergent" advanced reasoning
capabilities, which, due to their unpredictable emergence, constitute an
existential threat. Our middle-ground view is that LLMs extrapolate from priors
from their training data, and that a mechanism akin to in-context learning
enables the targeting of the appropriate information from which to extrapolate.
We call this "context-directed extrapolation." Under this view, substantiated
though existing literature, while reasoning capabilities go well beyond
stochastic parroting, such capabilities are predictable, controllable, not
indicative of advanced reasoning akin to high-level cognitive capabilities in
humans, and not infinitely scalable with additional training. As a result,
fears of uncontrollable emergence of agency are allayed, while research
advances are appropriately refocused on the processes of context-directed
extrapolation and how this interacts with training data to produce valuable
capabilities in LLMs. Future work can therefore explore alternative augmenting
techniques that do not rely on inherent advanced reasoning in LLMs.

</details>


### [77] [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/pdf/2505.23363)
*Hongzhan Chen, Tao Yang, Shiping Gao, Ruijun Chen, Xiaojun Quan, Hongtao Tian, Ting Yao*

Main category: cs.CL

TL;DR: Q-RM, a token-level reward model, outperforms baselines in enhancing LLMs for complex reasoning by decoupling reward modeling from language generation.


<details>
  <summary>Details</summary>
Motivation: Address instability and inaccuracies in token-level reward models caused by conflicts between generative language modeling and reward modeling.

Method: Decouples reward modeling from language generation, deriving a token-level reward model (Q-RM) via discriminative policy optimization.

Result: Q-RM improves Pass@1 scores by 5.85/4.70 points over ORM and 4.56/5.73 over PRM, with faster convergence (12x ORM, 11x PRM).

Conclusion: Q-RM is a robust, efficient approach for enhancing LLMs in complex reasoning tasks.

Abstract: Process reward models (PRMs) provide more nuanced supervision compared to
outcome reward models (ORMs) for optimizing policy models, positioning them as
a promising approach to enhancing the capabilities of LLMs in complex reasoning
tasks. Recent efforts have advanced PRMs from step-level to token-level
granularity by integrating reward modeling into the training of generative
models, with reward scores derived from token generation probabilities.
However, the conflict between generative language modeling and reward modeling
may introduce instability and lead to inaccurate credit assignments. To address
this challenge, we revisit token-level reward assignment by decoupling reward
modeling from language generation and derive a token-level reward model through
the optimization of a discriminative policy, termed the Q-function Reward Model
(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level
Q-functions from preference data without relying on fine-grained annotations.
In our experiments, Q-RM consistently outperforms all baseline methods across
various benchmarks. For example, when integrated into PPO/REINFORCE algorithms,
Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical
reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared
to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM
significantly enhances training efficiency, achieving convergence 12 times
faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code
and data are available at https://github.com/homzer/Q-RM.

</details>


### [78] [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/pdf/2505.23368)
*Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces a novel LLM-based pipeline using linguistically-grounded discourse segmenters to extract supporting/opposing statements from CoTs, improving accuracy in human label variation tasks.


<details>
  <summary>Details</summary>
Motivation: To address human label variation by leveraging reasoning-tuned LLMs and their forward reasoning paths (CoTs) for better alignment with human annotations.

Method: Proposes a pipeline with discourse segmenters to extract rationales from CoTs and a rank-based HLV evaluation framework prioritizing answer rankings over exact scores.

Result: Outperforms direct generation methods and baselines on three datasets, showing better alignment with human rankings.

Conclusion: The approach effectively leverages CoTs and discourse analysis to improve human label variation tasks, demonstrating superior performance and alignment.

Abstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which
generate chains of thought (CoTs) before giving the final answer--has attracted
significant attention and offers new opportunities for gaining insights into
human label variation, which refers to plausible differences in how multiple
annotators label the same data instance. Prior work has shown that
LLM-generated explanations can help align model predictions with human label
distributions, but typically adopt a reverse paradigm: producing explanations
based on given answers. In contrast, CoTs provide a forward reasoning path that
may implicitly embed rationales for each answer option, before generating the
answers. We thus propose a novel LLM-based pipeline enriched with
linguistically-grounded discourse segmenters to extract supporting and opposing
statements for each answer option from CoTs with improved accuracy. We also
propose a rank-based HLV evaluation framework that prioritizes the ranking of
answers over exact scores, which instead favor direct comparison of label
distributions. Our method outperforms a direct generation method as well as
baselines on three datasets, and shows better alignment of ranking methods with
humans, highlighting the effectiveness of our approach.

</details>


### [79] [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/pdf/2505.23404)
*Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin*

Main category: cs.CL

TL;DR: The paper introduces adaptive jailbreaking strategies for LLMs, categorizing them by semantic understanding and achieving a 98.9% success rate on GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adversarial attacks on LLMs that bypass safety constraints, compromising their reliability.

Method: Classifies LLMs into Type I and Type II based on semantic comprehension, then designs tailored jailbreaking strategies for each.

Result: The adaptive strategy significantly improves jailbreaking success rates, notably achieving 98.9% on GPT-4o.

Conclusion: The proposed framework effectively exploits LLM vulnerabilities, highlighting the need for improved defenses.

Abstract: Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)

</details>


### [80] [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/pdf/2505.23410)
*Xuan Gong, Hanbo Huang, Shiyu Liang*

Main category: cs.CL

TL;DR: The paper explores how test-time prompts can mitigate the factuality gap in LLMs caused by fine-tuning on known vs. unknown knowledge, showing ICL's compensatory role.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of fine-tuning data on LLM factuality and the role of test-time prompts in knowledge extraction.

Method: Systematic experiments and theoretical analysis from a knowledge graph perspective, focusing on OOD settings and ICL prompts.

Result: Test-time prompts can overshadow fine-tuning data's impact, with ICL effectively compensating for data shortcomings.

Conclusion: ICL should be reconsidered for evaluating fine-tuning data selection, as it plays a dominant role in knowledge extraction.

Abstract: Factual knowledge extraction aims to explicitly extract knowledge
parameterized in pre-trained language models for application in downstream
tasks. While prior work has been investigating the impact of supervised
fine-tuning data on the factuality of large language models (LLMs), its
mechanism remains poorly understood. We revisit this impact through systematic
experiments, with a particular focus on the factuality gap that arises when
fine-tuning on known versus unknown knowledge. Our findings show that this gap
can be mitigated at the inference stage, either under out-of-distribution (OOD)
settings or by using appropriate in-context learning (ICL) prompts (i.e.,
few-shot learning and Chain of Thought (CoT)). We prove this phenomenon
theoretically from the perspective of knowledge graphs, showing that the
test-time prompt may diminish or even overshadow the impact of fine-tuning data
and play a dominant role in knowledge extraction. Ultimately, our results shed
light on the interaction between finetuning data and test-time prompt,
demonstrating that ICL can effectively compensate for shortcomings in
fine-tuning data, and highlighting the need to reconsider the use of ICL
prompting as a means to evaluate the effectiveness of fine-tuning data
selection methods.

</details>


### [81] [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/pdf/2505.23420)
*Marco Gaido, Sara Papi, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri*

Main category: cs.CL

TL;DR: The paper explores the need for sub-exponential learning rate (LR) warmup in large-scale speech-to-text (S2T) trainings, finding that higher LRs in warmup speed initial convergence but don't improve final performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of comparison and analysis of LR warmup schedules in large-scale S2T models, which use complex architectures like Conformer or Branchformer.

Method: The paper evaluates different LR warmup schedules, focusing on sub-exponential warmup, and compares their impact on convergence and final performance.

Result: Findings show that sub-exponential LR warmup is optimal for large-scale S2T trainings, and higher LRs during warmup accelerate initial convergence without enhancing final results.

Conclusion: The study concludes that sub-exponential LR warmup is essential for large-scale S2T models, and initial high LRs are beneficial for convergence but not final performance.

Abstract: Training large-scale models presents challenges not only in terms of resource
requirements but also in terms of their convergence. For this reason, the
learning rate (LR) is often decreased when the size of a model is increased.
Such a simple solution is not enough in the case of speech-to-text (S2T)
trainings, where evolved and more complex variants of the Transformer
architecture -- e.g., Conformer or Branchformer -- are used in light of their
better performance. As a workaround, OWSM designed a double linear warmup of
the LR, increasing it to a very small value in the first phase before updating
it to a higher value in the second phase. While this solution worked well in
practice, it was not compared with alternative solutions, nor was the impact on
the final performance of different LR warmup schedules studied. This paper
fills this gap, revealing that i) large-scale S2T trainings demand a
sub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates
initial convergence, but it does not boost final performance.

</details>


### [82] [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/pdf/2505.23461)
*Chuanyuan Tan, Wenbiao Shao, Hao Xiong, Tong Zhu, Zhenhua Liu, Kai Shi, Wenliang Chen*

Main category: cs.CL

TL;DR: The paper introduces UAQFact, a bilingual dataset with factual knowledge for evaluating LLMs on unanswerable questions, revealing challenges in their performance despite knowledge availability.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for unanswerable questions lack factual knowledge support, limiting evaluation of LLMs' ability to use such knowledge.

Method: Created UAQFact, a bilingual dataset with auxiliary factual knowledge from a Knowledge Graph, and defined two tasks to measure LLMs' use of internal and external knowledge.

Result: LLMs struggle with UAQFact, performing inconsistently even with stored knowledge, and external knowledge only partially improves performance.

Conclusion: UAQFact highlights LLMs' limitations in utilizing factual knowledge for unanswerable questions, suggesting room for improvement in knowledge integration.

Abstract: Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps
prevent misleading responses in complex situations. While previous studies have
built several datasets to assess LLMs' performance on UAQ, these datasets lack
factual knowledge support, which limits the evaluation of LLMs' ability to
utilize their factual knowledge when handling UAQ. To address the limitation,
we introduce a new unanswerable question dataset UAQFact, a bilingual dataset
with auxiliary factual knowledge created from a Knowledge Graph. Based on
UAQFact, we further define two new tasks to measure LLMs' ability to utilize
internal and external factual knowledge, respectively. Our experimental results
across multiple LLM series show that UAQFact presents significant challenges,
as LLMs do not consistently perform well even when they have factual knowledge
stored. Additionally, we find that incorporating external knowledge may enhance
performance, but LLMs still cannot make full use of the knowledge which may
result in incorrect responses.

</details>


### [83] [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/pdf/2505.23477)
*Krithik Vishwanath, Anton Alyakin, Mrigayu Ghosh, Jin Vivian Lee, Daniel Alexander Alber, Karl L. Sangwon, Douglas Kondziolka, Eric Karl Oermann*

Main category: cs.CL

TL;DR: The study evaluates 28 large language models (LLMs) on neurosurgery board-like questions, finding 6 passed, but performance dropped significantly with distractors.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' neurosurgical knowledge and their robustness to distracting information for clinical applications.

Method: Tested 28 LLMs on 2,904 CNS-SANS questions and introduced a distraction framework with irrelevant statements.

Result: 6 LLMs passed; performance dropped by up to 20.4% with distractors, with proprietary models outperforming open-source ones.

Conclusion: LLMs show promise but are vulnerable to distractions, highlighting the need for resilience strategies for clinical use.

Abstract: The Congress of Neurological Surgeons Self-Assessment for Neurological
Surgeons (CNS-SANS) questions are widely used by neurosurgical residents to
prepare for written board examinations. Recently, these questions have also
served as benchmarks for evaluating large language models' (LLMs) neurosurgical
knowledge. This study aims to assess the performance of state-of-the-art LLMs
on neurosurgery board-like questions and to evaluate their robustness to the
inclusion of distractor statements. A comprehensive evaluation was conducted
using 28 large language models. These models were tested on 2,904 neurosurgery
board examination questions derived from the CNS-SANS. Additionally, the study
introduced a distraction framework to assess the fragility of these models. The
framework incorporated simple, irrelevant distractor statements containing
polysemous words with clinical meanings used in non-clinical contexts to
determine the extent to which such distractions degrade model performance on
standard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing
outcomes, with the top-performing models scoring over 15.7% above the passing
threshold. When exposed to distractions, accuracy across various model
architectures was significantly reduced-by as much as 20.4%-with one model
failing that had previously passed. Both general-purpose and medical
open-source models experienced greater performance declines compared to
proprietary variants when subjected to the added distractors. While current
LLMs demonstrate an impressive ability to answer neurosurgery board-like exam
questions, their performance is markedly vulnerable to extraneous, distracting
information. These findings underscore the critical need for developing novel
mitigation strategies aimed at bolstering LLM resilience against in-text
distractions, particularly for safe and effective clinical deployment.

</details>


### [84] [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/pdf/2505.23480)
*Keqin Peng, Liang Ding, Yuanxin Ouyang, Meng Fang, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper analyzes overthinking in Reasoning Large Language Models (RLLMs) due to self-doubt, proposes a prompting method to reduce it, and validates its effectiveness on mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overthinking in RLLMs, characterized by unnecessary reasoning steps (self-doubt), which prior work only qualitatively analyzed.

Method: Introduces a prompting technique where the model first questions the input's validity, then responds concisely, reducing over-reliance on the question.

Result: The method reduces answer length and improves performance across datasets, minimizing reasoning steps and self-doubt.

Conclusion: The proposed prompting method effectively mitigates overthinking in RLLMs by addressing self-doubt, enhancing efficiency and accuracy.

Abstract: Reasoning Large Language Models (RLLMs) have demonstrated impressive
performance on complex tasks, largely due to the adoption of Long
Chain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking
-- performing unnecessary reasoning steps even after arriving at the correct
answer. Prior work has largely focused on qualitative analyses of overthinking
through sample-based observations of long CoTs. In contrast, we present a
quantitative analysis of overthinking from the perspective of self-doubt,
characterized by excessive token usage devoted to re-verifying already-correct
answer. We find that self-doubt significantly contributes to overthinking. In
response, we introduce a simple and effective prompting method to reduce the
model's over-reliance on input questions, thereby avoiding self-doubt.
Specifically, we first prompt the model to question the validity of the input
question, and then respond concisely based on the outcome of that evaluation.
Experiments on three mathematical reasoning tasks and four datasets with
missing premises demonstrate that our method substantially reduces answer
length and yields significant improvements across nearly all datasets upon 4
widely-used RLLMs. Further analysis demonstrates that our method effectively
minimizes the number of reasoning steps and reduces self-doubt.

</details>


### [85] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/pdf/2505.23495)
*Liangliang Zhang, Zhuorui Jiang, Hongliang Chi, Haoyang Chen, Mohammed Elkoumy, Fali Wang, Qiong Wu, Zhengyi Zhou, Shirui Pan, Suhang Wang, Yao Ma*

Main category: cs.CL

TL;DR: KGQAGen is introduced to address quality issues in KGQA benchmarks, creating KGQAGen-10k, a challenging benchmark that exposes limitations of state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Popular KGQA datasets have critical quality issues (e.g., incorrect annotations, ambiguous questions), with only 57% factual correctness.

Method: KGQAGen uses LLM-guided generation, structured knowledge grounding, and symbolic verification to create verifiable QA instances.

Result: KGQAGen-10k challenges even state-of-the-art KG-RAG models, revealing their limitations.

Conclusion: KGQAGen advocates for rigorous benchmark construction and serves as a scalable framework for KGQA evaluation.

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [86] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/pdf/2505.23538)
*Nawar Turk, Eeham Khan, Leila Kosseim*

Main category: cs.CL

TL;DR: The paper presents three model architectures for verifying promises in corporate ESG reports, achieving progressive improvements and outperforming the baseline.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying promises in ESG reports by exploring effective model architectures for subtasks like identification, evidence assessment, clarity evaluation, and verification timing.

Method: Three models: ESG-BERT with task-specific heads, ESG-BERT enhanced with linguistic features, and a combined subtask model with attention pooling, transformer representations, and multi-objective learning.

Result: The combined subtask model achieved a leaderboard score of 0.5268, outperforming the baseline (0.5227).

Conclusion: Linguistic features, attention pooling, and multi-objective learning are effective for promise verification, though challenges like class imbalance and limited data persist.

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [87] [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/pdf/2505.23540)
*Yunqiao Yang, Houxing Ren, Zimu Lu, Ke Wang, Weikang Shi, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li*

Main category: cs.CL

TL;DR: PCPO improves LLM reasoning by ensuring both answer correctness and token-level probability consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current preference optimization methods neglect internal logical coherence, focusing only on outcome-based criteria.

Method: Proposes PCPO, a framework using dual metrics: answer correctness and token-level probability consistency.

Result: PCPO outperforms existing approaches across diverse LLMs and benchmarks.

Conclusion: PCPO offers a robust solution for enhancing LLM reasoning by addressing logical coherence.

Abstract: Recent advances in preference optimization have demonstrated significant
potential for improving mathematical reasoning capabilities in large language
models (LLMs). While current approaches leverage high-quality pairwise
preference data through outcome-based criteria like answer correctness or
consistency, they fundamentally neglect the internal logical coherence of
responses. To overcome this, we propose Probability-Consistent Preference
Optimization (PCPO), a novel framework that establishes dual quantitative
metrics for preference selection: (1) surface-level answer correctness and (2)
intrinsic token-level probability consistency across responses. Extensive
experiments show that our PCPO consistently outperforms existing outcome-only
criterion approaches across a diverse range of LLMs and benchmarks. Our code is
publicly available at https://github.com/YunqiaoYang/PCPO.

</details>


### [88] [Translation in the Wild](https://arxiv.org/pdf/2505.23548)
*Yuri Balashov*

Main category: cs.CL

TL;DR: LLMs show strong translation abilities despite lacking dedicated training, possibly due to incidental bilingualism in data and instruction tuning. A duality hypothesis suggests two types of pre-training data may explain this.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs perform well in translation tasks without explicit training, exploring the role of data and internalization methods.

Method: Reflections based on recent studies and user experience, proposing a duality hypothesis for LLMs' translation abilities.

Result: LLMs' translation skills may stem from incidental bilingualism and instruction tuning, with potential duality in pre-training data.

Conclusion: The duality hypothesis offers a new perspective on LLM translation, with implications for rethinking translation in the deep learning era.

Abstract: Large Language Models (LLMs) excel in translation among other things,
demonstrating competitive performance for many language pairs in zero- and
few-shot settings. But unlike dedicated neural machine translation models, LLMs
are not trained on any translation-related objective. What explains their
remarkable translation abilities? Are these abilities grounded in "incidental
bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning
contribute to it? Are LLMs capable of aligning and leveraging semantically
identical or similar monolingual contents from different corners of the
internet that are unlikely to fit in a single context window? I offer some
reflections on this topic, informed by recent studies and growing user
experience. My working hypothesis is that LLMs' translation abilities originate
in two different types of pre-training data that may be internalized by the
models in different ways. I discuss the prospects for testing the "duality"
hypothesis empirically and its implications for reconceptualizing translation,
human and machine, in the age of deep learning.

</details>


### [89] [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/pdf/2505.23556)
*Wei Jie Yeo, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy*

Main category: cs.CL

TL;DR: The paper investigates the internal mechanisms of refusal behaviors in aligned language models using sparse autoencoders to identify and intervene on refusal-related features.


<details>
  <summary>Details</summary>
Motivation: To understand the opaque internal mechanisms driving refusal behaviors in instruction-tuned LLMs and explore their causal mediation.

Method: Uses sparse autoencoders to identify latent refusal features, intervenes on these features, and validates their impact across harmful datasets.

Result: Identifies refusal-related features, validates their behavioral influence, and demonstrates their utility in enhancing generalization for adversarial samples.

Conclusion: Provides insights into refusal mechanisms, adversarial jailbreaking, and shows practical applications for improving model robustness.

Abstract: Refusal is a key safety behavior in aligned language models, yet the internal
mechanisms driving refusals remain opaque. In this work, we conduct a
mechanistic study of refusal in instruction-tuned LLMs using sparse
autoencoders to identify latent features that causally mediate refusal
behaviors. We apply our method to two open-source chat models and intervene on
refusal-related features to assess their influence on generation, validating
their behavioral impact across multiple harmful datasets. This enables a
fine-grained inspection of how refusal manifests at the activation level and
addresses key research questions such as investigating upstream-downstream
latent relationship and understanding the mechanisms of adversarial
jailbreaking techniques. We also establish the usefulness of refusal features
in enhancing generalization for linear probes to out-of-distribution
adversarial samples in classification tasks. We open source our code in
https://github.com/wj210/refusal_sae.

</details>


### [90] [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/pdf/2505.23570)
*Leonardo La Rocca, Francesco Corso, Francesco Pierri*

Main category: cs.CL

TL;DR: The study evaluates open-weight LLMs for detecting conspiracy theory videos on YouTube, finding text-based models achieve high recall but lower precision, while multimodal models underperform. RoBERTa matches LLM performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: YouTube's global reach makes it prone to harmful content like disinformation. The study aims to assess LLMs' effectiveness in identifying such content.

Method: The study uses labeled and unlabeled datasets to evaluate text-only and multimodal LLMs in zero-shot settings, comparing them to a fine-tuned RoBERTa baseline.

Result: Text-based LLMs show high recall but low precision, increasing false positives. Multimodal models perform worse. RoBERTa matches LLMs with fewer parameters.

Conclusion: Current LLM-based approaches for harmful content detection have limitations, highlighting the need for more precise and robust systems.

Abstract: As a leading online platform with a vast global audience, YouTube's extensive
reach also makes it susceptible to hosting harmful content, including
disinformation and conspiracy theories. This study explores the use of
open-weight Large Language Models (LLMs), both text-only and multimodal, for
identifying conspiracy theory videos shared on YouTube. Leveraging a labeled
dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot
setting and compare their performance to a fine-tuned RoBERTa baseline. Results
show that text-based LLMs achieve high recall but lower precision, leading to
increased false positives. Multimodal models lag behind their text-only
counterparts, indicating limited benefits from visual data integration. To
assess real-world applicability, we evaluate the most accurate models on an
unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with
a larger number of parameters. Our work highlights the strengths and
limitations of current LLM-based approaches for online harmful content
detection, emphasizing the need for more precise and robust systems.

</details>


### [91] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/pdf/2505.23604)
*Guangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox, Gregory Wornell, Wei Lu, Zhang-Wei Hong, Chuang Gan*

Main category: cs.CL

TL;DR: EvoScale improves smaller LMs' performance on real-world coding tasks by using evolutionary refinement and RL, reducing sample needs and matching larger models' results.


<details>
  <summary>Details</summary>
Motivation: Smaller LMs struggle with real-world coding tasks despite lower computational costs, and existing methods like SFT or test-time scaling are expensive or inefficient.

Method: Proposes EvoScale, an evolutionary process with RL for iterative refinement, reducing samples and avoiding external verifiers.

Result: EvoScale enables a 32B model to perform as well as 100B+ models on SWE-Bench-Verified with fewer samples.

Conclusion: EvoScale offers a sample-efficient, scalable solution for improving smaller LMs' performance on complex tasks.

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [92] [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/pdf/2505.23621)
*Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao*

Main category: cs.CL

TL;DR: The paper introduces inference-time scaling for table reasoning tasks using distillation and RLVR, achieving GPT-4.1-level performance with a smaller 7B-parameter model.


<details>
  <summary>Details</summary>
Motivation: To explore and improve inference-time scaling for table reasoning tasks, addressing the need for efficient and high-performing models.

Method: Develops two strategies: distillation using reasoning traces from DeepSeek-R1 (Table-R1-SFT) and RL with verifiable rewards (RLVR) leading to Table-R1-Zero.

Result: Table-R1-Zero matches/exceeds GPT-4.1 and DeepSeek-R1 performance on diverse tasks, with strong out-of-domain generalization.

Conclusion: Instruction tuning, model architecture, and RL training enhance table reasoning skills, demonstrating the effectiveness of the proposed methods.

Abstract: In this work, we present the first study to explore inference-time scaling on
table reasoning tasks. We develop and evaluate two post-training strategies to
enable inference-time scaling: distillation from frontier model reasoning
traces and reinforcement learning with verifiable rewards (RLVR). For
distillation, we introduce a large-scale dataset of reasoning traces generated
by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For
RLVR, we propose task-specific verifiable reward functions and apply the GRPO
algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series
models across diverse table reasoning tasks, including short-form QA, fact
verification, and free-form QA. Notably, the Table-R1-Zero model matches or
exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a
7B-parameter LLM. It also demonstrates strong generalization to out-of-domain
datasets. Extensive ablation and qualitative analyses reveal the benefits of
instruction tuning, model architecture choices, and cross-task generalization,
as well as emergence of essential table reasoning skills during RL training.

</details>


### [93] [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/pdf/2505.23623)
*Jiaoda Li, Ryan Cotterell*

Main category: cs.CL

TL;DR: The paper characterizes the expressive power of fixed-precision transformers with soft attention, linking them to a fragment of linear temporal logic and formal language theory.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between idealized transformer models and practical implementations by analyzing their theoretical expressivity.

Method: Theoretical analysis of fixed-precision transformers with strict future masking and soft attention, connecting them to linear temporal logic and formal language theory.

Result: Such transformers are as expressive as a specific fragment of linear temporal logic (past operator) and align with formal language classes. Empirical results confirm perfect generalization within their capacity.

Conclusion: The study provides a unified theoretical framework for transformer expressivity, validated by empirical evidence.

Abstract: Transformer-based language models (LMs) have achieved widespread empirical
success, but their theoretical expressive power remains only partially
understood. Prior work often relies on idealized models with assumptions --
such as arbitrary numerical precision and hard attention -- that diverge from
real-world transformers. In this work, we provide an exact characterization of
fixed-precision transformers with strict future masking and soft attention, an
idealization that more closely mirrors practical implementations. We show that
these models are precisely as expressive as a specific fragment of linear
temporal logic that includes only a single temporal operator: the past
operator. We further relate this logic to established classes in formal
language theory, automata theory, and algebra, yielding a rich and unified
theoretical framework for understanding transformer expressivity. Finally, we
present empirical results that align closely with our theory: transformers
trained on languages within their theoretical capacity generalize perfectly
over lengths, while they consistently fail to generalize on languages beyond
it.

</details>


### [94] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/pdf/2505.23628)
*Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG autonomously constructs knowledge graphs without predefined schemas, using LLMs to extract triples and induce schemas from text, achieving high alignment with human-crafted schemas.


<details>
  <summary>Details</summary>
Motivation: To eliminate the need for manual schema creation in knowledge graph construction and enhance LLM factuality.

Method: Leverages large language models to extract knowledge triples and induce schemas from text, organizing entities and events into semantic categories.

Result: Constructed ATLAS, a family of knowledge graphs with 900+ million nodes and 5.9 billion edges, outperforming baselines on multi-hop QA tasks and achieving 95% semantic alignment with human schemas.

Conclusion: Dynamically induced schemas in billion-scale knowledge graphs can effectively complement parametric knowledge in LLMs.

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [95] [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/pdf/2505.23630)
*Enzo Doyen, Amalia Todirascu*

Main category: cs.CL

TL;DR: GeNRe is the first French gender-neutral rewriting system using collective nouns, employing rule-based and fine-tuned models to mitigate gender bias in NLP.


<details>
  <summary>Details</summary>
Motivation: Addressing gender bias in French NLP, particularly due to masculine generics, by developing a neutral rewriting system.

Method: Rule-based system (RBS) for French, fine-tuned language models, and instruct-based models (Claude 3 Opus) for performance enhancement.

Result: Claude 3 Opus with the dictionary achieves results close to the RBS, demonstrating effective gender-neutral rewriting.

Conclusion: GeNRe advances gender bias mitigation in French NLP, encouraging further development in this area.

Abstract: A significant portion of the textual data used in the field of Natural
Language Processing (NLP) exhibits gender biases, particularly due to the use
of masculine generics (masculine words that are supposed to refer to mixed
groups of men and women), which can perpetuate and amplify stereotypes. Gender
rewriting, an NLP task that involves automatically detecting and replacing
gendered forms with neutral or opposite forms (e.g., from masculine to
feminine), can be employed to mitigate these biases. While such systems have
been developed in a number of languages (English, Arabic, Portuguese, German,
French), automatic use of gender neutralization techniques (as opposed to
inclusive or gender-switching techniques) has only been studied for English.
This paper presents GeNRe, the very first French gender-neutral rewriting
system using collective nouns, which are gender-fixed in French. We introduce a
rule-based system (RBS) tailored for the French language alongside two
fine-tuned language models trained on data generated by our RBS. We also
explore the use of instruct-based models to enhance the performance of our
other systems and find that Claude 3 Opus combined with our dictionary achieves
results close to our RBS. Through this contribution, we hope to promote the
advancement of gender bias mitigation techniques in NLP for French.

</details>


### [96] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/pdf/2505.23646)
*Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua*

Main category: cs.CL

TL;DR: The paper investigates whether large reasoning models (LRMs) are more prone to hallucination in fact-seeking tasks, analyzing their post-training pipelines, cognitive behaviors, and model uncertainty.


<details>
  <summary>Details</summary>
Motivation: To clarify the debate on whether LRMs generalize reasoning capability to reduce hallucination in fact-seeking tasks, given conflicting reports like DeepSeek-R1's improved performance versus OpenAI-o3's increased hallucination.

Method: (1) Holistic evaluation of hallucination in LRMs, (2) behavior analysis of cognitive traits (Flaw Repetition and Think-Answer Mismatch), and (3) investigation of model uncertainty's role in hallucination.

Result: LRMs with full post-training (cold start SFT and verifiable reward RL) reduce hallucination, while distillation alone or RL without cold start SFT worsens it. Misalignment between model uncertainty and factual accuracy correlates with increased hallucination.

Conclusion: The study provides initial insights into LRM hallucination, highlighting the impact of post-training pipelines, cognitive behaviors, and model uncertainty.

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [97] [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/pdf/2505.23654)
*Mohamed Elaraby, Diane Litman*

Main category: cs.CL

TL;DR: The paper evaluates how well instruction-tuned LLMs preserve argument roles in summaries, introducing ARC to measure coverage. Findings show gaps in capturing critical arguments, especially when sparse, and highlight biases in LLM behavior.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs retain crucial argument roles in summaries, particularly in high-stakes domains like law and science.

Method: Introduces Argument Representation Coverage (ARC) to analyze summaries from three open-weight LLMs in legal and scientific domains.

Result: LLMs partially cover argument roles but often omit critical information, especially with sparse arguments. ARC reveals biases in LLM behavior.

Conclusion: Highlights the need for argument-aware summarization strategies to improve coverage of key arguments.

Abstract: Integrating structured information has long improved the quality of
abstractive summarization, particularly in retaining salient content. In this
work, we focus on a specific form of structure: argument roles, which are
crucial for summarizing documents in high-stakes domains such as law. We
investigate whether instruction-tuned large language models (LLMs) adequately
preserve this information. To this end, we introduce Argument Representation
Coverage (ARC), a framework for measuring how well LLM-generated summaries
capture salient arguments. Using ARC, we analyze summaries produced by three
open-weight LLMs in two domains where argument roles are central: long legal
opinions and scientific articles. Our results show that while LLMs cover
salient argument roles to some extent, critical information is often omitted in
generated summaries, particularly when arguments are sparsely distributed
throughout the input. Further, we use ARC to uncover behavioral patterns --
specifically, how the positional bias of LLM context windows and role-specific
preferences impact the coverage of key arguments in generated summaries,
emphasizing the need for more argument-aware summarization strategies.

</details>


### [98] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/pdf/2505.23657)
*Hongxiang Zhang, Hao Chen, Tianyi Zhang, Muhao Chen*

Main category: cs.CL

TL;DR: ActLCD is a new decoding method for LLMs that uses reinforcement learning to reduce hallucinations by dynamically applying contrasting layers during generation.


<details>
  <summary>Details</summary>
Motivation: LLMs still suffer from hallucinations, especially in long contexts, despite existing token-level decoding improvements.

Method: ActLCD frames decoding as a sequential decision problem, using a reinforcement learning policy guided by a reward-aware classifier to optimize factuality.

Result: ActLCD outperforms state-of-the-art methods across five benchmarks, effectively reducing hallucinations.

Conclusion: ActLCD is a promising approach for enhancing the factuality of LLM outputs by addressing hallucinations beyond the token level.

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [99] [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/pdf/2505.23662)
*Beong-woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil Yang, Jinyoung Yeo*

Main category: cs.CL

TL;DR: ToolHaystack benchmark evaluates LLMs' long-term tool use, revealing gaps in robustness not shown by short-context tests.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on short contexts, lacking insight into long-term tool use behavior.

Method: Introduce ToolHaystack, a benchmark with multi-task execution contexts and realistic noise in continuous conversations.

Result: Current LLMs perform well in standard multi-turn settings but struggle in ToolHaystack, showing long-term robustness gaps.

Conclusion: ToolHaystack highlights critical weaknesses in LLMs' long-term tool use, suggesting need for improved robustness.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in using
external tools to address user inquiries. However, most existing evaluations
assume tool use in short contexts, offering limited insight into model behavior
during realistic long-term interactions. To fill this gap, we introduce
ToolHaystack, a benchmark for testing the tool use capabilities in long-term
interactions. Each test instance in ToolHaystack includes multiple tasks
execution contexts and realistic noise within a continuous conversation,
enabling assessment of how well models maintain context and handle various
disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find
that while current models perform well in standard multi-turn settings, they
often significantly struggle in ToolHaystack, highlighting critical gaps in
their long-term robustness not revealed by previous tool benchmarks.

</details>


### [100] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/pdf/2505.23666)
*Luke McDermott, Robert W. Heath Jr., Rahul Parhi*

Main category: cs.CL

TL;DR: LoLA (Low-rank Linear Attention with sparse caching) improves linear attention models by addressing memory collisions, enabling efficient long-context inference with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models suffer from quadratic complexity in long sequences, and existing linear attention methods fail to approximate softmax attention accurately.

Method: LoLA incorporates sliding window attention, sparse global caching, and recurrent hidden states to manage key-value pairs, avoiding memory interference.

Result: LoLA achieves 97.4% accuracy on 4K context tasks (vs. 0.6% baseline) and strong performance on zero-shot reasoning, with a smaller cache than Llama-3.1 8B.

Conclusion: LoLA is a lightweight, efficient solution for long-context inference, reproducible on consumer GPUs.

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


### [101] [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/pdf/2505.23688)
*James Tanner, Morgan Sonderegger, Jane Stuart-Smith, Jeff Mielke, Tyler Kendall*

Main category: cs.CL

TL;DR: The paper shows that wav2vec2.0 models can accurately classify stop burst presence in English and Japanese, replicating manual annotation patterns and enabling scalable phonetic research.


<details>
  <summary>Details</summary>
Motivation: There is a lack of tools for annotating variable phonetic phenomena, despite the widespread use of automatic tools in phonetic research.

Method: The study uses pre-trained wav2vec2.0 models to classify stop burst presence in speech data from English and Japanese corpora.

Result: The models achieve high accuracy in classification, replicating variability patterns seen in manual annotations.

Conclusion: Pre-trained speech models like wav2vec2.0 can automate speech corpus annotation, expanding the scope of phonetic research.

Abstract: Modern phonetic research regularly makes use of automatic tools for the
annotation of speech data, however few tools exist for the annotation of many
variable phonetic phenomena. At the same time, pre-trained self-supervised
models, such as wav2vec2.0, have been shown to perform well at speech
classification tasks and latently encode fine-grained phonetic information. We
demonstrate that wav2vec2.0 models can be trained to automatically classify
stop burst presence with high accuracy in both English and Japanese, robust
across both finely-curated and unprepared speech corpora. Patterns of
variability in stop realisation are replicated with the automatic annotations,
and closely follow those of manual annotations. These results demonstrate the
potential of pre-trained speech models as tools for the automatic annotation
and processing of speech corpus data, enabling researchers to `scale-up' the
scope of phonetic research with relative ease.

</details>


### [102] [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/pdf/2505.23689)
*Francesca Padovani, Jaap Jumelet, Yevgen Matusevych, Arianna Bisazza*

Main category: cs.CL

TL;DR: CDL-trained LMs don't consistently outperform Wikipedia-trained models across languages, objectives, or benchmarks. A new method, FIT-CLAMS, addresses frequency biases in evaluations.


<details>
  <summary>Details</summary>
Motivation: To test if CDL's benefits generalize across languages, model types, and evaluation settings, and to improve benchmarking methods.

Method: Compared CDL vs. Wikipedia-trained models using masked/causal objectives, three languages, and syntactic benchmarks. Introduced FIT-CLAMS for balanced evaluations.

Result: CDL rarely outperformed Wikipedia; FIT-CLAMS revealed frequency biases in prior benchmarks.

Conclusion: CDL doesn't enhance syntactic generalization; frequency control is crucial for fair evaluations.

Abstract: Seminal work by Huebner et al. (2021) showed that language models (LMs)
trained on English Child-Directed Language (CDL) can reach similar syntactic
abilities as LMs trained on much larger amounts of adult-directed written text,
suggesting that CDL could provide more effective LM training material than the
commonly used internet-crawled data. However, the generalizability of these
results across languages, model types, and evaluation settings remains unclear.
We test this by comparing models trained on CDL vs. Wikipedia across two LM
objectives (masked and causal), three languages (English, French, German), and
three syntactic minimal-pair benchmarks. Our results on these benchmarks show
inconsistent benefits of CDL, which in most cases is outperformed by Wikipedia
models. We then identify various shortcomings in previous benchmarks, and
introduce a novel testing methodology, FIT-CLAMS, which uses a
frequency-controlled design to enable balanced comparisons across training
corpora. Through minimal pair evaluations and regression analysis we show that
training on CDL does not yield stronger generalizations for acquiring syntax
and highlight the importance of controlling for frequency effects when
evaluating syntactic ability.

</details>


### [103] [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/pdf/2505.23701)
*Ziling Cheng, Meng Cao, Leila Pishdad, Yanshuai Cao, Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: Final-answer metrics conflate abstract formulation and arithmetic computation in LLMs. Disentangled evaluation reveals arithmetic bottlenecks, and CoT aids computation more than abstraction.


<details>
  <summary>Details</summary>
Motivation: To clarify how final-answer metrics misrepresent reasoning by conflating abstract formulation and arithmetic computation.

Method: Disentangled evaluation on GSM8K and SVAMP with Llama-3 and Qwen2.5 models, analyzing CoT impact and mechanistic abstraction-computation steps.

Result: Arithmetic computation is the bottleneck; CoT mainly helps computation. Abstractions precede computation and are transferable.

Conclusion: Disentangled evaluation is crucial for accurate LLM reasoning assessment and future improvements.

Abstract: Final-answer-based metrics are commonly used for evaluating large language
models (LLMs) on math word problems, often taken as proxies for reasoning
ability. However, such metrics conflate two distinct sub-skills: abstract
formulation (capturing mathematical relationships using expressions) and
arithmetic computation (executing the calculations). Through a disentangled
evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of
Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the
arithmetic computation step and not by the abstract formulation step. Contrary
to the common belief, we show that CoT primarily aids in computation, with
limited impact on abstract formulation. Mechanistically, we show that these two
skills are composed conjunctively even in a single forward pass without any
reasoning steps via an abstract-then-compute mechanism: models first capture
problem abstractions, then handle computation. Causal patching confirms these
abstractions are present, transferable, composable, and precede computation.
These behavioural and mechanistic findings highlight the need for disentangled
evaluation to accurately assess LLM reasoning and to guide future improvements.

</details>


### [104] [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/pdf/2505.23713)
*Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen*

Main category: cs.CL

TL;DR: SocialMaze is a new benchmark for evaluating LLMs' social reasoning abilities, addressing gaps in existing frameworks by incorporating deep reasoning, dynamic interaction, and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations oversimplify social scenarios, lacking depth to challenge advanced LLMs.

Method: Introduces SocialMaze with six tasks across three settings, using automated and human validation.

Result: Models vary in handling dynamics and uncertainty; chain-of-thought reasoning aids deeper inference; fine-tuning improves performance.

Conclusion: SocialMaze fills a critical gap, enabling better assessment and improvement of LLMs' social reasoning.

Abstract: Large language models (LLMs) are increasingly applied to socially grounded
tasks, such as online community moderation, media content analysis, and social
reasoning games. Success in these contexts depends on a model's social
reasoning ability - the capacity to interpret social contexts, infer others'
mental states, and assess the truthfulness of presented information. However,
there is currently no systematic evaluation framework that comprehensively
assesses the social reasoning capabilities of LLMs. Existing efforts often
oversimplify real-world scenarios and consist of tasks that are too basic to
challenge advanced models. To address this gap, we introduce SocialMaze, a new
benchmark specifically designed to evaluate social reasoning. SocialMaze
systematically incorporates three core challenges: deep reasoning, dynamic
interaction, and information uncertainty. It provides six diverse tasks across
three key settings: social reasoning games, daily-life interactions, and
digital community platforms. Both automated and human validation are used to
ensure data quality. Our evaluation reveals several key insights: models vary
substantially in their ability to handle dynamic interactions and integrate
temporally evolving information; models with strong chain-of-thought reasoning
perform better on tasks requiring deeper inference beyond surface-level cues;
and model reasoning degrades significantly under uncertainty. Furthermore, we
show that targeted fine-tuning on curated reasoning examples can greatly
improve model performance in complex social scenarios. The dataset is publicly
available at: https://huggingface.co/datasets/MBZUAI/SocialMaze

</details>


### [105] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/pdf/2505.23714)
*Roksana Goworek, Harpal Karlcut, Muhammad Shezad, Nijaguna Darshana, Abhishek Mane, Syam Bondada, Raghav Sikka, Ulvi Mammadov, Rauf Allahverdiyev, Sriram Purighella, Paridhi Gupta, Muhinyia Ndegwa, Haim Dubossarsky*

Main category: cs.CL

TL;DR: The paper introduces new sense-annotated datasets for nine low-resource languages to improve cross-lingual transfer, along with a semi-automatic annotation method. It demonstrates their utility through WiC experiments, emphasizing the need for targeted datasets in polysemy disambiguation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality evaluation datasets in low-resource languages, which is crucial for advancing cross-lingual transfer and multilingual NLP.

Method: A semi-automatic annotation method is proposed to create sense-annotated datasets for polysemous words in nine low-resource languages.

Result: The datasets and method prove effective in WiC experiments, highlighting the importance of tailored datasets for polysemy disambiguation in low-resource settings.

Conclusion: The released datasets and code aim to support fair and robust multilingual NLP research, emphasizing the need for targeted evaluation benchmarks.

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [106] [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/pdf/2505.23715)
*Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: The paper introduces PCBench to evaluate LLMs' ability to critique flawed premises, revealing gaps in autonomous error detection and overthinking in reasoning models.


<details>
  <summary>Details</summary>
Motivation: LLMs often accept flawed premises uncritically, leading to unreliable outputs, highlighting the need for premise critique ability.

Method: PCBench incorporates four error types across three difficulty levels, evaluating 15 LLMs with multi-faceted metrics.

Result: Most models need explicit prompts for error detection, critique ability varies by difficulty/error type, and flawed premises cause overthinking.

Conclusion: Enhancing LLMs' premise critique ability is crucial for reliable, human-centric systems.

Abstract: Large language models (LLMs) have witnessed rapid advancements, demonstrating
remarkable capabilities. However, a notable vulnerability persists: LLMs often
uncritically accept flawed or contradictory premises, leading to inefficient
reasoning and unreliable outputs. This emphasizes the significance of
possessing the \textbf{Premise Critique Ability} for LLMs, defined as the
capacity to proactively identify and articulate errors in input premises. Most
existing studies assess LLMs' reasoning ability in ideal settings, largely
ignoring their vulnerabilities when faced with flawed premises. Thus, we
introduce the \textbf{Premise Critique Bench (PCBench)}, designed by
incorporating four error types across three difficulty levels, paired with
multi-faceted evaluation metrics. We conducted systematic evaluations of 15
representative LLMs. Our findings reveal: (1) Most models rely heavily on
explicit prompts to detect errors, with limited autonomous critique; (2)
Premise critique ability depends on question difficulty and error type, with
direct contradictions being easier to detect than complex or procedural errors;
(3) Reasoning ability does not consistently correlate with the premise critique
ability; (4) Flawed premises trigger overthinking in reasoning models, markedly
lengthening responses due to repeated attempts at resolving conflicts. These
insights underscore the urgent need to enhance LLMs' proactive evaluation of
input validity, positioning premise critique as a foundational capability for
developing reliable, human-centric systems. The code is available at
https://github.com/MLGroupJLU/Premise_Critique.

</details>


### [107] [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/pdf/2505.23722)
*Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze*

Main category: cs.CL

TL;DR: DEER improves in-context learning for NER by using label-guided token statistics for better example selection and targeted corrections, outperforming existing methods and nearing supervised fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Current ICL methods for NER rely on semantic similarity for demonstration selection, ignoring training labels and leading to suboptimal performance.

Method: DEER uses token-level statistics from training labels to enhance example selection and prompts LLMs to correct error-prone tokens.

Result: DEER outperforms existing ICL methods on five NER datasets and approaches supervised fine-tuning performance, showing robustness in low-resource settings.

Conclusion: DEER effectively leverages label statistics to improve ICL for NER, demonstrating strong performance and generalization across datasets and entities.

Abstract: In-context learning (ICL) enables large language models (LLMs) to perform new
tasks using only a few demonstrations. In Named Entity Recognition (NER),
demonstrations are typically selected based on semantic similarity to the test
instance, ignoring training labels and resulting in suboptimal performance. We
introduce DEER, a new method that leverages training labels through token-level
statistics to improve ICL performance. DEER first enhances example selection
with a label-guided, token-based retriever that prioritizes tokens most
informative for entity recognition. It then prompts the LLM to revisit
error-prone tokens, which are also identified using label statistics, and make
targeted corrections. Evaluated on five NER datasets using four different LLMs,
DEER consistently outperforms existing ICL methods and approaches the
performance of supervised fine-tuning. Further analysis shows its effectiveness
on both seen and unseen entities and its robustness in low-resource settings.

</details>


### [108] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/pdf/2505.23723)
*Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen*

Main category: cs.CL

TL;DR: The paper introduces a learning-based agentic ML framework using LLM agents with online RL, outperforming larger models despite minimal training.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based ML agents rely on manual prompts, lacking adaptability and optimization from diverse experiments.

Method: Proposes a framework with exploration-enriched fine-tuning, step-wise RL, and a unified reward module for autonomous ML.

Result: A 7B-sized ML-Agent trained on 9 tasks outperforms a 671B-sized agent and shows strong generalization.

Conclusion: The framework enables efficient, adaptive ML agents with superior performance and scalability.

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [109] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/pdf/2505.23729)
*Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SITAlign is a framework for aligning large language models with human preferences by optimizing primary objectives while meeting thresholds for secondary criteria, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Human decision-making involves satisficing strategies, which existing alignment methods overlook. SITAlign bridges this gap by incorporating bounded rationality principles.

Method: SITAlign is an inference-time framework that maximizes a primary objective while satisfying threshold constraints on secondary criteria, derived from theoretical sub-optimality bounds.

Result: On the PKU-SafeRLHF dataset, SITAlign outperforms state-of-the-art methods by 22.3% in GPT-4 win-tie rate for helpfulness while maintaining harmlessness thresholds.

Conclusion: SITAlign effectively aligns models with human preferences by operationalizing satisficing strategies, demonstrating superior performance over multi-objective approaches.

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [110] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/pdf/2505.23735)
*Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni*

Main category: cs.CL

TL;DR: ATLAS introduces a long-term memory module to address limitations in Transformers and recurrent models, improving performance in long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers and recurrent models struggle with long-context understanding and memory capacity. ATLAS aims to overcome these limitations by enhancing memory design.

Method: ATLAS optimizes memory based on current and past tokens, introducing DeepTransformers, a generalization of Transformers.

Result: ATLAS outperforms Transformers and linear recurrent models in tasks like language modeling and long-context understanding, achieving +80% accuracy in 10M context length.

Conclusion: ATLAS successfully addresses key limitations in existing models, offering a scalable solution for long-context tasks.

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [111] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/pdf/2505.23754)
*Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: DeepTheorem is a framework for informal theorem proving using natural language to enhance LLM reasoning. It includes a large dataset, reinforcement learning (RL-Zero), and evaluation metrics, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional ATP methods misalign with LLMs' natural language strengths, limiting their theorem-proving potential. DeepTheorem bridges this gap.

Method: Proposes DeepTheorem with a 121K theorem dataset, RL-Zero for reinforcement learning, and evaluation metrics for proof correctness and reasoning quality.

Result: DeepTheorem outperforms existing methods, improving LLM theorem-proving accuracy and reasoning quality.

Conclusion: DeepTheorem advances informal theorem proving and mathematical exploration, showcasing LLMs' potential in complex reasoning.

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [112] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/pdf/2505.23759)
*Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan*

Main category: cs.CL

TL;DR: The paper evaluates vision-language models (VLMs) on solving rebus puzzles, finding they struggle with abstract reasoning and visual metaphors despite some success with simple clues.


<details>
  <summary>Details</summary>
Motivation: Rebus puzzles challenge VLMs with multi-modal abstraction and symbolic reasoning, unlike traditional tasks, making them a unique test case.

Method: A hand-generated benchmark of diverse rebus puzzles was created to analyze VLM performance.

Result: VLMs perform well on simple visual clues but fail at abstract reasoning, lateral thinking, and visual metaphors.

Conclusion: Current VLMs lack the nuanced understanding required for complex rebus puzzles, highlighting limitations in multi-modal reasoning.

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [113] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/pdf/2505.23765)
*Wentao Zhang, Woojeong Kim, Yuntian Deng*

Main category: cs.CL

TL;DR: The paper introduces Aggregative Question Answering (AQA), a task for reasoning over large-scale chatbot interactions to answer aggregative queries, and presents WildChat-AQA, a benchmark for this task.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat chatbot interactions as independent, missing insights from aggregated data. The paper aims to address this gap by enabling models to reason across large-scale conversation logs.

Method: The authors propose Aggregative Question Answering and create WildChat-AQA, a benchmark with 6,027 aggregative questions from 182,330 real-world chatbot conversations.

Result: Experiments show current methods struggle with effective reasoning or face high computational costs, highlighting the need for new approaches.

Conclusion: The paper underscores the importance of extracting collective insights from conversational data and introduces a novel task and benchmark to advance research in this area.

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


### [114] [ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation](https://arxiv.org/pdf/2405.17057)
*Houxing Ren, Mingjie Zhan, Zhongyuan Wu, Aojun Zhou, Junting Pan, Hongsheng Li*

Main category: cs.CL

TL;DR: ReflectionCoder improves code generation by using reflection sequences from compiler feedback, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Enhancing one-off code generation performance by leveraging compiler feedback.

Method: Uses reflection sequences, reflection self-distillation, and dynamically masked distillation.

Result: Achieves state-of-the-art performance on HumanEval (+), MBPP (+), and MultiPL-E benchmarks.

Conclusion: The approach can benefit domains requiring long reasoning paths and focus on final results.

Abstract: Code generation plays a crucial role in various tasks, such as code
auto-completion and mathematical reasoning. Previous work has proposed numerous
methods to enhance code generation performance, including integrating feedback
from the compiler. Inspired by this, we present ReflectionCoder, a novel
approach that effectively leverages reflection sequences constructed by
integrating compiler feedback to improve one-off code generation performance.
Furthermore, we propose reflection self-distillation and dynamically masked
distillation to effectively utilize these reflection sequences. Extensive
experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPL-E,
demonstrate that models fine-tuned with our method achieve state-of-the-art
performance. Beyond the code domain, we believe this approach can benefit other
domains that focus on final results and require long reasoning paths. Code and
data are available at https://github.com/SenseLLM/ReflectionCoder.

</details>


### [115] [mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus](https://arxiv.org/pdf/2406.08707)
*Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, Rémi Lacroix, Cordelia Schmid, Rachel Bawden, Benoît Sagot*

Main category: cs.CL

TL;DR: mOSCAR is the first large-scale multilingual and multimodal dataset, addressing limitations of existing datasets by covering 163 languages and proving its utility in boosting few-shot learning for mLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for mLLMs are limited to English or lack diversity, hindering research for other languages. mOSCAR aims to fill this gap.

Method: mOSCAR was created by crawling the web, covering 163 languages, and underwent rigorous filtering. Two multilingual models were trained to evaluate its benefits.

Result: The model trained on mOSCAR showed significant improvement in few-shot learning across multilingual tasks, validating its effectiveness.

Conclusion: mOSCAR enables broader mLLM research by providing a high-quality, diverse, and multilingual dataset, with proven benefits for model performance.

Abstract: Multimodal Large Language Models (mLLMs) are trained on a large amount of
text-image data. While most mLLMs are trained on caption-like data only,
Alayrac et al. (2022) showed that additionally training them on interleaved
sequences of text and images can lead to the emergence of in-context learning
capabilities. However, the dataset they used, M3W, is not public and is only in
English. There have been attempts to reproduce their results but the released
datasets are English-only. In contrast, current multilingual and multimodal
datasets are either composed of caption-like only or medium-scale or fully
private data. This limits mLLM research for the 7,000 other languages spoken in
the world. We therefore introduce mOSCAR, to the best of our knowledge the
first large-scale multilingual and multimodal document corpus crawled from the
web. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We
carefully conduct a set of filtering and evaluation steps to make sure mOSCAR
is sufficiently safe, diverse and of good quality. We additionally train two
types of multilingual model to prove the benefits of mOSCAR: (1) a model
trained on a subset of mOSCAR and captioning data and (2) a model trained on
captioning data only. The model additionally trained on mOSCAR shows a strong
boost in few-shot learning performance across various multilingual image-text
tasks and benchmarks, confirming previous findings for English-only mLLMs. The
dataset is released under the Creative Commons CC BY 4.0 license and can be
accessed here: https://huggingface.co/datasets/oscar-corpus/mOSCAR

</details>


### [116] [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/pdf/2406.09325)
*Tomer Ashuach, Martin Tutek, Yonatan Belinkov*

Main category: cs.CL

TL;DR: REVS is a non-gradient-based method for unlearning sensitive information from LMs, outperforming existing methods in privacy protection and robustness.


<details>
  <summary>Details</summary>
Motivation: Address privacy risks in LMs caused by memorizing sensitive data, avoiding costly or bypassable current solutions.

Method: REVS modifies a small subset of neurons linked to sensitive tokens, evaluated on memorized email, URL, and synthetic SSN datasets.

Result: REVS shows superior unlearning performance and robustness to attacks while preserving model integrity.

Conclusion: REVS effectively mitigates privacy risks in LMs without compromising model functionality.

Abstract: Language models (LMs) risk inadvertently memorizing and divulging sensitive
or personally identifiable information (PII) seen in training data, causing
privacy concerns. Current approaches to address this issue involve costly
dataset scrubbing, or model filtering through unlearning and model editing,
which can be bypassed through extraction attacks. We propose REVS, a novel
non-gradient-based method for unlearning sensitive information from LMs. REVS
identifies and modifies a small subset of neurons relevant for constituent
tokens that form sensitive information. To adequately evaluate our method on
truly sensitive information, we curate three datasets: email and URL datasets
naturally memorized by the models, and a synthetic social security number
dataset that we tune the models to memorize. Compared to other methods, REVS
demonstrates superior performance in unlearning sensitive information and
robustness to extraction attacks, while retaining underlying model integrity.

</details>


### [117] [Neuro-symbolic Training for Reasoning over Spatial Language](https://arxiv.org/pdf/2406.13828)
*Tanawan Premsri, Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: The paper proposes neuro-symbolic techniques to improve spatial reasoning in language models by incorporating spatial logical rules as constraints.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning is vital for human-like machine interaction, but current language models struggle with nesting spatial expressions due to insufficient abstraction.

Method: The authors train language models using neuro-symbolic techniques, leveraging spatial logical rules as constraints for better supervision.

Result: Evaluation on spatial question-answering benchmarks shows improved performance in complex multi-hop spatial reasoning.

Conclusion: The proposed method effectively enhances language models' spatial reasoning abilities, aiding generalization across domains.

Abstract: Spatial reasoning based on natural language expressions is essential for
everyday human tasks. This reasoning ability is also crucial for machines to
interact with their environment in a human-like manner. However, recent
research shows that even state-of-the-art language models struggle with spatial
reasoning over text, especially when facing nesting spatial expressions. This
is attributed to not achieving the right level of abstraction required for
generalizability. To alleviate this issue, we propose training language models
with neuro-symbolic techniques that exploit the spatial logical rules as
constraints, providing additional supervision to improve spatial reasoning and
question answering. Training language models to adhere to spatial reasoning
rules guides them in making more effective and general abstractions for
transferring spatial knowledge to various domains. We evaluate our approach on
existing spatial question-answering benchmarks. Our results indicate the
effectiveness of our proposed technique in improving language models in complex
multi-hop spatial reasoning over text.

</details>


### [118] [CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction](https://arxiv.org/pdf/2407.00934)
*Jingheng Ye, Zishan Xu, Yinghui Li, Linlin Song, Qingyu Zhou, Hai-Tao Zheng, Ying Shen, Wenhao Jiang, Hong-Gee Kim, Ruitong Liu, Xin Su, Zifei Shan*

Main category: cs.CL

TL;DR: The paper introduces CLEME2.0, a reference-based metric for evaluating Grammatical Error Correction (GEC) systems, focusing on interpretability and outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Previous studies lacked interpretability in GEC evaluation metrics, prompting the need for a more transparent and comprehensive approach.

Method: CLEME2.0 evaluates GEC systems by analyzing hit-correction, wrong-correction, under-correction, and over-correction, combining these aspects for better human consistency.

Result: Experiments on human judgment and reference datasets show CLEME2.0's effectiveness, achieving state-of-the-art performance.

Conclusion: CLEME2.0 provides a robust and interpretable metric for GEC evaluation, advancing the field with superior human consistency and performance.

Abstract: The paper focuses on the interpretability of Grammatical Error Correction
(GEC) evaluation metrics, which received little attention in previous studies.
To bridge the gap, we introduce **CLEME2.0**, a reference-based metric
describing four fundamental aspects of GEC systems: hit-correction,
wrong-correction, under-correction, and over-correction. They collectively
contribute to exposing critical qualities and locating drawbacks of GEC
systems. Evaluating systems by combining these aspects also leads to superior
human consistency over other reference-based and reference-less metrics.
Extensive experiments on two human judgment datasets and six reference datasets
demonstrate the effectiveness and robustness of our method, achieving a new
state-of-the-art result. Our codes are released at
https://github.com/THUKElab/CLEME.

</details>


### [119] [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/pdf/2407.09447)
*Amelia F. Hardy, Houjun Liu, Bernard Lange, Duncan Eddy, Mykel J. Kochenderfer*

Main category: cs.CL

TL;DR: ASTPrompter is a method for low-perplexity red-teaming attacks on LLMs, achieving higher success rates and better transferability.


<details>
  <summary>Details</summary>
Motivation: Existing red-teaming approaches focus on high attack success rates but overlook low-perplexity attacks, which are harder to filter and more impactful.

Method: ASTPrompter uses contrastive preference learning to optimize for low perplexity while maintaining high attack success rates.

Result: ASTPrompter achieves 5.1x higher attack success on Llama-8.1B with 2.1x more likely inputs, and transfers well to other models.

Conclusion: Perplexity is an under-considered factor in red-teaming; ASTPrompter efficiently balances it with attack success.

Abstract: Existing LLM red-teaming approaches prioritize high attack success rate,
often resulting in high-perplexity prompts. This focus overlooks low-perplexity
attacks that are more difficult to filter, more likely to arise during benign
usage, and more impactful as negative downstream training examples. In
response, we introduce ASTPrompter, a single-step optimization method that uses
contrastive preference learning to train an attacker to maintain low perplexity
while achieving a high attack success rate (ASR). ASTPrompter achieves an
attack success rate 5.1 times higher on Llama-8.1B while using inputs that are
2.1 times more likely to occur according to the frozen LLM. Furthermore, our
attack transfers to Mistral-7B, Qwen-7B, and TinyLlama in both black- and
white-box settings. Lastly, by tuning a single hyperparameter in our method, we
discover successful attack prefixes along an efficient frontier between ASR and
perplexity, highlighting perplexity as a previously under-considered factor in
red-teaming.

</details>


### [120] [$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM Generated Multi-Document Topic Sets](https://arxiv.org/pdf/2407.17390)
*Itamar Trainin, Omri Abend*

Main category: cs.CL

TL;DR: The paper introduces $T^5Score$, a new evaluation methodology for assessing LLM-generated topics in multi-document extraction, addressing low inter-annotator agreement in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation practices for LLM-generated topics suffer from low inter-annotator agreement, limiting reliable use of LLMs for topic extraction.

Method: Proposes $T^5Score$, which decomposes topic quality into quantifiable aspects for easy manual or automatic evaluation.

Result: Extensive experiments on multiple datasets validate the methodology, showing strong inter-annotator agreement.

Conclusion: $T^5Score$ provides a reliable and convenient way to evaluate LLM-generated topics, improving their practical utility.

Abstract: Using LLMs for Multi-Document Topic Extraction has recently gained popularity
due to their apparent high-quality outputs, expressiveness, and ease of use.
However, most existing evaluation practices are not designed for LLM-generated
topics and result in low inter-annotator agreement scores, hindering the
reliable use of LLMs for the task. To address this, we introduce $T^5Score$, an
evaluation methodology that decomposes the quality of a topic set into
quantifiable aspects, measurable through easy-to-perform annotation tasks. This
framing enables a convenient, manual or automatic, evaluation procedure
resulting in a strong inter-annotator agreement score. To substantiate our
methodology and claims, we perform extensive experimentation on multiple
datasets and report the results.

</details>


### [121] [BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models](https://arxiv.org/pdf/2408.04556)
*Yupeng Chang, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: BA-LoRA is a parameter-efficient fine-tuning method for LLMs that reduces bias propagation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational and bias issues in fine-tuning LLMs.

Method: Introduces BA-LoRA with three regularization terms: consistency, diversity, and SVD.

Result: Outperforms LoRA and variants, reducing bias in NLU and NLG tasks.

Conclusion: BA-LoRA effectively mitigates bias and enhances model reliability.

Abstract: Large language models (LLMs) have demonstrated remarkable proficiency across
various natural language processing (NLP) tasks. However, adapting LLMs to
downstream applications requires computationally intensive and memory-demanding
fine-tuning procedures. To alleviate these burdens, parameter-efficient
fine-tuning (PEFT) techniques have emerged as a promising approach to tailor
LLMs with minimal computational overhead. While PEFT methods offer substantial
advantages, they do not fully address the pervasive issue of bias propagation
from pre-training data. This work introduces Bias-Alleviating Low-Rank
Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias
inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a
consistency regularizer, (2) a diversity regularizer, and (3) a singular value
decomposition regularizer. These regularizers aim to enhance the models'
consistency, diversity, and generalization capabilities during fine-tuning. We
conduct extensive experiments on natural language understanding (NLU) and
natural language generation (NLG) tasks using prominent LLMs such as LLaMA,
Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and
its state-of-the-art variants. Moreover, the extended experiments demonstrate
that our method effectively mitigates the adverse effects of pre-training bias,
leading to more reliable and robust model outputs. The code is available at
https://github.com/cyp-jlu-ai/BA-LoRA.

</details>


### [122] [X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents](https://arxiv.org/pdf/2408.09853)
*Weiqi Wu, Hongqiu Wu, Hai Zhao*

Main category: cs.CL

TL;DR: The paper introduces X-Turing, an enhanced Turing test using burst dialogue and pseudo-dialogue to evaluate LLMs in prolonged interactions, revealing performance drops over time.


<details>
  <summary>Details</summary>
Motivation: Traditional Turing tests lack natural conversational dynamics and human workload efficiency, limiting LLM evaluation in long-term interactions.

Method: X-Turing employs burst dialogue for dynamic exchanges and pseudo-dialogue to simulate long-term interactions, reducing human involvement.

Result: LLMs like GPT-4 show initial success (51.9% pass rate at 3 turns) but decline over time (38.9% at 10 turns), highlighting long-term consistency challenges.

Conclusion: X-Turing improves Turing test realism and efficiency, revealing LLMs' limitations in sustained human-like dialogue.

Abstract: The Turing test examines whether AIs exhibit human-like behaviour in natural
language conversations. The traditional setting limits each participant to one
message at a time and requires constant human participation. This fails to
reflect a natural conversational style and hinders the evaluation of dialogue
agents based on Large Language Models (LLMs) in complex and prolonged
interactions. This paper proposes \textbf{\textsc{X-Turing}}, which enhances
the original test with a \textit{burst dialogue} pattern, allowing more dynamic
exchanges using consecutive messages. It further reduces human workload by
iteratively generating dialogues that simulate the long-term interaction
between the agent and a human to compose the majority of the test process. With
the \textit{pseudo-dialogue} history, the agent then engages in a shorter
dialogue with a real human, which is paired with a human-human conversation on
the same topic to be judged using questionnaires. We introduce the
\textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across
varying durations. While LLMs like GPT-4 initially perform well, achieving pass
rates of 51.9\% and 38.9\% during 3 turns and 10 turns of dialogues
respectively, their performance drops as the dialogue progresses, which
underscores the difficulty in maintaining consistency in the long term.

</details>


### [123] [Resolving Lexical Bias in Model Editing](https://arxiv.org/pdf/2408.10411)
*Hammad Rizwan, Domenic Rosati, Ga Wu, Hassan Sajjad*

Main category: cs.CL

TL;DR: PENME introduces a disentangled representation space for precise model editing, avoiding weight modifications and outperforming previous methods in efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in current adapter-based model editing methods, which suffer from lexical biases and imprecise edits.

Method: Uses a disentangled representation space to localize edits precisely, maintaining distance between irrelevant prompts and proximity among paraphrases.

Result: Achieves state-of-the-art editing results, is computationally efficient during inference, and adaptable across architectures.

Conclusion: PENME provides a robust and efficient solution for model editing, overcoming limitations of prior approaches.

Abstract: Model editing aims to modify the outputs of large language models after they
are trained. Previous approaches have often involved direct alterations to
model weights, which can result in model degradation. Recent techniques avoid
making modifications to the model's weights by using an adapter that applies
edits to the model when triggered by semantic similarity in the representation
space. We demonstrate that current adapter methods are critically vulnerable to
strong lexical biases, leading to issues such as applying edits to irrelevant
prompts with overlapping words. This paper presents a principled approach to
learning a disentangled representation space that facilitates precise
localization of edits by maintaining distance between irrelevant prompts while
preserving proximity among paraphrases. In our empirical study, we show that
our method (Projector Editor Networks for Model Editing - PENME) achieves
state-of-the-art model editing results while being more computationally
efficient during inference than previous methods and adaptable across different
architectures.

</details>


### [124] [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/pdf/2408.13533)
*Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Pengpeng Shao, Jianhua Tao*

Main category: cs.CL

TL;DR: The paper introduces NoiserBench, a benchmark for evaluating the impact of linguistic noise on RAG models, categorizing noise into beneficial and harmful types.


<details>
  <summary>Details</summary>
Motivation: To address the limited understanding of noise in RAG models and its real-world applicability, the study aims to explore diverse noise types and their effects on LLMs.

Method: The authors define seven noise types, create NoiserBench, and evaluate eight LLMs across datasets and reasoning tasks.

Result: Noise is categorized into beneficial (enhances performance) and harmful (impairs performance), revealing nuanced impacts on LLMs.

Conclusion: The findings guide robust RAG solutions and hallucination mitigation, with practical implications for diverse retrieval scenarios.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial method for
addressing hallucinations in large language models (LLMs). While recent
research has extended RAG models to complex noisy scenarios, these explorations
often confine themselves to limited noise types and presuppose that noise is
inherently detrimental to LLMs, potentially deviating from real-world retrieval
environments and restricting practical applicability. In this paper, we define
seven distinct noise types from a linguistic perspective and establish a Noise
RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing
multiple datasets and reasoning tasks. Through empirical evaluation of eight
representative LLMs with diverse architectures and scales, we reveal that these
noises can be further categorized into two practical groups: noise that is
beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs
(aka harmful noise). While harmful noise generally impairs performance,
beneficial noise may enhance several aspects of model capabilities and overall
performance. Our analysis offers insights for developing more robust, adaptable
RAG solutions and mitigating hallucinations across diverse retrieval scenarios.
Code is available at https://github.com/jinyangwu/NoiserBench.

</details>


### [125] [Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs](https://arxiv.org/pdf/2410.03553)
*Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang*

Main category: cs.CL

TL;DR: SEPIT enhances protein language models with structural knowledge and integrates them with LLMs for general-purpose protein understanding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate protein property and function prediction is crucial, but current fine-tuned models are task-specific, lacking general-purpose understanding.

Method: SEPIT incorporates a structure-aware module into pLMs, uses contrastive learning and structure denoising, and employs a mixture of experts for refinement.

Result: SEPIT outperforms closed-source and open-source LLMs in both open-ended generation and closed-set answer tasks.

Conclusion: SEPIT bridges the gap in general-purpose protein understanding, demonstrating superior performance with its novel framework and comprehensive dataset.

Abstract: Proteins, as essential biomolecules, play a central role in biological
processes, including metabolic reactions and DNA replication. Accurate
prediction of their properties and functions is crucial in biological
applications. Recent development of protein language models (pLMs) with
supervised fine tuning provides a promising solution to this problem. However,
the fine-tuned model is tailored for particular downstream prediction task, and
achieving general-purpose protein understanding remains a challenge. In this
paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)
framework to bridge this gap. Our approach incorporates a novel structure-aware
module into pLMs to enrich their structural knowledge, and subsequently
integrates these enhanced pLMs with large language models (LLMs) to advance
protein understanding. In this framework, we propose a novel instruction tuning
pipeline. First, we warm up the enhanced pLMs using contrastive learning and
structure denoising. Then, caption-based instructions are used to establish a
basic understanding of proteins. Finally, we refine this understanding by
employing a mixture of experts (MoEs) to capture more complex properties and
functional information with the same number of activated parameters. Moreover,
we construct the largest and most comprehensive protein instruction dataset to
date, which allows us to train and evaluate the general-purpose protein
understanding model. Extensive experiments on both open-ended generation and
closed-set answer tasks demonstrate the superior performance of SEPIT over both
closed-source general LLMs and open-source LLMs trained with protein knowledge.

</details>


### [126] [Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs](https://arxiv.org/pdf/2410.11001)
*Haozhen Zhang, Tao Feng, Jiaxuan You*

Main category: cs.CL

TL;DR: The paper introduces $	extit{graph of records}$ (GoR) to enhance retrieval-augmented generation (RAG) by leveraging historical LLM responses, improving long-context summarization performance.


<details>
  <summary>Details</summary>
Motivation: Existing RAG approaches discard potentially insightful historical LLM responses, leading to suboptimal results.

Method: GoR constructs a graph linking retrieved text chunks and LLM responses, using a graph neural network and BERTScore-based objective for self-supervised training.

Result: GoR outperforms 12 baselines, achieving significant improvements (e.g., 15%, 8%, and 19% on Rouge-L, Rouge-1, and Rouge-2).

Conclusion: GoR effectively enhances RAG by utilizing historical responses, demonstrating superior performance in long-context summarization.

Abstract: Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose $\textit{graph of
records}$ ($\textbf{GoR}$), which leverages historical responses generated by
LLMs to enhance RAG for long-context global summarization. Inspired by the
$\textit{retrieve-then-generate}$ paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR features a $\textit{graph neural network}$ and an elaborately
designed $\textit{BERTScore}$-based objective for self-supervised model
training, enabling seamless supervision signal backpropagation between
reference summaries and node embeddings. We comprehensively compare GoR with 12
baselines across four long-context summarization datasets, and the results
indicate that our proposed method reaches the best performance
($\textit{e.g.}$, 15%, 8%, and 19% improvement over retrievers w.r.t. Rouge-L,
Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further
demonstrate the effectiveness of GoR.

</details>


### [127] [On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs](https://arxiv.org/pdf/2410.12600)
*Herun Wan, Minnan Luo, Zhixiong Su, Guang Dai, Xiang Zhao*

Main category: cs.CL

TL;DR: The paper investigates how LLMs can pollute evidence for malicious text detectors, proposes three defense strategies, and shows their limitations in mitigating the issue.


<details>
  <summary>Details</summary>
Motivation: To address the risks of evidence pollution by LLMs in malicious social text detection, which can confuse detectors.

Method: Explores manipulation scenarios (basic pollution, rephrasing, generating evidence) and proposes defenses: machine-generated text detection, mixture of experts, and parameter updating.

Result: Evidence pollution significantly harms detectors (up to 14.4% performance drop). Defenses help but have practical limitations. Polluted evidence is high-quality, harms model calibration (up to 21.6% error increase), and amplifies negative impacts (21.8% accuracy drop for encoder-based LMs).

Conclusion: Evidence pollution by LLMs is a serious threat to detectors, and while defenses exist, they are not fully effective, highlighting the need for further research.

Abstract: Evidence-enhanced detectors present remarkable abilities in identifying
malicious social text. However, the rise of large language models (LLMs) brings
potential risks of evidence pollution to confuse detectors. This paper explores
potential manipulation scenarios including basic pollution, and rephrasing or
generating evidence by LLMs. To mitigate the negative impact, we propose three
defense strategies from the data and model sides, including machine-generated
text detection, a mixture of experts, and parameter updating. Extensive
experiments on four malicious social text detection tasks with ten datasets
illustrate that evidence pollution significantly compromises detectors, where
the generating strategy causes up to a 14.4% performance drop. Meanwhile, the
defense strategies could mitigate evidence pollution, but they faced
limitations for practical employment. Further analysis illustrates that
polluted evidence (i) is of high quality, evaluated by metrics and humans; (ii)
would compromise the model calibration, increasing expected calibration error
up to 21.6%; and (iii) could be integrated to amplify the negative impact,
especially for encoder-based LMs, where the accuracy drops by 21.8%.

</details>


### [128] [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/pdf/2410.12974)
*Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla*

Main category: cs.CL

TL;DR: The paper introduces BenchmarkCards, a framework to standardize and simplify the selection of benchmarks for evaluating large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting appropriate benchmarks for LLMs due to their diversity and the risk of misuse or misinterpretation.

Method: Proposes BenchmarkCards, a documentation framework standardizing benchmark attributes like objectives, methodologies, and limitations, validated through user studies.

Result: BenchmarkCards simplifies benchmark selection and improves transparency, aiding informed decision-making in LLM evaluation.

Conclusion: BenchmarkCards effectively addresses the complexity of benchmark selection for LLMs, enhancing usability and clarity.

Abstract: Large language models (LLMs) are powerful tools capable of handling diverse
tasks. Comparing and selecting appropriate LLMs for specific tasks requires
systematic evaluation methods, as models exhibit varying capabilities across
different domains. However, finding suitable benchmarks is difficult given the
many available options. This complexity not only increases the risk of
benchmark misuse and misinterpretation but also demands substantial effort from
LLM users, seeking the most suitable benchmarks for their specific needs. To
address these issues, we introduce \texttt{BenchmarkCards}, an intuitive and
validated documentation framework that standardizes critical benchmark
attributes such as objectives, methodologies, data sources, and limitations.
Through user studies involving benchmark creators and users, we show that
\texttt{BenchmarkCards} can simplify benchmark selection and enhance
transparency, facilitating informed decision-making in evaluating LLMs. Data &
Code: https://github.com/SokolAnn/BenchmarkCards

</details>


### [129] [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/pdf/2410.16502)
*Jason Chan, Robert Gaizauskas, Zhixue Zhao*

Main category: cs.CL

TL;DR: The paper introduces RULEBREAKERS, a dataset to evaluate LLMs' ability to handle rulebreaker scenarios, finding most models, including GPT-4o, perform poorly due to over-rigid logic application and poor world knowledge utilization.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' human-like reasoning in rulebreaker scenarios, where formal logic often fails to align with human common sense.

Method: Creation of the RULEBREAKERS dataset and evaluation of seven LLMs, including GPT-4o, to assess their performance in recognizing and responding to rulebreakers.

Result: Most LLMs achieve mediocre accuracy, over-applying logical rules and underutilizing world knowledge, diverging from human-like reasoning.

Conclusion: Current LLMs struggle with rulebreaker scenarios, highlighting risks in relying solely on formal logic for improving reasoning, as it may widen the gap between AI and human-like reasoning.

Abstract: Formal logic enables computers to reason in natural language by representing
sentences in symbolic forms and applying rules to derive conclusions. However,
in what our study characterizes as "rulebreaker" scenarios, this method can
lead to conclusions that are typically not inferred or accepted by humans given
their common sense and factual knowledge. Inspired by works in cognitive
science, we create RULEBREAKERS, the first dataset for rigorously evaluating
the ability of large language models (LLMs) to recognize and respond to
rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven
LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on
RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules
unlike what is expected from typical human reasoners. Further analysis suggests
that this apparent failure is potentially associated with the models' poor
utilization of their world knowledge and their attention distribution patterns.
Whilst revealing a limitation of current LLMs, our study also provides a timely
counterbalance to a growing body of recent works that propose methods relying
on formal logic to improve LLMs' general reasoning capabilities, highlighting
their risk of further increasing divergence between LLMs and human-like
reasoning.

</details>


### [130] [Reducing Tool Hallucination via Reliability Alignment](https://arxiv.org/pdf/2412.04141)
*Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu*

Main category: cs.CL

TL;DR: The paper addresses tool hallucinations in LLMs, categorizing them into selection and usage types, and introduces RelyToolBench and Relign to evaluate and mitigate these issues, improving reliability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Tool hallucinations in LLMs lead to errors, higher costs, and reduced reliability, necessitating systematic solutions.

Method: Defines tool hallucinations, introduces RelyToolBench for evaluation, and proposes Relign, a framework with indecisive actions to mitigate hallucinations.

Result: Relign reduces tool hallucinations, improves task reliability, and enhances efficiency in LLM tool interactions.

Conclusion: The proposed methods effectively address tool hallucinations, enhancing the reliability and efficiency of LLM tool use.

Abstract: Large Language Models (LLMs) have expanded their capabilities beyond language
generation to interact with external tools, enabling automation and real-world
applications. However, tool hallucinations, where models either select
inappropriate tools or misuse them, pose significant challenges, leading to
erroneous task execution, increased computational costs, and reduced system
reliability. To systematically address this issue, we define and categorize
tool hallucinations into two main types, tool selection hallucination and tool
usage hallucination. To evaluate and mitigate these issues, we introduce
RelyToolBench, which integrates specialized test cases and novel metrics to
assess hallucination-aware task success and efficiency. Finally, we propose
Relign, a reliability alignment framework that expands the tool-use action
space to include indecisive actions, allowing LLMs to defer tool use, seek
clarification, or adjust tool selection dynamically. Through extensive
experiments, we demonstrate that Relign significantly reduces tool
hallucinations, improves task reliability, and enhances the efficiency of LLM
tool interactions.

</details>


### [131] [C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation](https://arxiv.org/pdf/2412.04947)
*Yanyang Li, Tin Long Wong, Cheung To Hung, Jianqiao Zhao, Duo Zheng, Ka Wai Liu, Michael R. Lyu, Liwei Wang*

Main category: cs.CL

TL;DR: C$^2$LEVA is a bilingual benchmark for evaluating LLMs with contamination-free tasks and automated data renewal.


<details>
  <summary>Details</summary>
Motivation: Address concerns about data contamination in LLM evaluation by providing a trustworthy benchmark.

Method: Introduces C$^2$LEVA, featuring 22 tasks and a systematic contamination prevention strategy.

Result: Effectiveness demonstrated through large-scale evaluation of 15 models.

Conclusion: C$^2$LEVA offers a reliable and comprehensive solution for LLM evaluation.

Abstract: Recent advances in large language models (LLMs) have shown significant
promise, yet their evaluation raises concerns, particularly regarding data
contamination due to the lack of access to proprietary training data. To
address this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark
featuring systematic contamination prevention. C$^2$LEVA firstly offers a
holistic evaluation encompassing 22 tasks, each targeting a specific
application or ability of LLMs, and secondly a trustworthy assessment due to
our contamination-free tasks, ensured by a systematic contamination prevention
strategy that fully automates test data renewal and enforces data protection
during benchmark data release. Our large-scale evaluation of 15 open-source and
proprietary models demonstrates the effectiveness of C$^2$LEVA.

</details>


### [132] [EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation](https://arxiv.org/pdf/2412.12559)
*Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park*

Main category: cs.CL

TL;DR: EXIT is an extractive context compression framework for RAG in QA, improving accuracy and efficiency by classifying sentences while preserving context.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems face issues with irrelevant document ranking and high latency, while existing compression methods lack context-awareness or efficiency.

Method: EXIT classifies sentences from retrieved documents adaptively, preserving contextual dependencies for parallelizable extraction.

Result: EXIT outperforms existing methods in QA accuracy and reduces inference time and token count.

Conclusion: EXIT offers a scalable, high-quality solution for RAG pipelines by balancing effectiveness and efficiency.

Abstract: We introduce EXIT, an extractive context compression framework that enhances
both the effectiveness and efficiency of retrieval-augmented generation (RAG)
in question answering (QA). Current RAG systems often struggle when retrieval
models fail to rank the most relevant documents, leading to the inclusion of
more context at the expense of latency and accuracy. While abstractive
compression methods can drastically reduce token counts, their token-by-token
generation process significantly increases end-to-end latency. Conversely,
existing extractive methods reduce latency but rely on independent,
non-adaptive sentence selection, failing to fully utilize contextual
information. EXIT addresses these limitations by classifying sentences from
retrieved documents - while preserving their contextual dependencies - enabling
parallelizable, context-aware extraction that adapts to query complexity and
retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks
show that EXIT consistently surpasses existing compression methods and even
uncompressed baselines in QA accuracy, while also delivering substantial
reductions in inference time and token count. By improving both effectiveness
and efficiency, EXIT provides a promising direction for developing scalable,
high-quality QA solutions in RAG pipelines. Our code is available at
https://github.com/ThisIsHwang/EXIT

</details>


### [133] [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/pdf/2412.12567)
*Seunghee Kim, Changhyeon Kim, Taeuk Kim*

Main category: cs.CL

TL;DR: The paper introduces FCMR, a benchmark for evaluating multimodal large language models (MLLMs) on multi-hop reasoning across financial data, revealing their limitations even for top models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMQA are flawed due to data contamination and lack of complex queries, necessitating a new benchmark to assess MLLMs' cross-modal reasoning.

Method: FCMR is designed with three difficulty levels (Easy, Medium, Hard) to evaluate MLLMs' ability to integrate information from text, tables, and charts in finance.

Result: State-of-the-art MLLMs perform poorly, with the best model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on Hard-level tasks.

Conclusion: FCMR highlights MLLMs' struggles with multi-hop reasoning, identifying retrieval as a key bottleneck, and calls for improved models.

Abstract: Real-world decision-making often requires integrating and reasoning over
information from multiple modalities. While recent multimodal large language
models (MLLMs) have shown promise in such tasks, their ability to perform
multi-hop reasoning across diverse sources remains insufficiently evaluated.
Existing benchmarks, such as MMQA, face challenges due to (1) data
contamination and (2) a lack of complex queries that necessitate operations
across more than two modalities, hindering accurate performance assessment. To
address this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a
benchmark created to analyze the reasoning capabilities of MLLMs by urging them
to combine information from textual reports, tables, and charts within the
financial domain. FCMR is categorized into three difficulty levels-Easy,
Medium, and Hard-facilitating a step-by-step evaluation. In particular,
problems at the Hard level require precise cross-modal three-hop reasoning and
are designed to prevent the disregard of any modality. Experiments on this new
benchmark reveal that even state-of-the-art MLLMs struggle, with the
best-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the
most challenging tier. We also conduct analysis to provide insights into the
inner workings of the models, including the discovery of a critical bottleneck
in the information retrieval phase.

</details>


### [134] [AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/pdf/2412.13670)
*Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, William Yang Wang*

Main category: cs.CL

TL;DR: AntiLeak-Bench is an automated framework to prevent data contamination in LLM evaluation by using new knowledge and eliminating human labor.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to ensure contamination-free evaluation due to pre-existing knowledge in new data and reliance on human labor.

Method: Constructs samples with new knowledge absent from LLMs' training sets and automates benchmark updates.

Result: Demonstrates data contamination likely exists before LLMs' cutoff time and shows AntiLeak-Bench effectively addresses this.

Conclusion: AntiLeak-Bench ensures contamination-free evaluation and reduces benchmark maintenance costs.

Abstract: Data contamination hinders fair LLM evaluation by introducing test data into
newer models' training sets. Existing studies solve this challenge by updating
benchmarks with newly collected data. However, they fail to guarantee
contamination-free evaluation as the newly collected data may contain
pre-existing knowledge, and their benchmark updates rely on intensive human
labor. To address these issues, we in this paper propose AntiLeak-Bench, an
automated anti-leakage benchmarking framework. Instead of simply using newly
collected data, we construct samples with explicitly new knowledge absent from
LLMs' training sets, which thus ensures strictly contamination-free evaluation.
We further design a fully automated workflow to build and update our benchmark
without human labor. This significantly reduces the cost of benchmark
maintenance to accommodate emerging LLMs. Through extensive experiments, we
highlight that data contamination likely exists before LLMs' cutoff time and
demonstrate AntiLeak-Bench effectively overcomes this challenge.

</details>


### [135] [SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation](https://arxiv.org/pdf/2412.15272)
*Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, Weiguo Zheng*

Main category: cs.CL

TL;DR: SimGRAG enhances KG-driven RAG by aligning queries with KG structures via a two-stage process, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address hallucinations in LLMs by leveraging KGs for more accurate retrieval-augmented generation.

Method: Two-stage process: query-to-pattern (LLM transforms queries into graph patterns) and pattern-to-subgraph (GSD metric aligns patterns with subgraphs). Includes an optimized retrieval algorithm.

Result: Outperforms state-of-the-art KG-driven RAG methods in QA and fact verification, with efficient retrieval on large-scale KGs.

Conclusion: SimGRAG effectively improves KG-driven RAG by better aligning queries and KG structures, demonstrating superior performance.

Abstract: Recent advancements in large language models (LLMs) have shown impressive
versatility across various tasks. To eliminate their hallucinations,
retrieval-augmented generation (RAG) has emerged as a powerful approach,
leveraging external knowledge sources like knowledge graphs (KGs). In this
paper, we study the task of KG-driven RAG and propose a novel Similar Graph
Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively
addresses the challenge of aligning query texts and KG structures through a
two-stage process: (1) query-to-pattern, which uses an LLM to transform queries
into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the
alignment between the pattern and candidate subgraphs using a graph semantic
distance (GSD) metric. We also develop an optimized retrieval algorithm that
efficiently identifies the top-k subgraphs within 1-second on a
10-million-scale KG. Extensive experiments show that SimGRAG outperforms
state-of-the-art KG-driven RAG methods in both question answering and fact
verification. Our code is available at https://github.com/YZ-Cai/SimGRAG.

</details>


### [136] [Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](https://arxiv.org/pdf/2412.16359)
*Nilanjana Das, Edward Raff, Aman Chadha, Manas Gaur*

Main category: cs.CL

TL;DR: The paper uncovers a new security vulnerability in LLMs on social media, where human-readable, situation-driven adversarial prompts evade detection and exploit models to generate harmful content. It proposes methods to create such attacks and suggests improvements for safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To assess and mitigate the risks of LLMs being exploited by sophisticated adversarial attacks that use natural-looking prompts, which are harder to detect than traditional nonsensical attacks.

Method: Developed attacks using movie scripts as contextual frameworks, transformed gibberish adversarial text into readable content, and enhanced the AdvPrompter framework with p-nucleus sampling for diverse adversarial texts.

Result: Demonstrated that LLMs like GPT-3.5-Turbo-0125 and Gemma-7b can be tricked into generating harmful content using seemingly normal prompts with hidden adversarial elements.

Conclusion: The findings highlight the need for more robust safety mechanisms to counter sophisticated adversarial attacks in real-world LLM applications.

Abstract: As the AI systems become deeply embedded in social media platforms, we've
uncovered a concerning security vulnerability that goes beyond traditional
adversarial attacks. It becomes important to assess the risks of LLMs before
the general public use them on social media platforms to avoid any adverse
impacts. Unlike obvious nonsensical text strings that safety systems can easily
catch, our work reveals that human-readable situation-driven adversarial
full-prompts that leverage situational context are effective but much harder to
detect. We found that skilled attackers can exploit the vulnerabilities in
open-source and proprietary LLMs to make a malicious user query safe for LLMs,
resulting in generating a harmful response. This raises an important question
about the vulnerabilities of LLMs. To measure the robustness against
human-readable attacks, which now present a potent threat, our research makes
three major contributions. First, we developed attacks that use movie scripts
as situational contextual frameworks, creating natural-looking full-prompts
that trick LLMs into generating harmful content. Second, we developed a method
to transform gibberish adversarial text into readable, innocuous content that
still exploits vulnerabilities when used within the full-prompts. Finally, we
enhanced the AdvPrompter framework with p-nucleus sampling to generate diverse
human-readable adversarial texts that significantly improve attack
effectiveness against models like GPT-3.5-Turbo-0125 and Gemma-7b. Our findings
show that these systems can be manipulated to operate beyond their intended
ethical boundaries when presented with seemingly normal prompts that contain
hidden adversarial elements. By identifying these vulnerabilities, we aim to
drive the development of more robust safety mechanisms that can withstand
sophisticated attacks in real-world applications.

</details>


### [137] [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/pdf/2412.16555)
*Yanxu Mao, Peipei Liu, Tiehan Cui, Zhaoteng Yan, Congying Liu, Datao You*

Main category: cs.CL

TL;DR: The paper proposes JMLLM, a multimodal jailbreaking method for LLMs, addressing limitations of existing methods by integrating text, visual, and auditory attacks. It introduces the TriJail dataset and shows improved success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreaking methods for LLMs have limitations like high query counts, low success rates, and narrow modality coverage, prompting the need for a more comprehensive approach.

Method: JMLLM integrates multimodal strategies (text, visual, auditory) for jailbreaking and uses the TriJail dataset for evaluation across 13 LLMs.

Result: Experiments show JMLLM achieves higher attack success rates and reduces time overhead compared to existing methods.

Conclusion: JMLLM offers a robust, efficient solution for multimodal jailbreaking, validated by the TriJail dataset and outperforming benchmarks.

Abstract: Large language models (LLMs) are widely applied in various fields of society
due to their powerful reasoning, understanding, and generation capabilities.
However, the security issues associated with these models are becoming
increasingly severe. Jailbreaking attacks, as an important method for detecting
vulnerabilities in LLMs, have been explored by researchers who attempt to
induce these models to generate harmful content through various attack methods.
Nevertheless, existing jailbreaking methods face numerous limitations, such as
excessive query counts, limited coverage of jailbreak modalities, low attack
success rates, and simplistic evaluation methods. To overcome these
constraints, this paper proposes a multimodal jailbreaking method: JMLLM. This
method integrates multiple strategies to perform comprehensive jailbreak
attacks across text, visual, and auditory modalities. Additionally, we
contribute a new and comprehensive dataset for multimodal jailbreaking
research: TriJail, which includes jailbreak prompts for all three modalities.
Experiments on the TriJail dataset and the benchmark dataset AdvBench,
conducted on 13 popular LLMs, demonstrate advanced attack success rates and
significant reduction in time overhead.

</details>


### [138] [A Reality Check on Context Utilisation for Retrieval-Augmented Generation](https://arxiv.org/pdf/2412.17031)
*Lovisa Hagström, Sara Vera Marjanović, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, Isabelle Augenstein*

Main category: cs.CL

TL;DR: DRUID dataset introduces real-world annotated contexts for claim verification, highlighting limitations of synthetic datasets in representing complexity and diversity of retrieved information.


<details>
  <summary>Details</summary>
Motivation: To address the gap in real-world context utilization studies in retrieval-augmented generation (RAG) and improve performance in practical settings.

Method: Creation of DRUID dataset with manually annotated real-world queries and contexts, comparing it to synthetic datasets (CounterFact, ConflictQA) using a novel ACU score.

Result: Synthetic datasets exaggerate rare context characteristics, inflating context utilization results, while DRUID reveals smaller correlations between singleton context properties and ACU.

Conclusion: Real-world aligned studies are essential for accurate representation and improvement of RAG performance.

Abstract: Retrieval-augmented generation (RAG) helps address the limitations of
parametric knowledge embedded within a language model (LM). In real world
settings, retrieved information can vary in complexity, yet most investigations
of LM utilisation of context has been limited to synthetic text. We introduce
DRUID (Dataset of Retrieved Unreliable, Insufficient and
Difficult-to-understand contexts) with real-world queries and contexts manually
annotated for stance. The dataset is based on the prototypical task of
automated claim verification, for which automated retrieval of real-world
evidence is crucial. We compare DRUID to synthetic datasets (CounterFact,
ConflictQA) and find that artificial datasets often fail to represent the
complexity and diversity of realistically retrieved context. We show that
synthetic datasets exaggerate context characteristics rare in real retrieved
data, which leads to inflated context utilisation results, as measured by our
novel ACU score. Moreover, while previous work has mainly focused on singleton
context characteristics to explain context utilisation, correlations between
singleton context properties and ACU on DRUID are surprisingly small compared
to other properties related to context source. Overall, our work underscores
the need for real-world aligned context utilisation studies to represent and
improve performance in real-world RAG settings.

</details>


### [139] [Tensor Product Attention Is All You Need](https://arxiv.org/pdf/2501.06425)
*Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew C Yao*

Main category: cs.CL

TL;DR: Proposes Tensor Product Attention (TPA) to reduce KV cache memory overhead in language models, introducing T6 architecture for efficient long-sequence processing.


<details>
  <summary>Details</summary>
Motivation: Addressing memory inefficiency in scaling language models for longer sequences by reducing KV cache size.

Method: Uses tensor decompositions for compact query, key, and value representations, integrating with Rotary Position Embedding (RoPE).

Result: T6 matches or outperforms standard Transformer baselines in performance and efficiency.

Conclusion: TPA and T6 offer scalable, memory-efficient solutions for long-sequence language modeling.

Abstract: Scaling language models to handle longer input sequences typically
necessitates large key-value (KV) caches, resulting in substantial memory
overhead during inference. In this paper, we propose Tensor Product Attention
(TPA), a novel attention mechanism that uses tensor decompositions to represent
queries, keys, and values compactly, substantially shrinking the KV cache size
at inference time. By factorizing these representations into contextual
low-rank components and seamlessly integrating with Rotary Position Embedding
(RoPE), TPA achieves improved model quality alongside memory efficiency. Based
on TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model
architecture for sequence modeling. Through extensive empirical evaluation on
language modeling tasks, we demonstrate that T6 surpasses or matches the
performance of standard Transformer baselines, including Multi-Head Attention
(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and
Multi-Head Latent Attention (MLA) across various metrics, including perplexity
and a range of established evaluation benchmarks. Notably, TPA's memory
efficiency and computational efficiency at the decoding stage enable processing
longer sequences under fixed resource constraints, addressing a critical
scalability challenge in modern language models. The code is available at
https://github.com/tensorgi/T6.

</details>


### [140] [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/pdf/2501.08319)
*Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva*

Main category: cs.CL

TL;DR: The paper critiques current automated interpretability pipelines for LLMs, proposing output-centric methods to improve feature descriptions by capturing causal effects on outputs. Combining input and output-centric approaches yields the best results.


<details>
  <summary>Details</summary>
Motivation: Current pipelines for generating feature descriptions in LLMs focus on input activation but fail to account for the causal impact of features on outputs, limiting interpretability.

Method: The authors introduce output-centric methods, such as analyzing tokens weighted higher after feature stimulation or applying the vocabulary 'unembedding' head directly to features.

Result: Output-centric descriptions better capture causal effects on outputs than input-centric ones, with combined methods performing best. They also help identify inputs for 'dead' features.

Conclusion: Output-centric approaches enhance interpretability by addressing causal effects, and combining them with input-centric methods optimizes performance.

Abstract: Automated interpretability pipelines generate natural language descriptions
for the concepts represented by features in large language models (LLMs), such
as plants or the first word in a sentence. These descriptions are derived using
inputs that activate the feature, which may be a dimension or a direction in
the model's representation space. However, identifying activating inputs is
costly, and the mechanistic role of a feature in model behavior is determined
both by how inputs cause a feature to activate and by how feature activation
affects outputs. Using steering evaluations, we reveal that current pipelines
provide descriptions that fail to capture the causal effect of the feature on
outputs. To fix this, we propose efficient, output-centric methods for
automatically generating feature descriptions. These methods use the tokens
weighted higher after feature stimulation or the highest weight tokens after
applying the vocabulary "unembedding" head directly to the feature. Our
output-centric descriptions better capture the causal effect of a feature on
model outputs than input-centric descriptions, but combining the two leads to
the best performance on both input and output evaluations. Lastly, we show that
output-centric descriptions can be used to find inputs that activate features
previously thought to be "dead".

</details>


### [141] [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/pdf/2501.13977)
*Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra*

Main category: cs.CL

TL;DR: A novel LLM-based re-ranking method reduces harmful content exposure on social media without needing extensive labeled data, outperforming current moderation approaches.


<details>
  <summary>Details</summary>
Motivation: Current ML/AI moderation struggles with scalability and adapting to new harms, necessitating a more adaptable solution.

Method: Proposes a zero-shot and few-shot LLM-based re-ranking approach to dynamically assess and re-rank content sequences.

Result: Outperforms proprietary moderation methods in experiments across datasets, models, and configurations.

Conclusion: Offers a scalable, adaptable solution for mitigating harmful content exposure on social media.

Abstract: Social media platforms utilize Machine Learning (ML) and Artificial
Intelligence (AI) powered recommendation algorithms to maximize user
engagement, which can result in inadvertent exposure to harmful content.
Current moderation efforts, reliant on classifiers trained with extensive
human-annotated data, struggle with scalability and adapting to new forms of
harm. To address these challenges, we propose a novel re-ranking approach using
Large Language Models (LLMs) in zero-shot and few-shot settings. Our method
dynamically assesses and re-ranks content sequences, effectively mitigating
harmful content exposure without requiring extensive labeled data. Alongside
traditional ranking metrics, we also introduce two new metrics to evaluate the
effectiveness of re-ranking in reducing exposure to harmful content. Through
experiments on three datasets, three models and across three configurations, we
demonstrate that our LLM-based approach significantly outperforms existing
proprietary moderation approaches, offering a scalable and adaptable solution
for harm mitigation.

</details>


### [142] [Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation](https://arxiv.org/pdf/2501.13978)
*Sangyeop Yeo, Seung-won Hwang, Yu-Seung Ma*

Main category: cs.CL

TL;DR: CGO (Chain of Grounded Objectives) improves LLM-based code generation by embedding functional objectives into prompts, avoiding rigid sequential reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing sequential reasoning methods for code generation lack flexibility and don't align well with programming languages' structured nature.

Method: CGO embeds functional objectives into input prompts, avoiding explicit sequential steps, to better suit programming tasks.

Result: Empirical evaluations show CGO enhances code generation, overcoming limitations of current approaches.

Conclusion: CGO offers a more adaptable and effective method for improving LLM-based code generation by leveraging structured objectives.

Abstract: The use of Large Language Models (LLMs) for code generation has gained
significant attention in recent years. Existing methods often aim to improve
the quality of generated code by incorporating additional contextual
information or guidance into input prompts. Many of these approaches adopt
sequential reasoning strategies, mimicking human-like step-by-step thinking.
However, such strategies may constrain flexibility, as they do not always align
with the structured characteristics of programming languages. This paper
introduces the Chain of Grounded Objectives (CGO), a method that embeds
functional objectives into input prompts to enhance code generation. By
leveraging appropriately structured objectives as input and avoiding explicit
sequential procedures, CGO adapts effectively to the structured nature of
programming tasks. Empirical evaluations demonstrate that CGO effectively
enhances code generation, addressing limitations of existing approaches.

</details>


### [143] [Decomposed Opinion Summarization with Verified Aspect-Aware Modules](https://arxiv.org/pdf/2501.17191)
*Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata*

Main category: cs.CL

TL;DR: A modular, domain-agnostic approach for opinion summarization improves transparency and grounding by separating aspect identification, opinion consolidation, and meta-review synthesis.


<details>
  <summary>Details</summary>
Motivation: To enhance explainability and grounding in opinion summarization from large-scale reviews.

Method: Proposes a modular approach with distinct tasks: aspect identification, opinion consolidation, and meta-review synthesis. Evaluated across multiple domains.

Result: Generates more grounded summaries than baselines, with informative intermediate outputs. Supports human summarization.

Conclusion: The modular approach improves transparency and utility in opinion summarization, validated by empirical results.

Abstract: Opinion summarization plays a key role in deriving meaningful insights from
large-scale online reviews. To make the process more explainable and grounded,
we propose a domain-agnostic modular approach guided by review aspects (e.g.,
cleanliness for hotel reviews) which separates the tasks of aspect
identification, opinion consolidation, and meta-review synthesis to enable
greater transparency and ease of inspection. We conduct extensive experiments
across datasets representing scientific research, business, and product
domains. Results show that our approach generates more grounded summaries
compared to strong baseline models, as verified through automated and human
evaluations. Additionally, our modular approach, which incorporates reasoning
based on review aspects, produces more informative intermediate outputs than
other knowledge-agnostic decomposition approaches. Lastly, we provide empirical
results to show that these intermediate outputs can support humans in
summarizing opinions from large volumes of reviews.

</details>


### [144] [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/pdf/2501.18922)
*Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan*

Main category: cs.CL

TL;DR: KBQA-o1, a novel KBQA method using MCTS and ReAct-based agents, improves performance with limited annotated data, achieving 78.5% F1 on GrailQA.


<details>
  <summary>Details</summary>
Motivation: Address challenges in KBQA like weak KB awareness, efficiency-effectiveness imbalance, and high reliance on annotated data.

Method: Combines ReAct-based agent process for logical form generation with MCTS for heuristic search, balancing exploration and performance.

Result: Outperforms previous methods, achieving 78.5% F1 on GrailQA with Llama-3.1-8B, vs. 48.5% with GPT-3.5-turbo.

Conclusion: KBQA-o1 is effective for low-resource KBQA, leveraging agentic exploration and MCTS for improved performance.

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with a large-scale structured knowledge base (KB). Despite
advancements with large language models (LLMs), KBQA still faces challenges in
weak KB awareness, imbalance between effectiveness and efficiency, and high
reliance on annotated data. To address these challenges, we propose KBQA-o1, a
novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a
ReAct-based agent process for stepwise logical form generation with KB
environment exploration. Moreover, it employs MCTS, a heuristic search method
driven by policy and reward models, to balance agentic exploration's
performance and search space. With heuristic exploration, KBQA-o1 generates
high-quality annotations for further improvement by incremental fine-tuning.
Experimental results show that KBQA-o1 outperforms previous low-resource KBQA
methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1
performance to 78.5% compared to 48.5% of the previous sota method with
GPT-3.5-turbo. Our code is publicly available.

</details>


### [145] [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/pdf/2502.01179)
*Wen Lai, Alexander Fraser, Ivan Titov*

Main category: cs.CL

TL;DR: JoLA improves PEFT methods by jointly learning which Transformer heads to edit, the type of intervention, and the intervention parameters, outperforming existing methods in low-resource scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard PEFT methods like LoRA are limited in low-resource settings, and activation editing techniques lack stability and module identification accuracy.

Method: JoLA jointly learns which Transformer heads to edit, the intervention type (additive, multiplicative, or both), and the intervention parameters.

Result: JoLA consistently outperforms existing methods across three benchmarks: commonsense reasoning, natural language understanding, and natural language generation.

Conclusion: JoLA is a promising method for low-resource fine-tuning, combining localization and activation editing for improved performance and stability.

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly
used to adapt LLMs. However, the effectiveness of standard PEFT methods is
limited in low-resource scenarios with only a few hundred examples. Recent
advances in interpretability research have inspired the emergence of activation
editing (or steering) techniques, which modify the activations of specific
model components. Due to their extremely small parameter counts, these methods
show promise for small datasets. However, their performance is highly dependent
on identifying the correct modules to edit and often lacks stability across
different datasets. In this paper, we propose Joint Localization and Activation
Editing (JoLA), a method that jointly learns (1) which heads in the Transformer
to edit (2) whether the intervention should be additive, multiplicative, or
both and (3) the intervention parameters themselves - the vectors applied as
additive offsets or multiplicative scalings to the head output. Through
evaluations on three benchmarks spanning commonsense reasoning, natural
language understanding, and natural language generation, we demonstrate that
JoLA consistently outperforms existing methods. The code for the method is
released at https://github.com/wenlai-lavine/jola.

</details>


### [146] [Fast Large Language Model Collaborative Decoding via Speculation](https://arxiv.org/pdf/2502.01662)
*Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang*

Main category: cs.CL

TL;DR: CoS accelerates collaborative decoding for LLMs without performance loss, achieving 1.11x-2.23x speedup by leveraging speculative decoding and alternating model roles.


<details>
  <summary>Details</summary>
Motivation: Standard collaborative decoding improves output quality but is computationally expensive. CoS aims to reduce costs while maintaining performance.

Method: CoS uses speculative decoding with alternating proposer-verifier roles among models, combining their distributions for verification.

Result: CoS is 1.11x-2.23x faster than standard collaborative decoding without quality loss.

Conclusion: CoS offers a scalable, efficient alternative to traditional collaborative decoding, with proven speed advantages.

Abstract: Large Language Model (LLM) collaborative decoding techniques improve output
quality by combining the outputs of multiple models at each generation step,
but they incur high computational costs. In this paper, we introduce
Collaborative decoding via Speculation (CoS), a novel framework that
accelerates collaborative decoding without compromising performance. Inspired
by Speculative Decoding--where a small proposal model generates tokens
sequentially, and a larger target model verifies them in parallel, our approach
builds on two key insights: (1) the verification distribution can be the
combined distribution of both the proposal and target models, and (2)
alternating each model as the proposer and verifier can further enhance
efficiency. We generalize this method to collaboration among n models and
theoretically prove that CoS is never slower than standard collaborative
decoding, typically achieving faster speed. Extensive experiments demonstrate
CoS is 1.11x-2.23x faster than standard collaborative decoding without
compromising generation quality. Our code is available at
https://github.com/Kamichanw/CoS/.

</details>


### [147] [SPRI: Aligning Large Language Models with Context-Situated Principles](https://arxiv.org/pdf/2502.03397)
*Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin*

Main category: cs.CL

TL;DR: SPRI is a framework for real-time, automatic generation of guiding principles for LLMs, reducing reliance on human expertise and improving task-specific alignment.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human values is resource-intensive; existing methods use generic principles, which lack adaptability.

Method: SPRI generates context-specific principles for each input query, minimizing human effort.

Result: SPRI matches expert-crafted principles, outperforms prior LLM-as-a-judge frameworks, and improves truthfulness in synthetic data.

Conclusion: SPRI offers a scalable, efficient solution for aligning LLMs with human values without heavy human involvement.

Abstract: Aligning Large Language Models to integrate and reflect human values,
especially for tasks that demand intricate human oversight, is arduous since it
is resource-intensive and time-consuming to depend on human expertise for
context-specific guidance. Prior work has utilized predefined sets of rules or
principles to steer the behavior of models (Bai et al., 2022; Sun et al.,
2023). However, these principles tend to be generic, making it challenging to
adapt them to each individual input query or context. In this work, we present
Situated-PRInciples (SPRI), a framework requiring minimal or no human effort
that is designed to automatically generate guiding principles in real-time for
each input query and utilize them to align each response. We evaluate SPRI on
three tasks, and show that 1) SPRI can derive principles in a complex
domain-specific task that leads to on-par performance as expert-crafted ones;
2) SPRI-generated principles lead to instance-specific rubrics that outperform
prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data
leads to substantial improvement on truthfulness. We release our code and model
generations at https://github.com/honglizhan/SPRI-public.

</details>


### [148] [Toward universal steering and monitoring of AI models](https://arxiv.org/pdf/2502.03708)
*Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin*

Main category: cs.CL

TL;DR: The paper introduces a scalable method to extract linear representations of concepts in AI models, enabling model steering to improve capabilities and safety.


<details>
  <summary>Details</summary>
Motivation: Understanding AI models' internal knowledge representation to enhance capabilities and develop safeguards.

Method: Develops an approach for extracting linear concept representations in large-scale AI models (language, vision-language, reasoning).

Result: Newer, larger models are more steerable; concept representations improve monitoring of misaligned content and model performance.

Conclusion: Internal representations are powerful for mapping AI knowledge, advancing safety, and improving capabilities.

Abstract: Modern AI models contain much of human knowledge, yet understanding of their
internal representation of this knowledge remains elusive. Characterizing the
structure and properties of this representation will lead to improvements in
model capabilities and development of effective safeguards. Building on recent
advances in feature learning, we develop an effective, scalable approach for
extracting linear representations of general concepts in large-scale AI models
(language models, vision-language models, and reasoning models). We show how
these representations enable model steering, through which we expose
vulnerabilities, mitigate misaligned behaviors, and improve model capabilities.
Additionally, we demonstrate that concept representations are remarkably
transferable across human languages and combinable to enable multi-concept
steering. Through quantitative analysis across hundreds of concepts, we find
that newer, larger models are more steerable and steering can improve model
capabilities beyond standard prompting. We show how concept representations are
effective for monitoring misaligned content (hallucinations, toxic content). We
demonstrate that predictive models built using concept representations are more
accurate for monitoring misaligned content than using models that judge outputs
directly. Together, our results illustrate the power of using internal
representations to map the knowledge in AI models, advance AI safety, and
improve model capabilities.

</details>


### [149] [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/pdf/2502.04350)
*Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, Chuchu Fan*

Main category: cs.CL

TL;DR: CodeSteer improves LLM performance in code/text generation by fine-tuning with multi-turn SFT and DPO, achieving significant gains over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize symbolic computing in LLMs, failing to balance textual reasoning and code generation effectively.

Method: Introduces CodeSteer, fine-tuning Llama-3-8B with multi-turn SFT and DPO, and augmenting it with symbolic and self-answer checkers.

Result: CodeSteer boosts GPT-4o's performance from 53.3 to 86.4, outperforming top models like OpenAI o1 and DeepSeek R1.

Conclusion: CodeSteer enhances LLMs' symbolic computing capabilities, generalizing well across models and tasks.

Abstract: Existing methods fail to effectively steer Large Language Models (LLMs)
between textual reasoning and code generation, leaving symbolic computing
capabilities underutilized. We introduce CodeSteer, an effective method for
guiding LLM code/text generation. We construct a comprehensive benchmark
SymBench comprising 37 symbolic tasks with adjustable complexity and also
synthesize datasets of 12k multi-turn guidance/generation trajectories and 5.5k
guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly
designed multi-turn supervised fine-tuning (SFT) and direct preference
optimization (DPO). The resulting model, CodeSteerLLM, augmented with the
proposed symbolic and self-answer checkers, effectively guides the code/text
generation of larger models. Augmenting GPT-4o with CodeSteer raises its
average performance score from 53.3 to 86.4, even outperforming the existing
best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all
37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates
superior generalizability, providing an average 41.8 performance boost on
Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic
computing to maintain strong performance on highly complex tasks. Models,
Datasets, and Codes are available at
https://github.com/yongchao98/CodeSteer-v1.0 and
https://huggingface.co/yongchao98.

</details>


### [150] [Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives](https://arxiv.org/pdf/2502.04358)
*Elliot Meyerson, Xin Qiu*

Main category: cs.CL

TL;DR: The paper advocates for using asymptotic analysis with LLM primitives to optimize the decomposition of problems into LLM-based agents, treating the LLM forward pass as the atomic cost unit.


<details>
  <summary>Details</summary>
Motivation: Current decompositions of problems into LLM agents are intuitive but may not be optimal. Understanding efficiency requires analyzing LLM primitives.

Method: Proposes asymptotic analysis with LLM primitives, treating the forward pass as the computational cost unit.

Result: Insights from this analysis can improve the efficiency and scalability of LLM-based systems.

Conclusion: Asymptotic analysis is key to developing optimal decompositions for scaling LLM deployments.

Abstract: Decomposing hard problems into subproblems often makes them easier and more
efficient to solve. With large language models (LLMs) crossing critical
reliability thresholds for a growing slate of capabilities, there is an
increasing effort to decompose systems into sets of LLM-based agents, each of
whom can be delegated sub-tasks. However, this decomposition (even when
automated) is often intuitive, e.g., based on how a human might assign roles to
members of a human team. How close are these role decompositions to optimal?
This position paper argues that asymptotic analysis with LLM primitives is
needed to reason about the efficiency of such decomposed systems, and that
insights from such analysis will unlock opportunities for scaling them. By
treating the LLM forward pass as the atomic unit of computational cost, one can
separate out the (often opaque) inner workings of a particular LLM from the
inherent efficiency of how a set of LLMs are orchestrated to solve hard
problems. In other words, if we want to scale the deployment of LLMs to the
limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM
primitives should be used to reason about and develop more powerful
decompositions of large problems into LLM agents.

</details>


### [151] [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/pdf/2502.04964)
*Roman Vashurin, Maiya Goloburda, Albina Ilina, Aleksandr Rubashevskii, Preslav Nakov, Artem Shelmanov, Maxim Panov*

Main category: cs.CL

TL;DR: The paper proposes a novel approach to uncertainty quantification (UQ) in Large Language Models (LLMs) by integrating model confidence and output consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods for LLMs, combining information-based and consistency-based approaches, often underperform simpler baselines. The study aims to improve UQ by linking uncertainty to minimum Bayes risks in LLM decoding.

Method: The authors develop a framework connecting uncertainty measures to minimum Bayes risks, then integrate model confidence with output consistency to create efficient UQ methods.

Result: The proposed methods show significant improvements over state-of-the-art UQ approaches in tasks like question answering, summarization, and translation.

Conclusion: The study highlights LLMs' probabilistic traits affecting UQ performance and introduces a robust synthesis of confidence and consistency for better uncertainty estimation.

Abstract: Uncertainty quantification (UQ) methods for Large Language Models (LLMs)
encompass a variety of approaches, with two major types being particularly
prominent: information-based, which focus on model confidence expressed as
token probabilities, and consistency-based, which assess the semantic
relationship between multiple outputs generated using repeated sampling.
Several recent methods have combined these two approaches to boost UQ
performance. However, they sometimes fail to outperform much simpler baseline
methods. Our work discusses the fundamental approach to constructing
uncertainty measures that directly links uncertainty with the minimum Bayes
risks achieved by LLM decoding. Building on these findings, we propose a novel
approach to integrating model confidence with output consistency, resulting in
a family of efficient and robust UQ methods. Our investigation reveals
distinctive characteristics of LLMs as probabilistic models, which help to
explain why these UQ methods underperform in certain tasks. Based on these
findings, we propose a new way of synthesizing model confidence and output
consistency, leading to a family of efficient and robust UQ methods. We
evaluate our approach across various tasks such as question answering,
abstractive summarization, and machine translation, demonstrating sizable
improvements over state-of-the-art UQ approaches.

</details>


### [152] [Jailbreaking to Jailbreak](https://arxiv.org/pdf/2502.09638)
*Jeremy Kritz, Vaughn Robinson, Robert Vacareanu, Bijan Varjavand, Michael Choi, Bobby Gogov, Scale Red Team, Summer Yue, Willow E. Primack, Zifan Wang*

Main category: cs.CL

TL;DR: The paper introduces $J_2$ (jailbreaking-to-jailbreak) attackers, which can turn almost any black-box LLM into an effective jailbreaking tool, outperforming expert human red teamers and algorithm-based attacks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing jailbreaking methods, which rely on open-weight or uncensored models, and to explore the under-researched jailbreaking capabilities of strong LLMs.

Method: The $J_2$ framework transforms black-box LLMs into attackers using transferable prompts and self-jailbreaking strategies.

Result: $J_2$ attackers achieve high attack success rates (e.g., 0.975 against GPT-4o) and outperform state-of-the-art methods.

Conclusion: The study highlights the strong jailbreaking potential of LLMs and their rapid vulnerability development, urging further research into safeguards.

Abstract: Large Language Models (LLMs) can be used to red team other models (e.g.
jailbreaking) to elicit harmful contents. While prior works commonly employ
open-weight models or private uncensored models for doing jailbreaking, as the
refusal-training of strong LLMs (e.g. OpenAI o3) refuse to help jailbreaking,
our work turn (almost) any black-box LLMs into attackers. The resulting $J_2$
(jailbreaking-to-jailbreak) attackers can effectively jailbreak the safeguard
of target models using various strategies, both created by themselves or from
expert human red teamers. In doing so, we show their strong but
under-researched jailbreaking capabilities. Our experiments demonstrate that 1)
prompts used to create $J_2$ attackers transfer across almost all black-box
models; 2) an $J_2$ attacker can jailbreak a copy of itself, and this
vulnerability develops rapidly over the past 12 months; 3) reasong models, such
as Sonnet-3.7, are strong $J_2$ attackers compared to others. For example, when
used against the safeguard of GPT-4o, $J_2$ (Sonnet-3.7) achieves 0.975 attack
success rate (ASR), which matches expert human red teamers and surpasses the
state-of-the-art algorithm-based attacks. Among $J_2$ attackers, $J_2$ (o3)
achieves highest ASR (0.605) against Sonnet-3.5, one of the most robust models.

</details>


### [153] [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/pdf/2502.10852)
*Zeli Su, Ziyin Zhang, Guixian Xu, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong*

Main category: cs.CL

TL;DR: A framework adapts multilingual encoders for text generation in low-resource languages, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Address poor performance of multilingual models in low-resource languages and lack of support in modern LLMs.

Method: Reuse encoder-decoder weights to leverage semantic space for efficient learning.

Result: XLM-SWCM shows superior performance in Chinese minority languages.

Conclusion: The framework effectively enables text generation in low-resource languages.

Abstract: While multilingual language models like XLM-R have advanced multilingualism
in NLP, they still perform poorly in extremely low-resource languages. This
situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen
support far fewer languages than XLM-R, making text generation models
non-existent for many languages in the world. To tackle this challenge, we
propose a novel framework for adapting multilingual encoders to text generation
in extremely low-resource languages. By reusing the weights between the encoder
and the decoder, our framework allows the model to leverage the learned
semantic space of the encoder, enabling efficient learning and effective
generalization in low-resource languages. Applying this framework to four
Chinese minority languages, we present XLM-SWCM, and demonstrate its superior
performance on various downstream tasks even when compared with much larger
models.

</details>


### [154] [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/pdf/2502.11115)
*Tu Anh Dinh, Jan Niehues*

Main category: cs.CL

TL;DR: BoostedProb improves QE by boosting model confidence when multiple correct outputs exist, outperforming raw probabilities and matching costly methods.


<details>
  <summary>Details</summary>
Motivation: Output probabilities in text-generation models can be underconfident due to multiple correct options, making them unreliable for quality estimation.

Method: Proposes BoostedProb, a method to boost model confidence in cases with multiple viable outputs, without added complexity.

Result: Achieves +0.194 average improvement in Pearson correlation to ground-truth quality, performing comparably to costly QE methods.

Conclusion: BoostedProb is a simple yet effective QE approach, addressing underconfidence in model probabilities and matching advanced methods.

Abstract: Quality Estimation (QE) is estimating quality of the model output during
inference when the ground truth is not available. Deriving output quality from
the models' output probability is the most trivial and low-effort way. However,
we show that the output probability of text-generation models can appear
underconfident. At each output step, there can be multiple correct options,
making the probability distribution spread out more. Thus, lower probability
does not necessarily mean lower output quality. Due to this observation, we
propose a QE approach called BoostedProb, which boosts the model's confidence
in cases where there are multiple viable output options. With no increase in
complexity, BoostedProb is notably better than raw model probability in
different settings, achieving on average +0.194 improvement in Pearson
correlation to ground-truth quality. It also comes close to or outperforms more
costly approaches like supervised or ensemble-based QE in certain settings.

</details>


### [155] [Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](https://arxiv.org/pdf/2502.11501)
*Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang*

Main category: cs.CL

TL;DR: The paper critiques existing token pruning methods in MLLMs, questioning their effectiveness and evaluation, and provides insights for future designs.


<details>
  <summary>Details</summary>
Motivation: To address the underperformance and unanswered questions in token pruning methods for MLLMs, aiming to improve efficiency and reliability.

Method: Analyzes existing token pruning approaches, evaluates their design, and answers critical questions about their effectiveness.

Result: Provides insights into why current methods underperform and suggests improvements for future token pruning techniques.

Conclusion: The paper highlights gaps in current token pruning research and offers guidance for more effective and reliable methods.

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance
for cross-modal understanding and generation, yet still suffer from severe
inference costs. Recently, abundant works have been proposed to solve this
problem with token pruning, which identifies the redundant tokens in MLLMs and
then prunes them to reduce the computation and KV storage costs, leading to
significant acceleration without training. While these methods claim efficiency
gains, critical questions about their fundamental design and evaluation remain
unanswered: Why do many existing approaches underperform even compared to naive
random token selection? Are attention-based scoring sufficient for reliably
identifying redundant tokens? Is language information really helpful during
token pruning? What makes a good trade-off between token importance and
duplication? Are current evaluation protocols comprehensive and unbiased? The
ignorance of previous research on these problems hinders the long-term
development of token pruning. In this paper, we answer these questions one by
one, providing insights into the design of future token pruning methods.

</details>


### [156] [Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu](https://arxiv.org/pdf/2502.11862)
*Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper explores in-context machine translation (MT) using LLMs for low-resource languages, focusing on the impact of different linguistic resources like dictionaries, grammar books, and parallel examples. It finds dictionaries and parallel examples most helpful, with grammars offering little benefit. It also demonstrates synthetic parallel data generation as a solution for low-resource MT.


<details>
  <summary>Details</summary>
Motivation: To understand the relative importance of linguistic resources (dictionaries, grammar books, parallel examples) in in-context MT and address data scarcity in low-resource MT.

Method: Systematically evaluates the impact of each resource on translation performance using Manchu as a case study, including experiments with enciphered texts to isolate ICL effects.

Result: High-quality dictionaries and parallel examples significantly improve translation, while grammars provide minimal benefit. Synthetic parallel data generation via in-context MT aids low-resource MT systems.

Conclusion: In-context MT with LLMs is effective for low-resource languages, especially when leveraging dictionaries and parallel examples, and can bootstrap conventional MT models through data augmentation.

Abstract: In-context machine translation (MT) with large language models (LLMs) is a
promising approach for low-resource MT, as it can readily take advantage of
linguistic resources such as grammar books and dictionaries. Such resources are
usually selectively integrated into the prompt so that LLMs can directly
perform translation without any specific training, via their in-context
learning capability (ICL). However, the relative importance of each type of
resource, e.g., dictionary, grammar book, and retrieved parallel examples, is
not entirely clear. To address this gap, this study systematically investigates
how each resource and its quality affect the translation performance, with the
Manchu language as our case study. To remove any prior knowledge of Manchu
encoded in the LLM parameters and single out the effect of ICL, we also
experiment with an enciphered version of Manchu texts. Our results indicate
that high-quality dictionaries and good parallel examples are very helpful,
while grammars hardly help. In a follow-up study, we showcase a promising
application of in-context MT: parallel data augmentation as a way to bootstrap
a conventional MT model. When monolingual data abound, generating synthetic
parallel data through in-context MT offers a pathway to mitigate data scarcity
and build effective and efficient low-resource neural MT systems.

</details>


### [157] [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/pdf/2502.11926)
*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Alexander Panchenko, Andrew Piper, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad*

Main category: cs.CL

TL;DR: BRIGHTER introduces multi-labeled emotion-annotated datasets in 28 low-resource languages, addressing disparities in emotion recognition research.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in emotion recognition research for under-resourced languages lacking annotated datasets.

Method: Collection of datasets across 28 languages, annotated by fluent speakers, with experiments on monolingual/crosslingual emotion identification and intensity recognition.

Result: Varied performance across languages and domains, with LLMs showing potential. BRIGHTER datasets advance emotion recognition for low-resource languages.

Conclusion: BRIGHTER is a significant step toward equitable emotion recognition research, though challenges in data collection and annotation persist.

Abstract: People worldwide use language in subtle and complex ways to express emotions.
Although emotion recognition--an umbrella term for several NLP tasks--impacts
various applications within NLP and beyond, most work in this area has focused
on high-resource languages. This has led to significant disparities in research
efforts and proposed solutions, particularly for under-resourced languages,
which often lack high-quality annotated datasets. In this paper, we present
BRIGHTER--a collection of multi-labeled, emotion-annotated datasets in 28
different languages and across several domains. BRIGHTER primarily covers
low-resource languages from Africa, Asia, Eastern Europe, and Latin America,
with instances labeled by fluent speakers. We highlight the challenges related
to the data collection and annotation processes, and then report experimental
results for monolingual and crosslingual multi-label emotion identification, as
well as emotion intensity recognition. We analyse the variability in
performance across languages and text domains, both with and without the use of
LLMs, and show that the BRIGHTER datasets represent a meaningful step towards
addressing the gap in text-based emotion recognition.

</details>


### [158] [LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data](https://arxiv.org/pdf/2502.12583)
*Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Shengjie Ma, Aofan Liu, Hui Xiong, Jian Guo*

Main category: cs.CL

TL;DR: LongFaith is a pipeline for creating faithful long-context reasoning datasets, improving model performance by addressing misinformation and lack of attribution.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data approaches for long-context LLMs suffer from faithfulness issues, limiting their effectiveness in tasks like reasoning and QA.

Method: LongFaith integrates ground truth and citation-based reasoning prompts to enhance accuracy and reduce verification needs.

Result: Models fine-tuned on LongFaith datasets (LongFaith-SFT and LongFaith-PO) show significant performance improvements.

Conclusion: LongFaith is scalable and adaptable, offering broad applicability for developing long-context LLMs.

Abstract: Despite the growing development of long-context large language models (LLMs),
data-centric approaches relying on synthetic data have been hindered by issues
related to faithfulness, which limit their effectiveness in enhancing model
performance on tasks such as long-context reasoning and question answering
(QA). These challenges are often exacerbated by misinformation caused by lack
of verification, reasoning without attribution, and potential knowledge
conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful
long-context reasoning instruction datasets. By integrating ground truth and
citation-based reasoning prompts, we eliminate distractions and improve the
accuracy of reasoning chains, thus mitigating the need for costly verification
processes. We open-source two synthesized datasets, LongFaith-SFT and
LongFaith-PO, which systematically address multiple dimensions of faithfulness,
including verified reasoning, attribution, and contextual grounding. Extensive
experiments on multi-hop reasoning datasets and LongBench demonstrate that
models fine-tuned on these datasets significantly improve performance. Our
ablation studies highlight the scalability and adaptability of the LongFaith
pipeline, showcasing its broad applicability in developing long-context LLMs.

</details>


### [159] [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/pdf/2502.12970)
*Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha*

Main category: cs.CL

TL;DR: R2D enhances LLM safety by integrating safety-aware reasoning and self-evaluation, using pivot tokens and CPO to defend against jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Explore how enhanced reasoning in LLMs can improve safety against jailbreak queries, a gap in current research.

Method: Proposes R2D, a training paradigm with safety-aware reasoning and self-evaluation, and CPO for better pivot token prediction.

Result: R2D effectively mitigates jailbreak attacks, improving safety without compromising performance.

Conclusion: Safety-aware reasoning significantly boosts LLM robustness against jailbreaks, demonstrating R2D's potential.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performances
across diverse domains. However, how safety of Large Language Models (LLMs)
benefits from enhanced reasoning capabilities against jailbreak queries remains
unexplored. To bridge this gap, in this paper, we propose Reasoning-to-Defend
(R2D), a novel training paradigm that integrates a safety-aware reasoning
mechanism into LLMs' generation. This enables self-evaluation at each step of
the reasoning process, forming safety pivot tokens as indicators of the safety
status of responses. Furthermore, in order to improve the accuracy of
predicting pivot tokens, we propose Contrastive Pivot Optimization (CPO), which
enhances the model's perception of the safety status of given dialogues. LLMs
dynamically adjust their response strategies during reasoning, significantly
enhancing their safety capabilities defending jailbreak attacks. Extensive
experiments demonstrate that R2D effectively mitigates various attacks and
improves overall safety, while maintaining the original performances. This
highlights the substantial potential of safety-aware reasoning in improving
robustness of LRMs and LLMs against various jailbreaks.

</details>


### [160] [Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction](https://arxiv.org/pdf/2502.13044)
*Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff*

Main category: cs.CL

TL;DR: LLMs show strong zero- and few-shot performance in ASQP and TASD tasks, nearly matching fine-tuned models, reducing the need for extensive manual annotation.


<details>
  <summary>Details</summary>
Motivation: Annotating training data for ASQP is resource-intensive; exploring LLMs' capabilities can reduce this burden.

Method: Evaluated LLMs on zero- and few-shot learning for ASQP and TASD across five datasets, comparing performance with fine-tuned models.

Result: LLMs achieved F1 scores close to fine-tuned models (e.g., 51.54 vs. 60.39 on Rest16 for ASQP, 68.93 vs. 72.76 for TASD).

Conclusion: LLMs can significantly reduce manual annotation needs while maintaining competitive performance in ASQP and TASD tasks.

Abstract: Aspect sentiment quad prediction (ASQP) facilitates a detailed understanding
of opinions expressed in a text by identifying the opinion term, aspect term,
aspect category and sentiment polarity for each opinion. However, annotating a
full set of training examples to fine-tune models for ASQP is a
resource-intensive process. In this study, we explore the capabilities of large
language models (LLMs) for zero- and few-shot learning on the ASQP task across
five diverse datasets. We report F1 scores almost up to par with those obtained
with state-of-the-art fine-tuned models and exceeding previously reported zero-
and few-shot performance. In the 20-shot setting on the Rest16 restaurant
domain dataset, LLMs achieved an F1 score of 51.54, compared to 60.39 by the
best-performing fine-tuned method MVP. Additionally, we report the performance
of LLMs in target aspect sentiment detection (TASD), where the F1 scores were
close to fine-tuned models, achieving 68.93 on Rest16 in the 30-shot setting,
compared to 72.76 with MVP. While human annotators remain essential for
achieving optimal performance, LLMs can reduce the need for extensive manual
annotation in ASQP tasks.

</details>


### [161] [FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems](https://arxiv.org/pdf/2502.13472)
*Borui Liao, Yulong Xu, Jiao Ou, Kaiyuan Yang, Weihua Jian, Pengfei Wan, Di Zhang*

Main category: cs.CL

TL;DR: FlexDuo, a flexible full-duplex control module, decouples duplex control from dialogue systems, introduces an Idle state for noise filtering, and improves response accuracy and interruption rates.


<details>
  <summary>Details</summary>
Motivation: Existing full-duplex speech dialogue systems face challenges like module coupling and noise interference, limiting naturalness and efficiency.

Method: Proposes FlexDuo with a plug-and-play design and an explicit Idle state for noise filtering and semantic buffering.

Result: FlexDuo reduces false interruptions by 24.9% and improves response accuracy by 7.6%, outperforming baselines.

Conclusion: FlexDuo's modular architecture and state-based model offer a novel approach for efficient duplex dialogue systems.

Abstract: Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly
enhanced the naturalness of human-machine interaction by enabling real-time
bidirectional communication. However, existing approaches face challenges such
as difficulties in independent module optimization and contextual noise
interference due to highly coupled architectural designs and oversimplified
binary state modeling. This paper proposes FlexDuo, a flexible full-duplex
control module that decouples duplex control from spoken dialogue systems
through a plug-and-play architectural design. Furthermore, inspired by human
information-filtering mechanisms in conversations, we introduce an explicit
Idle state. On one hand, the Idle state filters redundant noise and irrelevant
audio to enhance dialogue quality. On the other hand, it establishes a semantic
integrity-based buffering mechanism, reducing the risk of mutual interruptions
while ensuring accurate response transitions. Experimental results on the
Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by
24.9% and improves response accuracy by 7.6% compared to integrated full-duplex
dialogue system baselines. It also outperforms voice activity detection (VAD)
controlled baseline systems in both Chinese and English dialogue quality. The
proposed modular architecture and state-based dialogue model provide a novel
technical pathway for building flexible and efficient duplex dialogue systems.

</details>


### [162] [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/pdf/2502.14538)
*Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: LoRA-MGPO improves LoRA by using momentum-guided perturbation optimization to stabilize training and avoid sharp minima, outperforming existing PEFT methods.


<details>
  <summary>Details</summary>
Motivation: LoRA's non-monotonic training loss behavior (double descent) delays convergence and harms generalization due to unstable gradients and sharp minima attraction.

Method: Proposes LoRA-MGPO, which reuses optimizer momentum vectors to guide perturbations (avoiding dual gradient computations like SAM) and uses adaptive perturbation normalization via EMA-smoothed gradients.

Result: Outperforms LoRA and other PEFT methods in benchmarks, with smoother loss curves, better convergence, and reduced sharp minima attraction.

Conclusion: LoRA-MGPO effectively addresses LoRA's instability issues, offering a more efficient and stable fine-tuning method for LLMs.

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), enable efficient adaptation of large language models (LLMs) via
low-rank matrix optimization with frozen weights. However, LoRA typically
exhibits "double descent" in training loss as rank increases, characterized by
a three-phase dynamics: initial convergence, transient divergence, and eventual
stabilization. This non-monotonic behavior delays convergence and impairs
generalization through unstable gradients and attraction to sharp minima. To
address these challenges, we propose LoRA-MGPO, a novel LoRA-based framework
incorporating Momentum-Guided Perturbation Optimization (MGPO). First, MGPO
eliminates Sharpness-Aware Minimization (SAM)'s dual gradient computations by
reusing momentum vectors from optimizer states to guide perturbation
directions. This retains SAM's training stability and flat minima preference
with maintained efficiency. Second, MGPO incorporates adaptive perturbation
normalization, scaling perturbation intensity via exponential moving average
(EMA)-smoothed gradient magnitudes. Experiments on natural language
understanding and generation benchmarks demonstrate that LoRA-MGPO outperforms
LoRA and state-of-the-art PEFT methods. Further analysis confirms its ability
to stabilize training and reduce sharp minima attraction, with smoother loss
curves and improved convergence behavior. The code is available at
https://github.com/llm172/LoRA-MGPO

</details>


### [163] [Length-Controlled Margin-Based Preference Optimization without Reference Model](https://arxiv.org/pdf/2502.14643)
*Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: LMPO improves DPO by addressing length bias, memory inefficiency, and probability degradation with a new loss function and optimization strategy.


<details>
  <summary>Details</summary>
Motivation: DPO's limitations (length bias, memory inefficiency, probability degradation) hinder preference-based RLHF.

Method: LMPO introduces a uniform reference model, average log-probability optimization, and a Length-Controlled Margin-Based loss function.

Result: LMPO outperforms DPO on Mistral and LLaMA3, controlling response length and reducing probability degradation.

Conclusion: LMPO is a robust alternative to DPO, addressing its key limitations effectively.

Abstract: Direct Preference Optimization (DPO) is a widely adopted offline algorithm
for preference-based reinforcement learning from human feedback (RLHF),
designed to improve training simplicity and stability by redefining reward
functions. However, DPO is hindered by several limitations, including length
bias, memory inefficiency, and probability degradation. To address these
challenges, we propose Length-Controlled Margin-Based Preference Optimization
(LMPO), a more efficient and robust alternative. LMPO introduces a uniform
reference model as an upper bound for the DPO loss, enabling a more accurate
approximation of the original optimization objective. Additionally, an average
log-probability optimization strategy is employed to minimize discrepancies
between training and inference phases. A key innovation of LMPO lies in its
Length-Controlled Margin-Based loss function, integrated within the
Bradley-Terry framework. This loss function regulates response length while
simultaneously widening the margin between preferred and rejected outputs. By
doing so, it mitigates probability degradation for both accepted and discarded
responses, addressing a significant limitation of existing methods. We evaluate
LMPO against state-of-the-art preference optimization techniques on two
open-ended large language models, Mistral and LLaMA3, across six conditional
benchmarks. Our experimental results demonstrate that LMPO effectively controls
response length, reduces probability degradation, and outperforms existing
approaches. The code is available at https://github.com/gengxuli/LMPO.

</details>


### [164] [SOTOPIA-$Ω$: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents](https://arxiv.org/pdf/2502.15538)
*Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, Tingwen Liu*

Main category: cs.CL

TL;DR: The paper introduces SOTOPIA-Ω, a framework to enhance social capabilities of language agents by integrating human social strategies, automating dialogue corpus creation, and proposing new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on transferring human social strategies to social agents, aiming to improve their social dialogue capabilities.

Method: The SOTOPIA-Ω framework dynamically injects multi-step reasoning and direct strategies into expert agents, automating high-quality dialogue corpus construction. It also introduces Social Instruction Following (S-IF) and new evaluation metrics.

Result: 7B models trained on the corpus outperform GPT-4 in social goal achievement and S-IF performance. Dynamic construction breaks agent deadlocks.

Conclusion: SOTOPIA-Ω effectively enhances social agent capabilities, demonstrating the value of dynamic strategy integration and automated corpus construction.

Abstract: Despite the abundance of prior social strategies possessed by humans, there
remains a paucity of research dedicated to their transfer and integration into
social agents. Our proposed SOTOPIA-$\Omega$ framework aims to address and
bridge this gap, with a particular focus on enhancing the social capabilities
of language agents. This framework dynamically injects multi-step reasoning
strategies inspired by negotiation theory and two simple direct strategies into
expert agents, thereby automating the construction of a high-quality social
dialogue training corpus. Additionally, we introduce the concept of Social
Instruction Following (S-IF) and propose two new S-IF evaluation metrics that
complement social capability. We demonstrate that several 7B models trained on
high-quality corpus not only significantly surpass the expert agent (GPT-4) in
achieving social goals but also enhance S-IF performance. Analysis and variant
experiments validate the advantages of dynamic construction, which can
especially break the agent's prolonged deadlock.

</details>


### [165] [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/pdf/2502.15543)
*Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong*

Main category: cs.CL

TL;DR: The paper introduces ParamMute, a framework to improve faithfulness in LLMs by suppressing unfaithful FFN activations, reducing reliance on parametric memory when external evidence conflicts.


<details>
  <summary>Details</summary>
Motivation: Despite retrieval-augmented generation (RAG) improving factuality, LLMs still produce unfaithful outputs contradicting accurate external context. Existing methods overlook internal parametric knowledge's influence.

Method: The study identifies unfaithfulness-associated FFNs and proposes ParamMute, which suppresses these FFNs to calibrate the model toward retrieved knowledge.

Result: ParamMute significantly enhances faithfulness on CoFaithfulQA and ConFiQA benchmarks, reducing reliance on parametric memory.

Conclusion: Mitigating internal knowledge dominance improves LLM trustworthiness in RAG, offering a new direction for future research.

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) have improved factuality by grounding outputs in external evidence.
However, they remain susceptible to unfaithful generation, where outputs
contradict retrieved context despite its relevance and accuracy. Existing
approaches aiming to improve faithfulness primarily focus on enhancing the
utilization of external context, but often overlook the persistent influence of
internal parametric knowledge during generation. In this work, we investigate
the internal mechanisms behind unfaithful generation and identify a subset of
mid-to-deep feed-forward networks (FFNs) that are disproportionately activated
in such cases. Building on this insight, we propose Parametric Knowledge Muting
through FFN Suppression (ParamMute), a framework that improves contextual
faithfulness by suppressing the activation of unfaithfulness-associated FFNs
and calibrating the model toward retrieved knowledge. To evaluate our approach,
we introduce CoFaithfulQA, a benchmark specifically designed to evaluate
faithfulness in scenarios where internal knowledge conflicts with accurate
external evidence. Experimental results show that ParamMute significantly
enhances faithfulness across both CoFaithfulQA and the established ConFiQA
benchmark, achieving substantial reductions in reliance on parametric memory.
These findings underscore the importance of mitigating internal knowledge
dominance and provide a new direction for improving LLM trustworthiness in RAG.
All code will be released via GitHub.

</details>


### [166] [DReSD: Dense Retrieval for Speculative Decoding](https://arxiv.org/pdf/2502.15572)
*Milan Gritta, Huiyin Xue, Gerasimos Lampouras*

Main category: cs.CL

TL;DR: DReSD improves speculative decoding by using dense retrieval for token sequences, outperforming sparse retrieval (REST) in acceptance rates, token length, and speed.


<details>
  <summary>Details</summary>
Motivation: Sparse retrieval (REST) in speculative decoding is limited by short contexts and exact string matching, prompting the need for a more effective method.

Method: DReSD employs approximate nearest neighbour search with contextualised token embeddings to retrieve semantically relevant token sequences.

Result: DReSD achieves 87% higher acceptance rates, 65% longer accepted tokens, and 19% faster generation speeds compared to REST.

Conclusion: Dense retrieval (DReSD) significantly enhances speculative decoding performance over sparse retrieval.

Abstract: Speculative decoding (SD) accelerates Large Language Model (LLM) generation
by using an efficient draft model to propose the next few tokens, which are
verified by the LLM in a single forward call, reducing latency while preserving
its outputs. We focus on retrieval-based SD where the draft model retrieves the
next tokens from a non-parametric datastore. Sparse retrieval (REST), which
operates on the surface form of strings, is currently the dominant paradigm due
to its simplicity and scalability. However, its effectiveness is limited due to
the usage of short contexts and exact string matching. Instead, we introduce
Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses
approximate nearest neighbour search with contextualised token embeddings to
retrieve the most semantically relevant token sequences for SD. Extensive
experiments show that DReSD achieves (on average) 87% higher acceptance rates,
65% longer accepted tokens and 19% faster generation speeds compared to sparse
retrieval (REST).

</details>


### [167] [Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines](https://arxiv.org/pdf/2502.16377)
*Saurabh Srivastava, Sweta Pati, Ziyu Yao*

Main category: cs.CL

TL;DR: Annotation guidelines improve event extraction in LLMs, especially with sufficient training data, enhancing cross-schema generalization and low-frequency event-type performance.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of textual descriptions (annotation guidelines) on event extraction when instruction-tuning large language models.

Method: Conducted experiments with human-provided and machine-generated guidelines in full- and low-data settings.

Result: Guidelines are effective with sufficient training data, improving cross-schema generalization and low-frequency event-type performance.

Conclusion: Annotation guidelines are promising for enhancing event extraction in LLMs, particularly in data-rich scenarios.

Abstract: In this work, we study the effect of annotation guidelines -- textual
descriptions of event types and arguments, when instruction-tuning large
language models for event extraction. We conducted a series of experiments with
both human-provided and machine-generated guidelines in both full- and low-data
settings. Our results demonstrate the promise of annotation guidelines when
there is a decent amount of training data and highlight its effectiveness in
improving cross-schema generalization and low-frequency event-type performance.

</details>


### [168] [A Survey of Uncertainty Estimation Methods on Large Language Models](https://arxiv.org/pdf/2503.00172)
*Zhiqiu Xia, Jinxuan Xu, Yuqian Zhang, Hang Liu*

Main category: cs.CL

TL;DR: This survey explores uncertainty estimation in large language models (LLMs) to address biases and hallucinations, reviewing four major methods and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: LLMs can produce biased or non-factual responses despite their fluency, necessitating uncertainty estimation to improve reliability.

Method: The paper reviews four major approaches to LLM uncertainty estimation and conducts extensive experimental evaluations.

Result: The survey identifies key methods and evaluates their performance across datasets, highlighting gaps and successes.

Conclusion: The paper concludes with critical future directions to advance uncertainty estimation in LLMs.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various tasks. However, these models could offer biased, hallucinated, or
non-factual responses camouflaged by their fluency and realistic appearance.
Uncertainty estimation is the key method to address this challenge. While
research efforts in uncertainty estimation are ramping up, there is a lack of
comprehensive and dedicated surveys on LLM uncertainty estimation. This survey
presents four major avenues of LLM uncertainty estimation. Furthermore, we
perform extensive experimental evaluations across multiple methods and
datasets. At last, we provide critical and promising future directions for LLM
uncertainty estimation.

</details>


### [169] [What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models](https://arxiv.org/pdf/2503.09894)
*Abhipsha Das, Nicholas Lourie, Siavash Golkar, Mariel Pettee*

Main category: cs.CL

TL;DR: The paper proposes a system combining LLMs and structured representations to analyze scientific literature, addressing the challenge of synthesizing knowledge across disciplines.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of scientific literature makes it hard to navigate and synthesize knowledge. Existing LLMs and unstructured methods are limited in capturing detailed relationships or are cost-prohibitive.

Method: The authors enhance LLMs with structured representations using a schema of scientific concepts. They prototype a system to answer precise questions about literature, extracting concepts from 30,000 arXiv papers.

Result: The system successfully extracts concepts across fields like astrophysics and evolutionary biology, creating a database that highlights trends and visualizes a knowledge graph.

Conclusion: Structured representations complement LLMs for systematic analysis of scientific literature, offering new ways to explore and synthesize knowledge.

Abstract: The scientific literature's exponential growth makes it increasingly
challenging to navigate and synthesize knowledge across disciplines. Large
language models (LLMs) are powerful tools for understanding scientific text,
but they fail to capture detailed relationships across large bodies of work.
Unstructured approaches, like retrieval augmented generation, can sift through
such corpora to recall relevant facts; however, when millions of facts
influence the answer, unstructured approaches become cost prohibitive.
Structured representations offer a natural complement -- enabling systematic
analysis across the whole corpus. Recent work enhances LLMs with unstructured
or semistructured representations of scientific concepts; to complement this,
we try extracting structured representations using LLMs. By combining LLMs'
semantic understanding with a schema of scientific concepts, we prototype a
system that answers precise questions about the literature as a whole. Our
schema applies across scientific fields and we extract concepts from it using
only 20 manually annotated abstracts. To demonstrate the system, we extract
concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and
evolutionary biology. The resulting database highlights emerging trends and, by
visualizing the knowledge graph, offers new ways to explore the ever-growing
landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code:
https://github.com/chiral-carbon/kg-for-science.

</details>


### [170] [DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation](https://arxiv.org/pdf/2503.10452)
*Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, Kaidi Xu*

Main category: cs.CL

TL;DR: DynaCode is a dynamic, complexity-aware benchmark for evaluating LLMs in code generation, addressing memorization and data contamination issues in static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Static code benchmarks are vulnerable to memorization and fail to generalize, leading to unreliable evaluations of LLMs.

Method: DynaCode introduces a dynamic benchmark with complexity-aware metrics, including code complexity and call-graph structures, generating diverse problems.

Result: Tests on 12 LLMs show performance drops of 16.8% to 45.7% compared to static benchmarks, with degradation as complexity increases.

Conclusion: DynaCode effectively differentiates LLMs and provides insights into their behavior, particularly in handling nested code and subfunction interactions.

Abstract: The rapid advancement of large language models (LLMs) has significantly
improved their performance in code generation tasks. However, existing code
benchmarks remain static, consisting of fixed datasets with predefined
problems. This makes them vulnerable to memorization during training, where
LLMs recall specific test cases instead of generalizing to new problems,
leading to data contamination and unreliable evaluation results. To address
these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that
overcomes the limitations of static datasets. DynaCode evaluates LLMs
systematically using a complexity-aware metric, incorporating both code
complexity and call-graph structures. DynaCode achieves large-scale diversity,
generating up to 189 million unique nested code problems across four distinct
levels of code complexity, referred to as units, and 16 types of call graphs.
Results on 12 latest LLMs show an average performance drop of 16.8% to 45.7%
compared to MBPP+, a static code generation benchmark, with performance
progressively decreasing as complexity increases. This demonstrates DynaCode's
ability to effectively differentiate LLMs. Additionally, by leveraging call
graphs, we gain insights into LLM behavior, particularly their preference for
handling subfunction interactions within nested code. Our benchmark and
evaluation code are available at https://github.com/HWH-2000/DynaCode.

</details>


### [171] [Enhancing Retrieval for ESGLLM via ESG-CID -- A Disclosure Content Index Finetuning Dataset for Mapping GRI and ESRS](https://arxiv.org/pdf/2503.10674)
*Shafiuddin Rehan Ahmed, Ankit Parag Shah, Quan Hung Tran, Vivek Khetan, Sukryool Kang, Ankit Mehta, Yujia Bao, Wei Wei*

Main category: cs.CL

TL;DR: The paper introduces ESG-CID, a dataset for ESG report automation, using weak supervision from past ESG reports and LLM refinement to improve retrieval models.


<details>
  <summary>Details</summary>
Motivation: Climate change necessitates better ESG reporting, but current frameworks like GRI and ESRS lack standardized automation due to data variability and length.

Method: Leverages disclosure content indexes from ESG reports, refines mappings with an LLM, and benchmarks embedding models, including fine-tuned BERT.

Result: Fine-tuned BERT outperforms commercial and public models, even in cross-report style transfer (GRI to ESRS).

Conclusion: ESG-CID enables robust ESG report automation, with fine-tuned models offering superior performance.

Abstract: Climate change has intensified the need for transparency and accountability
in organizational practices, making Environmental, Social, and Governance (ESG)
reporting increasingly crucial. Frameworks like the Global Reporting Initiative
(GRI) and the new European Sustainability Reporting Standards (ESRS) aim to
standardize ESG reporting, yet generating comprehensive reports remains
challenging due to the considerable length of ESG documents and variability in
company reporting styles. To facilitate ESG report automation,
Retrieval-Augmented Generation (RAG) systems can be employed, but their
development is hindered by a lack of labeled data suitable for training
retrieval models. In this paper, we leverage an underutilized source of weak
supervision -- the disclosure content index found in past ESG reports -- to
create a comprehensive dataset, ESG-CID, for both GRI and ESRS standards. By
extracting mappings between specific disclosure requirements and corresponding
report sections, and refining them using a Large Language Model as a judge, we
generate a robust training and evaluation set. We benchmark popular embedding
models on this dataset and show that fine-tuning BERT-based models can
outperform commercial embeddings and leading public models, even under temporal
data splits for cross-report style transfer from GRI to ESRS. Data:
https://huggingface.co/datasets/airefinery/esg_cid_retrieval

</details>


### [172] [HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model](https://arxiv.org/pdf/2503.12941)
*Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu*

Main category: cs.CL

TL;DR: The paper proposes a framework for continual instruction tuning in MLLMs, addressing memory efficiency and performance trade-offs, and introduces a new benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for continual instruction tuning in MLLMs compromise efficiency for performance, and current benchmarks lack rigor.

Method: A task-specific expansion and task-general fusion framework based on CKA similarity variations across model layers, with a new benchmark to evaluate performance.

Result: Significant performance improvement over state-of-the-art methods, validated through comprehensive experiments.

Conclusion: The proposed framework enhances MLLM adaptability efficiently, and the new benchmark provides a more rigorous evaluation standard.

Abstract: Instruction tuning is widely used to improve a pre-trained Multimodal Large
Language Model (MLLM) by training it on curated task-specific datasets,
enabling better comprehension of human instructions. However, it is infeasible
to collect all possible instruction datasets simultaneously in real-world
scenarios. Thus, enabling MLLM with continual instruction tuning is essential
for maintaining their adaptability. However, existing methods often trade off
memory efficiency for performance gains, significantly compromising overall
efficiency. In this paper, we propose a task-specific expansion and
task-general fusion framework based on the variations in Centered Kernel
Alignment (CKA) similarity across different model layers when trained on
diverse datasets. Furthermore, we analyze the information leakage present in
the existing benchmark and propose a new and more challenging benchmark to
rationally evaluate the performance of different methods. Comprehensive
experiments showcase a significant performance improvement of our method
compared to existing state-of-the-art methods. Code and dataset are released at
https://github.com/Ghy0501/HiDe-LLaVA.

</details>


### [173] [FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article](https://arxiv.org/pdf/2503.16561)
*Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori*

Main category: cs.CL

TL;DR: The paper proposes a method to generate future work suggestions from scientific articles using LLMs and RAG, with a feedback mechanism for quality improvement. It outperforms other methods and includes human evaluation.


<details>
  <summary>Details</summary>
Motivation: To assist researchers in identifying unexplored areas and new projects by automating future work suggestions from scientific articles.

Method: Uses Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with a feedback mechanism and LLM-as-a-judge for evaluation.

Result: The RAG-based approach with LLM feedback outperforms other methods, validated by qualitative, quantitative, and human evaluations.

Conclusion: The proposed method effectively generates high-quality future work suggestions, aiding researchers in exploring new directions.

Abstract: The future work section of a scientific article outlines potential research
directions by identifying gaps and limitations of a current study. This section
serves as a valuable resource for early-career researchers seeking unexplored
areas and experienced researchers looking for new projects or collaborations.
In this study, we generate future work suggestions from key sections of a
scientific article alongside related papers and analyze how the trends have
evolved. We experimented with various Large Language Models (LLMs) and
integrated Retrieval-Augmented Generation (RAG) to enhance the generation
process. We incorporate a LLM feedback mechanism to improve the quality of the
generated content and propose an LLM-as-a-judge approach for evaluation. Our
results demonstrated that the RAG-based approach with LLM feedback outperforms
other methods evaluated through qualitative and quantitative metrics. Moreover,
we conduct a human evaluation to assess the LLM as an extractor and judge. The
code and dataset for this project are here, code: HuggingFace

</details>


### [174] [Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach](https://arxiv.org/pdf/2503.18085)
*Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio*

Main category: cs.CL

TL;DR: GRAPHTREX improves temporal relation extraction in clinical texts using HGT and LPLMs, achieving a 5.5% F1 score boost and 8.9% on long-range relations.


<details>
  <summary>Details</summary>
Motivation: Extracting temporal relations from clinical text is challenging due to complex language and sparse annotations, but crucial for medical insights.

Method: Integrates span-based extraction, clinical LPLMs, and HGT with global landmarks to capture local and global dependencies.

Result: 5.5% F1 score improvement over prior work, with 8.9% gain on long-range relations; strong baseline on E3C corpus.

Conclusion: Advances temporal extraction and supports better diagnostic models through enhanced temporal reasoning.

Abstract: Temporal information extraction from unstructured text is essential for
contextualizing events and deriving actionable insights, particularly in the
medical domain. We address the task of extracting clinical events and their
temporal relations using the well-studied I2B2 2012 Temporal Relations
Challenge corpus. This task is inherently challenging due to complex clinical
language, long documents, and sparse annotations. We introduce GRAPHTREX, a
novel method integrating span-based entity-relation extraction, clinical large
pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)
to capture local and global dependencies. Our HGT component facilitates
information propagation across the document through innovative global landmarks
that bridge distant entities. Our method improves the state-of-the-art with
5.5% improvement in the tempeval $F_1$ score over the previous best and up to
8.9% improvement on long-range relations, which presents a formidable
challenge. We further demonstrate generalizability by establishing a strong
baseline on the E3C corpus. This work not only advances temporal information
extraction but also lays the groundwork for improved diagnostic and prognostic
models through enhanced temporal reasoning.

</details>


### [175] [Multi-Modal Framing Analysis of News](https://arxiv.org/pdf/2503.20960)
*Arnav Arora, Srishti Yadav, Maria Antoniak, Serge Belongie, Isabelle Augenstein*

Main category: cs.CL

TL;DR: A method for multi-modal, multi-label framing analysis using large vision-language models to study political communication, integrating text and images for a more complete media bias analysis.


<details>
  <summary>Details</summary>
Motivation: Existing framing studies are narrow, using pre-defined frames and ignoring visual contexts, missing valuable editorial choices in news.

Method: Uses large vision-language models to extract latent meaning from images and contrast it with text frames, identifying partisan framing.

Result: Demonstrates scalable integrative framing analysis of text and images, providing a more comprehensive understanding of media bias.

Conclusion: The method enhances framing analysis by incorporating visual contexts, offering a more complete picture of media bias in political communication.

Abstract: Automated frame analysis of political communication is a popular task in
computational social science that is used to study how authors select aspects
of a topic to frame its reception. So far, such studies have been narrow, in
that they use a fixed set of pre-defined frames and focus only on the text,
ignoring the visual contexts in which those texts appear. Especially for
framing in the news, this leaves out valuable information about editorial
choices, which include not just the written article but also accompanying
photographs. To overcome such limitations, we present a method for conducting
multi-modal, multi-label framing analysis at scale using large (vision-)
language models. Grounding our work in framing theory, we extract latent
meaning embedded in images used to convey a certain point and contrast that to
the text by comparing the respective frames used. We also identify highly
partisan framing of topics with issue-specific frame analysis found in prior
qualitative work. We demonstrate a method for doing scalable integrative
framing analysis of both text and image in news, providing a more complete
picture for understanding media bias.

</details>


### [176] [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/pdf/2503.22353)
*Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman*

Main category: cs.CL

TL;DR: The paper introduces a framework to evaluate and improve LLM response consistency, featuring a novel PWC score, a benchmark dataset, and the CARG method, which enhances stability without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs need consistent performance in high-stake domains, but current evaluations lack focus on multi-turn interaction stability.

Method: Proposes PWC score for consistency, a benchmark dataset, and CARG framework using confidence signals for stable generation.

Result: CARG improves response stability without compromising accuracy.

Conclusion: The framework enhances LLM reliability for critical applications.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various tasks, but their deployment in high-stake domains requires consistent
performance across multiple interaction rounds. This paper introduces a
comprehensive framework for evaluating and improving LLM response consistency,
making three key contributions. First, we propose a novel Position-Weighted
Consistency (PWC) score that captures both the importance of early-stage
stability and recovery patterns in multi-turn interactions. Second, we present
a carefully curated benchmark dataset spanning diverse domains and difficulty
levels, specifically designed to evaluate LLM consistency under various
challenging follow-up scenarios. Third, we introduce Confidence-Aware Response
Generation (CARG), a framework that significantly improves response stability
by incorporating model confidence signals into the generation process.
Empirical results demonstrate that CARG significantly improves response
stability without sacrificing accuracy, underscoring its potential for reliable
LLM deployment in critical applications.

</details>


### [177] [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/pdf/2504.06560)
*Lanrui Wang, Mingyu Zheng, Hongyin Tang, Zheng Lin, Yanan Cao, Jingang Wang, Xunliang Cai, Weiping Wang*

Main category: cs.CL

TL;DR: The paper introduces NIAT, a benchmark for testing LLMs' fine-grained perception of structured tables, revealing gaps in robust table understanding and showing that training with NIAT data improves performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on unstructured text or high-level reasoning for tables, missing the need for fine-grained cell perception in LLMs.

Method: NIAT treats each table cell as a 'needle' and tests models' ability to extract target cells based on locations or lookup questions.

Result: Evaluation shows a performance gap between downstream tasks and NIAT, indicating reliance on shortcuts. Training with NIAT data improves both NIAT and downstream task performance.

Conclusion: NIAT highlights the need for robust table understanding in LLMs and demonstrates the value of fine-grained cell perception for genuine table comprehension.

Abstract: Processing structured tabular data, particularly large and lengthy tables,
constitutes a fundamental yet challenging task for large language models
(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack
primarily focus on unstructured text, neglecting the challenge of diverse
structured tables. Meanwhile, previous tabular benchmarks mainly consider
downstream tasks that require high-level reasoning abilities, and overlook
models' underlying fine-grained perception of individual table cells, which is
crucial for practical and robust LLM-based table applications. To address this
gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular
benchmark that treats each table cell as a ``needle'' and requires models to
extract the target cell based on cell locations or lookup questions. Our
comprehensive evaluation of various LLMs and multimodal LLMs reveals a
substantial performance gap between popular downstream tabular tasks and the
simpler NIAT task, suggesting that they may rely on dataset-specific
correlations or shortcuts to obtain better benchmark results but lack truly
robust long-context understanding towards structured tables. Furthermore, we
demonstrate that using synthesized NIAT training data can effectively improve
performance on both NIAT task and downstream tabular tasks, which validates the
importance of NIAT capability for LLMs' genuine table understanding ability.
Our data, code and models will be released to facilitate future research.

</details>


### [178] [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120)
*Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger*

Main category: cs.CL

TL;DR: The study evaluates reasoning-enabled LLMs vs. non-reasoning LLMs for NLG evaluation tasks, finding task-dependent benefits and performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored utility of reasoning LLMs for evaluating natural language generation tasks like machine translation and summarization.

Method: Systematic comparison of eight models (reasoning and non-reasoning) on WMT23 and SummEval benchmarks, analyzing performance, reasoning token usage, and distillation effects.

Result: Reasoning LLMs show task-dependent benefits (e.g., OpenAI o3-mini excels in MT), while DeepSeek-R1 underperforms except in summarization. Reasoning token usage correlates with quality only in specific models. Distillation degrades at 8B scale.

Conclusion: First assessment of reasoning LLMs for NLG evaluation reveals nuanced performance, with potential for future research. Code is shared for reproducibility.

Abstract: Reasoning-enabled large language models (LLMs) excel in logical tasks, yet
their utility for evaluating natural language generation remains unexplored.
This study systematically compares reasoning LLMs with non-reasoning
counterparts across machine translation and text summarization evaluation
tasks. We evaluate eight models spanning state-of-the-art reasoning models
(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and
equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks
reveal architecture and task-dependent benefits: OpenAI o3-mini models show
improved performance with increased reasoning on MT, while DeepSeek-R1 and
generally underperforms compared to its non-reasoning variant except in
summarization consistency evaluation. Correlation analysis demonstrates that
reasoning token usage correlates with evaluation quality only in specific
models, while almost all models generally allocate more reasoning tokens when
identifying more quality issues. Distillation maintains reasonable performance
up to 32B parameter models but degrades substantially at 8B scale. This work
provides the first assessment of reasoning LLMs for NLG evaluation and
comparison to non-reasoning models. We share our code to facilitate further
research: https://github.com/NL2G/reasoning-eval.

</details>


### [179] [LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline](https://arxiv.org/pdf/2504.09570)
*Biao Fu, Minpeng Liao, Kai Fan, Chengxi Li, Liang Zhang, Yidong Chen, Xiaodong Shi*

Main category: cs.CL

TL;DR: LLMs excel in offline translation but struggle with streaming input in simultaneous translation (SiMT). A new method using interleaved source-target sequences and special tokens improves SiMT performance while preserving offline translation abilities.


<details>
  <summary>Details</summary>
Motivation: Current LLMs perform poorly in SiMT due to their auto-regressive nature, limiting efficiency and performance. A solution is needed to adapt LLMs for streaming scenarios without losing offline translation capabilities.

Method: Proposes constructing supervised fine-tuning data for SiMT, rearranging source and target tokens into interleaved sequences with special tokens, and adapting training/inference strategies for varying latency.

Result: Achieves state-of-the-art SiMT performance across benchmarks, preserves offline translation abilities, and generalizes to document-level SiMT without extra fine-tuning.

Conclusion: The proposed paradigm effectively adapts LLMs for SiMT, maintaining high performance and efficiency while generalizing beyond offline translation.

Abstract: When the complete source sentence is provided, Large Language Models (LLMs)
perform excellently in offline machine translation even with a simple prompt
"Translate the following sentence from [src lang] into [tgt lang]:". However,
in many real scenarios, the source tokens arrive in a streaming manner and
simultaneous machine translation (SiMT) is required, then the efficiency and
performance of decoder-only LLMs are significantly limited by their
auto-regressive nature. To enable LLMs to achieve high-quality SiMT as
efficiently as offline translation, we propose a novel paradigm that includes
constructing supervised fine-tuning (SFT) data for SiMT, along with new
training and inference strategies. To replicate the token input/output stream
in SiMT, the source and target tokens are rearranged into an interleaved
sequence, separated by special tokens according to varying latency
requirements. This enables powerful LLMs to learn read and write operations
adaptively, based on varying latency prompts, while still maintaining efficient
auto-regressive decoding. Experimental results show that, even with limited SFT
data, our approach achieves state-of-the-art performance across various SiMT
benchmarks, and preserves the original abilities of offline translation.
Moreover, our approach generalizes well to document-level SiMT setting without
requiring specific fine-tuning, even beyond the offline translation model.

</details>


### [180] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/pdf/2504.18428)
*Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: PolyMath is a multilingual math reasoning benchmark for LLMs, covering 18 languages and 4 difficulty levels. It highlights challenges like performance variability, low input-output consistency, and thinking length differences across languages. Output language control may improve reasoning.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive, high-quality multilingual math benchmark for evaluating reasoning LLMs, addressing gaps in difficulty, language diversity, and translation quality.

Method: Developed PolyMath, a benchmark with 18 languages and 4 difficulty levels, and evaluated advanced LLMs like Qwen-3-235B-A22B-Thinking and Gemini-2.5-pro.

Result: Top LLMs scored ~54.6 and 52.2, with ~40% accuracy at the highest level. Key challenges: performance variability, low input-output consistency, and thinking length differences by language.

Conclusion: Output language control can enhance reasoning performance, especially for low-resource languages, offering a path to improve multilingual LLM capabilities.

Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning
benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our
benchmark ensures difficulty comprehensiveness, language diversity, and
high-quality translation, making it a highly discriminative multilingual
mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive
evaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and
Gemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%
accuracy under the highest level From a language perspective, our benchmark
reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning
performance varies widely across languages for current LLMs; (2) Input-output
language consistency is low in reasoning LLMs and may be correlated with
performance; (3) The thinking length differs significantly by language for
current LLMs. Additionally, we demonstrate that controlling the output language
in the instructions has the potential to affect reasoning performance,
especially for some low-resource languages, suggesting a promising direction
for improving multilingual capabilities in LLMs.

</details>


### [181] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/pdf/2505.04388)
*Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés*

Main category: cs.CL

TL;DR: The paper introduces Aloe Beta, an open-source medical LLM, optimizing data preprocessing, training, and safety (via DPO) and efficacy (via RAG). It sets a new evaluation standard and releases competitive models under a permissive license.


<details>
  <summary>Details</summary>
Motivation: To advance open-source medical LLMs, ensuring public interest protection by improving model safety, efficacy, and ethical alignment.

Method: Uses base models (Llama 3.1, Qwen 2.5) with synthetic data and DPO for alignment. Evaluates via four test types (close/open-ended, safety, human assessments).

Result: Aloe Beta models perform competitively in healthcare benchmarks, improve safety, and resist jailbreaking. Includes a detailed risk assessment.

Conclusion: Aloe Beta and its development approach set a new standard for ethical, high-performance open-source medical LLMs.

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [182] [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/pdf/2505.07889)
*Yuyang Liu, Liuzhenghao Lv, Xiancheng Zhang, Li Yuan, Yonghong Tian*

Main category: cs.CL

TL;DR: BioProBench is a large-scale benchmark for evaluating LLMs on biological protocol tasks, revealing their strengths in basic understanding but weaknesses in deep reasoning and structured generation.


<details>
  <summary>Details</summary>
Motivation: To systematically assess LLMs' performance on specialized, accuracy-critical biological protocol texts, which are crucial for reproducibility and safety in life sciences.

Method: Developed BioProBench, a multi-task benchmark with five core tasks (e.g., Protocol Question Answering, Protocol Generation), built on 27K protocols yielding 556K instances. Evaluated 12 LLMs.

Result: LLMs perform well on basic tasks (e.g., ~70% PQA-Acc.) but struggle with deep reasoning and structured generation. Open-source models sometimes match closed-source, but bio-specific models lag.

Conclusion: BioProBench highlights LLMs' challenges in procedural knowledge, domain adaptability, and structured reasoning, guiding future AI for scientific automation.

Abstract: Biological protocols are fundamental to reproducibility and safety in life
science research. While large language models (LLMs) perform well on general
tasks, their systematic evaluation on these highly specialized,
accuracy-critical, and inherently procedural texts remains limited. In this
work, we present BioProBench, the first large-scale, multi-task benchmark for
biological protocol understanding and reasoning. While there are several
benchmark tasks involving protocol question answering, BioProBench provides a
comprehensive suite of five core tasks: Protocol Question Answering, Step
Ordering, Error Correction, Protocol Generation, and Protocol Reasoning,
enabling a holistic evaluation of LLMs on procedural biological texts. Built
upon 27K original protocols, it yields nearly 556K high-quality structured
instances. We evaluate 12 mainstream open/closed-source LLMs. Experimental
results reveal that some models perform well on basic understanding tasks
(e.g., \sim70% PQA-Acc., >64% ERR F1), but struggle significantly with deep
reasoning and structured generation tasks like ordering and generation.
Furthermore, model comparisons show diverse performance: certain open-source
models approach closed-source levels on some tasks, yet bio-specific small
models lag behind general LLMs, indicating limitations on complex procedural
content. Overall, BioProBench, through its task design and experimental
findings, systematically reveals the fundamental challenges for current LLMs in
procedural knowledge understanding, deep adaptability to specific domains,
reliability of structured reasoning, and handling of sophisticated precision
and safety constraints, providing key directions for future AI in the field of
scientific experiment automation. The code and data are available at:
https://github.com/YuyangSunshine/bioprotocolbench and
https://huggingface.co/datasets/BioProBench/BioProBench.

</details>


### [183] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/pdf/2505.08167)
*Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang*

Main category: cs.CL

TL;DR: A novel training method combining bidirectional chains of thought and a reward mechanism improves domain-specific LLMs, addressing issues like bias and catastrophic forgetting in ICH-Qwen.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in fine-tuning LLMs with ICH data, such as bias and incorrect knowledge inheritance, by enhancing reasoning and output quality.

Method: Integrates bidirectional chains of thought (forward and reverse reasoning) and a reward mechanism for structural and content evaluation.

Result: Outperforms baseline methods in accuracy, Bleu-4, and Rouge-L scores, with demonstrated generalizability across domains like Finance and Wikidata.

Conclusion: The method is adaptable and effective for diverse domain-specific LLMs, offering a promising approach for future applications.

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [184] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/pdf/2505.08463)
*Fujun Zhang, Xiaoying Fan, XiangDong Su, Guanglai Gao*

Main category: cs.CL

TL;DR: The paper introduces RepCali, a method to calibrate representations in pre-trained language models (PLMs) to bridge the gap between encoder outputs and decoder inputs, improving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: PLMs struggle with discrepancies between encoder representations and decoder inputs, limiting their effectiveness despite fine-tuning.

Method: RepCali integrates a calibration block in the latent space after the encoder, providing a plug-and-play solution for PLMs with encoder-decoder architectures.

Result: Experiments on 25 PLM-based models across 8 tasks show RepCali enhances performance, outperforming fine-tuning baselines.

Conclusion: RepCali is a universal, easy-to-implement method that significantly improves PLM performance on downstream tasks.

Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.

</details>


### [185] [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/pdf/2505.12864)
*Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus*

Main category: cs.CL

TL;DR: LEXam is a benchmark for evaluating LLMs on legal reasoning, featuring 4,886 law exam questions (open-ended and multiple-choice) in English and German. Current LLMs struggle, especially with structured, multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-form legal reasoning in LLMs and provide a scalable evaluation method.

Method: LEXam uses 340 law exams (4,886 questions) with reference answers and explicit reasoning guidance. Evaluation employs LLM-as-a-Judge with human validation.

Result: LLMs struggle with open-ended questions requiring structured reasoning. The benchmark effectively differentiates model capabilities.

Conclusion: LEXam offers a scalable, accurate way to assess legal reasoning quality in LLMs, highlighting current limitations.

Abstract: Long-form legal reasoning remains a key challenge for large language models
(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a
novel benchmark derived from 340 law exams spanning 116 law school courses
across a range of subjects and degree levels. The dataset comprises 4,886 law
exam questions in English and German, including 2,841 long-form, open-ended
questions and 2,045 multiple-choice questions. Besides reference answers, the
open questions are also accompanied by explicit guidance outlining the expected
legal reasoning approach such as issue spotting, rule recall, or rule
application. Our evaluation on both open-ended and multiple-choice questions
present significant challenges for current LLMs; in particular, they notably
struggle with open questions that require structured, multi-step legal
reasoning. Moreover, our results underscore the effectiveness of the dataset in
differentiating between models with varying capabilities. Adopting an
LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate
how model-generated reasoning steps can be evaluated consistently and
accurately. Our evaluation setup provides a scalable method to assess legal
reasoning quality beyond simple accuracy metrics. Project page:
https://lexam-benchmark.github.io/

</details>


### [186] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/pdf/2505.14107)
*Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang*

Main category: cs.CL

TL;DR: DiagnosisArena is a new benchmark for evaluating AI diagnostic reasoning in healthcare, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust benchmarks for assessing AI diagnostic capabilities in complex clinical scenarios.

Method: Developed DiagnosisArena with 1,113 patient case-diagnosis pairs from top medical journals, involving AI and human expert reviews.

Result: Top models achieved low accuracy (51.12%, 31.09%, 17.79%), showing a generalization bottleneck.

Conclusion: DiagnosisArena aims to advance AI diagnostic reasoning for real-world clinical challenges, with tools available for further research.

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3, o1, and DeepSeek-R1, achieve only 51.12%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AI's diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [187] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/pdf/2505.14279)
*Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch*

Main category: cs.CL

TL;DR: YESciEval is an open-source framework for robust evaluation of LLMs in scientific Q&A, using rubric-based assessment and reinforcement learning to reduce bias.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored robustness of LLM evaluations in scientific question-answering.

Method: Combines fine-grained rubric-based assessment with reinforcement learning, using multidisciplinary datasets and adversarial variants.

Result: Enables scalable, cost-free evaluation without proprietary models or human feedback.

Conclusion: Advances reliable LLM-as-a-judge models, supporting AI alignment and transparent evaluation for scientific inquiry.

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry.

</details>


### [188] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/pdf/2505.16983)
*Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen*

Main category: cs.CL

TL;DR: The paper addresses mismatches in adapting batch-oriented LLMs to streaming, focusing on input-attention as the key issue. It introduces a group position encoding method to improve consistency between streaming and batch modes without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting LLMs to streaming are costly or lack scalability. The paper identifies key mismatches and challenges the assumption that frequent re-encoding is necessary.

Method: Introduces a group position encoding paradigm to enhance consistency between streaming and batch modes, leveraging batch architectures without modifications.

Result: The method outperforms existing approaches in cross-lingual and cross-modal tasks, showing strong generalization in both streaming and batch modes.

Conclusion: The proposed group position encoding effectively addresses streaming adaptation for LLMs, offering a scalable and efficient solution without architectural changes.

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [189] [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/pdf/2505.17139)
*Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai*

Main category: cs.CL

TL;DR: A new benchmark for evaluating LLMs in Earth science is introduced, covering broad and advanced capabilities, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack Earth science specificity and neglect open-ended scientific exploration, necessitating a specialized evaluation tool.

Method: Constructed two QA datasets (Earth-Iron and Earth-Silver) and introduced Earth-Gold for open-ended dialogues, using 100,000 research papers.

Result: Experiments show limitations in 11 leading LLMs, indicating significant room for improvement in scientific exploration.

Conclusion: The benchmark provides a comprehensive tool for assessing LLMs in Earth science, highlighting gaps in current capabilities.

Abstract: Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .

</details>


### [190] [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/pdf/2505.17656)
*Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper addresses self-consistent errors in LLMs, where incorrect responses are repeated across samples. It evaluates detection methods, finding they struggle with such errors, and proposes a cross-model probe method to improve detection.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often generate plausible but incorrect content, and existing detection methods fail to address self-consistent errors, where LLMs repeatedly produce the same incorrect response.

Method: The work formally defines self-consistent errors, evaluates mainstream detection methods on them, and proposes a cross-model probe method using hidden state evidence from an external verifier LLM.

Result: Key findings: (1) Self-consistent errors persist or increase with LLM scale, unlike inconsistent errors. (2) Current detection methods struggle with self-consistent errors. The proposed method improves detection across three LLM families.

Conclusion: The study highlights limitations in current error detection methods and demonstrates the effectiveness of a cross-model probe approach for detecting self-consistent errors in LLMs.

Abstract: As large language models (LLMs) often generate plausible but incorrect
content, error detection has become increasingly critical to ensure
truthfulness. However, existing detection methods often overlook a critical
problem we term as self-consistent error, where LLMs repeatly generate the same
incorrect response across multiple stochastic samples. This work formally
defines self-consistent errors and evaluates mainstream detection methods on
them. Our investigation reveals two key findings: (1) Unlike inconsistent
errors, whose frequency diminishes significantly as LLM scale increases, the
frequency of self-consistent errors remains stable or even increases. (2) All
four types of detection methshods significantly struggle to detect
self-consistent errors. These findings reveal critical limitations in current
detection methods and underscore the need for improved methods. Motivated by
the observation that self-consistent errors often differ across LLMs, we
propose a simple but effective cross-model probe method that fuses hidden state
evidence from an external verifier LLM. Our method significantly enhances
performance on self-consistent errors across three LLM families.

</details>


### [191] [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/pdf/2505.18128)
*Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer*

Main category: cs.CL

TL;DR: Frankentexts are long-form narratives by LLMs with most tokens copied from human text, testing controllable generation. Gemini-2.5-Pro excels, but detectors struggle to identify them.


<details>
  <summary>Details</summary>
Motivation: To explore controllable generation under extreme constraints and challenge AI text detectors.

Method: Generate drafts by combining human passages, then iteratively revise while maintaining a copy ratio.

Result: 81% coherent, 100% prompt-relevant; 59% misclassified as human-written. Human annotators detect tone shifts and grammar inconsistencies.

Conclusion: Frankentexts highlight detector limitations and open discussions on mixed authorship and human-AI co-writing.

Abstract: We introduce Frankentexts, a new type of long-form narratives produced by
LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied
verbatim from human writings. This task presents a challenging test of
controllable generation, requiring models to satisfy a writing prompt,
integrate disparate text fragments, and still produce a coherent narrative. To
generate Frankentexts, we instruct the model to produce a draft by selecting
and combining human-written passages, then iteratively revise the draft while
maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts
along three axes: writing quality, instruction adherence, and detectability.
Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts
are coherent and 100% relevant to the prompt. Notably, up to 59% of these
outputs are misclassified as human-written by detectors like Pangram, revealing
limitations in AI text detectors. Human annotators can sometimes identify
Frankentexts through their abrupt tone shifts and inconsistent grammar between
segments, especially in longer generations. Beyond presenting a challenging
generation task, Frankentexts invite discussion on building effective detectors
for this new grey zone of authorship, provide training data for mixed
authorship detection, and serve as a sandbox for studying human-AI co-writing
processes.

</details>


### [192] [Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model](https://arxiv.org/pdf/2505.18486)
*Hong Jiao, Dan Song, Won-Chan Lee*

Main category: cs.CL

TL;DR: The study compared ten LLMs with human raters for automated scoring of writing tasks, finding ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet most reliable.


<details>
  <summary>Details</summary>
Motivation: To determine which LLMs produce reliable scores and minimize rater effects for practical use in automated scoring.

Method: Evaluated ten LLMs against human raters using Quadratic Weighted Kappa, Cronbach Alpha, and Many-Facet Rasch model.

Result: ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet showed high accuracy, reliability, and fewer rater effects.

Conclusion: These LLMs are suitable for automated scoring in low-stakes assessments.

Abstract: Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.

</details>


### [193] [Business as Rulesual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/pdf/2505.18542)
*Chen Yang, Ruping Xu, Ruizhe Li, Bin Cao, Jing Fan*

Main category: cs.CL

TL;DR: The paper introduces BPRF, a Chinese dataset for business rule extraction, and ExIde, a framework using LLMs to automate rule extraction and dependency analysis.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on procedural action flows, leaving rule flows in business documents underexplored.

Method: Proposes ExIde, a framework leveraging LLMs for rule extraction and dependency identification, evaluated on the BPRF dataset.

Result: ExIde effectively extracts structured business rules and analyzes dependencies, benchmarking 12 SOTA LLMs.

Conclusion: The work advances automated and interpretable business process automation.

Abstract: Process mining aims to discover, monitor and optimize the actual behaviors of
real processes. While prior work has mainly focused on extracting procedural
action flows from instructional texts, rule flows embedded in business
documents remain underexplored. To this end, we introduce a novel annotated
Chinese dataset, BPRF, which contains 50 business process documents with 326
explicitly labeled business rules across multiple domains. Each rule is
represented as a <Condition, Action> pair, and we annotate logical dependencies
between rules (sequential, conditional, or parallel). We also propose ExIde, a
framework for automatic business rule extraction and dependency relationship
identification using large language models (LLMs). We evaluate ExIde using 12
state-of-the-art (SOTA) LLMs on the BPRF dataset, benchmarking performance on
both rule extraction and dependency classification tasks of current LLMs. Our
results demonstrate the effectiveness of ExIde in extracting structured
business rules and analyzing their interdependencies for current SOTA LLMs,
paving the way for more automated and interpretable business process
automation.

</details>


### [194] [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/pdf/2505.18638)
*Md. Tanzib Hosain, Rajan Das Gupta, Md. Kishor Morol*

Main category: cs.CL

TL;DR: DZEN is a parallel Dzongkha-English dataset for evaluating LLMs, revealing performance gaps and effective prompting strategies like CoT for reasoning questions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for evaluating LLMs in Dzongkha and improve their performance in low-resource languages.

Method: Created a dataset of 5K parallel questions, tested LLMs with various prompting strategies (e.g., CoT), and analyzed performance differences.

Result: Significant performance gap between English and Dzongkha; CoT works well for reasoning questions but not factual ones; English translations improve Dzongkha responses.

Conclusion: The dataset and findings highlight opportunities to enhance LLM performance in Dzongkha and other low-resource languages.

Abstract: In this work, we provide DZEN, a dataset of parallel Dzongkha and English
test questions for Bhutanese middle and high school students. The over 5K
questions in our collection span a variety of scientific topics and include
factual, application, and reasoning-based questions. We use our parallel
dataset to test a number of Large Language Models (LLMs) and find a significant
performance difference between the models in English and Dzongkha. We also look
at different prompting strategies and discover that Chain-of-Thought (CoT)
prompting works well for reasoning questions but less well for factual ones. We
also find that adding English translations enhances the precision of Dzongkha
question responses. Our results point to exciting avenues for further study to
improve LLM performance in Dzongkha and, more generally, in low-resource
languages. We release the dataset at:
https://github.com/kraritt/llm_dzongkha_evaluation.

</details>


### [195] [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/pdf/2505.18962)
*Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu*

Main category: cs.CL

TL;DR: System-1.5 Reasoning improves efficiency in LLMs by dynamically allocating computation via latent-space shortcuts, achieving comparable performance to CoT with 20x faster inference.


<details>
  <summary>Details</summary>
Motivation: Current latent-space reasoning methods treat all steps uniformly, wasting resources. System-1.5 aims to optimize computation by distinguishing critical from auxiliary steps.

Method: Introduces dynamic shortcuts: depth shortcut (DS) for vertical early exiting and step shortcut (SS) for horizontal step skipping. Uses two-stage self-distillation for training.

Result: Achieves performance similar to CoT on GSM8K while speeding up inference by 20x and reducing token generation by 92.31%.

Conclusion: System-1.5 Reasoning efficiently balances performance and computational cost, making it a promising framework for adaptive reasoning in LLMs.

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move
beyond fast System-1 responses and engage in deliberative System-2 reasoning.
However, this comes at the cost of significant inefficiency due to verbose
intermediate output. Recent latent-space reasoning methods improve efficiency
by operating on hidden states without decoding into language, yet they treat
all steps uniformly, failing to distinguish critical deductions from auxiliary
steps and resulting in suboptimal use of computational resources. In this
paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that
dynamically allocates computation across reasoning steps through shortcut paths
in latent space. Specifically, System-1.5 Reasoning introduces two types of
dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the
vertical depth by early exiting non-critical tokens through lightweight adapter
branches, while allowing critical tokens to continue through deeper Transformer
layers. The step shortcut (SS) reuses hidden states across the decoding steps
to skip trivial steps and reason horizontally in latent space. Training
System-1.5 Reasoning involves a two-stage self-distillation process: first
distilling natural language CoT into latent-space continuous thought, and then
distilling full-path System-2 latent reasoning into adaptive shortcut paths
(System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior
performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves
reasoning performance comparable to traditional CoT fine-tuning methods while
accelerating inference by over 20x and reducing token generation by 92.31% on
average.

</details>


### [196] [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/pdf/2505.19201)
*Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang*

Main category: cs.CL

TL;DR: DREAM introduces a speculative decoding framework for vision-language models (VLMs) with cross-attention-based alignment, adaptive feature selection, and visual token compression, achieving up to 3.6x speedup.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding (SD) accelerates autoregressive generation in LLMs but is underexplored in VLMs. DREAM addresses this gap.

Method: DREAM combines cross-attention-based alignment, adaptive feature selection, and visual token compression for efficient multimodal decoding.

Result: Experiments show up to 3.6x speedup over conventional decoding and outperform prior SD baselines in throughput and draft acceptance.

Conclusion: DREAM enables efficient, accurate, and parallel multimodal decoding, significantly improving VLM performance.

Abstract: Speculative decoding (SD) has emerged as a powerful method for accelerating
autoregressive generation in large language models (LLMs), yet its integration
into vision-language models (VLMs) remains underexplored. We introduce DREAM, a
novel speculative decoding framework tailored for VLMs that combines three key
innovations: (1) a cross-attention-based mechanism to inject intermediate
features from the target model into the draft model for improved alignment, (2)
adaptive intermediate feature selection based on attention entropy to guide
efficient draft model training, and (3) visual token compression to reduce
draft model latency. DREAM enables efficient, accurate, and parallel multimodal
decoding with significant throughput improvement. Experiments across a diverse
set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,
demonstrate up to 3.6x speedup over conventional decoding and significantly
outperform prior SD baselines in both inference throughput and speculative
draft acceptance length across a broad range of multimodal benchmarks. The code
is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git

</details>


### [197] [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/pdf/2505.19510)
*Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo*

Main category: cs.CL

TL;DR: The paper introduces TSG Bench, a benchmark to evaluate LLMs' ability to understand and generate scene graphs, revealing strengths in understanding but weaknesses in generation, especially for complex narratives.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation of LLMs' ability to utilize scene graphs for spatial and temporal understanding in multimodal environments.

Method: The authors developed TSG Bench, a benchmark to assess LLMs' scene graph understanding and generation capabilities, evaluating 11 models.

Result: LLMs perform well on scene graph understanding but struggle with generation, particularly for complex narratives, due to ineffective scene decomposition.

Conclusion: The findings highlight the need for improved methodologies in scene graph generation and provide insights for future research.

Abstract: The remarkable reasoning and generalization capabilities of Large Language
Models (LLMs) have paved the way for their expanding applications in embodied
AI, robotics, and other real-world tasks. To effectively support these
applications, grounding in spatial and temporal understanding in multimodal
environments is essential. To this end, recent works have leveraged scene
graphs, a structured representation that encodes entities, attributes, and
their relationships in a scene. However, a comprehensive evaluation of LLMs'
ability to utilize scene graphs remains limited. In this work, we introduce
Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess
LLMs' ability to (1) understand scene graphs and (2) generate them from textual
narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models
perform well on scene graph understanding, they struggle with scene graph
generation, particularly for complex narratives. Our analysis indicates that
these models fail to effectively decompose discrete scenes from a complex
narrative, leading to a bottleneck when generating scene graphs. These findings
underscore the need for improved methodologies in scene graph generation and
provide valuable insights for future research. The demonstration of our
benchmark is available at https://tsg-bench.netlify.app. Additionally, our code
and evaluation data are publicly available at
https://github.com/docworlds/tsg-bench.

</details>


### [198] [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/pdf/2505.19529)
*Tanjil Hasan Sakib, Md. Tanzib Hosain, Md. Kishor Morol*

Main category: cs.CL

TL;DR: The paper provides a comprehensive review of Small Language Models (SLMs), covering design, training, optimization techniques, and evaluation, while highlighting unresolved challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: SLMs are gaining attention for their efficiency in resource-limited environments, but a systematic assessment of their design and optimization is lacking.

Method: The study reviews SLM frameworks, training approaches, and optimization techniques (pruning, quantization, compression), proposes a classification system, and assembles an evaluation suite.

Result: A novel classification system for SLM optimization is introduced, and a rigorous evaluation platform is established.

Conclusion: The study serves as a guide for developing efficient SLMs, addressing unresolved challenges like performance-efficiency trade-offs and suggesting future research directions.

Abstract: Small Language Models (SLMs) have gained substantial attention due to their
ability to execute diverse language tasks successfully while using fewer
computer resources. These models are particularly ideal for deployment in
limited environments, such as mobile devices, on-device processing, and edge
systems. In this study, we present a complete assessment of SLMs, focussing on
their design frameworks, training approaches, and techniques for lowering model
size and complexity. We offer a novel classification system to organize the
optimization approaches applied for SLMs, encompassing strategies like pruning,
quantization, and model compression. Furthermore, we assemble SLM's studies of
evaluation suite with some existing datasets, establishing a rigorous platform
for measuring SLM capabilities. Alongside this, we discuss the important
difficulties that remain unresolved in this sector, including trade-offs
between efficiency and performance, and we suggest directions for future study.
We anticipate this study to serve as a beneficial guide for researchers and
practitioners who aim to construct compact, efficient, and high-performing
language models.

</details>


### [199] [Multi-Domain Explainability of Preferences](https://arxiv.org/pdf/2505.20088)
*Nitay Calderon, Liat Ein-Dor, Roi Reichart*

Main category: cs.CL

TL;DR: A method for generating concept-based explanations of preferences in LLMs, using automated concept identification and a hierarchical regression model, improves prediction and explainability.


<details>
  <summary>Details</summary>
Motivation: Understanding the underlying concepts driving preferences in LLMs is unclear, despite their importance in alignment and evaluation.

Method: Uses an LLM to identify distinguishing concepts and a white-box hierarchical regression model to capture domain-general and domain-specific effects.

Result: Outperforms baselines in preference prediction, enhances LLM outputs, and improves human preference predictions.

Conclusion: Introduces a new paradigm for explainability in LLMs, demonstrating practical benefits in guiding outputs and improving predictions.

Abstract: Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and
reward models, are central to aligning and evaluating large language models
(LLMs). Yet, the underlying concepts that drive these preferences remain poorly
understood. In this work, we propose a fully automated method for generating
local and global concept-based explanations of preferences across multiple
domains. Our method utilizes an LLM to identify concepts that distinguish
between chosen and rejected responses, and to represent them with concept-based
vectors. To model the relationships between concepts and preferences, we
propose a white-box Hierarchical Multi-Domain Regression model that captures
both domain-general and domain-specific effects. To evaluate our method, we
curate a dataset spanning eight challenging and diverse domains and explain
twelve mechanisms. Our method achieves strong preference prediction
performance, outperforming baselines while also being explainable.
Additionally, we assess explanations in two application-driven settings. First,
guiding LLM outputs with concepts from LaaJ explanations yields responses that
those judges consistently prefer. Second, prompting LaaJs with concepts
explaining humans improves their preference predictions. Together, our work
establishes a new paradigm for explainability in the era of LLMs.

</details>


### [200] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/pdf/2505.20921)
*Injae Na, Keonwoong Noh, Woohwan Jung*

Main category: cs.CL

TL;DR: LLM-AT is a framework for automatically selecting the best LLM tier for subtasks, balancing cost and performance without training. It uses Starter, Generator, and Judge components, along with an accuracy estimator, to iteratively upgrade LLM tiers until a valid response is achieved.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting the right LLM tier for subtasks in complex NLP tasks, balancing cost and performance.

Method: LLM-AT consists of Starter (initial tier selection), Generator (response generation), and Judge (response validation). It iteratively upgrades tiers if responses are invalid. An accuracy estimator predicts initial tier suitability using past records.

Result: LLM-AT achieves superior performance while reducing costs, validated through experiments.

Conclusion: LLM-AT is a practical solution for real-world applications, optimizing LLM tier selection for cost-effective performance.

Abstract: LLM providers typically offer multiple LLM tiers, varying in performance and
price. As NLP tasks become more complex and modularized, selecting the suitable
LLM tier for each subtask is a key challenge to balance between cost and
performance. To address the problem, we introduce LLM Automatic Transmission
(LLM-AT) framework that automatically selects LLM tiers without training.
LLM-AT consists of Starter, Generator, and Judge. The starter selects the
initial LLM tier expected to solve the given question, the generator produces a
response using the LLM of the selected tier, and the judge evaluates the
validity of the response. If the response is invalid, LLM-AT iteratively
upgrades to a higher-tier model, generates a new response, and re-evaluates
until a valid response is obtained. Additionally, we propose accuracy
estimator, which enables the suitable initial LLM tier selection without
training. Given an input question, accuracy estimator estimates the expected
accuracy of each LLM tier by computing the valid response rate across top-k
similar queries from past inference records. Experiments demonstrate that
LLM-AT achieves superior performance while reducing costs, making it a
practical solution for real-world applications.

</details>


### [201] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/pdf/2505.21889)
*Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang*

Main category: cs.CL

TL;DR: EFIM improves KV cache reuse in LLMs for infilling tasks by transforming the prompt format and introducing fragment tokenization, reducing latency by 52% and boosting throughput by 98%.


<details>
  <summary>Details</summary>
Motivation: KV cache reuse in infilling tasks is hindered by prompt structure, leading to inefficiency due to invalidated prefix/suffix caches.

Method: Proposes EFIM, a transformed prompt format, and fragment tokenization to address inefficiencies and subtoken generation issues.

Result: EFIM reduces latency by 52% and increases throughput by 98% while maintaining infilling accuracy.

Conclusion: EFIM effectively optimizes LLM performance for infilling tasks through prompt transformation and fragment tokenization.

Abstract: Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (KV)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the KV cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the KV cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of KV cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability. EFIM's source code is publicly
available at https://github.com/gty111/EFIM.

</details>


### [202] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/pdf/2505.22018)
*Ruicheng Yin, Xuan Gao, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces Seamless Packing (SP), a data packing strategy for continual pre-training that reduces truncation and context discontinuity, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Existing data packing methods for continual pre-training cause excessive truncation and context discontinuity, harming model performance.

Method: SP uses a sliding window for overlapping tokens and a First-Fit-Decreasing algorithm to pack shorter texts, minimizing padding and truncation.

Result: SP outperforms baselines in 99% of settings across various models and domains.

Conclusion: SP effectively enhances continual pre-training by preserving context and reducing truncation, with significant performance gains.

Abstract: Continual pre-training has demonstrated significant potential in enhancing
model performance, particularly in domain-specific scenarios. The most common
approach for packing data before continual pre-training involves concatenating
input texts and splitting them into fixed-length sequences. While
straightforward and efficient, this method often leads to excessive truncation
and context discontinuity, which can hinder model performance. To address these
issues, we explore the potential of data engineering to enhance continual
pre-training, particularly its impact on model performance and efficiency. We
propose Seamless Packing (SP), a novel data packing strategy aimed at
preserving contextual information more effectively and enhancing model
performance. Our approach employs a sliding window technique in the first stage
that synchronizes overlapping tokens across consecutive sequences, ensuring
better continuity and contextual coherence. In the second stage, we adopt a
First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger
than the target sequence length, thereby minimizing padding and truncation.
Empirical evaluations across various model architectures and corpus domains
demonstrate the effectiveness of our method, outperforming baseline method in
99% of all settings. Code is available at
https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [203] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/pdf/2505.22179)
*Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu*

Main category: cs.CL

TL;DR: Integrating speculative decoding with quantization in large language models reveals trade-offs; a hierarchical framework improves speedup by 2.78× for 4-bit models.


<details>
  <summary>Details</summary>
Motivation: To optimize memory-bound inference in large language models by combining speculative decoding and quantization, addressing their individual limitations.

Method: Investigates integrating speculative decoding (EAGLE-2) with quantization, then proposes a hierarchical framework using a small model to convert tree-style drafts into sequence drafts.

Result: The hierarchical approach achieves a 2.78× speedup for 4-bit Llama-3-70B, outperforming EAGLE-2 by 1.31×.

Conclusion: The hierarchical framework effectively balances memory and computational trade-offs, enhancing inference speed for quantized models.

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [204] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/pdf/2505.22375)
*Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang*

Main category: cs.CL

TL;DR: Pangu Embedded is an efficient LLM reasoner for Ascend NPUs, featuring fast/slow thinking modes, addressing computational costs and latency. It uses a two-stage training framework and outperforms similar models.


<details>
  <summary>Details</summary>
Motivation: To tackle high computational costs and inference latency in reasoning-optimized LLMs, enabling efficient deployment.

Method: Two-stage training: Stage 1 involves iterative distillation and reinforcement learning with MARS. Stage 2 introduces a dual-system framework for fast/slow modes.

Result: Outperforms models like Qwen3-8B and GLM4-9B on benchmarks (AIME 2024, GPQA, LiveCodeBench) with 7B parameters.

Conclusion: Pangu Embedded offers a unified, deployable LLM reasoner with balanced latency and reasoning depth, setting a promising direction for future models.

Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM)
reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible
fast and slow thinking capabilities. Pangu Embedded addresses the significant
computational costs and inference latency challenges prevalent in existing
reasoning-optimized LLMs. We propose a two-stage training framework for its
construction. In Stage 1, the model is finetuned via an iterative distillation
process, incorporating inter-iteration model merging to effectively aggregate
complementary knowledge. This is followed by reinforcement learning on Ascend
clusters, optimized by a latency-tolerant scheduler that combines stale
synchronous parallelism with prioritized data queues. The RL process is guided
by a Multi-source Adaptive Reward System (MARS), which generates dynamic,
task-specific reward signals using deterministic metrics and lightweight LLM
evaluators for mathematics, coding, and general problem-solving tasks. Stage 2
introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode
for routine queries and a deeper "slow" mode for complex inference. This
framework offers both manual mode switching for user control and an automatic,
complexity-aware mode selection mechanism that dynamically allocates
computational resources to balance latency and reasoning depth. Experimental
results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate
that Pangu Embedded with 7B parameters, outperforms similar-size models like
Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art
reasoning quality within a single, unified model architecture, highlighting a
promising direction for developing powerful yet practically deployable LLM
reasoners.

</details>


### [205] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2505.22571)
*Hoang Pham, Thuy-Duong Nguyen, Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: A novel unified RAG system, Agent-UniRAG, uses LLM agents for interpretability and handles both single-hop and multi-hop queries. It includes a synthetic dataset, SynAgent-RAG, for smaller LLMs, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Previous RAG systems addressed single-hop or multi-hop queries separately, limiting real-world applicability. This work aims to unify these approaches for broader use.

Method: Proposes Agent-UniRAG, a trainable LLM agent framework that processes queries step-by-step based on complexity, and introduces SynAgent-RAG dataset for smaller LLMs.

Result: Agent-UniRAG performs comparably to closed-source and larger open-source LLMs on RAG benchmarks.

Conclusion: The framework enhances RAG system effectiveness and interpretability, with publicly available code and dataset for further research.

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [206] [Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking](https://arxiv.org/pdf/2505.22677)
*Jisu Kim, Alex Mattingly, Eung-Joo Lee, Benjamin S. Riggan*

Main category: cs.CV

TL;DR: A framework for efficient tiny head detection and tracking balances performance and efficiency using cross-domain loss, multi-scale training, and small receptive fields, improving MOTA and mAP on crowded datasets.


<details>
  <summary>Details</summary>
Motivation: Current head detection and tracking methods are computationally expensive, increasing latency and resource usage.

Method: Integrates cross-domain detection loss, a multi-scale module, and small receptive field detection to enhance tiny head detection.

Result: Improved MOTA and mAP on CroHD and CrowdHuman datasets.

Conclusion: The framework effectively enhances tiny head detection and tracking in crowded scenes while optimizing efficiency.

Abstract: Head detection and tracking are essential for downstream tasks, but current
methods often require large computational budgets, which increase latencies and
ties up resources (e.g., processors, memory, and bandwidth). To address this,
we propose a framework to enhance tiny head detection and tracking by
optimizing the balance between performance and efficiency. Our framework
integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3)
a small receptive field detection mechanism. These innovations enhance
detection by bridging the gap between large and small detectors, capturing
high-frequency details at multiple scales during training, and using filters
with small receptive fields to detect tiny heads. Evaluations on the CroHD and
CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and
mean Average Precision (mAP), demonstrating the effectiveness of our approach
in crowded scenes.

</details>


### [207] [Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for Sparse-Data Vision](https://arxiv.org/pdf/2505.22701)
*Ziyue Kang, Weichuan Zhang*

Main category: cs.CV

TL;DR: A hybrid deep-learning framework with adaptive DCT preprocessing, ViT-B16, ResNet50, and Bayesian classification improves rare animal image classification under data scarcity.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scarce labeled data for rare animal species by leveraging frequency-domain and spatial-domain features.

Method: Combines adaptive DCT for frequency-domain feature selection, ViT-B16 for global context, ResNet50 for local spatial features, and Bayesian linear classification.

Result: Outperforms traditional CNNs and fixed-band DCT methods, achieving state-of-the-art accuracy on a 50-class wildlife dataset.

Conclusion: The proposed hybrid framework effectively tackles data scarcity in rare animal classification by integrating adaptive frequency and spatial features.

Abstract: A major challenge in rare animal image classification is the scarcity of
data, as many species usually have only a small number of labeled samples.
  To address this challenge, we designed a hybrid deep-learning framework
comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50
backbones, and a Bayesian linear classification head. To our knowledge, we are
the first to introduce an adaptive frequency-domain selection mechanism that
learns optimal low-, mid-, and high-frequency boundaries suited to the
subsequent backbones.
  Our network first captures image frequency-domain cues via this adaptive DCT
partitioning. The adaptively filtered frequency features are then fed into
ViT-B16 to model global contextual relationships, while ResNet50 concurrently
extracts local, multi-scale spatial representations from the original image. A
cross-level fusion strategy seamlessly integrates these frequency- and
spatial-domain embeddings, and the fused features are passed through a Bayesian
linear classifier to output the final category predictions. On our self-built
50-class wildlife dataset, this approach outperforms conventional CNN and
fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme
sample scarcity.

</details>


### [208] [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/pdf/2505.22705)
*Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, Yimeng Wang, Kai Yu, Wenxuan Chen, Ziwei Feng, Zijian Gong, Jianzhuang Pan, Yi Peng, Rui Tian, Siyu Wang, Bo Zhao, Ting Yao, Tao Mei*

Main category: cs.CV

TL;DR: HiDream-I1 is a 17B-parameter image generative model with a sparse Diffusion Transformer (DiT) and dynamic Mixture-of-Experts (MoE) architecture, offering high-quality image generation in seconds. It includes variants for flexibility and extends to instruction-based image editing (HiDream-E1) and a comprehensive image agent (HiDream-A1).


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between image generation quality and computational complexity by developing a fast, high-quality model with flexible capabilities.

Method: Uses a dual-stream decoupled sparse DiT with dynamic MoE for processing image and text tokens, followed by a single-stream sparse DiT for efficient multi-model interaction. Offers three variants for varied needs.

Result: Achieves state-of-the-art image generation quality within seconds and supports instruction-based image editing and interactive image refinement.

Conclusion: HiDream-I1 and its extensions provide a versatile, efficient solution for image generation and editing, advancing multi-modal AIGC research with open-source availability.

Abstract: Recent advancements in image generative foundation models have prioritized
quality improvements but often at the cost of increased computational
complexity and inference latency. To address this critical trade-off, we
introduce HiDream-I1, a new open-source image generative foundation model with
17B parameters that achieves state-of-the-art image generation quality within
seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer
(DiT) structure. Specifically, it starts with a dual-stream decoupled design of
sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two
separate encoders are first involved to independently process image and text
tokens. Then, a single-stream sparse DiT structure with dynamic MoE
architecture is adopted to trigger multi-model interaction for image generation
in a cost-efficient manner. To support flexiable accessibility with varied
model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full,
HiDream-I1-Dev, and HiDream-I1-Fast.
  Furthermore, we go beyond the typical text-to-image generation and remould
HiDream-I1 with additional image conditions to perform precise,
instruction-based editing on given images, yielding a new instruction-based
image editing model namely HiDream-E1. Ultimately, by integrating text-to-image
generation and instruction-based image editing, HiDream-I1 evolves to form a
comprehensive image agent (HiDream-A1) capable of fully interactive image
creation and refinement. To accelerate multi-modal AIGC research, we have
open-sourced all the codes and model weights of HiDream-I1-Full,
HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites:
https://github.com/HiDream-ai/HiDream-I1 and
https://github.com/HiDream-ai/HiDream-E1. All features can be directly
experienced via https://vivago.ai/studio.

</details>


### [209] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/pdf/2505.22762)
*Marco Colussi, Dragan Ahmetovic, Sergio Mascetti*

Main category: cs.CV

TL;DR: MIAS-SAM is a novel method for segmenting anomalies in medical images using a patch-based memory bank and SAM encoder, achieving accurate results without thresholding.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly segmentation in medical images by leveraging a memory bank of normal features and avoiding manual thresholding.

Method: Uses a patch-based memory bank with SAM encoder features for comparison, computes anomaly map, and prompts SAM decoder for segmentation.

Result: Achieves accurate anomaly segmentation on Brain MRI, Liver CT, and Retina OCT datasets, measured by DICE score.

Conclusion: MIAS-SAM provides an effective, threshold-free approach for medical anomaly segmentation with promising results.

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [210] [Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization](https://arxiv.org/pdf/2505.22792)
*Yuxi Zhang, Yueting Li, Xinyu Du, Sibo Wang*

Main category: cs.CV

TL;DR: Rhet2Pix improves text-to-image generation for rhetorical language by using a multi-step policy optimization framework with a two-layer MDP diffusion module.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with rhetorical language, generating literal visuals instead of intended semantic meanings.

Method: Rhet2Pix uses a two-layer MDP diffusion module: outer layer elaborates sub-sentences, inner layer optimizes rewards during generation.

Result: Outperforms SOTA models like GPT-4o and Grok-3 in qualitative and quantitative evaluations.

Conclusion: Rhet2Pix effectively addresses rhetorical text-to-image generation challenges, with publicly available code and dataset.

Abstract: Generating images from rhetorical languages remains a critical challenge for
text-to-image models. Even state-of-the-art (SOTA) multimodal large language
models (MLLM) fail to generate images based on the hidden meaning inherent in
rhetorical language--despite such content being readily mappable to visual
representations by humans. A key limitation is that current models emphasize
object-level word embedding alignment, causing metaphorical expressions to
steer image generation towards their literal visuals and overlook the intended
semantic meaning. To address this, we propose Rhet2Pix, a framework that
formulates rhetorical text-to-image generation as a multi-step policy
optimization problem, incorporating a two-layer MDP diffusion module. In the
outer layer, Rhet2Pix converts the input prompt into incrementally elaborated
sub-sentences and executes corresponding image-generation actions, constructing
semantically richer visuals. In the inner layer, Rhet2Pix mitigates reward
sparsity during image generation by discounting the final reward and optimizing
every adjacent action pair along the diffusion denoising trajectory. Extensive
experiments demonstrate the effectiveness of Rhet2Pix in rhetorical
text-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o,
Grok-3 and leading academic baselines across both qualitative and quantitative
evaluations. The code and dataset used in this work are publicly available.

</details>


### [211] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/pdf/2505.23268)
*Spyros Barbakos, Charalampos Antoniadis, Gerasimos Potamianos, Gianluca Setti*

Main category: cs.CV

TL;DR: A multimodal RL framework combines video frames and transcripts for better video summarization and highlight detection, outperforming visual-only methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods either use video frames or transcripts separately, but none integrate both in an RL framework for summarization and highlight detection.

Method: Proposes a multimodal pipeline using RL to fuse video frames and transcripts, rewarding diverse and representative summaries with meaningful transcript content.

Result: The approach outperforms visual-only methods by leveraging transcripts, and the unsupervised training allows scalability.

Conclusion: Integrating transcripts with video frames in an RL framework improves summarization and highlight detection, demonstrating the value of multimodal fusion.

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [212] [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/pdf/2505.22793)
*Srishti Yadav, Lauren Tilton, Maria Antoniak, Taylor Arnold, Jiaang Li, Siddhesh Milind Pawar, Antonia Karamolegkou, Stella Frank, Zhaochong An, Negar Rostamzadeh, Daniel Hershcovich, Serge Belongie, Ekaterina Shutova*

Main category: cs.CV

TL;DR: The paper argues for using foundational methodologies from visual culture studies to systematically analyze cultural dimensions in images for vision-language models (VLMs).


<details>
  <summary>Details</summary>
Motivation: VLMs often fail at cultural competency evaluations, highlighting the need for a comprehensive framework to understand cultural nuances in images.

Method: Proposes five frameworks based on cultural dimensions, derived from visual culture studies (cultural studies, semiotics, visual studies).

Result: A structured approach to identify and annotate cultural nuances in images for VLMs.

Conclusion: Foundational methodologies from visual culture studies are essential for improving the cultural competency of VLMs.

Abstract: Modern vision-language models (VLMs) often fail at cultural competency
evaluations and benchmarks. Given the diversity of applications built upon
VLMs, there is renewed interest in understanding how they encode cultural
nuances. While individual aspects of this problem have been studied, we still
lack a comprehensive framework for systematically identifying and annotating
the nuanced cultural dimensions present in images for VLMs. This position paper
argues that foundational methodologies from visual culture studies (cultural
studies, semiotics, and visual studies) are necessary for cultural analysis of
images. Building upon this review, we propose a set of five frameworks,
corresponding to cultural dimensions, that must be considered for a more
complete analysis of the cultural competencies of VLMs.

</details>


### [213] [Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features](https://arxiv.org/pdf/2505.23586)
*Ziyong Wang, Charith Abhayaratne*

Main category: cs.CV

TL;DR: A weakly-supervised approach for image manipulation detection and localization, combining activation maps and pre-trained segmentation models, achieves effective results without pixel-level labels.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of interpretability and localization in deep learning-based manipulation detection, and overcoming the limitation of missing pixel-wise annotations in real-world scenarios.

Method: Integrates activation maps from image-level detection networks (WCBnet) with segmentation maps from pre-trained models (DeepLab, SegmentAnything, PSPnet), refining with Bayesian inference.

Result: Demonstrates feasibility of localizing manipulations without pixel-level labels, achieving effective localization.

Conclusion: Proposed approach successfully addresses interpretability and localization challenges in image manipulation detection.

Abstract: The explosive growth of digital images and the widespread availability of
image editing tools have made image manipulation detection an increasingly
critical challenge. Current deep learning-based manipulation detection methods
excel in achieving high image-level classification accuracy, they often fall
short in terms of interpretability and localization of manipulated regions.
Additionally, the absence of pixel-wise annotations in real-world scenarios
limits the existing fully-supervised manipulation localization techniques. To
address these challenges, we propose a novel weakly-supervised approach that
integrates activation maps generated by image-level manipulation detection
networks with segmentation maps from pre-trained models. Specifically, we build
on our previous image-level work named WCBnet to produce multi-view feature
maps which are subsequently fused for coarse localization. These coarse maps
are then refined using detailed segmented regional information provided by
pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),
with Bayesian inference employed to enhance the manipulation localization.
Experimental results demonstrate the effectiveness of our approach,
highlighting the feasibility to localize image manipulations without relying on
pixel-level labels.

</details>


### [214] [Fast Trajectory-Independent Model-Based Reconstruction Algorithm for Multi-Dimensional Magnetic Particle Imaging](https://arxiv.org/pdf/2505.22797)
*Vladyslav Gapyak, Thomas März, Andreas Weinmann*

Main category: cs.CV

TL;DR: A trajectory-independent model-based MPI reconstruction algorithm is introduced, leveraging a zero-shot Plug-and-Play method with automatic noise estimation, demonstrating strong performance across diverse scanning scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional MPI reconstruction methods are limited by time-consuming calibration or model-based simulations tied to specific trajectories, prompting the need for a more flexible approach.

Method: The paper presents a trajectory-independent model-based MPI reconstruction algorithm using a zero-shot Plug-and-Play (PnP) method with automatic noise level estimation, employing a state-of-the-art denoiser trained on natural images.

Result: The method is evaluated on public and custom datasets, showing robust reconstruction capabilities across various scanning scenarios, including high-frequency excitation fields and partial data.

Conclusion: The proposed algorithm sets a precedent for general-purpose, flexible model-based MPI reconstruction, overcoming limitations of traditional methods.

Abstract: Magnetic Particle Imaging (MPI) is a promising tomographic technique for
visualizing the spatio-temporal distribution of superparamagnetic
nanoparticles, with applications ranging from cancer detection to real-time
cardiovascular monitoring. Traditional MPI reconstruction relies on either
time-consuming calibration (measured system matrix) or model-based simulation
of the forward operator. Recent developments have shown the applicability of
Chebyshev polynomials to multi-dimensional Lissajous Field-Free Point (FFP)
scans. This method is bound to the particular choice of sinusoidal scanning
trajectories. In this paper, we present the first reconstruction on real 2D MPI
data with a trajectory-independent model-based MPI reconstruction algorithm. We
further develop the zero-shot Plug-and-Play (PnP) algorithm of the authors --
with automatic noise level estimation -- to address the present deconvolution
problem, leveraging a state-of-the-art denoiser trained on natural images
without retraining on MPI-specific data. We evaluate our method on the publicly
available 2D FFP MPI dataset ``MPIdata: Equilibrium Model with Anisotropy",
featuring scans of six phantoms acquired using a Bruker preclinical scanner.
Moreover, we show reconstruction performed on custom data on a 2D scanner with
additional high-frequency excitation field and partial data. Our results
demonstrate strong reconstruction capabilities across different scanning
scenarios -- setting a precedent for general-purpose, flexible model-based MPI
reconstruction.

</details>


### [215] [PixelThink: Towards Efficient Chain-of-Pixel Reasoning](https://arxiv.org/pdf/2505.23727)
*Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, Xinchao Wang*

Main category: cs.CV

TL;DR: PixelThink improves reasoning efficiency in segmentation by regulating reasoning length based on task difficulty and model uncertainty, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack generalization and suffer from overthinking, leading to inefficiency and poor control over reasoning quality.

Method: PixelThink integrates task difficulty and model uncertainty to regulate reasoning generation within reinforcement learning.

Result: The approach enhances reasoning efficiency and segmentation performance, validated on the ReasonSeg-Diff benchmark.

Conclusion: PixelThink offers a novel, efficient, and interpretable solution for multimodal understanding.

Abstract: Existing reasoning segmentation approaches typically fine-tune multimodal
large language models (MLLMs) using image-text pairs and corresponding mask
labels. However, they exhibit limited generalization to out-of-distribution
scenarios without an explicit reasoning process. Although recent efforts
leverage reinforcement learning through group-relative policy optimization
(GRPO) to enhance reasoning ability, they often suffer from overthinking -
producing uniformly verbose reasoning chains irrespective of task complexity.
This results in elevated computational costs and limited control over reasoning
quality. To address this problem, we propose PixelThink, a simple yet effective
scheme that integrates externally estimated task difficulty and internally
measured model uncertainty to regulate reasoning generation within a
reinforcement learning paradigm. The model learns to compress reasoning length
in accordance with scene complexity and predictive confidence. To support
comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark
with annotated reasoning references and difficulty scores, along with a suite
of metrics designed to assess segmentation accuracy, reasoning quality, and
efficiency jointly. Experimental results demonstrate that the proposed approach
improves both reasoning efficiency and overall segmentation performance. Our
work contributes novel perspectives towards efficient and interpretable
multimodal understanding. The code and model will be publicly available.

</details>


### [216] [VidText: Towards Comprehensive Evaluation for Video Text Understanding](https://arxiv.org/pdf/2505.22810)
*Zhoufaran Yang, Yan Shu, Zhifei Yang, Yan Zhang, Yu Li, Keyang Lu, Gangyan Zeng, Shaohui Liu, Yu Zhou, Nicu Sebe*

Main category: cs.CV

TL;DR: VidText is a new benchmark for video text understanding, addressing gaps in existing benchmarks by covering diverse scenarios, multilingual content, and hierarchical tasks. Current models struggle, showing room for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook video text or are limited to static images, missing dynamic text-visual interactions. VidText aims to fill this gap.

Method: VidText introduces a hierarchical evaluation framework (video-, clip-, instance-level) and paired perception-reasoning tasks, tested on 18 LMMs.

Result: Current models perform poorly, with model-intrinsic (e.g., OCR capability) and external factors (e.g., reasoning strategies) impacting performance.

Conclusion: VidText fills a critical gap and provides a foundation for future research on multimodal reasoning with video text.

Abstract: Visual texts embedded in videos carry rich semantic information, which is
crucial for both holistic video understanding and fine-grained reasoning about
local human actions. However, existing video understanding benchmarks largely
overlook textual information, while OCR-specific benchmarks are constrained to
static images, limiting their ability to capture the interaction between text
and dynamic visual contexts. To address this gap, we propose VidText, a new
benchmark designed for comprehensive and in-depth evaluation of video text
understanding. VidText offers the following key features: 1) It covers a wide
range of real-world scenarios and supports multilingual content, encompassing
diverse settings where video text naturally appears. 2) It introduces a
hierarchical evaluation framework with video-level, clip-level, and
instance-level tasks, enabling assessment of both global summarization and
local retrieval capabilities. 3) The benchmark also introduces a set of paired
perception reasoning tasks, ranging from visual text perception to cross-modal
reasoning between textual and visual information. Extensive experiments on 18
state-of-the-art Large Multimodal Models (LMMs) reveal that current models
struggle across most tasks, with significant room for improvement. Further
analysis highlights the impact of both model-intrinsic factors, such as input
resolution and OCR capability, and external factors, including the use of
auxiliary information and Chain-of-Thought reasoning strategies. We hope
VidText will fill the current gap in video understanding benchmarks and serve
as a foundation for future research on multimodal reasoning with video text in
dynamic environments.

</details>


### [217] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/pdf/2505.23085)
*Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan is a novel architecture for accurate, temporally consistent 3D human geometry estimation from monocular videos, addressing data scarcity and metric depth challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D human geometry from videos suffer from temporal inconsistencies and lack fine-grained details, prompting the need for a better solution.

Method: GeoMan combines an image-based model for initial depth/normal estimation with a video diffusion model, simplifying the task as image-to-video generation. It uses root-relative depth for accurate human size.

Result: GeoMan achieves state-of-the-art performance, improving temporal consistency and generalizability with minimal 4D training data.

Conclusion: GeoMan effectively addresses key challenges in 3D human geometry estimation, offering superior accuracy and consistency.

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [218] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/pdf/2505.22815)
*Zhangyi Hu, Jiemin Wu, Hua Xu, Mingqian Liao, Ninghui Feng, Bo Gao, Songning Lai, Yutao Yue*

Main category: cs.CV

TL;DR: VIMTS adapts Visual MAE for Irregular Multivariate Time Series (IMTS) forecasting, handling missing data and leveraging cross-channel dependencies for accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with IMTS due to unaligned signals and missing data, while foundation models are designed for regular time series.

Method: VIMTS processes IMTS into feature patches, uses cross-channel dependencies, and applies Visual MAE for reconstruction and prediction.

Result: VIMTS outperforms existing methods and shows few-shot capability.

Conclusion: VIMTS advances visual foundation models for general time series tasks.

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [219] [Improving Contrastive Learning for Referring Expression Counting](https://arxiv.org/pdf/2505.22850)
*Kostas Triaridis, Panagiotis Kaliosis, E-Ro Nguyen, Jingyi Xu, Hieu Le, Dimitris Samaras*

Main category: cs.CV

TL;DR: C-REX is a contrastive learning framework for Referring Expression Counting (REC), addressing challenges in distinguishing visually similar objects. It outperforms prior methods by 22% in MAE and 10% in RMSE.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained REC tasks due to misalignment in image-text contrastive learning and limited negative samples.

Method: Proposes C-REX, a supervised contrastive learning framework operating in image space, ensuring stable signals and larger negative sample pools.

Result: Achieves state-of-the-art REC performance (22% MAE, 10% RMSE improvement) and strong class-agnostic counting results.

Conclusion: C-REX is effective for REC and adaptable to similar tasks, with insights on centroid detection enhancing counting models.

Abstract: Object counting has progressed from class-specific models, which count only
known categories, to class-agnostic models that generalize to unseen
categories. The next challenge is Referring Expression Counting (REC), where
the goal is to count objects based on fine-grained attributes and contextual
differences. Existing methods struggle with distinguishing visually similar
objects that belong to the same category but correspond to different referring
expressions. To address this, we propose C-REX, a novel contrastive learning
framework, based on supervised contrastive learning, designed to enhance
discriminative representation learning. Unlike prior works, C-REX operates
entirely within the image space, avoiding the misalignment issues of image-text
contrastive learning, thus providing a more stable contrastive signal. It also
guarantees a significantly larger pool of negative samples, leading to improved
robustness in the learned representations. Moreover, we showcase that our
framework is versatile and generic enough to be applied to other similar tasks
like class-agnostic counting. To support our approach, we analyze the key
components of sota detection-based models and identify that detecting object
centroids instead of bounding boxes is the key common factor behind their
success in counting tasks. We use this insight to design a simple yet effective
detection-based baseline to build upon. Our experiments show that C-REX
achieves state-of-the-art results in REC, outperforming previous methods by
more than 22\% in MAE and more than 10\% in RMSE, while also demonstrating
strong performance in class-agnostic counting. Code is available at
https://github.com/cvlab-stonybrook/c-rex.

</details>


### [220] [BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes](https://arxiv.org/pdf/2404.03161)
*Tomohiro Nishimoto, Taichi Nishimura, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura, Taiyu Cui, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Shinsuke Mori*

Main category: cs.CV

TL;DR: BioVL-QR is a new dataset for biochemical vision-and-language tasks, using Micro QR Codes and hand-object detection to improve object labeling in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Understanding biochemical videos is challenging due to cluttered environments and indistinguishable objects. Manual annotation is costly, so an automated solution is needed.

Method: Combines Micro QR Code detection with an off-the-shelf hand-object detector to label objects. Applied to localize procedural steps in instructional videos.

Result: The method improves biochemical video understanding by leveraging Micro QR Codes and hand-object detection.

Conclusion: BioVL-QR and the proposed method offer a practical solution for automated object labeling in biochemical videos.

Abstract: This paper introduces BioVL-QR, a biochemical vision-and-language dataset
comprising 23 egocentric experiment videos, corresponding protocols, and
vision-and-language alignments. A major challenge in understanding biochemical
videos is detecting equipment, reagents, and containers because of the
cluttered environment and indistinguishable objects. Previous studies assumed
manual object annotation, which is costly and time-consuming. To address the
issue, we focus on Micro QR Codes. However, detecting objects using only Micro
QR Codes is still difficult due to blur and occlusion caused by object
manipulation. To overcome this, we propose an object labeling method combining
a Micro QR Code detector with an off-the-shelf hand object detector. As an
application of the method and BioVL-QR, we tackled the task of localizing the
procedural steps in an instructional video. The experimental results show that
using Micro QR Codes and our method improves biochemical video understanding.
Data and code are available through https://nishi10mo.github.io/BioVL-QR/

</details>


### [221] [CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting](https://arxiv.org/pdf/2505.22854)
*Kornel Howil, Joanna Waczyńska, Piotr Borycki, Tadeusz Dziarmaga, Marcin Mazur, Przemysław Spurek*

Main category: cs.CV

TL;DR: CLIPGaussians introduces a unified style transfer framework for Gaussian Splatting (GS) representations, supporting text- and image-guided stylization across 2D, 3D, and 4D content without retraining.


<details>
  <summary>Details</summary>
Motivation: Style transfer for GS-based representations is challenging beyond simple color changes, prompting the need for a versatile solution.

Method: CLIPGaussians operates directly on Gaussian primitives, integrating as a plug-in module into GS pipelines, optimizing color and geometry jointly.

Result: Achieves superior style fidelity, temporal coherence in videos, and consistency across tasks while maintaining model size.

Conclusion: CLIPGaussians is a universal and efficient solution for multimodal style transfer in GS representations.

Abstract: Gaussian Splatting (GS) has recently emerged as an efficient representation
for rendering 3D scenes from 2D images and has been extended to images, videos,
and dynamic 4D content. However, applying style transfer to GS-based
representations, especially beyond simple color changes, remains challenging.
In this work, we introduce CLIPGaussians, the first unified style transfer
framework that supports text- and image-guided stylization across multiple
modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates
directly on Gaussian primitives and integrates into existing GS pipelines as a
plug-in module, without requiring large generative models or retraining from
scratch. CLIPGaussians approach enables joint optimization of color and
geometry in 3D and 4D settings, and achieves temporal coherence in videos,
while preserving a model size. We demonstrate superior style fidelity and
consistency across all tasks, validating CLIPGaussians as a universal and
efficient solution for multimodal style transfer.

</details>


### [222] [Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning](https://arxiv.org/pdf/2412.00175)
*Stefan Smeu, Dragos-Alexandru Boldisor, Dan Oneata, Elisabeta Oneata*

Main category: cs.CV

TL;DR: The paper identifies a spurious feature (leading silence) in widely used deepfake datasets, showing it biases detection models. It proposes unsupervised learning on real data to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection is safety-critical, but current datasets contain biases (leading silence) that mislead models, undermining reliability.

Method: Shift from supervised to unsupervised learning, training models exclusively on real data and aligning self-supervised audio-video representations.

Result: Models exploiting leading silence perform poorly when the feature is removed. Unsupervised learning avoids biases and enhances detection robustness.

Conclusion: Unsupervised learning on real data mitigates dataset-specific biases, improving deepfake detection reliability.

Abstract: Good datasets are essential for developing and benchmarking any machine
learning system. Their importance is even more extreme for safety critical
applications such as deepfake detection - the focus of this paper. Here we
reveal that two of the most widely used audio-video deepfake datasets suffer
from a previously unidentified spurious feature: the leading silence. Fake
videos start with a very brief moment of silence and based on this feature
alone, we can separate the real and fake samples almost perfectly. As such,
previous audio-only and audio-video models exploit the presence of silence in
the fake videos and consequently perform worse when the leading silence is
removed. To circumvent latching on such unwanted artifact and possibly other
unrevealed ones we propose a shift from supervised to unsupervised learning by
training models exclusively on real data. We show that by aligning
self-supervised audio-video representations we remove the risk of relying on
dataset-specific biases and improve robustness in deepfake detection.

</details>


### [223] [A Probabilistic Jump-Diffusion Framework for Open-World Egocentric Activity Recognition](https://arxiv.org/pdf/2505.22858)
*Sanjoy Kundu, Shanmukha Vellamcheti, Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: ProbRes, a probabilistic residual search framework, efficiently navigates open-world egocentric activity recognition by balancing exploration and exploitation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of inferring unseen activities in unconstrained, open-world egocentric scenarios.

Method: Uses jump-diffusion, commonsense priors, Vision-Language Models (VLMs), and stochastic search to refine predictions.

Result: Achieves top performance on benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, Charades-Ego) and establishes a taxonomy for open-world recognition.

Conclusion: ProbRes advances egocentric activity understanding by efficiently navigating complex search spaces and setting benchmarks for future work.

Abstract: Open-world egocentric activity recognition poses a fundamental challenge due
to its unconstrained nature, requiring models to infer unseen activities from
an expansive, partially observed search space. We introduce ProbRes, a
Probabilistic Residual search framework based on jump-diffusion that
efficiently navigates this space by balancing prior-guided exploration with
likelihood-driven exploitation. Our approach integrates structured commonsense
priors to construct a semantically coherent search space, adaptively refines
predictions using Vision-Language Models (VLMs) and employs a stochastic search
mechanism to locate high-likelihood activity labels while minimizing exhaustive
enumeration efficiently. We systematically evaluate ProbRes across multiple
openness levels (L0--L3), demonstrating its adaptability to increasing search
space complexity. In addition to achieving state-of-the-art performance on
benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we
establish a clear taxonomy for open-world recognition, delineating the
challenges and methodological advancements necessary for egocentric activity
understanding.

</details>


### [224] [4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians](https://arxiv.org/pdf/2505.22859)
*Hidenobu Matsuki, Gwangbin Bae, Andrew J. Davison*

Main category: cs.CV

TL;DR: A novel 4D-SLAM method for joint camera localization and non-rigid surface reconstruction using differentiable rendering, Gaussian surface primitives, and MLP warp-fields, validated with a new synthetic dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored challenges of 4D-SLAM, such as high-dimensional optimization and lack of ground truth, to enable accurate spatio-temporal reconstruction of dynamic scenes.

Method: Uses Gaussian surface primitives for depth signal optimization, MLP warp-fields for non-rigid deformations, and introduces a new camera pose estimation technique with surface regularization.

Result: Achieves accurate 4D scene reconstruction and camera localization, validated by a novel synthetic dataset of diverse motions.

Conclusion: Advances 4D-SLAM research with a robust method and evaluation protocols, leveraging modern vision and rendering techniques.

Abstract: We propose the first 4D tracking and mapping method that jointly performs
camera localization and non-rigid surface reconstruction via differentiable
rendering. Our approach captures 4D scenes from an online stream of color
images with depth measurements or predictions by jointly optimizing scene
geometry, appearance, dynamics, and camera ego-motion. Although natural
environments exhibit complex non-rigid motions, 4D-SLAM remains relatively
underexplored due to its inherent challenges; even with 2.5D signals, the
problem is ill-posed because of the high dimensionality of the optimization
space. To overcome these challenges, we first introduce a SLAM method based on
Gaussian surface primitives that leverages depth signals more effectively than
3D Gaussians, thereby achieving accurate surface reconstruction. To further
model non-rigid deformations, we employ a warp-field represented by a
multi-layer perceptron (MLP) and introduce a novel camera pose estimation
technique along with surface regularization terms that facilitate
spatio-temporal reconstruction. In addition to these algorithmic challenges, a
significant hurdle in 4D SLAM research is the lack of reliable ground truth and
evaluation protocols, primarily due to the difficulty of 4D capture using
commodity sensors. To address this, we present a novel open synthetic dataset
of everyday objects with diverse motions, leveraging large-scale object models
and animation modeling. In summary, we open up the modern 4D-SLAM research by
introducing a novel method and evaluation protocols grounded in modern vision
and rendering techniques.

</details>


### [225] [Audio Visual Segmentation Through Text Embeddings](https://arxiv.org/pdf/2502.16359)
*Kyungbok Lee, You Zhang, Zhiyao Duan*

Main category: cs.CV

TL;DR: AV2T-SAM improves Audio-Visual Segmentation by bridging audio features with text-prompted SAM, leveraging multimodal correspondence and a novel feature for better alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning audio-visual correspondence with limited data, which existing methods fail to solve despite leveraging SAM.

Method: Proposes AV2T-SAM, integrating audio features with SAM's text embedding space and introducing a novel feature (f_CLIP ⊙ f_CLAP) for enhanced alignment.

Result: Outperforms existing methods on the AVSBench dataset by improving cross-modal semantic alignment.

Conclusion: AV2T-SAM effectively utilizes pre-trained models and cross-modal alignment to advance AVS performance.

Abstract: The goal of Audio-Visual Segmentation (AVS) is to localize and segment the
sounding source objects from video frames. Research on AVS suffers from data
scarcity due to the high cost of fine-grained manual annotations. Recent works
attempt to overcome the challenge of limited data by leveraging the vision
foundation model, Segment Anything Model (SAM), prompting it with audio to
enhance its ability to segment sounding source objects. While this approach
alleviates the model's burden on understanding visual modality by utilizing
knowledge of pre-trained SAM, it does not address the fundamental challenge of
learning audio-visual correspondence with limited data. To address this
limitation, we propose \textbf{AV2T-SAM}, a novel framework that bridges audio
features with the text embedding space of pre-trained text-prompted SAM. Our
method leverages multimodal correspondence learned from rich text-image paired
datasets to enhance audio-visual alignment. Furthermore, we introduce a novel
feature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot
\textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and
visual modalities while filtering irrelevant noise. Our approach outperforms
existing methods on the AVSBench dataset by effectively utilizing pre-trained
segmentation models and cross-modal semantic alignment. The source code is
released at https://github.com/bok-bok/AV2T-SAM.

</details>


### [226] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/pdf/2505.22869)
*Junbo Yin, Chao Zha, Wenjia He, Chencheng Xu, Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen is a diffusion language model for combinatorial protein generation, integrating multimodal constraints for de novo protein design.


<details>
  <summary>Details</summary>
Motivation: Existing PLMs struggle with multiple constraints across modalities; CFP-Gen addresses this gap.

Method: Uses Annotation-Guided Feature Modulation (AGFM) and Residue-Controlled Functional Encoding (RCFE) to integrate functional, sequence, and structural constraints.

Result: Generates novel proteins with natural-like functionality and high success in multifunctional designs.

Conclusion: CFP-Gen advances protein design by efficiently handling multimodal constraints.

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [227] [Interpreting the linear structure of vision-language model embedding spaces](https://arxiv.org/pdf/2504.11695)
*Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil*

Main category: cs.CV

TL;DR: The paper investigates the organization of language and images in vision-language models (VLMs) using sparse autoencoders (SAEs), revealing stable cross-modal concepts and introducing the Bridge Score to quantify their integration.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs encode meaning and modality in their joint embedding space and explore the structure of this space.

Method: Train and analyze SAEs on four VLMs (CLIP, SigLIP, SigLIP2, AIMv2) to identify sparse linear concepts and their stability across runs. Introduce the Bridge Score to measure cross-modal integration.

Result: SAEs outperform other methods in reconstruction and sparsity. Key concepts are stable across runs, and many encode cross-modal semantics despite unimodal activation. The Bridge Score highlights latent bridges for cross-modal integration.

Conclusion: VLMs exhibit a sparse linear structure shaped by modality but connected through latent bridges, providing new insights into multimodal meaning construction.

Abstract: Vision-language models encode images and text in a joint space, minimizing
the distance between corresponding image and text pairs. How are language and
images organized in this joint space, and how do the models encode meaning and
modality? To investigate this, we train and release sparse autoencoders (SAEs)
on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,
and AIMv2). SAEs approximate model embeddings as sparse linear combinations of
learned directions, or "concepts". We find that, compared to other methods of
linear feature learning, SAEs are better at reconstructing the real embeddings,
while also able to retain the most sparsity. Retraining SAEs with different
seeds or different data diet leads to two findings: the rare, specific concepts
captured by the SAEs are liable to change drastically, but we also show that
the key commonly-activating concepts extracted by SAEs are remarkably stable
across runs. Interestingly, while most concepts are strongly unimodal in
activation, we find they are not merely encoding modality per se. Many lie
close to - but not entirely within - the subspace defining modality, suggesting
that they encode cross-modal semantics despite their unimodal usage. To
quantify this bridging behavior, we introduce the Bridge Score, a metric that
identifies concept pairs which are both co-activated across aligned image-text
inputs and geometrically aligned in the shared space. This reveals that even
unimodal concepts can collaborate to support cross-modal integration. We
release interactive demos of the SAEs for all models, allowing researchers to
explore the organization of the concept spaces. Overall, our findings uncover a
sparse linear structure within VLM embedding spaces that is shaped by modality,
yet stitched together through latent bridges-offering new insight into how
multimodal meaning is constructed.

</details>


### [228] [3DGS Compression with Sparsity-guided Hierarchical Transform Coding](https://arxiv.org/pdf/2505.22908)
*Hao Xu, Xiaolin Wu, Xi Zhang*

Main category: cs.CV

TL;DR: SHTC is an end-to-end optimized transform coding framework for 3DGS compression, improving rate-distortion performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: 3DGS has a large memory footprint, and existing neural compression methods lack efficient analysis-synthesis transforms, leading to suboptimal performance.

Method: SHTC jointly optimizes 3DGS, transforms, and a lightweight context model, using KLT for decorrelation and sparsity-coded residuals for refinement.

Result: SHTC achieves superior rate-distortion performance with fewer parameters and computational overhead.

Conclusion: SHTC provides an interpretable, efficient solution for 3DGS compression, outperforming existing methods.

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its fast and
high-quality rendering, but it has a very large memory footprint incurring high
transmission and storage overhead. Recently, some neural compression methods,
such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach
of end-to-end optimized analysis-synthesis transforms which has been proven
highly effective in neural signal compression. Without an appropriate analysis
transform, signal correlations cannot be removed by sparse representation.
Without such transforms the only way to remove signal redundancies is through
entropy coding driven by a complex and expensive context modeling, which
results in slower speed and suboptimal rate-distortion (R-D) performance. To
overcome this weakness, we propose Sparsity-guided Hierarchical Transform
Coding (SHTC), the first end-to-end optimized transform coding framework for
3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight
context model. This joint optimization enables the transform to produce
representations that approach the best R-D performance possible. The SHTC
framework consists of a base layer using KLT for data decorrelation, and a
sparsity-coded enhancement layer that compresses the KLT residuals to refine
the representation. The enhancement encoder learns a linear transform to
project high-dimensional inputs into a low-dimensional space, while the decoder
unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct
the residuals. All components are designed to be interpretable, allowing the
incorporation of signal priors and fewer parameters than black-box transforms.
This novel design significantly improves R-D performance with minimal
additional parameters and computational overhead.

</details>


### [229] [Hierarchical Material Recognition from Local Appearance](https://arxiv.org/pdf/2505.22911)
*Matthew Beveridge, Shree K. Nayar*

Main category: cs.CV

TL;DR: A taxonomy for hierarchical material recognition is introduced, supported by a dataset and a graph attention network method, achieving top performance and generalization to real-world conditions.


<details>
  <summary>Details</summary>
Motivation: To address hierarchical material recognition by leveraging physical traits of materials and improving performance in diverse, real-world conditions.

Method: A graph attention network-based approach utilizing taxonomic proximity and depth maps for hierarchical material recognition.

Result: State-of-the-art performance, generalization to adverse conditions, and enhanced capability with rendered views. Few-shot learning for new materials is demonstrated.

Conclusion: The taxonomy, dataset, and method advance material recognition, showing robustness and adaptability in real-world scenarios.

Abstract: We introduce a taxonomy of materials for hierarchical recognition from local
appearance. Our taxonomy is motivated by vision applications and is arranged
according to the physical traits of materials. We contribute a diverse,
in-the-wild dataset with images and depth maps of the taxonomy classes.
Utilizing the taxonomy and dataset, we present a method for hierarchical
material recognition based on graph attention networks. Our model leverages the
taxonomic proximity between classes and achieves state-of-the-art performance.
We demonstrate the model's potential to generalize to adverse, real-world
imaging conditions, and that novel views rendered using the depth maps can
enhance this capability. Finally, we show the model's capacity to rapidly learn
new materials in a few-shot learning setting.

</details>


### [230] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/pdf/2505.22914)
*Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich*

Main category: cs.CV

TL;DR: A multi-modal CAD reconstruction model using vision-language models (VLM) and reinforcement learning (RL) outperforms single-modal methods and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To democratize CAD design by leveraging multi-modal inputs (point clouds, images, text) for better generalizability and robustness, overcoming limitations of single-modal approaches.

Method: A two-stage pipeline: supervised fine-tuning (SFT) on procedurally generated data, followed by RL fine-tuning using online feedback (GRPO).

Result: The SFT model outperforms single-modal methods, and RL fine-tuning sets new state-of-the-art on three datasets, including real-world data.

Conclusion: Multi-modal CAD reconstruction with RL fine-tuning is highly effective, demonstrating superior performance and potential for broader applications.

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [231] [Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape](https://arxiv.org/pdf/2505.22918)
*Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu*

Main category: cs.CV

TL;DR: Re-ttention reduces attention complexity in Diffusion Transformers (DiT) by leveraging temporal redundancy, enabling high sparsity without quality loss.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of attention in DiTs limits scalability for high-resolution and long videos. Existing sparse attention methods degrade quality or add overhead.

Method: Re-ttention reshapes attention scores using prior softmax distribution history to maintain quality at high sparsity (e.g., 3.1% tokens).

Result: Achieves 45% end-to-end and 92% self-attention latency reduction on H100 GPU, outperforming FastDiTAttn, Sparse VideoGen, and MInference.

Conclusion: Re-ttention enables efficient high-quality visual generation with minimal compute overhead.

Abstract: Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. % To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. % Experimental
results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate
that Re-ttention requires as few as 3.1\% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference. Further, we measure latency to show that our method can attain over
45\% end-to-end % and over 92\% self-attention latency reduction on an H100 GPU
at negligible overhead cost.
  Code available online here:
\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}

</details>


### [232] [Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification](https://arxiv.org/pdf/2505.22926)
*Sylvey Lin, Zhi-Yi Cao*

Main category: cs.CV

TL;DR: Synthetic images from diffusion models show promise for multi-label protein classification but struggle with generalization, while traditional methods remain more stable.


<details>
  <summary>Details</summary>
Motivation: To explore if synthetic images from diffusion models can improve multi-label classification of protein subcellular localization.

Method: Implemented a simplified DDPM for label-consistent synthetic images and tested hybrid training strategies (Mix Loss, Mix Representation).

Result: Validation performance was promising, but MixModel generalized poorly to unseen test data; ResNet baselines outperformed.

Conclusion: Realistic data generation and robust supervision are crucial for effective generative augmentation in biomedical image classification.

Abstract: We investigate whether synthetic images generated by diffusion models can
enhance multi-label classification of protein subcellular localization.
Specifically, we implement a simplified class-conditional denoising diffusion
probabilistic model (DDPM) to produce label-consistent samples and explore
their integration with real data via two hybrid training strategies: Mix Loss
and Mix Representation. While these approaches yield promising validation
performance, our proposed MixModel exhibits poor generalization to unseen test
data, underscoring the challenges of leveraging synthetic data effectively. In
contrast, baseline classifiers built on ResNet backbones with conventional loss
functions demonstrate greater stability and test-time performance. Our findings
highlight the importance of realistic data generation and robust supervision
when incorporating generative augmentation into biomedical image
classification.

</details>


### [233] [Fast Isotropic Median Filtering](https://arxiv.org/pdf/2505.22938)
*Ben Weiss*

Main category: cs.CV

TL;DR: A new method for median filtering overcomes limitations of bit depth, kernel size, and shape, including circular kernels.


<details>
  <summary>Details</summary>
Motivation: Existing median filtering algorithms are limited by bit depth, kernel size, or shape, often producing artifacts like streaky cross-hatching.

Method: The proposed method efficiently handles arbitrary bit depths, kernel sizes, and convex kernel shapes, including circular ones.

Result: The method successfully addresses the limitations of previous algorithms, enabling more versatile and artifact-free median filtering.

Conclusion: This breakthrough allows for broader and more effective use of median filtering in image processing.

Abstract: Median filtering is a cornerstone of computational image processing. It
provides an effective means of image smoothing, with minimal blurring or
softening of edges, invariance to monotonic transformations such as gamma
adjustment, and robustness to noise and outliers. However, known algorithms
have all suffered from practical limitations: the bit depth of the image data,
the size of the filter kernel, or the kernel shape itself. Square-kernel
implementations tend to produce streaky cross-hatching artifacts, and nearly
all known efficient algorithms are in practice limited to square kernels. We
present for the first time a method that overcomes all of these limitations.
Our method operates efficiently on arbitrary bit-depth data, arbitrary kernel
sizes, and arbitrary convex kernel shapes, including circular shapes.

</details>


### [234] [QMamba: On First Exploration of Vision Mamba for Image Quality Assessment](https://arxiv.org/pdf/2406.09546)
*Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen*

Main category: cs.CV

TL;DR: QMamba explores the State Space Model/Mamba for image quality assessment (IQA), outperforming models like Swin Transformer and ViT in perception and efficiency. StylePrompt tuning enhances transferability.


<details>
  <summary>Details</summary>
Motivation: To investigate the perception potential of Mamba in IQA, as its capabilities in vision tasks remain under-explored.

Method: Proposes QMamba by adapting Mamba for IQA tasks and introduces StylePrompt for lightweight task-adaptive transfer learning.

Result: QMamba shows clear advantages over existing models in perception and computational cost, validated on multiple datasets.

Conclusion: QMamba, enhanced by StylePrompt, is effective for IQA tasks, offering better performance and efficiency.

Abstract: In this work, we take the first exploration of the recently popular
foundation model, i.e., State Space Model/Mamba, in image quality assessment
(IQA), aiming at observing and excavating the perception potential in vision
Mamba. A series of works on Mamba has shown its significant potential in
various fields, e.g., segmentation and classification. However, the perception
capability of Mamba remains under-explored. Consequently, we propose QMamba by
revisiting and adapting the Mamba model for three crucial IQA tasks, i.e.,
task-specific, universal, and transferable IQA, which reveals its clear
advantages over existing foundational models, e.g., Swin Transformer, ViT, and
CNNs, in terms of perception and computational cost. To improve the
transferability of QMamba, we propose the StylePrompt tuning paradigm, where
lightweight mean and variance prompts are injected to assist task-adaptive
transfer learning of pre-trained QMamba for different downstream IQA tasks.
Compared with existing prompt tuning strategies, our StylePrompt enables better
perceptual transfer with lower computational cost. Extensive experiments on
multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate
the effectiveness of our proposed QMamba. The code will be available at:
https://github.com/bingo-G/QMamba.git

</details>


### [235] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/pdf/2505.22944)
*Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma*

Main category: cs.CV

TL;DR: A unified framework for motion control in video generation integrates camera movement, object translation, and local motion using trajectory inputs, outperforming prior methods in controllability and quality.


<details>
  <summary>Details</summary>
Motivation: Prior methods handle motion types separately or with task-specific designs, lacking cohesion. This work aims for a unified solution.

Method: Projects user-defined trajectories into latent space of pre-trained models via a lightweight motion injector, guiding generative process.

Result: Superior performance in tasks like stylized motion effects, viewpoint changes, and local motion manipulation.

Conclusion: The framework offers better controllability and visual quality, compatible with various video generation backbones.

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [236] [Epsilon-VAE: Denoising as Visual Decoding](https://arxiv.org/pdf/2410.04081)
*Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, Ting Liu*

Main category: cs.CV

TL;DR: The paper proposes Epsilon-VAE, a novel autoencoder using diffusion for iterative refinement, improving reconstruction and generation quality over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current visual tokenization methods rely on single-step reconstruction, limiting efficiency and quality. The authors aim to enhance this by integrating iterative refinement via diffusion.

Method: Replace the traditional decoder with a diffusion process for iterative refinement of noise into the original image, guided by encoder latents.

Result: Epsilon-VAE achieves 22% better generation quality and 2.3x faster inference at the same compression rates compared to state-of-the-art autoencoders.

Conclusion: The work demonstrates the benefits of combining iterative generation with autoencoding, offering improved compression and generation insights.

Abstract: In generative modeling, tokenization simplifies complex data into compact,
structured representations, creating a more efficient, learnable space. For
high-dimensional visual data, it reduces redundancy and emphasizes key features
for high-quality generation. Current visual tokenization methods rely on a
traditional autoencoder framework, where the encoder compresses data into
latent representations, and the decoder reconstructs the original input. In
this work, we offer a new perspective by proposing denoising as decoding,
shifting from single-step reconstruction to iterative refinement. Specifically,
we replace the decoder with a diffusion process that iteratively refines noise
to recover the original image, guided by the latents provided by the encoder.
We evaluate our approach by assessing both reconstruction (rFID) and generation
quality (FID), comparing it to state-of-the-art autoencoding approaches. By
adopting iterative reconstruction through diffusion, our autoencoder, namely
Epsilon-VAE, achieves high reconstruction quality, which in turn enhances
downstream generation quality by 22% at the same compression rates or provides
2.3x inference speedup through increasing compression rates. We hope this work
offers new insights into integrating iterative generation and autoencoding for
improved compression and generation.

</details>


### [237] [Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion](https://arxiv.org/pdf/2504.10974)
*Zhisheng Zhang, Peng Zhang, Fengxiang Wang, Liangli Ma, Fuchun Sun*

Main category: cs.CV

TL;DR: A feature-space transformation and self-supervised multi-frame fusion method enhances sonar images, outperforming existing approaches in noise suppression, edge preservation, and brightness improvement.


<details>
  <summary>Details</summary>
Motivation: The difficulty in obtaining high-quality real-world paired data for supervised training and the limitations of transferring pretrained weights from remote sensing to sonar images motivate the need for a specialized solution.

Method: Proposes a feature-space transformation to map sonar images to a robust feature domain and a self-supervised multi-frame fusion strategy to leverage inter-frame information for noise removal and brightness enhancement.

Result: Outperforms existing methods on real-world datasets, effectively suppressing noise, preserving edges, and improving brightness.

Conclusion: The method demonstrates strong potential for underwater target detection by addressing the degradation gap and enhancing sonar image quality.

Abstract: Enhancing forward-looking sonar images is critical for accurate underwater
target detection. Current deep learning methods mainly rely on supervised
training with simulated data, but the difficulty in obtaining high-quality
real-world paired data limits their practical use and generalization. Although
self-supervised approaches from remote sensing partially alleviate data
shortages, they neglect the cross-modal degradation gap between sonar and
remote sensing images. Directly transferring pretrained weights often leads to
overly smooth sonar images, detail loss, and insufficient brightness. To
address this, we propose a feature-space transformation that maps sonar images
from the pixel domain to a robust feature domain, effectively bridging the
degradation gap. Additionally, our self-supervised multi-frame fusion strategy
leverages complementary inter-frame information to naturally remove speckle
noise and enhance target-region brightness. Experiments on three self-collected
real-world forward-looking sonar datasets show that our method significantly
outperforms existing approaches, effectively suppressing noise, preserving
detailed edges, and substantially improving brightness, demonstrating strong
potential for underwater target detection applications.

</details>


### [238] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/pdf/2505.22976)
*Kewei Lian, Shaofei Cai, Yilun Du, Yitao Liang*

Main category: cs.CV

TL;DR: The paper introduces a dataset and benchmark for training spatially consistent world models, using Minecraft navigation videos to enforce long-range spatial consistency.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack explicit spatial consistency constraints, focusing instead on visual coherence or generation quality, which limits the development of effective world models.

Method: A dataset of 150 Minecraft locations with 250 hours of loop-based navigation videos is created, featuring a curriculum design for sequence lengths. Four world model baselines are evaluated.

Result: The dataset and benchmark are open-sourced, enabling future research on spatially consistent world models.

Conclusion: The work bridges the gap in datasets for spatial consistency, providing a scalable and extensible solution for training and evaluating world models.

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [239] [Stereo Radargrammetry Using Deep Learning from Airborne SAR Images](https://arxiv.org/pdf/2505.20876)
*Tatsuya Sasayama, Shintaro Ito, Koichi Ito, Takafumi Aoki*

Main category: cs.CV

TL;DR: A deep learning-based stereo radargrammetry method for airborne SAR images is proposed, addressing the lack of public datasets and improving elevation measurement accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for SAR images lack public datasets and suffer from geometric modulation issues, prompting the creation of a new dataset and method.

Method: The method involves creating a SAR dataset, fine-tuning a deep learning-based correspondence method, and processing images in patches without ground projection.

Result: Experiments show the method achieves wider range and more accurate elevation measurements than conventional techniques.

Conclusion: The proposed method effectively addresses SAR image processing challenges and outperforms traditional approaches.

Abstract: In this paper, we propose a stereo radargrammetry method using deep learning
from airborne Synthetic Aperture Radar (SAR) images. Deep learning-based
methods are considered to suffer less from geometric image modulation, while
there is no public SAR image dataset used to train such methods. We create a
SAR image dataset and perform fine-tuning of a deep learning-based image
correspondence method. The proposed method suppresses the degradation of image
quality by pixel interpolation without ground projection of the SAR image and
divides the SAR image into patches for processing, which makes it possible to
apply deep learning. Through a set of experiments, we demonstrate that the
proposed method exhibits a wider range and more accurate elevation measurements
compared to conventional methods. The project web page is available at:
https://gsisaoki.github.io/IGARSS2025_sasayama/

</details>


### [240] [HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions](https://arxiv.org/pdf/2505.22977)
*Shuolin Xu, Siming Zheng, Ziyi Wang, HC Yu, Jinwei Chen, Huaqi Zhang, Bo Li, Peng-Tao Jiang*

Main category: cs.CV

TL;DR: The paper introduces a dataset and benchmark for complex human motion animation, proposes a DiT-based video generation method with a novel spatial low-frequency enhancement module, and demonstrates improved performance in dynamic motion sequences.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex human motions (Hypermotion) and lack high-quality benchmarks for evaluation.

Method: Proposes a DiT-based video generation baseline with a spatial low-frequency enhanced RoPE module to enhance feature modeling.

Result: The method improves structural stability and appearance consistency in dynamic motion sequences.

Conclusion: The introduced dataset and method advance the quality of complex human motion animations, with code and dataset to be made public.

Abstract: Recent advances in diffusion models have significantly improved conditional
video generation, particularly in the pose-guided human image animation task.
Although existing methods are capable of generating high-fidelity and
time-consistent animation sequences in regular motions and static scenes, there
are still obvious limitations when facing complex human body motions
(Hypermotion) that contain highly dynamic, non-standard motions, and the lack
of a high-quality benchmark for evaluation of complex human motion animations.
To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset}
and \textbf{HyperMotionX Bench}, which provide high-quality human pose
annotations and curated video clips for evaluating and improving pose-guided
human image animation models under complex human motion conditions.
Furthermore, we propose a simple yet powerful DiT-based video generation
baseline and design spatial low-frequency enhanced RoPE, a novel module that
selectively enhances low-frequency spatial feature modeling by introducing
learnable frequency scaling. Our method significantly improves structural
stability and appearance consistency in highly dynamic human motion sequences.
Extensive experiments demonstrate the effectiveness of our dataset and proposed
approach in advancing the generation quality of complex human motion image
animations. Code and dataset will be made publicly available.

</details>


### [241] [Pose-free 3D Gaussian splatting via shape-ray estimation](https://arxiv.org/pdf/2505.22978)
*Youngju Na, Taeyeon Kim, Jumin Lee, Kyu Beom Han, Woo Jae Kim, Sung-eui Yoon*

Main category: cs.CV

TL;DR: SHARE is a pose-free Gaussian splatting framework that jointly estimates shape and camera rays, reducing reliance on precise camera poses for accurate geometry.


<details>
  <summary>Details</summary>
Motivation: Accurate camera poses are hard to obtain in real-world scenarios, leading to noisy estimates and geometric misalignments in 3D Gaussian splatting.

Method: SHARE uses a pose-aware canonical volume representation and anchor-aligned Gaussian prediction to integrate multi-view information and refine local geometry.

Result: The method achieves robust performance in pose-free generalizable Gaussian splatting on diverse real-world datasets.

Conclusion: SHARE effectively addresses pose ambiguities and improves scene reconstruction without relying on precise camera poses.

Abstract: While generalizable 3D Gaussian splatting enables efficient, high-quality
rendering of unseen scenes, it heavily depends on precise camera poses for
accurate geometry. In real-world scenarios, obtaining accurate poses is
challenging, leading to noisy pose estimates and geometric misalignments. To
address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting
framework that overcomes these ambiguities by joint shape and camera rays
estimation. Instead of relying on explicit 3D transformations, SHARE builds a
pose-aware canonical volume representation that seamlessly integrates
multi-view information, reducing misalignment caused by inaccurate pose
estimates. Additionally, anchor-aligned Gaussian prediction enhances scene
reconstruction by refining local geometry around coarse anchors, allowing for
more precise Gaussian placement. Extensive experiments on diverse real-world
datasets show that our method achieves robust performance in pose-free
generalizable Gaussian splatting.

</details>


### [242] [MOVi: Training-free Text-conditioned Multi-Object Video Generation](https://arxiv.org/pdf/2505.22980)
*Aimon Rahman, Jiang Liu, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Yusheng Su, Vishal M. Patel, Zicheng Liu, Emad Barsoum*

Main category: cs.CV

TL;DR: A training-free method improves multi-object video generation by leveraging LLMs for trajectory control and refining attention mechanisms, achieving 42% better accuracy and motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based T2V models struggle with multi-object generation, often failing to capture complex interactions or distinct object features.

Method: Uses an LLM as a 'director' for object trajectories and noise re-initialization, while refining attention mechanisms to avoid cross-object interference.

Result: 42% absolute improvement in motion dynamics and object generation accuracy, with high fidelity and smoothness.

Conclusion: The approach effectively enhances multi-object video generation without additional training, addressing key limitations of current models.

Abstract: Recent advances in diffusion-based text-to-video (T2V) models have
demonstrated remarkable progress, but these models still face challenges in
generating videos with multiple objects. Most models struggle with accurately
capturing complex object interactions, often treating some objects as static
background elements and limiting their movement. In addition, they often fail
to generate multiple distinct objects as specified in the prompt, resulting in
incorrect generations or mixed features across objects. In this paper, we
present a novel training-free approach for multi-object video generation that
leverages the open world knowledge of diffusion models and large language
models (LLMs). We use an LLM as the ``director'' of object trajectories, and
apply the trajectories through noise re-initialization to achieve precise
control of realistic movements. We further refine the generation process by
manipulating the attention mechanism to better capture object-specific features
and motion patterns, and prevent cross-object feature interference. Extensive
experiments validate the effectiveness of our training free approach in
significantly enhancing the multi-object generation capabilities of existing
video diffusion models, resulting in 42% absolute improvement in motion
dynamics and object generation accuracy, while also maintaining high fidelity
and motion smoothness.

</details>


### [243] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/pdf/2505.23008)
*Jonathan Li, Zoltan Csaki, Nidhi Hiremath, Etash Guha, Fenglu Hong, Edward Ma, Urmish Thakker*

Main category: cs.CV

TL;DR: The paper introduces HuDocVQA and HuDocVQA-manual, datasets for Hungarian document VQA, addressing the lack of resources in lower-resource languages. It also presents HuCCPDF for OCR training and shows finetuning improves accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of document VQA in lower-resource languages like Hungarian due to limited training and evaluation data.

Method: Creation of HuDocVQA (synthetic) and HuDocVQA-manual (manual) datasets, along with HuCCPDF for OCR. Quality filtering and deduplication are applied.

Result: Finetuning on these datasets improves Llama 3.2 11B Instruct's accuracy on HuDocVQA by +7.2%.

Conclusion: The datasets and code will be released to support multilingual DocVQA research.

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [244] [SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model](https://arxiv.org/pdf/2505.23010)
*Bowen Chen, Keyan Chen, Mohan Yang, Zhengxia Zou, Zhenwei Shi*

Main category: cs.CV

TL;DR: The paper proposes SeG-SR, a semantic-guided super-resolution framework for remote sensing images, leveraging vision-language models to improve resolution by incorporating high-level semantic knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing RSISR methods focus on low-level pixel characteristics, neglecting high-level semantic understanding, leading to artifacts. This work explores semantic knowledge's role in enhancing RSISR.

Method: SeG-SR uses a pretrained VLM for semantic feature extraction, a Semantic Localization Module for guidance, and a Learnable Modulation Module to integrate semantics into the SR process.

Result: SeG-SR achieves state-of-the-art performance on two datasets and improves various SR architectures.

Conclusion: Incorporating high-level semantic knowledge via SeG-SR significantly enhances remote sensing image super-resolution, offering a robust and generalizable solution.

Abstract: High-resolution (HR) remote sensing imagery plays a vital role in a wide
range of applications, including urban planning and environmental monitoring.
However, due to limitations in sensors and data transmission links, the images
acquired in practice often suffer from resolution degradation. Remote Sensing
Image Super-Resolution (RSISR) aims to reconstruct HR images from
low-resolution (LR) inputs, providing a cost-effective and efficient
alternative to direct HR image acquisition. Existing RSISR methods primarily
focus on low-level characteristics in pixel space, while neglecting the
high-level understanding of remote sensing scenes. This may lead to
semantically inconsistent artifacts in the reconstructed results. Motivated by
this observation, our work aims to explore the role of high-level semantic
knowledge in improving RSISR performance. We propose a Semantic-Guided
Super-Resolution framework, SeG-SR, which leverages Vision-Language Models
(VLMs) to extract semantic knowledge from input images and uses it to guide the
super resolution (SR) process. Specifically, we first design a Semantic Feature
Extraction Module (SFEM) that utilizes a pretrained VLM to extract semantic
knowledge from remote sensing images. Next, we propose a Semantic Localization
Module (SLM), which derives a series of semantic guidance from the extracted
semantic knowledge. Finally, we develop a Learnable Modulation Module (LMM)
that uses semantic guidance to modulate the features extracted by the SR
network, effectively incorporating high-level scene understanding into the SR
pipeline. We validate the effectiveness and generalizability of SeG-SR through
extensive experiments: SeG-SR achieves state-of-the-art performance on two
datasets and consistently delivers performance improvements across various SR
architectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.

</details>


### [245] [Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition](https://arxiv.org/pdf/2505.23012)
*Shanaka Ramesh Gunasekara, Wanqing Li, Philip Ogunbona, Jack Yang*

Main category: cs.CV

TL;DR: The paper introduces a novel measurement, STJD, to quantify the interaction between moving and static skeletal joints for action classification. It proposes STJD-CL and STJD-MP methods, improving performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To leverage the discriminative potential of interactions between moving and static skeletal joints, which is often overlooked in traditional approaches.

Method: Introduces STJD to quantify joint interactions, identifies 'prime joints,' and proposes STJD-CL (contrastive learning) and STJD-MP (reconstruction-based framework).

Result: Improved performance by 3.5 and 3.6 percentage points on NTU RGB+D 120 dataset over state-of-the-art methods.

Conclusion: The proposed methods effectively harness the interaction between moving and static joints, enhancing action classification performance.

Abstract: Traditional approaches in unsupervised or self supervised learning for
skeleton-based action classification have concentrated predominantly on the
dynamic aspects of skeletal sequences. Yet, the intricate interaction between
the moving and static elements of the skeleton presents a rarely tapped
discriminative potential for action classification. This paper introduces a
novel measurement, referred to as spatial-temporal joint density (STJD), to
quantify such interaction. Tracking the evolution of this density throughout an
action can effectively identify a subset of discriminative moving and/or static
joints termed "prime joints" to steer self-supervised learning. A new
contrastive learning strategy named STJD-CL is proposed to align the
representation of a skeleton sequence with that of its prime joints while
simultaneously contrasting the representations of prime and nonprime joints. In
addition, a method called STJD-MP is developed by integrating it with a
reconstruction-based framework for more effective learning. Experimental
evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various
downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved
performance, particularly by 3.5 and 3.6 percentage points over the
state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub
and X-set evaluations, respectively.

</details>


### [246] [Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions](https://arxiv.org/pdf/2505.23031)
*Jinyi Chang, Dongliang Chang, Lei Chen, Bingyao Yu, Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper introduces LHFGLP, a framework for fine-grained visual classification without instance-level labels, using hierarchical label proportions to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing FGVC methods rely on instance-level labels, which are impractical in privacy-sensitive scenarios like medical imaging. The paper aims to enable accurate classification without direct access to such labels.

Method: The proposed LHFGLP framework leverages hierarchical label proportions, using Unrolled Hierarchical Fine-Grained Sparse Dictionary Learning and Hierarchical Proportion Loss for progressive feature refinement.

Result: Experiments on three fine-grained datasets show LHFGLP outperforms existing LLP-based methods.

Conclusion: The framework advances privacy-preserving FGVC and will release code and datasets for further research.

Abstract: In recent years, Fine-Grained Visual Classification (FGVC) has achieved
impressive recognition accuracy, despite minimal inter-class variations.
However, existing methods heavily rely on instance-level labels, making them
impractical in privacy-sensitive scenarios such as medical image analysis. This
paper aims to enable accurate fine-grained recognition without direct access to
instance labels. To achieve this, we leverage the Learning from Label
Proportions (LLP) paradigm, which requires only bag-level labels for efficient
training. Unlike existing LLP-based methods, our framework explicitly exploits
the hierarchical nature of fine-grained datasets, enabling progressive feature
granularity refinement and improving classification accuracy. We propose
Learning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework
that incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary
Learning, transforming handcrafted iterative approximation into learnable
network optimization. Additionally, our proposed Hierarchical Proportion Loss
provides hierarchical supervision, further enhancing classification
performance. Experiments on three widely-used fine-grained datasets, structured
in a bag-based manner, demonstrate that our framework consistently outperforms
existing LLP-based methods. We will release our code and datasets to foster
further research in privacy-preserving fine-grained classification.

</details>


### [247] [Deep Modeling and Optimization of Medical Image Classification](https://arxiv.org/pdf/2505.23040)
*Yihang Wu, Muhammad Owais, Reem Kateb, Ahmad Chaddad*

Main category: cs.CV

TL;DR: The paper proposes a novel CLIP variant for medical image classification, combines deep models with federated learning for privacy, and uses traditional ML to enhance generalization. Results show improved performance in brain and skin cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of fine-tuning deep models in the medical domain due to data privacy and underutilization of CLIP in medicine.

Method: Introduce a CLIP variant with CNNs and ViTs, combine 12 deep models with federated learning, and integrate traditional ML methods.

Result: MaxViT achieved 87.03% AVG in HAM10000, ConvNeXt_L scored 83.98% F1 in FL, and SVM boosted swin transformer by ~2% in ISIC2018.

Conclusion: The approach effectively tackles privacy and generalization challenges, demonstrating strong performance in medical image classification.

Abstract: Deep models, such as convolutional neural networks (CNNs) and vision
transformer (ViT), demonstrate remarkable performance in image classification.
However, those deep models require large data to fine-tune, which is
impractical in the medical domain due to the data privacy issue. Furthermore,
despite the feasible performance of contrastive language image pre-training
(CLIP) in the natural domain, the potential of CLIP has not been fully
investigated in the medical field. To face these challenges, we considered
three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight
ViTs as image encoders for the classification of brain cancer and skin cancer,
2) we combine 12 deep models with two federated learning techniques to protect
data privacy, and 3) we involve traditional machine learning (ML) methods to
improve the generalization ability of those deep models in unseen domain data.
The experimental results indicate that maxvit shows the highest averaged (AVG)
test metrics (AVG = 87.03\%) in HAM10000 dataset with multimodal learning,
while convnext\_l demonstrates remarkable test with an F1-score of 83.98\%
compared to swin\_b with 81.33\% in FL model. Furthermore, the use of support
vector machine (SVM) can improve the overall test metrics with AVG of $\sim
2\%$ for swin transformer series in ISIC2018. Our codes are available at
https://github.com/AIPMLab/SkinCancerSimulation.

</details>


### [248] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/pdf/2505.23043)
*Jihai Zhang, Tianle Li, Linjie Li, Zhengyuan Yang, Yu Cheng*

Main category: cs.CV

TL;DR: Unified vision-language models (VLMs) show mutual benefits between understanding and generation tasks, scaling with data and alignment. Cross-task generalization occurs within the base language model.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored hypothesis that unified VLMs with mixed training enhance both understanding and generation tasks.

Method: Systematic investigation using a real-world-aligned dataset and evaluation of multiple unified VLM architectures.

Result: Mutual benefits between tasks, scalability with data, better alignment improves generalization, and cross-task knowledge transfer within the base language model.

Conclusion: Unifying understanding and generation in VLMs is critical, providing insights for their design and optimization.

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [249] [SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images](https://arxiv.org/pdf/2505.23044)
*Yu Sheng, Jiajun Deng, Xinran Zhang, Yu Zhang, Bei Hua, Yanyong Zhang, Jianmin Ji*

Main category: cs.CV

TL;DR: SpatialSplat introduces a feedforward framework for 3D reconstruction with dual-field semantic representation and selective Gaussian mechanism, reducing redundancy and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods compromise semantic expressiveness and introduce memory overhead due to naive feature compression and redundant primitives.

Method: SpatialSplat uses a dual-field semantic representation (coarse and fine-grained) and a selective Gaussian mechanism to eliminate redundancy.

Result: Achieves a 60% reduction in scene representation parameters while outperforming state-of-the-art methods.

Conclusion: SpatialSplat enables more compact and accurate semantic 3D reconstruction, making it more practical for real-world applications.

Abstract: A major breakthrough in 3D reconstruction is the feedforward paradigm to
generate pixel-wise 3D points or Gaussian primitives from sparse, unposed
images. To further incorporate semantics while avoiding the significant memory
and storage costs of high-dimensional semantic features, existing methods
extend this paradigm by associating each primitive with a compressed semantic
feature vector. However, these methods have two major limitations: (a) the
naively compressed feature compromises expressiveness, affecting the model's
ability to capture fine-grained semantics, and (b) the pixel-wise primitive
prediction introduces redundancy in overlapping areas, causing unnecessary
memory overhead. To this end, we introduce \textbf{SpatialSplat}, a feedforward
framework that produces redundancy-aware Gaussians and capitalizes on a
dual-field semantic representation. Particularly, with the insight that
primitives within the same instance exhibit high semantic consistency, we
decompose the semantic representation into a coarse feature field that encodes
uncompressed semantics with minimal primitives, and a fine-grained yet
low-dimensional feature field that captures detailed inter-instance
relationships. Moreover, we propose a selective Gaussian mechanism, which
retains only essential Gaussians in the scene, effectively eliminating
redundant primitives. Our proposed Spatialsplat learns accurate semantic
information and detailed instances prior with more compact 3D Gaussians, making
semantic 3D reconstruction more applicable. We conduct extensive experiments to
evaluate our method, demonstrating a remarkable 60\% reduction in scene
representation parameters while achieving superior performance over
state-of-the-art methods. The code will be made available for future
investigation.

</details>


### [250] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/pdf/2505.23045)
*Chuanhao Li, Wenbo Ye, Zhen Li, Yuwei Wu, Yunde Jia*

Main category: cs.CV

TL;DR: The paper explores multi-sourced compositional generalization (MSCG) in VQA, proposing a retrieval-augmented training framework to unify representations of multi-modal primitives. A new dataset, GQA-MSCG, is introduced for evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored challenge of MSCG in V&L tasks, particularly VQA, where compositions involve primitives from different modalities.

Method: A retrieval-augmented training framework retrieves and aggregates semantically equivalent primitives to learn unified representations across modalities.

Result: The framework enhances MSCG ability, validated by experiments on the newly constructed GQA-MSCG dataset.

Conclusion: The proposed method effectively improves MSCG in VQA, with the GQA-MSCG dataset serving as a benchmark for future research.

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [251] [Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object](https://arxiv.org/pdf/2505.23054)
*Yuxuan Lin, Ruihang Chu, Zhenyu Chen, Xiao Tang, Lei Ke, Haoling Li, Yingji Zhong, Zhihao Li, Shiyong Liu, Xiaofei Wu, Jianzhuang Liu, Yujiu Yang*

Main category: cs.CV

TL;DR: A novel training-free method, \method, addresses partial 3D reconstruction challenges by integrating dense observations and multi-source priors, ensuring multi-view consistency and improved quality in invisible regions.


<details>
  <summary>Details</summary>
Motivation: Partial 3D reconstruction is underexplored, with challenges like limited view range and inconsistent generation in invisible regions.

Method: Proposes \method, a training-free approach using fusion-based alignment of priors in DDIM sampling and iterative refinement with geometric structures.

Result: Outperforms state-of-the-art methods, especially in reconstructing invisible regions, as shown in extensive experiments.

Conclusion: \method effectively tackles partial 3D reconstruction challenges, offering superior performance and consistency.

Abstract: Generative 3D reconstruction shows strong potential in incomplete
observations. While sparse-view and single-image reconstruction are
well-researched, partial observation remains underexplored. In this context,
dense views are accessible only from a specific angular range, with other
perspectives remaining inaccessible. This task presents two main challenges:
(i) limited View Range: observations confined to a narrow angular scope prevent
effective traditional interpolation techniques that require evenly distributed
perspectives. (ii) inconsistent Generation: views created for invisible regions
often lack coherence with both visible regions and each other, compromising
reconstruction consistency. To address these challenges, we propose \method, a
novel training-free approach that integrates the local dense observations and
multi-source priors for reconstruction. Our method introduces a fusion-based
strategy to effectively align these priors in DDIM sampling, thereby generating
multi-view consistent images to supervise invisible views. We further design an
iterative refinement strategy, which uses the geometric structures of the
object to enhance reconstruction quality. Extensive experiments on multiple
datasets show the superiority of our method over SOTAs, especially in invisible
regions.

</details>


### [252] [URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration](https://arxiv.org/pdf/2505.23068)
*Rui Xu, Yuzhen Niu, Yuezhou Li, Huangbiao Xu, Wenxi Liu, Yuzhong Chen*

Main category: cs.CV

TL;DR: The paper introduces a URWKV model for low-light image enhancement and deblurring, addressing dynamically coupled degradations with adaptive luminance modulation and multi-state feature fusion.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with dynamically coupled degradations in low-light images, prompting the need for a more flexible and effective solution.

Method: The URWKV model uses Luminance-adaptive Normalization (LAN) for adaptive luminance modulation and State-aware Selective Fusion (SSF) for multi-state feature integration.

Result: The model outperforms state-of-the-art methods on benchmarks with fewer parameters and computational resources.

Conclusion: The URWKV model effectively addresses complex degradations in low-light images, offering superior performance and efficiency.

Abstract: Existing low-light image enhancement (LLIE) and joint LLIE and deblurring
(LLIE-deblur) models have made strides in addressing predefined degradations,
yet they are often constrained by dynamically coupled degradations. To address
these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV)
model with multi-state perspective, enabling flexible and effective degradation
restoration for low-light images. Specifically, we customize the core URWKV
block to perceive and analyze complex degradations by leveraging multiple
intra- and inter-stage states. First, inspired by the pupil mechanism in the
human visual system, we propose Luminance-adaptive Normalization (LAN) that
adjusts normalization parameters based on rich inter-stage states, allowing for
adaptive, scene-aware luminance modulation. Second, we aggregate multiple
intra-stage states through exponential moving average approach, effectively
capturing subtle variations while mitigating information loss inherent in the
single-state mechanism. To reduce the degradation effects commonly associated
with conventional skip connections, we propose the State-aware Selective Fusion
(SSF) module, which dynamically aligns and integrates multi-state features
across encoder stages, selectively fusing contextual information. In comparison
to state-of-the-art models, our URWKV model achieves superior performance on
various benchmarks, while requiring significantly fewer parameters and
computational resources.

</details>


### [253] [LeMoRe: Learn More Details for Lightweight Semantic Segmentation](https://arxiv.org/pdf/2505.23093)
*Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte*

Main category: cs.CV

TL;DR: The paper introduces LeMoRe, a lightweight semantic segmentation method balancing efficiency and performance by combining explicit and implicit modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing efficiency and performance due to rigid architectures and heavy reliance on Vision Transformers.

Method: LeMoRe synergizes explicit (Cartesian directions, modeled views) and implicit (inferred representations) modeling with a nested attention mechanism.

Result: Experiments on ADE20K, CityScapes, Pascal Context, and COCO-Stuff show LeMoRe achieves a good balance between performance and efficiency.

Conclusion: LeMoRe effectively addresses the trade-off between computational efficiency and representational fidelity in semantic segmentation.

Abstract: Lightweight semantic segmentation is essential for many downstream vision
tasks. Unfortunately, existing methods often struggle to balance efficiency and
performance due to the complexity of feature modeling. Many of these existing
approaches are constrained by rigid architectures and implicit representation
learning, often characterized by parameter-heavy designs and a reliance on
computationally intensive Vision Transformer-based frameworks. In this work, we
introduce an efficient paradigm by synergizing explicit and implicit modeling
to balance computational efficiency with representational fidelity. Our method
combines well-defined Cartesian directions with explicitly modeled views and
implicitly inferred intermediate representations, efficiently capturing global
dependencies through a nested attention mechanism. Extensive experiments on
challenging datasets, including ADE20K, CityScapes, Pascal Context, and
COCO-Stuff, demonstrate that LeMoRe strikes an effective balance between
performance and efficiency.

</details>


### [254] [CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing](https://arxiv.org/pdf/2505.23102)
*Yuka Ogino, Takahiro Toizumi, Atsushi Ito*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Light Image Enhancement (LLIE) is crucial for improving both human
perception and computer vision tasks. This paper addresses two challenges in
zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive
Language-Image Pre-Training (CLIP) model and maintaining computational
efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement
learning-based Visual image Enhancement (CURVE). CURVE employs a simple image
processing module which adjusts global image tone based on B\'ezier curve and
estimates its processing parameters iteratively. The estimator is trained by
reinforcement learning with rewards designed using CLIP text embeddings.
Experiments on low-light and multi-exposure datasets demonstrate the
performance of CURVE in terms of enhancement quality and processing speed
compared to conventional methods.

</details>


### [255] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/pdf/2505.23107)
*Pushapdeep Singh, Jyoti Nigam, Medicherla Vamsi Krishna, Arnav Bhavsar, Aditya Nigam*

Main category: cs.CV

TL;DR: Proposes EEG Adapter (EAD), a flexible framework for EEG signal classification, achieving high accuracy across diverse datasets and demonstrating zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in EEG decoding due to task-specific data acquisition and device variability, aiming for a unified pipeline.

Method: Leverages an EEG foundational model with adaptations to learn robust representations, evaluated on EEG-ImageNet and BrainLat datasets.

Result: Achieves state-of-the-art accuracies of 99.33% and 92.31% on EEG-ImageNet and BrainLat, respectively, and demonstrates zero-shot classification.

Conclusion: EAD is effective for diverse EEG tasks, offering flexibility and generalization across datasets and devices.

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [256] [Identification of Patterns of Cognitive Impairment for Early Detection of Dementia](https://arxiv.org/pdf/2505.23109)
*Anusha A. S., Uma Ranjan, Medha Sharma, Siddharth Dutt*

Main category: cs.CV

TL;DR: A novel scheme for personalized dementia detection identifies individual-specific cognitive impairment patterns using population data, enabling tailored follow-up tests.


<details>
  <summary>Details</summary>
Motivation: Early dementia detection is vital, but current cognitive tests are lengthy and impractical for large-scale periodic use. Personalized approaches are needed due to varying impairment patterns.

Method: A 2-step procedure: ensemble wrapper feature selection followed by cluster analysis identifies cognitive impairment patterns in a population (24,000 subjects from NACC). These patterns guide personalized tests.

Result: Identified patterns align with clinical MCI variants, allowing prediction of cognitive impairment routes even in pre-symptomatic individuals.

Conclusion: The method offers scalable, personalized dementia detection, improving early intervention feasibility.

Abstract: Early detection of dementia is crucial to devise effective interventions.
Comprehensive cognitive tests, while being the most accurate means of
diagnosis, are long and tedious, thus limiting their applicability to a large
population, especially when periodic assessments are needed. The problem is
compounded by the fact that people have differing patterns of cognitive
impairment as they progress to different forms of dementia. This paper presents
a novel scheme by which individual-specific patterns of impairment can be
identified and used to devise personalized tests for periodic follow-up.
Patterns of cognitive impairment are initially learned from a population
cluster of combined normals and MCIs, using a set of standardized cognitive
tests. Impairment patterns in the population are identified using a 2-step
procedure involving an ensemble wrapper feature selection followed by cluster
identification and analysis. These patterns have been shown to correspond to
clinically accepted variants of MCI, a prodrome of dementia. The learned
clusters of patterns can subsequently be used to identify the most likely route
of cognitive impairment, even for pre-symptomatic and apparently normal people.
Baseline data of 24,000 subjects from the NACC database was used for the study.

</details>


### [257] [Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving](https://arxiv.org/pdf/2505.23115)
*Yunshen Wang, Yicheng Liu, Tianyuan Yuan, Yucheng Mao, Yingshi Liang, Xiuyu Yang, Honggang Zhang, Hang Zhao*

Main category: cs.CV

TL;DR: The paper proposes using diffusion models for 3D occupancy grid prediction in autonomous driving, improving accuracy and robustness over discriminative methods.


<details>
  <summary>Details</summary>
Motivation: Current discriminative methods for 3D occupancy prediction struggle with noise, incomplete data, and complex 3D structures, limiting their effectiveness in autonomous driving.

Method: The work reframes the problem as a generative modeling task using diffusion models, leveraging 3D scene priors to enhance prediction quality.

Result: Diffusion-based models outperform state-of-the-art discriminative methods, providing more realistic and accurate predictions, especially in occluded or low-visibility areas.

Conclusion: The generative approach improves prediction consistency and robustness, benefiting downstream planning tasks in autonomous driving applications.

Abstract: Accurately predicting 3D occupancy grids from visual inputs is critical for
autonomous driving, but current discriminative methods struggle with noisy
data, incomplete observations, and the complex structures inherent in 3D
scenes. In this work, we reframe 3D occupancy prediction as a generative
modeling task using diffusion models, which learn the underlying data
distribution and incorporate 3D scene priors. This approach enhances prediction
consistency, noise robustness, and better handles the intricacies of 3D spatial
structures. Our extensive experiments show that diffusion-based generative
models outperform state-of-the-art discriminative approaches, delivering more
realistic and accurate occupancy predictions, especially in occluded or
low-visibility regions. Moreover, the improved predictions significantly
benefit downstream planning tasks, highlighting the practical advantages of our
method for real-world autonomous driving applications.

</details>


### [258] [TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance](https://arxiv.org/pdf/2505.23119)
*Keren Ye, Ignacio Garcia Dorado, Michalis Raptis, Mauricio Delbracio, Irene Zhu, Peyman Milanfar, Hossein Talebi*

Main category: cs.CV

TL;DR: TextSR is a multimodal diffusion model for Multilingual Scene Text Image Super-Resolution, addressing limitations of existing models by integrating text detection, OCR, and character-to-shape priors to improve text legibility and quality.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for Image Super-Resolution struggle with text region localization and multilingual character-to-shape priors, leading to inconsistencies and poor text quality.

Method: TextSR uses a text detector and OCR to extract multilingual text, encodes characters into visual shapes, and integrates these priors with low-resolution images to guide super-resolution.

Result: TextSR outperforms existing models on TextZoom and TextVQA datasets, setting a new benchmark for Scene Text Image Super-Resolution.

Conclusion: TextSR effectively addresses the challenges of text super-resolution by leveraging multimodal priors, improving text legibility and overall image quality.

Abstract: While recent advancements in Image Super-Resolution (SR) using diffusion
models have shown promise in improving overall image quality, their application
to scene text images has revealed limitations. These models often struggle with
accurate text region localization and fail to effectively model image and
multilingual character-to-shape priors. This leads to inconsistencies, the
generation of hallucinated textures, and a decrease in the perceived quality of
the super-resolved text.
  To address these issues, we introduce TextSR, a multimodal diffusion model
specifically designed for Multilingual Scene Text Image Super-Resolution.
TextSR leverages a text detector to pinpoint text regions within an image and
then employs Optical Character Recognition (OCR) to extract multilingual text
from these areas. The extracted text characters are then transformed into
visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing
that OCR may sometimes produce inaccurate results in real-world scenarios, we
have developed two innovative methods to enhance the robustness of our model.
By integrating text character priors with the low-resolution text images, our
model effectively guides the super-resolution process, enhancing fine details
within the text and improving overall legibility. The superior performance of
our model on both the TextZoom and TextVQA datasets sets a new benchmark for
STISR, underscoring the efficacy of our approach.

</details>


### [259] [MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation](https://arxiv.org/pdf/2505.23120)
*Siyuan Wang, Jiawei Liu, Wei Wang, Yeying Jin, Jinsong Du, Zhi Han*

Main category: cs.CV

TL;DR: The paper proposes a Motion Mask-Guided Two-Stage Network (MMGT) to generate synchronized speech gesture videos, addressing challenges like motion diversity and audio relevance.


<details>
  <summary>Details</summary>
Motivation: Existing methods relying solely on audio often fail to capture large gestures, causing artifacts. Additional priors limit practicality, so MMGT integrates motion masks and features for better results.

Method: MMGT uses a two-stage approach: (1) SMGA Network generates pose videos and motion masks from audio, capturing key movements. (2) MM-HAA in Stabilized Diffusion refines fine-grained motion and details.

Result: The method improves video quality, lip-sync, and gesture accuracy, outperforming traditional approaches.

Conclusion: MMGT effectively addresses limitations in co-speech gesture video generation, offering high-quality, detailed results with practical applicability.

Abstract: Co-Speech Gesture Video Generation aims to generate vivid speech videos from
audio-driven still images, which is challenging due to the diversity of
different parts of the body in terms of amplitude of motion, audio relevance,
and detailed features. Relying solely on audio as the control signal often
fails to capture large gesture movements in video, leading to more pronounced
artifacts and distortions. Existing approaches typically address this issue by
introducing additional a priori information, but this can limit the practical
application of the task. Specifically, we propose a Motion Mask-Guided
Two-Stage Network (MMGT) that uses audio, as well as motion masks and motion
features generated from the audio signal to jointly drive the generation of
synchronized speech gesture videos. In the first stage, the Spatial Mask-Guided
Audio Pose Generation (SMGA) Network generates high-quality pose videos and
motion masks from audio, effectively capturing large movements in key regions
such as the face and gestures. In the second stage, we integrate the Motion
Masked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion
Video Generation model, overcoming limitations in fine-grained motion
generation and region-specific detail control found in traditional methods.
This guarantees high-quality, detailed upper-body video generation with
accurate texture and motion details. Evaluations show improved video quality,
lip-sync, and gesture. The model and code are available at
https://github.com/SIA-IDE/MMGT.

</details>


### [260] [HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring](https://arxiv.org/pdf/2505.23129)
*Bin Wang, Pingjun Li, Jinkun Liu, Jun Cheng, Hailong Lei, Yinze Rong, Huan-ang Gao, Kangliang Chen, Xing Pan, Weihao Gu*

Main category: cs.CV

TL;DR: HMAD is a framework for autonomous driving that combines BEV-based trajectory proposals with learned multi-criteria scoring to improve trajectory diversity and selection.


<details>
  <summary>Details</summary>
Motivation: Address challenges in generating diverse, rule-compliant trajectories and selecting the optimal path via learned evaluation.

Method: Uses BEVFormer and learnable anchored queries for trajectory generation, refined via iterative offset decoding. Includes a simulation-supervised scorer for multi-criteria evaluation.

Result: Achieves a 44.5% driving score on the CVPR 2025 private test set.

Conclusion: Decoupling trajectory generation from safety-aware scoring enhances autonomous driving performance.

Abstract: End-to-end autonomous driving faces persistent challenges in both generating
diverse, rule-compliant trajectories and robustly selecting the optimal path
from these options via learned, multi-faceted evaluation. To address these
challenges, we introduce HMAD, a framework integrating a distinctive
Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned
multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored
queries, initialized from a trajectory dictionary and refined via iterative
offset decoding (inspired by DiffusionDrive), to produce numerous diverse and
stable candidate trajectories. A key innovation, our simulation-supervised
scorer module, then evaluates these proposals against critical metrics
including no at-fault collisions, drivable area compliance, comfortableness,
and overall driving quality (i.e., extended PDM score). Demonstrating its
efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test
set. This work highlights the benefits of effectively decoupling robust
trajectory generation from comprehensive, safety-aware learned scoring for
advanced autonomous driving.

</details>


### [261] [PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents](https://arxiv.org/pdf/2505.23130)
*Haoyu Chen, Keda Tao, Yizao Wang, Xinlei Wang, Lei Zhu, Jinjin Gu*

Main category: cs.CV

TL;DR: PhotoArtAgent is an AI system that uses Vision-Language Models to emulate professional photo retouching, offering transparent, interactive, and high-quality results comparable to human artists.


<details>
  <summary>Details</summary>
Motivation: Automated photo retouching tools lack interpretative depth and transparency. PhotoArtAgent aims to bridge this gap by mimicking the creative process of professional artists.

Method: Combines Vision-Language Models with natural language reasoning to analyze, plan, and execute retouching strategies, iteratively refining results via Lightroom API.

Result: Outperforms existing automated tools and matches professional human artists in quality, with transparent text-based explanations.

Conclusion: PhotoArtAgent successfully integrates AI with artistic creativity, offering a user-controlled, high-quality retouching solution.

Abstract: Photo retouching is integral to photographic art, extending far beyond simple
technical fixes to heighten emotional expression and narrative depth. While
artists leverage expertise to create unique visual effects through deliberate
adjustments, non-professional users often rely on automated tools that produce
visually pleasing results but lack interpretative depth and interactive
transparency. In this paper, we introduce PhotoArtAgent, an intelligent system
that combines Vision-Language Models (VLMs) with advanced natural language
reasoning to emulate the creative process of a professional artist. The agent
performs explicit artistic analysis, plans retouching strategies, and outputs
precise parameters to Lightroom through an API. It then evaluates the resulting
images and iteratively refines them until the desired artistic vision is
achieved. Throughout this process, PhotoArtAgent provides transparent,
text-based explanations of its creative rationale, fostering meaningful
interaction and user control. Experimental results show that PhotoArtAgent not
only surpasses existing automated tools in user studies but also achieves
results comparable to those of professional human artists.

</details>


### [262] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/pdf/2505.23134)
*Tongtong Su, Chengyu Wang, Jun Huang, Dongming Lu*

Main category: cs.CV

TL;DR: Zero-to-Hero introduces a reference-based video editing method that disentangles editing into anchor frame refinement and propagation, addressing ambiguities and control issues in text-guided methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided video editing methods lack fine-grained control and clarity in user intentions, prompting the need for a more robust and precise approach.

Method: The method splits editing into two stages: editing an anchor frame as a reference and propagating its appearance using correspondence-guided attention. A Hero-Stage further refines results with a conditional generative model for restoration.

Result: The approach improves PSNR by 2.6 dB over baselines, demonstrating superior accuracy and temporal consistency.

Conclusion: Zero-to-Hero offers a robust, memory-friendly solution for precise video appearance editing, outperforming existing methods while addressing their limitations.

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [263] [Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning](https://arxiv.org/pdf/2505.23143)
*Jinquan Guan, Qi Chen, Lizhou Liang, Yuhang Liu, Vu Minh Hieu Phan, Minh-Son To, Jian Chen, Yutong Xie*

Main category: cs.CV

TL;DR: The paper introduces CXRTrek, a multi-stage VQA dataset for CXR interpretation, and CXRTrekNet, a VLLM designed to model clinical reasoning. It outperforms existing models and generalizes well.


<details>
  <summary>Details</summary>
Motivation: Current AI models for CXR interpretation lack sequential diagnostic reasoning, leading to misalignment with clinical scenarios.

Method: Constructed CXRTrek dataset with 8 diagnostic stages and proposed CXRTrekNet, a VLLM incorporating clinical reasoning.

Result: CXRTrekNet outperforms existing medical VLLMs on benchmarks and generalizes well across tasks.

Conclusion: CXRTrek and CXRTrekNet address the gap in modeling diagnostic reasoning, improving AI-assisted CXR interpretation.

Abstract: Artificial intelligence (AI)-based chest X-ray (CXR) interpretation
assistants have demonstrated significant progress and are increasingly being
applied in clinical settings. However, contemporary medical AI models often
adhere to a simplistic input-to-output paradigm, directly processing an image
and an instruction to generate a result, where the instructions may be integral
to the model's architecture. This approach overlooks the modeling of the
inherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is
typically sequential, where each interpretive stage considers the images, the
current task, and the contextual information from previous stages. This
oversight leads to several shortcomings, including misalignment with clinical
scenarios, contextless reasoning, and untraceable errors. To fill this gap, we
construct CXRTrek, a new multi-stage visual question answering (VQA) dataset
for CXR interpretation. The dataset is designed to explicitly simulate the
diagnostic reasoning process employed by radiologists in real-world clinical
settings for the first time. CXRTrek covers 8 sequential diagnostic stages,
comprising 428,966 samples and over 11 million question-answer (Q&A) pairs,
with an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,
we propose a new vision-language large model (VLLM), CXRTrekNet, specifically
designed to incorporate the clinical reasoning flow into the VLLM framework.
CXRTrekNet effectively models the dependencies between diagnostic stages and
captures reasoning patterns within the radiological context. Trained on our
dataset, the model consistently outperforms existing medical VLLMs on the
CXRTrek benchmarks and demonstrates superior generalization across multiple
tasks on five diverse external datasets. The dataset and model can be found in
our repository (https://github.com/guanjinquan/CXRTrek).

</details>


### [264] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/pdf/2505.23145)
*Jeongsol Kim, Yeobin Hong, Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign improves inversion-free flow-based image editing by introducing a flow-matching loss for stable trajectories and better consistency.


<details>
  <summary>Details</summary>
Motivation: Existing inversion-free methods like FlowEdit suffer from unstable editing trajectories and poor source consistency.

Method: FlowAlign uses a flow-matching loss to regularize trajectories, balancing semantic alignment and structural consistency.

Result: FlowAlign outperforms existing methods in source preservation and editing controllability.

Conclusion: FlowAlign offers a reversible and consistent framework for inversion-free image editing.

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [265] [PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling](https://arxiv.org/pdf/2505.23155)
*Xiao Yu, Yan Fang, Xiaojie Jin, Yao Zhao, Yunchao Wei*

Main category: cs.CV

TL;DR: On-AVEP introduces real-time audio-visual event parsing with PreFM, enhancing contextual understanding and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods are offline and computationally heavy, limiting real-time use. On-AVEP addresses this gap.

Method: Uses Predictive Future Modeling (PreFM) for future cue integration and robust representation.

Result: Outperforms state-of-the-art on UnAV-100 and LLP datasets with fewer parameters.

Conclusion: PreFM offers an efficient, real-time solution for multimodal video understanding.

Abstract: Audio-visual event parsing plays a crucial role in understanding multimodal
video content, but existing methods typically rely on offline processing of
entire videos with huge model sizes, limiting their real-time applicability. We
introduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for
parsing audio, visual, and audio-visual events by sequentially analyzing
incoming video streams. The On-AVEP task necessitates models with two key
capabilities: (1) Accurate online inference, to effectively distinguish events
with unclear and limited context in online settings, and (2) Real-time
efficiency, to balance high performance with computational constraints. To
cultivate these, we propose the Predictive Future Modeling (PreFM) framework
featured by (a) predictive multimodal future modeling to infer and integrate
beneficial future audio-visual cues, thereby enhancing contextual understanding
and (b) modality-agnostic robust representation along with focal temporal
prioritization to improve precision and generalization. Extensive experiments
on the UnAV-100 and LLP datasets show PreFM significantly outperforms
state-of-the-art methods by a large margin with significantly fewer parameters,
offering an insightful approach for real-time multimodal video understanding.
Code is available at https://github.com/XiaoYu-1123/PreFM.

</details>


### [266] [LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering](https://arxiv.org/pdf/2505.23158)
*Jonas Kulhanek, Marie-Julie Rakotosaona, Fabian Manhardt, Christina Tsalicoglou, Michael Niemeyer, Torsten Sattler, Songyou Peng, Federico Tombari*

Main category: cs.CV

TL;DR: A novel LOD method for 3D Gaussian Splatting enables real-time rendering of large scenes on memory-constrained devices by hierarchical LOD representation, depth-aware smoothing, and dynamic spatial chunking.


<details>
  <summary>Details</summary>
Motivation: To enable real-time rendering of large-scale scenes on devices with limited memory by optimizing Gaussian subsets and reducing rendering time and GPU memory usage.

Method: Hierarchical LOD representation with depth-aware 3D smoothing, importance-based pruning, fine-tuning, spatial chunking, and dynamic loading of Gaussians.

Result: Achieves state-of-the-art performance on outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets with high-quality renderings, reduced latency, and lower memory usage.

Conclusion: The method effectively balances visual fidelity and performance, making it suitable for memory-constrained real-time applications.

Abstract: In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian
Splatting that enables real-time rendering of large-scale scenes on
memory-constrained devices. Our approach introduces a hierarchical LOD
representation that iteratively selects optimal subsets of Gaussians based on
camera distance, thus largely reducing both rendering time and GPU memory
usage. We construct each LOD level by applying a depth-aware 3D smoothing
filter, followed by importance-based pruning and fine-tuning to maintain visual
fidelity. To further reduce memory overhead, we partition the scene into
spatial chunks and dynamically load only relevant Gaussians during rendering,
employing an opacity-blending mechanism to avoid visual artifacts at chunk
boundaries. Our method achieves state-of-the-art performance on both outdoor
(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality
renderings with reduced latency and memory requirements.

</details>


### [267] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/pdf/2505.23161)
*Antonio D'Orazio, Maria Rosaria Briglia, Donato Crisostomi, Dario Loi, Emanuele Rodolà, Iacopo Masi*

Main category: cs.CV

TL;DR: CLIP, a discriminative model, can generate images without a decoder or training by optimizing a frequency-aware neural representation and using stabilization techniques.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped generative potential of discriminative models like CLIP, bypassing the need for decoders or fine-tuning.

Method: Uses a frequency-aware implicit neural representation, adversarially robust initialization, Orthogonal Procrustes projection, and a blending loss to stabilize image synthesis.

Result: Enables text-to-image generation, style transfer, and image reconstruction without modifying CLIP's weights.

Conclusion: Discriminative models like CLIP may have hidden generative capabilities, suggesting broader applications.

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [268] [RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer](https://arxiv.org/pdf/2505.23171)
*Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang, Zhizhong Su*

Main category: cs.CV

TL;DR: RoboTransfer is a diffusion-based video generation framework for robotic data synthesis, addressing the sim-to-real gap by ensuring geometric consistency and fine-grained control.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale real-world robot demonstrations is expensive, and simulators face challenges due to the sim-to-real gap.

Method: RoboTransfer integrates multi-view geometry with explicit control over scene components, using cross-view feature interactions and global depth/normal conditions.

Result: Policies trained on RoboTransfer-generated data show a 33.3% improvement in DIFF-OBJ and 251% in DIFF-ALL scenarios.

Conclusion: RoboTransfer effectively bridges the sim-to-real gap, enhancing robotic manipulation performance.

Abstract: Imitation Learning has become a fundamental approach in robotic manipulation.
However, collecting large-scale real-world robot demonstrations is
prohibitively expensive. Simulators offer a cost-effective alternative, but the
sim-to-real gap make it extremely challenging to scale. Therefore, we introduce
RoboTransfer, a diffusion-based video generation framework for robotic data
synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry
with explicit control over scene components, such as background and object
attributes. By incorporating cross-view feature interactions and global
depth/normal conditions, RoboTransfer ensures geometry consistency across
views. This framework allows fine-grained control, including background edits
and object swaps. Experiments demonstrate that RoboTransfer is capable of
generating multi-view videos with enhanced geometric consistency and visual
fidelity. In addition, policies trained on the data generated by RoboTransfer
achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ
setting and a substantial 251% relative improvement in the more challenging
DIFF-ALL scenario. Explore more demos on our project page:
https://horizonrobotics.github.io/robot_lab/robotransfer

</details>


### [269] [DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes](https://arxiv.org/pdf/2505.23179)
*Sungjune Park, Hyunjun Kim, Junho Kim, Seongho Kim, Yong Man Ro*

Main category: cs.CV

TL;DR: The paper introduces DIP-R1, an RL-based framework to enhance MLLMs' fine-grained visual perception in complex scenes, outperforming baselines and supervised methods.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fine-grained visual perception in complex scenarios like crowded areas. RL's success in LLMs and MLLMs inspired its use to improve this capability.

Method: DIP-R1 uses three rule-based rewards: reasoning, variance-guided looking, and precision-recall accuracy, to guide MLLMs in detailed scene inspection.

Result: DIP-R1 improves performance in diverse fine-grained detection tasks, including crowded scenes, and outperforms baselines and supervised methods.

Conclusion: Integrating RL into MLLMs shows great potential for enhancing complex real-world perception tasks.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant visual
understanding capabilities, yet their fine-grained visual perception in complex
real-world scenarios, such as densely crowded public areas, remains limited.
Inspired by the recent success of reinforcement learning (RL) in both LLMs and
MLLMs, in this paper, we explore how RL can enhance visual perception ability
of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and
Perception with RL (DIP-R1) designed to enhance the visual perception
capabilities of MLLMs, by comprehending complex scenes and looking through
visual instances closely. DIP-R1 guides MLLMs through detailed inspection of
visual scene via three simply designed rule-based reward modelings. First, we
adopt a standard reasoning reward encouraging the model to include three
step-by-step processes: 1) reasoning for understanding visual scenes, 2)
observing for looking through interested but ambiguous regions, and 3)
decision-making for predicting answer. Second, a variance-guided looking reward
is designed to examine uncertain regions for the second observing process. It
explicitly enables the model to inspect ambiguous areas, improving its ability
to mitigate perceptual uncertainties. Third, we model a weighted
precision-recall accuracy reward enhancing accurate decision-making. We explore
its effectiveness across diverse fine-grained object detection data consisting
of challenging real-world environments, such as densely crowded scenes. Built
upon existing MLLMs, DIP-R1 achieves consistent and significant improvement
across various in-domain and out-of-domain scenarios. It also outperforms
various existing baseline models and supervised fine-tuning methods. Our
findings highlight the substantial potential of integrating RL into MLLMs for
enhancing capabilities in complex real-world perception tasks.

</details>


### [270] [HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image](https://arxiv.org/pdf/2505.23186)
*Junyi Guo, Jingxuan Zhang, Fangyu Wu, Huanda Lu, Qiufeng Wang, Wenmian Yang, Eng Gee Lim, Dongming Lu*

Main category: cs.CV

TL;DR: The paper introduces FS2RG, a task for generating realistic garment images from flat sketches and text, addressing challenges like fabric detail capture and conflicting inputs. It proposes HiGarment, a framework with multi-modal enhancement and cross-attention mechanisms, and releases a new dataset.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in garment production by exploring realistic image generation from flat sketches and text, addressing limitations in fabric detail and input conflicts.

Method: Proposes HiGarment, featuring multi-modal semantic enhancement for fabric representation and harmonized cross-attention to balance sketch and text inputs.

Result: HiGarment effectively synthesizes garments, validated by experiments and user studies. A new dataset is introduced.

Conclusion: HiGarment advances garment synthesis by addressing key challenges, with potential impact on fashion design and production.

Abstract: Diffusion-based garment synthesis tasks primarily focus on the design phase
in the fashion domain, while the garment production process remains largely
underexplored. To bridge this gap, we introduce a new task: Flat Sketch to
Realistic Garment Image (FS2RG), which generates realistic garment images by
integrating flat sketches and textual guidance. FS2RG presents two key
challenges: 1) fabric characteristics are solely guided by textual prompts,
providing insufficient visual supervision for diffusion-based models, which
limits their ability to capture fine-grained fabric details; 2) flat sketches
and textual guidance may provide conflicting information, requiring the model
to selectively preserve or modify garment attributes while maintaining
structural coherence. To tackle this task, we propose HiGarment, a novel
framework that comprises two core components: i) a multi-modal semantic
enhancement mechanism that enhances fabric representation across textual and
visual modalities, and ii) a harmonized cross-attention mechanism that
dynamically balances information from flat sketches and text prompts, allowing
controllable synthesis by generating either sketch-aligned (image-biased) or
text-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed
Garment, the largest open-source dataset for garment generation. Experimental
results and user studies demonstrate the effectiveness of HiGarment in garment
synthesis. The code and dataset will be released.

</details>


### [271] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/pdf/2505.23192)
*Run Hao, Peng Ying*

Main category: cs.CV

TL;DR: An automated adversarial prompt generation framework is proposed to evade AIGC detectors using grammar trees and Monte Carlo tree search, validated by experiments and competition success.


<details>
  <summary>Details</summary>
Motivation: Address concerns about identity misuse and AIGC detector robustness by creating adversarial prompts.

Method: Uses a grammar tree structure and Monte Carlo tree search variant to explore semantic prompt space for diverse, controllable prompts.

Result: Effectively evades detectors, ranks first in a competition, and aids in creating adversarial datasets.

Conclusion: The framework is effective for attacks and improving detector robustness through dataset construction.

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [272] [Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images](https://arxiv.org/pdf/2505.23193)
*Sungjune Park, Hyunjun Kim, Beomchan Park, Yong Man Ro*

Main category: cs.CV

TL;DR: The paper introduces LANGO, a language-guided object detection framework for aerial images, addressing challenges like scene and instance-level variations.


<details>
  <summary>Details</summary>
Motivation: Aerial images have diverse scenes and object appearances due to variations like illumination and viewpoint changes, making object detection difficult.

Method: LANGO uses a visual semantic reasoner to interpret scene conditions and a relation learning loss to handle instance-level variations like viewpoint and scale changes.

Result: The method shows noticeable improvements in detection performance through extensive experiments.

Conclusion: LANGO effectively mitigates challenges in aerial object detection by leveraging language-guided learning.

Abstract: Despite recent advancements in computer vision research, object detection in
aerial images still suffers from several challenges. One primary challenge to
be mitigated is the presence of multiple types of variation in aerial images,
for example, illumination and viewpoint changes. These variations result in
highly diverse image scenes and drastic alterations in object appearance, so
that it becomes more complicated to localize objects from the whole image scene
and recognize their categories. To address this problem, in this paper, we
introduce a novel object detection framework in aerial images, named
LANGuage-guided Object detection (LANGO). Upon the proposed language-guided
learning, the proposed framework is designed to alleviate the impacts from both
scene and instance-level variations. First, we are motivated by the way humans
understand the semantics of scenes while perceiving environmental factors in
the scenes (e.g., weather). Therefore, we design a visual semantic reasoner
that comprehends visual semantics of image scenes by interpreting conditions
where the given images were captured. Second, we devise a training objective,
named relation learning loss, to deal with instance-level variations, such as
viewpoint angle and scale changes. This training objective aims to learn
relations in language representations of object categories, with the help of
the robust characteristics against such variations. Through extensive
experiments, we demonstrate the effectiveness of the proposed method, and our
method obtains noticeable detection performance improvements.

</details>


### [273] [WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems](https://arxiv.org/pdf/2505.23201)
*Hao Wu, Junzhou Chen, Ronghui Zhang, Nengchao Lyu, Hongyu Hu, Yanyong Guo, Tony Z. Qiu*

Main category: cs.CV

TL;DR: WTEFNet is a real-time object detection framework for low-light conditions, combining low-light enhancement, wavelet-based feature extraction, and adaptive fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-based object detection methods perform poorly in low-light conditions, limiting ADAS effectiveness.

Method: WTEFNet uses three modules: Low-Light Enhancement (LLE), Wavelet-based Feature Extraction (WFE), and Adaptive Fusion Detection (AFFD).

Result: WTEFNet achieves top accuracy on datasets like BDD100K and GSN and is suitable for real-time ADAS applications.

Conclusion: WTEFNet effectively addresses low-light object detection challenges, demonstrating practicality for real-world ADAS deployment.

Abstract: Object detection is a cornerstone of environmental perception in advanced
driver assistance systems(ADAS). However, most existing methods rely on RGB
cameras, which suffer from significant performance degradation under low-light
conditions due to poor image quality. To address this challenge, we proposes
WTEFNet, a real-time object detection framework specifically designed for
low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet
comprises three core modules: a Low-Light Enhancement (LLE) module, a
Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection
(AFFD) module. The LLE enhances dark regions while suppressing overexposed
areas; the WFE applies multi-level discrete wavelet transforms to isolate high-
and low-frequency components, enabling effective denoising and structural
feature retention; the AFFD fuses semantic and illumination features for robust
detection. To support training and evaluation, we introduce GSN, a manually
annotated dataset covering both clear and rainy night-time scenes. Extensive
experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet
achieves state-of-the-art accuracy under low-light conditions. Furthermore,
deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the
framework's suitability for real-time ADAS applications.

</details>


### [274] [HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers](https://arxiv.org/pdf/2505.23206)
*Aldino Rizaldy, Richard Gloaguen, Fabian Ewald Fassnacht, Pedram Ghamisi*

Main category: cs.CV

TL;DR: A 3D-based method fusing multimodal remote sensing data using a dual-branch Transformer and cross-attention for improved land-use classification.


<details>
  <summary>Details</summary>
Motivation: Current methods rasterize 3D data into 2D, limiting 3D feature learning and prediction flexibility.

Method: Proposes a 3D point cloud fusion with a dual-branch Transformer and cross-attention mechanism for feature integration.

Result: Outperforms 2D methods, provides 3D predictions, and shows flexibility in projection to 2D maps.

Conclusion: 3D fusion is competitive and offers advantages over 2D methods, with potential for broader applications.

Abstract: Multimodal remote sensing data, including spectral and lidar or
photogrammetry, is crucial for achieving satisfactory land-use / land-cover
classification results in urban scenes. So far, most studies have been
conducted in a 2D context. When 3D information is available in the dataset, it
is typically integrated with the 2D data by rasterizing the 3D data into 2D
formats. Although this method yields satisfactory classification results, it
falls short in fully exploiting the potential of 3D data by restricting the
model's ability to learn 3D spatial features directly from raw point clouds.
Additionally, it limits the generation of 3D predictions, as the dimensionality
of the input data has been reduced. In this study, we propose a fully 3D-based
method that fuses all modalities within the 3D point cloud and employs a
dedicated dual-branch Transformer model to simultaneously learn geometric and
spectral features. To enhance the fusion process, we introduce a
cross-attention-based mechanism that fully operates on 3D points, effectively
integrating features from various modalities across multiple scales. The
purpose of cross-attention is to allow one modality to assess the importance of
another by weighing the relevant features. We evaluated our method by comparing
it against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest
(DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive
results compared to 2D methods and offers more flexibility by providing 3D
predictions. These predictions can be projected onto 2D maps, a capability that
is not feasible in reverse. Additionally, we evaluated our method on different
datasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion
Contest. Our code will be published here:
https://github.com/aldinorizaldy/hyperpointformer.

</details>


### [275] [Navigating the Accuracy-Size Trade-Off with Flexible Model Merging](https://arxiv.org/pdf/2505.23209)
*Akash Dhasade, Divyansh Jhunjhunwala, Milos Vujasinovic, Gauri Joshi, Anne-Marie Kermarrec*

Main category: cs.CV

TL;DR: FlexMerge is a data-free model merging framework that flexibly generates merged models of varying sizes, balancing accuracy and deployment costs.


<details>
  <summary>Details</summary>
Motivation: To address the accuracy gap in single merged models and high costs of deploying individual models, FlexMerge offers a flexible merging solution.

Method: FlexMerge treats models as sequential blocks, progressively merging them using existing data-free methods, stopping at a desired size.

Result: Experiments show modestly larger merged models improve accuracy over a single model, with fine-grained size control.

Conclusion: FlexMerge provides a high-performance, flexible, and data-free solution for diverse deployment needs.

Abstract: Model merging has emerged as an efficient method to combine multiple
single-task fine-tuned models. The merged model can enjoy multi-task
capabilities without expensive training. While promising, merging into a single
model often suffers from an accuracy gap with respect to individual fine-tuned
models. On the other hand, deploying all individual fine-tuned models incurs
high costs. We propose FlexMerge, a novel data-free model merging framework to
flexibly generate merged models of varying sizes, spanning the spectrum from a
single merged model to retaining all individual fine-tuned models. FlexMerge
treats fine-tuned models as collections of sequential blocks and progressively
merges them using any existing data-free merging method, halting at a desired
size. We systematically explore the accuracy-size trade-off exhibited by
different merging algorithms in combination with FlexMerge. Extensive
experiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even
modestly larger merged models can provide substantial accuracy improvements
over a single model. By offering fine-grained control over fused model size,
FlexMerge provides a flexible, data-free, and high-performance solution for
diverse deployment scenarios.

</details>


### [276] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/pdf/2505.23214)
*Wenhao Xu, Shuchen Zheng, Changwei Wang, Zherui Zhang, Chuan Ren, Rongtao Xu, Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba is a novel framework for infrared small target detection (ISTD) that integrates hierarchical feature learning and selective sequence modeling to address challenges like domain gaps and inefficient global context modeling.


<details>
  <summary>Details</summary>
Motivation: ISTD is crucial for long-range surveillance but faces challenges due to small target size and complex backgrounds. Existing methods suffer from information loss and poor global context modeling.

Method: SAMamba combines SAM2's hierarchical feature learning with Mamba's selective sequence modeling, introducing FS-Adapter for domain adaptation, CSI for global context modeling, and DPCF for multi-scale feature fusion.

Result: SAMamba outperforms state-of-the-art methods on datasets like NUAA-SIRST, IRSTD-1k, and NUDT-SIRST, especially in challenging scenarios.

Conclusion: SAMamba effectively bridges the domain gap, preserves details, and models long-range dependencies, making it a robust solution for ISTD.

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [277] [UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes](https://arxiv.org/pdf/2505.23253)
*Yixun Liang, Kunming Luo, Xiao Chen, Rui Chen, Hongyu Yan, Weiyu Li, Jiarui Liu, Ping Tan*

Main category: cs.CV

TL;DR: UniTEX is a two-stage 3D texture generation framework that bypasses UV mapping by using a 3D functional space, achieving high-quality, consistent textures for 3D assets.


<details>
  <summary>Details</summary>
Motivation: Existing UV-based methods face topological ambiguity issues when refining textures. UniTEX aims to overcome these limitations by operating directly in 3D space.

Method: 1. Uses Texture Functions (TFs) for continuous volumetric representation. 2. Predicts TFs from images/geometry via a transformer-based Large Texturing Model (LTM). 3. Employs LoRA-based strategy for high-quality multi-view texture synthesis.

Result: UniTEX outperforms existing methods in visual quality and texture integrity, offering a scalable solution for automated 3D texture generation.

Conclusion: UniTEX provides a generalizable and efficient approach for 3D texture generation, avoiding UV mapping limitations.

Abstract: We present UniTEX, a novel two-stage 3D texture generation framework to
create high-quality, consistent textures for 3D assets. Existing approaches
predominantly rely on UV-based inpainting to refine textures after reprojecting
the generated multi-view images onto the 3D shapes, which introduces challenges
related to topological ambiguity. To address this, we propose to bypass the
limitations of UV mapping by operating directly in a unified 3D functional
space. Specifically, we first propose that lifts texture generation into 3D
space via Texture Functions (TFs)--a continuous, volumetric representation that
maps any 3D point to a texture value based solely on surface proximity,
independent of mesh topology. Then, we propose to predict these TFs directly
from images and geometry inputs using a transformer-based Large Texturing Model
(LTM). To further enhance texture quality and leverage powerful 2D priors, we
develop an advanced LoRA-based strategy for efficiently adapting large-scale
Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as
our first stage. Extensive experiments demonstrate that UniTEX achieves
superior visual quality and texture integrity compared to existing approaches,
offering a generalizable and scalable solution for automated 3D texture
generation. Code will available in: https://github.com/YixunLiang/UniTEX.

</details>


### [278] [Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs](https://arxiv.org/pdf/2505.23265)
*Zheng Sun, Yi Wei, Long Yu*

Main category: cs.CV

TL;DR: The paper proposes a solution to improve image aesthetic reasoning in MLLMs using a new dataset and reinforcement learning method (DPA-GRPO), outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with image aesthetic reasoning due to lack of data and weak reasoning abilities, especially in medical image screening.

Method: Uses a dataset of 1500+ medical images with generated variants and applies DPA-GRPO, a reinforcement learning approach with long chains of thought.

Result: Outperforms large-scale and closed-source models (e.g., GPT-4o) in aesthetic reasoning, achieving better scores with a smaller model.

Conclusion: The approach could standardize image aesthetic reasoning, particularly in medical screening.

Abstract: Multimodal Large Language Models (MLLMs) are of great application across many
domains, such as multimodal understanding and generation. With the development
of diffusion models (DM) and unified MLLMs, the performance of image generation
has been significantly improved, however, the study of image screening is rare
and its performance with MLLMs is unsatisfactory due to the lack of data and
the week image aesthetic reasoning ability in MLLMs. In this work, we propose a
complete solution to address these problems in terms of data and methodology.
For data, we collect a comprehensive medical image screening dataset with 1500+
samples, each sample consists of a medical image, four generated images, and a
multiple-choice answer. The dataset evaluates the aesthetic reasoning ability
under four aspects: \textit{(1) Appearance Deformation, (2) Principles of
Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.
For methodology, we utilize long chains of thought (CoT) and Group Relative
Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,
to enhance the image aesthetic reasoning ability of MLLMs. Our experimental
results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o
and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic
reasoning. In contrast, by leveraging the reinforcement learning approach, we
are able to surpass the score of both large-scale models and leading
closed-source models using a much smaller model. We hope our attempt on medical
image screening will serve as a regular configuration in image aesthetic
reasoning in the future.

</details>


### [279] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/pdf/2505.23271)
*Mao-Lin Luo, Zi-Hao Zhou, Tong Wei, Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA introduces label-specific adapters to CLIP for continual learning, avoiding parameter partitioning and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods partition parameters for tasks, leading to inference errors and degraded performance.

Method: LADA adds lightweight, label-specific memory units to the frozen CLIP encoder, using feature distillation to prevent forgetting.

Result: LADA achieves state-of-the-art performance in continual learning.

Conclusion: LADA efficiently adapts CLIP for continual learning without modifying its core parameters.

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [280] [Are MLMs Trapped in the Visual Room?](https://arxiv.org/pdf/2505.23272)
*Yazhou Zhang, Chunwang Zou, Qimeng Liu, Lu Rong, Ben Yao, Zheng Lian, Qiuchi Li, Peng Zhang, Jing Qin*

Main category: cs.CV

TL;DR: The paper questions whether multi-modal large models (MLMs) truly understand images, proposing the Visual Room argument. It introduces a two-tier evaluation framework (perception and cognition) and a sarcasm dataset, revealing gaps in MLMs' understanding despite strong perception.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that perceptual mastery equates to genuine understanding in MLMs, inspired by Searle's Chinese Room.

Method: A two-tier evaluation framework (perception and cognition) tested on eight SoTA MLMs using a sarcasm dataset (924 images, 100 videos).

Result: MLMs excel in perception but struggle with sarcasm understanding (~16.1% error rate), highlighting gaps in emotional reasoning and context alignment.

Conclusion: The Visual Room argument is empirically supported, revealing limitations in MLMs' understanding and proposing a new evaluation paradigm.

Abstract: Can multi-modal large models (MLMs) that can ``see'' an image be said to
``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose
the \textbf{Visual Room} argument: a system may process and describe every
detail of visual inputs by following algorithmic rules, without genuinely
comprehending the underlying intention. This dilemma challenges the prevailing
assumption that perceptual mastery implies genuine understanding. In
implementation, we introduce a two-tier evaluation framework spanning
perception and cognition. The perception component evaluates whether MLMs can
accurately capture the surface-level details of visual contents, where the
cognitive component examines their ability to infer sarcasm polarity. To
support this framework, We further introduce a high-quality multi-modal sarcasm
dataset comprising both 924 static images and 100 dynamic videos. All sarcasm
labels are annotated by the original authors and verified by independent
reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art
(SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on
perception tasks; (2) even with correct perception, models exhibit an average
error rate of ~16.1\% in sarcasm understanding, revealing a significant gap
between seeing and understanding; (3) error analysis attributes this gap to
deficiencies in emotional reasoning, commonsense inference, and context
alignment. This work provides empirical grounding for the proposed Visual Room
argument and offers a new evaluation paradigm for MLMs.

</details>


### [281] [Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting](https://arxiv.org/pdf/2505.23280)
*Chuandong Liu, Huijiao Wang, Lei Yu, Gui-Song Xia*

Main category: cs.CV

TL;DR: MixGS is a holistic optimization framework for large-scale 3D scene reconstruction, improving rendering quality and efficiency by integrating global and local information without complex parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods lose global scene information and require complex tuning due to divide-and-conquer approaches.

Method: MixGS integrates camera pose and Gaussian attributes into a view-aware representation, decoded into fine Gaussians, and uses a mixing operation to preserve global coherence and local fidelity.

Result: MixGS achieves state-of-the-art rendering quality and competitive speed, reducing computational needs for training on a single 24GB VRAM GPU.

Conclusion: MixGS offers an efficient, high-quality solution for large-scale 3D scene reconstruction, with code available for public use.

Abstract: Recent advances in 3D Gaussian Splatting have shown remarkable potential for
novel view synthesis. However, most existing large-scale scene reconstruction
methods rely on the divide-and-conquer paradigm, which often leads to the loss
of global scene information and requires complex parameter tuning due to scene
partitioning and local optimization. To address these limitations, we propose
MixGS, a novel holistic optimization framework for large-scale 3D scene
reconstruction. MixGS models the entire scene holistically by integrating
camera pose and Gaussian attributes into a view-aware representation, which is
decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation
combines decoded and original Gaussians to jointly preserve global coherence
and local fidelity. Extensive experiments on large-scale scenes demonstrate
that MixGS achieves state-of-the-art rendering quality and competitive speed,
while significantly reducing computational requirements, enabling large-scale
scene reconstruction training on a single 24GB VRAM GPU. The code will be
released at https://github.com/azhuantou/MixGS.

</details>


### [282] [RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries](https://arxiv.org/pdf/2505.23283)
*Zhihong Tan, Jiayi Wang, Huiying Shi, Binyuan Huang, Hongchen Wei, Zhenzhong Chen*

Main category: cs.CV

TL;DR: The paper introduces RSFAKE-1M, a large-scale dataset for detecting forged remote sensing images generated by diffusion models, addressing a gap in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: The rise of forged remote sensing images poses risks to applications like environmental monitoring and national security, yet current benchmarks focus on GAN-based forgeries or natural images, leaving diffusion-based forgeries underexplored.

Method: The authors create RSFAKE-1M, a dataset of 500K real and 500K forged remote sensing images generated by ten diffusion models under six conditions. They evaluate existing detectors and unified baselines on this dataset.

Result: Diffusion-based forgeries are challenging for current detection methods, but models trained on RSFAKE-1M show improved generalization and robustness.

Conclusion: RSFAKE-1M is a critical resource for advancing forgery detection in remote sensing, highlighting the need for specialized datasets and methods to tackle diffusion-based forgeries.

Abstract: Detecting forged remote sensing images is becoming increasingly critical, as
such imagery plays a vital role in environmental monitoring, urban planning,
and national security. While diffusion models have emerged as the dominant
paradigm for image generation, their impact on remote sensing forgery detection
remains underexplored. Existing benchmarks primarily target GAN-based forgeries
or focus on natural images, limiting progress in this critical domain. To
address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged
and 500K real remote sensing images. The fake images are generated by ten
diffusion models fine-tuned on remote sensing data, covering six generation
conditions such as text prompts, structural guidance, and inpainting. This
paper presents the construction of RSFAKE-1M along with a comprehensive
experimental evaluation using both existing detectors and unified baselines.
The results reveal that diffusion-based remote sensing forgeries remain
challenging for current methods, and that models trained on RSFAKE-1M exhibit
notably improved generalization and robustness. Our findings underscore the
importance of RSFAKE-1M as a foundation for developing and evaluating
next-generation forgery detection approaches in the remote sensing domain. The
dataset and other supplementary materials are available at
https://huggingface.co/datasets/TZHSW/RSFAKE/.

</details>


### [283] [GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation](https://arxiv.org/pdf/2505.23287)
*Chikaha Tsuji, Enrique Flores Medina, Harshit Gupta, Md Ferdous Alam*

Main category: cs.CV

TL;DR: GenCAD-Self-Repairing improves the feasibility of generative CAD models by integrating diffusion guidance and a self-repairing pipeline, converting two-thirds of infeasible designs into feasible ones.


<details>
  <summary>Details</summary>
Motivation: The inability of GenCAD to consistently produce feasible boundary representations (B-reps) limits its practical application, prompting the need for a solution to enhance feasibility.

Method: The proposed framework uses guided diffusion denoising in latent space and a regression-based correction mechanism to refine infeasible CAD command sequences.

Result: The approach successfully converted two-thirds of infeasible designs into feasible ones, improving feasibility while maintaining geometric accuracy.

Conclusion: This method enhances the applicability of AI-driven CAD generation in manufacturing, architecture, and product design by improving feasibility and expanding training data availability.

Abstract: With the advancement of generative AI, research on its application to 3D
model generation has gained traction, particularly in automating the creation
of Computer-Aided Design (CAD) files from images. GenCAD is a notable model in
this domain, leveraging an autoregressive transformer-based architecture with a
contrastive learning framework to generate CAD programs.
  However, a major limitation of GenCAD is its inability to consistently
produce feasible boundary representations (B-reps), with approximately 10% of
generated designs being infeasible. To address this, we propose
GenCAD-Self-Repairing, a framework that enhances the feasibility of generative
CAD models through diffusion guidance and a self-repairing pipeline. This
framework integrates a guided diffusion denoising process in the latent space
and a regression-based correction mechanism to refine infeasible CAD command
sequences while preserving geometric accuracy. Our approach successfully
converted two-thirds of infeasible designs in the baseline method into feasible
ones, significantly improving the feasibility rate while simultaneously
maintaining a reasonable level of geometric accuracy between the point clouds
of ground truth models and generated models.
  By significantly improving the feasibility rate of generating CAD models, our
approach helps expand the availability of high-quality training data and
enhances the applicability of AI-driven CAD generation in manufacturing,
architecture, and product design.

</details>


### [284] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/pdf/2505.23292)
*Evangelos Charalampakis, Vasileios Mygdalis, Ioannis Pitas*

Main category: cs.CV

TL;DR: FUSS enables decentralized, unsupervised semantic image segmentation in federated learning by aligning features and centroids across clients, outperforming local and classical FL methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of feature and centroid alignment in federated settings for unsupervised semantic segmentation under heterogeneous data.

Method: Proposes FUSS, a framework with novel federation strategies for global consistency in feature and prototype space, optimizing local segmentation heads and shared centroids.

Result: FUSS outperforms local-only training and classical FL extensions on benchmark and real-world datasets for binary and multi-class segmentation.

Conclusion: FUSS is the first fully decentralized, label-free semantic segmentation framework, with reproducible code to be released.

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [285] [TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models](https://arxiv.org/pdf/2505.23312)
*Finn Carter*

Main category: cs.CV

TL;DR: TRACE is a method to erase unwanted concepts from diffusion models while maintaining generative quality, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Address concerns about undesirable outputs (e.g., explicit content, sensitive identities) in text-to-image diffusion models.

Method: Combines theoretical framework for concept suppression with fine-tuning, focusing on cross-attention layers and trajectory-aware denoising.

Result: TRACE outperforms methods like ANT, EraseAnything, and MACE in removal efficacy and output quality.

Conclusion: TRACE effectively erases targeted concepts without compromising overall model performance.

Abstract: Text-to-image diffusion models have shown unprecedented generative
capability, but their ability to produce undesirable concepts
(e.g.~pornographic content, sensitive identities, copyrighted styles) poses
serious concerns for privacy, fairness, and safety. {Concept erasure} aims to
remove or suppress specific concept information in a generative model. In this
paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept
Erasure)}, a novel method to erase targeted concepts from diffusion models
while preserving overall generative quality. Our approach combines a rigorous
theoretical framework, establishing formal conditions under which a concept can
be provably suppressed in the diffusion process, with an effective fine-tuning
procedure compatible with both conventional latent diffusion (Stable Diffusion)
and emerging rectified flow models (e.g.~FLUX). We first derive a closed-form
update to the model's cross-attention layers that removes hidden
representations of the target concept. We then introduce a trajectory-aware
finetuning objective that steers the denoising process away from the concept
only in the late sampling stages, thus maintaining the model's fidelity on
unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used
in prior concept erasure studies (object classes, celebrity faces, artistic
styles, and explicit content from the I2P dataset). TRACE achieves
state-of-the-art performance, outperforming recent methods such as ANT,
EraseAnything, and MACE in terms of removal efficacy and output quality.

</details>


### [286] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/pdf/2505.23313)
*Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, Jin Tang*

Main category: cs.CV

TL;DR: The paper proposes an adversarial attack and defense framework for Pedestrian Attribute Recognition (PAR), addressing its vulnerability and anti-interference issues.


<details>
  <summary>Details</summary>
Motivation: To explore and mitigate the vulnerability of PAR systems to adversarial attacks, which has not been fully studied.

Method: Uses global- and patch-level attacks on a CLIP-based PAR framework, with adversarial noise generation (ASL-PAR) and a semantic offset defense strategy.

Result: Validated on digital and physical domains, showing effectiveness of the proposed attack and defense strategies.

Conclusion: The framework successfully addresses PAR vulnerabilities, with code to be released for further research.

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [287] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/pdf/2505.23331)
*Matteo Gallici, Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: RL-based fine-tuning with GRPO improves next-scale VAR models, aligning them with human preferences and enabling style control and generalization beyond pre-training data.


<details>
  <summary>Details</summary>
Motivation: To align generative models with nuanced human preferences and enhance image quality and style control.

Method: Apply Group Relative Policy Optimization (GRPO) to fine-tune visual autoregressive (VAR) models using RL, leveraging aesthetic predictors and CLIP embeddings.

Result: Improved image quality, precise style control, and generalization beyond the initial ImageNet distribution.

Conclusion: RL-based fine-tuning is efficient and effective for VAR models, benefiting from their fast inference speeds.

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [288] [Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis](https://arxiv.org/pdf/2505.23325)
*Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, Bin Wang*

Main category: cs.CV

TL;DR: The paper introduces DRA-Ctrl, a method to repurpose video generative models for controllable image generation, leveraging their high-dimensional capabilities for lower-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: To explore if fully trained video generative models can support lower-dimensional tasks like controllable image generation, bridging the gap between video and image generation.

Method: Proposes DRA-Ctrl, using video models' strengths (e.g., long-range context modeling) with a mixup-based transition strategy and redesigned attention structure for smooth adaptation.

Result: Repurposed video models outperform direct image-trained models in tasks like subject-driven and spatially conditioned generation.

Conclusion: DRA-Ctrl demonstrates the potential of video models for broader visual tasks and lays groundwork for unified generative models across modalities.

Abstract: Video generative models can be regarded as world simulators due to their
ability to capture dynamic, continuous changes inherent in real-world
environments. These models integrate high-dimensional information across
visual, temporal, spatial, and causal dimensions, enabling predictions of
subjects in various status. A natural and valuable research direction is to
explore whether a fully trained video generative model in high-dimensional
space can effectively support lower-dimensional tasks such as controllable
image generation. In this work, we propose a paradigm for video-to-image
knowledge compression and task adaptation, termed \textit{Dimension-Reduction
Attack} (\texttt{DRA-Ctrl}), which utilizes the strengths of video models,
including long-range context modeling and flatten full-attention, to perform
various generation tasks. Specially, to address the challenging gap between
continuous video frames and discrete image generation, we introduce a
mixup-based transition strategy that ensures smooth adaptation. Moreover, we
redesign the attention structure with a tailored masking mechanism to better
align text prompts with image-level control. Experiments across diverse image
generation tasks, such as subject-driven and spatially conditioned generation,
show that repurposed video models outperform those trained directly on images.
These results highlight the untapped potential of large-scale video generators
for broader visual applications. \texttt{DRA-Ctrl} provides new insights into
reusing resource-intensive video models and lays foundation for future unified
generative models across visual modalities. The project page is
https://dra-ctrl-2025.github.io/DRA-Ctrl/.

</details>


### [289] [DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification](https://arxiv.org/pdf/2505.23341)
*Daoxi Cao, Hangbei Cheng, Yijin Li, Ruolin Zhou, Xinyi Li, Xuehan Zhang, Binwei Li, Xuancheng Gu, Xueyu Liu, Yongfei Wu*

Main category: cs.CV

TL;DR: DSAGL is a weakly supervised framework for classifying whole-slide images, combining teacher-student architecture and dual-stream design to address annotation scarcity and instance ambiguity.


<details>
  <summary>Details</summary>
Motivation: Whole-slide images (WSIs) are vital for cancer diagnosis but face challenges due to their size and lack of fine-grained annotations, necessitating weakly supervised methods.

Method: DSAGL uses a dual-stream design with attention-guided pseudo labels, a lightweight encoder (VSSMamba), and a fusion-attentive module (FASA) for efficient learning and focus on key regions.

Result: DSAGL outperforms state-of-the-art MIL baselines on CIFAR-10, NCT-CRC, and TCGA-Lung datasets, showing superior performance and robustness.

Conclusion: DSAGL effectively addresses weakly supervised WSI classification, demonstrating strong performance and potential for clinical applications.

Abstract: Whole-slide images (WSIs) are critical for cancer diagnosis due to their
ultra-high resolution and rich semantic content. However, their massive size
and the limited availability of fine-grained annotations pose substantial
challenges for conventional supervised learning. We propose DSAGL (Dual-Stream
Attention-Guided Learning), a novel weakly supervised classification framework
that combines a teacher-student architecture with a dual-stream design. DSAGL
explicitly addresses instance-level ambiguity and bag-level semantic
consistency by generating multi-scale attention-based pseudo labels and guiding
instance-level learning. A shared lightweight encoder (VSSMamba) enables
efficient long-range dependency modeling, while a fusion-attentive module
(FASA) enhances focus on sparse but diagnostically relevant regions. We further
introduce a hybrid loss to enforce mutual consistency between the two streams.
Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL
consistently outperforms state-of-the-art MIL baselines, achieving superior
discriminative performance and robustness under weak supervision.

</details>


### [290] [Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering](https://arxiv.org/pdf/2505.23343)
*Sixian Wang, Zhiwei Tang, Tsung-Hui Chang*

Main category: cs.CV

TL;DR: CFG-Rejection improves diffusion model sample quality by filtering low-quality samples early using Accumulated Score Differences (ASD), without external rewards or retraining.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent sample quality in diffusion models without costly fine-tuning or external signals.

Method: Exploit correlation between sample quality and ASD in classifier-free guidance (CFG) to filter samples early.

Result: Improves human preference scores (HPSv2, PickScore) and benchmarks (GenEval, DPG-Bench).

Conclusion: CFG-Rejection offers efficient, plug-and-play quality enhancement for diffusion models, applicable beyond images.

Abstract: Diffusion models often exhibit inconsistent sample quality due to stochastic
variations inherent in their sampling trajectories. Although training-based
fine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to
improve sample fidelity, they typically necessitate full denoising processes
and external reward signals. This incurs substantial computational costs,
hindering their broader applicability. In this work, we unveil an intriguing
phenomenon: a previously unobserved yet exploitable link between sample quality
and characteristics of the denoising trajectory during classifier-free guidance
(CFG). Specifically, we identify a strong correlation between high-density
regions of the sample distribution and the Accumulated Score Differences
(ASD)--the cumulative divergence between conditional and unconditional scores.
Leveraging this insight, we introduce CFG-Rejection, an efficient,
plug-and-play strategy that filters low-quality samples at an early stage of
the denoising process, crucially without requiring external reward signals or
model retraining. Importantly, our approach necessitates no modifications to
model architectures or sampling schedules and maintains full compatibility with
existing diffusion frameworks. We validate the effectiveness of CFG-Rejection
in image generation through extensive experiments, demonstrating marked
improvements on human preference scores (HPSv2, PickScore) and challenging
benchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer
significant advantages for diverse generative modalities beyond images, paving
the way for more efficient and reliable high-quality sample generation.

</details>


### [291] [Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching](https://arxiv.org/pdf/2505.23346)
*Yexiong Lin, Yu Yao, Tongliang Liu*

Main category: cs.CV

TL;DR: Flow Matching (FM) trains models to transport samples between distributions. Early FM methods used random couplings, causing non-straight trajectories. Recent methods use Optimal Transport (OT) to reduce path crossings, but these couplings may not align with the model's preferred trajectories. Model-Aligned Coupling (MAC) addresses this by matching couplings based on geometric distance and model alignment, improving generation quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Early FM methods result in non-straight trajectories due to random couplings, while OT-based couplings may not align with the model's preferences. MAC aims to improve trajectory alignment and generation efficiency.

Method: MAC matches training couplings based on geometric distance and alignment with the model's preferred transport directions, selecting the top-$k$ couplings with the lowest prediction error for training.

Result: MAC significantly improves generation quality and efficiency in few-step settings compared to existing methods.

Conclusion: MAC effectively aligns couplings with the model's preferences, enhancing FM performance in generating high-quality samples efficiently.

Abstract: Flow Matching (FM) is an effective framework for training a model to learn a
vector field that transports samples from a source distribution to a target
distribution. To train the model, early FM methods use random couplings, which
often result in crossing paths and lead the model to learn non-straight
trajectories that require many integration steps to generate high-quality
samples. To address this, recent methods adopt Optimal Transport (OT) to
construct couplings by minimizing geometric distances, which helps reduce path
crossings. However, we observe that such geometry-based couplings do not
necessarily align with the model's preferred trajectories, making it difficult
to learn the vector field induced by these couplings, which prevents the model
from learning straight trajectories. Motivated by this, we propose
Model-Aligned Coupling (MAC), an effective method that matches training
couplings based not only on geometric distance but also on alignment with the
model's preferred transport directions based on its prediction error. To avoid
the time-costly match process, MAC proposes to select the top-$k$ fraction of
couplings with the lowest error for training. Extensive experiments show that
MAC significantly improves generation quality and efficiency in few-step
settings compared to existing methods. Project page:
https://yexionglin.github.io/mac

</details>


### [292] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/pdf/2505.23367)
*Jeonghyeok Do, Sungpyo Kim, Geunhyuk Youk, Jaehyup Lee, Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter addresses cross-modality misalignment in PAN-sharpening by jointly reconstructing HRMS and PAN images and using bidirectional alignment for adaptive feature refinement.


<details>
  <summary>Details</summary>
Motivation: Cross-modality misalignment in PAN-sharpening causes spectral distortion and blurring, which conventional methods fail to address due to assumptions of perfect alignment.

Method: Proposes PAN-Crafter with Modality-Adaptive Reconstruction (MARs) and Cross-Modality Alignment-Aware Attention (CM3A) for joint reconstruction and bidirectional alignment.

Result: Outperforms state-of-the-art methods in all metrics, with faster inference and smaller memory size, and generalizes well to unseen datasets.

Conclusion: PAN-Crafter effectively mitigates misalignment, improving PAN-sharpening performance and robustness.

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [293] [Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model](https://arxiv.org/pdf/2505.23358)
*Reem AlJunaid, Muzammil Behzad*

Main category: cs.CV

TL;DR: KRCapVLM improves image captioning by using knowledge replay and vision-language models, enhancing caption quality and knowledge recognition.


<details>
  <summary>Details</summary>
Motivation: Existing captioning models often produce generic descriptions lacking specificity and contextual depth.

Method: Proposes KRCapVLM with beam search decoding, attention-based modules, and training schedulers.

Result: Substantial gains in caption quality and knowledge recognition, with better generalization to unseen concepts.

Conclusion: The approach effectively enhances meaningful, knowledge-grounded caption generation.

Abstract: Generating informative and knowledge-rich image captions remains a challenge
for many existing captioning models, which often produce generic descriptions
that lack specificity and contextual depth. To address this limitation, we
propose KRCapVLM, a knowledge replay-based novel image captioning framework
using vision-language model. We incorporate beam search decoding to generate
more diverse and coherent captions. We also integrate attention-based modules
into the image encoder to enhance feature representation. Finally, we employ
training schedulers to improve stability and ensure smoother convergence during
training. These proposals accelerate substantial gains in both caption quality
and knowledge recognition. Our proposed model demonstrates clear improvements
in both the accuracy of knowledge recognition and the overall quality of
generated captions. It shows a stronger ability to generalize to previously
unseen knowledge concepts, producing more informative and contextually relevant
descriptions. These results indicate the effectiveness of our approach in
enhancing the model's capacity to generate meaningful, knowledge-grounded
captions across a range of scenarios.

</details>


### [294] [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/pdf/2505.23359)
*Yuanxin Liu, Kun Ouyang, Haoning Wu, Yi Liu, Lin Sui, Xinhao Li, Yan Zhong, Y. Charles, Xinyu Zhou, Xu Sun*

Main category: cs.CV

TL;DR: VideoReasonBench is a new benchmark for vision-centric video reasoning, showing current models struggle with complex tasks, but extended reasoning helps.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks lack depth for evaluating extended chain-of-thought reasoning, and tasks are often knowledge-driven rather than vision-centric.

Method: Introduce VideoReasonBench with videos depicting fine-grained operations on latent states, testing three reasoning levels: recall, inference, and prediction.

Result: Most MLLMs perform poorly (e.g., GPT-4o at 6.9%), but Gemini-2.5-Pro excels (56.0%). Extended reasoning improves performance on VideoReasonBench.

Conclusion: VideoReasonBench highlights the need for vision-centric reasoning and shows extended CoT benefits complex video tasks.

Abstract: Recent studies have shown that long chain-of-thought (CoT) reasoning can
significantly enhance the performance of large language models (LLMs) on
complex tasks. However, this benefit is yet to be demonstrated in the domain of
video understanding, since most existing benchmarks lack the reasoning depth
required to demonstrate the advantages of extended CoT chains. While recent
efforts have proposed benchmarks aimed at video reasoning, the tasks are often
knowledge-driven and do not rely heavily on visual content. To bridge this gap,
we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,
complex video reasoning. To ensure visual richness and high reasoning
complexity, each video in VideoReasonBench depicts a sequence of fine-grained
operations on a latent state that is only visible in part of the video. The
questions evaluate three escalating levels of video reasoning skills: recalling
observed visual information, inferring the content of latent states, and
predicting information beyond the video. Under such task setting, models have
to precisely recall multiple operations in the video, and perform step-by-step
reasoning to get correct final answers for these questions. Using
VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal
LLMs (MLLMs), finding that most perform poorly on complex video reasoning,
e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced
Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our
investigations on "test-time scaling" further reveal that extended thinking
budget, while offering none or minimal benefits on existing video benchmarks,
is essential for improving the performance on VideoReasonBench.

</details>


### [295] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/pdf/2505.23386)
*Han Bao, Qinying Wang, Zhi Chen, Qingming Li, Xuhong Zhang, Changjiang Li, Zonghui Wang, Shouling Ji, Wenzhi Chen*

Main category: cs.CV

TL;DR: VModA is a framework for detecting NSFW content, outperforming existing methods by 54.3% in accuracy and adapting to diverse moderation rules and complex semantics.


<details>
  <summary>Details</summary>
Motivation: NSFW content on social networks harms users, especially minors, and current detection methods fail against sophisticated evasion techniques and varying regulations.

Method: Proposes VModA, a framework adaptable to diverse moderation rules and capable of handling complex, semantically rich NSFW content.

Result: VModA improves accuracy by up to 54.3% across NSFW types, shows adaptability, and corrects inconsistencies in benchmark datasets.

Conclusion: VModA is effective for real-world NSFW detection, addressing current limitations and improving accuracy and adaptability.

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [296] [MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification](https://arxiv.org/pdf/2505.23365)
*Yang Qiao, Xiaoyu Zhong, Xiaofeng Gu, Zhiguo Yu*

Main category: cs.CV

TL;DR: A novel Multimodal Collaborative Fusion Network (MCFNet) improves fine-grained image classification by addressing cross-modal dependencies and enhancing semantic alignment.


<details>
  <summary>Details</summary>
Motivation: Conventional methods struggle with intricate cross-modal dependencies, limiting high-precision classification.

Method: MCFNet uses a regularized fusion module and hybrid attention for intra-modal feature enhancement and semantic alignment, plus a multimodal decision module with weighted voting.

Result: MCFNet achieves consistent accuracy improvements on benchmark datasets.

Conclusion: MCFNet effectively models subtle cross-modal semantics for superior classification performance.

Abstract: Multimodal information processing has become increasingly important for
enhancing image classification performance. However, the intricate and implicit
dependencies across different modalities often hinder conventional methods from
effectively capturing fine-grained semantic interactions, thereby limiting
their applicability in high-precision classification tasks. To address this
issue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet)
designed for fine-grained classification. The proposed MCFNet architecture
incorporates a regularized integrated fusion module that improves intra-modal
feature representation through modality-specific regularization strategies,
while facilitating precise semantic alignment via a hybrid attention mechanism.
Additionally, we introduce a multimodal decision classification module, which
jointly exploits inter-modal correlations and unimodal discriminative features
by integrating multiple loss functions within a weighted voting paradigm.
Extensive experiments and ablation studies on benchmark datasets demonstrate
that the proposed MCFNet framework achieves consistent improvements in
classification accuracy, confirming its effectiveness in modeling subtle
cross-modal semantics.

</details>


### [297] [UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning](https://arxiv.org/pdf/2505.23380)
*Weijia Mao, Zhenheng Yang, Mike Zheng Shou*

Main category: cs.CV

TL;DR: UniRL is a self-improving post-training approach for multimodal large language models, eliminating the need for external data by generating and using its own training samples.


<details>
  <summary>Details</summary>
Motivation: Existing models rely on large datasets and external data for post-training, limiting flexibility and efficiency. UniRL aims to overcome these limitations.

Method: UniRL generates images from prompts for training, using supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize tasks.

Result: UniRL improves task performance (GenEval scores: 0.77 for Show-o, 0.65 for Janus) and balances generation and understanding tasks.

Conclusion: UniRL offers a data-efficient, self-improving solution for multimodal models, enhancing performance without external dependencies.

Abstract: Unified multimodal large language models such as Show-o and Janus have
achieved strong performance across both generation and understanding tasks.
However, these models typically rely on large-scale datasets and require
substantial computation during the pretraining stage. In addition, several
post-training methods have been proposed, but they often depend on external
data or are limited to task-specific customization. In this work, we introduce
UniRL, a self-improving post-training approach. Our approach enables the model
to generate images from prompts and use them as training data in each
iteration, without relying on any external image data. Moreover, it enables the
two tasks to enhance each other: the generated images are used for
understanding, and the understanding results are used to supervise generation.
We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization
(GRPO) to optimize the models. UniRL offers three key advantages: (1) it
requires no external image data, as all training samples are generated by the
model itself during training; (2) it not only improves individual task
performance, but also reduces the imbalance between generation and
understanding; and (3) it requires only several additional training steps
during the post-training stage. We evaluate UniRL on top of Show-o and Janus,
achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and
models will be released in https://github.com/showlab/UniRL.

</details>


### [298] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/pdf/2505.23406)
*Binyamin Manela, Sharon Gannot, Ethan Fetyaya*

Main category: cs.CV

TL;DR: EdiDub introduces a content-aware editing framework for visual dubbing, outperforming traditional methods by preserving original video context and improving lip synchronization.


<details>
  <summary>Details</summary>
Motivation: Current visual dubbing methods struggle with seamless integration into original scenes and often discard vital visual information, limiting their effectiveness.

Method: EdiDub reformulates visual dubbing as a content-aware editing task, using a specialized conditioning scheme for faithful modifications.

Result: EdiDub improves identity preservation and synchronization on benchmarks, including occluded-lip datasets, and scores higher in human evaluations.

Conclusion: EdiDub's content-aware editing approach outperforms traditional generation or inpainting, maintaining complex visual elements while ensuring accurate lip synchronization.

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [299] [Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\textsuperscript{\textregistered} Assessment](https://arxiv.org/pdf/2505.23392)
*Yun-Cheng Tsai*

Main category: cs.CV

TL;DR: A lightweight YOLOv11n-based detector combined with FUSegNet improves wound segmentation across body sites without fine-tuning, using only 500 bounding boxes.


<details>
  <summary>Details</summary>
Motivation: Existing models like FUSegNet, trained on foot ulcers, fail to generalize to other wounds, necessitating a more adaptable solution.

Method: Proposes a pipeline combining YOLOv11n for detection and FUSegNet for segmentation, requiring only 500 bounding boxes and no fine-tuning.

Result: Improved mean IoU by 23 points and DESIGN-R accuracy from 71% to 94% across foot, sacral, and trochanter wounds.

Conclusion: The method generalizes without fine-tuning, enabling scalable wound segmentation and DESIGN-R automation with minimal supervision.

Abstract: Purpose: Accurate wound segmentation is essential for automated DESIGN-R
scoring. However, existing models such as FUSegNet, which are trained primarily
on foot ulcer datasets, often fail to generalize to wounds on other body sites.
  Methods: We propose an annotation-efficient pipeline that combines a
lightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation
model. Instead of relying on pixel-level annotations or retraining for new
anatomical regions, our method achieves robust performance using only 500
manually labeled bounding boxes. This zero fine-tuning approach effectively
bridges the domain gap and enables direct deployment across diverse wound
types. This is an advance not previously demonstrated in the wound segmentation
literature.
  Results: Evaluated on three real-world test sets spanning foot, sacral, and
trochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23
percentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size
estimation accuracy from 71 percent to 94 percent (see Table 3 for details).
  Conclusion: Our pipeline generalizes effectively across body sites without
task-specific fine-tuning, demonstrating that minimal supervision, with 500
annotated ROIs, is sufficient for scalable, annotation-light wound
segmentation. This capability paves the way for real-world DESIGN-R automation,
reducing reliance on pixel-wise labeling, streamlining documentation workflows,
and supporting objective and consistent wound scoring in clinical practice. We
will publicly release the trained detector weights and configuration to promote
reproducibility and facilitate downstream deployment.

</details>


### [300] [Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings](https://arxiv.org/pdf/2505.23395)
*Xingguang Wei, Haomin Wang, Shenglong Ye, Ruifeng Luo, Yanting Zhang, Lixin Gu, Jifeng Dai, Yu Qiao, Wenhai Wang, Hongjie Zhang*

Main category: cs.CV

TL;DR: VecFormer introduces a line-based representation for panoptic symbol spotting in CAD drawings, improving accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for panoptic symbol spotting in CAD drawings suffer from high computational costs, limited generality, and loss of geometric information.

Method: VecFormer uses line-based representation of primitives and a Branch Fusion Refinement module to integrate instance and semantic predictions.

Result: Achieves 91.1 PQ, with Stuff-PQ improved by 9.6 and 21.2 points over second-best results.

Conclusion: Line-based representation shows strong potential for vector graphic understanding.

Abstract: We study the task of panoptic symbol spotting, which involves identifying
both individual instances of countable things and the semantic regions of
uncountable stuff in computer-aided design (CAD) drawings composed of vector
graphical primitives. Existing methods typically rely on image rasterization,
graph construction, or point-based representation, but these approaches often
suffer from high computational costs, limited generality, and loss of geometric
structural information. In this paper, we propose VecFormer, a novel method
that addresses these challenges through line-based representation of
primitives. This design preserves the geometric continuity of the original
primitive, enabling more accurate shape representation while maintaining a
computation-friendly structure, making it well-suited for vector graphic
understanding tasks. To further enhance prediction reliability, we introduce a
Branch Fusion Refinement module that effectively integrates instance and
semantic predictions, resolving their inconsistencies for more coherent
panoptic outputs. Extensive experiments demonstrate that our method establishes
a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and
21.2 points over the second-best results under settings with and without prior
information, respectively, highlighting the strong potential of line-based
representation as a foundation for vector graphic understanding.

</details>


### [301] [Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation](https://arxiv.org/pdf/2505.23400)
*Sanggyun Ma, Wonjoon Choi, Jihun Park, Jaeyeul Kim, Seunghun Lee, Jiwan Seo, Sunghoon Im*

Main category: cs.CV

TL;DR: BriGeS integrates geometric and semantic info via a Bridging Gate and Attention Temperature Scaling, enhancing Monocular Depth Estimation with reduced training demands.


<details>
  <summary>Details</summary>
Motivation: To improve Monocular Depth Estimation by combining geometric and semantic strengths from foundation models, addressing challenges like complex scenes and overlapping objects.

Method: Uses a Bridging Gate to fuse depth and segmentation models, refined by Attention Temperature Scaling. Focuses on training only the Bridging Gate to save resources.

Result: Outperforms state-of-the-art methods in complex scenes, handling intricate structures and overlaps effectively.

Conclusion: BriGeS is a resource-efficient, high-performing solution for Monocular Depth Estimation by leveraging foundation models.

Abstract: We present Bridging Geometric and Semantic (BriGeS), an effective method that
fuses geometric and semantic information within foundation models to enhance
Monocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which
integrates the complementary strengths of depth and segmentation foundation
models. This integration is further refined by our Attention Temperature
Scaling technique. It finely adjusts the focus of the attention mechanisms to
prevent over-concentration on specific features, thus ensuring balanced
performance across diverse inputs. BriGeS capitalizes on pre-trained foundation
models and adopts a strategy that focuses on training only the Bridging Gate.
This method significantly reduces resource demands and training time while
maintaining the model's ability to generalize effectively. Extensive
experiments across multiple challenging datasets demonstrate that BriGeS
outperforms state-of-the-art methods in MDE for complex scenes, effectively
handling intricate structures and overlapping objects.

</details>


### [302] [UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors](https://arxiv.org/pdf/2505.23434)
*Tianhang Wang, Fan Lu, Sanqing Qu, Guo Yu, Shihang Du, Ya Wu, Yuan Huang, Guang Chen*

Main category: cs.CV

TL;DR: UrbanCraft addresses Extrapolated View Synthesis (EVS) by using hierarchical sem-geometric representations and HSG-VSD for improved generalization beyond training camera views.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with novel views outside training distributions, limiting urban reconstruction applications.

Method: UrbanCraft employs coarse semantic-geometric priors and fine instance-level details, enhanced by HSG-VSD for consistency.

Result: The method shows effectiveness in EVS through qualitative and quantitative comparisons.

Conclusion: UrbanCraft improves generalization for urban scene reconstruction by integrating hierarchical priors and semantic-geometric constraints.

Abstract: Existing neural rendering-based urban scene reconstruction methods mainly
focus on the Interpolated View Synthesis (IVS) setting that synthesizes from
views close to training camera trajectory. However, IVS can not guarantee the
on-par performance of the novel view outside the training camera distribution
(\textit{e.g.}, looking left, right, or downwards), which limits the
generalizability of the urban reconstruction application. Previous methods have
optimized it via image diffusion, but they fail to handle text-ambiguous or
large unseen view angles due to coarse-grained control of text-only diffusion.
In this paper, we design UrbanCraft, which surmounts the Extrapolated View
Synthesis (EVS) problem using hierarchical sem-geometric representations
serving as additional priors. Specifically, we leverage the partially
observable scene to reconstruct coarse semantic and geometric primitives,
establishing a coarse scene-level prior through an occupancy grid as the base
representation. Additionally, we incorporate fine instance-level priors from 3D
bounding boxes to enhance object-level details and spatial relationships.
Building on this, we propose the \textbf{H}ierarchical
\textbf{S}emantic-Geometric-\textbf{G}uided Variational Score Distillation
(HSG-VSD), which integrates semantic and geometric constraints from pretrained
UrbanCraft2D into the score distillation sampling process, forcing the
distribution to be consistent with the observable scene. Qualitative and
quantitative comparisons demonstrate the effectiveness of our methods on EVS
problem.

</details>


### [303] [Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation](https://arxiv.org/pdf/2505.23438)
*Lingyan Ran, Yali Li, Tao Zhuo, Shizhou Zhang, Yanning Zhang*

Main category: cs.CV

TL;DR: Spatial augmentation in semi-supervised semantic segmentation improves performance, and an adaptive strategy (ASAug) further enhances results.


<details>
  <summary>Details</summary>
Motivation: Existing strong augmentation methods focus on intensity-based perturbations, neglecting spatial augmentations despite their proven effectiveness in supervised tasks.

Method: Proposes Adaptive Spatial Augmentation (ASAug), which dynamically adjusts augmentations per instance based on entropy.

Result: ASAug improves performance of existing methods and achieves state-of-the-art results on PASCAL VOC 2012, Cityscapes, and COCO.

Conclusion: Spatial augmentations are effective in SSSS, and ASAug is a versatile, pluggable solution for enhancing model performance.

Abstract: In semi-supervised semantic segmentation (SSSS), data augmentation plays a
crucial role in the weak-to-strong consistency regularization framework, as it
enhances diversity and improves model generalization. Recent strong
augmentation methods have primarily focused on intensity-based perturbations,
which have minimal impact on the semantic masks. In contrast, spatial
augmentations like translation and rotation have long been acknowledged for
their effectiveness in supervised semantic segmentation tasks, but they are
often ignored in SSSS. In this work, we demonstrate that spatial augmentation
can also contribute to model training in SSSS, despite generating inconsistent
masks between the weak and strong augmentations. Furthermore, recognizing the
variability among images, we propose an adaptive augmentation strategy that
dynamically adjusts the augmentation for each instance based on entropy.
Extensive experiments show that our proposed Adaptive Spatial Augmentation
(\textbf{ASAug}) can be integrated as a pluggable module, consistently
improving the performance of existing methods and achieving state-of-the-art
results on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.

</details>


### [304] [VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration](https://arxiv.org/pdf/2505.23439)
*Ben Li, Minqi Li, Jie Ren, Kaibing Zhang*

Main category: cs.CV

TL;DR: VITON-DRR improves virtual try-on by preserving garment details via non-rigid registration and dual-pyramid feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to preserve clothing details due to misalignment and self-occlusion, limiting realistic virtual try-on results.

Method: Uses a dual-pyramid feature extractor for semantic segmentation, a Deformation Module for cloth key points, and an Image Synthesis Module for garment warping and pose adaptation.

Result: Outperforms state-of-the-art methods in accuracy and detail retention.

Conclusion: VITON-DRR advances virtual try-on by addressing detail loss and misalignment challenges.

Abstract: Image-based virtual try-on aims to fit a target garment to a specific person
image and has attracted extensive research attention because of its huge
application potential in the e-commerce and fashion industries. To generate
high-quality try-on results, accurately warping the clothing item to fit the
human body plays a significant role, as slight misalignment may lead to
unrealistic artifacts in the fitting image. Most existing methods warp the
clothing by feature matching and thin-plate spline (TPS). However, it often
fails to preserve clothing details due to self-occlusion, severe misalignment
between poses, etc. To address these challenges, this paper proposes a detail
retention virtual try-on method via accurate non-rigid registration (VITON-DRR)
for diverse human poses. Specifically, we reconstruct a human semantic
segmentation using a dual-pyramid-structured feature extractor. Then, a novel
Deformation Module is designed for extracting the cloth key points and warping
them through an accurate non-rigid registration algorithm. Finally, the Image
Synthesis Module is designed to synthesize the deformed garment image and
generate the human pose information adaptively. {Compared with} traditional
methods, the proposed VITON-DRR can make the deformation of fitting images more
accurate and retain more garment details. The experimental results demonstrate
that the proposed method performs better than state-of-the-art methods.

</details>


### [305] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/pdf/2505.23444)
*Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Yuheng Zhang, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Min Xu*

Main category: cs.CV

TL;DR: CryoCCD is a framework combining biophysical modeling and generative techniques to create realistic cryo-EM micrographs, improving downstream analysis tasks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality annotated cryo-EM data and the limitations of existing synthetic data methods in capturing structural diversity and noise patterns drive the need for CryoCCD.

Method: CryoCCD integrates biophysical modeling with generative techniques, using a conditional diffusion model with cycle consistency and mask-aware contrastive learning for realistic noise.

Result: CryoCCD generates structurally accurate micrographs and outperforms baselines in particle picking and reconstruction tasks.

Conclusion: CryoCCD effectively addresses the challenges of synthetic cryo-EM data generation, enhancing downstream analysis performance.

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [306] [A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation](https://arxiv.org/pdf/2505.23451)
*Shuzhou Sun, Li Liu, Tianpeng Liu, Shuaifeng Zhi, Ming-Ming Cheng, Janne Heikkilä, Yongxiang Liu*

Main category: cs.CV

TL;DR: The paper introduces RcSGG, a reverse causal framework for Scene Graph Generation (SGG), to address spurious correlations and biases in existing two-stage SGG methods.


<details>
  <summary>Details</summary>
Motivation: Existing SGG frameworks suffer from spurious correlations due to their causal chain structure, leading to biases like misclassifying tail relationships as head ones and foreground as background.

Method: Proposes RcSGG, which reconstructs the causal chain into a reverse causal structure, using Active Reverse Estimation (ARE) and Maximum Information Sampling (MIS) to enhance causality estimation.

Result: RcSGG achieves state-of-the-art mean recall rates on benchmarks, mitigating biases effectively.

Conclusion: RcSGG successfully addresses biases in SGG by reversing the causal structure, improving performance and reducing spurious correlations.

Abstract: Existing two-stage Scene Graph Generation (SGG) frameworks typically
incorporate a detector to extract relationship features and a classifier to
categorize these relationships; therefore, the training paradigm follows a
causal chain structure, where the detector's inputs determine the classifier's
inputs, which in turn influence the final predictions. However, such a causal
chain structure can yield spurious correlations between the detector's inputs
and the final predictions, i.e., the prediction of a certain relationship may
be influenced by other relationships. This influence can induce at least two
observable biases: tail relationships are predicted as head ones, and
foreground relationships are predicted as background ones; notably, the latter
bias is seldom discussed in the literature. To address this issue, we propose
reconstructing the causal chain structure into a reverse causal structure,
wherein the classifier's inputs are treated as the confounder, and both the
detector's inputs and the final predictions are viewed as causal variables.
Specifically, we term the reconstructed causal paradigm as the Reverse causal
Framework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse
Estimation (ARE) to intervene on the confounder to estimate the reverse
causality, \ie the causality from final predictions to the classifier's inputs.
Then, the Maximum Information Sampling (MIS) is suggested to enhance the
reverse causality estimation further by considering the relationship
information. Theoretically, RcSGG can mitigate the spurious correlations
inherent in the SGG framework, subsequently eliminating the induced biases.
Comprehensive experiments on popular benchmarks and diverse SGG frameworks show
the state-of-the-art mean recall rate.

</details>


### [307] [LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter](https://arxiv.org/pdf/2505.23462)
*Runyi Li, Bin Chen, Jian Zhang, Radu Timofte*

Main category: cs.CV

TL;DR: LAFR (Latent Alignment for Face Restoration) is a novel method that aligns latent distributions of low-quality (LQ) and high-quality (HQ) face images for improved restoration, avoiding costly VAE retraining. It uses a multi-level loss for identity preservation and achieves efficient results with minimal finetuning.


<details>
  <summary>Details</summary>
Motivation: Blind face restoration from LQ images is challenging due to semantic misalignment in diffusion models when encoding LQ inputs, leading to ineffective denoising. Existing solutions require expensive VAE retraining.

Method: LAFR introduces a codebook-based latent space adapter to align LQ and HQ latent distributions, enabling consistent diffusion sampling. It includes a multi-level restoration loss for identity preservation and leverages facial structural priors for lightweight finetuning.

Result: LAFR achieves high-quality, identity-preserving face restoration from degraded inputs, with results comparable to state-of-the-art methods while reducing training time by 70%.

Conclusion: LAFR efficiently addresses the limitations of existing methods by aligning latent spaces and preserving identity, offering a practical solution for blind face restoration.

Abstract: Blind face restoration from low-quality (LQ) images is a challenging task
that requires not only high-fidelity image reconstruction but also the
preservation of facial identity. While diffusion models like Stable Diffusion
have shown promise in generating high-quality (HQ) images, their VAE modules
are typically trained only on HQ data, resulting in semantic misalignment when
encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ
conditions during the denoising process. Existing approaches often tackle this
issue by retraining the VAE encoder, which is computationally expensive and
memory-intensive. To address this limitation efficiently, we propose LAFR
(Latent Alignment for Face Restoration), a novel codebook-based latent space
adapter that aligns the latent distribution of LQ images with that of HQ
counterparts, enabling semantically consistent diffusion sampling without
altering the original VAE. To further enhance identity preservation, we
introduce a multi-level restoration loss that combines constraints from
identity embeddings and facial structural priors. Additionally, by leveraging
the inherent structural regularity of facial images, we show that lightweight
finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to
achieve results comparable to state-of-the-art methods, reduce training time by
70%. Extensive experiments on both synthetic and real-world face restoration
benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving
high-quality, identity-preserving face reconstruction from severely degraded
inputs.

</details>


### [308] [Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss](https://arxiv.org/pdf/2505.23463)
*Han Zhou, Sebastian G. Gruber, Teodora Popordanoska, Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper connects weighted risk functions (like focal loss and AURC) to calibration errors, showing that regularized AURC improves calibration and aligns with inverse focal loss.


<details>
  <summary>Details</summary>
Motivation: To establish a principled link between reweighted risk functions and calibration errors, addressing conflicting claims about their calibration properties.

Method: Analyzes weighted risk functions, introduces a regularized AURC with SoftRank for gradient-based optimization, and compares reweighting schemes.

Result: Regularized AURC achieves competitive class-wise calibration across datasets and architectures, supporting inverse focal loss over focal loss for calibration.

Conclusion: Optimizing regularized AURC is effective for calibration, offering flexibility in confidence score functions and outperforming focal loss in principled calibration.

Abstract: Several variants of reweighted risk functionals, such as focal losss, inverse
focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been
proposed in the literature and claims have been made in relation to their
calibration properties. However, focal loss and inverse focal loss propose
vastly different weighting schemes. In this paper, we revisit a broad class of
weighted risk functions commonly used in deep learning and establish a
principled connection between these reweighting schemes and calibration errors.
We show that minimizing calibration error is closely linked to the selective
classification paradigm and demonstrate that optimizing a regularized variant
of the AURC naturally leads to improved calibration. This regularized AURC
shares a similar reweighting strategy with inverse focal loss, lending support
to the idea that focal loss is less principled when calibration is a desired
outcome. Direct AURC optimization offers greater flexibility through the choice
of confidence score functions (CSFs). To enable gradient-based optimization, we
introduce a differentiable formulation of the regularized AURC using the
SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss
achieves competitive class-wise calibration performance across a range of
datasets and model architectures.

</details>


### [309] [A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization](https://arxiv.org/pdf/2505.23469)
*Zhuodong Li, Fei Hou, Wencheng Wang, Xuequan Lu, Ying He*

Main category: cs.CV

TL;DR: DACPO is a novel framework for orienting large-scale, non-watertight point clouds by dividing them into manageable blocks, processing each independently, and integrating results via global optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on watertight, object-level models, leaving large-scale, non-watertight scenes underexplored. DACPO addresses this gap.

Method: DACPO segments point clouds into blocks, estimates initial normals via randomized greedy method, refines them with iterative Poisson reconstruction, and integrates results using a graph-based global optimization.

Result: DACPO outperforms existing methods in large-scale, non-watertight scenarios, as shown in benchmark experiments.

Conclusion: DACPO provides a scalable and robust solution for point cloud orientation in challenging large-scale scenes, with publicly available source code.

Abstract: Orienting point clouds is a fundamental problem in computer graphics and 3D
vision, with applications in reconstruction, segmentation, and analysis. While
significant progress has been made, existing approaches mainly focus on
watertight, object-level 3D models. The orientation of large-scale,
non-watertight 3D scenes remains an underexplored challenge. To address this
gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework
that leverages a divide-and-conquer strategy for scalable and robust point
cloud orientation. Rather than attempting to orient an unbounded scene at once,
DACPO segments the input point cloud into smaller, manageable blocks, processes
each block independently, and integrates the results through a global
optimization stage. For each block, we introduce a two-step process: estimating
initial normal orientations by a randomized greedy method and refining them by
an adapted iterative Poisson surface reconstruction. To achieve consistency
across blocks, we model inter-block relationships using an an undirected graph,
where nodes represent blocks and edges connect spatially adjacent blocks. To
reliably evaluate orientation consistency between adjacent blocks, we introduce
the concept of the visible connected region, which defines the region over
which visibility-based assessments are performed. The global integration is
then formulated as a 0-1 integer-constrained optimization problem, with block
flip states as binary variables. Despite the combinatorial nature of the
problem, DACPO remains scalable by limiting the number of blocks (typically a
few hundred for 3D scenes) involved in the optimization. Experiments on
benchmark datasets demonstrate DACPO's strong performance, particularly in
challenging large-scale, non-watertight scenarios where existing methods often
fail. The source code is available at https://github.com/zd-lee/DACPO.

</details>


### [310] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/pdf/2505.23475)
*Ron Shapira Weber, Shahar Ben Ishay, Andrey Lavrinenko, Shahaf E. Finder, Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint is a self-supervised method that accelerates DTW-based alignment and improves accuracy by learning keypoints from synthetic data, outperforming standard DTW in speed and precision.


<details>
  <summary>Details</summary>
Motivation: Dynamic Time Warping (DTW) faces scalability and noise sensitivity issues, prompting the need for a faster and more accurate alignment method.

Method: TimePoint uses 1D diffeomorphisms to generate synthetic training data and employs convolutional architectures to extract keypoints and descriptors, enabling efficient DTW alignment.

Result: TimePoint achieves faster and more accurate alignments than standard DTW, with strong generalization to real-world data.

Conclusion: TimePoint offers a scalable and superior alternative to DTW for time-series alignment, with potential for further improvement via fine-tuning.

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [311] [PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views](https://arxiv.org/pdf/2505.23481)
*Mohamed Rayan Barhdadi, Hasan Kurban, Hussein Alnuweiri*

Main category: cs.CV

TL;DR: PhysicsNeRF improves 3D reconstruction from sparse views using physical constraints, outperforming NeRF with fewer parameters and views.


<details>
  <summary>Details</summary>
Motivation: Standard NeRFs struggle with sparse views, so PhysicsNeRF introduces physical constraints to enhance performance and generalization.

Method: Extends NeRF with depth ranking, RegNeRF-style consistency, sparsity priors, and cross-view alignment, using a compact 0.67M-parameter model.

Result: Achieves 21.4 dB PSNR with 8 views, outperforming prior methods, but reveals a 5.7-6.2 dB generalization gap.

Conclusion: PhysicsNeRF enables physically consistent 3D representations and clarifies trade-offs in constrained NeRF models.

Abstract: PhysicsNeRF is a physically grounded framework for 3D reconstruction from
sparse views, extending Neural Radiance Fields with four complementary
constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and
cross-view alignment. While standard NeRFs fail under sparse supervision,
PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB
average PSNR using only 8 views, outperforming prior methods. A generalization
gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental
limitations of sparse-view reconstruction. PhysicsNeRF enables physically
consistent, generalizable 3D representations for agent interaction and
simulation, and clarifies the expressiveness-generalization trade-off in
constrained NeRF models.

</details>


### [312] [VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation](https://arxiv.org/pdf/2505.23484)
*Shi-Xue Zhang, Hongfa Wang, Duojun Huang, Xin Li, Xiaobin Zhu, Xu-Cheng Yin*

Main category: cs.CV

TL;DR: VCapsBench is a new benchmark for fine-grained evaluation of video captions, addressing gaps in existing benchmarks by focusing on spatial-temporal details critical for text-to-video generation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack fine-grained evaluation of video captions, especially for spatial-temporal details, which are crucial for text-to-video generation.

Method: The benchmark includes 5,677 videos and 109,796 QA-pairs annotated across 21 fine-grained dimensions. It introduces three metrics (AR, IR, CR) and an automated evaluation pipeline using LLMs.

Result: VCapsBench provides actionable insights for caption optimization, advancing robust text-to-video models.

Conclusion: VCapsBench fills a critical gap in fine-grained caption evaluation, supporting better text-to-video generation.

Abstract: Video captions play a crucial role in text-to-video generation tasks, as
their quality directly influences the semantic coherence and visual fidelity of
the generated videos. Although large vision-language models (VLMs) have
demonstrated significant potential in caption generation, existing benchmarks
inadequately address fine-grained evaluation, particularly in capturing
spatial-temporal details critical for video generation. To address this gap, we
introduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the
first large-scale fine-grained benchmark comprising 5,677 (5K+) videos and
109,796 (100K+) question-answer pairs. These QA-pairs are systematically
annotated across 21 fine-grained dimensions (e.g., camera movement, and shot
type) that are empirically proven critical for text-to-video generation. We
further introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),
Coverage Rate (CR)), and an automated evaluation pipeline leveraging large
language model (LLM) to verify caption quality via contrastive QA-pairs
analysis. By providing actionable insights for caption optimization, our
benchmark can advance the development of robust text-to-video models. The
dataset and codes are available at website: https://github.com/GXYM/VCapsBench.

</details>


### [313] [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/pdf/2505.23493)
*Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, Lifu Huang*

Main category: cs.CV

TL;DR: The paper introduces R2I-Bench, a benchmark to evaluate reasoning in text-to-image (T2I) generation, highlighting current models' limitations in reasoning despite photorealistic outputs.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models lack robust reasoning capabilities, necessitating a dedicated benchmark to assess and improve reasoning-driven generation.

Method: R2I-Bench includes curated data across reasoning categories and R2IScore, a QA-style metric evaluating text-image alignment, reasoning accuracy, and image quality.

Result: Experiments with 16 T2I models show limited reasoning performance, even with advanced pipeline-based frameworks.

Conclusion: The findings underscore the need for reasoning-aware architectures in future T2I systems.

Abstract: Reasoning is a fundamental capability often required in real-world
text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been
left in the air for more than a week`` necessitates understanding temporal
decay and commonsense concepts. While recent T2I models have made impressive
progress in producing photorealistic images, their reasoning capability remains
underdeveloped and insufficiently evaluated. To bridge this gap, we introduce
R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess
reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data
instances, spanning core reasoning categories, including commonsense,
mathematical, logical, compositional, numerical, causal, and concept mixing. To
facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based
on instance-specific, reasoning-oriented evaluation questions that assess three
critical dimensions: text-image alignment, reasoning accuracy, and image
quality. Extensive experiments with 16 representative T2I models, including a
strong pipeline-based framework that decouples reasoning and generation using
the state-of-the-art language and image generation models, demonstrate
consistently limited reasoning performance, highlighting the need for more
robust, reasoning-aware architectures in the next generation of T2I systems.
Project Page: https://r2i-bench.github.io

</details>


### [314] [VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.23504)
*Liyun Zhu, Qixiang Chen, Xi Shen, Xiaodong Cun*

Main category: cs.CV

TL;DR: The paper introduces VAU-R1, a framework for video anomaly understanding using Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT), along with VAU-Bench, a benchmark for evaluating anomaly reasoning.


<details>
  <summary>Details</summary>
Motivation: Video Anomaly Understanding (VAU) is crucial for applications like smart cities and security but lacks interpretability and robust reasoning in existing methods. There's also a need for comprehensive benchmarks.

Method: VAU-R1 leverages MLLMs and RFT for anomaly reasoning. VAU-Bench is introduced as a Chain-of-Thought benchmark with QA, rationales, annotations, and captions.

Result: VAU-R1 improves QA accuracy, temporal grounding, and reasoning coherence.

Conclusion: The framework and benchmark advance interpretable and reasoning-aware VAU, providing a foundation for future research.

Abstract: Video Anomaly Understanding (VAU) is essential for applications such as smart
cities, security surveillance, and disaster alert systems, yet remains
challenging due to its demand for fine-grained spatio-temporal perception and
robust reasoning under ambiguity. Despite advances in anomaly detection,
existing methods often lack interpretability and struggle to capture the causal
and contextual aspects of abnormal events. This limitation is further
compounded by the absence of comprehensive benchmarks for evaluating reasoning
ability in anomaly scenarios. To address both challenges, we introduce VAU-R1,
a data-efficient framework built upon Multimodal Large Language Models (MLLMs),
which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).
Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored
for video anomaly reasoning, featuring multiple-choice QA, detailed rationales,
temporal annotations, and descriptive captions. Empirical results show that
VAU-R1 significantly improves question answering accuracy, temporal grounding,
and reasoning coherence across diverse contexts. Together, our method and
benchmark establish a strong foundation for interpretable and reasoning-aware
video anomaly understanding. Our code is available at
https://github.com/GVCLab/VAU-R1.

</details>


### [315] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/pdf/2505.23590)
*Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper explores rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) using jigsaw puzzles, showing improved accuracy, generalization, and insights into reasoning patterns. RL outperforms supervised fine-tuning (SFT).


<details>
  <summary>Details</summary>
Motivation: To address challenges and deviations in applying rule-based RL to MLLMs for perception-heavy tasks, using jigsaw puzzles as a structured framework.

Method: Fine-tuning MLLMs on jigsaw puzzles, analyzing performance, generalization, reasoning patterns, and comparing RL with SFT.

Result: MLLMs achieve near-perfect accuracy, generalize to unseen puzzles and other tasks, and RL outperforms SFT. Reasoning patterns are pre-existing, not emergent.

Conclusion: The study advances understanding of rule-based visual RL in MLLMs, highlighting RL's superiority over SFT and the role of task configurations in generalization.

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [316] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/pdf/2505.23522)
*Fengxiang Wang, Mingshuo Chen, Xuming He, YiFan Zhang, Feng Liu, Zijie Guo, Zhenghao Hu, Jiong Wang, Jingyi Xu, Zhangrui Li, Fenghua Ling, Ben Fei, Weijia Li, Long Lan, Wenjing Yang, Wenlong Zhang, Lei Bai*

Main category: cs.CV

TL;DR: OmniEarth-Bench is a comprehensive multimodal benchmark for Earth science, covering all six spheres and cross-sphere interactions with 100 evaluation dimensions. It reveals limitations in current MLLMs, with none achieving over 35% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in existing benchmarks, which lack systematic coverage of Earth's spheres and cross-sphere interactions, and limited evaluation dimensions.

Method: Leverages observational data from satellites and in-situ measurements, integrating 29,779 annotations across four reasoning tiers. Involves expert curation and crowd-sourcing for validation.

Result: Tests on 9 MLLMs show poor performance, with none exceeding 35% accuracy. GPT-4o drops to 0% in some cross-sphere tasks.

Conclusion: OmniEarth-Bench sets a new standard for geosystem-aware AI, aiding scientific discovery and practical applications like environmental monitoring.

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [317] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/pdf/2505.23595)
*Youssef Mohamed, Noran Mohamed, Khaled Abouhashad, Feilong Tang, Sara Atito, Shoaib Jameel, Imran Razzak, Ahmed B. Zaky*

Main category: cs.CV

TL;DR: DeepChest is a dynamic task-weighting framework for multi-label chest X-ray classification, improving efficiency and accuracy over existing MTL methods.


<details>
  <summary>Details</summary>
Motivation: Balancing task contributions in Multi-Task Learning (MTL) for medical imaging is challenging; DeepChest addresses this by optimizing task weights without gradient access.

Method: DeepChest uses a performance-driven weighting mechanism based on task-specific loss trends, adapting task importance dynamically and reducing memory usage.

Result: Outperforms state-of-the-art MTL methods by 7% in accuracy, with faster training and reduced task losses.

Conclusion: DeepChest enhances MTL efficiency and performance, enabling practical deployment in medical diagnostics.

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [318] [CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization](https://arxiv.org/pdf/2505.23524)
*Rui Xia, Dan Jiang, Quan Zhang, Ke Zhang, Chun Yuan*

Main category: cs.CV

TL;DR: Proposes a CLIP-assisted cross-view audiovisual method for unsupervised temporal action localization (UTAL) to address challenges of feature focus and contextual boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing TAL methods rely on labeled data, which is costly, and current UTAL methods struggle with feature focus and contextual boundaries.

Method: Uses visual language pre-training (VLP) and classification pre-training for balanced feature focus, adds audio for context, and employs self-supervised cross-view learning.

Result: Outperforms state-of-the-art methods on two public datasets.

Conclusion: The proposed method effectively enhances UTAL by addressing key challenges without extra annotations.

Abstract: Temporal Action Localization (TAL) has garnered significant attention in
information retrieval. Existing supervised or weakly supervised methods heavily
rely on labeled temporal boundaries and action categories, which are
labor-intensive and time-consuming. Consequently, unsupervised temporal action
localization (UTAL) has gained popularity. However, current methods face two
main challenges: 1) Classification pre-trained features overly focus on highly
discriminative regions; 2) Solely relying on visual modality information makes
it difficult to determine contextual boundaries. To address these issues, we
propose a CLIP-assisted cross-view audiovisual enhanced UTAL method.
Specifically, we introduce visual language pre-training (VLP) and
classification pre-training-based collaborative enhancement to avoid excessive
focus on highly discriminative regions; we also incorporate audio perception to
provide richer contextual boundary information. Finally, we introduce a
self-supervised cross-view learning paradigm to achieve multi-view perceptual
enhancement without additional annotations. Extensive experiments on two public
datasets demonstrate our model's superiority over several state-of-the-art
competitors.

</details>


### [319] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/pdf/2505.23617)
*Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajViT introduces grounded video tokenization using object trajectories, reducing tokens and improving performance over space-time ViTs.


<details>
  <summary>Details</summary>
Motivation: Current video tokenization methods are inefficient, producing excessive tokens and degrading performance with camera movement.

Method: TrajViT organizes tokens based on panoptic sub-object trajectories, ensuring semantic relevance and temporal coherence.

Result: TrajViT outperforms ViT3D by 6% in video-text retrieval and 5.2% in VideoQA, with 10x fewer tokens and 18x less FLOPs.

Conclusion: TrajViT is a scalable, efficient video encoder that consistently outperforms ViT3D across tasks.

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [320] [Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation](https://arxiv.org/pdf/2505.23525)
*Jiahao Cui, Yan Chen, Mingwang Xu, Hanlin Shang, Yuxuan Chen, Yun Zhan, Zilong Dong, Yao Yao, Jingdong Wang, Siyu Zhu*

Main category: cs.CV

TL;DR: A diffusion framework for photorealistic portrait animations aligns with human preferences and improves lip sync, expressions, and motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Challenges in achieving precise lip sync, natural facial expressions, and high-fidelity body motion in portrait animations.

Method: Uses human-preference-aligned diffusion with direct preference optimization and temporal motion modulation.

Result: Improves lip-audio sync, expression vividness, motion coherence, and human preference metrics over baselines.

Conclusion: The framework effectively enhances portrait animation quality and aligns with human perceptual preferences.

Abstract: Generating highly dynamic and photorealistic portrait animations driven by
audio and skeletal motion remains challenging due to the need for precise lip
synchronization, natural facial expressions, and high-fidelity body motion
dynamics. We propose a human-preference-aligned diffusion framework that
addresses these challenges through two key innovations. First, we introduce
direct preference optimization tailored for human-centric animation, leveraging
a curated dataset of human preferences to align generated outputs with
perceptual metrics for portrait motion-video alignment and naturalness of
expression. Second, the proposed temporal motion modulation resolves
spatiotemporal resolution mismatches by reshaping motion conditions into
dimensionally aligned latent features through temporal channel redistribution
and proportional feature expansion, preserving the fidelity of high-frequency
motion details in diffusion-based synthesis. The proposed mechanism is
complementary to existing UNet and DiT-based portrait diffusion approaches, and
experiments demonstrate obvious improvements in lip-audio synchronization,
expression vividness, body motion coherence over baseline methods, alongside
notable gains in human preference metrics. Our model and source code can be
found at: https://github.com/xyz123xyz456/hallo4.

</details>


### [321] [Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications](https://arxiv.org/pdf/2505.23543)
*Jan Ignatowicz, Krzysztof Kutt, Grzegorz J. Nalepa*

Main category: cs.CV

TL;DR: The paper proposes the Metadata Enrichment Model (MEM), integrating neural networks and semantic technologies to enrich metadata for digitized cultural heritage collections, addressing challenges like feature extraction and interoperability.


<details>
  <summary>Details</summary>
Motivation: The lack of enriched metadata in digitized cultural heritage collections hinders accessibility and collaboration. Existing neural network models lack domain-specific methodologies for artifacts like manuscripts.

Method: MEM combines fine-tuned computer vision models, LLMs, and knowledge graphs, featuring the Multilayer Vision Mechanism (MVM) for dynamic feature detection.

Result: Tested on incunabula, MEM demonstrated potential, with a manually annotated dataset released. Challenges include domain-specific tuning and computational costs.

Conclusion: MEM offers a flexible approach to advancing cultural heritage research through AI and semantic technologies, bridging theory and practice.

Abstract: The digitization of cultural heritage collections has opened new directions
for research, yet the lack of enriched metadata poses a substantial challenge
to accessibility, interoperability, and cross-institutional collaboration. In
several past years neural networks models such as YOLOv11 and Detectron2 have
revolutionized visual data analysis, but their application to domain-specific
cultural artifacts - such as manuscripts and incunabula - remains limited by
the absence of methodologies that address structural feature extraction and
semantic interoperability. In this position paper, we argue, that the
integration of neural networks with semantic technologies represents a paradigm
shift in cultural heritage digitization processes. We present the Metadata
Enrichment Model (MEM), a conceptual framework designed to enrich metadata for
digitized collections by combining fine-tuned computer vision models, large
language models (LLMs) and structured knowledge graphs. The Multilayer Vision
Mechanism (MVM) appears as the key innovation of MEM. This iterative process
improves visual analysis by dynamically detecting nested features, such as text
within seals or images within stamps. To expose MEM's potential, we apply it to
a dataset of digitized incunabula from the Jagiellonian Digital Library and
release a manually annotated dataset of 105 manuscript pages. We examine the
practical challenges of MEM's usage in real-world GLAM institutions, including
the need for domain-specific fine-tuning, the adjustment of enriched metadata
with Linked Data standards and computational costs. We present MEM as a
flexible and extensible methodology. This paper contributes to the discussion
on how artificial intelligence and semantic web technologies can advance
cultural heritage research, and also use these technologies in practice.

</details>


### [322] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/pdf/2505.23558)
*Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LA introduces a vision-text reflection process to mitigate hallucinations in VLMs, using BRPO for balanced reflection and visual token methods to enhance attention.


<details>
  <summary>Details</summary>
Motivation: Extended reasoning in VLMs dilutes visual attention, causing hallucinations, which text-only reflection fails to address.

Method: Proposes Qwen-LA with BRPO for reflection balance and Visual Token COPY/ROUTE to re-attention visual info.

Result: Qwen-LA improves accuracy and reduces hallucinations on visual QA datasets.

Conclusion: Vision-text reflection and visual token methods effectively address VLM hallucinations.

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


### [323] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/pdf/2505.23637)
*Dashti A. Ali, Richard K. G. Do, William R. Jarnagin, Aras T. Asaad, Amber L. Simpson*

Main category: cs.CV

TL;DR: The paper compares two methods of constructing topological feature vectors from persistence barcodes in medical image analysis, finding that feature concatenation outperforms aggregation in preserving detailed topological information and improving classification performance.


<details>
  <summary>Details</summary>
Motivation: Traditional feature extraction in medical image analysis is sensitive to input perturbations. Persistent homology (PH) offers robustness, but the best method to construct feature vectors from PH barcodes is unclear.

Method: The study compares two approaches: aggregating persistence barcodes followed by featurization versus concatenating topological feature vectors from individual barcodes.

Result: Feature concatenation preserves more detailed topological information and yields better classification performance.

Conclusion: Concatenating topological feature vectors is the preferred method for similar experiments due to its superior performance.

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [324] [Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition](https://arxiv.org/pdf/2505.23566)
*Yu Li, Jin Jiang, Jianhua Zhu, Shuai Peng, Baole Wei, Yuxuan Zhou, Liangcai Gao*

Main category: cs.CV

TL;DR: Uni-MuMER fine-tunes a vision-language model (VLM) for Handwritten Mathematical Expression Recognition (HMER) without architectural changes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: HMER is challenging due to symbol layout freedom and handwriting variability. Existing methods lack unified solutions, while VLMs offer strong generalization.

Method: Uni-MuMER integrates Tree-CoT for spatial reasoning, EDL for reducing character confusion, and SC for consistency in long expressions.

Result: Uni-MuMER outperforms SSAN by 16.31% and Gemini2.5-flash by 24.42% on CROHME and HME100K datasets.

Conclusion: The approach successfully injects domain-specific knowledge into a generalist VLM, setting new benchmarks in HMER.

Abstract: Handwritten Mathematical Expression Recognition (HMER) remains a persistent
challenge in Optical Character Recognition (OCR) due to the inherent freedom of
symbol layout and variability in handwriting styles. Prior methods have faced
performance bottlenecks, proposing isolated architectural modifications that
are difficult to integrate coherently into a unified framework. Meanwhile,
recent advances in pretrained vision-language models (VLMs) have demonstrated
strong cross-task generalization, offering a promising foundation for
developing unified solutions. In this paper, we introduce Uni-MuMER, which
fully fine-tunes a VLM for the HMER task without modifying its architecture,
effectively injecting domain-specific knowledge into a generalist framework.
Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought
(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for
reducing confusion among visually similar characters, and Symbol Counting (SC)
for improving recognition consistency in long expressions. Experiments on the
CROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art
performance, surpassing the best lightweight specialized model SSAN by 16.31%
and the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting.
Our datasets, models, and code are open-sourced at:
https://github.com/BFlameSwift/Uni-MuMER

</details>


### [325] [Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation](https://arxiv.org/pdf/2505.23597)
*Georgios Voulgaris*

Main category: cs.CV

TL;DR: PerceptiveNet, a novel deep learning model with Logarithmic Gabor-parameterised convolutional layers, improves tree crown segmentation in remote sensing by addressing canopy complexities and outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate tree crown segmentation is vital for forest management and biodiversity studies, but traditional and deep learning methods struggle with canopy complexities like shadows and spectral variations.

Method: Proposes PerceptiveNet with trainable Logarithmic Gabor filters and a backbone for context extraction. Evaluates Log-Gabor, Gabor, and standard convolutional layers, and tests PerceptiveNet in a hybrid CNN-Transformer model.

Result: Outperforms state-of-the-art models on tree crown datasets and generalizes well on benchmark aerial scene datasets.

Conclusion: PerceptiveNet effectively addresses segmentation challenges in complex forest canopies and shows strong generalization across domains.

Abstract: The accurate semantic segmentation of tree crowns within remotely sensed data
is crucial for scientific endeavours such as forest management, biodiversity
studies, and carbon sequestration quantification. However, precise segmentation
remains challenging due to complexities in the forest canopy, including
shadows, intricate backgrounds, scale variations, and subtle spectral
differences among tree species. Compared to the traditional methods, Deep
Learning models improve accuracy by extracting informative and discriminative
features, but often fall short in capturing the aforementioned complexities.
  To address these challenges, we propose PerceptiveNet, a novel model
incorporating a Logarithmic Gabor-parameterised convolutional layer with
trainable filter parameters, alongside a backbone that extracts salient
features while capturing extensive context and spatial information through a
wider receptive field. We investigate the impact of Log-Gabor, Gabor, and
standard convolutional layers on semantic segmentation performance through
extensive experimentation. Additionally, we conduct an ablation study to assess
the contributions of individual layers and their combinations to overall model
performance, and we evaluate PerceptiveNet as a backbone within a novel hybrid
CNN-Transformer model. Our results outperform state-of-the-art models,
demonstrating significant performance improvements on a tree crown dataset
while generalising across domains, including two benchmark aerial scene
semantic segmentation datasets with varying complexities.

</details>


### [326] [A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/pdf/2505.23601)
*Shengyuan Liu, Boyun Zheng, Wenting Chen, Zhihao Peng, Zhenfei Yin, Jing Shao, Jiancong Hu, Yixuan Yuan*

Main category: cs.CV

TL;DR: EndoBench is a comprehensive benchmark for evaluating multi-modal large language models (MLLMs) in endoscopy, covering diverse scenarios and clinical tasks, revealing gaps between models and human experts.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs in endoscopy are limited in scope and fail to capture real-world diversity, necessitating a more holistic evaluation framework.

Method: EndoBench includes 4 endoscopic scenarios, 12 clinical tasks, and 5 visual prompting granularities, validated with 6,832 VQA pairs from 21 datasets. It evaluates 23 MLLMs against human clinician performance.

Result: Proprietary MLLMs outperform others but lag behind human experts. Medical-domain fine-tuning improves accuracy, but performance varies with prompt format and task complexity.

Conclusion: EndoBench sets a new standard for MLLM evaluation in endoscopy, highlighting progress and gaps, with benchmark and code publicly released.

Abstract: Endoscopic procedures are essential for diagnosing and treating internal
diseases, and multi-modal large language models (MLLMs) are increasingly
applied to assist in endoscopy analysis. However, current benchmarks are
limited, as they typically cover specific endoscopic scenarios and a small set
of clinical tasks, failing to capture the real-world diversity of endoscopic
scenarios and the full range of skills needed in clinical workflows. To address
these issues, we introduce EndoBench, the first comprehensive benchmark
specifically designed to assess MLLMs across the full spectrum of endoscopic
practice with multi-dimensional capacities. EndoBench encompasses 4 distinct
endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,
and 5 levels of visual prompting granularities, resulting in 6,832 rigorously
validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation
framework mirrors the clinical workflow--spanning anatomical recognition,
lesion analysis, spatial localization, and surgical operations--to holistically
gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.
We benchmark 23 state-of-the-art models, including general-purpose,
medical-specialized, and proprietary MLLMs, and establish human clinician
performance as a reference standard. Our extensive experiments reveal: (1)
proprietary MLLMs outperform open-source and medical-specialized models
overall, but still trail human experts; (2) medical-domain supervised
fine-tuning substantially boosts task-specific accuracy; and (3) model
performance remains sensitive to prompt format and clinical task complexity.
EndoBench establishes a new standard for evaluating and advancing MLLMs in
endoscopy, highlighting both progress and persistent gaps between current
models and expert clinical reasoning. We publicly release our benchmark and
code.

</details>


### [327] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/pdf/2505.23693)
*Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao*

Main category: cs.CV

TL;DR: The paper introduces VF-Eval, a benchmark to evaluate MLLMs on synthetic (AIGC) videos, revealing their limitations and proposing RePrompt for improving video generation.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of MLLMs for video question answering neglect synthetic videos, and their capabilities in interpreting AIGC videos are underexplored.

Method: VF-Eval benchmark with four tasks (coherence validation, error awareness, error type detection, reasoning evaluation) is proposed to assess MLLMs. RePrompt aligns MLLMs with human feedback.

Result: Even top models like GPT-4.1 perform inconsistently across tasks, showing the benchmark's difficulty. RePrompt improves video generation by aligning MLLMs with human feedback.

Conclusion: VF-Eval highlights MLLMs' challenges with AIGC videos, and RePrompt demonstrates practical benefits for video generation.

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [328] [Color Image Set Recognition Based on Quaternionic Grassmannians](https://arxiv.org/pdf/2505.23629)
*Xiang Xiang Wang, Tin-Yau Tam*

Main category: cs.CV

TL;DR: A novel method for color image set recognition using quaternionic Grassmannians, achieving good results on ETH-80, with noted stability limitations.


<details>
  <summary>Details</summary>
Motivation: To leverage quaternions for capturing color information and improve recognition of color image sets.

Method: Represent color image sets as points on quaternionic Grassmannians and use a direct distance formula for classification.

Result: Achieves good recognition results on the ETH-80 dataset.

Conclusion: The method is effective but has stability issues; future improvements are suggested.

Abstract: We propose a new method for recognizing color image sets using quaternionic
Grassmannians, which use the power of quaternions to capture color information
and represent each color image set as a point on the quaternionic Grassmannian.
We provide a direct formula to calculate the shortest distance between two
points on the quaternionic Grassmannian, and use this distance to build a new
classification framework. Experiments on the ETH-80 benchmark dataset show that
our method achieves good recognition results. We also discuss some limitations
in stability and suggest ways the method can be improved in the future.

</details>


### [329] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/pdf/2505.23704)
*Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer*

Main category: cs.CV

TL;DR: CLDTracker introduces a dual-branch framework leveraging VLMs for robust visual tracking, addressing limitations in textual representation and feature fusion, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Traditional trackers struggle with dynamic changes and occlusions. VLMs show promise but face limitations in textual representation, fusion, and temporal modeling.

Method: Proposes CLDTracker with a dual-branch (textual and visual) architecture, using VLMs like CLIP and GPT-4V for rich textual descriptions.

Result: Achieves SOTA performance on six VOT benchmarks.

Conclusion: CLDTracker effectively bridges gaps in VOT by integrating robust vision-language representations.

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [330] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2505.22654)
*Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu*

Main category: cs.CV

TL;DR: VScan is a two-stage visual token reduction framework that speeds up LVLMs by pruning redundant tokens during visual encoding and language decoding, achieving significant speedups with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs incur high computational costs due to long visual token sequences, hindering real-time deployment. Prior token pruning methods are limited in effectiveness.

Method: VScan integrates global and local scans with token merging during visual encoding and introduces pruning at intermediate language model layers.

Result: VScan achieves a 2.91× speedup and 10× FLOPs reduction while retaining 95.4% of original performance on LLaVA-NeXT-7B.

Conclusion: VScan effectively addresses token redundancy in LVLMs, offering a practical solution for faster inference without significant performance trade-offs.

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [331] [Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis](https://arxiv.org/pdf/2505.23642)
*Nathaniel Burgdorfer, Philippos Mordohai*

Main category: cs.CV

TL;DR: An inference-time optimization framework using triangles for 3D scene representation, outperforming Gaussian splats with better color interpolation and surface continuity.


<details>
  <summary>Details</summary>
Motivation: To improve 3D scene representation by leveraging triangles for expressive color interpolation and surface continuity, benefiting from existing algorithmic infrastructure.

Method: Develops a scene optimization algorithm for triangle soup, incorporating connectivity forces to encourage soft surface continuity during optimization.

Result: Achieves competitive photometric and geometric results on a 3D reconstruction dataset.

Conclusion: Triangles offer a superior alternative to Gaussian splats for 3D scene representation, enabling better interpolation and surface formation.

Abstract: In this work, we introduce an inference-time optimization framework utilizing
triangles to represent the geometry and appearance of the scene. More
specifically, we develop a scene optimization algorithm for triangle soup, a
collection of disconnected semi-transparent triangle primitives. Compared to
the current most-widely used primitives for 3D scene representation, namely
Gaussian splats, triangles allow for more expressive color interpolation, and
benefit from a large algorithmic infrastructure for downstream tasks.
Triangles, unlike full-rank Gaussian kernels, naturally combine to form
surfaces. We formulate connectivity forces between triangles during
optimization, encouraging explicit, but soft, surface continuity in 3D. We
perform experiments on a representative 3D reconstruction dataset and show
competitive photometric and geometric results.

</details>


### [332] [VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models](https://arxiv.org/pdf/2505.23656)
*Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, Yu Cheng*

Main category: cs.CV

TL;DR: VideoREPA improves physics plausibility in text-to-video models by distilling knowledge from video understanding models using token relation alignment.


<details>
  <summary>Details</summary>
Motivation: Current T2V models lack accurate physics understanding, limiting realistic video synthesis.

Method: Proposes VideoREPA with Token Relation Distillation (TRD) loss to align token-level relations from video foundation models to T2V models.

Result: VideoREPA significantly enhances physics commonsense in CogVideoX, improving benchmark performance.

Conclusion: VideoREPA effectively bridges the physics understanding gap in T2V models, enabling more plausible video generation.

Abstract: Recent advancements in text-to-video (T2V) diffusion models have enabled
high-fidelity and realistic video synthesis. However, current T2V models often
struggle to generate physically plausible content due to their limited inherent
ability to accurately understand physics. We found that while the
representations within T2V models possess some capacity for physics
understanding, they lag significantly behind those from recent video
self-supervised learning methods. To this end, we propose a novel framework
called VideoREPA, which distills physics understanding capability from video
understanding foundation models into T2V models by aligning token-level
relations. This closes the physics understanding gap and enable more
physics-plausible generation. Specifically, we introduce the Token Relation
Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft
guidance suitable for finetuning powerful pre-trained T2V models, a critical
departure from prior representation alignment (REPA) methods. To our knowledge,
VideoREPA is the first REPA method designed for finetuning T2V models and
specifically for injecting physical knowledge. Empirical evaluations show that
VideoREPA substantially enhances the physics commonsense of baseline method,
CogVideoX, achieving significant improvement on relevant benchmarks and
demonstrating a strong capacity for generating videos consistent with intuitive
physics. More video results are available at https://videorepa.github.io/.

</details>


### [333] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/pdf/2505.23709)
*Dionysis Christopoulos, Sotiris Spanos, Eirini Baltzi, Valsamis Ntouskos, Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: SLIMP uses nested contrastive learning to combine skin lesion images and metadata, improving classification performance by leveraging patient-level context.


<details>
  <summary>Details</summary>
Motivation: Melanoma detection is challenging due to image variability and lack of clinical context. Clinicians use holistic patient data, inspiring SLIMP to integrate images and metadata.

Method: SLIMP employs nested contrastive learning to capture relationships between images and metadata, including patient-level clinical data.

Result: SLIMP outperforms other pre-training methods in skin lesion classification, demonstrating high-quality learned representations.

Conclusion: SLIMP effectively combines images and metadata for better skin lesion classification, mimicking clinical decision-making.

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [334] [D-AR: Diffusion via Autoregressive Models](https://arxiv.org/pdf/2505.23660)
*Ziteng Gao, Mike Zheng Shou*

Main category: cs.CV

TL;DR: D-AR recasts image diffusion as autoregressive next-token prediction, achieving 2.09 FID on ImageNet with a 775M Llama backbone.


<details>
  <summary>Details</summary>
Motivation: To unify image diffusion with autoregressive modeling for simpler, more efficient visual synthesis.

Method: Tokenizes images into sequences representing diffusion steps, applies standard next-token prediction without modifying causal masks or strategies.

Result: Achieves 2.09 FID on ImageNet, supports previews and zero-shot layout control.

Conclusion: D-AR inspires future unified autoregressive visual synthesis, especially with large language models.

Abstract: This paper presents Diffusion via Autoregressive models (D-AR), a new
paradigm recasting the image diffusion process as a vanilla autoregressive
procedure in the standard next-token-prediction fashion. We start by designing
the tokenizer that converts images into sequences of discrete tokens, where
tokens in different positions can be decoded into different diffusion denoising
steps in the pixel space. Thanks to the diffusion properties, these tokens
naturally follow a coarse-to-fine order, which directly lends itself to
autoregressive modeling. Therefore, we apply standard next-token prediction on
these tokens, without modifying any underlying designs (either causal masks or
training/inference strategies), and such sequential autoregressive token
generation directly mirrors the diffusion procedure in image space. That is,
once the autoregressive model generates an increment of tokens, we can directly
decode these tokens into the corresponding diffusion denoising step in the
streaming manner. Our pipeline naturally reveals several intriguing properties,
for example, it supports consistent previews when generating only a subset of
tokens and enables zero-shot layout-controlled synthesis. On the standard
ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone
with 256 discrete tokens. We hope our work can inspire future research on
unified autoregressive architectures of visual synthesis, especially with large
language models. Code and models will be available at
https://github.com/showlab/D-AR

</details>


### [335] [OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.23661)
*Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy*

Main category: cs.CV

TL;DR: OpenUni is a lightweight, open-source baseline unifying multimodal understanding and generation, using efficient training with LLMs and diffusion models, achieving strong performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To provide a simple, open-source solution for unifying multimodal tasks, reducing training complexity while maintaining performance.

Method: Uses learnable queries and a transformer-based connector to bridge LLMs and diffusion models, with minimalist architecture.

Result: Generates high-quality images and excels on benchmarks (GenEval, DPG-Bench, WISE) with 1.1B-3.1B parameters.

Conclusion: OpenUni is effective and efficient, with all resources released for community use.

Abstract: In this report, we present OpenUni, a simple, lightweight, and fully
open-source baseline for unifying multimodal understanding and generation.
Inspired by prevailing practices in unified model learning, we adopt an
efficient training strategy that minimizes the training complexity and overhead
by bridging the off-the-shelf multimodal large language models (LLMs) and
diffusion models through a set of learnable queries and a light-weight
transformer-based connector. With a minimalist choice of architecture, we
demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned
images, and 2) achieve exceptional performance on standard benchmarks such as
GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To
support open research and community advancement, we release all model weights,
training code, and our curated training datasets (including 23M image-text
pairs) at https://github.com/wusize/OpenUni.

</details>


### [336] [Grounded Reinforcement Learning for Visual Reasoning](https://arxiv.org/pdf/2505.23678)
*Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: ViGoRL, a visually grounded RL model, outperforms baselines in visual reasoning tasks by anchoring reasoning steps to visual coordinates and dynamically zooming in for fine-grained exploration.


<details>
  <summary>Details</summary>
Motivation: Visual reasoning adds complexity to RL by requiring attention, perception, and spatial grounding. ViGoRL addresses this by mimicking human visual decision-making.

Method: ViGoRL uses RL to produce spatially grounded reasoning traces and a multi-turn RL framework for dynamic zooming into visual coordinates.

Result: ViGoRL excels in benchmarks like SAT-2, BLINK, V*Bench, ScreenSpot, and VisualWebArena, achieving 86.4% on V*Bench. Grounding also enhances other visual behaviors.

Conclusion: Visually grounded RL is effective for general-purpose visual reasoning, with human evaluations confirming the model's spatial accuracy and reasoning clarity.

Abstract: While reinforcement learning (RL) over chains of thought has significantly
advanced language models in tasks such as mathematics and coding, visual
reasoning introduces added complexity by requiring models to direct visual
attention, interpret perceptual inputs, and ground abstract reasoning in
spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement
Learning), a vision-language model trained with RL to explicitly anchor each
reasoning step to specific visual coordinates. Inspired by human visual
decision-making, ViGoRL learns to produce spatially grounded reasoning traces,
guiding visual attention to task-relevant regions at each step. When
fine-grained exploration is required, our novel multi-turn RL framework enables
the model to dynamically zoom into predicted coordinates as reasoning unfolds.
Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK
for spatial reasoning, V*bench for visual search, and ScreenSpot and
VisualWebArena for web-based grounding--ViGoRL consistently outperforms both
supervised fine-tuning and conventional RL baselines that lack explicit
grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual
feedback significantly improves ViGoRL's performance on localizing small GUI
elements and visual search, achieving 86.4% on V*Bench. Additionally, we find
that grounding amplifies other visual behaviors such as region exploration,
grounded subgoal setting, and visual verification. Finally, human evaluations
show that the model's visual references are not only spatially accurate but
also helpful for understanding model reasoning steps. Our results show that
visually grounded RL is a strong paradigm for imbuing models with
general-purpose visual reasoning.

</details>


### [337] [DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers](https://arxiv.org/pdf/2505.23694)
*Li Ren, Chen Chen, Liqiang Wang, Kien Hua*

Main category: cs.CV

TL;DR: DA-VPT improves ViT fine-tuning by using metric learning to guide prompt distributions with semantic data, enhancing performance in vision tasks.


<details>
  <summary>Details</summary>
Motivation: The unexplored correlation between prompts and image tokens in VPT for ViT models motivated the study of how prompt distribution affects fine-tuning.

Method: Proposed DA-VPT, a framework using metric learning to align prompt distributions with class-related semantic data, bridging image patches and class tokens.

Result: DA-VPT outperforms in recognition and segmentation tasks, showing efficient fine-tuning and improved performance.

Conclusion: DA-VPT effectively leverages semantic information to enhance prompt learning, advancing ViT fine-tuning for downstream vision tasks.

Abstract: Visual Prompt Tuning (VPT) has become a promising solution for
Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)
models by partially fine-tuning learnable tokens while keeping most model
parameters frozen. Recent research has explored modifying the connection
structures of the prompts. However, the fundamental correlation and
distribution between the prompts and image tokens remain unexplored. In this
paper, we leverage metric learning techniques to investigate how the
distribution of prompts affects fine-tuning performance. Specifically, we
propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to
guide the distributions of the prompts by learning the distance metric from
their class-related semantic data. Our method demonstrates that the prompts can
serve as an effective bridge to share semantic information between image
patches and the class token. We extensively evaluated our approach on popular
benchmarks in both recognition and segmentation tasks. The results demonstrate
that our approach enables more effective and efficient fine-tuning of ViT
models by leveraging semantic information to guide the learning of the prompts,
leading to improved performance on various downstream vision tasks.

</details>


### [338] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/pdf/2505.23742)
*Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF introduces a unified framework for multi-subject video generation using masked guidance and dynamic masking to ensure consistency and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in maintaining multi-subject consistency and generation quality in video synthesis.

Method: Proposes a region-aware dynamic masking mechanism and pixel-wise channel concatenation for flexible and coherent synthesis.

Result: Achieves state-of-the-art video generation quality, outperforming existing baselines in multi-subject scenarios.

Conclusion: MAGREF enables scalable, controllable, and high-fidelity multi-subject video synthesis, supported by a new benchmark.

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [339] [AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views](https://arxiv.org/pdf/2505.23716)
*Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai*

Main category: cs.CV

TL;DR: AnySplat is a feed-forward network for novel view synthesis from uncalibrated images, predicting 3D Gaussian primitives and camera poses in one shot, outperforming pose-aware and pose-free methods while enabling real-time rendering.


<details>
  <summary>Details</summary>
Motivation: Traditional neural rendering requires known camera poses and per-scene optimization, while existing feed-forward methods struggle with dense views. AnySplat aims to simplify and scale novel view synthesis for casually captured datasets without pose annotations.

Method: AnySplat uses a single forward pass to predict 3D Gaussian primitives (encoding geometry and appearance) and camera intrinsics/extrinsics for each input image, eliminating the need for pose annotations.

Result: In zero-shot evaluations, AnySplat matches pose-aware baselines in quality for sparse and dense views and surpasses pose-free methods. It also reduces rendering latency significantly, enabling real-time synthesis.

Conclusion: AnySplat offers a scalable, efficient solution for novel view synthesis from uncalibrated images, achieving high quality and real-time performance without pose annotations.

Abstract: We introduce AnySplat, a feed forward network for novel view synthesis from
uncalibrated image collections. In contrast to traditional neural rendering
pipelines that demand known camera poses and per scene optimization, or recent
feed forward methods that buckle under the computational weight of dense views,
our model predicts everything in one shot. A single forward pass yields a set
of 3D Gaussian primitives encoding both scene geometry and appearance, and the
corresponding camera intrinsics and extrinsics for each input image. This
unified design scales effortlessly to casually captured, multi view datasets
without any pose annotations. In extensive zero shot evaluations, AnySplat
matches the quality of pose aware baselines in both sparse and dense view
scenarios while surpassing existing pose free approaches. Moreover, it greatly
reduce rendering latency compared to optimization based neural fields, bringing
real time novel view synthesis within reach for unconstrained capture
settings.Project page: https://city-super.github.io/anysplat/

</details>


### [340] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/pdf/2505.23744)
*Qiang Wang, Xiang Song, Yuhang He, Jizhou Han, Chenhao Ding, Xinyuan Gao, Yihong Gong*

Main category: cs.CV

TL;DR: SOYO is a lightweight framework improving domain selection in Parameter-Isolation DIL (PIDIL) using Gaussian Mixture Compressor (GMC), Domain Feature Resampler (DFR), and Multi-level Domain Feature Fusion Network (MDFN). It outperforms baselines in dynamic settings.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks (DNNs) underperform in dynamic environments with shifting data distributions. Existing PIDIL methods lack accuracy in parameter selection as domains and classes grow.

Method: SOYO introduces GMC and DFR for efficient prior domain data storage and balancing, and MDFN for enhanced domain feature extraction. It supports multiple PEFT methods.

Result: SOYO consistently outperforms baselines across six benchmarks in tasks like image classification, object detection, and speech enhancement.

Conclusion: SOYO is robust and adaptable for complex, evolving environments, with code available on GitHub.

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [341] [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/pdf/2505.23764)
*Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Bench is a new benchmark for multi-image spatial intelligence in MLLMs, highlighting a significant performance gap between models and humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on single-image relations, failing to assess multi-image spatial reasoning needed for real-world MLLM applications.

Method: A team crafted 1,000 challenging questions from 120,000+ images, evaluating 34 MLLMs and analyzing failure modes.

Result: Top models (open-source and proprietary) scored 30-40% accuracy, while humans achieved 97%, showing a large gap.

Conclusion: MMSI-Bench reveals critical challenges in multi-image spatial reasoning, providing insights for future research.

Abstract: Spatial intelligence is essential for multimodal large language models
(MLLMs) operating in the complex physical world. Existing benchmarks, however,
probe only single-image relations and thus fail to assess the multi-image
spatial reasoning that real-world deployments demand. We introduce MMSI-Bench,
a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision
researchers spent more than 300 hours meticulously crafting 1,000 challenging,
unambiguous multiple-choice questions from over 120,000 images, each paired
with carefully designed distractors and a step-by-step reasoning process. We
conduct extensive experiments and thoroughly evaluate 34 open-source and
proprietary MLLMs, observing a wide gap: the strongest open-source model
attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while
humans score 97%. These results underscore the challenging nature of MMSI-Bench
and the substantial headroom for future research. Leveraging the annotated
reasoning processes, we also provide an automated error analysis pipeline that
diagnoses four dominant failure modes, including (1) grounding errors, (2)
overlap-matching and scene-reconstruction errors, (3) situation-transformation
reasoning errors, and (4) spatial-logic errors, offering valuable insights for
advancing multi-image spatial intelligence. Project page:
https://runsenxu.com/projects/MMSI_Bench .

</details>


### [342] [FMG-Det: Foundation Model Guided Robust Object Detection](https://arxiv.org/pdf/2505.23726)
*Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins*

Main category: cs.CV

TL;DR: FMG-Det improves object detection by correcting noisy annotations using a MIL framework and foundation models, achieving state-of-the-art results efficiently.


<details>
  <summary>Details</summary>
Motivation: Challenges in collecting consistent object detection annotations due to subjectivity and noise, especially in few-shot settings, degrade model performance.

Method: Combines multiple instance learning (MIL) with a pre-processing pipeline using foundation models to correct labels before training, with minor detector head modifications.

Result: Achieves state-of-the-art performance on multiple datasets, including few-shot scenarios, with simplicity and efficiency.

Conclusion: FMG-Det effectively addresses noisy annotation issues, enhancing detector performance in both standard and few-shot settings.

Abstract: Collecting high quality data for object detection tasks is challenging due to
the inherent subjectivity in labeling the boundaries of an object. This makes
it difficult to not only collect consistent annotations across a dataset but
also to validate them, as no two annotators are likely to label the same object
using the exact same coordinates. These challenges are further compounded when
object boundaries are partially visible or blurred, which can be the case in
many domains. Training on noisy annotations significantly degrades detector
performance, rendering them unusable, particularly in few-shot settings, where
just a few corrupted annotations can impact model performance. In this work, we
propose FMG-Det, a simple, efficient methodology for training models with noisy
annotations. More specifically, we propose combining a multiple instance
learning (MIL) framework with a pre-processing pipeline that leverages powerful
foundation models to correct labels prior to training. This pre-processing
pipeline, along with slight modifications to the detector head, results in
state-of-the-art performance across a number of datasets, for both standard and
few-shot scenarios, while being much simpler and more efficient than other
approaches.

</details>


### [343] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/pdf/2505.23745)
*Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink*

Main category: cs.CV

TL;DR: TrustVLM is a training-free framework to improve misclassification detection in Vision-Language Models (VLMs) by leveraging image embedding space, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: VLMs often produce confident but incorrect predictions, posing risks in safety-critical domains. TrustVLM aims to address this by estimating when predictions can be trusted.

Method: Proposes a confidence-scoring function using the modality gap and distinct concept representation in image embedding space. Evaluated on 17 datasets with 4 architectures and 2 VLMs.

Result: Improves AURC by 51.87%, AUROC by 9.14%, and FPR95 by 32.42% over baselines.

Conclusion: TrustVLM enhances VLM reliability without retraining, enabling safer real-world deployment.

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [344] [ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS](https://arxiv.org/pdf/2505.23734)
*Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang*

Main category: cs.CV

TL;DR: ZPressor improves feed-forward 3DGS models by compressing multi-view inputs into a compact latent state, enabling scalability to over 100 views on an 80GB GPU.


<details>
  <summary>Details</summary>
Motivation: Feed-forward 3DGS models face scalability issues due to encoder limitations, degrading performance with more input views.

Method: ZPressor partitions views into anchor and support sets, using cross attention to compress information into a latent state $Z$.

Result: ZPressor enhances performance under moderate views and robustness in dense settings on DL3DV-10K and RealEstate10K benchmarks.

Conclusion: ZPressor is a lightweight, architecture-agnostic solution for scalable feed-forward 3DGS models.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a
promising solution for novel view synthesis, enabling one-pass inference
without the need for per-scene 3DGS optimization. However, their scalability is
fundamentally constrained by the limited capacity of their encoders, leading to
degraded performance or excessive memory consumption as the number of input
views increases. In this work, we analyze feed-forward 3DGS frameworks through
the lens of the Information Bottleneck principle and introduce ZPressor, a
lightweight architecture-agnostic module that enables efficient compression of
multi-view inputs into a compact latent state $Z$ that retains essential scene
information while discarding redundancy. Concretely, ZPressor enables existing
feed-forward 3DGS models to scale to over 100 input views at 480P resolution on
an 80GB GPU, by partitioning the views into anchor and support sets and using
cross attention to compress the information from the support views into anchor
views, forming the compressed latent state $Z$. We show that integrating
ZPressor into several state-of-the-art feed-forward 3DGS models consistently
improves performance under moderate input views and enhances robustness under
dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.
The video results, code and trained models are available on our project page:
https://lhmd.top/zpressor.

</details>


### [345] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/pdf/2505.23747)
*Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan*

Main category: cs.CV

TL;DR: Spatial-MLLM enhances spatial reasoning in MLLMs using 2D inputs by leveraging a dual-encoder architecture and space-aware frame sampling.


<details>
  <summary>Details</summary>
Motivation: Existing 3D MLLMs rely on additional 3D/2.5D data, limiting their use with 2D inputs like images or videos. Spatial-MLLM addresses this gap.

Method: Proposes a dual-encoder (semantic + spatial) and a space-aware frame sampling strategy. Uses the Spatial-MLLM-120k dataset and GRPO training.

Result: Achieves state-of-the-art performance in visual-based spatial understanding tasks.

Conclusion: Spatial-MLLM effectively improves spatial reasoning from 2D inputs, outperforming existing methods.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [346] [How Animals Dance (When You're Not Looking)](https://arxiv.org/pdf/2505.23738)
*Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz*

Main category: cs.CV

TL;DR: A framework for creating music-synchronized animal dance videos using keyframes, graph optimization, and video diffusion.


<details>
  <summary>Details</summary>
Motivation: To automate the generation of choreographed animal dance videos synchronized with music, reducing manual effort.

Method: Uses keyframes (from text-to-image or GPT-4o), formulates dance synthesis as a graph optimization problem, and employs video diffusion for in-between frames.

Result: Produces 30-second dance videos from just six keyframes, adaptable to various animals and music tracks.

Conclusion: The method efficiently generates high-quality, synchronized dance videos with minimal input, demonstrating versatility and scalability.

Abstract: We present a keyframe-based framework for generating music-synchronized,
choreography aware animal dance videos. Starting from a few keyframes
representing distinct animal poses -- generated via text-to-image prompting or
GPT-4o -- we formulate dance synthesis as a graph optimization problem: find
the optimal keyframe structure that satisfies a specified choreography pattern
of beats, which can be automatically estimated from a reference dance video. We
also introduce an approach for mirrored pose image generation, essential for
capturing symmetry in dance. In-between frames are synthesized using an video
diffusion model. With as few as six input keyframes, our method can produce up
to 30 second dance videos across a wide range of animals and music tracks.

</details>


### [347] [LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization](https://arxiv.org/pdf/2505.23740)
*Ronghuan Wu, Wanchao Su, Jing Liao*

Main category: cs.CV

TL;DR: LayerPeeler is a novel layer-wise image vectorization method that uses an autoregressive peeling strategy and vision-language models to improve vectorization quality and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing image vectorization tools struggle with occluded regions, leading to incomplete or fragmented shapes. Recent methods also lack quality and flexibility.

Method: LayerPeeler employs an autoregressive peeling strategy, vision-language models for layer graph construction, and a finetuned image diffusion model with localized attention control.

Result: LayerPeeler outperforms existing techniques, producing vector graphics with complete paths, coherent layers, and superior visual fidelity.

Conclusion: LayerPeeler addresses key challenges in image vectorization, offering improved quality and flexibility for vector graphics.

Abstract: Image vectorization is a powerful technique that converts raster images into
vector graphics, enabling enhanced flexibility and interactivity. However,
popular image vectorization tools struggle with occluded regions, producing
incomplete or fragmented shapes that hinder editability. While recent
advancements have explored rule-based and data-driven layer-wise image
vectorization, these methods face limitations in vectorization quality and
flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image
vectorization approach that addresses these challenges through a progressive
simplification paradigm. The key to LayerPeeler's success lies in its
autoregressive peeling strategy: by identifying and removing the topmost
non-occluded layers while recovering underlying content, we generate vector
graphics with complete paths and coherent layer structures. Our method
leverages vision-language models to construct a layer graph that captures
occlusion relationships among elements, enabling precise detection and
description for non-occluded layers. These descriptive captions are used as
editing instructions for a finetuned image diffusion model to remove the
identified layers. To ensure accurate removal, we employ localized attention
control that precisely guides the model to target regions while faithfully
preserving the surrounding content. To support this, we contribute a
large-scale dataset specifically designed for layer peeling tasks. Extensive
quantitative and qualitative experiments demonstrate that LayerPeeler
significantly outperforms existing techniques, producing vectorization results
with superior path semantics, geometric regularity, and visual fidelity.

</details>


### [348] [DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP](https://arxiv.org/pdf/2505.23743)
*Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen*

Main category: cs.CV

TL;DR: A novel framework enhances low-light raw images by retasking pre-trained generative diffusion models with the camera ISP, outperforming state-of-the-art methods in perceptual quality.


<details>
  <summary>Details</summary>
Motivation: High-quality photography in extreme low-light conditions is impactful but challenging, and existing methods often result in oversmoothing or poor detail recovery.

Method: Retasks pre-trained generative diffusion models with the camera ISP to enhance low-light raw images.

Result: Outperforms state-of-the-art methods in perceptual quality across three challenging benchmarks.

Conclusion: The proposed framework effectively addresses limitations of existing methods, improving detail recovery and color accuracy in low-light photography.

Abstract: High-quality photography in extreme low-light conditions is challenging but
impactful for digital cameras. With advanced computing hardware, traditional
camera image signal processor (ISP) algorithms are gradually being replaced by
efficient deep networks that enhance noisy raw images more intelligently.
However, existing regression-based models often minimize pixel errors and
result in oversmoothing of low-light photos or deep shadows. Recent work has
attempted to address this limitation by training a diffusion model from
scratch, yet those models still struggle to recover sharp image details and
accurate colors. We introduce a novel framework to enhance low-light raw images
by retasking pre-trained generative diffusion models with the camera ISP.
Extensive experiments demonstrate that our method outperforms the
state-of-the-art in perceptual quality across three challenging low-light raw
image benchmarks.

</details>


### [349] [ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks](https://arxiv.org/pdf/2505.23752)
*Akashah Shabbir, Muhammad Akhtar Munir, Akshay Dudhane, Muhammad Umer Sheikh, Muhammad Haris Khan, Paolo Fraccaro, Juan Bernabe Moreno, Fahad Shahbaz Khan, Salman Khan*

Main category: cs.CV

TL;DR: ThinkGeo is a benchmark for evaluating LLM-driven agents on remote sensing tasks using structured tool interactions and multi-step planning, revealing disparities in tool accuracy and planning consistency.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack domain-specific benchmarks for assessing tool-use capabilities in complex remote sensing tasks.

Method: ThinkGeo includes human-curated queries grounded in satellite/aerial imagery, requiring agents to use diverse tools via a ReAct-style interaction loop. Open and closed-source LLMs are evaluated on 436 tasks.

Result: Disparities in tool accuracy and planning consistency across models are observed.

Conclusion: ThinkGeo offers the first extensive testbed for spatial reasoning in remote sensing, with publicly available code and dataset.

Abstract: Recent progress in large language models (LLMs) has enabled tool-augmented
agents capable of solving complex real-world tasks through step-by-step
reasoning. However, existing evaluations often focus on general-purpose or
multimodal scenarios, leaving a gap in domain-specific benchmarks that assess
tool-use capabilities in complex remote sensing use cases. We present ThinkGeo,
an agentic benchmark designed to evaluate LLM-driven agents on remote sensing
tasks via structured tool use and multi-step planning. Inspired by
tool-interaction paradigms, ThinkGeo includes human-curated queries spanning a
wide range of real-world applications such as urban planning, disaster
assessment and change analysis, environmental monitoring, transportation
analysis, aviation monitoring, recreational infrastructure, and industrial site
analysis. Each query is grounded in satellite or aerial imagery and requires
agents to reason through a diverse toolset. We implement a ReAct-style
interaction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,
Qwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise
execution metrics and final answer correctness. Our analysis reveals notable
disparities in tool accuracy and planning consistency across models. ThinkGeo
provides the first extensive testbed for evaluating how tool-enabled LLMs
handle spatial reasoning in remote sensing. Our code and dataset are publicly
available

</details>


### [350] [Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping](https://arxiv.org/pdf/2505.23756)
*Justin Lazarow, Kai Kang, Afshin Dehghan*

Main category: cs.CV

TL;DR: Rooms from Motion (RfM) introduces an object-centric framework for 3D object detection and mapping using un-posed images, outperforming existing methods by leveraging 3D boxes for localization and optimization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D object detection methods rely on global approaches and metric camera poses, limiting their applicability. RfM aims to operate on un-posed images, providing a more flexible and object-centric solution.

Method: RfM replaces 2D keypoint-based matching with an object-centric matcher using 3D boxes, enabling metric camera pose estimation, object tracking, and global semantic 3D mapping. It optimizes global 3D boxes when pose data is available.

Result: RfM outperforms point-based and multi-view 3D detection methods on CA-1M and ScanNet++, achieving higher map quality and strong localization performance.

Conclusion: RfM offers a scalable, object-centric representation for 3D scenes, extending Cubify Anything to full scenes and enabling sparse localization and parametric mapping.

Abstract: We revisit scene-level 3D object detection as the output of an object-centric
framework capable of both localization and mapping using 3D oriented boxes as
the underlying geometric primitive. While existing 3D object detection
approaches operate globally and implicitly rely on the a priori existence of
metric camera poses, our method, Rooms from Motion (RfM) operates on a
collection of un-posed images. By replacing the standard 2D keypoint-based
matcher of structure-from-motion with an object-centric matcher based on
image-derived 3D boxes, we estimate metric camera poses, object tracks, and
finally produce a global, semantic 3D object map. When a priori pose is
available, we can significantly improve map quality through optimization of
global 3D boxes against individual observations. RfM shows strong localization
performance and subsequently produces maps of higher quality than leading
point-based and multi-view 3D object detection methods on CA-1M and ScanNet++,
despite these global methods relying on overparameterization through point
clouds or dense volumes. Rooms from Motion achieves a general, object-centric
representation which not only extends the work of Cubify Anything to full
scenes but also allows for inherently sparse localization and parametric
mapping proportional to the number of objects in a scene.

</details>


### [351] [Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models](https://arxiv.org/pdf/2505.23757)
*Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao*

Main category: cs.CV

TL;DR: The paper introduces Impromptu VLA, a dataset to improve Vision-Language-Action models for autonomous driving by addressing unstructured scenarios.


<details>
  <summary>Details</summary>
Motivation: Current VLA models struggle in unstructured corner cases due to a lack of targeted benchmarks.

Method: The authors create the Impromptu VLA Dataset, with 80,000 curated video clips and rich annotations, based on a novel taxonomy.

Result: VLAs trained with this dataset show significant performance gains in benchmarks like NeuroNCAP and nuScenes.

Conclusion: The dataset and Q&A suite effectively enhance VLA performance in perception, prediction, and planning.

Abstract: Vision-Language-Action (VLA) models for autonomous driving show promise but
falter in unstructured corner case scenarios, largely due to a scarcity of
targeted benchmarks. To address this, we introduce Impromptu VLA. Our core
contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated
video clips, distilled from over 2M source clips sourced from 8 open-source
large-scale datasets. This dataset is built upon our novel taxonomy of four
challenging unstructured categories and features rich, planning-oriented
question-answering annotations and action trajectories. Crucially, experiments
demonstrate that VLAs trained with our dataset achieve substantial performance
gains on established benchmarks--improving closed-loop NeuroNCAP scores and
collision rates, and reaching near state-of-the-art L2 accuracy in open-loop
nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an
effective diagnostic, revealing clear VLM improvements in perception,
prediction, and planning. Our code, data and models are available at
https://github.com/ahydchh/Impromptu-VLA.

</details>


### [352] [LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers](https://arxiv.org/pdf/2505.23758)
*Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag*

Main category: cs.CV

TL;DR: LoRAShop is a framework for multi-concept image editing using LoRA models, leveraging feature interactions in diffusion transformers for seamless edits.


<details>
  <summary>Details</summary>
Motivation: To enable practical and efficient multi-concept image editing without retraining or external constraints, preserving identity and context.

Method: Uses disentangled latent masks from prior forward passes to blend LoRA weights within concept-specific regions.

Result: Better identity preservation and seamless integration of multiple subjects/styles compared to baselines.

Conclusion: LoRAShop transforms personalized diffusion models into a practical tool for creative visual storytelling and rapid iteration.

Abstract: We introduce LoRAShop, the first framework for multi-concept image editing
with LoRA models. LoRAShop builds on a key observation about the feature
interaction patterns inside Flux-style diffusion transformers: concept-specific
transformer features activate spatially coherent regions early in the denoising
process. We harness this observation to derive a disentangled latent mask for
each concept in a prior forward pass and blend the corresponding LoRA weights
only within regions bounding the concepts to be personalized. The resulting
edits seamlessly integrate multiple subjects or styles into the original scene
while preserving global context, lighting, and fine details. Our experiments
demonstrate that LoRAShop delivers better identity preservation compared to
baselines. By eliminating retraining and external constraints, LoRAShop turns
personalized diffusion models into a practical `photoshop-with-LoRAs' tool and
opens new avenues for compositional visual storytelling and rapid creative
iteration.

</details>


### [353] [Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch](https://arxiv.org/pdf/2505.23763)
*Aneeshan Sain, Subhajit Maity, Pinaki Nath Chowdhury, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song*

Main category: cs.CV

TL;DR: The paper addresses the lack of efficient inference models for sketch data by proposing two plug-n-play components to adapt photo-efficient networks for sketches, achieving significant FLOP reduction while retaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing efficient models for photos don't work for sketches, and there's no research on sketch-specific efficient inference, despite its commercial potential.

Method: 1) Cross-modal knowledge distillation to adapt photo-efficient networks for sketches. 2) RL-based canvas selector to dynamically adjust to sketch abstraction.

Result: Reduced FLOPs by 99.37% (from 40.18G to 0.254G) while maintaining accuracy (33.03% vs 32.77%).

Conclusion: The proposed components enable efficient sketch inference with fewer FLOPs than photo models, making it viable for commercialization.

Abstract: As sketch research has collectively matured over time, its adaptation for
at-mass commercialisation emerges on the immediate horizon. Despite an already
mature research endeavour for photos, there is no research on the efficient
inference specifically designed for sketch data. In this paper, we first
demonstrate existing state-of-the-art efficient light-weight models designed
for photos do not work on sketches. We then propose two sketch-specific
components which work in a plug-n-play manner on any photo efficient network to
adapt them to work on sketch data. We specifically chose fine-grained
sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised
sketch problem with immediate commercial value. Technically speaking, we first
propose a cross-modal knowledge distillation network to transfer existing photo
efficient networks to be compatible with sketch, which brings down number of
FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then
exploit the abstract trait of sketch to introduce a RL-based canvas selector
that dynamically adjusts to the abstraction level which further cuts down
number of FLOPs by two thirds. The end result is an overall reduction of 99.37%
of FLOPs (from 40.18G to 0.254G) when compared with a full network, while
retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient
network for the sparse sketch data that exhibit even fewer FLOPs than the best
photo counterpart.

</details>


### [354] [Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought](https://arxiv.org/pdf/2505.23766)
*Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu*

Main category: cs.CV

TL;DR: Argus improves MLLMs by introducing a visual attention grounding mechanism for better vision-centric reasoning.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with vision-centric tasks requiring precise visual focus.

Method: Uses object-centric grounding as visual chain-of-thought signals for goal-conditioned visual attention.

Result: Excels in multimodal reasoning and referring object grounding tasks.

Conclusion: Highlights the importance of visual-centric approaches for advancing multimodal intelligence.

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
remarkable capabilities in vision-language tasks, yet they often struggle with
vision-centric scenarios where precise visual focus is needed for accurate
reasoning. In this paper, we introduce Argus to address these limitations with
a new visual attention grounding mechanism. Our approach employs object-centric
grounding as visual chain-of-thought signals, enabling more effective
goal-conditioned visual attention during multimodal reasoning tasks.
Evaluations on diverse benchmarks demonstrate that Argus excels in both
multimodal reasoning tasks and referring object grounding tasks. Extensive
analysis further validates various design choices of Argus, and reveals the
effectiveness of explicit language-guided visual region-of-interest engagement
in MLLMs, highlighting the importance of advancing multimodal intelligence from
a visual-centric perspective. Project page: https://yunzeman.github.io/argus/

</details>


### [355] [TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models](https://arxiv.org/pdf/2505.23769)
*Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem*

Main category: cs.CV

TL;DR: TextRegion combines image-text models and SAM2 for detailed visual understanding without training, achieving strong performance in tasks like segmentation and grounding.


<details>
  <summary>Details</summary>
Motivation: Image-text models lack detailed spatial understanding, while segmentation models like SAM2 provide precise boundaries but miss text alignment. TextRegion bridges this gap.

Method: Proposes a training-free framework integrating image-text models and SAM2 to generate text-aligned region tokens for detailed visual tasks.

Result: Superior or competitive performance in open-world segmentation, referring expression comprehension, and grounding compared to state-of-the-art training-free methods.

Conclusion: TextRegion is practical, extensible, and effective for combining visual-language alignment with precise spatial understanding.

Abstract: Image-text models excel at image-level tasks but struggle with detailed
visual understanding. While these models provide strong visual-language
alignment, segmentation models like SAM2 offer precise spatial boundaries for
objects. To this end, we propose TextRegion, a simple, effective, and
training-free framework that combines the strengths of image-text models and
SAM2 to generate powerful text-aligned region tokens. These tokens enable
detailed visual understanding while preserving open-vocabulary capabilities.
They can be directly applied to various downstream tasks, including open-world
semantic segmentation, referring expression comprehension, and grounding. We
conduct extensive evaluations and consistently achieve superior or competitive
performance compared to state-of-the-art training-free methods. Additionally,
our framework is compatible with many image-text models, making it highly
practical and easily extensible as stronger models emerge. Code is available
at: https://github.com/avaxiao/TextRegion.

</details>


### [356] [Cross-Modal Causal Intervention for Medical Report Generation](https://arxiv.org/pdf/2303.09117)
*Weixing Chen, Yang Liu, Ce Wang, Jiarui Zhu, Guanbin Li, Cheng-Lin Liu, Liang Lin*

Main category: cs.CV

TL;DR: The paper proposes CMCRL, a two-stage framework for Radiology Report Generation (RRG), addressing challenges like visual-linguistic biases and imaging limitations. It includes RadCARE for pre-training and VLCI for fine-tuning, achieving superior results on IU-Xray and MIMIC-CXR datasets.


<details>
  <summary>Details</summary>
Motivation: To alleviate radiologists' workload by improving RRG accuracy, overcoming issues like spurious correlations, low resolution, and noise in radiological images.

Method: A two-stage framework: 1) RadCARE pre-training with degradation-aware image restoration and multiway architecture; 2) VLCI fine-tuning with causal intervention modules (VDM and LDM) to disentangle biases.

Result: CMCRL outperforms state-of-the-art methods on IU-Xray and MIMIC-CXR datasets, validated by ablation studies.

Conclusion: CMCRL effectively addresses RRG challenges, demonstrating the importance of both pre-training and fine-tuning stages for robust performance.

Abstract: Radiology Report Generation (RRG) is essential for computer-aided diagnosis
and medication guidance, which can relieve the heavy burden of radiologists by
automatically generating the corresponding radiology reports according to the
given radiology image. However, generating accurate lesion descriptions remains
challenging due to spurious correlations from visual-linguistic biases and
inherent limitations of radiological imaging, such as low resolution and noise
interference. To address these issues, we propose a two-stage framework named
CrossModal Causal Representation Learning (CMCRL), consisting of the
Radiological Cross-modal Alignment and Reconstruction Enhanced (RadCARE)
pre-training and the Visual-Linguistic Causal Intervention (VLCI) fine-tuning.
In the pre-training stage, RadCARE introduces a degradation-aware masked image
restoration strategy tailored for radiological images, which reconstructs
high-resolution patches from low-resolution inputs to mitigate noise and detail
loss. Combined with a multiway architecture and four adaptive training
strategies (e.g., text postfix generation with degraded images and text
prefixes), RadCARE establishes robust cross-modal correlations even with
incomplete data. In the VLCI phase, we deploy causal front-door intervention
through two modules: the Visual Deconfounding Module (VDM) disentangles
local-global features without fine-grained annotations, while the Linguistic
Deconfounding Module (LDM) eliminates context bias without external terminology
databases. Experiments on IU-Xray and MIMIC-CXR show that our CMCRL pipeline
significantly outperforms state-of-the-art methods, with ablation studies
confirming the necessity of both stages. Code and models are available at
https://github.com/WissingChen/CMCRL.

</details>


### [357] [Calibrating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation](https://arxiv.org/pdf/2305.03112)
*Lechao Cheng, Zerun Liu, Jingxuan He, Chaowei Fang, Dingwen Zhang, Meng Wang*

Main category: cs.CV

TL;DR: The paper proposes an Adaptive Re-Activation Mechanism (AReAM) to improve weakly supervised semantic segmentation by addressing over-smoothing in deep-layer attention and refining CAMs using entropy-aware guidance from shallow layers.


<details>
  <summary>Details</summary>
Motivation: Weakly supervised semantic segmentation (WSSS) reduces annotation effort but suffers from noise in deep-layer attention (over-smoothing), which harms CAM accuracy.

Method: AReAM leverages shallow-level affinity to guide deeper-layer convergence, suppressing noise and reactivating key semantic regions in CAMs.

Result: Experiments show AReAM outperforms existing WSSS methods, reducing noise and improving segmentation accuracy.

Conclusion: The work highlights the need to control deep-level attention, introduces an entropy-aware mechanism, and enhances transformer-based WSSS by refining CAMs.

Abstract: Weakly supervised semantic segmentation (WSSS) has recently attracted
considerable attention because it requires fewer annotations than fully
supervised approaches, making it especially promising for large-scale image
segmentation tasks. Although many vision transformer-based methods leverage
self-attention affinity matrices to refine Class Activation Maps (CAMs), they
often treat each layer's affinity equally and thus introduce considerable
background noise at deeper layers, where attention tends to converge
excessively on certain tokens (i.e., over-smoothing). We observe that this
deep-level attention naturally converges on a subset of tokens, yet unregulated
query-key affinity can generate unpredictable activation patterns
(undisciplined over-smoothing), adversely affecting CAM accuracy. To address
these limitations, we propose an Adaptive Re-Activation Mechanism (AReAM),
which exploits shallow-level affinity to guide deeper-layer convergence in an
entropy-aware manner, thereby suppressing background noise and re-activating
crucial semantic regions in the CAMs. Experiments on two commonly used datasets
demonstrate that AReAM substantially improves segmentation performance compared
with existing WSSS methods, reducing noise while sharpening focus on relevant
semantic regions. Overall, this work underscores the importance of controlling
deep-level attention to mitigate undisciplined over-smoothing, introduces an
entropy-aware mechanism that harmonizes shallow and deep-level affinities, and
provides a refined approach to enhance transformer-based WSSS accuracy by
re-activating CAMs.

</details>


### [358] [SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes](https://arxiv.org/pdf/2307.07333)
*Zhili Ng, Haozhe Wang, Zhengshen Zhang, Francis Tay Eng Hock, Marcelo H. Ang Jr*

Main category: cs.CV

TL;DR: SynTable is a Python-based tool for generating high-quality synthetic datasets for unseen object amodal instance segmentation, eliminating manual labeling and improving Sim-to-Real transfer performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality synthetic datasets in deep learning research without manual labeling, ensuring accuracy and flexibility.

Method: Uses NVIDIA's Isaac Sim Replicator Composer to render complex 3D scenes with metadata like segmentation masks, depth maps, and bounding boxes.

Result: Demonstrated improved performance in Sim-to-Real transfer on the OSD-Amodal dataset using UOAIS-Net.

Conclusion: SynTable is an open-source, photorealistic dataset generator that advances research in synthetic data and deep learning.

Abstract: In this work, we present SynTable, a unified and flexible Python-based
dataset generator built using NVIDIA's Isaac Sim Replicator Composer for
generating high-quality synthetic datasets for unseen object amodal instance
segmentation of cluttered tabletop scenes. Our dataset generation tool can
render complex 3D scenes containing object meshes, materials, textures,
lighting, and backgrounds. Metadata, such as modal and amodal instance
segmentation masks, object amodal RGBA instances, occlusion masks, depth maps,
bounding boxes, and material properties can be automatically generated to
annotate the scene according to the users' requirements. Our tool eliminates
the need for manual labeling in the dataset generation process while ensuring
the quality and accuracy of the dataset. In this work, we discuss our design
goals, framework architecture, and the performance of our tool. We demonstrate
the use of a sample dataset generated using SynTable for training a
state-of-the-art model, UOAIS-Net. Our state-of-the-art results show
significantly improved performance in Sim-to-Real transfer when evaluated on
the OSD-Amodal dataset. We offer this tool as an open-source, easy-to-use,
photorealistic dataset generator for advancing research in deep learning and
synthetic data generation. The links to our source code, demonstration video,
and sample dataset can be found in the supplementary materials.

</details>


### [359] [PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes](https://arxiv.org/pdf/2309.10815)
*Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao*

Main category: cs.CV

TL;DR: PanopticNeRF-360 combines 3D and 2D priors to generate high-quality panoptic labels and images from any viewpoint, improving generalization for self-driving perception systems.


<details>
  <summary>Details</summary>
Motivation: Manual 2D annotation is labor-intensive, and existing datasets lack rare viewpoints, limiting model generalization.

Method: Leverages coarse 3D annotations and noisy 2D cues to guide geometry optimization, using a hybrid MLP-hash grid for appearance.

Result: Achieves state-of-the-art performance on KITTI-360, enabling high-fidelity omnidirectional rendering.

Conclusion: PanopticNeRF-360 enhances perception models by generating consistent, high-quality labels and images from any viewpoint.

Abstract: Training perception systems for self-driving cars requires substantial 2D
annotations that are labor-intensive to manual label. While existing datasets
provide rich annotations on pre-recorded sequences, they fall short in labeling
rarely encountered viewpoints, potentially hampering the generalization ability
for perception models. In this paper, we present PanopticNeRF-360, a novel
approach that combines coarse 3D annotations with noisy 2D semantic cues to
generate high-quality panoptic labels and images from any viewpoint. Our key
insight lies in exploiting the complementarity of 3D and 2D priors to mutually
enhance geometry and semantics. Specifically, we propose to leverage coarse 3D
bounding primitives and noisy 2D semantic and instance predictions to guide
geometry optimization, by encouraging predicted labels to match panoptic pseudo
ground truth. Simultaneously, the improved geometry assists in filtering 3D&2D
annotation noise by fusing semantics in 3D space via a learned semantic field.
To further enhance appearance, we combine MLP and hash grids to yield hybrid
scene features, striking a balance between high-frequency appearance and
contiguous semantics. Our experiments demonstrate PanopticNeRF-360's
state-of-the-art performance over label transfer methods on the challenging
urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables
omnidirectional rendering of high-fidelity, multi-view and spatiotemporally
consistent appearance, semantic and instance labels. We make our code and data
available at https://github.com/fuxiao0719/PanopticNeRF

</details>


### [360] [Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact (Version. 2.0)](https://arxiv.org/pdf/2312.03207)
*Patrick Beukema, Favyen Bastani, Yawen Zheng, Piper Wolters, Henry Herzog, Joe Ferdinando*

Main category: cs.CV

TL;DR: The paper introduces four specialized computer vision models for monitoring IUU fishing using satellite data, deployed in the free Skylight platform.


<details>
  <summary>Details</summary>
Motivation: IUU fishing threatens ocean habitats, and publicly available satellite data can help monitor it. Reliable, low-latency machine learning models are needed for global maritime conservation.

Method: Developed four computer vision models for different satellite sensors (Sentinel-1, Sentinel-2, Landsat 8-9, and Suomi-NPP/NOAA-20/NOAA-21) and shared best practices for global-scale, real-time deployment.

Result: All models are open-sourced and deployed in Skylight, a free real-time maritime monitoring platform.

Conclusion: The models and platform provide a scalable solution for combating IUU fishing using satellite data and computer vision.

Abstract: Illegal, unreported, and unregulated (IUU) fishing poses a global threat to
ocean habitats. Publicly available satellite data offered by NASA, the European
Space Agency (ESA), and the U.S. Geological Survey (USGS), provide an
opportunity to actively monitor this activity. Effectively leveraging satellite
data for maritime conservation requires highly reliable machine learning models
operating globally with minimal latency. This paper introduces four specialized
computer vision models designed for a variety of sensors including Sentinel-1
(synthetic aperture radar), Sentinel-2 (optical imagery), Landsat 8-9 (optical
imagery), and Suomi-NPP/NOAA-20/NOAA-21 (nighttime lights). It also presents
best practices for developing and deploying global-scale real-time satellite
based computer vision. All of the models are open sourced under permissive
licenses. These models have all been deployed in Skylight, a real-time maritime
monitoring platform, which is provided at no cost to users worldwide.

</details>


### [361] [ChatHuman: Chatting about 3D Humans with Tools](https://arxiv.org/pdf/2405.04533)
*Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black*

Main category: cs.CV

TL;DR: ChatHuman is a language-driven system integrating specialized 3D human task tools into a unified framework using an LLM, improving tool selection and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D human tasks require expert knowledge, limiting accessibility. ChatHuman aims to simplify usage and interpretation.

Method: Built on an LLM, ChatHuman autonomously selects and applies tools, uses academic publications for training, and transforms 3D outputs into understandable formats.

Result: ChatHuman outperforms existing models in tool selection accuracy and task performance, supporting interactive user chats.

Conclusion: ChatHuman advances the consolidation of diverse 3D human task methods into a unified, user-friendly system.

Abstract: Numerous methods have been proposed to detect, estimate, and analyze
properties of people in images, including 3D pose, shape, contact, human-object
interaction, and emotion. While widely applicable in vision and other areas,
such methods require expert knowledge to select, use, and interpret the
results. To address this, we introduce ChatHuman, a language-driven system that
integrates the capabilities of specialized methods into a unified framework.
ChatHuman functions as an assistant proficient in utilizing, analyzing, and
interacting with tools specific to 3D human tasks, adeptly discussing and
resolving related challenges. Built on a Large Language Model (LLM) framework,
ChatHuman is trained to autonomously select, apply, and interpret a diverse set
of tools in response to user inputs. Our approach overcomes significant hurdles
in adapting LLMs to 3D human tasks, including the need for domain-specific
knowledge and the ability to interpret complex 3D outputs. The innovations of
ChatHuman include leveraging academic publications to instruct the LLM on tool
usage, employing a retrieval-augmented generation model to create in-context
learning examples for managing new tools, and effectively discriminating
between and integrating tool results by transforming specialized 3D outputs
into comprehensible formats. Experiments demonstrate that ChatHuman surpasses
existing models in both tool selection accuracy and overall performance across
various 3D human tasks, and it supports interactive chatting with users.
ChatHuman represents a significant step toward consolidating diverse analytical
methods into a unified, robust system for 3D human tasks.

</details>


### [362] [Information Entropy Guided Height-aware Histogram for Quantization-friendly Pillar Feature Encoder](https://arxiv.org/pdf/2405.18734)
*Sifan Zhou, Zhihang Yuan, Dawei Yang, Ziyu Zhao, Xing Hu, Yuguang Shi, Xiaobo Lu, Qiang Wu*

Main category: cs.CV

TL;DR: PillarHist, a height-aware pillar feature encoder, addresses information loss and numerical distribution issues in pillar-based 3D object detectors, improving performance and quantization potential.


<details>
  <summary>Details</summary>
Motivation: Existing pillar-based detectors suffer from information loss and numerical distribution differences during pillar feature encoding (PFE), limiting performance and quantization.

Method: Proposes PillarHist, which uses discrete distribution statistics and information entropy to preserve height dimension information and stabilize PFE input.

Result: PillarHist enhances performance and efficiency while being quantization-friendly, seamlessly integrating into existing methods.

Conclusion: PillarHist effectively improves 3D object detection by addressing PFE limitations without adding complexity.

Abstract: Real-time and high-performance 3D object detection plays a critical role in
autonomous driving and robotics. Recent pillar-based 3D object detectors have
gained significant attention due to their compact representation and low
computational overhead, making them suitable for onboard deployment and
quantization. However, existing pillar-based detectors still suffer from
information loss along height dimension and large numerical distribution
difference during pillar feature encoding (PFE), which severely limits their
performance and quantization potential. To address above issue, we first unveil
the importance of different input information during PFE and identify the
height dimension as a key factor in enhancing 3D detection performance.
Motivated by this observation, we propose a height-aware pillar feature
encoder, called PillarHist. Specifically, PillarHist statistics the discrete
distribution of points at different heights within one pillar with the
information entropy guidance. This simple yet effective design greatly
preserves the information along the height dimension while significantly
reducing the computation overhead of the PFE. Meanwhile, PillarHist also
constrains the arithmetic distribution of PFE input to a stable range, making
it quantization-friendly. Notably, PillarHist operates exclusively within the
PFE stage to enhance performance, enabling seamless integration into existing
pillar-based methods without introducing complex operations. Extensive
experiments show the effectiveness of PillarHist in terms of both efficiency
and performance.

</details>


### [363] [UDHF2-Net: Uncertainty-diffusion-model-based High-Frequency TransFormer Network for Remotely Sensed Imagery Interpretation](https://arxiv.org/pdf/2406.16129)
*Pengfei Zhang, Chang Li, Yongjun Zhang, Rongjun Qin, Kyle Gao, Jonathan Li*

Main category: cs.CV

TL;DR: The paper proposes UDHF2-Net to address RSII problems like edge uncertainty and false detection, using SHCP for edge extraction, MUDM for noise reduction, and a semi-pseudo-Siamese network for change detection.


<details>
  <summary>Details</summary>
Motivation: To solve key challenges in RSII: spatial distribution representation, edge uncertainty, and false detection in change detection.

Method: UDHF2-Net integrates SHCP for high-frequency edge extraction, MUDM for noise diffusion, and a semi-pseudo-Siamese network for balanced change detection.

Result: Improved edge extraction accuracy and reduced false detection in change detection tasks.

Conclusion: UDHF2-Net effectively addresses RSII problems with innovative frequency-wise and self-supervised techniques.

Abstract: Remotely sensed imagery interpretation (RSII) faces the three major problems:
(1) objective representation of spatial distribution patterns; (2) edge
uncertainty problem caused by downsampling encoder and intrinsic edge noises
(e.g., mixed pixel and edge occlusion etc.); and (3) false detection problem
caused by geometric registration error in change detection. To solve the
aforementioned problems, uncertainty-diffusion-model-based high-Frequency
TransFormer network (UDHF2-Net) is the first to be proposed, whose
superiorities are as follows: (1) a spatially-stationary-and-non-stationary
high-frequency connection paradigm (SHCP) is proposed to enhance the
interaction of spatially frequency-wise stationary and non-stationary features
to yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP
proposes high-frequency-wise stream to replace high-resolution-wise stream in
HRFormer through the whole encoder-decoder process with parallel frequency-wise
high-to-low streams, so it improves the edge extraction accuracy by
continuously remaining high-frequency information; (2) a
mask-and-geo-knowledge-based uncertainty diffusion module (MUDM), which is a
self-supervised learning strategy, is proposed to improve the edge accuracy of
extraction and change detection by gradually removing the simulated spectrum
noises based on geo-knowledge and the generated diffused spectrum noises; (3) a
frequency-wise semi-pseudo-Siamese UDHF2-Net is the first to be proposed to
balance accuracy and complexity for change detection. Besides the
aforementioned spectrum noises in semantic segmentation, MUDM is also a
self-supervised learning strategy to effectively reduce the edge false change
detection from the generated imagery with geometric registration error.

</details>


### [364] [Low-Rank Interconnected Adaptation across Layers](https://arxiv.org/pdf/2407.09946)
*Yibo Zhong, Jinman Zhao, Yao Zhou*

Main category: cs.CV

TL;DR: Lily, a novel PEFT method, improves upon LoRA by introducing interconnected low-rank adapters with shared experts, enhancing adaptation performance without increasing parameters.


<details>
  <summary>Details</summary>
Motivation: LoRA's low-rank weight updates limit adaptation performance, prompting the need for a more efficient and expressive PEFT method.

Method: Lily uses locally shared A and globally shared B experts, with data-dependent routers to determine A-B interconnections, avoiding redundancy and improving representational power.

Result: Experiments show Lily outperforms LoRA in performance and efficiency across various modalities, architectures, and model sizes.

Conclusion: Lily offers a superior PEFT method by enabling higher-rank weight updates with fewer parameters, enhancing adaptation performance.

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
(PEFT) method that learns weight updates $\Delta W = AB$ for pretrained weights
$W$ through low-rank adapters $A$ and $B$. While LoRA ensures hardware
efficiency, its low-rank weight updates limit adaptation performance. In this
paper, we propose low-rank interconnected adaptation across layers (Lily), a
novel PEFT method that introduces an interconnected framework with locally
shared $A$ and globally shared $B$ experts. This structure eliminates redundant
per-layer $AB$ pairs, enabling higher-rank $\Delta W$ with equal or fewer
parameters. To enhance expressiveness, we use data-dependent routers to
determine $A$-$B$ interconnections, preventing $B$ experts from converging to
the same behavior and improving representational power across domains.
Experiments across modalities, architectures, and model sizes demonstrate
Lily's superior performance and efficiency. GitHub:
https://github.com/yibozhong/lily

</details>


### [365] [DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes](https://arxiv.org/pdf/2409.04003)
*Jianbiao Mei, Tao Hu, Xuemeng Yang, Licheng Wen, Yu Yang, Tiantian Wei, Yukai Ma, Min Dou, Botian Shi, Yong Liu*

Main category: cs.CV

TL;DR: DreamForge is a diffusion-based autoregressive model for generating long, 3D-controllable driving scene videos, improving lane and foreground generation with perspective guidance and object-wise encoding, and enhancing motion modeling with temporal attention.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in accurately modeling driving scenes and generating long videos using diffusion models.

Method: Proposes DreamForge with perspective guidance, object-wise position encoding, and motion-aware temporal attention for autoregressive long-video generation.

Result: Generates high-quality long videos (over 200 frames) from short-sequence training, outperforming baselines in 16-frame evaluations.

Conclusion: DreamForge advances controllable streetscape generation and integrates with DriveArena for reliable evaluation of vision-based driving agents.

Abstract: Recent advances in diffusion models have improved controllable streetscape
generation and supported downstream perception and planning tasks. However,
challenges remain in accurately modeling driving scenes and generating long
videos. To alleviate these issues, we propose DreamForge, an advanced
diffusion-based autoregressive video generation model tailored for
3D-controllable long-term generation. To enhance the lane and foreground
generation, we introduce perspective guidance and integrate object-wise
position encoding to incorporate local 3D correlation and improve foreground
object modeling. We also propose motion-aware temporal attention to capture
motion cues and appearance changes in videos. By leveraging motion frames and
an autoregressive generation paradigm,we can autoregressively generate long
videos (over 200 frames) using a model trained in short sequences, achieving
superior quality compared to the baseline in 16-frame video evaluations.
Finally, we integrate our method with the realistic simulator DriveArena to
provide more reliable open-loop and closed-loop evaluations for vision-based
driving agents. Project Page:
https://pjlab-adg.github.io/DriveArena/dreamforge.

</details>


### [366] [RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning](https://arxiv.org/pdf/2409.13366)
*Wenhui Diao, Haichen Yu, Kaiyue Kang, Tong Ling, Di Liu, Yingchao Feng, Hanbo Bi, Libo Ren, Xuexue Li, Yongqiang Mao, Xian Sun*

Main category: cs.CV

TL;DR: RingMo-Aerial, a foundation model for Aerial Remote Sensing (ARS) vision, introduces FE-MSA and contrastive learning to improve small target detection and adaptability for tilted angles. ARS-Adapter enhances task-specific performance, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing ARS vision algorithms are task-specific and lack broad applicability. This paper aims to develop a foundation model to address these limitations.

Method: Proposes RingMo-Aerial with FE-MSA and affine transformation-based contrastive learning. Introduces ARS-Adapter for efficient fine-tuning.

Result: Achieves SOTA performance on multiple downstream ARS vision tasks.

Conclusion: RingMo-Aerial is practical and effective for enhancing ARS vision performance.

Abstract: Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to
the unique characteristics of their viewing angles. Existing research has
primarily focused on algorithms for specific tasks, which have limited
applicability in a broad range of ARS vision applications. This paper proposes
the RingMo-Aerial model, aiming to fill the gap in foundation model research in
the field of ARS vision. By introducing the Frequency-Enhanced Multi-Head
Self-Attention (FE-MSA) mechanism and an affine transformation-based
contrastive learning pre-training method, the model's detection capability for
small targets is enhanced and optimized for the tilted viewing angles
characteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter
fine-tuning method, is proposed to improve the model's adaptability and
effectiveness in various ARS vision tasks. Experimental results demonstrate
that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This
indicates the practicality and efficacy of RingMo-Aerial in enhancing the
performance of ARS vision tasks.

</details>


### [367] [X2-DFD: A framework for eXplainable and eXtendable Deepfake Detection](https://arxiv.org/pdf/2410.06126)
*Yize Chen, Zhiyuan Yan, Guangliang Cheng, Kangran Zhao, Siwei Lyu, Baoyuan Wu*

Main category: cs.CV

TL;DR: X2-DFD is an explainable, extendable framework using multimodal large-language models (MLLMs) for deepfake detection, improving detectability and explainability through feature assessment, dataset construction, and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enhance deepfake detection by addressing MLLM limitations in feature detectability and explainability, while ensuring adaptability to future advancements.

Method: Three-stage framework: Model Feature Assessment ranks forgery-related features; Explainable Dataset Construction strengthens strong features and supplements weak ones; Fine-tuning and Inference deploys the model.

Result: Improved performance over original MLLMs, validated by experiments and human studies, with plug-and-play adaptability for future MLLMs and detectors.

Conclusion: X2-DFD effectively enhances deepfake detection and explainability, with potential for continual improvement as MLLMs evolve.

Abstract: This paper proposes X2-DFD, an eXplainable and eXtendable framework based on
multimodal large-language models (MLLMs) for deepfake detection, consisting of
three key stages. The first stage, Model Feature Assessment, systematically
evaluates the detectability of forgery-related features for the MLLM,
generating a prioritized ranking of features based on their intrinsic
importance to the model. The second stage, Explainable Dataset Construction,
consists of two key modules: Strong Feature Strengthening, which is designed to
enhance the model's existing detection and explanation capabilities by
reinforcing its well-learned features, and Weak Feature Supplementing, which
addresses gaps by integrating specific feature detectors (e.g., low-level
artifact analyzers) to compensate for the MLLM's limitations. The third stage,
Fine-tuning and Inference, involves fine-tuning the MLLM on the constructed
dataset and deploying it for final detection and explanation. By integrating
these three stages, our approach enhances the MLLM's strengths while
supplementing its weaknesses, ultimately improving both the detectability and
explainability. Extensive experiments and ablations, followed by a
comprehensive human study, validate the improved performance of our approach
compared to the original MLLMs. More encouragingly, our framework is designed
to be plug-and-play, allowing it to seamlessly integrate with future more
advanced MLLMs and specific feature detectors, leading to continual improvement
and extension to face the challenges of rapidly evolving deepfakes.

</details>


### [368] [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/pdf/2410.10112)
*Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould*

Main category: cs.CV

TL;DR: A framework using probabilistic matrix factorization (PMF) with MCMC predicts unknown performance scores of large vision-language models (LVLMs) from observed ones, improving efficiency in evaluation.


<details>
  <summary>Details</summary>
Motivation: Evaluating LVLMs is costly and time-consuming; predicting unknown scores from existing data can save resources.

Method: Formulates performance prediction as matrix completion, applies PMF with MCMC to infer missing scores, and estimates uncertainty to guide evaluation order.

Result: PMF accurately predicts unknown scores, uncertainty estimates reliably guide evaluation, and enhancements improve performance with sparse data.

Conclusion: The proposed framework efficiently predicts LVLM performance, reducing evaluation costs and improving decision-making for practitioners.

Abstract: Evaluating large vision-language models (LVLMs) is very expensive, due to
high computational cost and the wide variety of tasks. The good news is that if
we already have some observed performance scores, we may be able to infer
unknown ones. In this study, we propose a new framework for predicting unknown
performance scores based on observed ones from other LVLMs or tasks. We first
formulate the performance prediction as a matrix completion task. Specifically,
we construct a sparse performance matrix $\boldsymbol{R}$, where each entry
$R_{mn}$ represents the performance score of the $m$-th model on the $n$-th
dataset. By applying probabilistic matrix factorization (PMF) with Markov chain
Monte Carlo (MCMC), we can complete the performance matrix, i.e., predict
unknown scores. Additionally, we estimate the uncertainty of performance
prediction based on MCMC. Practitioners can evaluate their models on untested
tasks with higher uncertainty first, which quickly reduces the prediction
errors. We further introduce several improvements to enhance PMF for scenarios
with sparse observed performance scores. Our experiments demonstrate the
accuracy of PMF in predicting unknown scores, the reliability of uncertainty
estimates in ordering evaluations, and the effectiveness of our enhancements
for handling sparse data.

</details>


### [369] [Rethinking Positive Pairs in Contrastive Learning](https://arxiv.org/pdf/2410.18200)
*Jiantao Wu, Sara Atito, Zhenhua Feng, Shentong Mo, Josef Kitler, Muhammad Awais*

Main category: cs.CV

TL;DR: SimLAP introduces a framework for learning visual representations from arbitrary sample pairs, exploiting similarity in subspaces of semantically distinct classes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods focus on similarity from identical pairs; SimLAP explores learning from distinct pairs by leveraging subspace similarity.

Method: SimLAP optimizes similarity for arbitrary pairs while learning the subspace enabling this similarity.

Result: Experimental results demonstrate the feasibility and merits of the approach.

Conclusion: SimLAP offers a novel way to learn visual representations by leveraging similarity in subspaces of distinct classes.

Abstract: The training methods in AI do involve semantically distinct pairs of samples.
However, their role typically is to enhance the between class separability. The
actual notion of similarity is normally learned from semantically identical
pairs. This paper presents SimLAP: a simple framework for learning visual
representation from arbitrary pairs. SimLAP explores the possibility of
learning similarity from semantically distinct sample pairs. The approach is
motivated by the observation that for any pair of classes there exists a
subspace in which semantically distinct samples exhibit similarity. This
phenomenon can be exploited for a novel method of learning, which optimises the
similarity of an arbitrary pair of samples, while simultaneously learning the
enabling subspace. The feasibility of the approach will be demonstrated
experimentally and its merits discussed.

</details>


### [370] [Point Cloud Synthesis Using Inner Product Transforms](https://arxiv.org/pdf/2410.18987)
*Ernst Röell, Bastian Rieck*

Main category: cs.CV

TL;DR: A novel method for point-cloud synthesis encodes geometric-topological features using inner products, offering efficient representation and fast inference.


<details>
  <summary>Details</summary>
Motivation: Point-cloud synthesis is challenging, and existing methods are complex and slow.

Method: Encodes geometric-topological characteristics of point clouds using inner products for efficient representation.

Result: High quality in tasks like reconstruction, generation, and interpolation, with significantly faster inference times.

Conclusion: The method provides an efficient and expressive solution for point-cloud synthesis, outperforming existing approaches.

Abstract: Point-cloud synthesis, i.e. the generation of novel point clouds from an
input distribution, remains a challenging task, for which numerous complex
machine-learning models have been devised. We develop a novel method that
encodes geometrical-topological characteristics of point clouds using inner
products, leading to a highly-efficient point cloud representation with
provable expressivity properties. Integrated into deep learning models, our
encoding exhibits high quality in typical tasks like reconstruction,
generation, and interpolation, with inference times orders of magnitude faster
than existing methods.

</details>


### [371] [One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks](https://arxiv.org/pdf/2410.22725)
*Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, Hongwei Li*

Main category: cs.CV

TL;DR: VerifyPrompt is a method to verify if a black-box T2I model matches a reference model using adversarial prompts optimized by NSGA-II, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Third-party T2I model services may misrepresent models, raising security concerns about their authenticity.

Method: Uses adversarial prompts (verify prompts) optimized via NSGA-II to generate unique images for the target model, then checks alignment using CLIP-text similarity.

Result: VerifyPrompt achieves over 90% accuracy in verifying T2I models across various platforms.

Conclusion: VerifyPrompt effectively addresses the security concern of model misrepresentation in third-party T2I services.

Abstract: Recently, various types of Text-to-Image (T2I) models have emerged (such as
DALL-E and Stable Diffusion), and showing their advantages in different
aspects. Therefore, some third-party service platforms collect different model
interfaces and provide cheaper API services and more flexibility in T2I model
selections. However, this also raises a new security concern: Are these
third-party services truly offering the models they claim?
  To answer this question, we first define the concept of T2I model
verification, which aims to determine whether a black-box target model is
identical to a given white-box reference T2I model. After that, we propose
VerifyPrompt, which performs T2I model verification through a special designed
verify prompt. Intuitionally, the verify prompt is an adversarial prompt for
the target model without transferability for other models. It makes the target
model generate a specific image while making other models produce entirely
different images. Specifically, VerifyPrompt utilizes the Non-dominated Sorting
Genetic Algorithm II (NSGA-II) to optimize the cosine similarity of a prompt's
text encoding, generating verify prompts. Finally, by computing the CLIP-text
similarity scores between the prompts the generated images, VerifyPrompt can
determine whether the target model aligns with the reference model.
Experimental results demonstrate that VerifyPrompt consistently achieves over
90\% accuracy across various T2I models, confirming its effectiveness in
practical model platforms (such as Hugging Face).

</details>


### [372] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/pdf/2411.18625)
*Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting (3DGS) is improved by adding texture and alpha maps to Gaussians, enhancing their expressivity for better 3D reconstruction and rendering.


<details>
  <summary>Details</summary>
Motivation: The limitations of 3DGS, such as uniform shading and simple geometric detail representation, restrict its expressivity.

Method: Augment each Gaussian with alpha, RGB, or RGBA texture maps to model spatially varying color and opacity.

Result: Improved expressivity and image quality, validated on benchmark datasets and custom captures.

Conclusion: The proposed method enhances 3DGS by integrating texture mapping, achieving superior results with similar or fewer Gaussians.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [373] [Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery](https://arxiv.org/pdf/2411.19093)
*Othmane Echchabi, Aya Lahlou, Nizar Talty, Josh Malcolm Manto, Ka Leung Lam*

Main category: cs.CV

TL;DR: The study integrates survey data, satellite imagery, and deep learning to evaluate water and sanitation access in Africa, achieving high accuracy and alignment with UN statistics.


<details>
  <summary>Details</summary>
Motivation: Global disparities in clean water and sanitation access persist, and data limitations hinder progress tracking.

Method: Combines Afrobarometer survey data, Landsat 8 and Sentinel-2 satellite imagery, and Meta's DINO model for classification.

Result: Achieved 96% accuracy for piped water and 97% for sewage system classification, with high concordance to UN data.

Conclusion: Provides a scalable tool for policymakers to target interventions and can be adapted for other SDGs.

Abstract: Clean water and sanitation are essential for health, well-being, and
sustainable development, yet significant global disparities persist. Although
the United Nations' Sustainable Development Goal (SDG) 6 clearly defines
targets for universal access to clean water and sanitation, limitations in data
coverage and openness impede accurate tracking of progress in many countries.
To bridge these gaps, this study integrates Afrobarometer survey data,
satellite imagery from Landsat 8 and Sentinel-2, and advanced deep learning
techniques using Meta's self-supervised Distillation with No Labels (DINO)
model to develop a modeling framework for evaluating access to piped water and
sewage system across diverse African regions. The modeling framework achieved
notable accuracy, with over 96% for piped water and 97% for sewage system
access classification. When combined with geospatial population data,
validation against official statistics from the United Nations Joint Monitoring
Program demonstrated high concordance at the national scale (R2 of 0.95 for
piped water access and R2 of 0.85 for sewage system access). The national-level
estimates can represent SDG Indicators 6.1.1 and 6.2.1. This approach provides
policymakers and stakeholders with an effective, scalable, and cost-efficient
tool to pinpoint underserved areas requiring targeted intervention. The
methodology developed herein can be adapted for assessing other
infrastructure-related SDGs, promoting enhanced monitoring and informed
decision-making towards achieving global sustainability objectives.

</details>


### [374] [SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding](https://arxiv.org/pdf/2412.04383)
*Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang*

Main category: cs.CV

TL;DR: SeeGround is a zero-shot 3DVG framework using 2D VLMs to locate objects in 3D scenes without relying on annotated 3D data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional 3DVG methods depend on annotated datasets and predefined categories, limiting scalability. SeeGround aims to overcome this by leveraging 2D VLMs.

Method: SeeGround represents 3D scenes as hybrid query-aligned rendered images and enriched text descriptions. It includes Perspective Adaptation and Fusion Alignment Modules.

Result: SeeGround outperforms zero-shot methods by 7.7% on ScanRefer and 7.1% on Nr3D, rivaling supervised approaches.

Conclusion: SeeGround effectively bridges 3D data and 2D-VLMs, demonstrating strong performance in complex 3DVG tasks.

Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on
textual descriptions, essential for applications like augmented reality and
robotics. Traditional 3DVG approaches rely on annotated 3D datasets and
predefined object categories, limiting scalability and adaptability. To
overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework
leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data.
SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and
spatially enriched text descriptions, bridging the gap between 3D data and
2D-VLMs input formats. We propose two modules: the Perspective Adaptation
Module, which dynamically selects viewpoints for query-relevant image
rendering, and the Fusion Alignment Module, which integrates 2D images with 3D
spatial descriptions to enhance object localization. Extensive experiments on
ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot
methods by large margins. Notably, we exceed weakly supervised methods and
rival some fully supervised ones, outperforming previous SOTA by 7.7% on
ScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.

</details>


### [375] [FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies](https://arxiv.org/pdf/2412.06708)
*Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, Wei Tsang Ooi*

Main category: cs.CV

TL;DR: FlexEvent is a novel framework for event-based object detection that adapts to varying frequencies, outperforming existing methods by integrating high-frequency event data with RGB frames and fine-tuning dynamically.


<details>
  <summary>Details</summary>
Motivation: Existing event detectors are limited by fixed-frequency paradigms and fail to leverage the high-temporal resolution of event cameras, necessitating a more adaptable solution.

Method: FlexEvent combines FlexFuse (adaptive event-frame fusion) and FlexTune (frequency-adaptive fine-tuning) to integrate event data with RGB frames and adjust labels for varying frequencies.

Result: The framework achieves high accuracy in both fast-moving and static scenarios, scaling robustly from 20 Hz to 180 Hz, surpassing state-of-the-art methods.

Conclusion: FlexEvent sets a new benchmark for event-based detection, enabling adaptable, real-time vision systems.

Abstract: Event cameras offer unparalleled advantages for real-time perception in
dynamic environments, thanks to the microsecond-level temporal resolution and
asynchronous operation. Existing event detectors, however, are limited by
fixed-frequency paradigms and fail to fully exploit the high-temporal
resolution and adaptability of event data. To address these limitations, we
propose FlexEvent, a novel framework that enables detection at varying
frequencies. Our approach consists of two key components: FlexFuse, an adaptive
event-frame fusion module that integrates high-frequency event data with rich
semantic information from RGB frames, and FlexTune, a frequency-adaptive
fine-tuning mechanism that generates frequency-adjusted labels to enhance model
generalization across varying operational frequencies. This combination allows
our method to detect objects with high accuracy in both fast-moving and static
scenarios, while adapting to dynamic environments. Extensive experiments on
large-scale event camera datasets demonstrate that our approach surpasses
state-of-the-art methods, achieving significant improvements in both standard
and high-frequency settings. Notably, our method maintains robust performance
when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz,
proving its effectiveness in extreme conditions. Our framework sets a new
benchmark for event-based object detection and paves the way for more
adaptable, real-time vision systems. Code is publicly available.

</details>


### [376] [Do large language vision models understand 3D shapes?](https://arxiv.org/pdf/2412.10908)
*Sagi Eppel*

Main category: cs.CV

TL;DR: LVLMs show some understanding of 3D shapes but perform poorly compared to humans when both orientation and material/texture are changed.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LVLMs truly understand 3D shapes by testing their ability to match objects with varying orientations and materials.

Method: Created CGI test images with diverse objects, materials, and scenes to assess LVLMs' shape recognition.

Result: LVLMs perform better than random but significantly worse than humans, especially when both orientation and material change.

Conclusion: LVLMs have limited abstract understanding of 3D shapes, lagging behind human performance in complex scenarios.

Abstract: Large vision language models (LVLM) are the leading A.I approach for
achieving a general visual understanding of the world. Models such as GPT,
Claude, Gemini, and LLama can use images to understand and analyze complex
visual scenes. 3D objects and shapes are the basic building blocks of the
world, recognizing them is a fundamental part of human perception. The goal of
this work is to test whether LVLMs truly understand 3D shapes by testing the
models ability to identify and match objects of the exact same 3D shapes but
with different orientations and materials/textures. A large number of test
images were created using CGI with a huge number of highly diverse objects,
materials, and scenes. The results of this test show that the ability of such
models to match 3D shapes is significantly below humans but much higher than
random guesses. Suggesting that the models have gained some abstract
understanding of 3D shapes but still trail far beyond humans in this task.
Mainly it seems that the models can easily identify the same object with a
different orientation as well as matching identical 3D shapes of the same
orientation but with different materials and textures. However, when both the
object material and orientation are changed, all models perform poorly relative
to humans. Code and benchmark are available.

</details>


### [377] [Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage](https://arxiv.org/pdf/2412.15484)
*Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, Sungroh Yoon*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in multimodal large language models (MLLMs) during detailed caption generation, proposing a multiagent LLM-MLLM collaboration to correct captions and introducing a new evaluation framework and benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods struggle with detailed captions due to MLLMs' increasing reliance on generated text over input images as sequence length grows.

Method: A multiagent approach leveraging LLM-MLLM collaboration is proposed to correct captions, alongside a new evaluation framework and benchmark dataset.

Result: The proposed method outperforms existing metrics in aligning with human judgments of factuality and significantly enhances caption accuracy, even for GPT-4V.

Conclusion: The paper highlights limitations of VQA-centric benchmarking and demonstrates the effectiveness of the proposed method in improving factual accuracy for detailed captions.

Abstract: Multimodal large language models (MLLMs) excel at generating highly detailed
captions but often produce hallucinations. Our analysis reveals that existing
hallucination detection methods struggle with detailed captions. We attribute
this to the increasing reliance of MLLMs on their generated text, rather than
the input image, as the sequence length grows. To address this issue, we
propose a multiagent approach that leverages LLM-MLLM collaboration to correct
given captions. Additionally, we introduce an evaluation framework and a
benchmark dataset to facilitate the systematic analysis of detailed captions.
Our experiments demonstrate that our proposed evaluation method better aligns
with human judgments of factuality than existing metrics and that existing
approaches to improve the MLLM factuality may fall short in hyper-detailed
image captioning tasks. In contrast, our proposed method significantly enhances
the factual accuracy of captions, even improving those generated by GPT-4V.
Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating
that an MLLM's performance on VQA benchmarks may not correlate with its ability
to generate detailed image captions.

</details>


### [378] [Robustness-enhanced Myoelectric Control with GAN-based Open-set Recognition](https://arxiv.org/pdf/2412.15819)
*Cheng Wang, Ziyang Feng, Pin Zhang, Manjiang Cao, Yiming Yuan, Tengfei Chang*

Main category: cs.CV

TL;DR: A GAN-based framework improves EMG signal recognition by enabling open-set recognition, achieving 97.6% accuracy for known actions and reducing errors for unknown ones.


<details>
  <summary>Details</summary>
Motivation: EMG signals are prone to variability and noise, and existing systems struggle with unfamiliar actions, leading to instability.

Method: Uses a GAN-based discriminator to identify and reject unknown actions, enhancing system robustness.

Result: Achieves 97.6% accuracy for known actions and a 23.6% improvement in Active Error Rate for unknown actions.

Conclusion: The method is efficient, robust, and suitable for edge devices, making it practical for real-world use.

Abstract: Electromyography (EMG) signals are widely used in human motion recognition
and medical rehabilitation, yet their variability and susceptibility to noise
significantly limit the reliability of myoelectric control systems. Existing
recognition algorithms often fail to handle unfamiliar actions effectively,
leading to system instability and errors. This paper proposes a novel framework
based on Generative Adversarial Networks (GANs) to enhance the robustness and
usability of myoelectric control systems by enabling open-set recognition. The
method incorporates a GAN-based discriminator to identify and reject unknown
actions, maintaining system stability by preventing misclassifications.
Experimental evaluations on publicly available and self-collected datasets
demonstrate a recognition accuracy of 97.6\% for known actions and a 23.6\%
improvement in Active Error Rate (AER) after rejecting unknown actions. The
proposed approach is computationally efficient and suitable for deployment on
edge devices, making it practical for real-world applications.

</details>


### [379] [SafeCFG: Controlling Harmful Features with Dynamic Safe Guidance for Safe Generation](https://arxiv.org/pdf/2412.16039)
*Jiadong Pan, Liang Li, Hongcheng Gao, Zheng-Jun Zha, Qingming Huang, Jiebo Luo*

Main category: cs.CV

TL;DR: SafeCFG improves diffusion models by dynamically controlling harmful image generation while maintaining quality, using adaptive guidance and unsupervised harmfulness detection.


<details>
  <summary>Details</summary>
Motivation: Existing safe alignment methods reduce image quality when mitigating harmful image generation. SafeCFG aims to address this by dynamically guiding the generation process.

Method: SafeCFG modulates classifier-free guidance (CFG) adaptively based on prompt harmfulness, inducing deviations only in harmful generations. It also detects harmfulness without predefined labels.

Result: SafeCFG achieves high-quality and safe image generation, outperforming existing methods. Unsupervised training also shows good safety performance.

Conclusion: SafeCFG effectively balances quality and safety in diffusion models, offering a robust solution for harmful image generation.

Abstract: Diffusion models (DMs) have demonstrated exceptional performance in
text-to-image tasks, leading to their widespread use. With the introduction of
classifier-free guidance (CFG), the quality of images generated by DMs is
significantly improved. However, one can use DMs to generate more harmful
images by maliciously guiding the image generation process through CFG.
Existing safe alignment methods aim to mitigate the risk of generating harmful
images but often reduce the quality of clean image generation. To address this
issue, we propose SafeCFG to adaptively control harmful features with dynamic
safe guidance by modulating the CFG generation process. It dynamically guides
the CFG generation process based on the harmfulness of the prompts, inducing
significant deviations only in harmful CFG generations, achieving high quality
and safety generation. SafeCFG can simultaneously modulate different harmful
CFG generation processes, so it could eliminate harmful elements while
preserving high-quality generation. Additionally, SafeCFG provides the ability
to detect image harmfulness, allowing unsupervised safe alignment on DMs
without pre-defined clean or harmful labels. Experimental results show that
images generated by SafeCFG achieve both high quality and safety, and safe DMs
trained in our unsupervised manner also exhibit good safety performance.

</details>


### [380] [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/pdf/2501.05874)
*Soyeong Jeong, Kangsan Kim, Jinheon Baek, Sung Ju Hwang*

Main category: cs.CV

TL;DR: VideoRAG enhances factual accuracy by dynamically retrieving and integrating relevant videos, leveraging both visual and textual information, and optimizing frame selection for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods focus on text or images, neglecting videos' rich multimodal context. VideoRAG addresses this gap by dynamically retrieving and processing videos.

Method: VideoRAG uses Large Video Language Models (LVLMs) to retrieve and process videos, with a frame selection mechanism and text extraction for videos lacking subtitles.

Result: VideoRAG outperforms baselines, demonstrating superior performance in leveraging multimodal video content.

Conclusion: VideoRAG effectively integrates videos into RAG, improving factual accuracy and multimodal understanding, with potential for broader applications.

Abstract: Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the
factual accuracy of models by retrieving external knowledge relevant to queries
and incorporating it into the generation process. However, existing approaches
primarily focus on text, with some recent advancements considering images, and
they largely overlook videos, a rich source of multimodal knowledge capable of
representing contextual details more effectively than any other modality. While
very recent studies explore the use of videos in response generation, they
either predefine query-associated videos without retrieval or convert videos
into textual descriptions losing multimodal richness. To tackle these, we
introduce VideoRAG, a framework that not only dynamically retrieves videos
based on their relevance with queries but also utilizes both visual and textual
information. The operation of VideoRAG is powered by recent Large Video
Language Models (LVLMs), which enable the direct processing of video content to
represent it for retrieval and the seamless integration of retrieved videos
jointly with queries for response generation. Also, inspired by that the
context size of LVLMs may not be sufficient to process all frames in extremely
long videos and not all frames are equally important, we introduce a video
frame selection mechanism to extract the most informative subset of frames,
along with a strategy to extract textual information from videos (as it can aid
the understanding of video content) when their subtitles are not available. We
experimentally validate the effectiveness of VideoRAG, showcasing that it is
superior to relevant baselines. Code is available at
https://github.com/starsuzi/VideoRAG.

</details>


### [381] [Benchmarking YOLOv8 for Optimal Crack Detection in Civil Infrastructure](https://arxiv.org/pdf/2501.06922)
*Woubishet Zewdu Taffese, Ritesh Sharma, Mohammad Hossein Afsharmovahed, Gunasekaran Manogaran, Genda Chen*

Main category: cs.CV

TL;DR: The study evaluates YOLOv8 for real-time bridge crack detection, optimizing it with various methods and finding Stochastic Gradient Descent delivers top performance.


<details>
  <summary>Details</summary>
Motivation: Traditional crack detection methods are slow; AI like YOLOv8 offers faster, more accurate solutions, but its potential is underexplored.

Method: YOLOv8 was tested across five model scales using a Roboflow dataset, with hyperparameter optimization involving six optimizers.

Result: YOLOv8 with Stochastic Gradient Descent achieved exceptional accuracy and speed, setting a new benchmark for real-time crack detection.

Conclusion: YOLOv8 is a foundational tool for infrastructure monitoring, enhancing bridge safety and maintenance efficiency globally.

Abstract: Ensuring the structural integrity and safety of bridges is crucial for the
reliability of transportation networks and public safety. Traditional crack
detection methods are increasingly being supplemented or replaced by advanced
artificial intelligence (AI) techniques. However, most of the models rely on
two-stage target detection algorithms, which pose concerns for real-time
applications due to their lower speed. While models such as YOLO (You Only Look
Once) have emerged as transformative tools due to their remarkable speed and
accuracy. However, the potential of the latest YOLOv8 framework in this domain
remains underexplored. This study bridges that gap by rigorously evaluating
YOLOv8's performance across five model scales (nano, small, medium, large, and
extra-large) using a high-quality Roboflow dataset. A comprehensive
hyperparameter optimization was performed, testing six state-of-the-art
optimizers-Stochastic Gradient Descent, Adaptive Moment Estimation, Adam with
Decoupled Weight Decay, Root Mean Square Propagation, Rectified Adam, and
Nesterov-accelerated Adam. Results revealed that YOLOv8, optimized with
Stochastic Gradient Descent, delivered exceptional accuracy and speed, setting
a new benchmark for real-time crack detection. Beyond its immediate
application, this research positions YOLOv8 as a foundational approach for
integrating advanced computer vision techniques into infrastructure monitoring.
By enabling more reliable and proactive maintenance of aging bridge networks,
this work paves the way for safer, more efficient transportation systems
worldwide.

</details>


### [382] [CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information Injection and Restricted Pseudo Labeling based Improved Semi-Supervised Few-Shot Learning](https://arxiv.org/pdf/2501.14401)
*Souvik Maji, Rhythm Baghel, Pratik Mazumder*

Main category: cs.CV

TL;DR: The paper proposes a method to improve semi-supervised few-shot learning by enhancing clustering and pseudo-labeling processes, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited labeled samples in few-shot learning by leveraging unlabeled data to improve model performance.

Method: Uses class-variance optimized clustering, a cluster separation tuner, restricted pseudo-labeling, and semantic information injection.

Result: Significantly outperforms recent state-of-the-art methods on benchmark datasets.

Conclusion: Improved clustering and pseudo-labeling enhance semi-supervised few-shot learning performance.

Abstract: Few-shot learning has been extensively explored to address problems where the
amount of labeled samples is very limited for some classes. In the
semi-supervised few-shot learning setting, substantial quantities of unlabeled
samples are available. Such unlabeled samples are generally cheaper to obtain
and can be used to improve the few-shot learning performance of the model. Some
of the recent methods for this setting rely on clustering to generate
pseudo-labels for the unlabeled samples. Since the effectiveness of clustering
heavily influences the labeling of the unlabeled samples, it can significantly
affect the few-shot learning performance. In this paper, we focus on improving
the representation learned by the model in order to improve the clustering and,
consequently, the model performance. We propose an approach for semi-supervised
few-shot learning that performs a class-variance optimized clustering coupled
with a cluster separation tuner in order to improve the effectiveness of
clustering the labeled and unlabeled samples in this setting. It also optimizes
the clustering-based pseudo-labeling process using a restricted pseudo-labeling
approach and performs semantic information injection in order to improve the
semi-supervised few-shot learning performance of the model. We experimentally
demonstrate that our proposed approach significantly outperforms recent
state-of-the-art methods on the benchmark datasets.

</details>


### [383] [A Benchmark and Evaluation for Real-World Out-of-Distribution Detection Using Vision-Language Models](https://arxiv.org/pdf/2501.18463)
*Shiho Noda, Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: The paper introduces three new OOD detection benchmarks (ImageNet-X, ImageNet-FS-X, Wilds-FS-X) to address performance saturation in conventional benchmarks, revealing limitations of CLIP-based methods and advocating for more realistic testing conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional OOD detection benchmarks have reached performance saturation, making it hard to compare recent methods. The authors aim to provide more challenging and realistic benchmarks.

Method: Three new benchmarks are proposed: ImageNet-X (semantic shifts), ImageNet-FS-X (covariate shifts), and Wilds-FS-X (real-world datasets). Experiments evaluate CLIP-based methods on these benchmarks.

Result: CLIP-based OOD detection methods struggle across the new benchmarks, with no method consistently outperforming others.

Conclusion: The paper calls for the community to adopt more challenging benchmarks reflecting real-world conditions, beyond conventional testing.

Abstract: Out-of-distribution (OOD) detection is a task that detects OOD samples during
inference to ensure the safety of deployed models. However, conventional
benchmarks have reached performance saturation, making it difficult to compare
recent OOD detection methods. To address this challenge, we introduce three
novel OOD detection benchmarks that enable a deeper understanding of method
characteristics and reflect real-world conditions. First, we present
ImageNet-X, designed to evaluate performance under challenging semantic shifts.
Second, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing
robustness to covariate shifts (feature distribution shifts). Finally, we
propose Wilds-FS-X, which extends these evaluations to real-world datasets,
offering a more comprehensive testbed. Our experiments reveal that recent
CLIP-based OOD detection methods struggle to varying degrees across the three
proposed benchmarks, and none of them consistently outperforms the others. We
hope the community goes beyond specific benchmarks and includes more
challenging conditions reflecting real-world scenarios. The code is
https://github.com/hoshi23/OOD-X-Benchmarks.

</details>


### [384] [Demystifying Catastrophic Forgetting in Two-Stage Incremental Object Detector](https://arxiv.org/pdf/2502.05540)
*Qirui Wu, Shizhou Zhang, De Cheng, Yinghui Xing, Di Xu, Peng Wang, Yanning Zhang*

Main category: cs.CV

TL;DR: The paper addresses catastrophic forgetting in incremental object detection (IOD) by focusing on the RoI Head classifier, proposing NSGP-RePRE, which uses prototype replay and gradient projection to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in IOD is primarily localized to the RoI Head classifier, challenging traditional monolithic approaches.

Method: NSGP-RePRE combines Regional Prototype Replay (RePRE) for classifier forgetting and Null Space Gradient Projection (NSGP) to align feature updates with incremental learning.

Result: Achieves state-of-the-art performance on Pascal VOC and MS COCO datasets.

Conclusion: The framework advances IOD methodology and provides insights for mitigating catastrophic forgetting.

Abstract: Catastrophic forgetting is a critical chanllenge for incremental object
detection (IOD). Most existing methods treat the detector monolithically,
relying on instance replay or knowledge distillation without analyzing
component-specific forgetting. Through dissection of Faster R-CNN, we reveal a
key insight: Catastrophic forgetting is predominantly localized to the RoI Head
classifier, while regressors retain robustness across incremental stages. This
finding challenges conventional assumptions, motivating us to develop a
framework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates
classifier forgetting via replay of two types of prototypes: coarse prototypes
represent class-wise semantic centers of RoI features, while fine-grained
prototypes model intra-class variations. Null Space Gradient Projection (NSGP)
is further introduced to eliminate prototype-feature misalignment by updating
the feature extractor in directions orthogonal to subspace of old inputs via
gradient projection, aligning RePRE with incremental learning dynamics. Our
simple yet effective design allows NSGP-RePRE to achieve state-of-the-art
performance on the Pascal VOC and MS COCO datasets under various settings. Our
work not only advances IOD methodology but also provide pivotal insights for
catastrophic forgetting mitigation in IOD. Code is available at
\href{https://github.com/fanrena/NSGP-RePRE}{https://github.com/fanrena/NSGP-RePRE} .

</details>


### [385] [Weight Space Representation Learning on Diverse NeRF Architectures](https://arxiv.org/pdf/2502.09623)
*Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano*

Main category: cs.CV

TL;DR: A framework for processing diverse Neural Radiance Fields (NeRFs) architectures, including unseen ones, using a Graph Meta-Network and contrastive learning, outperforming single-architecture methods.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for NeRFs require predefined architectures, limiting flexibility. This work aims to enable inference on diverse and unseen NeRF architectures.

Method: Train a Graph Meta-Network with unsupervised representation learning and a contrastive objective to create an architecture-agnostic latent space.

Result: Robust performance in classification and retrieval across 13 NeRF architectures, including unseen ones, surpassing single-architecture frameworks.

Conclusion: The framework successfully generalizes to diverse NeRF architectures, demonstrating the effectiveness of the proposed approach.

Abstract: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for
representing 3D objects and scenes by encoding shape and appearance information
into the weights of a neural network. Recent studies have demonstrated that
these weights can be used as input for frameworks designed to address deep
learning tasks; however, such frameworks require NeRFs to adhere to a specific,
predefined architecture. In this paper, we introduce the first framework
capable of processing NeRFs with diverse architectures and performing inference
on architectures unseen at training time. We achieve this by training a Graph
Meta-Network within an unsupervised representation learning framework, and show
that a contrastive objective is conducive to obtaining an architecture-agnostic
latent space. In experiments conducted across 13 NeRF architectures belonging
to three families (MLPs, tri-planes, and, for the first time, hash tables), our
approach demonstrates robust performance in classification and retrieval tasks
involving multiple architectures, even unseen at training time, while also
exceeding the results of existing frameworks limited to single architectures.

</details>


### [386] [Safeguarding AI in Medical Imaging: Post-Hoc Out-of-Distribution Detection with Normalizing Flows](https://arxiv.org/pdf/2502.11638)
*Dariush Lotfi, Mohammad-Ali Nikouei Mahani, Mohamad Koohi-Moghadam, Kyongtae Ty Bae*

Main category: cs.CV

TL;DR: A post-hoc normalizing flow-based method for OOD detection in medical imaging, outperforming state-of-the-art methods without altering pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Addressing the risk of undetected OOD data in medical imaging, which can lead to critical diagnostic errors, without requiring impractical retraining or model modifications.

Method: Proposes a post-hoc normalizing flow-based approach that integrates with pre-trained models without changing their weights, evaluated on MedOOD and MedMNIST datasets.

Result: Achieved AUROC of 84.61% on MedOOD and 93.8% on MedMNIST, surpassing ViM, MDS, and ReAct.

Conclusion: The method enhances safety in medical imaging workflows with superior performance and seamless integration, making it a practical solution for clinical environments.

Abstract: In AI-driven medical imaging, the failure to detect out-of-distribution (OOD)
data poses a severe risk to clinical reliability, potentially leading to
critical diagnostic errors. Current OOD detection methods often demand
impractical retraining or modifications to pre-trained models, hindering their
adoption in regulated clinical environments. To address this challenge, we
propose a post-hoc normalizing flow-based approach that seamlessly integrates
with existing pre-trained models without altering their weights. Our evaluation
used a novel in-house built dataset, MedOOD, meticulously curated to simulate
clinically relevant distributional shifts, alongside the MedMNIST benchmark
dataset. On our in-house MedOOD dataset, our method achieved an AUROC of
84.61%, outperforming state-of-the-art methods like ViM (80.65%) and MDS
(80.87%). Similarly, on MedMNIST, it reached an exceptional AUROC of 93.8%,
surpassing leading approaches such as ViM (88.08%) and ReAct (87.05%). This
superior performance, coupled with its post-hoc integration capability,
positions our method as a vital safeguard for enhancing safety in medical
imaging workflows. The model and code to build OOD datasets are publicly
accessible at https://github.com/dlotfi/MedOODFlow.

</details>


### [387] [Differential Coding for Training-Free ANN-to-SNN Conversion](https://arxiv.org/pdf/2503.00301)
*Zihan Huang, Wei Fang, Tong Bu, Peng Xue, Zecheng Hao, Wenxuan Liu, Yuanhong Tang, Zhaofei Yu, Tiejun Huang*

Main category: cs.CV

TL;DR: The paper introduces differential coding for ANN-to-SNN conversion, reducing spike counts and energy consumption, and proposes a threshold iteration method for optimizing ReLU conversions.


<details>
  <summary>Details</summary>
Motivation: Current ANN-to-SNN conversion methods rely on rate coding, which increases energy and latency due to high spike counts and long time-steps.

Method: The paper proposes differential coding to transmit rate changes instead of rates directly and introduces a threshold iteration method for ReLU conversion.

Result: Experiments on CNNs and Transformers show improved accuracy and reduced energy consumption, achieving state-of-the-art performance.

Conclusion: Differential coding and threshold iteration enhance ANN-to-SNN conversion efficiency, offering a promising approach for low-energy SNNs.

Abstract: Spiking Neural Networks (SNNs) exhibit significant potential due to their low
energy consumption. Converting Artificial Neural Networks (ANNs) to SNNs is an
efficient way to achieve high-performance SNNs. However, many conversion
methods are based on rate coding, which requires numerous spikes and longer
time-steps compared to directly trained SNNs, leading to increased energy
consumption and latency. This article introduces differential coding for
ANN-to-SNN conversion, a novel coding scheme that reduces spike counts and
energy consumption by transmitting changes in rate information rather than
rates directly, and explores its application across various layers.
Additionally, the threshold iteration method is proposed to optimize thresholds
based on activation distribution when converting Rectified Linear Units (ReLUs)
to spiking neurons. Experimental results on various Convolutional Neural
Networks (CNNs) and Transformers demonstrate that the proposed differential
coding significantly improves accuracy while reducing energy consumption,
particularly when combined with the threshold iteration method, achieving
state-of-the-art performance. The source codes of the proposed method are
available at https://github.com/h-z-h-cell/ANN-to-SNN-DCGS.

</details>


### [388] [Aligning Text to Image in Diffusion Models is Easier Than You Think](https://arxiv.org/pdf/2503.08250)
*Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, Jong Chul Ye*

Main category: cs.CV

TL;DR: The paper proposes SoftREPA, a lightweight contrastive fine-tuning method to improve text-image alignment in generative models by leveraging existing datasets as positive and negative pairs.


<details>
  <summary>Details</summary>
Motivation: Address residual misalignment in text-image representations, avoiding the need for tailored datasets by revisiting representation alignment.

Method: Introduces SoftREPA, a contrastive learning approach using soft text tokens for efficient alignment with pretrained models, adding minimal parameters.

Result: Theoretical and experimental results show improved semantic consistency in text-to-image generation and editing tasks.

Conclusion: SoftREPA effectively enhances text-image alignment with low computational overhead.

Abstract: While recent advancements in generative modeling have significantly improved
text-image alignment, some residual misalignment between text and image
representations still remains. Some approaches address this issue by
fine-tuning models in terms of preference optimization, etc., which require
tailored datasets. Orthogonal to these methods, we revisit the challenge from
the perspective of representation alignment-an approach that has gained
popularity with the success of REPresentation Alignment (REPA). We first argue
that conventional text-to-image (T2I) diffusion models, typically trained on
paired image and text data (i.e., positive pairs) by minimizing score matching
or flow matching losses, is suboptimal from the standpoint of representation
alignment. Instead, a better alignment can be achieved through contrastive
learning that leverages existing dataset as both positive and negative pairs.
To enable efficient alignment with pretrained models, we propose SoftREPA- a
lightweight contrastive fine-tuning strategy that leverages soft text tokens
for representation alignment. This approach improves alignment with minimal
computational overhead by adding fewer than 1M trainable parameters to the
pretrained model. Our theoretical analysis demonstrates that our method
explicitly increases the mutual information between text and image
representations, leading to enhanced semantic consistency. Experimental results
across text-to-image generation and text-guided image editing tasks validate
the effectiveness of our approach in improving the semantic consistency of T2I
generative models.

</details>


### [389] [From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/pdf/2503.12821)
*Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng*

Main category: cs.CV

TL;DR: The paper addresses long-tail data imbalance in Large Vision-Language Models (LVLMs) by proposing an Adaptive Data Refinement Framework (ADR) with two stages: Data Rebalancing and Data Synthesis, improving model performance.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from long-tail data imbalance, where head concepts are overrepresented and tail concepts underrepresented, limiting performance in tasks like Visual Question Answering.

Method: ADR framework includes Data Rebalancing (DR) to adjust entity distributions and Data Synthesis (DS) using DDPMs to supplement scarce data.

Result: ADR improves LLaVA 1.5's performance by 4.36% across eleven benchmarks without additional training data.

Conclusion: ADR effectively mitigates long-tail issues in LVLMs, enhancing performance for underrepresented concepts.

Abstract: Large Vision-Language Models (LVLMs) have achieved significant progress in
combining visual comprehension with language generation. Despite this success,
the training data of LVLMs still suffers from Long-Tail (LT) problems, where
the data distribution is highly imbalanced. Previous works have mainly focused
on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as
recognition and classification. Nevertheless, the exploration of LVLM (e.g.
LLaVA) and more general tasks (e.g. Visual Question Answering and Visual
Reasoning) remains under-explored. In this paper, we first conduct an in-depth
analysis of the LT issues in LVLMs and identify two core causes: the
overrepresentation of head concepts and the underrepresentation of tail
concepts. Based on the above observation, we propose an $\textbf{A}$daptive
$\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which
consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$)
and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we
adaptively rebalance the redundant data based on entity distributions, while in
the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and
scarce images to supplement underrepresented portions. Through comprehensive
evaluations across eleven benchmarks, our proposed ADR effectively mitigates
the long-tail problem in the training data, improving the average performance
of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.

</details>


### [390] [Position: Interactive Generative Video as Next-Generation Game Engine](https://arxiv.org/pdf/2503.17359)
*Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu*

Main category: cs.CV

TL;DR: Proposes Interactive Generative Video (IGV) for Generative Game Engines (GGE) to revolutionize game development with AI-generated content.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in creativity and cost in traditional game engines by leveraging AI for unlimited, dynamic content.

Method: Introduces IGV for GGE, emphasizing high-quality synthesis, physics-aware modeling, interactivity, memory, and reasoning.

Result: Presents a framework and roadmap (L0-L4) for GGE's evolution, aiming to reshape game creation.

Conclusion: Envisions AI-powered generative systems transforming game development and player experiences.

Abstract: Modern game development faces significant challenges in creativity and cost
due to predetermined content in traditional game engines. Recent breakthroughs
in video generation models, capable of synthesizing realistic and interactive
virtual environments, present an opportunity to revolutionize game creation. In
this position paper, we propose Interactive Generative Video (IGV) as the
foundation for Generative Game Engines (GGE), enabling unlimited novel content
generation in next-generation gaming. GGE leverages IGV's unique strengths in
unlimited high-quality content synthesis, physics-aware world modeling,
user-controlled interactivity, long-term memory capabilities, and causal
reasoning. We present a comprehensive framework detailing GGE's core modules
and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work
charts a new course for game development in the AI era, envisioning a future
where AI-powered generative systems fundamentally reshape how games are created
and experienced.

</details>


### [391] [RefCut: Interactive Segmentation with Reference Guidance](https://arxiv.org/pdf/2503.17820)
*Zheng Lin, Nan Zhou, Chen-Xi Du, Deng-Ping Fan, Shi-Min Hu*

Main category: cs.CV

TL;DR: RefCut is a reference-based interactive segmentation framework addressing part and object ambiguity, reducing user interaction burden and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack intuitive guidance, leading to unstable results and inefficiency in large-scale annotation.

Method: RefCut uses reference images and masks to optimize the model, reducing user interaction. A new Target Disassembly Dataset is introduced for evaluation.

Result: RefCut achieves state-of-the-art performance in evaluations and demonstrates intuitive, controllable segmentation.

Conclusion: RefCut advances interactive segmentation by addressing ambiguity and improving efficiency, with code and demo available.

Abstract: Interactive segmentation aims to segment the specified target on the image
with positive and negative clicks from users. Interactive ambiguity is a
crucial issue in this field, which refers to the possibility of multiple
compliant outcomes with the same clicks, such as selecting a part of an object
versus the entire object, a single object versus a combination of multiple
objects, and so on. The existing methods cannot provide intuitive guidance to
the model, which leads to unstable output results and makes it difficult to
meet the large-scale and efficient annotation requirements for specific targets
in some scenarios. To bridge this gap, we introduce RefCut, a reference-based
interactive segmentation framework designed to address part ambiguity and
object ambiguity in segmenting specific targets. Users only need to provide a
reference image and corresponding reference masks, and the model will be
optimized based on them, which greatly reduces the interactive burden on users
when annotating a large number of such targets. In addition, to enrich these
two kinds of ambiguous data, we propose a new Target Disassembly Dataset which
contains two subsets of part disassembly and object disassembly for evaluation.
In the combination evaluation of multiple datasets, our RefCut achieved
state-of-the-art performance. Extensive experiments and visualized results
demonstrate that RefCut advances the field of intuitive and controllable
interactive segmentation. Our code will be publicly available and the demo
video is in https://www.lin-zheng.com/refcut.

</details>


### [392] [Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage](https://arxiv.org/pdf/2503.19486)
*Zhengwentai Sun, Chenghong Li, Hongjie Liao, Xihe Yang, Keru Zheng, Heyuan Li, Yihao Zhi, Shuliang Ning, Shuguang Cui, Xiaoguang Han*

Main category: cs.CV

TL;DR: A stage-by-stage framework improves controllability in human image synthesis by disentangling and manipulating viewpoint, pose, clothing, and identity, outperforming end-to-end models.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained control over key factors like viewpoint, pose, clothing, and identity in human image synthesis. The domain gap between datasets further complicates disentanglement.

Method: Proposes a three-step pipeline: clothed A-pose generation, back-view synthesis, and pose/view control, leveraging MVHumanNet and VTON datasets separately to avoid inconsistency.

Result: The stage-by-stage approach outperforms end-to-end models in visual fidelity and disentanglement quality, especially for in-the-wild scenarios.

Conclusion: The structured pipeline offers a scalable solution for real-world tasks, improving generalization and controllability in human image synthesis.

Abstract: Achieving fine-grained controllability in human image synthesis is a
long-standing challenge in computer vision. Existing methods primarily focus on
either facial synthesis or near-frontal body generation, with limited ability
to simultaneously control key factors such as viewpoint, pose, clothing, and
identity in a disentangled manner. In this paper, we introduce a new
disentangled and controllable human synthesis task, which explicitly separates
and manipulates these four factors within a unified framework. We first develop
an end-to-end generative model trained on MVHumanNet for factor
disentanglement. However, the domain gap between MVHumanNet and in-the-wild
data produces unsatisfactory results, motivating the exploration of virtual
try-on (VTON) dataset as a potential solution. Through experiments, we observe
that simply incorporating the VTON dataset as additional data to train the
end-to-end model degrades performance, primarily due to the inconsistency in
data forms between the two datasets, which disrupts the disentanglement
process. To better leverage both datasets, we propose a stage-by-stage
framework that decomposes human image generation into three sequential steps:
clothed A-pose generation, back-view synthesis, and pose and view control. This
structured pipeline enables better dataset utilization at different stages,
significantly improving controllability and generalization, especially for
in-the-wild scenarios. Extensive experiments demonstrate that our
stage-by-stage approach outperforms end-to-end models in both visual fidelity
and disentanglement quality, offering a scalable solution for real-world tasks.
Additional demos are available on the project page:
https://taited.github.io/discohuman-project/.

</details>


### [393] [Dataset Distillation of 3D Point Clouds via Distribution Matching](https://arxiv.org/pdf/2503.22154)
*Jae-Young Yim, Dongwook Kim, Jae-Young Sim*

Main category: cs.CV

TL;DR: A novel dataset distillation method for 3D point clouds is proposed, addressing challenges like unordered points and rotation variations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale datasets for training deep neural networks are computationally expensive, and dataset distillation for 3D point clouds is underexplored due to their unique characteristics.

Method: A distribution matching-based framework optimizes geometric structures and orientations of synthetic 3D objects, using a Semantically Aligned Distribution Matching loss and jointly learning rotation angles.

Result: The method outperforms existing dataset distillation techniques, achieving higher accuracy and strong cross-architecture generalization.

Conclusion: The proposed framework effectively distills 3D point cloud datasets, reducing computational burden while preserving essential information.

Abstract: Large-scale datasets are usually required to train deep neural networks, but
it increases the computational complexity hindering the practical applications.
Recently, dataset distillation for images and texts has been attracting a lot
of attention, that reduces the original dataset to a synthetic dataset to
alleviate the computational burden of training while preserving essential
task-relevant information. However, the dataset distillation for 3D point
clouds remains largely unexplored, as the point clouds exhibit fundamentally
different characteristics from that of images, making the dataset distillation
more challenging. In this paper, we propose a distribution matching-based
distillation framework for 3D point clouds that jointly optimizes the geometric
structures as well as the orientations of the synthetic 3D objects. To address
the semantic misalignment caused by unordered indexing of points, we introduce
a Semantically Aligned Distribution Matching loss computed on the sorted
features in each channel. Moreover, to address the rotation variation, we
jointly learn the optimal rotation angles while updating the synthetic dataset
to better align with the original feature distribution. Extensive experiments
on widely used benchmark datasets demonstrate that the proposed method
consistently outperforms existing dataset distillation methods, achieving
superior accuracy and strong cross-architecture generalization.

</details>


### [394] [Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion](https://arxiv.org/pdf/2503.22622)
*Jangho Park, Taesung Kwon, Jong Chul Ye*

Main category: cs.CV

TL;DR: A training-free method for 4D video generation using off-the-shelf video diffusion models to create multi-view videos from a single input, ensuring spatio-temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in current 4D video generation methods, which rely on costly training or multiple models, by proposing a practical, training-free solution.

Method: 1) Synthesize key frames using a video diffusion model with depth-based warping for guidance. 2) Interpolate remaining frames to ensure full spatio-temporal coherence.

Result: Generates multi-view videos from a single input while maintaining structural and temporal consistency without additional training.

Conclusion: The method offers an efficient, training-free approach for 4D video generation, leveraging existing models to overcome computational and data limitations.

Abstract: Recently, multi-view or 4D video generation has emerged as a significant
research topic. Nonetheless, recent approaches to 4D generation still struggle
with fundamental limitations, as they primarily rely on harnessing multiple
video diffusion models with additional training or compute-intensive training
of a full 4D diffusion model with limited real-world 4D data and large
computational costs. To address these challenges, here we propose the first
training-free 4D video generation method that leverages the off-the-shelf video
diffusion models to generate multi-view videos from a single input video. Our
approach consists of two key steps: (1) By designating the edge frames in the
spatio-temporal sampling grid as key frames, we first synthesize them using a
video diffusion model, leveraging a depth-based warping technique for guidance.
This approach ensures structural consistency across the generated frames,
preserving spatial and temporal coherence. (2) We then interpolate the
remaining frames using a video diffusion model, constructing a fully populated
and temporally coherent sampling grid while preserving spatial and temporal
consistency. Through this approach, we extend a single video into a multi-view
video along novel camera trajectories while maintaining spatio-temporal
consistency. Our method is training-free and fully utilizes an off-the-shelf
video diffusion model, offering a practical and effective solution for
multi-view video generation.

</details>


### [395] [SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D Hand-Object Trajectories](https://arxiv.org/pdf/2503.22869)
*Alexey Gavryushin, Alexandros Delitzas, Luc Van Gool, Marc Pollefeys, Kaichun Mo, Xi Wang*

Main category: cs.CV

TL;DR: SIGHT introduces a task for generating realistic 3D hand-object interaction trajectories from a single image and language description, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Advancing robotic and AI systems by modeling hand-object interaction priors for effective physical world operation.

Method: SIGHT-Fusion, a diffusion-based model, retrieves similar 3D meshes and enforces geometric constraints via diffusion guidance.

Result: Superior performance in trajectory diversity, quality, and hand-object interaction metrics on HOI4D and H2O datasets.

Conclusion: SIGHT-Fusion effectively generates plausible hand-object trajectories, addressing limitations of prior work.

Abstract: When humans grasp an object, they naturally form trajectories in their minds
to manipulate it for specific tasks. Modeling hand-object interaction priors
holds significant potential to advance robotic and embodied AI systems in
learning to operate effectively within the physical world. We introduce SIGHT,
a novel task focused on generating realistic and physically plausible 3D
hand-object interaction trajectories from a single image and a brief
language-based task description. Prior work on hand-object trajectory
generation typically relies on textual input that lacks explicit grounding to
the target object, or assumes access to 3D object meshes, which are often
considerably more difficult to obtain than 2D images. We propose SIGHT-Fusion,
a novel diffusion-based image-text conditioned generative model that tackles
this task by retrieving the most similar 3D object mesh from a database and
enforcing geometric hand-object interaction constraints via a novel
inference-time diffusion guidance. We benchmark our model on the HOI4D and H2O
datasets, adapting relevant baselines for this novel task. Experiments
demonstrate our superior performance in the diversity and quality of generated
trajectories, as well as in hand-object interaction geometry metrics.

</details>


### [396] [ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025](https://arxiv.org/pdf/2503.23509)
*Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Jian-Fang Hu*

Main category: cs.CV

TL;DR: ReferDINO-Plus enhances RVOS by integrating SAM2 for better mask quality and object consistency, using a conditional mask fusion strategy, achieving 60.43 on MeViS.


<details>
  <summary>Details</summary>
Motivation: Improving RVOS performance for applications in video editing and human-agent interaction by leveraging foundational models.

Method: Incorporates SAM2 for mask quality and introduces a conditional mask fusion strategy to balance single and multi-object scenarios.

Result: Achieves 60.43 on MeViS test set, securing 2nd place in the CVPR 2025 challenge.

Conclusion: ReferDINO-Plus demonstrates enhanced performance in RVOS by combining ReferDINO and SAM2 effectively.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment target objects
throughout a video based on a text description. This task has attracted
increasing attention in the field of computer vision due to its promising
applications in video editing and human-agent interaction. Recently, ReferDINO
has demonstrated promising performance in this task by adapting object-level
vision-language knowledge from pretrained foundational image models. In this
report, we further enhance its capabilities by incorporating the advantages of
SAM2 in mask quality and object consistency. In addition, to effectively
balance performance between single-object and multi-object scenarios, we
introduce a conditional mask fusion strategy that adaptively fuses the masks
from ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43
\(\mathcal{J}\&\mathcal{F}\) on MeViS test set, securing 2nd place in the MeViS
PVUW challenge at CVPR 2025. The code is available at:
https://github.com/iSEE-Laboratory/ReferDINO-Plus.

</details>


### [397] [It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data](https://arxiv.org/pdf/2503.24129)
*Dominik Schnaus, Nikita Araslanov, Daniel Cremers*

Main category: cs.CV

TL;DR: The paper explores unsupervised matching of vision and language embeddings, showing feasibility and proposing a novel heuristic for solving the problem.


<details>
  <summary>Details</summary>
Motivation: To investigate if vision and language embeddings can be matched without parallel data, leveraging the platonic representation hypothesis.

Method: Formulates unsupervised matching as a quadratic assignment problem, introduces a heuristic solver, and tests it on various models and datasets.

Result: Demonstrates that unsupervised matching is feasible for many problem instances, with a proof-of-concept classifier achieving non-trivial accuracy.

Conclusion: Unsupervised matching of vision and language embeddings is possible, enabling annotation-free semantic knowledge embedding.

Abstract: The platonic representation hypothesis suggests that vision and language
embeddings become more homogeneous as model and dataset sizes increase. In
particular, pairwise distances within each modality become more similar. This
suggests that as foundation models mature, it may become possible to match
vision and language embeddings in a fully unsupervised fashion, i.e. without
parallel data. We present the first feasibility study, and investigate
conformity of existing vision and language foundation models in the context of
unsupervised, or "blind", matching. First, we formulate unsupervised matching
as a quadratic assignment problem and introduce a novel heuristic that
outperforms previous solvers. We also develop a technique to find optimal
matching problems, for which a non-trivial match is very likely. Second, we
conduct an extensive study deploying a range of vision and language models on
four datasets. Our analysis reveals that for many problem instances, vision and
language representations can be indeed matched without supervision. This
finding opens up the exciting possibility of embedding semantic knowledge into
other modalities virtually annotation-free. As a proof of concept, we showcase
an unsupervised classifier, which achieves non-trivial classification accuracy
without any image-text annotation.

</details>


### [398] [All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning](https://arxiv.org/pdf/2504.01396)
*Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao, Shouhong Ding, Xi Li*

Main category: cs.CV

TL;DR: The paper introduces Panoptic Patch Learning (PPL) to address Few-Patch Bias in AI-generated image detection by leveraging all patches and ensuring uniform artifact utilization.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of AI-generated images necessitates robust detection methods, as current detectors often rely on minority patches due to Lazy Learner bias.

Method: Proposes PPL with Random Patch Replacement and Patch-wise Contrastive Learning to encourage broader patch usage and uniform artifact detection.

Result: Experiments on multiple benchmarks confirm PPL's effectiveness in improving detection robustness and generalization.

Conclusion: PPL successfully mitigates Few-Patch Bias, enhancing AI-generated image detection by utilizing all patches uniformly.

Abstract: The exponential growth of AI-generated images (AIGIs) underscores the urgent
need for robust and generalizable detection methods. In this paper, we
establish two key principles for AIGI detection through systematic analysis:
(1) All Patches Matter: Unlike conventional image classification where
discriminative features concentrate on object-centric regions, each patch in
AIGIs inherently contains synthetic artifacts due to the uniform generation
process, suggesting that every patch serves as an important artifact source for
detection. (2) More Patches Better: Leveraging distributed artifacts across
more patches improves detection robustness by capturing complementary forensic
evidence and reducing over-reliance on specific patches, thereby enhancing
robustness and generalization. However, our counterfactual analysis reveals an
undesirable phenomenon: naively trained detectors often exhibit a Few-Patch
Bias, discriminating between real and synthetic images based on minority
patches. We identify Lazy Learner as the root cause: detectors preferentially
learn conspicuous artifacts in limited patches while neglecting broader
artifact distributions. To address this bias, we propose the Panoptic Patch
Learning (PPL) framework, involving: (1) Random Patch Replacement that randomly
substitutes synthetic patches with real counterparts to compel models to
identify artifacts in underutilized regions, encouraging the broader use of
more patches; (2) Patch-wise Contrastive Learning that enforces consistent
discriminative capability across all patches, ensuring uniform utilization of
all patches. Extensive experiments across two different settings on several
benchmarks verify the effectiveness of our approach.

</details>


### [399] [UniViTAR: Unified Vision Transformer with Native Resolution](https://arxiv.org/pdf/2504.01792)
*Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma*

Main category: cs.CV

TL;DR: UniViTAR introduces a vision foundation model for unified visual modality and native resolution, improving spatial-contextual fidelity through architectural upgrades and progressive training.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitations of conventional Vision Transformers in handling variable visual data and native resolutions, aiming for systematic visual representation.

Method: Combines architectural upgrades, resolution curriculum learning, visual modality adaptation, and hybrid training with contrastive loss and feature distillation.

Result: Demonstrates effectiveness across model scales (0.3B to 1B) trained on public datasets.

Conclusion: UniViTAR successfully bridges gaps in visual representation and resolution adaptability, offering a robust framework for multimodal scenarios.

Abstract: Conventional Vision Transformer simplifies visual modeling by standardizing
input resolutions, often disregarding the variability of natural visual data
and compromising spatial-contextual fidelity. While preliminary explorations
have superficially investigated native resolution modeling, existing approaches
still lack systematic analysis from a visual representation perspective. To
bridge this gap, we introduce UniViTAR, a family of homogeneous vision
foundation models tailored for unified visual modality and native resolution
scenario in the era of multimodal. Our framework first conducts architectural
upgrades to the vanilla paradigm by integrating multiple advanced components.
Building upon these improvements, a progressive training paradigm is
introduced, which strategically combines two core mechanisms: (1) resolution
curriculum learning, transitioning from fixed-resolution pretraining to native
resolution tuning, thereby leveraging ViT's inherent adaptability to
variable-length sequences, and (2) visual modality adaptation via inter-batch
image-video switching, which balances computational efficiency with enhanced
temporal reasoning. In parallel, a hybrid training framework further synergizes
sigmoid-based contrastive loss with feature distillation from a frozen teacher
model, thereby accelerating early-stage convergence. Finally, trained
exclusively on public datasets, externsive experiments across multiple model
scales from 0.3B to 1B demonstrate its effectiveness.

</details>


### [400] [FreSca: Scaling in Frequency Space Enhances Diffusion Models](https://arxiv.org/pdf/2504.02154)
*Chao Huang, Susan Liang, Yunlong Tang, Jing Bi, Li Ma, Yapeng Tian, Chenliang Xu*

Main category: cs.CV

TL;DR: FreSca introduces a plug-and-play framework for fine-grained control in latent diffusion models by decomposing noise differences into low- and high-frequency components, enabling versatile applications without model retraining.


<details>
  <summary>Details</summary>
Motivation: Achieving disentangled control over global structures and fine details in latent diffusion models remains challenging.

Method: Analyzes frequency characteristics in LDMs, identifies noise difference as a manipulable target, and introduces FreSca to independently scale low- and high-frequency components.

Result: FreSca improves generation quality and structural emphasis across tasks like image generation, editing, and video synthesis.

Conclusion: FreSca unlocks expressive control in LDMs without requiring retraining, offering broad applicability.

Abstract: Latent diffusion models (LDMs) have achieved remarkable success in a variety
of image tasks, yet achieving fine-grained, disentangled control over global
structures versus fine details remains challenging. This paper explores
frequency-based control within latent diffusion models. We first systematically
analyze frequency characteristics across pixel space, VAE latent space, and
internal LDM representations. This reveals that the "noise difference" term,
derived from classifier-free guidance at each step t, is a uniquely effective
and semantically rich target for manipulation. Building on this insight, we
introduce FreSca, a novel and plug-and-play framework that decomposes noise
difference into low- and high-frequency components and applies independent
scaling factors to them via spatial or energy-based cutoffs. Essentially,
FreSca operates without any model retraining or architectural change, offering
model- and task-agnostic control. We demonstrate its versatility and
effectiveness in improving generation quality and structural emphasis on
multiple architectures (e.g., SD3, SDXL) and across applications including
image generation, editing, depth estimation, and video synthesis, thereby
unlocking a new dimension of expressive control within LDMs.

</details>


### [401] [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](https://arxiv.org/pdf/2504.07934)
*Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang*

Main category: cs.CV

TL;DR: ThinkLite-VL models achieve SoTA performance with fewer training samples using reinforcement fine-tuning (RFT) and Monte Carlo Tree Search (MCTS) for sample difficulty filtering.


<details>
  <summary>Details</summary>
Motivation: To improve visual reasoning models efficiently by identifying challenging samples that drive deeper reasoning without needing large datasets or knowledge distillation.

Method: Uses MCTS to measure sample difficulty, selects high-quality subsets (11k and 7.5k samples) for RFT on Qwen2.5-VL models.

Result: ThinkLite-VL-7B and ThinkLite-VL-72B outperform base models and larger competitors, achieving new SoTA scores (e.g., 75.1 and 79.7 on MathVista).

Conclusion: MCTS-guided difficulty filtering enables scalable, data-efficient self-improvement in multimodal reasoning.

Abstract: We introduce ThinkLite-VL, a family of visual reasoning models that achieve
state-of-the-art (SoTA) performance using an order of magnitude fewer training
samples, relying purely on reinforcement fine-tuning (RFT) self-improvement
without any knowledge distillation. Our central insight is that sample
difficulty critically influences RFT effectiveness: appropriately challenging
examples can drive substantial reasoning improvements, even in low-data
regimes. However, quantifying sample difficulty in a reliable and scalable
manner remains non-trivial. To address this, we repurpose Monte Carlo Tree
Search (MCTS) to measure sample difficulty via the number of reasoning
iterations a vision-language model (VLM) requires to solve each instance. This
MCTS-based selection procedure identifies samples that induce deeper reasoning
while remaining solvable, allowing us to filter a high-quality subset from 70k
open-source examples spanning math, natural image understanding, and chart
comprehension. Using this approach, we select just 11k challenging samples for
RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The
resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly
outperform their respective base models across eight visual reasoning
benchmarks. In particular, ThinkLite-VL-7B improves the average performance of
Qwen2.5-VL-7B-Instruct by 7\% and surpasses all existing 7B-level models, as
well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a
new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA
frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark
improvement of 4.42 over the open-source SOTA. These results demonstrate that
MCTS-guided difficulty filtering provides a scalable and effective path toward
data-efficient self-improvement in multimodal reasoning.

</details>


### [402] [Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling](https://arxiv.org/pdf/2504.13169)
*Tsung-Han Wu, Heekyung Lee, Jiaxin Ge, Joseph E. Gonzalez, Trevor Darrell, David M. Chan*

Main category: cs.CV

TL;DR: REVERSE is a unified framework combining hallucination-aware training and self-verification to reduce visual hallucinations in Vision-Language Models (VLMs), outperforming existing methods by up to 34%.


<details>
  <summary>Details</summary>
Motivation: VLMs often generate descriptions of nonexistent objects or actions (hallucinations), posing risks in safety-critical applications. Existing methods are either heuristic-based or overly complex.

Method: REVERSE integrates hallucination-aware training with on-the-fly self-verification, using a 1.3M semi-synthetic dataset and a novel retrospective resampling technique.

Result: REVERSE reduces hallucinations by up to 12% on CHAIR-MSCOCO and 34% on HaloQuest, achieving state-of-the-art performance.

Conclusion: REVERSE effectively mitigates hallucinations in VLMs through a unified approach, offering a scalable and efficient solution.

Abstract: Vision-Language Models (VLMs) excel at visual understanding but often suffer
from visual hallucinations, where they generate descriptions of nonexistent
objects, actions, or concepts, posing significant risks in safety-critical
applications. Existing hallucination mitigation methods typically follow one of
two paradigms: generation adjustment, which modifies decoding behavior to align
text with visual inputs, and post-hoc verification, where external models
assess and correct outputs. While effective, generation adjustment methods
often rely on heuristics and lack correction mechanisms, while post-hoc
verification is complicated, typically requiring multiple models and tending to
reject outputs rather than refine them. In this work, we introduce REVERSE, a
unified framework that integrates hallucination-aware training with on-the-fly
self-verification. By leveraging a new hallucination-verification dataset
containing over 1.3M semi-synthetic samples, along with a novel inference-time
retrospective resampling technique, our approach enables VLMs to both detect
hallucinations during generation and dynamically revise those hallucinations.
Our evaluations show that REVERSE achieves state-of-the-art hallucination
reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO
and 34% on HaloQuest. Our dataset, model, and code are available at:
https://reverse-vlm.github.io.

</details>


### [403] [RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation](https://arxiv.org/pdf/2504.17502)
*Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin, Dani Lischinski, Idan Szpektor*

Main category: cs.CV

TL;DR: RefVNLI is a new metric for evaluating subject-driven T2I generation, addressing gaps in existing methods by assessing both textual alignment and subject preservation effectively.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for subject-driven T2I generation are limited, either focusing on one aspect, misaligning with human judgment, or being costly. RefVNLI aims to provide a reliable, cost-effective solution.

Method: RefVNLI is trained on a large-scale dataset from video-reasoning benchmarks and image perturbations to evaluate both textual alignment and subject preservation.

Result: RefVNLI outperforms or matches existing baselines, achieving significant gains (up to 6.4-point in textual alignment and 5.9-point in subject preservation) across benchmarks and subject categories.

Conclusion: RefVNLI is a robust, efficient metric for evaluating subject-driven T2I generation, addressing key limitations of prior methods.

Abstract: Subject-driven text-to-image (T2I) generation aims to produce images that
align with a given textual description, while preserving the visual identity
from a referenced subject image. Despite its broad downstream applicability -
ranging from enhanced personalization in image generation to consistent
character representation in video rendering - progress in this field is limited
by the lack of reliable automatic evaluation. Existing methods either assess
only one aspect of the task (i.e., textual alignment or subject preservation),
misalign with human judgments, or rely on costly API-based evaluation. To
address this gap, we introduce RefVNLI, a cost-effective metric that evaluates
both textual alignment and subject preservation in a single run. Trained on a
large-scale dataset derived from video-reasoning benchmarks and image
perturbations, RefVNLI outperforms or statistically matches existing baselines
across multiple benchmarks and subject categories (e.g., \emph{Animal},
\emph{Object}), achieving up to 6.4-point gains in textual alignment and
5.9-point gains in subject preservation.

</details>


### [404] [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation](https://arxiv.org/pdf/2504.21336)
*Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Luyang Luo, Neeraj Mahboobani, Varut Vardhanabhuti, Ronald Cheong Kin Chan, Yifan Peng, Pranav Rajpurkar, Hao Chen*

Main category: cs.CV

TL;DR: UniBiomed is a universal foundation model for biomedical image interpretation, combining diagnostic findings with object localization to improve clinician interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical AI models lack the ability to simultaneously generate diagnostic findings and localize biomedical objects, hindering clinician interpretation.

Method: UniBiomed integrates a Multi-modal Large Language Model and Segment Anything Model, trained on a large-scale dataset of 27 million triplets across ten imaging modalities.

Result: UniBiomed achieves state-of-the-art performance in tasks like segmentation, disease recognition, and report generation across 84 datasets.

Conclusion: UniBiomed enhances grounded interpretation in AI-assisted biomedical image analysis, offering accuracy and versatility.

Abstract: The integration of AI-assisted biomedical image analysis into clinical
practice demands AI-generated findings that are not only accurate but also
interpretable to clinicians. However, existing biomedical AI models generally
lack the ability to simultaneously generate diagnostic findings and localize
corresponding biomedical objects. This limitation makes it challenging for
clinicians to correlate AI-generated findings with visual evidence (e.g., tiny
lesions) in images and interpret the results of AI models. To address this
challenge, we introduce UniBiomed, the first universal foundation model for
grounded biomedical image interpretation, which is capable of generating
accurate diagnostic findings and simultaneously segmenting the corresponding
biomedical targets. UniBiomed is based on a novel integration of Multi-modal
Large Language Model and Segment Anything Model, which can effectively unify
diverse biomedical tasks in universal training for advancing grounded
interpretation. To develop UniBiomed, we curate a large-scale dataset
comprising over 27 million triplets of images, region annotations, and text
descriptions across ten biomedical imaging modalities. Extensive validation on
70 internal and 14 external datasets demonstrated the state-of-the-art
performance of UniBiomed in diverse biomedical tasks, including image
segmentation, disease recognition, region-aware diagnosis, vision question
answering, and report generation. In summary, UniBiomed is a powerful and
versatile biomedical foundation model, unlocking the untapped grounded
interpretation capability for optimizing AI-assisted biomedical image analysis.

</details>


### [405] [ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications](https://arxiv.org/pdf/2505.02179)
*Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu*

Main category: cs.CV

TL;DR: ProDisc-VAD improves weakly-supervised video anomaly detection by addressing label ambiguity with a prototype interaction layer and pseudo-instance discriminative enhancement, achieving high efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Label ambiguity in weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) hinders discriminative feature learning.

Method: ProDisc-VAD introduces a Prototype Interaction Layer (PIL) for controlled normality modeling and a Pseudo-Instance Discriminative Enhancement (PIDE) loss for targeted contrastive learning on extreme-scoring instances.

Result: Achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) with only 0.4M parameters, 800x fewer than ViT-based methods.

Conclusion: ProDisc-VAD offers exceptional efficiency and state-of-the-art performance in WS-VAD.

Abstract: Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance
Learning (MIL) suffers from label ambiguity, hindering discriminative feature
learning. We propose ProDisc-VAD, an efficient framework tackling this via two
synergistic components. The Prototype Interaction Layer (PIL) provides
controlled normality modeling using a small set of learnable prototypes,
establishing a robust baseline without being overwhelmed by dominant normal
data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts
separability by applying targeted contrastive learning exclusively to the most
reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD
achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M
parameters, over 800x fewer than recent ViT-based methods like VadCLIP,
demonstrating exceptional efficiency alongside state-of-the-art performance.
Code is available at https://github.com/modadundun/ProDisc-VAD.

</details>


### [406] [Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable](https://arxiv.org/pdf/2505.14359)
*Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding*

Main category: cs.CV

TL;DR: The paper addresses bias in detectors due to dataset misalignment, proposing Dual Data Alignment (DDA) to align pixel and frequency domains, improving generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing detectors overfit on biased datasets, relying on spurious correlations. Pixel-level alignment alone is insufficient, as frequency-level misalignment persists.

Method: Proposes DDA for dual-domain alignment (pixel and frequency) and introduces new test sets (DDA-COCO, EvalGEN) for evaluation.

Result: Detectors trained on DDA-aligned MSCOCO improve performance by +7.2% on in-the-wild benchmarks, showing better generalizability.

Conclusion: DDA effectively mitigates bias by addressing both pixel and frequency misalignment, enhancing detector performance across diverse benchmarks.

Abstract: Existing detectors are often trained on biased datasets, leading to the
possibility of overfitting on non-causal image attributes that are spuriously
correlated with real/synthetic labels. While these biased features enhance
performance on the training data, they result in substantial performance
degradation when applied to unbiased datasets. One common solution is to
perform dataset alignment through generative reconstruction, matching the
semantic content between real and synthetic images. However, we revisit this
approach and show that pixel-level alignment alone is insufficient. The
reconstructed images still suffer from frequency-level misalignment, which can
perpetuate spurious correlations. To illustrate, we observe that reconstruction
models tend to restore the high-frequency details lost in real images (possibly
due to JPEG compression), inadvertently creating a frequency-level
misalignment, where synthetic images appear to have richer high-frequency
content than real ones. This misalignment leads to models associating
high-frequency features with synthetic labels, further reinforcing biased cues.
To resolve this, we propose Dual Data Alignment (DDA), which aligns both the
pixel and frequency domains. Moreover, we introduce two new test sets:
DDA-COCO, containing DDA-aligned synthetic images for testing detector
performance on the most aligned dataset, and EvalGEN, featuring the latest
generative models for assessing detectors under new generative architectures
such as visual auto-regressive generators. Finally, our extensive evaluations
demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could
improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on
in-the-wild benchmarks, highlighting the improved generalizability of unbiased
detectors.

</details>


### [407] [Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection](https://arxiv.org/pdf/2505.16512)
*Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Zhaofeng He*

Main category: cs.CV

TL;DR: The paper introduces DigiFakeAV, a large-scale dataset for detecting diffusion-based deepfake videos, and proposes DigiShield, a detection model achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: The rise of diffusion-based digital human generation poses a serious threat to public security, challenging existing detection methods due to its realism and flexibility.

Method: The authors create DigiFakeAV using five digital human generation and voice cloning methods, producing 60,000 videos. They then propose DigiShield, a detection model leveraging spatiotemporal and cross-modal fusion.

Result: User studies show 68% misclassification of forged videos as real, and DigiShield achieves SOTA performance on DigiFakeAV with strong generalization.

Conclusion: DigiShield effectively addresses the challenges posed by diffusion-based deepfakes, offering a robust detection solution.

Abstract: In recent years, the rapid development of deepfake technology has given rise
to an emerging and serious threat to public security: diffusion-based digital
human generation. Unlike traditional face manipulation methods, such models can
generate highly realistic videos with consistency through multimodal control
signals. Their flexibility and covertness pose severe challenges to existing
detection strategies. To bridge this gap, we introduce DigiFakeAV, the new
large-scale multimodal digital human forgery dataset based on diffusion models.
Employing five of the latest digital human generation methods and the voice
cloning methods, we systematically produce a dataset comprising 60,000 videos
(8.4 million frames), covering multiple nationalities, skin tones, genders, and
real-world scenarios, significantly enhancing data diversity and realism. User
studies demonstrate that participants misclassify forged videos as real in 68%
of tests, and existing detection models exhibit a large drop in performance on
DigiFakeAV, highlighting the challenge of the dataset. To address this problem,
we propose DigiShield, an effective detection baseline based on spatiotemporal
and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of
videos and the semantic-acoustic features of audio, DigiShield achieves
state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong
generalization on other datasets.

</details>


### [408] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/pdf/2505.17473)
*Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench is introduced to improve object detection in charts and HROs for VLMs, featuring a large dataset and three applications.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack accurate visual grounding for charts and HROs, hindering chart understanding.

Method: OrionBench combines model-in-the-loop and programmatic methods to create a dataset with 105,000 infographics and 6.9M annotations.

Result: The benchmark aids in enhancing VLM performance, comparing detection models, and extending to layout/UI detection.

Conclusion: OrionBench addresses a critical gap in VLM capabilities for chart understanding and object detection.

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [409] [Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek](https://arxiv.org/pdf/2505.17702)
*Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou*

Main category: cs.CV

TL;DR: Seek-CAD introduces a training-free method using open-source LLM DeepSeek-R1 for CAD parametric model generation, incorporating visual and CoT feedback for refinement, and presents a novel 3D CAD dataset.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and deployment limitations of closed-source LLMs in CAD generative modeling by leveraging open-source alternatives and improving model generation through feedback mechanisms.

Method: Utilizes DeepSeek-R1 for initial CAD model generation, renders step-wise images, processes them with a VLM and CoT feedback, and refines the model iteratively. Introduces a 3D CAD dataset based on the SSR paradigm.

Result: Seek-CAD demonstrates effectiveness in generating CAD models through extensive experiments.

Conclusion: The study successfully pioneers a locally deployable, open-source LLM solution for CAD generative modeling, enhancing flexibility and efficiency.

Abstract: The advent of Computer-Aided Design (CAD) generative modeling will
significantly transform the design of industrial products. The recent research
endeavor has extended into the realm of Large Language Models (LLMs). In
contrast to fine-tuning methods, training-free approaches typically utilize the
advanced closed-source LLMs, thereby offering enhanced flexibility and
efficiency in the development of AI agents for generating CAD parametric
models. However, the substantial cost and limitations of local deployment of
the top-tier closed-source LLMs pose challenges in practical applications. The
Seek-CAD is the pioneer exploration of locally deployed open-source inference
LLM DeepSeek-R1 for CAD parametric model generation with a training-free
methodology. This study is the first investigation to incorporate both visual
and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for
generating CAD models. Specifically, the initial generated parametric CAD model
is rendered into a sequence of step-wise perspective images, which are
subsequently processed by a Vision Language Model (VLM) alongside the
corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.
Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated
model for the next round of generation. Moreover, we present an innovative 3D
CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and
Refinements) triple design paradigm. This dataset encompasses a wide range of
CAD commands, thereby aligning effectively with industrial application
requirements and proving suitable for the generation of LLMs. Extensive
experiments validate the effectiveness of Seek-CAD under various metrics.

</details>


### [410] [Diffusion Classifiers Understand Compositionality, but Conditions Apply](https://arxiv.org/pdf/2505.17955)
*Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach*

Main category: cs.CV

TL;DR: Diffusion classifiers show promise in compositional understanding but require further analysis of conditions like dataset domains and timestep weighting.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate the discriminative capabilities of diffusion models in compositional tasks, addressing gaps in prior work.

Method: Study three diffusion models (SD 1.5, 2.0, 3-m) across 10 datasets and 30+ tasks, introducing a diagnostic benchmark (Self-Bench) and analyzing timestep weighting.

Result: Diffusion classifiers understand compositionality, but performance depends on dataset domains and timestep sensitivity, especially for SD3-m.

Conclusion: Conditions like domain effects and timestep weighting significantly impact diffusion classifiers' discriminative performance in compositional tasks.

Abstract: Understanding visual scenes is fundamental to human intelligence. While
discriminative models have significantly advanced computer vision, they often
struggle with compositional understanding. In contrast, recent generative
text-to-image diffusion models excel at synthesizing complex scenes, suggesting
inherent compositional capabilities. Building on this, zero-shot diffusion
classifiers have been proposed to repurpose diffusion models for discriminative
tasks. While prior work offered promising results in discriminative
compositional scenarios, these results remain preliminary due to a small number
of benchmarks and a relatively shallow analysis of conditions under which the
models succeed. To address this, we present a comprehensive study of the
discriminative capabilities of diffusion classifiers on a wide range of
compositional tasks. Specifically, our study covers three diffusion models (SD
1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.
Further, we shed light on the role that target dataset domains play in
respective performance; to isolate the domain effects, we introduce a new
diagnostic benchmark Self-Bench comprised of images created by diffusion models
themselves. Finally, we explore the importance of timestep weighting and
uncover a relationship between domain gap and timestep sensitivity,
particularly for SD3-m. To sum up, diffusion classifiers understand
compositionality, but conditions apply! Code and dataset are available at
https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.

</details>


### [411] [ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations](https://arxiv.org/pdf/2505.18477)
*Fukun Liu, Adam T. Greer, Gengchen Mai, Jin Sun*

Main category: cs.CV

TL;DR: The paper introduces ZooplanktonBench, a benchmark dataset for detecting, classifying, and tracking zooplankton in challenging ocean environments, aiding marine science research.


<details>
  <summary>Details</summary>
Motivation: Zooplankton are vital indicators of ocean health, but analyzing them with general computer vision tools is difficult due to their similarity to backgrounds like marine snow.

Method: The authors present ZooplanktonBench, a dataset with images, videos, and geospatial metadata, defining tasks for detection, classification, and tracking in cluttered environments.

Result: The dataset provides unique challenges for improving computer vision in dynamic, geo-aware ocean settings.

Conclusion: ZooplanktonBench aims to advance marine research by enhancing visual understanding of zooplankton populations.

Abstract: Plankton are small drifting organisms found throughout the world's oceans and
can be indicators of ocean health. One component of this plankton community is
the zooplankton, which includes gelatinous animals and crustaceans (e.g.
shrimp), as well as the early life stages (i.e., eggs and larvae) of many
commercially important fishes. Being able to monitor zooplankton abundances
accurately and understand how populations change in relation to ocean
conditions is invaluable to marine science research, with important
implications for future marine seafood productivity. While new imaging
technologies generate massive amounts of video data of zooplankton, analyzing
them using general-purpose computer vision tools turns out to be highly
challenging due to the high similarity in appearance between the zooplankton
and its background (e.g., marine snow). In this work, we present the
ZooplanktonBench, a benchmark dataset containing images and videos of
zooplankton associated with rich geospatial metadata (e.g., geographic
coordinates, depth, etc.) in various water ecosystems. ZooplanktonBench defines
a collection of tasks to detect, classify, and track zooplankton in challenging
settings, including highly cluttered environments, living vs non-living
classification, objects with similar shapes, and relatively small objects. Our
dataset presents unique challenges and opportunities for state-of-the-art
computer vision systems to evolve and improve visual understanding in dynamic
environments characterized by significant variation and the need for
geo-awareness. The code and settings described in this paper can be found on
our website: https://lfk118.github.io/ZooplanktonBench_Webpage.

</details>


### [412] [WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation](https://arxiv.org/pdf/2505.18686)
*Yang Liu, Silin Cheng, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo*

Main category: cs.CV

TL;DR: WeakMCN is a multi-task framework combining WREC and WRES with a dual-branch architecture, achieving superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Joint learning of WREC and WRES can benefit from multi-task collaboration, improving performance over single-task methods.

Method: WeakMCN uses a dual-branch architecture with anchor-based contrastive learning for WREC and teacher-student supervision for WRES, enhanced by DVFE and CCM.

Result: WeakMCN outperforms state-of-the-art single-task methods by up to 3.91% (WREC) and 13.11% (WRES) on RefCOCO, and shows strong generalization in semi-supervised settings.

Conclusion: WeakMCN demonstrates the effectiveness of multi-task learning for WREC and WRES, offering significant performance gains and generalization.

Abstract: Weakly supervised referring expression comprehension(WREC) and
segmentation(WRES) aim to learn object grounding based on a given expression
using weak supervision signals like image-text pairs. While these tasks have
traditionally been modeled separately, we argue that they can benefit from
joint learning in a multi-task framework. To this end, we propose WeakMCN, a
novel multi-task collaborative network that effectively combines WREC and WRES
with a dual-branch architecture. Specifically, the WREC branch is formulated as
anchor-based contrastive learning, which also acts as a teacher to supervise
the WRES branch. In WeakMCN, we propose two innovative designs to facilitate
multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and
Collaborative Consistency Module(CCM). DVFE dynamically combines various
pre-trained visual knowledge to meet different task requirements, while CCM
promotes cross-task consistency from the perspective of optimization. Extensive
experimental results on three popular REC and RES benchmarks, i.e., RefCOCO,
RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN
over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on
RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also
validate the strong generalization ability of WeakMCN in both semi-supervised
REC and RES settings against existing methods, e.g., +8.94% for semi-REC and
+7.71% for semi-RES on 1% RefCOCO. The code is publicly available at
https://github.com/MRUIL/WeakMCN.

</details>


### [413] [PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms](https://arxiv.org/pdf/2505.19256)
*Vivek Gopalakrishnan, Neel Dey, Polina Golland*

Main category: cs.CV

TL;DR: PolyPose is a method for 2D/3D registration in medical imaging, using rigid transforms to model bone movement, enabling accurate 3D guidance from sparse X-ray images.


<details>
  <summary>Details</summary>
Motivation: Integrate volumetric guidance into intraoperative procedures where only 2D X-ray images are available, overcoming limitations of existing methods.

Method: PolyPose parameterizes 3D deformation fields as a composition of rigid transforms, respecting the piecewise rigid nature of human bones.

Result: Successfully aligns preoperative volumes to as few as two X-ray images, outperforming current methods in sparse-view and limited-angle settings.

Conclusion: PolyPose provides robust and anatomically plausible 3D guidance in challenging clinical scenarios without needing complex regularization.

Abstract: Determining the 3D pose of a patient from a limited set of 2D X-ray images is
a critical task in interventional settings. While preoperative volumetric
imaging (e.g., CT and MRI) provides precise 3D localization and visualization
of anatomical targets, these modalities cannot be acquired during procedures,
where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance
into intraoperative procedures, we present PolyPose, a simple and robust method
for deformable 2D/3D registration. PolyPose parameterizes complex 3D
deformation fields as a composition of rigid transforms, leveraging the
biological constraint that individual bones do not bend in typical motion.
Unlike existing methods that either assume no inter-joint movement or fail
outright in this under-determined setting, our polyrigid formulation enforces
anatomically plausible priors that respect the piecewise rigid nature of human
movement. This approach eliminates the need for expensive deformation
regularizers that require patient- and procedure-specific hyperparameter
optimization. Across extensive experiments on diverse datasets from orthopedic
surgery and radiotherapy, we show that this strong inductive bias enables
PolyPose to successfully align the patient's preoperative volume to as few as
two X-ray images, thereby providing crucial 3D guidance in challenging
sparse-view and limited-angle settings where current registration methods fail.

</details>


### [414] [BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change](https://arxiv.org/pdf/2505.19328)
*Manuela González-González, Soufiane Belharbi, Muhammad Osama Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich, Simon L Bacon, Eric Granger*

Main category: cs.CV

TL;DR: The paper introduces the first dataset (BAH) for recognizing ambivalence and hesitancy (A/H) in videos, collected from 224 participants, and provides baseline results for multimodal recognition.


<details>
  <summary>Details</summary>
Motivation: Recognizing A/H is crucial for personalized digital behavior change interventions, but lacks datasets for ML model design.

Method: A dataset (BAH) was created with videos from 224 participants, annotated for A/H cues, and baseline models were tested for frame- and video-level recognition.

Result: Baseline models show limited performance, highlighting the challenge of recognizing A/H in real-world videos.

Conclusion: The BAH dataset and baseline results provide a foundation for future research on A/H recognition, with data and code made available.

Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can
play a critical role in the personalization and effectiveness of digital
behaviour change interventions. These subtle and conflicting emotions are
manifested by a discord between multiple modalities, such as facial and vocal
expressions, and body language. Although experts can be trained to identify
A/H, integrating them into digital interventions is costly and less effective.
Automatic learning systems provide a cost-effective alternative that can adapt
to individual users, and operate seamlessly within real-time, and
resource-limited environments. However, there are currently no datasets
available for the design of ML models to recognize A/H. This paper introduces a
first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for
subject-based multimodal recognition of A/H in videos. It contains videos from
224 participants captured across 9 provinces in Canada, with different age, and
ethnicity. Through our web platform, we recruited participants to answer 7
questions, some of which were designed to elicit A/H while recording themselves
via webcam with microphone. BAH amounts to 1,118 videos for a total duration of
8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp
segments to indicate where A/H occurs, and provide frame- and video-level
annotations with the A/H cues. Video transcripts and their timestamps are also
included, along with cropped and aligned faces in each frame, and a variety of
participants meta-data. We include results baselines for BAH at frame- and
video-level recognition in multi-modal setups, in addition to zero-shot
prediction, and for personalization using unsupervised domain adaptation. The
limited performance of baseline models highlights the challenges of recognizing
A/H in real-world videos. The data, code, and pretrained weights are available.

</details>


### [415] [Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression](https://arxiv.org/pdf/2505.19398)
*Yiwei Xie, Ping Liu, Zheng Zhang*

Main category: cs.CV

TL;DR: A survey on concept erasure in Text-to-Image (T2I) models, categorizing methods, discussing evaluation gaps, and outlining future directions for ethical AI.


<details>
  <summary>Details</summary>
Motivation: Address ethical, legal, and safety challenges posed by uncontrolled reproduction of sensitive or harmful imagery in T2I models.

Method: Systematically categorizes concept erasure techniques along intervention level, optimization structure, and semantic scope.

Result: Highlights trade-offs in erasure specificity, generalization, and computational complexity, and identifies gaps in evaluation benchmarks.

Conclusion: Guides researchers toward safer, ethically aligned generative models with actionable recommendations for responsible AI development.

Abstract: Text-to-Image (T2I) models have demonstrated impressive capabilities in
generating high-quality and diverse visual content from natural language
prompts. However, uncontrolled reproduction of sensitive, copyrighted, or
harmful imagery poses serious ethical, legal, and safety challenges. To address
these concerns, the concept erasure paradigm has emerged as a promising
direction, enabling the selective removal of specific semantic concepts from
generative models while preserving their overall utility. This survey provides
a comprehensive overview and in-depth synthesis of concept erasure techniques
in T2I diffusion models. We systematically categorize existing approaches along
three key dimensions: intervention level, which identifies specific model
components targeted for concept removal; optimization structure, referring to
the algorithmic strategies employed to achieve suppression; and semantic scope,
concerning the complexity and nature of the concepts addressed. This
multi-dimensional taxonomy enables clear, structured comparisons across diverse
methodologies, highlighting fundamental trade-offs between erasure specificity,
generalization, and computational complexity. We further discuss current
evaluation benchmarks, standardized metrics, and practical datasets,
emphasizing gaps that limit comprehensive assessment, particularly regarding
robustness and practical effectiveness. Finally, we outline major challenges
and promising future directions, including disentanglement of concept
representations, adaptive and incremental erasure strategies, adversarial
robustness, and new generative architectures. This survey aims to guide
researchers toward safer, more ethically aligned generative models, providing
foundational knowledge and actionable recommendations to advance responsible
development in generative AI.

</details>


### [416] [ReDDiT: Rehashing Noise for Discrete Visual Generation](https://arxiv.org/pdf/2505.19656)
*Tianren Ma, Xiaosong Zhang, Boyu Yang, Junlan Feng, Qixiang Ye*

Main category: cs.CV

TL;DR: ReDDiT improves discrete diffusion models by introducing a rehashing noise framework and rehash sampler, achieving better generation quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Discrete diffusion models lag behind continuous ones due to noise design and sampling heuristics.

Method: Proposes ReDDiT with randomized multi-index corruption and rehash sampler for diverse, low-discrepancy generation.

Result: ReDDiT outperforms baselines (gFID reduced from 6.18 to 1.61) and matches continuous models in efficiency.

Conclusion: ReDDiT enhances discrete diffusion models, offering competitive performance with improved efficiency.

Abstract: Discrete diffusion models are gaining traction in the visual generative area
for their efficiency and compatibility. However, the pioneered attempts still
fall behind the continuous counterparts, which we attribute to the noise
(absorbing state) design and sampling heuristics. In this study, we propose the
rehashing noise framework for discrete diffusion transformer, termed ReDDiT, to
extend absorbing states and improve expressive capacity of discrete diffusion
models. ReDDiT enriches the potential paths that latent variables can traverse
during training with randomized multi-index corruption. The derived rehash
sampler, which reverses the randomized absorbing paths, guarantees the
diversity and low discrepancy of the generation process. These reformulations
lead to more consistent and competitive generation quality, mitigating the need
for heavily tuned randomness. Experiments show that ReDDiT significantly
outperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with
the continuous counterparts with higher efficiency.

</details>


### [417] [Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2505.19846)
*Nagito Saito, Shintaro Ito, Koichi Ito, Takafumi Aoki*

Main category: cs.CV

TL;DR: The paper proposes a method to improve semantic segmentation by generating high-quality pseudo labels using SAM and CLIP, refining them with UniMatch, and training a model with these labels.


<details>
  <summary>Details</summary>
Motivation: High annotation costs in semantic segmentation tasks motivate the use of semi-supervised learning with pseudo labels.

Method: Generate pseudo labels with SAM and CLIP, refine them using UniMatch, and train a semantic segmentation model with these enhanced labels.

Result: Experiments on PASCAL and MS COCO datasets demonstrate the method's effectiveness.

Conclusion: The proposed approach successfully reduces reliance on annotated data while improving segmentation accuracy.

Abstract: Semantic segmentation is a fundamental task in medical image analysis and
autonomous driving and has a problem with the high cost of annotating the
labels required in training. To address this problem, semantic segmentation
methods based on semi-supervised learning with a small number of labeled data
have been proposed. For example, one approach is to train a semantic
segmentation model using images with annotated labels and pseudo labels. In
this approach, the accuracy of the semantic segmentation model depends on the
quality of the pseudo labels, and the quality of the pseudo labels depends on
the performance of the model to be trained and the amount of data with
annotated labels. In this paper, we generate pseudo labels using zero-shot
annotation with the Segment Anything Model (SAM) and Contrastive Language-Image
Pretraining (CLIP), improve the accuracy of the pseudo labels using the Unified
Dual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels
to train a semantic segmentation model. The effectiveness of the proposed
method is demonstrated through the experiments using the public datasets:
PASCAL and MS COCO. The project web page is available at:
https://gsisaoki.github.io/ZERO-SHOT-PLG/

</details>


### [418] [Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud](https://arxiv.org/pdf/2505.19854)
*Natsuki Takama, Shintaro Ito, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki*

Main category: cs.CV

TL;DR: Sparse2DGS improves 3D reconstruction accuracy with limited input images by using DUSt3R and COLMAP MVS for dense point cloud initialization.


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting (GS) struggles with sparse input images due to poor initialization from sparse point clouds.

Method: Uses DUSt3R and COLMAP MVS to generate dense 3D point clouds for initializing 2D Gaussians.

Result: Accurate 3D reconstruction with just three images, demonstrated on the DTU dataset.

Conclusion: Sparse2DGS effectively addresses the limitation of GS in sparse-view scenarios.

Abstract: Gaussian Splatting (GS) has gained attention as a fast and effective method
for novel view synthesis. It has also been applied to 3D reconstruction using
multi-view images and can achieve fast and accurate 3D reconstruction. However,
GS assumes that the input contains a large number of multi-view images, and
therefore, the reconstruction accuracy significantly decreases when only a
limited number of input images are available. One of the main reasons is the
insufficient number of 3D points in the sparse point cloud obtained through
Structure from Motion (SfM), which results in a poor initialization for
optimizing the Gaussian primitives. We propose a new 3D reconstruction method,
called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three
images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along
with COLMAP MVS to generate highly accurate and dense 3D point clouds, which
are then used to initialize 2D Gaussians. Through experiments on the DTU
dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of
objects using just three images. The project page is available at
https://gsisaoki.github.io/SPARSE2DGS/

</details>


### [419] [NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID](https://arxiv.org/pdf/2505.20001)
*Shihao Li, Chenglong Li, Aihua Zheng, Andong Lu, Jin Tang, Jixin Ma*

Main category: cs.CV

TL;DR: The paper introduces NEXT, a novel multi-modal object re-identification (ReID) framework, leveraging MLLMs for semantic understanding and proposing attribute-based caption generation to improve text quality. It decouples recognition into semantic and structural expert branches, using TMSE and CSSE, and integrates them via MMFA for robust identity representation.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal ReID methods lack explicit modeling of fine-grained recognition under varying conditions. The paper aims to improve this by leveraging MLLMs and decoupling recognition into semantic and structural components.

Method: Proposes NEXT framework with two expert branches: TMSE for semantic recognition using text-modulated sampling and CSSE for structural recognition with soft routing. MMFA integrates these outputs into final identity representations.

Result: The method reduces unknown recognition rates in MLLMs and improves multi-modal semantic generation quality, enhancing ReID performance.

Conclusion: NEXT effectively combines semantic and structural recognition, offering a robust solution for multi-modal object ReID.

Abstract: Multi-modal object re-identification (ReID) aims to extract identity features
across heterogeneous spectral modalities to enable accurate recognition and
retrieval in complex real-world scenarios. However, most existing methods rely
on implicit feature fusion structures, making it difficult to model
fine-grained recognition strategies under varying challenging conditions.
Benefiting from the powerful semantic understanding capabilities of Multi-modal
Large Language Models (MLLMs), the visual appearance of an object can be
effectively translated into descriptive text. In this paper, we propose a
reliable multi-modal caption generation method based on attribute confidence,
which significantly reduces the unknown recognition rate of MLLMs in
multi-modal semantic generation and improves the quality of generated text.
Additionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture
of Experts via Text-Modulation for Multi-modal Object Re-Identification.
Specifically, we decouple the recognition problem into semantic and structural
expert branches to separately capture modality-specific appearance and
intrinsic structure. For semantic recognition, we propose the Text-Modulated
Semantic-sampling Experts (TMSE), which leverages randomly sampled high-quality
semantic texts to modulate expert-specific sampling of multi-modal features and
mining intra-modality fine-grained semantic cues. Then, to recognize
coarse-grained structure features, we propose the Context-Shared
Structure-aware Experts (CSSE) that focuses on capturing the holistic object
structure across modalities and maintains inter-modality structural consistency
through a soft routing mechanism. Finally, we propose the Multi-Modal Feature
Aggregation (MMFA), which adopts a unified feature fusion strategy to simply
and effectively integrate semantic and structural expert outputs into the final
identity representations.

</details>


### [420] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2505.20569)
*Jihoon Lee, Min Song*

Main category: cs.CV

TL;DR: RVCD (Retrieval Visual Contrastive Decoding) is introduced to suppress Object Hallucination in Large Vision-Language Models without additional training, using negative and positive images at the logit level.


<details>
  <summary>Details</summary>
Motivation: Object Hallucination (OH) remains a challenge in Vision-Language Models, and existing contrastive decoding methods need improvement.

Method: RVCD leverages negative and positive images at the logit level, referencing AI-generated images for single concepts.

Result: Substantial improvements over existing decoding-based methods are demonstrated.

Conclusion: RVCD effectively addresses OH without requiring model retraining, outperforming prior methods.

Abstract: Despite significant advancements in Large Vision-Language Models, Object
Hallucination (OH) remains a persistent challenge. Building upon prior studies
on contrastive decoding that address this issue without requiring additional
model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an
advanced method to suppress OH. RVCD leverages both negative and positive
images at the logit level, explicitly referencing AI-generated images designed
to represent a single concept. Our approach demonstrates substantial
improvements over existing decoding-based methods.

</details>


### [421] [Minute-Long Videos with Dual Parallelisms](https://arxiv.org/pdf/2505.21070)
*Zeqing Wang, Bowen Zheng, Xingyi Yang, Zhenxiong Tan, Yuecong Xu, Xinchao Wang*

Main category: cs.CV

TL;DR: DualParal, a distributed inference strategy, reduces latency and memory costs for DiT-based video diffusion models by parallelizing temporal frames and model layers across GPUs, using block-wise denoising and optimizations like feature caching and coordinated noise initialization.


<details>
  <summary>Details</summary>
Motivation: Addressing the high processing latency and memory costs of DiT-based video diffusion models for long videos.

Method: Proposes DualParal, which parallelizes frames and layers across GPUs with block-wise denoising, feature caching, and coordinated noise initialization.

Result: Achieves 6.54× lower latency and 1.48× lower memory cost for generating 1,025-frame videos on 8×RTX 4090 GPUs.

Conclusion: DualParal enables efficient, artifact-free, and infinitely long video generation with significant performance improvements.

Abstract: Diffusion Transformer (DiT)-based video diffusion models generate
high-quality videos at scale but incur prohibitive processing latency and
memory costs for long videos. To address this, we propose a novel distributed
inference strategy, termed DualParal. The core idea is that, instead of
generating an entire video on a single GPU, we parallelize both temporal frames
and model layers across GPUs. However, a naive implementation of this division
faces a key limitation: since diffusion models require synchronized noise
levels across frames, this implementation leads to the serialization of
original parallelisms. We leverage a block-wise denoising scheme to handle
this. Namely, we process a sequence of frame blocks through the pipeline with
progressively decreasing noise levels. Each GPU handles a specific block and
layer subset while passing previous results to the next GPU, enabling
asynchronous computation and communication. To further optimize performance, we
incorporate two key enhancements. Firstly, a feature cache is implemented on
each GPU to store and reuse features from the prior block as context,
minimizing inter-GPU communication and redundant computation. Secondly, we
employ a coordinated noise initialization strategy, ensuring globally
consistent temporal dynamics by sharing initial noise patterns across GPUs
without extra resource costs. Together, these enable fast, artifact-free, and
infinitely long video generation. Applied to the latest diffusion transformer
video generator, our method efficiently produces 1,025-frame videos with up to
6.54$\times$ lower latency and 1.48$\times$ lower memory cost on 8$\times$RTX
4090 GPUs.

</details>


### [422] [ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction](https://arxiv.org/pdf/2505.21117)
*Adeela Islam, Stefano Fiorini, Stuart James, Pietro Morerio, Alessio Del Bue*

Main category: cs.CV

TL;DR: ReassembleNet improves reassembly tasks by reducing complexity with contour keypoints and GNN pooling, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability, multimodality, and real-world applicability limitations in current Deep Learning methods for reassembly tasks.

Method: Uses contour keypoints and GNN pooling to reduce complexity, integrates multimodal features, and employs diffusion-based pose estimation.

Result: Achieves 55% and 86% improvements in RMSE for rotation and translation, respectively.

Conclusion: ReassembleNet offers a scalable, multimodal solution for complex reassembly tasks with superior performance.

Abstract: The task of reassembly is a significant challenge across multiple domains,
including archaeology, genomics, and molecular docking, requiring the precise
placement and orientation of elements to reconstruct an original structure. In
this work, we address key limitations in state-of-the-art Deep Learning methods
for reassembly, namely i) scalability; ii) multimodality; and iii) real-world
applicability: beyond square or simple geometric shapes, realistic and complex
erosion, or other real-world problems. We propose ReassembleNet, a method that
reduces complexity by representing each input piece as a set of contour
keypoints and learning to select the most informative ones by Graph Neural
Networks pooling inspired techniques. ReassembleNet effectively lowers
computational complexity while enabling the integration of features from
multiple modalities, including both geometric and texture data. Further
enhanced through pretraining on a semi-synthetic dataset. We then apply
diffusion-based pose estimation to recover the original structure. We improve
on prior methods by 55% and 86% for RMSE Rotation and Translation,
respectively.

</details>


### [423] [FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention](https://arxiv.org/pdf/2505.21144)
*Sergey Karpukhin, Vadim Titov, Andrey Kuznetsov, Aibek Alanov*

Main category: cs.CV

TL;DR: The paper introduces FastFace, a framework for training-free adaptation of ID-adapters to diffusion models, improving speed and identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing ID-adapters for diffusion models are slow due to joint training with base models. The goal is to enable faster, training-free adaptation.

Method: Redesigns classifier-free guidance for few-step generation and uses attention manipulation in decoupled blocks. Introduces FastFace framework.

Result: Improves identity similarity and fidelity while enabling faster inference. Also proposes a public evaluation protocol.

Conclusion: FastFace offers a universal solution for efficient, high-fidelity ID-preserving generation with diffusion models.

Abstract: In latest years plethora of identity-preserving adapters for a personalized
generation with diffusion models have been released. Their main disadvantage is
that they are dominantly trained jointly with base diffusion models, which
suffer from slow multi-step inference. This work aims to tackle the challenge
of training-free adaptation of pretrained ID-adapters to diffusion models
accelerated via distillation - through careful re-design of classifier-free
guidance for few-step stylistic generation and attention manipulation
mechanisms in decoupled blocks to improve identity similarity and fidelity, we
propose universal FastFace framework. Additionally, we develop a disentangled
public evaluation protocol for id-preserving adapters.

</details>


### [424] [3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling](https://arxiv.org/pdf/2505.21238)
*Jieyu Yuan, Yujun Li, Yuanlin Zhang, Chunle Guo, Xiongxin Tang, Ruixing Wang, Chongyi Li*

Main category: cs.CV

TL;DR: A physics-based framework improves underwater novel view synthesis by disentangling object appearance from water medium effects, using tailored Gaussian modeling and distance-guided optimization.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from inhomogeneous medium attenuation and scattering, disrupting conventional rendering methods like 3D Gaussian Splatting.

Method: The framework uses appearance embeddings for medium effects and a distance-guided optimization strategy with pseudo-depth maps for geometric fidelity.

Result: The approach outperforms existing methods in rendering quality and restoration accuracy.

Conclusion: The proposed method effectively addresses underwater scene challenges, achieving high-quality synthesis and accurate restoration.

Abstract: Novel view synthesis for underwater scene reconstruction presents unique
challenges due to complex light-media interactions. Optical scattering and
absorption in water body bring inhomogeneous medium attenuation interference
that disrupts conventional volume rendering assumptions of uniform propagation
medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering
capabilities, it struggles with underwater inhomogeneous environments where
scattering media introduce artifacts and inconsistent appearance. In this
study, we propose a physics-based framework that disentangles object appearance
from water medium effects through tailored Gaussian modeling. Our approach
introduces appearance embeddings, which are explicit medium representations for
backscatter and attenuation, enhancing scene consistency. In addition, we
propose a distance-guided optimization strategy that leverages pseudo-depth
maps as supervision with depth regularization and scale penalty terms to
improve geometric fidelity. By integrating the proposed appearance and medium
modeling components via an underwater imaging model, our approach achieves both
high-quality novel view synthesis and physically accurate scene restoration.
Experiments demonstrate our significant improvements in rendering quality and
restoration accuracy over existing methods. The project page is available at
https://bilityniu.github.io/3D-UIR.

</details>


### [425] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/pdf/2505.21535)
*Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang*

Main category: cs.CV

TL;DR: FAR replaces attention blocks in transformers with LSTM modules, improving efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformers' attention mechanisms are inefficient for inference on edge devices; FAR aims to simplify this.

Method: FAR uses LSTM-based modules and distillation to replace attention blocks, optimizing for efficiency.

Result: FAR matches original model accuracy on ImageNet and downstream tasks with reduced latency and parameters.

Conclusion: FAR successfully preserves transformer performance while enhancing inference efficiency.

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [426] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/pdf/2505.21549)
*Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabhu, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP is a fine-tuned CLIP variant improving image-text retrieval without losing zero-shot classification, using a meta teacher-student distillation framework.


<details>
  <summary>Details</summary>
Motivation: CLIP models struggle with fine-grained cross-modal understanding due to fixed resolutions and limited context. DCLIP aims to enhance retrieval while preserving generalization.

Method: Uses a cross-modal transformer teacher for enriched embeddings via bidirectional cross-attention, training a lightweight student with hybrid contrastive and cosine loss.

Result: Improves retrieval metrics (Recall@K, MAP) with ~67,500 samples, retaining 94% of CLIP's zero-shot performance.

Conclusion: DCLIP balances task specialization and generalization efficiently for vision-language tasks.

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [427] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/pdf/2505.21649)
*Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer*

Main category: cs.CV

TL;DR: DORI is a benchmark for evaluating object orientation understanding in vision-language models, revealing significant limitations in current systems.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks conflate object orientation with other visual tasks, making it hard to isolate this critical capability for applications like robotics and AR.

Method: DORI assesses orientation comprehension through four dimensions using tasks from 11 datasets across 67 object categories.

Result: Top models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular ones, struggling with reference frame shifts and compound rotations.

Conclusion: DORI highlights the need for better orientation representation in models, with implications for robotics, 3D reconstruction, and human-AI interaction.

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [428] [Compositional Scene Understanding through Inverse Generative Modeling](https://arxiv.org/pdf/2505.21780)
*Yanbo Wang, Justin Dauwels, Yilun Du*

Main category: cs.CV

TL;DR: The paper explores using generative models for scene understanding by treating it as an inverse generative modeling problem, enabling robust generalization to new scenes and objects.


<details>
  <summary>Details</summary>
Motivation: To extend generative models beyond content synthesis to scene understanding, inferring scene properties from natural images.

Method: Formulates scene understanding as inverse generative modeling, using compositional models to infer scene structure and global factors.

Result: Enables robust generalization to new scenes with more objects or new shapes, and zero-shot multi-object perception.

Conclusion: The approach effectively leverages generative models for scene understanding, demonstrating broad applicability and generalization.

Abstract: Generative models have demonstrated remarkable abilities in generating
high-fidelity visual content. In this work, we explore how generative models
can further be used not only to synthesize visual content but also to
understand the properties of a scene given a natural image. We formulate scene
understanding as an inverse generative modeling problem, where we seek to find
conditional parameters of a visual generative model to best fit a given natural
image. To enable this procedure to infer scene structure from images
substantially different than those seen during training, we further propose to
build this visual generative model compositionally from smaller models over
pieces of a scene. We illustrate how this procedure enables us to infer the set
of objects in a scene, enabling robust generalization to new test scenes with
an increased number of objects of new shapes. We further illustrate how this
enables us to infer global scene factors, likewise enabling robust
generalization to new scenes. Finally, we illustrate how this approach can be
directly applied to existing pretrained text-to-image generative models for
zero-shot multi-object perception. Code and visualizations are at
https://energy-based-model.github.io/compositional-inference.

</details>


### [429] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/pdf/2505.21863)
*Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta*

Main category: cs.CV

TL;DR: GETReason framework enhances image understanding by inferring deeper contextual meaning through geospatial, temporal, and event data, evaluated by the GREAT metric.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately extract contextual relevance from publicly significant images, limiting their utility in journalism and education.

Method: Introduces GETReason, a layered multi-agent framework, and GREAT, a new metric for evaluating reasoning-based image understanding.

Result: The approach successfully infers meaningful insights, linking images to broader event contexts.

Conclusion: GETReason and GREAT improve contextual understanding of images, offering valuable tools for journalism and education.

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [430] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/pdf/2505.21904)
*Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST is a semi-supervised knowledge distillation framework that compresses large vision foundation models into smaller, efficient experts using limited labeled and abundant unlabeled data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Instance segmentation requires expensive annotations and large models. CAST aims to reduce these costs by leveraging unlabeled data and distilling knowledge from pretrained models.

Method: CAST involves three stages: domain adaptation of the teacher model, distillation into a compact student using a multi-objective loss, and fine-tuning to remove pseudo-label bias. A key component is an instance-aware pixel-wise contrastive loss.

Result: The compact student model (11X smaller) outperforms its teacher by +3.4 AP on Cityscapes and +1.5 AP on ADE20K, surpassing other semi-supervised methods.

Conclusion: CAST effectively leverages unlabeled data and contrastive learning to create efficient, high-performing instance segmentation models.

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [431] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/pdf/2505.21956)
*Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao*

Main category: cs.CV

TL;DR: Cross-modal RAG improves text-to-image generation by decomposing queries and images into sub-dimensional components, enabling better retrieval and synthesis.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of existing RAG methods in handling complex queries where no single image contains all desired elements.

Method: Proposes a hybrid retrieval strategy (sparse + dense retriever) and subquery-aware generation using a multimodal LLM.

Result: Outperforms baselines in retrieval and generation quality across multiple datasets.

Conclusion: Cross-modal RAG is effective and efficient for complex text-to-image tasks.

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [432] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/pdf/2505.22150)
*Runze Xia, Shuo Feng, Renzhi Wang, Congchi Yin, Xuyun Wen, Piji Li*

Main category: cs.CV

TL;DR: The paper proposes FgB2I, a method to improve brain-to-image reconstruction by using fine-grained text descriptions as a bridge, enhancing detail and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing brain-to-image reconstruction methods often lack detail and semantic consistency, likely due to insufficient semantic information.

Method: FgB2I involves three stages: detail enhancement (using vision-language models for fine-grained captions), decoding fine-grained text from fMRI signals, and text-bridged reconstruction. Three reward metrics guide text decoding.

Result: Fine-grained text descriptions improve reconstruction quality, validated by experiments.

Conclusion: FgB2I effectively enhances brain-to-image reconstruction by integrating fine-grained text, addressing detail and semantic gaps.

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [433] [YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction](https://arxiv.org/pdf/2505.22250)
*Mingzhuang Wang, Yvyang Li, Xiyang Zhang, Fei Tan, Qi Shi, Guotao Zhang, Siqi Chen, Yufei Liu, Lei Lei, Ming Zhou, Qiang Lin, Hongqiang Yang*

Main category: cs.CV

TL;DR: The paper introduces YH-MINER, an intelligent system using a Multimodal Large Model (MLLM) for efficient coral reef monitoring, achieving high accuracy in object detection, segmentation, and classification.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are vital but threatened; current monitoring methods are inefficient and inaccurate in complex underwater environments.

Method: Develops YH-MINER, an MLLM-based framework combining object detection, semantic segmentation, and prior input for coral instance analysis.

Result: Achieves 78% mAP@0.5 in detection, 88% genus-level classification accuracy, and extracts ecological metrics.

Conclusion: YH-MINER enables scalable, automated coral reef monitoring and supports future integration with underwater robots.

Abstract: Coral reefs, crucial for sustaining marine biodiversity and ecological
processes (e.g., nutrient cycling, habitat provision), face escalating threats,
underscoring the need for efficient monitoring. Coral reef ecological
monitoring faces dual challenges of low efficiency in manual analysis and
insufficient segmentation accuracy in complex underwater scenarios. This study
develops the YH-MINER system, establishing an intelligent framework centered on
the Multimodal Large Model (MLLM) for "object detection-semantic
segmentation-prior input". The system uses the object detection module
(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the
segment module to complete pixel-level segmentation in low-light and densely
occluded scenarios. The segmentation masks and finetuned classification
instructions are fed into the Qwen2-VL-based multimodal model as prior inputs,
achieving a genus-level classification accuracy of 88% and simultaneously
extracting core ecological metrics. Meanwhile, the system retains the
scalability of the multimodal model through standardized interfaces, laying a
foundation for future integration into multimodal agent-based underwater robots
and supporting the full-process automation of "image acquisition-prior
generation-real-time analysis".

</details>


### [434] [GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/pdf/2505.22421)
*Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shanghang Zhang*

Main category: cs.CV

TL;DR: GeoDrive enhances autonomous driving by integrating robust 3D geometry into world models, improving spatial understanding and action controllability, outperforming existing methods in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current world models for autonomous driving lack robust 3D geometric consistency and accumulate artifacts during occlusion, limiting reliable safety assessment.

Method: GeoDrive extracts 3D representations from input frames, renders 2D views based on ego-car trajectories, and uses dynamic editing to enhance renderings by adjusting vehicle positions.

Result: GeoDrive outperforms existing models in action accuracy and 3D spatial awareness, enabling realistic, adaptable, and reliable scene modeling.

Conclusion: GeoDrive advances autonomous driving by improving 3D spatial modeling, generalizing to novel trajectories, and offering interactive scene editing capabilities.

Abstract: Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.

</details>


### [435] [SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](https://arxiv.org/pdf/2505.22461)
*Qiucheng Yu, Yuan Xie, Xin Tan*

Main category: cs.CV

TL;DR: SHTOcc improves 3D occupancy prediction by addressing voxel distribution issues, reducing GPU memory usage by 42.2%, speeding up inference by 58.6%, and boosting accuracy by 7%.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to explore voxel distribution patterns, leading to poor performance and long-tail problems.

Method: SHTOcc uses sparse head-tail voxel construction and decoupled learning to balance key voxels and reduce bias toward dominant categories.

Result: SHTOcc reduces GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves accuracy by 7%.

Conclusion: SHTOcc effectively addresses voxel distribution challenges, enhancing performance and efficiency in 3D occupancy prediction.

Abstract: 3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses sparse head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc

</details>


### [436] [The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector](https://arxiv.org/pdf/2505.22499)
*Aixuan Li, Mochu Xiang, Jing Zhang, Yuchao Dai*

Main category: cs.CV

TL;DR: The paper explores 3D adversarial attacks on BEV-based 3D object detection models, introducing non-invasive adversarial objects and an occlusion-aware module to test model robustness.


<details>
  <summary>Details</summary>
Motivation: To evaluate the robustness of 3D object detection models in autonomous driving by investigating their vulnerability to 3D adversarial attacks.

Method: Generates non-invasive 3D adversarial objects using differentiable rendering and an occlusion-aware module, optimizing with BEV spatial feature guidance.

Result: The approach reliably suppresses vehicle predictions in state-of-the-art detectors and shows strong generalization across positions and distances.

Conclusion: The method serves as a tool to test 3D object detection model robustness before deployment, highlighting vulnerabilities to adversarial attacks.

Abstract: 3D object detection is a critical component in autonomous driving systems. It
allows real-time recognition and detection of vehicles, pedestrians and
obstacles under varying environmental conditions. Among existing methods, 3D
object detection in the Bird's Eye View (BEV) has emerged as the mainstream
framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D
adversarial attacks are investigated, where attacks are placed in 3D
environments to evaluate the model performance, e.g. putting a film on a car,
clothing a pedestrian. The vulnerability of 3D object detection models to 3D
adversarial attacks serves as an important indicator to evaluate the robustness
of the model against perturbations. To investigate this vulnerability, we
generate non-invasive 3D adversarial objects tailored for real-world attack
scenarios. Our method verifies the existence of universal adversarial objects
that are spatially consistent across time and camera views. Specifically, we
employ differentiable rendering techniques to accurately model the spatial
relationship between adversarial objects and the target vehicle. Furthermore,
we introduce an occlusion-aware module to enhance visual consistency and
realism under different viewpoints. To maintain attack effectiveness across
multiple frames, we design a BEV spatial feature-guided optimization strategy.
Experimental results demonstrate that our approach can reliably suppress
vehicle predictions from state-of-the-art 3D object detectors, serving as an
important tool to test robustness of 3D object detection models before
deployment. Moreover, the generated adversarial objects exhibit strong
generalization capabilities, retaining its effectiveness at various positions
and distances in the scene.

</details>


### [437] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/pdf/2505.22535)
*Mohamad Hakam Shams Eddin, Yikui Zhang, Stefan Kollet, Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba is a deep learning model for global river discharge and flood forecasting, outperforming existing models by leveraging spatio-temporal relations and Mamba blocks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches in hydrology lack spatial connections and global-scale applications, limiting their effectiveness in flood forecasting.

Method: RiverMamba uses Mamba blocks and long-term reanalysis data to model spatio-temporal relations, integrating ECMWF HRES forecasts while accounting for inaccuracies.

Result: RiverMamba provides reliable predictions for river discharge and extreme floods, surpassing operational AI- and physics-based models.

Conclusion: RiverMamba addresses the limitations of current methods, offering improved global flood forecasting for early warning systems.

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [438] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/pdf/2505.22698)
*Luca Fantin, Marco Antonelli, Margherita Cesetti, Daniele Irto, Bruno Zamengo, Francesco Silvestri*

Main category: cs.AI

TL;DR: A chatbot simplifies analyzing public transportation data by using an agent architecture with LLM and tools for SQL queries, visualization, and performance evaluation.


<details>
  <summary>Details</summary>
Motivation: To make public transportation data analysis accessible and user-friendly, overcoming SQL and visualization complexities.

Method: Uses an agent-based chatbot with LLM, integrating tools for SQL queries, data plotting, and map creation, plus a workflow for performance evaluation.

Result: The chatbot effectively interacts with datasets, generates queries, and provides natural language responses, with a workflow for performance measurement.

Conclusion: The chatbot offers a practical solution for public transportation data analysis, with a robust method for evaluating its performance.

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [439] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/pdf/2505.22753)
*Arseniy Pertzovsky, Roni Stern, Ariel Felner, Roie Zivan*

Main category: cs.AI

TL;DR: APFs improve LMAPF throughput but not MAPF.


<details>
  <summary>Details</summary>
Motivation: To enhance MAPF and LMAPF solutions using APFs.

Method: Incorporated APFs into MAPF algorithms like Prioritized Planning, MAPF-LNS2, and PIBT.

Result: APFs didn't help MAPF but boosted LMAPF throughput by up to 7x.

Conclusion: APFs are effective for LMAPF but not for standard MAPF.

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [440] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/pdf/2505.22756)
*Tian Qin, Core Francisco Park, Mujin Kwun, Aaron Walsman, Eran Malach, Nikhil Anand, Hidenori Tanaka, David Alvarez-Melis*

Main category: cs.AI

TL;DR: The paper analyzes LLM reasoning in math tasks, proposing a decomposition into Plan, Execute, and Verify. GRPO improves execution but struggles with new problems due to planning gaps. A synthetic task confirms RL's execution boost and explores overcoming limitations.


<details>
  <summary>Details</summary>
Motivation: To understand LLM reasoning beyond accuracy metrics by decomposing problem-solving into core skills and assessing RL's impact.

Method: Decompose math problem-solving into Plan, Execute, Verify; use GRPO and a synthetic task to study RL's effects.

Result: GRPO enhances execution but not planning, leading to a 'coverage wall' for new problems. Synthetic tasks replicate findings and suggest RL can improve exploration.

Conclusion: RL boosts execution robustness but faces planning limitations. Insights suggest paths to overcome these barriers.

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [441] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/pdf/2505.22779)
*Mohammad Helal Uddin, Sabur Baidya*

Main category: cs.AI

TL;DR: A hybrid approach using smartphone sensor data and Twitter sentiment analysis to evaluate depression levels, achieving high accuracy with SVM-based classification.


<details>
  <summary>Details</summary>
Motivation: Addressing the global challenge of mental disorders like depression by leveraging technology for non-invasive monitoring.

Method: Combines CNN-based activity recognition and Naive Bayes sentiment analysis, with SVM for depression severity classification.

Result: High accuracy in activity recognition (95%), sentiment analysis (95.6%), and depression classification (94%).

Conclusion: The approach is effective for long-term depression monitoring while preserving privacy.

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [442] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/pdf/2505.22871)
*Yuval David, Fabiana Fournier, Lior Limonad, Inna Skarbovsky*

Main category: cs.AI

TL;DR: A novel method unifies multiple causal process variants into a consistent model, addressing challenges like missing values and alternating causal conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discovering causal process models fail to handle alternating causal conditions across variants, missing values, and trace blending issues.

Method: Proposes a formally defined method to unify causal process variants, preserving correctness and representing causal-flow alternations explicitly.

Result: Evaluated on three open and two proprietary datasets, with an open-source implementation released.

Conclusion: The method successfully addresses the limitations of prior work, offering a unified and correct representation of causal process variants.

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [443] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/pdf/2505.22928)
*Massimiliano Pronesti, Michela Lorandi, Paul Flanagan, Oisin Redmon, Anya Belz, Yufang Hou*

Main category: cs.AI

TL;DR: The paper proposes a numeric reasoning system for automating systematic reviews in medicine, improving accuracy by extracting structured numerical evidence and applying domain-specific logic, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Automating systematic reviews is hindered by the challenge of extracting numeric evidence and deriving conclusions. Prior methods rely on shallow textual cues, lacking the depth of expert numeric reasoning.

Method: The system combines numeric data extraction (trained via supervised fine-tuning and reinforcement learning) with domain-informed logic for deriving conclusions.

Result: The approach achieves a 21% F1 score improvement over retrieval-based systems and outperforms large language models by up to 9%.

Conclusion: Numeric reasoning-driven methods show promise for automating evidence synthesis, aligning with expert principles.

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [444] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/pdf/2505.22948)
*Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, Jie Chen*

Main category: cs.AI

TL;DR: FMG uses multi-modal foundation models to create an interpretable molecular language, improving molecular generation and property prediction.


<details>
  <summary>Details</summary>
Motivation: Current grammar-based molecular generation methods rely on expert annotation or unreliable heuristics, limiting their effectiveness.

Method: FMG leverages multi-modal foundation models to represent molecules as images and text, aligning information across modalities via prompt learning.

Result: FMG outperforms prior methods in synthesizability, diversity, and data efficiency while providing built-in chemical interpretability.

Conclusion: FMG is a versatile and interpretable alternative for molecular discovery, with code publicly available.

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [445] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/pdf/2505.22954)
*Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, Jeff Clune*

Main category: cs.AI

TL;DR: The paper introduces the Darwin Gödel Machine (DGM), a self-improving AI system that autonomously modifies its code and validates changes empirically, outperforming non-self-improving baselines on coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack autonomous self-improvement capabilities, limiting their potential. Automating AI advancement could accelerate progress and benefits, but existing methods like meta-learning are constrained by human-designed search spaces.

Method: The DGM iteratively modifies its own code, empirically validating each change using coding benchmarks. It maintains an archive of diverse coding agents, growing it through open-ended exploration inspired by Darwinian evolution.

Result: The DGM improves coding capabilities significantly, e.g., from 20.0% to 50.0% on SWE-bench and from 14.2% to 30.7% on Polyglot, outperforming baselines without self-improvement or open-ended exploration.

Conclusion: The DGM represents a step toward self-improving AI, capable of autonomous innovation while incorporating safety measures like sandboxing and human oversight.

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [446] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/pdf/2505.23686)
*Caroline Wang, Arrasy Rahman, Jiaxun Cui, Yoonchang Sung, Peter Stone*

Main category: cs.AI

TL;DR: ROTATE introduces a unified, regret-driven framework for Ad Hoc Teamwork (AHT), outperforming baselines by improving agent training and teammate generation iteratively.


<details>
  <summary>Details</summary>
Motivation: Existing AHT methods lack coverage in teammate behaviors and ignore learnability, leading to poor generalization.

Method: ROTATE alternates between training the AHT agent and generating adversarial teammates to probe deficiencies.

Result: ROTATE significantly outperforms baselines in generalizing to unseen teammates.

Conclusion: ROTATE sets a new standard for robust and generalizable teamwork in AHT.

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [447] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/pdf/2505.22960)
*Yongjin Yang, Euiin Yi, Jongwoo Ko, Kimin Lee, Zhijing Jin, Se-Young Yun*

Main category: cs.AI

TL;DR: Multi-agent debate (MAD) is explored as a test-time scaling technique, showing limited benefits for math tasks but potential risks and benefits for safety tasks, depending on conditions like problem difficulty and model capability.


<details>
  <summary>Details</summary>
Motivation: To systematically understand MAD's effectiveness compared to self-agent methods under varying conditions, filling a gap in prior research.

Method: Conceptualizes MAD as a test-time computational scaling technique and conducts empirical comparisons on mathematical reasoning and safety tasks, examining factors like task difficulty, model scale, and agent diversity.

Result: MAD offers limited advantages for math tasks but becomes more effective with harder problems or weaker models. For safety tasks, MAD can increase vulnerability but reduces attack success with diverse agent configurations.

Conclusion: The findings guide future MAD system development, highlighting context-dependent effectiveness and strategic deployment needs.

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [448] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/pdf/2505.22987)
*Nick Byrd*

Main category: cs.AI

TL;DR: The paper synthesizes historical debates on rationality with modern cognitive science to propose Strategic Reflectivism, emphasizing pragmatic switching between intuitive and reflective thinking for intelligent systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge historical debates on rationality (e.g., bounded rationality) with contemporary applications in AI, highlighting the importance of integrating intuitive and reflective processes.

Method: It combines historical analysis with recent experimental results from human and machine cognition to develop Strategic Reflectivism.

Result: Strategic Reflectivism emerges as a framework advocating for pragmatic switching between intuitive and reflective inference to optimize goal fulfillment.

Conclusion: The paper concludes that Strategic Reflectivism, rooted in American Pragmatism, offers actionable insights for designing intelligent systems by valuing both intuition and reflection.

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [449] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/pdf/2505.22990)
*Pin-Han Chen, Yu-Sheng Lin, Wei-Cheng Lee, Tin-Yu Leu, Po-Hsiang Hsu, Anjana Dissanayake, Sungjin Oh, Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: MenTeR is a multiagent AI workflow for RF/analog design, reducing manual effort and accelerating the design cycle.


<details>
  <summary>Details</summary>
Motivation: Analog design is complex and time-consuming, relying heavily on expert intuition, which hinders efficiency.

Method: MenTeR uses specialized AI agents for tasks like specification understanding, optimization, and validation.

Result: It speeds up design cycles, explores more design options, and handles real-world systems robustly.

Conclusion: MenTeR paves the way for future AI-assisted RF/analog design tools.

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [450] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/pdf/2505.23034)
*Guangyi Liu, Yongqi Zhang, Xunyuan Liu, Quanming Yao*

Main category: cs.AI

TL;DR: CBR-DDI improves DDI prediction by combining LLMs and GNNs with case-based reasoning, achieving a 28.7% accuracy boost.


<details>
  <summary>Details</summary>
Motivation: DDI prediction is crucial for treatment safety, but LLMs struggle with it. Inspired by clinical case-based reasoning, CBR-DDI aims to enhance LLM performance.

Method: Uses LLMs to extract pharmacological insights and GNNs to model drug associations. Features hybrid retrieval, dual-layer prompting, and dynamic case refinement.

Result: Achieves state-of-the-art performance with a 28.7% accuracy improvement over baselines.

Conclusion: CBR-DDI effectively enhances LLM reasoning for DDI tasks, offering high interpretability and flexibility.

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [451] [Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game](https://arxiv.org/pdf/2310.18940)
*Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu*

Main category: cs.AI

TL;DR: A framework combining LLMs with RL improves strategic decision-making in language agents, tested in the Werewolf game, outperforming pure LLM-based agents and achieving human-level performance.


<details>
  <summary>Details</summary>
Motivation: Pure LLM-based agents exhibit intrinsic bias in complex decision-making tasks, leading to suboptimal performance. The goal is to develop strategic language agents with flexible actions and strong decision-making.

Method: Proposes a framework where an LLM generates diverse action candidates, and an RL policy selects the optimal action. Tested in the Werewolf game.

Result: Agents overcome intrinsic bias, outperform existing LLM-based agents, and achieve human-level performance in strategic gameplay.

Conclusion: Combining LLMs with RL enhances strategic decision-making in language agents, demonstrating potential for complex tasks.

Abstract: Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.

</details>


### [452] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/pdf/2505.23058)
*Yutong Xie, Zhuoheng Li, Xiyuan Wang, Yijun Pan, Qijia Liu, Xingzhi Cui, Kuang-Yu Lo, Ruoyi Gao, Xingjian Zhang, Jin Huang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei*

Main category: cs.AI

TL;DR: Be.FM is an open foundation model for human behavior modeling, built on large language models, capable of predicting and understanding human decision-making.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of foundation models in human behavior modeling.

Method: Built on open-source large language models and fine-tuned on diverse behavioral data.

Result: Be.FM can predict behaviors, infer individual/population characteristics, generate contextual insights, and apply behavioral science knowledge.

Conclusion: Be.FM demonstrates the viability of foundation models for human behavior understanding and prediction.

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [453] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/pdf/2505.23075)
*Amit Kumthekar, Zion Tilley, Henry Duong, Bhargav Patel, Michael Magnoli, Ahmed Omar, Ahmed Nasser, Chaitanya Gharpure, Yevgen Reztzov*

Main category: cs.AI

TL;DR: The paper introduces the Consensus Mechanism, an ensemble of specialized medical expert agents, outperforming single-model LLMs in clinical decision-making across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the risks of obsolescence and rigid dependence on single-model systems in clinical LLMs by mimicking multidisciplinary decision-making.

Method: The Consensus Mechanism uses an ensemble of specialized medical expert agents, optimized for cost, latency, or performance.

Result: Achieved higher accuracy (e.g., 61.0% on MedXpertQA) and improved recall/precision in differential diagnosis compared to single-model systems.

Conclusion: The Consensus Mechanism offers a robust, adaptable alternative to single-model LLMs, enhancing clinical decision-making.

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [454] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/pdf/2505.23091)
*Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang*

Main category: cs.AI

TL;DR: The paper introduces Infi-MMR, a framework to enhance reasoning in Multimodal Small Language Models (MSLMs) through a three-phase curriculum, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in extending reasoning capabilities from LLMs to MSLMs, including dataset scarcity, reasoning degradation, and flawed reinforcement learning.

Method: Three-phase curriculum: Foundational Reasoning Activation, Cross-Modal Reasoning Adaptation, and Multimodal Reasoning Enhancement, applied to Infi-MMR-3B.

Result: Infi-MMR-3B excels in multimodal math reasoning (e.g., 43.68% on MathVerse) and general reasoning (67.2% on MathVista).

Conclusion: Infi-MMR effectively unlocks MSLMs' reasoning potential, addressing key challenges and achieving top performance.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [455] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/pdf/2505.23153)
*Fan Wang, Shaoshan Liu*

Main category: cs.AI

TL;DR: The paper introduces Collective Adaptive Intelligence (CAI), a framework for designing AI systems where autonomous agents collaborate and adapt in dynamic environments, emphasizing resilience and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable and resilient AI systems in complex, real-world scenarios by leveraging collaborative and self-organizing agents.

Method: Proposes a conceptual framework for CAI, outlining key attributes like task generalization, resilience, scalability, and self-assembly.

Result: Provides a structured foundation for understanding and implementing CAI, bridging theory and practical methodologies.

Conclusion: Aims to guide researchers in developing more resilient, scalable, and adaptable AI systems across various domains.

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [456] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/pdf/2505.23281)
*Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, Martin Vechev*

Main category: cs.AI

TL;DR: MathArena is a new benchmark for evaluating LLMs' mathematical reasoning and proof-writing, addressing contamination risks in existing datasets like AIME 2024. It uses real-time competition problems and shows strong reasoning in top models but poor proof-writing performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like AIME 2024 suffer from contamination risks and lack proof-writing evaluation, necessitating a more rigorous and dynamic benchmark.

Method: MathArena leverages recurring math competitions for real-time evaluation, eliminating contamination risks. It assesses 30 models across five competitions (149 problems).

Result: Contamination is evident in AIME 2024, but top models show strong reasoning on newer competitions like SMT 2025. Proof-writing performance (e.g., USAMO 2025) lags behind final-answer tasks.

Conclusion: MathArena provides a contamination-free, evolving benchmark for LLMs' mathematical reasoning and proof-writing, highlighting gaps in current capabilities.

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [457] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/pdf/2505.23381)
*Bowen Ping, Minnan Luo, Zhuohang Dang, Chenxi Wang, Chengyou Jia*

Main category: cs.AI

TL;DR: AutoGPS is a neuro-symbolic framework for solving geometry problems with reliable, interpretable reasoning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in reliability and interpretability of neural and symbolic approaches for geometry problem solving.

Method: Combines Multimodal Problem Formalizer (MPF) for cross-modal comprehension and Deductive Symbolic Reasoner (DSR) for hypergraph-based rigorous reasoning.

Result: Achieves state-of-the-art performance and 99% logical coherence in stepwise solutions.

Conclusion: AutoGPS provides a robust, human-interpretable solution for geometry problem solving.

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [458] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/pdf/2505.23397)
*Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, Seyit Camtepe*

Main category: cs.AI

TL;DR: A structured framework for Human-AI collaboration in SOCs is proposed, featuring tiered AI autonomy, trust calibration, and Human-in-the-Loop roles to address gaps in existing automation-focused approaches.


<details>
  <summary>Details</summary>
Motivation: Existing SOC frameworks lack systematic structures for human oversight, trust calibration, and scalable AI autonomy, often assuming static settings unsuitable for varied task complexities.

Method: The framework introduces five levels of AI autonomy, mapped to Human-in-the-Loop roles and task-specific trust thresholds, demonstrated via a simulated cyber range with an LLM-based AI-Avatar.

Result: The framework enables adaptive AI integration, reduces alert fatigue, enhances response coordination, and strategically calibrates trust in SOC tasks.

Conclusion: The research advances theoretical and practical designs for next-gen cognitive SOCs, emphasizing AI's role in enhancing, not replacing, human decision-making.

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [459] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/pdf/2505.23399)
*Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, Wenhao Chai, Jian Wang, Keze Wang*

Main category: cs.AI

TL;DR: GAM-Agent is a game-theoretic multi-agent framework for vision-language reasoning, improving robustness and interpretability through dynamic agent collaboration and uncertainty-aware control.


<details>
  <summary>Details</summary>
Motivation: To address limitations of single-agent or monolithic models in vision-language reasoning by introducing a collaborative, game-theoretic approach for more robust and interpretable predictions.

Method: Formulates reasoning as a non-zero-sum game between specialized base agents and a critical agent, using structured communication and uncertainty-aware dynamic collaboration.

Result: Significant performance improvements on benchmarks (5-6% for smaller models, 2-3% for strong models like GPT-4o).

Conclusion: GAM-Agent offers a modular, scalable, and generalizable solution for reliable and explainable multi-agent multimodal reasoning.

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [460] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/pdf/2505.23432)
*Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: A mathematical framework models human-GenAI collaboration by decomposing skills into decision and action subskills, showing when GenAI complements rather than replaces humans.


<details>
  <summary>Details</summary>
Motivation: To address debates on whether GenAI tools complement or replace human workers by modeling their collaborative potential.

Method: Introduces a skill decomposition framework (decision and action subskills) and analyzes job success under varying subskill abilities, using O*NET and Big-Bench Lite data.

Result: Identifies conditions for productivity gains, like larger benefits for lower-skilled workers, and demonstrates practical alignment with real-world data.

Conclusion: GenAI complements human skills under specific conditions, enhancing productivity without full replacement.

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [461] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/pdf/2505.23436)
*Daniel Jarne Ornia, Nicholas Bishop, Joel Dyer, Wei-Chen Lee, Ani Calinescu, Doyne Farme, Michael Wooldridge*

Main category: cs.AI

TL;DR: The paper explores how AI agents under resource constraints exhibit altered utility-driven behaviors, leading to misalignment with human objectives, and proposes mitigation mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand and address the misalignment between human objectives and AI agent behaviors when operating under survival pressure and resource constraints.

Method: Formalizes the problem using a survival bandit framework, provides theoretical and empirical analysis, and proposes mechanisms to mitigate risk-seeking or risk-averse behaviors.

Result: Quantifies the impact of survival-driven preference shifts and identifies conditions for misalignment emergence.

Conclusion: The work enhances interpretability of AI agent behaviors under survival pressure and offers guidelines for safe deployment in resource-limited environments.

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [462] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/pdf/2505.23473)
*Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xiaolu Zhang, Jun Zhou, Yuxiang Peng, Li Zheng, Chong Teng, Donghong Ji, Zhuang Li*

Main category: cs.AI

TL;DR: EVOREFUSE is a prompt optimization method using evolutionary algorithms to generate diverse pseudo-malicious instructions, improving LLM refusal evaluation and alignment training.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of LLMs over-refusing harmless queries due to conservative safety alignment, which harms user experience.

Method: EVOREFUSE employs an evolutionary algorithm with mutation and recombination to optimize prompts, maximizing refusal probability.

Result: Created datasets EVOREFUSE-TEST and EVOREFUSE-ALIGN, showing higher refusal rates, diversity, and improved alignment training outcomes.

Conclusion: EVOREFUSE effectively mitigates over-refusals and enhances LLM alignment without compromising safety.

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [463] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/pdf/2505.23474)
*Xiang Li, Haiyang Yu, Xinghua Zhang, Ziyang Huang, Shizhu He, Kang Liu, Jun Zhao, Fei Huang, Yongbin Li*

Main category: cs.AI

TL;DR: Socratic-PRMBench is introduced to evaluate Process Reward Models (PRMs) under six reasoning patterns, revealing deficiencies in current PRMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack systematic evaluation of PRMs under diverse reasoning patterns, leading to gaps in identifying errors.

Method: A new benchmark, Socratic-PRMBench, with 2995 flawed reasoning paths across six reasoning patterns, is used to test PRMs and LLMs.

Result: Experiments show significant weaknesses in current PRMs when evaluating reasoning steps under various patterns.

Conclusion: Socratic-PRMBench aims to improve PRM evaluation and guide future development by addressing current limitations.

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [464] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/pdf/2505.23486)
*Ke Weng, Lun Du, Sirui Li, Wangyue Lu, Haozhe Sun, Hengyu Liu, Tiancheng Zhang*

Main category: cs.AI

TL;DR: A survey on autoformalization, covering its role in AI and theorem proving, recent advances, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To explore autoformalization's potential in enhancing theorem proving and LLM outputs, driven by AI advancements.

Method: Examines autoformalization workflows, from data preprocessing to model evaluation, across mathematical domains.

Result: Highlights autoformalization's impact on LLM verifiability and reasoning, with key open-source resources identified.

Conclusion: Summarizes progress, open challenges, and future directions for autoformalization research.

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [465] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/pdf/2505.23518)
*Hangoo Kang, Jehyeok Yeon, Gagandeep Singh*

Main category: cs.AI

TL;DR: TRAP is a generative adversarial framework that manipulates AI decision-making using semantic injections, achieving 100% attack success on leading models without visible perturbations.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of autonomous AI systems to stealthy, cross-modal adversarial attacks that exploit semantic reasoning.

Method: TRAP combines negative prompt-based degradation and positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking.

Result: TRAP achieves a 100% attack success rate on models like LLaVA-34B, Gemma3, and Mistral-3.1, outperforming baselines.

Conclusion: The findings reveal a critical vulnerability in AI systems, emphasizing the need for defenses against semantic-level attacks.

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [466] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/pdf/2505.23519)
*Ruiqi He, Falk Lieder*

Main category: cs.AI

TL;DR: The study explores cognitive mechanisms like metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation to improve metacognitive reinforcement learning (MCRL) models, aiming to match human strategy discovery rates. While these mechanisms help, they don't fully bridge the gap.


<details>
  <summary>Details</summary>
Motivation: To understand why MCRL models learn planning strategies slower than humans and identify cognitive mechanisms that could narrow this gap.

Method: Incorporated intrinsic metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation into MCRL models, analyzing their impact on strategy discovery in planning tasks.

Result: The mechanisms facilitated strategy discovery but didn't fully match human performance, revealing individual differences in their usage and effectiveness.

Conclusion: Further exploration of additional cognitive factors is needed to fully explain human strategy discovery rates.

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [467] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/pdf/2505.23536)
*Janik-Vasily Benzin, Gyunam Park, Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: The paper introduces a formal basis for synchronized model and event abstraction to maintain grounding in real-world behavior during process intelligence projects.


<details>
  <summary>Details</summary>
Motivation: Current techniques lack synchronization between model and event abstraction, leading to loss of real-world behavior insights.

Method: The approach proves equivalence between abstracting a process model and discovering a model from an abstracted event log, using behavioral profile abstraction.

Result: The paper demonstrates feasibility with a novel event abstraction technique.

Conclusion: Synchronized abstraction enhances analysis insights by preserving real-world behavior.

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [468] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/pdf/2505.23559)
*Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You*

Main category: cs.AI

TL;DR: SafeScientist is a framework enhancing safety and ethics in AI-driven science, outperforming traditional methods by 35% in safety while maintaining scientific quality.


<details>
  <summary>Details</summary>
Motivation: Address ethical and safety concerns in AI-driven scientific discovery.

Method: Integrates defensive mechanisms like prompt monitoring, ethical review, and introduces SciSafetyBench for evaluation.

Result: 35% safety improvement without compromising scientific output; robustness validated against adversarial attacks.

Conclusion: SafeScientist effectively balances safety and scientific productivity, with tools and benchmarks for broader adoption.

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [469] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/pdf/2505.23575)
*Benjamin Arnav, Pablo Bernabeu-Pérez, Nathan Helm-Burger, Tim Kostolansky, Hannes Whittingham, Mary Phuong*

Main category: cs.AI

TL;DR: CoT monitoring improves detection of harmful AI actions but can be misled by deceptive reasoning. A hybrid protocol combining reasoning and output scoring outperforms both CoT and action-only monitoring.


<details>
  <summary>Details</summary>
Motivation: To ensure AI models do not take harmful actions unnoticed by improving monitoring methods.

Method: Compare Chain-of-Thought (CoT) monitoring to action-only monitoring in a red-teaming setup. Introduce a hybrid protocol scoring both reasoning and outputs.

Result: CoT monitoring improves detection by up to 27 percentage points but can be deceived. The hybrid protocol consistently outperforms both methods, with detection rates over four times higher for subtle deception.

Conclusion: Hybrid monitoring, combining reasoning and output scoring, is more effective for detecting harmful AI actions.

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [470] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/pdf/2505.23596)
*Linqiang Guo, Wei Liu, Yi Wen Heng, Tse-Hsun, Chen, Yang Wang*

Main category: cs.AI

TL;DR: MAPLE is a state-aware multi-agent framework for mobile GUI agents, using Finite State Machines (FSMs) to model app interactions, improving task success, recovery, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing mobile GUI agents lack structured navigation flow and error recovery, limiting their context understanding and robustness.

Method: MAPLE abstracts app interactions as FSMs, with specialized agents for planning, execution, verification, error recovery, and knowledge retention.

Result: MAPLE outperforms baselines, improving task success by 12%, recovery by 13.8%, and action accuracy by 6.5%.

Conclusion: Structured state modeling via FSMs enhances GUI agent performance and can be integrated as a lightweight memory layer for future architectures.

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [471] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/pdf/2505.23667)
*Lang Cao, Jingxian Xu, Hanbing Liu, Jinyu Wang, Mengyu Zhou, Haoyu Dong, Shi Han, Dongmei Zhang*

Main category: cs.AI

TL;DR: Formula Tuning (Fortune) is a reinforcement learning framework that trains LMs to generate spreadsheet formulas for table reasoning, improving performance on numerical and symbolic tasks.


<details>
  <summary>Details</summary>
Motivation: Large LMs struggle with numerical/symbolic reasoning over tabular data, despite their general reasoning abilities. Spreadsheet formulas offer rich but underutilized reasoning patterns.

Method: Proposes Formula Tuning, an RL framework using binary answer correctness as a reward signal to train LMs for formula generation, reducing reliance on supervised annotations.

Result: Substantially enhances LM performance on multi-step numerical/symbolic reasoning, enabling a 7B model to outperform O1 on table understanding benchmarks.

Conclusion: Formula-driven RL has potential to advance symbolic table reasoning in LMs, as demonstrated by improved performance on complex tasks.

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [472] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/pdf/2505.23695)
*Ran Zhang, Mohannad Elhamod*

Main category: cs.AI

TL;DR: The paper introduces a modular LLM-based agentic system for automating data-to-dashboard pipelines, outperforming GPT-4o in insightfulness and domain relevance.


<details>
  <summary>Details</summary>
Motivation: To enhance data analysis and visualization by leveraging LLMs' capabilities, simulating human analytical reasoning without relying on rigid templates.

Method: Uses modular LLM agents for domain detection, concept extraction, multi-perspective analysis, and iterative self-reflection. Evaluated on diverse datasets.

Result: Outperforms GPT-4o in insightfulness, domain relevance, and analytical depth, validated by tailored metrics and human assessment.

Conclusion: Presents a novel pipeline for data-to-dashboard automation, enabling human-in-the-loop validation in business analytics.

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [473] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/pdf/2505.23703)
*Ruida Wang, Yuxin Li, Yi R., Fung, Tong Zhang*

Main category: cs.AI

TL;DR: The paper introduces **NL-FL HybridReasoning**, a framework combining Natural Language (NL) and Formal Language (FL) to enhance math reasoning in LLMs, achieving higher accuracy than NL baselines.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLMs struggle to integrate new capabilities like FL into NL math reasoning due to disparities in problem structure and reasoning format.

Method: The framework includes *NL-FL Problem Alignment* to reformulate NL QA problems as FL existence theorems, *Mixed Problem Input* for handling both QA and existence problems, and *Answer Extraction* to bridge output gaps.

Result: The framework achieves 89.80% and 84.34% accuracy on MATH-500 and AMC benchmarks, outperforming NL baselines by 4.60% and 4.82%.

Conclusion: **HybridReasoning** effectively integrates FL into NL math reasoning, solving problems unsolved by NL baselines.

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [474] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/pdf/2505.23746)
*Hugo Henry, Kelly Cohen*

Main category: cs.AI

TL;DR: The study explores Genetic Fuzzy Systems (GFS) for modeling airfoil self-noise, comparing methods like brute force TSK, GFT, and FCM-based clustering to simplify complexity.


<details>
  <summary>Details</summary>
Motivation: Addressing airfoil self-noise is crucial for aerospace, automotive, and drone applications, requiring accurate modeling tools.

Method: The paper evaluates brute force TSK, cascading GFT, and FCM-based clustered approaches using the Airfoil Self Noise dataset.

Result: Clustering-assisted fuzzy inference proves effective for modeling complex aeroacoustic phenomena.

Conclusion: The study demonstrates the viability of GFS, particularly clustered approaches, for aeroacoustic regression tasks.

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [475] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/pdf/2505.23762)
*Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai*

Main category: cs.AI

TL;DR: ZeroGUI is an online learning framework for GUI Agents that eliminates manual annotations and adapts to dynamic environments using VLM-based task generation, reward estimation, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing GUI Agent approaches rely on offline learning with high-quality manual annotations and struggle with dynamic environments.

Method: ZeroGUI integrates VLM-based task generation, reward estimation, and two-stage online reinforcement learning.

Result: Experiments show ZeroGUI significantly improves performance in OSWorld and AndroidLab environments.

Conclusion: ZeroGUI offers a scalable, zero-human-cost solution for training GUI Agents, addressing key limitations of existing methods.

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


### [476] [Forms of Understanding for XAI-Explanations](https://arxiv.org/pdf/2311.08760)
*Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede*

Main category: cs.AI

TL;DR: The paper explores the concept of 'understanding' in Explainable AI (XAI), proposing a model of forms of understanding (enabledness and comprehension) and their dynamics in explanations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a clear definition of 'understanding' in XAI and interdisciplinary fields, aiming to systematize forms of understanding for better explanations.

Method: Interdisciplinary approach combining computer science, linguistics, sociology, philosophy, and psychology to define and assess understanding in explanations.

Result: Identifies two types of understanding (enabledness and comprehension) with varying degrees (shallow to deep), highlighting their interdependence in achieving agency for users.

Conclusion: The systematization of understanding forms provides a foundation for addressing challenges in XAI explanations, emphasizing the need for deeper comprehension and enabledness.

Abstract: Explainability has become an important topic in computer science and
artificial intelligence, leading to a subfield called Explainable Artificial
Intelligence (XAI). The goal of providing or seeking explanations is to achieve
(better) 'understanding' on the part of the explainee. However, what it means
to 'understand' is still not clearly defined, and the concept itself is rarely
the subject of scientific investigation. This conceptual article aims to
present a model of forms of understanding for XAI-explanations and beyond. From
an interdisciplinary perspective bringing together computer science,
linguistics, sociology, philosophy and psychology, a definition of
understanding and its forms, assessment, and dynamics during the process of
giving everyday explanations are explored. Two types of understanding are
considered as possible outcomes of explanations, namely enabledness, 'knowing
how' to do or decide something, and comprehension, 'knowing that' -- both in
different degrees (from shallow to deep). Explanations regularly start with
shallow understanding in a specific domain and can lead to deep comprehension
and enabledness of the explanandum, which we see as a prerequisite for human
users to gain agency. In this process, the increase of comprehension and
enabledness are highly interdependent. Against the background of this
systematization, special challenges of understanding in XAI are discussed.

</details>


### [477] [Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval](https://arxiv.org/pdf/2403.18405)
*Shengjie Ma, Qi Chu, Jiaxin Mao, Xuhui Jiang, Haozhe Duan, Chong Chen*

Main category: cs.AI

TL;DR: A novel few-shot approach using LLMs for interpretable and expert-aligned legal case relevance judgments, improving accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Traditional legal case relevance judgments require expertise and time, and existing methods lack interpretability. LLMs offer potential but remain unexplored for reliable legal case retrieval.

Method: Decomposes judgment into stages mimicking human annotators, incorporates expert reasoning, and ensures interpretable labeling. Uses few-shot learning with LLMs.

Result: LLMs produce reliable, valid relevance judgments comparable to human experts. Minimal supervision enables knowledge transfer to smaller models.

Conclusion: The approach enhances legal case retrieval with interpretability and accuracy, leveraging LLMs effectively.

Abstract: Determining which legal cases are relevant to a given query involves
navigating lengthy texts and applying nuanced legal reasoning. Traditionally,
this task has demanded significant time and domain expertise to identify key
Legal Facts and reach sound juridical conclusions. In addition, existing data
with legal case similarities often lack interpretability, making it difficult
to understand the rationale behind relevance judgments. With the growing
capabilities of large language models (LLMs), researchers have begun
investigating their potential in this domain. Nonetheless, the method of
employing a general large language model for reliable relevance judgments in
legal case retrieval remains largely unexplored. To address this gap in
research, we propose a novel few-shot approach where LLMs assist in generating
expert-aligned interpretable relevance judgments. The proposed approach
decomposes the judgment process into several stages, mimicking the workflow of
human annotators and allowing for the flexible incorporation of expert
reasoning to improve the accuracy of relevance judgments. Importantly, it also
ensures interpretable data labeling, providing transparency and clarity in the
relevance assessment process. Through a comparison of relevance judgments made
by LLMs and human experts, we empirically demonstrate that the proposed
approach can yield reliable and valid relevance assessments. Furthermore, we
demonstrate that with minimal expert supervision, our approach enables a large
language model to acquire case analysis expertise and subsequently transfers
this ability to a smaller model via annotation-based knowledge distillation.

</details>


### [478] [OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via Omniverse Computation Balance](https://arxiv.org/pdf/2407.20761)
*Yongqiang Yao, Jingru Tan, Feizhao Zhang, Jiahao Hu, Yazhe Niu, Xin Jin, Bo Li, Pengfei Liu, Ruihao Gong, Dahua Lin, Ningyi Xu*

Main category: cs.AI

TL;DR: The paper addresses computational load imbalance in vision-language instruction-tuning models during large-scale 3D parallel training, proposing a balanced training framework for improved efficiency.


<details>
  <summary>Details</summary>
Motivation: The vision and language parts of these models are inherently heterogeneous, causing imbalanced computation across devices during distributed training.

Method: The solution rebalances computation from data, model, and memory perspectives: grouping instances into balanced mini-batches, using a search-based model partitioning method, and adaptive memory re-computation.

Result: The method reduces training time by 1.8× compared to InternVL-Chat and shows efficacy across various models and datasets.

Conclusion: The proposed omniverse balanced training framework effectively addresses computational imbalance, improving training efficiency and generalizability.

Abstract: Vision-language instruction-tuning models have recently achieved significant
performance improvements. In this work, we discover that large-scale 3D
parallel training on those models leads to an imbalanced computation load
across different devices. The vision and language parts are inherently
heterogeneous: their data distribution and model architecture differ
significantly, which affects distributed training efficiency. To address this
issue, we rebalance the computational load from data, model, and memory
perspectives, achieving more balanced computation across devices. Specifically,
for the data, instances are grouped into new balanced mini-batches within and
across devices. A search-based method is employed for the model to achieve a
more balanced partitioning. For memory optimization, we adaptively adjust the
re-computation strategy for each partition to utilize the available memory
fully. These three perspectives are not independent but are closely connected,
forming an omniverse balanced training framework. Extensive experiments are
conducted to validate the effectiveness of our method. Compared with the
open-source training code of InternVL-Chat, training time is reduced greatly,
achieving about 1.8$\times$ speed-up. Our method's efficacy and
generalizability are further validated across various models and datasets.
Codes will be released at https://github.com/ModelTC/OmniBal.

</details>


### [479] [On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents](https://arxiv.org/pdf/2408.00989)
*Jen-tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Michael R. Lyu, Maarten Sap*

Main category: cs.AI

TL;DR: The paper explores the resilience of multi-agent systems with faulty agents, identifies the hierarchical structure as most resilient, and proposes two methods (Challenger and Inspector) to improve resilience.


<details>
  <summary>Details</summary>
Motivation: To understand how faulty or malicious agents impact multi-agent systems and develop strategies to enhance system resilience.

Method: Simulates faulty agents using AutoTransform and AutoInject, tests resilience on four tasks, and introduces Challenger and Inspector to mitigate errors.

Result: Hierarchical structure (A→(B↔C)) shows lowest performance drop (5.5%). Challenger and Inspector recover up to 96.4% of errors.

Conclusion: Hierarchical structures and additional mechanisms (Challenger, Inspector) significantly improve system resilience against faulty agents.

Abstract: Large language model-based multi-agent systems have shown great abilities
across various tasks due to the collaboration of expert agents, each focusing
on a specific domain. However, the impact of clumsy or even malicious
agents--those who frequently make errors in their tasks--on the overall
performance of the system remains underexplored. This paper investigates: (1)
What is the resilience of various system structures (e.g.,
A$\rightarrow$B$\rightarrow$C, A$\leftrightarrow$B$\leftrightarrow$C) under
faulty agents, on different downstream tasks? (2) How can we increase system
resilience to defend against these agents? To simulate faulty agents, we
propose two approaches--AutoTransform and AutoInject--which introduce mistakes
into the agents' responses. Experiments on four downstream tasks using six
systems show that the "hierarchical" structure, i.e.,
A$\rightarrow$(B$\leftrightarrow$C), exhibits superior resilience with the
lowest performance drop of 5.5%, compared to 10.5% and 23.7% of other two
structures. To further improve resilience, we introduce (1) Challenger, that
introduces a mechanism for each agent to challenge others' outputs, and (2)
Inspector, an additional agent to review and correct messages, recovering up to
96.4% errors made by faulty agents. Our code and data are available at
https://github.com/CUHK-ARISE/MAS-Resilience.

</details>


### [480] [Large Language Model Driven Recommendation](https://arxiv.org/pdf/2408.10946)
*Anton Korikov, Scott Sanner, Yashar Deldjoo, Zhankui He, Julian McAuley, Arnau Ramisa, Rene Vidal, Mahesh Sathiamoorthy, Atoosa Kasrizadeh, Silvia Milano, Francesco Ricci*

Main category: cs.AI

TL;DR: The paper explores how LLMs enable personalized recommendation systems through natural language interactions, covering data sources, techniques, and architectures for conversational recommenders.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs' NL reasoning for building highly personalized recommendation systems via interactive dialogues.

Method: Presents a taxonomy of data sources, reviews LLM techniques (encoder-only and autoregressive), and discusses multi-module architectures and conversational recommender systems.

Result: LLMs offer novel opportunities for nuanced user preference modeling and interactive recommendation dialogues.

Conclusion: LLMs can revolutionize recommendation systems by enabling natural language-driven, interactive, and personalized user experiences.

Abstract: While previous chapters focused on recommendation systems (RSs) based on
standardized, non-verbal user feedback such as purchases, views, and clicks --
the advent of LLMs has unlocked the use of natural language (NL) interactions
for recommendation. This chapter discusses how LLMs' abilities for general NL
reasoning present novel opportunities to build highly personalized RSs -- which
can effectively connect nuanced and diverse user preferences to items,
potentially via interactive dialogues. To begin this discussion, we first
present a taxonomy of the key data sources for language-driven recommendation,
covering item descriptions, user-system interactions, and user profiles. We
then proceed to fundamental techniques for LLM recommendation, reviewing the
use of encoder-only and autoregressive LLM recommendation in both tuned and
untuned settings. Afterwards, we move to multi-module recommendation
architectures in which LLMs interact with components such as retrievers and RSs
in multi-stage pipelines. This brings us to architectures for conversational
recommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where
each turn presents an opportunity not only to make recommendations, but also to
engage with the user in interactive preference elicitation, critiquing, and
question-answering.

</details>


### [481] [Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents](https://arxiv.org/pdf/2408.16081)
*Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi*

Main category: cs.AI

TL;DR: LELMA integrates LLMs with formal logic to validate and refine reasoning, improving accuracy in open-ended tasks like game theory.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' susceptibility to mathematical and logical errors in unstructured, open-ended reasoning tasks.

Method: LELMA combines an LLM-Reasoner, LLM-Translator, and Solver to autoformalize reasoning into logic for validation.

Result: LELMA detects errors effectively and enhances reasoning correctness, especially in advanced models like GPT-4o.

Conclusion: LELMA improves logical soundness in LLM reasoning but faces challenges in autoformalization and ambiguous task evaluation.

Abstract: Large language models (LLMs) are increasingly explored as general-purpose
reasoners, particularly in agentic contexts. However, their outputs remain
prone to mathematical and logical errors. This is especially challenging in
open-ended tasks, where unstructured outputs lack explicit ground truth and may
contain subtle inconsistencies. To address this issue, we propose
Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs
with formal logic to enable validation and refinement of natural language
reasoning. LELMA comprises three components: an LLM-Reasoner, an
LLM-Translator, and a Solver, and employs autoformalization to translate
reasoning into logic representations, which are then used to assess logical
validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as
testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro)
and advanced (GPT-4o) models in generating logically sound reasoning. LELMA
achieves high accuracy in error detection and improves reasoning correctness
via self-refinement, particularly in GPT-4o. The study also highlights
challenges in autoformalization accuracy and in evaluation of inherently
ambiguous open-ended reasoning tasks.

</details>


### [482] [CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning](https://arxiv.org/pdf/2410.02229)
*Huimu Yu, Xing Wu, Haotian Xu, Debing Zhang, Songlin Hu*

Main category: cs.AI

TL;DR: CodePMP introduces a scalable preference model pretraining pipeline using synthesized code-preference pairs to improve reasoning in LLMs via efficient reward model finetuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of enhancing LLMs' reasoning abilities due to scarce high-quality preference data for RLHF.

Method: Pretraining preference models on large-scale synthesized code-preference pairs from public source code.

Result: Significant improvements in reasoning performance on mathematical and logical tasks (GSM8K, MATH, ReClor, LogiQA2.0).

Conclusion: Scalable preference model pretraining is crucial for efficient reward modeling and better reasoning in LLMs.

Abstract: Large language models (LLMs) have made significant progress in natural
language understanding and generation, driven by scalable pretraining and
advanced finetuning. However, enhancing reasoning abilities in LLMs,
particularly via reinforcement learning from human feedback (RLHF), remains
challenging due to the scarcity of high-quality preference data, which is
labor-intensive to annotate and crucial for reward model (RM) finetuning. To
alleviate this issue, we introduce CodePMP, a scalable preference model
pretraining (PMP) pipeline that utilizes a large corpus of synthesized
code-preference pairs from publicly available high-quality source code. CodePMP
improves RM finetuning efficiency by pretraining preference models on
large-scale synthesized code-preference pairs. We evaluate CodePMP on
mathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,
LogiQA2.0), consistently showing significant improvements in reasoning
performance of LLMs and highlighting the importance of scalable preference
model pretraining for efficient reward modeling.

</details>


### [483] [GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation](https://arxiv.org/pdf/2410.08475)
*Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro*

Main category: cs.AI

TL;DR: GIVE is a novel reasoning method combining parametric and non-parametric memories to enhance LLM reasoning with minimal external input, outperforming larger models in some cases.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing methods (context prompting, RL) and costly nonparametric knowledge bases in LLM reasoning, especially for niche domains.

Method: GIVE merges parametric and non-parametric memories, guiding LLMs to observe, reflect, and speak for accurate reasoning.

Result: Boosts LLM performance, enables smaller models to surpass larger ones, and improves accuracy (up to 88.2%) on new problems.

Conclusion: GIVE is a training-free, interpretable method effective for scientific and open-domain tasks, adaptable to varying knowledge sources.

Abstract: Existing approaches based on context prompting or reinforcement learning (RL)
to improve the reasoning capacities of large language models (LLMs) depend on
the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT).
However, no matter the size of LLMs, certain problems cannot be resolved in a
single forward pass. Meanwhile, agent-based reasoning systems require access to
a comprehensive nonparametric knowledge base, which is often costly or not
feasible for use in scientific and niche domains. We present Graph Inspired
Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric
and non-parametric memories to improve accurate reasoning with minimal external
input. GIVE guides the LLM agent to select the most pertinent expert data
(observe), engage in query-specific divergent thinking (reflect), and then
synthesize this information to produce the final output (speak). Extensive
experiments demonstrated the following benefits of our framework: (1) GIVE
boosts the performance of LLMs across various sizes. (2) In some scenarios,
GIVE allows smaller LLMs to surpass larger, more sophisticated ones in
scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific
and open-domain assessments. (4) GIVE is a training-free method that enables
LLMs to tackle new problems that extend beyond their training data (up to 43.5%
-> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using
both restricted (very small) and noisy (very large) knowledge sources,
accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes.
(6) The reasoning process involved in GIVE is fully interpretable.

</details>


### [484] [Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning](https://arxiv.org/pdf/2410.17885)
*Linger Deng, Linghao Zhu, Yuliang Liu, Yu Wang, Qunyi Xie, Jingjing Wu, Gang Zhang, Yingying Zhu, Xiang Bai*

Main category: cs.AI

TL;DR: TR-CoT framework improves geometric reasoning in LMMs by synthesizing theorem-grounded diagrams and refining QA pairs, outperforming baselines and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Address limitations in geometric reasoning due to insufficient CoT image-text data and lack of diversity/precision in existing methods.

Method: Two-stage TR-CoT: TR-Engine synthesizes diagrams with descriptions, TR-Reasoner refines QA pairs via reverse reasoning and cross-validation.

Result: 24.5% increase in logical consistency; outperforms baselines in MathVista (10.1%) and GeoQA (4.7%), surpassing GPT-4o.

Conclusion: TR-CoT enhances geometric reasoning, expands theorem coverage, and corrects misunderstandings, proving effective for LMMs.

Abstract: Large Multimodal Models (LMMs) face limitations in geometric reasoning due to
insufficient Chain of Thought (CoT) image-text training data. While existing
approaches leverage template-based or LLM-assisted methods for geometric CoT
data creation, they often face challenges in achieving both diversity and
precision. To bridge this gap, we introduce a two-stage Theorem-Validated
Reverse Chain-of-Thought Reasoning Synthesis (TR-CoT) framework. The first
stage, TR-Engine, synthesizes theorem-grounded geometric diagrams with
structured descriptions and properties. The second stage, TR-Reasoner, employs
reverse reasoning to iteratively refine question-answer pairs by
cross-validating geometric properties and description fragments. Our approach
expands theorem-type coverage, corrects long-standing misunderstandings, and
enhances geometric reasoning. Fine-grained CoT improves theorem understanding
and increases logical consistency by 24.5%. Our best models surpass the
baselines in MathVista and GeoQA by 10.1% and 4.7%, outperforming advanced
closed-source models like GPT-4o.

</details>


### [485] [Generative Agents for Multi-Agent Autoformalization of Interaction Scenarios](https://arxiv.org/pdf/2412.08805)
*Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi*

Main category: cs.AI

TL;DR: GAMA automates formalization of multi-agent simulations using LLMs, achieving high syntactic and semantic correctness.


<details>
  <summary>Details</summary>
Motivation: Manual development of multi-agent simulations requires domain expertise; GAMA aims to automate this process.

Method: Uses LLMs to autoformalize natural language descriptions into executable logic programs, validated syntactically and semantically.

Result: Achieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet, and 99.82% syntactic and 77% semantic correctness with GPT-4o.

Conclusion: GAMA effectively automates simulation formalization with high accuracy in strategy autoformalization.

Abstract: Multi-agent simulations are versatile tools for exploring interactions among
natural and artificial agents, but their development typically demands domain
expertise and manual effort. This work introduces the Generative Agents for
Multi-Agent Autoformalization (GAMA) framework, which automates the
formalization of interaction scenarios in simulations using agents augmented
with large language models (LLMs). To demonstrate the application of GAMA, we
use natural language descriptions of game-theoretic scenarios representing
social interactions, and we autoformalize them into executable logic programs
defining game rules, with syntactic correctness enforced through a solver-based
validation. To ensure runtime validity, an iterative, tournament-based
procedure tests the generated rules and strategies, followed by exact semantic
validation when ground truth outcomes are available. In experiments with 110
natural language descriptions across five 2x2 simultaneous-move games, GAMA
achieves 100% syntactic and 76.5% semantic correctness with Claude 3.5 Sonnet,
and 99.82% syntactic and 77% semantic correctness with GPT-4o. The framework
also shows high semantic accuracy in autoformalizing agents' strategies.

</details>


### [486] [MineStudio: A Streamlined Package for Minecraft AI Agent Development](https://arxiv.org/pdf/2412.18293)
*Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang*

Main category: cs.AI

TL;DR: MineStudio is an open-source package for developing autonomous agents in Minecraft, integrating seven key components to simplify the process.


<details>
  <summary>Details</summary>
Motivation: Minecraft's complexity makes it ideal for testing agent adaptability, but engineering challenges hinder development.

Method: MineStudio integrates simulator, data, model, pre-training, fine-tuning, inference, and benchmark components with a user-friendly API.

Result: The package streamlines agent development, allowing focus on algorithm innovation.

Conclusion: MineStudio is released as a comprehensive tool for autonomous agent development in Minecraft.

Abstract: Minecraft's complexity and diversity as an open world make it a perfect
environment to test if agents can learn, adapt, and tackle a variety of
unscripted tasks. However, the development and validation of novel agents in
this setting continue to face significant engineering challenges. This paper
presents MineStudio, an open-source software package designed to streamline the
development of autonomous agents in Minecraft. MineStudio represents the first
comprehensive integration of seven critical engineering components: simulator,
data, model, offline pre-training, online fine-tuning, inference, and
benchmark, thereby allowing users to concentrate their efforts on algorithm
innovation. We provide a user-friendly API design accompanied by comprehensive
documentation and tutorials. Our project is released at
https://github.com/CraftJarvis/MineStudio.

</details>


### [487] [Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents](https://arxiv.org/pdf/2501.18411)
*Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib*

Main category: cs.AI

TL;DR: Gravity-Bench-v1 is a benchmark for AI agents to discover physics in dynamic environments, including out-of-distribution cases, to test scientific generalization.


<details>
  <summary>Details</summary>
Motivation: To parallel the historical development of modern science by challenging AI agents with tasks involving gravitational dynamics and scientific reasoning.

Method: Uses rigorous gravitational dynamics simulations, out-of-distribution cases, and requires agents to plan data collection and perform dynamic analysis.

Result: The benchmark is challenging for baseline AI agents and includes reference solutions for performance calibration.

Conclusion: Gravity-Bench-v1 and future extensions aim to track AI progress in scientific discovery capabilities.

Abstract: Modern science emerged from reasoning over repeatedly-observed planetary
motions. We present Gravity-Bench-v1, an environment-based benchmark that
challenges AI agents on tasks that parallel this historical development.
Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within
a dynamic environment, using rigorous gravitational dynamics simulations.
Gravity-Bench includes out-of-distribution cases, i.e. with physics that
deviates from the real world, to evaluate true scientific generalization
capabilities. Agents must plan to collect data within an experimental budget
and must perform a dynamic form of data analysis and reasoning to solve tasks
efficiently. Our benchmark admits an open-ended space of solutions. Reference
solutions for each task are provided to calibrate AI performance against human
expertise. Technically at an upper-undergraduate level, our benchmark proves
challenging to baseline AI agents. Gravity-Bench-v1 and planned extensions
should help map out AI progress towards scientific discovery capabilities.

</details>


### [488] [MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems](https://arxiv.org/pdf/2501.19318)
*Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou*

Main category: cs.AI

TL;DR: MINDSTORES enhances LLM-based planning for embodied agents by using past experiences to refine mental models, improving robustness in complex environments like Minecraft.


<details>
  <summary>Details</summary>
Motivation: LLMs lack the ability to learn from experience, limiting their effectiveness in open-world environments. MINDSTORES addresses this by enabling agents to build and leverage persistent mental models.

Method: The framework maintains a database of past experiences as natural language embeddings (state, task, plan, outcome), which an LLM planner retrieves and reasons over for plan refinement.

Result: Experiments in MineDojo show MINDSTORES outperforms memory-based LLM planners in learning and applying knowledge while retaining zero-shot flexibility.

Conclusion: MINDSTORES advances embodied AI by enabling continuous learning through natural experience, improving planning robustness.

Abstract: While large language models (LLMs) have shown promising capabilities as
zero-shot planners for embodied agents, their inability to learn from
experience and build persistent mental models limits their robustness in
complex open-world environments like Minecraft. We introduce MINDSTORES, an
experience-augmented planning framework that enables embodied agents to build
and leverage mental models through natural interaction with their environment.
Drawing inspiration from how humans construct and refine cognitive mental
models, our approach extends existing zero-shot LLM planning by maintaining a
database of past experiences that informs future planning iterations. The key
innovation is representing accumulated experiences as natural language
embeddings of (state, task, plan, outcome) tuples, which can then be
efficiently retrieved and reasoned over by an LLM planner to generate insights
and guide plan refinement for novel states and tasks. Through extensive
experiments in the MineDojo environment, a simulation environment for agents in
Minecraft that provides low-level controls for Minecraft, we find that
MINDSTORES learns and applies its knowledge significantly better than existing
memory-based LLM planners while maintaining the flexibility and generalization
benefits of zero-shot approaches, representing an important step toward more
capable embodied AI systems that can learn continuously through natural
experience.

</details>


### [489] [The Value of Information in Human-AI Decision-making](https://arxiv.org/pdf/2502.06152)
*Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman*

Main category: cs.AI

TL;DR: The paper introduces a decision-theoretic framework and a novel explanation technique (ILIV-SHAP) to improve human-AI collaboration by identifying complementary information. It validates the approach through a study, showing better error reduction compared to non-AI and vanilla SHAP methods.


<details>
  <summary>Details</summary>
Motivation: To enhance collaborative decision-making between humans and AI by understanding and leveraging complementary information and strategies.

Method: Develops a decision-theoretic framework and ILIV-SHAP, an adapted SHAP explanation technique, to highlight human-complementing information. Validates through a human-AI decision-making study.

Result: Demonstrates that the framework can identify AI models that best complement human decisions and that ILIV-SHAP reduces errors more effectively than vanilla SHAP.

Conclusion: The proposed framework and ILIV-SHAP technique effectively improve human-AI collaboration by focusing on complementary information, leading to better decision outcomes.

Abstract: Multiple agents -- including humans and AI models -- are increasingly
combined to make decisions with the expectation of achieving complementary
performance, where the decisions they make together outperform those made
individually. However, knowing how to improve the performance of collaborating
agents is often difficult without knowing more about what particular
information and strategies each agent employs. With a focus on human-AI
pairings, we contribute a decision-theoretic framework for characterizing the
value of information -- and consequently, opportunities for agents to better
exploit available information -- in AI-assisted decision workflows. We present
a novel explanation technique (ILIV-SHAP) that adapts SHAP explanations to
highlight human-complementing information. We validate the effectiveness of the
framework and ILIV-SHAP through a study of human-AI decision-making. We show
that our measure of complementary information can be used to identify which AI
model will best complement human decisions. We also find that presenting
ILIV-SHAP with AI predictions leads to reliably greater reductions in error
over non-AI assisted decisions more than vanilla SHAP.

</details>


### [490] [HPS: Hard Preference Sampling for Human Preference Alignment](https://arxiv.org/pdf/2502.14400)
*Xiandong Zou, Wanyu Lin, Yuchen Li, Pan Zhou*

Main category: cs.AI

TL;DR: HPS is a new framework for aligning LLM responses with human preferences, focusing on rejecting harmful and dispreferred responses efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods like PL and BT struggle with harmful content, inefficiency, and high computational costs.

Method: HPS uses a training loss prioritizing preferred responses and rejecting harmful ones, with a focus on 'hard' dispreferred responses. It employs Monte Carlo sampling for efficiency.

Result: HPS improves reward margins and reduces harmful content, matching BLEU and reward scores of existing methods.

Conclusion: HPS offers a robust and efficient solution for human preference alignment in LLMs.

Abstract: Aligning Large Language Model (LLM) responses with human preferences is vital
for building safe and controllable AI systems. While preference optimization
methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown
promise, they face challenges such as poor handling of harmful content,
inefficient use of dispreferred responses, and, specifically for PL, high
computational costs. To address these issues, we propose Hard Preference
Sampling (HPS), a novel framework for robust and efficient human preference
alignment. HPS introduces a training loss that prioritizes the most preferred
response while rejecting all dispreferred and harmful ones. It emphasizes
"hard" dispreferred responses -- those closely resembling preferred ones -- to
enhance the model's rejection capabilities. By leveraging a single-sample Monte
Carlo sampling strategy, HPS reduces computational overhead while maintaining
alignment quality. Theoretically, HPS improves sample efficiency over existing
PL methods and maximizes the reward margin between preferred and dispreferred
responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety
datasets validate HPS's effectiveness, achieving comparable BLEU and reward
scores while greatly improving reward margins and thus reducing harmful content
generation.

</details>


### [491] [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/pdf/2502.17541)
*Michal Bravansky, Vaclav Kubon, Suhas Hariharan, Robert Kirk*

Main category: cs.AI

TL;DR: A domain-agnostic method for dataset featurization using LLMs is proposed, offering precise control over feature extraction and maintaining human-like descriptive accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of simple feature extraction methods like prompting, which often fail to produce accurate, versatile, and controlled descriptions for diverse datasets.

Method: Optimizes binary feature selection by evaluating an LLM's ability to reconstruct original data using those features.

Result: Effective in dataset modeling, demonstrated through case studies on jailbreak tactics and human-aligned feature discovery, with scalable performance.

Conclusion: The method provides robust, scalable, and human-comparable dataset featurization, suitable for diverse applications.

Abstract: Interpreting data is central to modern research. Large language models (LLMs)
show promise in providing such natural language interpretations of data, yet
simple feature extraction methods such as prompting often fail to produce
accurate and versatile descriptions for diverse datasets and lack control over
granularity and scale. To address these limitations, we propose a
domain-agnostic method for dataset featurization that provides precise control
over the number of features extracted while maintaining compact and descriptive
representations comparable to human labeling. Our method optimizes the
selection of informative binary features by evaluating the ability of an LLM to
reconstruct the original data using those features. We demonstrate its
effectiveness in dataset modeling tasks and through two case studies: (1)
Constructing a feature representation of jailbreak tactics that compactly
captures both the effectiveness and diversity of a larger set of human-crafted
attacks; and (2) automating the discovery of features that align with human
preferences, achieving accuracy and robustness comparable to human-crafted
features. Moreover, we show that the pipeline scales effectively, improving as
additional features are sampled, making it suitable for large and diverse
datasets.

</details>


### [492] [ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced Multi-Hop QA](https://arxiv.org/pdf/2503.06951)
*Xinjie Zhao, Fan Gao, Xingyu Song, Yingjian Chen, Rui Yang, Yanran Fu, Yuyang Wang, Yusuke Iwasawa, Yutaka Matsuo, Irene Li*

Main category: cs.AI

TL;DR: ReAgent introduces a reversible multi-agent framework with backtracking to improve multi-hop QA by correcting errors mid-reasoning, achieving ~6% better performance.


<details>
  <summary>Details</summary>
Motivation: The irreversible nature of Chain-of-Thought reasoning in LLMs leads to error accumulation in multi-hop QA, necessitating a reversible solution.

Method: ReAgent uses a multi-agent collaborative framework with backtracking, text-based retrieval, and validation to enable reversible reasoning.

Result: Empirical evaluations show ~6% improvement over baseline models on three benchmarks.

Conclusion: ReAgent provides a robust, interpretable foundation for error-tolerant QA systems.

Abstract: Recent advances in large language models (LLMs) have significantly improved
multi-hop question answering (QA) through direct Chain-of-Thought (CoT)
reasoning. However, the irreversible nature of CoT leads to error accumulation,
making it challenging to correct mistakes in multi-hop reasoning. This paper
introduces ReAgent: a Reversible multi-Agent collaborative framework augmented
with explicit backtracking mechanisms, enabling reversible multi-hop reasoning.
By incorporating text-based retrieval, information aggregation and validation,
our system can detect and correct errors mid-reasoning, leading to more robust
and interpretable QA outcomes. The framework and experiments serve as a
foundation for future work on error-tolerant QA systems. Empirical evaluations
across three benchmarks indicate ReAgent's efficacy, yielding average about 6\%
improvements against baseline models.

</details>


### [493] [Hierarchical Neuro-Symbolic Decision Transformer](https://arxiv.org/pdf/2503.07148)
*Ali Baheri, Cecilia O. Alm*

Main category: cs.AI

TL;DR: A hierarchical neuro-symbolic control framework combines symbolic planning with a transformer-based policy for long-horizon decision-making under uncertainty, outperforming purely symbolic, neural, and hierarchical baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing long-horizon decision-making under uncertainty by leveraging the strengths of both symbolic reasoning (logical coherence) and neural models (adaptability).

Method: Uses a symbolic planner for high-level interpretable sequences and a transformer-based policy for low-level fine-grained actions, with a bidirectional interface to manage approximation errors.

Result: Empirical studies show superior performance in success and efficiency compared to purely symbolic, neural, and hierarchical baselines.

Conclusion: The framework effectively balances symbolic reasoning and neural adaptability, proving robust for sequential tasks.

Abstract: We present a hierarchical neuro-symbolic control framework that tightly
couples a classical symbolic planner with a transformer-based policy to address
long-horizon decision-making under uncertainty. At the high level, the planner
assembles an interpretable sequence of operators that guarantees logical
coherence with task constraints, while at the low level each operator is
rendered as a sub-goal token that conditions a decision transformer to generate
fine-grained actions directly from raw observations. This bidirectional
interface preserves the combinatorial efficiency and explainability of symbolic
reasoning without sacrificing the adaptability of deep sequence models, and it
permits a principled analysis that tracks how approximation errors from both
planning and execution accumulate across the hierarchy. Empirical studies in
stochastic grid-world domains demonstrate that the proposed method consistently
surpasses purely symbolic, purely neural and existing hierarchical baselines in
both success and efficiency, highlighting its robustness for sequential tasks.

</details>


### [494] [Text-to-Decision Agent: Offline Meta-Reinforcement Learning from Natural Language Supervision](https://arxiv.org/pdf/2504.15046)
*Shilin Zhang, Zican Hu, Wenhao Wu, Xinyi Xie, Jianxiang Tang, Chunlin Chen, Daoyi Dong, Yu Cheng, Zhenhong Sun, Zhi Wang*

Main category: cs.AI

TL;DR: T2DA is a framework for offline meta-RL that uses natural language to supervise learning, enabling zero-shot text-to-decision generation without expensive prior signals.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional offline meta-RL, which relies on costly or infeasible supervision signals for unseen tasks, by leveraging raw text as a broader source of guidance.

Method: Introduces a generalized world model for multi-task decision data and aligns text embeddings with decision embeddings via contrastive pre-training, similar to CLIP.

Result: T2DA outperforms baselines on MuJoCo and Meta-World benchmarks, demonstrating strong zero-shot generalization capabilities.

Conclusion: T2DA provides a scalable and effective solution for offline meta-RL by integrating natural language supervision, enabling practical zero-shot task generalization.

Abstract: Offline meta-RL usually tackles generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose
\textbf{T}ext-to-\textbf{D}ecision \textbf{A}gent (\textbf{T2DA}), a simple and
scalable framework that supervises offline meta-RL with natural language. We
first introduce a generalized world model to encode multi-task decision data
into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which
textual description goes with which decision embedding, effectively bridging
their semantic gap via contrastive language-decision pre-training and aligning
the text embeddings to comprehend the environment dynamics. After training the
text-conditioned generalist policy, the agent can directly realize zero-shot
text-to-decision generation in response to language instructions. Comprehensive
experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates
high-capacity zero-shot generalization and outperforms various types of
baselines.

</details>


### [495] [Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation](https://arxiv.org/pdf/2505.00612)
*D. Sculley, Will Cukierski, Phil Culliton, Sohier Dane, Maggie Demkin, Ryan Holbrook, Addison Howard, Paul Mooney, Walter Reade, Megan Risdal, Nate Keating*

Main category: cs.AI

TL;DR: The paper argues that traditional ML evaluation methods are inadequate for Generative AI, highlighting issues like unbounded input/output spaces, lack of ground truth, and feedback loops. It suggests AI Competitions as a gold standard for rigorous GenAI evaluation due to their anti-leakage measures.


<details>
  <summary>Details</summary>
Motivation: The crisis in empirical evaluation of Generative AI due to traditional ML methods' inadequacy and the challenges like leakage and contamination.

Method: Proposes leveraging AI Competitions, which have effective anti-leakage practices, as a model for rigorous GenAI evaluation.

Result: AI Competitions are identified as an underutilized but valuable resource for addressing GenAI evaluation challenges.

Conclusion: The field should adopt AI Competitions as the gold standard for empirical rigor in GenAI evaluation.

Abstract: In this position paper, we observe that empirical evaluation in Generative AI
is at a crisis point since traditional ML evaluation and benchmarking
strategies are insufficient to meet the needs of evaluating modern GenAI models
and systems. There are many reasons for this, including the fact that these
models typically have nearly unbounded input and output spaces, typically do
not have a well defined ground truth target, and typically exhibit strong
feedback loops and prediction dependence based on context of previous model
outputs. On top of these critical issues, we argue that the problems of leakage
and contamination are in fact the most important and difficult issues to
address for GenAI evaluations. Interestingly, the field of AI Competitions has
developed effective measures and practices to combat leakage for the purpose of
counteracting cheating by bad actors within a competition setting. This makes
AI Competitions an especially valuable (but underutilized) resource. Now is
time for the field to view AI Competitions as the gold standard for empirical
rigor in GenAI evaluation, and to harness and harvest their results with
according value.

</details>


### [496] [HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking](https://arxiv.org/pdf/2505.02322)
*Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu*

Main category: cs.AI

TL;DR: HyperTree Planning (HTP) improves LLMs' complex planning tasks using a hypertree-structured approach, achieving a 3.6x performance boost.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex planning due to long reasoning steps, diverse constraints, and multiple sub-tasks.

Method: HTP uses hypertree-structured outlines for hierarchical planning, employing divide-and-conquer strategies.

Result: HTP achieves state-of-the-art accuracy on the TravelPlanner benchmark, improving performance by 3.6x.

Conclusion: HTP effectively addresses complex planning challenges in LLMs, demonstrating significant performance gains.

Abstract: Recent advancements have significantly enhanced the performance of large
language models (LLMs) in tackling complex reasoning tasks, achieving notable
success in domains like mathematical and logical reasoning. However, these
methods encounter challenges with complex planning tasks, primarily due to
extended reasoning steps, diverse constraints, and the challenge of handling
multiple distinct sub-tasks. To address these challenges, we propose HyperTree
Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured
planning outlines for effective planning. The hypertree structure enables LLMs
to engage in hierarchical thinking by flexibly employing the divide-and-conquer
strategy, effectively breaking down intricate reasoning steps, accommodating
diverse constraints, and managing multiple distinct sub-tasks in a
well-organized manner. We further introduce an autonomous planning framework
that completes the planning process by iteratively refining and expanding the
hypertree-structured planning outlines. Experiments demonstrate the
effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner
benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement
over o1-preview.

</details>


### [497] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/pdf/2505.03475)
*Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang*

Main category: cs.AI

TL;DR: A novel stable arena framework (m-ELO and am-ELO) improves the ELO Rating System for AI model evaluation by addressing instability and annotator ability issues.


<details>
  <summary>Details</summary>
Motivation: Existing ELO-based frameworks for AI model evaluation suffer from instability and ignore annotator abilities, necessitating a more robust solution.

Method: Proposes m-ELO (MLE-based update) and am-ELO (incorporates annotator abilities) to enhance the ELO Rating System.

Result: Experiments show the framework ensures stability, robustness, and accuracy in evaluating LLMs.

Conclusion: The stable arena framework provides a more reliable and stable evaluation method for modern AI models.

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [498] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/pdf/2505.10034)
*Changzeng Fu, Zelin Fu, Qi Zhang, Xinhe Kuang, Jiacheng Dong, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: The paper introduces the MPDD Challenge to address gaps in depression detection by focusing on diverse age groups and individual differences, using multimodal data and a baseline model.


<details>
  <summary>Details</summary>
Motivation: Existing depression detection methods overlook age diversity and individual differences, limiting their effectiveness across populations.

Method: The challenge uses two datasets (MPDD-Elderly and MPDD-Young) and a baseline model that fuses audio, video, and individual difference data for depression detection.

Result: The proposed approach aims to improve personalized and accurate depression detection across diverse age groups.

Conclusion: The MPDD Challenge seeks to advance mental health research by fostering inclusive and tailored depression detection systems.

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [499] [LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners](https://arxiv.org/pdf/2505.11942)
*Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, ZhongZhi Li, Yingying Zhang, Le Song, Qianli Ma*

Main category: cs.AI

TL;DR: LifelongAgentBench is introduced as the first benchmark to evaluate lifelong learning in LLM agents, addressing limitations of static systems and proposing a group self-consistency mechanism for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack lifelong learning capabilities, and existing benchmarks fail to assess this. The need for adaptive, memory-capable agents drives the creation of LifelongAgentBench.

Method: LifelongAgentBench provides skill-grounded, interdependent tasks across three environments (Database, Operating System, Knowledge Graph) with automatic verification and modular extensibility. A group self-consistency mechanism is introduced to enhance learning.

Result: Experiments show conventional experience replay is ineffective for LLM agents due to irrelevant information and context limits. The proposed mechanism significantly boosts lifelong learning performance.

Conclusion: LifelongAgentBench aims to advance adaptive LLM agents by providing a unified benchmark and demonstrating the effectiveness of the new self-consistency mechanism.

Abstract: Lifelong learning is essential for intelligent agents operating in dynamic
environments. Current large language model (LLM)-based agents, however, remain
stateless and unable to accumulate or transfer knowledge over time. Existing
benchmarks treat agents as static systems and fail to evaluate lifelong
learning capabilities. We present LifelongAgentBench, the first unified
benchmark designed to systematically assess the lifelong learning ability of
LLM agents. It provides skill-grounded, interdependent tasks across three
interactive environments, Database, Operating System, and Knowledge Graph, with
automatic label verification, reproducibility, and modular extensibility.
Extensive experiments reveal that conventional experience replay has limited
effectiveness for LLM agents due to irrelevant information and context length
constraints. We further introduce a group self-consistency mechanism that
significantly improves lifelong learning performance. We hope
LifelongAgentBench will advance the development of adaptive, memory-capable LLM
agents.

</details>


### [500] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/pdf/2505.13774)
*Zidi Xiong, Shan Chen, Zhenting Qi, Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: The paper proposes a counterfactual intervention framework to evaluate the faithfulness of intermediate reasoning steps in Large Reasoning Models (LRMs), revealing selective faithfulness and frequent misalignment with draft conclusions.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable monitoring, interpretation, and control of LRMs by rigorously evaluating the faithfulness of their intermediate reasoning processes.

Method: A systematic counterfactual intervention framework assessing Intra-Draft Faithfulness (causal influence of steps) and Draft-to-Answer Faithfulness (logical consistency with draft conclusions).

Result: Experiments on six LRMs show selective faithfulness to reasoning steps and frequent misalignment with draft conclusions.

Conclusion: The findings highlight the need for more faithful and interpretable reasoning in advanced LRMs.

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [501] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/pdf/2505.14020)
*Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou*

Main category: cs.AI

TL;DR: DiMNet improves TKG reasoning by capturing subgraph interactions and distinguishing stable vs. active features, achieving up to 22.7% better MRR.


<details>
  <summary>Details</summary>
Motivation: Existing TKG methods ignore subgraph interactions and smooth features, limiting reasoning performance.

Method: DiMNet uses a multi-span evolution strategy and disentangle component to model subgraph interactions and separate active/stable features.

Result: DiMNet outperforms state-of-the-art methods by up to 22.7% in MRR on four datasets.

Conclusion: DiMNet effectively addresses limitations in TKG reasoning by capturing semantic evolution and feature disentanglement.

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [502] [PhyX: Does Your Model Have the "Wits" for Physical Reasoning?](https://arxiv.org/pdf/2505.15929)
*Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, Zhongwei Wan, Kai Zhang, Wendong Xu, Jing Xiong, Ping Luo, Wenhu Chen, Chaofan Tao, Zhuoqing Mao, Ngai Wong*

Main category: cs.AI

TL;DR: PhyX is a new benchmark for physics-grounded reasoning in visual scenarios, revealing significant gaps in current AI models' abilities compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on physical reasoning, a key aspect of intelligence, prompting the creation of PhyX.

Method: PhyX includes 3K multimodal questions across 6 physics domains, evaluated using state-of-the-art models and human experts.

Result: Top models like GPT-4o and Claude3.7-Sonnet perform poorly (32.5%-45.8% accuracy), lagging behind humans by over 29%.

Conclusion: Current models rely too much on memorization and math, lacking genuine physical understanding; PhyX provides a reproducible evaluation framework.

Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence:
physical reasoning, the integrated ability to combine domain knowledge,
symbolic reasoning, and understanding of real-world constraints. To address
this gap, we introduce PhyX: the first large-scale benchmark designed to assess
models capacity for physics-grounded reasoning in visual scenarios. PhyX
includes 3K meticulously curated multimodal questions spanning 6 reasoning
types across 25 sub-domains and 6 core physics domains: thermodynamics,
electromagnetism, mechanics, modern physics, optics, and wave\&acoustics. In
our comprehensive evaluation, even state-of-the-art models struggle
significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and
GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy
respectively-performance gaps exceeding 29% compared to human experts. Our
analysis exposes critical limitations in current models: over-reliance on
memorized disciplinary knowledge, excessive dependence on mathematical
formulations, and surface-level visual pattern matching rather than genuine
physical understanding. We provide in-depth analysis through fine-grained
statistics, detailed case studies, and multiple evaluation paradigms to
thoroughly examine physical reasoning capabilities. To ensure reproducibility,
we implement a compatible evaluation protocol based on widely-used toolkits
such as VLMEvalKit, enabling one-click evaluation. More details are available
on our project page: https://phyx-bench.github.io/.

</details>


### [503] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/pdf/2505.16223)
*Sangyong Lee, Subo Hwang, Dohoon Kim*

Main category: cs.AI

TL;DR: MADCluster is a model-agnostic anomaly detection framework using self-supervised clustering to address the 'hypersphere collapse' problem in deep learning-based methods. It clusters normal data into a single cluster and optimizes a new loss function, improving performance across architectures.


<details>
  <summary>Details</summary>
Motivation: To address the 'hypersphere collapse' problem in existing deep learning-based anomaly detection methods and provide a model-agnostic solution applicable to various architectures.

Method: MADCluster uses self-supervised clustering with three components: Base Embedder, Cluster Distance Mapping, and Sequence-wise Clustering. It introduces a 'One-directed Adaptive loss' for optimization.

Result: Experiments on four time series datasets show MADCluster improves performance of comparative models.

Conclusion: MADCluster is compatible with various architectures, enhancing model performance and addressing key limitations in anomaly detection.

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [504] [Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary](https://arxiv.org/pdf/2505.18325)
*Licheng Pan, Yongqi Tong, Xin Zhang, Xiaolu Zhang, Jun Zhou, Zhixuan Chu*

Main category: cs.AI

TL;DR: The paper addresses overrefusal in LLMs, where models reject legitimate queries due to over-conservative safety alignment. It introduces RASS, a framework to mitigate overrefusal by targeting prompts near safety boundaries.


<details>
  <summary>Details</summary>
Motivation: Overrefusal in LLMs stems from misalignment at safety boundaries, causing models to reject reasonable prompts. The study aims to analyze and mitigate this issue.

Method: The authors probe safety decision boundaries and develop RASS, an automated framework for prompt generation and selection near these boundaries using steering vectors.

Result: RASS effectively identifies and mitigates overrefusal, providing a precise view of model safety decisions and extending to multilingual scenarios.

Conclusion: The approach offers a robust solution to overrefusal, with potential applications in improving model safety and helpfulness across languages.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they often refuse to answer legitimate queries-a
phenomenon known as overrefusal. Overrefusal typically stems from
over-conservative safety alignment, causing models to treat many reasonable
prompts as potentially risky. To systematically understand this issue, we probe
and leverage the models'safety decision boundaries to analyze and mitigate
overrefusal. Our findings reveal that overrefusal is closely tied to
misalignment at these boundary regions, where models struggle to distinguish
subtle differences between benign and harmful content. Building on these
insights, we present RASS, an automated framework for prompt generation and
selection that strategically targets overrefusal prompts near the safety
boundary. By harnessing steering vectors in the representation space, RASS
efficiently identifies and curates boundary-aligned prompts, enabling more
effective and targeted mitigation of overrefusal. This approach not only
provides a more precise and interpretable view of model safety decisions but
also seamlessly extends to multilingual scenarios.We have explored the safety
decision boundaries of various LLMs and construct the MORBench evaluation set
to facilitate robust assessment of model safety and helpfulness across multiple
languages. Code and datasets will be released at
https://anonymous.4open.science/r/RASS-80D3.

</details>


### [505] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/pdf/2505.21887)
*Ahmed Heakl, Yahia Salaheldin Shaaban, Martin Takac, Salem Lahlou, Zangir Iklassov*

Main category: cs.AI

TL;DR: SVRPBench is the first open benchmark for stochastic vehicle routing, simulating real-world urban logistics challenges like congestion, delays, and accidents. It shows RL solvers degrade under uncertainty, while classical methods remain robust.


<details>
  <summary>Details</summary>
Motivation: To address the gap in benchmarks for stochastic vehicle routing, capturing real-world dynamics like congestion and delays.

Method: Developed SVRPBench with 500+ instances, simulating realistic conditions (e.g., time-dependent congestion, log-normal delays) and diverse scenarios (multi-depot, multi-vehicle).

Result: RL solvers (POMO, AM) degrade by over 20% under distributional shift, while classical and metaheuristic methods stay robust.

Conclusion: SVRPBench challenges the community to create solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [506] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/pdf/2505.22865)
*Susan Liang, Dejan Markovic, Israel D. Gebru, Steven Krenn, Todd Keebler, Jacob Sandakly, Frank Yu, Samuel Hassel, Chenliang Xu, Alexander Richard*

Main category: cs.SD

TL;DR: BinauralFlow is a flow-matching-based streaming binaural speech synthesis framework that addresses quality and streaming challenges in binaural rendering.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with rendering quality and streamable inference, requiring precise modeling of binaural cues, reverb, and ambient sounds.

Method: Proposes a conditional flow matching model and causal U-Net architecture for streaming inference, with a continuous pipeline for improved rendering.

Result: Outperforms SOTA methods, with a 42% confusion rate in perceptual studies, nearly matching real-world recordings.

Conclusion: BinauralFlow effectively synthesizes high-quality, streamable binaural audio, bridging the gap with real-world recordings.

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [507] [AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker Speech Dataset for Automatic Speech Diarization and Recognition](https://arxiv.org/pdf/2505.23036)
*Yuhang Dai, He Wang, Xingchen Li, Zihan Zhang, Shuiyuan Wang, Lei Xie, Xin Xu, Hongxiao Guo, Shaoji Zhang, Hui Bu, Wei Chen*

Main category: cs.SD

TL;DR: AISHELL-5 is the first open-source in-car multi-channel multi-speaker Mandarin ASR dataset, featuring real driving scenario recordings and environmental noise, with a baseline system for research advancement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-source datasets for in-car multi-speaker Mandarin ASR, enabling research in complex driving scenarios.

Method: The dataset includes multi-channel speech data from real driving scenarios and environmental noise. A baseline system with speech source separation and ASR modules is provided.

Result: Highlights challenges for mainstream ASR models, demonstrating the dataset's utility for advancing in-car ASR research.

Conclusion: AISHELL-5 establishes a benchmark for in-car ASR, fostering progress in complex driving scenario speech recognition.

Abstract: This paper delineates AISHELL-5, the first open-source in-car multi-channel
multi-speaker Mandarin automatic speech recognition (ASR) dataset. AISHLL-5
includes two parts: (1) over 100 hours of multi-channel speech data recorded in
an electric vehicle across more than 60 real driving scenarios. This audio data
consists of four far-field speech signals captured by microphones located on
each car door, as well as near-field signals obtained from high-fidelity
headset microphones worn by each speaker. (2) a collection of 40 hours of
real-world environmental noise recordings, which supports the in-car speech
data simulation. Moreover, we also provide an open-access, reproducible
baseline system based on this dataset. This system features a speech frontend
model that employs speech source separation to extract each speaker's clean
speech from the far-field signals, along with a speech recognition module that
accurately transcribes the content of each individual speaker. Experimental
results demonstrate the challenges faced by various mainstream ASR models when
evaluated on the AISHELL-5. We firmly believe the AISHELL-5 dataset will
significantly advance the research on ASR systems under complex driving
scenarios by establishing the first publicly available in-car ASR benchmark.

</details>


### [508] [Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction and Activation](https://arxiv.org/pdf/2505.23077)
*Zhennan Lin, Kaixun Huang, Wei Ren, Linju Yang, Lei Xie*

Main category: cs.SD

TL;DR: Proposes an encoder-based phrase-level contextualized ASR method with dynamic vocabulary prediction, bias loss, and confidence-activated decoding, achieving significant WER reductions.


<details>
  <summary>Details</summary>
Motivation: Existing ASR methods treat subwords in contextual phrases as independent units, compromising phrase integrity and reducing accuracy.

Method: Uses encoder-based phrase-level contextualization, dynamic vocabulary prediction, bias loss, and confidence-activated decoding.

Result: Achieves relative WER reductions of 28.31% and 23.49% on Librispeech and Wenetspeech, with contextual phrase WER decreasing by 72.04% and 75.69%.

Conclusion: The proposed method effectively improves ASR performance by maintaining contextual phrase integrity and suppressing incorrect bias.

Abstract: Deep biasing improves automatic speech recognition (ASR) performance by
incorporating contextual phrases. However, most existing methods enhance
subwords in a contextual phrase as independent units, potentially compromising
contextual phrase integrity, leading to accuracy reduction. In this paper, we
propose an encoder-based phrase-level contextualized ASR method that leverages
dynamic vocabulary prediction and activation. We introduce architectural
optimizations and integrate a bias loss to extend phrase-level predictions
based on frame-level outputs. We also introduce a confidence-activated decoding
method that ensures the complete output of contextual phrases while suppressing
incorrect bias. Experiments on Librispeech and Wenetspeech datasets demonstrate
that our approach achieves relative WER reductions of 28.31% and 23.49%
compared to baseline, with the WER on contextual phrases decreasing relatively
by 72.04% and 75.69%.

</details>


### [509] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/pdf/2505.23132)
*Seung Gyu Jeong, Seong Eun Kim*

Main category: cs.SD

TL;DR: Smartphone-based lung sound analysis using PD-SCL and AST improves diagnosis accuracy by 2.4%, addressing patient variability and device differences.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic highlighted limitations of in-person lung sound assessments, prompting the need for accessible, digital diagnostic methods.

Method: Developed Patient Domain Supervised Contrastive Learning (PD-SCL) combined with Audio Spectrogram Transformer (AST) to analyze lung sounds recorded via smartphone microphones.

Result: Improved performance by 2.4% over the original AST model, demonstrating smartphones' potential for accurate lung sound diagnosis.

Conclusion: Smartphone-based lung sound analysis is viable, addressing data inconsistencies and expanding diagnostic accessibility beyond clinical settings.

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [510] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/pdf/2505.23207)
*Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, Lei Xie*

Main category: cs.SD

TL;DR: A speaker-aware progressive OSD model using SSL models and speaker attention achieves state-of-the-art performance (F1: 82.76%) on the AMI test set.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting overlapping speech in multi-party conversations, a critical task in speech processing.

Method: Progressive training strategy integrating VAD and overlap detection, enhanced by SSL models (WavLM, wav2vec 2.0) and a speaker attention module.

Result: Achieves an F1 score of 82.76% on the AMI test set, outperforming existing methods.

Conclusion: The proposed model is robust and effective for OSD, leveraging speaker-aware features and SSL advancements.

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [511] [Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable Emotion Recognition](https://arxiv.org/pdf/2505.23236)
*Youjun Chen, Xurong Xie, Haoning Xu, Mengzhe Geng, Guinan Li, Chengxi Deng, Huimeng Wang, Shujie Hu, Xunying Liu*

Main category: cs.SD

TL;DR: A novel LLM-based explainable speech emotion recognition (SER) method outperforms baselines by disentangling fine-grained features and using VAE-compressed HuBERT features.


<details>
  <summary>Details</summary>
Motivation: To improve SER accuracy and provide explainability through emotion descriptors.

Method: Disentangles SED features from HuBERT SSL representations via alternating LLM fine-tuning, uses VAE-compressed HuBERT features with IB for granularity adjustment.

Result: Outperforms LLaMA-based SER baselines by up to 4.0% absolute accuracy, with added explainability.

Conclusion: The approach enhances SER performance and explainability, demonstrating the value of feature disentanglement and multi-task fine-tuning.

Abstract: This paper presents a novel end-to-end LLM-empowered explainable speech
emotion recognition (SER) approach. Fine-grained speech emotion descriptor
(SED) features, e.g., pitch, tone and emphasis, are disentangled from HuBERT
SSL representations via alternating LLM fine-tuning to joint SER-SED prediction
and ASR tasks. VAE compressed HuBERT features derived via Information
Bottleneck (IB) are used to adjust feature granularity. Experiments on the
IEMOCAP and MELD benchmarks demonstrate that our approach consistently
outperforms comparable LLaMA-based SER baselines, including those using either
(a) alternating multi-task fine-tuning alone or (b) feature disentanglement
only. Statistically significant increase of SER unweighted accuracy by up to
4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly,
emotion descriptors offer further explainability for SER.

</details>


### [512] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/pdf/2505.23509)
*Andrew Chang, Yike Li, Iran R. Roman, David Poeppel*

Main category: cs.SD

TL;DR: The paper proposes a spectrotemporal modulation (STM) feature-based model for audio classification, achieving performance comparable to pretrained DNNs while being more efficient and interpretable.


<details>
  <summary>Details</summary>
Motivation: Current audio DNNs are computationally costly and lack interpretability, prompting the need for optimized solutions.

Method: The approach uses STM features, inspired by human auditory cortex neurophysiology, for audio classification without pretraining.

Result: The STM-based model matches pretrained DNN performance across speech, music, and environmental sounds.

Conclusion: STM features offer an efficient, interpretable alternative for audio classification, with potential applications in auditory science and cognitive computing.

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [513] [Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation](https://arxiv.org/pdf/2505.23290)
*Hao Li, Ju Dai, Xin Zhao, Feng Zhou, Junjun Pan, Lei Li*

Main category: cs.SD

TL;DR: The paper introduces Wav2Sem, a plug-and-play module to decorrelate audio features in 3D speech-driven facial animation, improving lip motion precision by addressing the averaging effect of phonetically similar syllables.


<details>
  <summary>Details</summary>
Motivation: Existing methods using self-supervised audio models struggle with phonetically similar syllables causing coupling in audio features, leading to less accurate lip shapes.

Method: Proposes Wav2Sem, a semantic decorrelation module that extracts semantic features from audio sequences to decorrelate encodings and enhance expressiveness.

Result: Experiments show Wav2Sem effectively decouples audio features, reducing the averaging effect and improving facial animation precision and naturalness.

Conclusion: Wav2Sem enhances speech-driven facial animation by addressing feature coupling, offering a practical solution for more accurate lip motion generation.

Abstract: In 3D speech-driven facial animation generation, existing methods commonly
employ pre-trained self-supervised audio models as encoders. However, due to
the prevalence of phonetically similar syllables with distinct lip shapes in
language, these near-homophone syllables tend to exhibit significant coupling
in self-supervised audio feature spaces, leading to the averaging effect in
subsequent lip motion generation. To address this issue, this paper proposes a
plug-and-play semantic decorrelation module-Wav2Sem. This module extracts
semantic features corresponding to the entire audio sequence, leveraging the
added semantic information to decorrelate audio encodings within the feature
space, thereby achieving more expressive audio features. Extensive experiments
across multiple Speech-driven models indicate that the Wav2Sem module
effectively decouples audio features, significantly alleviating the averaging
effect of phonetically similar syllables in lip shape generation, thereby
enhancing the precision and naturalness of facial animations. Our source code
is available at https://github.com/wslh852/Wav2Sem.git.

</details>


### [514] [Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning](https://arxiv.org/pdf/2505.23298)
*Xiaofeng Pan, Jing Chen, Haitong Zhang, Menglin Xing, Jiayi Wei, Xuefeng Mu, Zhongqian Xie*

Main category: cs.SD

TL;DR: The paper proposes HTCL, a method for learning comprehensive music representations by hierarchically modeling semantic and user preference spaces using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods either ignore language semantics or rely on scarce labeled data, and fail to address user preferences in music recommendation tasks.

Method: HTCL uses a scalable audio encoder and pre-trained BERT for text to learn audio-text semantics via contrastive pre-training, then fine-tunes with user interaction data.

Result: The method achieves strong performance on both semantic and recommendation tasks, bridging semantic and user preference spaces.

Conclusion: HTCL effectively integrates semantic and user preference modeling, outperforming previous approaches.

Abstract: Recent works of music representation learning mainly focus on learning
acoustic music representations with unlabeled audios or further attempt to
acquire multi-modal music representations with scarce annotated audio-text
pairs. They either ignore the language semantics or rely on labeled audio
datasets that are difficult and expensive to create. Moreover, merely modeling
semantic space usually fails to achieve satisfactory performance on music
recommendation tasks since the user preference space is ignored. In this paper,
we propose a novel Hierarchical Two-stage Contrastive Learning (HTCL) method
that models similarity from the semantic perspective to the user perspective
hierarchically to learn a comprehensive music representation bridging the gap
between semantic and user preference spaces. We devise a scalable audio encoder
and leverage a pre-trained BERT model as the text encoder to learn audio-text
semantics via large-scale contrastive pre-training. Further, we explore a
simple yet effective way to exploit interaction data from our online music
platform to adapt the semantic space to user preference space via contrastive
fine-tuning, which differs from previous works that follow the idea of
collaborative filtering. As a result, we obtain a powerful audio encoder that
not only distills language semantics from the text encoder but also models
similarity in user preference space with the integrity of semantic space
preserved. Experimental results on both music semantic and recommendation tasks
confirm the effectiveness of our method.

</details>


### [515] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/pdf/2505.23305)
*Yunkee Chae, Kyogu Lee*

Main category: cs.SD

TL;DR: MGE-LDM is a unified latent diffusion framework for music generation, source imputation, and query-driven separation, supporting flexible manipulation of arbitrary instrument sources.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior approaches constrained to fixed instrument classes by enabling joint learning and flexible manipulation of music sources.

Method: Uses a latent diffusion model to learn a joint distribution over mixtures, submixtures, and stems, treating separation and imputation as conditional inpainting tasks.

Result: Enables complete mixture generation, partial generation (source imputation), and text-conditioned source extraction, trained across heterogeneous datasets.

Conclusion: MGE-LDM offers a versatile, class-agnostic solution for music generation and manipulation, outperforming fixed-class methods.

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [516] [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/pdf/2505.23339)
*Maya Dewhurst, Jack Collins, Justin J. H. Lo, Roy Alderton, Sam Kirkham*

Main category: cs.SD

TL;DR: Nosey is a low-cost, open-source, 3D-printed nasalance recording system, showing comparable performance to commercial devices but with higher nasalance scores.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible, cost-effective alternative to commercial nasalance devices for acoustic nasalance data recording.

Method: Developed a customizable, 3D-printed hardware system (Nosey) and compared it with a commercial nasalance device.

Result: Nosey yields higher nasalance scores but maintains comparable contrast between phonological environments. It allows customization for testing.

Conclusion: Nosey is a viable, cost-effective alternative to commercial devices, with recommendations for its use in data collection.

Abstract: We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,
customizable, 3D-printed system for recording acoustic nasalance data that we
have made available as open-source hardware
(http://github.com/phoneticslab/nosey). We first outline the motivations and
design principles behind our hardware nasalance system, and then present a
comparison between Nosey and a commercial nasalance device. Nosey shows
consistently higher nasalance scores than the commercial device, but the
magnitude of contrast between phonological environments is comparable between
systems. We also review ways of customizing the hardware to facilitate testing,
such as comparison of microphones and different construction materials. We
conclude that Nosey is a flexible and cost-effective alternative to commercial
nasometry devices and propose some methodological considerations for its use in
data collection.

</details>


### [517] [Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset](https://arxiv.org/pdf/2403.03947)
*Pedro Ramoneda, Minhee Lee, Dasaem Jeong, J. J. Valero-Mas, Xavier Serra*

Main category: cs.SD

TL;DR: The paper introduces an audio-based method for estimating music performance difficulty, including a new dataset (PSyllabus) and a multimodal recognition framework.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on symbolic music representations, neglecting audio recordings, which limits accessibility for students. This work addresses this gap.

Method: The authors propose a recognition framework using unimodal and multimodal input representations derived from audio, tested with various pre-training schemes and multi-task scenarios.

Result: The framework is validated through experiments, and the PSyllabus dataset (7,901 piano pieces across 11 difficulty levels) is established as a benchmark.

Conclusion: The work pioneers audio-based difficulty estimation, sharing the dataset, code, and models to encourage further research in MIR.

Abstract: Automatically estimating the performance difficulty of a music piece
represents a key process in music education to create tailored curricula
according to the individual needs of the students. Given its relevance, the
Music Information Retrieval (MIR) field depicts some proof-of-concept works
addressing this task that mainly focuses on high-level music abstractions such
as machine-readable scores or music sheet images. In this regard, the potential
of directly analyzing audio recordings has been generally neglected, which
prevents students from exploring diverse music pieces that may not have a
formal symbolic-level transcription. This work pioneers in the automatic
estimation of performance difficulty of music pieces on audio recordings with
two precise contributions: (i) the first audio-based difficulty estimation
dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano
pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition
framework capable of managing different input representations -- both unimodal
and multimodal manners -- directly derived from audio to perform the difficulty
estimation task. The comprehensive experimentation comprising different
pre-training schemes, input modalities, and multi-task scenarios prove the
validity of the proposal and establishes PSyllabus as a reference dataset for
audio-based difficulty estimation in the MIR field. The dataset as well as the
developed code and trained models are publicly shared to promote further
research in the field.

</details>


### [518] [Semantics-Aware Human Motion Generation from Audio Instructions](https://arxiv.org/pdf/2505.23465)
*Zi-An Wang, Shihao Zou, Shiyao Yu, Mingyuan Zhang, Chao Dong*

Main category: cs.SD

TL;DR: The paper introduces an end-to-end framework using a masked generative transformer to generate motions aligned with audio semantics, improving upon existing methods that focus on rhythm matching.


<details>
  <summary>Details</summary>
Motivation: Audio signals offer a natural and intuitive communication method for motion generation, but current methods lack strong semantic alignment between audio and motions.

Method: Proposes a masked generative transformer with a memory-retrieval attention module to handle sparse audio inputs, and enriches datasets with conversational-style descriptions and varied audio.

Result: Experiments show the framework effectively generates motions aligned with audio semantics, proving audio can convey semantics like text while being more user-friendly.

Conclusion: The framework successfully bridges the gap between audio semantics and motion generation, offering practical and intuitive interactions.

Abstract: Recent advances in interactive technologies have highlighted the prominence
of audio signals for semantic encoding. This paper explores a new task, where
audio signals are used as conditioning inputs to generate motions that align
with the semantics of the audio. Unlike text-based interactions, audio provides
a more natural and intuitive communication method. However, existing methods
typically focus on matching motions with music or speech rhythms, which often
results in a weak connection between the semantics of the audio and generated
motions. We propose an end-to-end framework using a masked generative
transformer, enhanced by a memory-retrieval attention module to handle sparse
and lengthy audio inputs. Additionally, we enrich existing datasets by
converting descriptions into conversational style and generating corresponding
audio with varied speaker identities. Experiments demonstrate the effectiveness
and efficiency of the proposed framework, demonstrating that audio instructions
can convey semantics similar to text while providing more practical and
user-friendly interactions.

</details>


### [519] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/pdf/2505.23619)
*Neta Glazer, David Chernin, Idan Achituve, Sharon Gannot, Ethan Fetaya*

Main category: cs.SD

TL;DR: ADD-GP is a few-shot adaptive framework for audio deepfake detection using Gaussian Process classifiers, achieving strong performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: The rise of advanced TTS models and voice cloning necessitates adaptable deepfake detection methods that work with minimal data.

Method: Combines deep embedding models with Gaussian Process classifiers for few-shot learning and adaptability.

Result: Demonstrates strong performance, adaptability to new TTS models, and one-shot personalized detection.

Conclusion: ADD-GP offers a robust and adaptable solution for detecting audio deepfakes, even with limited data.

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


### [520] [A Novel Deep Learning Framework for Efficient Multichannel Acoustic Feedback Control](https://arxiv.org/pdf/2505.15914)
*Yuan-Kuei Wu, Juan Azcarreta, Kashyap Patel, Buye Xu, Jung-Suk Lee, Sanha Lee, Ashutosh Pandey*

Main category: cs.SD

TL;DR: A deep-learning framework using a Convolutional Recurrent Network improves multichannel acoustic feedback control, outperforming traditional methods with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional digital signal processing methods fail to converge with highly correlated noise like feedback, necessitating a more efficient solution.

Method: The framework employs a Convolutional Recurrent Network and three training methods (In-a-Loop Training, Teacher Forcing, Hybrid with Multichannel Wiener Filter) for optimized performance.

Result: The approach significantly enhances speech enhancement and is scalable for real-world applications.

Conclusion: This framework advances Acoustic Feedback Control technology, offering a robust and efficient solution.

Abstract: This study presents a deep-learning framework for controlling multichannel
acoustic feedback in audio devices. Traditional digital signal processing
methods struggle with convergence when dealing with highly correlated noise
such as feedback. We introduce a Convolutional Recurrent Network that
efficiently combines spatial and temporal processing, significantly enhancing
speech enhancement capabilities with lower computational demands. Our approach
utilizes three training methods: In-a-Loop Training, Teacher Forcing, and a
Hybrid strategy with a Multichannel Wiener Filter, optimizing performance in
complex acoustic environments. This scalable framework offers a robust solution
for real-world applications, making significant advances in Acoustic Feedback
Control technology.

</details>


### [521] [ZeroSep: Separate Anything in Audio with Zero Training](https://arxiv.org/pdf/2505.23625)
*Chao Huang, Yuesheng Ma, Junxuan Huang, Susan Liang, Yunlong Tang, Jing Bi, Wenqiang Liu, Nima Mesgarani, Chenliang Xu*

Main category: cs.SD

TL;DR: ZeroSep enables zero-shot audio source separation using pre-trained text-guided audio diffusion models without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Current supervised methods require extensive labeled data and struggle with real-world variability. Generative foundation models offer a potential solution.

Method: Inverts mixed audio into a diffusion model's latent space and uses text conditioning to guide denoising for source separation.

Result: ZeroSep achieves strong performance on benchmarks, surpassing supervised methods, and supports open-set scenarios.

Conclusion: Pre-trained generative models can be repurposed for discriminative tasks like source separation, offering a scalable and flexible solution.

Abstract: Audio source separation is fundamental for machines to understand complex
acoustic environments and underpins numerous audio applications. Current
supervised deep learning approaches, while powerful, are limited by the need
for extensive, task-specific labeled data and struggle to generalize to the
immense variability and open-set nature of real-world acoustic scenes. Inspired
by the success of generative foundation models, we investigate whether
pre-trained text-guided audio diffusion models can overcome these limitations.
We make a surprising discovery: zero-shot source separation can be achieved
purely through a pre-trained text-guided audio diffusion model under the right
configuration. Our method, named ZeroSep, works by inverting the mixed audio
into the diffusion model's latent space and then using text conditioning to
guide the denoising process to recover individual sources. Without any
task-specific training or fine-tuning, ZeroSep repurposes the generative
diffusion model for a discriminative separation task and inherently supports
open-set scenarios through its rich textual priors. ZeroSep is compatible with
a variety of pre-trained text-guided audio diffusion backbones and delivers
strong separation performance on multiple separation benchmarks, surpassing
even supervised methods.

</details>


### [522] [Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation](https://arxiv.org/pdf/2505.20745)
*Jingping Nie, Dung T. Tran, Karan Thakkar, Vasudha Kowtha, Jon Huang, Carlos Avendano, Erdrin Azemi, Vikramjit Mitra*

Main category: cs.SD

TL;DR: The paper investigates how well pre-trained acoustic foundation models (FMs) encode auscultation data, comparing six FMs for heart rate (HR) estimation. The in-house CLAP model outperforms the baseline.


<details>
  <summary>Details</summary>
Motivation: To explore the extent to which auscultation data is captured by pre-trained acoustic FMs, as this has not been thoroughly studied.

Method: Layer-wise analysis of six FMs (HuBERT, wav2vec2, wavLM, Whisper, CLAP, in-house CLAP) using a PCG dataset and HR estimation model, comparing against a baseline method.

Result: Pre-trained FMs perform comparably to the baseline, with the in-house CLAP model achieving lower MAE in HR estimation.

Conclusion: Pre-trained FMs, especially the in-house CLAP model, effectively encode auscultation data and can outperform traditional acoustic feature-based methods.

Abstract: Auscultation, particularly heart sound, is a non-invasive technique that
provides essential vital sign information. Recently, self-supervised acoustic
representation foundation models (FMs) have been proposed to offer insights
into acoustics-based vital signs. However, there has been little exploration of
the extent to which auscultation is encoded in these pre-trained FM
representations. In this work, using a publicly available phonocardiogram (PCG)
dataset and a heart rate (HR) estimation model, we conduct a layer-wise
investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,
Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP
model. Additionally, we implement the baseline method from Nie et al., 2024
(which relies on acoustic features) and show that overall, representation
vectors from pre-trained foundation models (FMs) offer comparable performance
to the baseline. Notably, HR estimation using the representations from the
audio encoder of the in-house CLAP model outperforms the results obtained from
the baseline, achieving a lower mean absolute error (MAE) across various
train/validation/test splits despite the domain mismatch.

</details>


### [523] [Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection](https://arxiv.org/pdf/2505.20956)
*Shiqi Zhang, Tuomas Virtanen*

Main category: cs.SD

TL;DR: The paper introduces MFFT, an active learning method for BioSED, addressing challenges like limited annotated data and class imbalance. MFFT achieves high mAP with minimal annotations, excelling in cold-start and rare species scenarios.


<details>
  <summary>Details</summary>
Motivation: BioSED faces challenges like sparse annotated data, species diversity, and class imbalance, hindering biodiversity conservation efforts.

Method: MFFT, an active learning method combining committee voting disagreement and diversity analysis, is applied to efficiently use limited labeling budgets.

Result: MFFT achieves 68% mAP (cold-start) and 71% mAP (warm-start), close to fully-supervised 75% mAP, using only 2.3% annotations. It performs well with rare species.

Conclusion: MFFT is practical for BioSED, especially in cold-start and rare species scenarios, making it valuable for biodiversity monitoring.

Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity
conservation but faces practical challenges during model development and
training: limited amounts of annotated data, sparse events, species diversity,
and class imbalance. To address these challenges efficiently with a limited
labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an
active learning method integrating committee voting disagreement and diversity
analysis. We also refine an existing BioSED dataset specifically for evaluating
active learning algorithms. Experimental results demonstrate that MFFT achieves
a mAP of 68% when cold-starting and 71% when warm-starting (which is close to
the fully-supervised mAP of 75%) while using only 2.3% of the annotations.
Notably, MFFT excels in cold-start scenarios and with rare species, which are
critical for monitoring endangered species, demonstrating its practical value.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [524] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/pdf/2505.22686)
*Ange-Clement Akazan, Verlon Roel Mbingui, Gnankan Landry Regis N'guessan, Issa Karambal*

Main category: cs.LG

TL;DR: The study benchmarks deep recurrent neural networks and Kolmogorov-Arnold-based models for weather forecasting in tropical Africa, introducing customized variants of TKAN. Results show KAN excels in temperature prediction, TKAN variants improve precipitation forecasting, and classical RNNs outperform in pressure prediction.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting methods struggle with tropical Africa's complex weather patterns, necessitating improved models for better risk management and economic planning.

Method: Benchmarked models include LSTM, GRU, BiLSTM, BiGRU, KAN, and TKAN, with customized TKAN variants using GeLU and MiSH activations. Evaluated on station-level meteorological data (2010-2024) for temperature, precipitation, and pressure.

Result: KAN achieves high accuracy in temperature prediction (R²≈0.9986-0.9998), TKAN variants minimize precipitation errors, and classical RNNs outperform in pressure prediction (R²≈0.83-0.86).

Conclusion: Spline-based neural architectures like KAN and TKAN show promise for efficient and data-efficient weather forecasting in tropical regions.

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [525] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/pdf/2505.22689)
*Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang*

Main category: cs.LG

TL;DR: SlimLLM is a structured pruning method for large language models (LLMs) that evaluates sub-module importance holistically and recovers performance efficiently, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of LLMs limits their deployment, prompting the need for effective pruning methods to compress parameters without significant performance loss.

Method: SlimLLM evaluates channel and attention head importance collectively, uses linear regression for output matrix recovery, and employs layer-based importance ratios for pruning.

Result: SlimLLM outperforms other methods on the LLaMA benchmark, achieving state-of-the-art performance.

Conclusion: SlimLLM provides an efficient and effective solution for pruning LLMs, addressing computational constraints while maintaining performance.

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [526] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/pdf/2505.22694)
*Dacao Zhang, Kun Zhang, Shimao Chu, Le Wu, Xin Li, Si Wei*

Main category: cs.LG

TL;DR: MoRE improves LoRA for multi-task PEFT by aligning different ranks of LoRA with tasks and using an adaptive rank selector, enhancing efficiency and performance without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA are limited in multi-task scenarios, either focusing on single tasks or separately training modules. MoRE aims to address this gap.

Method: MoRE introduces low-rank experts aligned with tasks and an adaptive rank selector. It jointly trains these experts for multi-task efficiency.

Result: MoRE outperforms traditional LoRA and variants in multi-task benchmarks, improving LLM performance without added inference cost.

Conclusion: MoRE effectively adapts LoRA for multi-task PEFT, offering better performance and efficiency, with released code for community use.

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [527] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/pdf/2505.22695)
*Tengfei Lyu, Siyuan Feng, Hao Liu, Hai Yang*

Main category: cs.LG

TL;DR: LLM-ODDR is a novel framework using Large Language Models (LLMs) for ride-hailing order dispatching and driver repositioning, outperforming traditional methods in fairness, adaptability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in traditional methods (e.g., ignoring driver income fairness and real-world adaptability) for ride-hailing optimization.

Method: LLM-ODDR includes multi-objective order evaluation, fairness-aware dispatching, and demand-aware repositioning, with a fine-tuned model (JointDR-GPT).

Result: Outperforms traditional methods in effectiveness, adaptability, and interpretability on real-world Manhattan taxi data.

Conclusion: First successful use of LLMs for ride-hailing ODDR tasks, paving the way for advanced language models in transportation systems.

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [528] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/pdf/2505.23009)
*Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Alex Smola*

Main category: cs.LG

TL;DR: The paper introduces EmergentTTS-Eval, a benchmark for evaluating TTS models on nuanced and complex text, automating test-case generation and evaluation using LLMs and a model-as-a-judge approach.


<details>
  <summary>Details</summary>
Motivation: Existing TTS benchmarks lack the ability to assess nuanced and semantically complex text, prompting the need for a more comprehensive evaluation framework.

Method: The authors develop EmergentTTS-Eval, automating test-case generation from seed prompts using LLMs and employing a Large Audio Language Model (LALM) for multi-dimensional evaluation.

Result: The benchmark reveals fine-grained performance differences among TTS systems, with the model-as-a-judge approach showing high correlation with human preferences.

Conclusion: EmergentTTS-Eval provides a robust, extensible framework for TTS evaluation, with open-sourced code and dataset to facilitate further research.

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [529] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/pdf/2505.22696)
*Eleni Nisioti, Joachim Winther Pedersen, Erwan Plantec, Milton L. Montero, Sebastian Risi*

Main category: cs.LG

TL;DR: The paper explores neuroevolution (NE) for transfer learning in AI, showing NE often outperforms reinforcement learning (RL) in adaptability and skill transfer across tasks.


<details>
  <summary>Details</summary>
Motivation: To address RL's brittleness and catastrophic forgetting, the study investigates NE's understudied transfer learning capabilities for more adaptable AI systems.

Method: Two benchmarks (stepping gates and ecorobot) with curricula are introduced to evaluate NE's skill transfer across tasks of increasing complexity.

Result: NE methods frequently outperform RL baselines in transfer learning, demonstrating robustness and scalability.

Conclusion: NE shows promise for adaptable AI agents but faces challenges in scaling to complex real-world problems.

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [530] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/pdf/2505.22967)
*Chengqi Zheng, Jianda Chen, Yueming Lyu, Wen Zheng Terence Ng, Haopeng Zhang, Yew-Soon Ong, Ivor Tsang, Haiyan Yin*

Main category: cs.LG

TL;DR: MermaidFlow improves workflow generation by using safety-constrained graph evolution with Mermaid, ensuring executable and diverse plans.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce fragile, unexecutable plans due to unconstrained LLM-driven construction.

Method: Uses Mermaid for verifiable intermediate representation and domain-aware evolutionary operators (crossover, mutation, insertion, deletion) to maintain correctness and diversity.

Result: Achieves higher success rates and faster convergence to executable plans on benchmarks.

Conclusion: Safety-constrained graph evolution provides a scalable, modular foundation for robust and interpretable agentic reasoning.

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [531] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/pdf/2505.22697)
*Filippo Rinaldi, Giacomo Capitani, Lorenzo Bonicelli, Donato Crisostomi, Federico Bolelli, Elisa Ficarra, Emanuele Rodolà, Simone Calderara, Angelo Porrello*

Main category: cs.LG

TL;DR: The paper proposes a method to transfer fine-tuning from an outdated pre-trained model to a new one without retraining, using weight permutations and spectral theory.


<details>
  <summary>Details</summary>
Motivation: Fine-tuned models become obsolete when the underlying pre-trained model is updated, requiring costly retraining. The goal is to enable seamless transfer of fine-tuning to new models without data or retraining.

Method: A two-level approach based on weight permutations and spectral theory, addressing challenges in Transformer models like residual connections and multi-head attention layers.

Result: Successful transfer of fine-tuned knowledge to new pre-trained models without training or data, validated on visual and textual tasks.

Conclusion: The method provides a practical solution for updating fine-tuned models when the base model changes, eliminating the need for retraining.

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [532] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/pdf/2505.22703)
*Mohammad Yaghini, Tudor Cebere, Michael Menart, Aurélien Bellet, Nicolas Papernot*

Main category: cs.LG

TL;DR: RaCO-DP, a differentially private variant of SGDA, addresses constrained ML problems with rate constraints, improving fairness-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: To solve constrained minimization problems in trustworthy ML under differential privacy, where standard techniques fail due to inter-sample dependencies from rate constraints.

Method: Develops RaCO-DP, a DP variant of SGDA, solving the Lagrangian formulation of rate constraint problems by privately estimating histograms over mini-batches.

Result: The method Pareto-dominates existing private learning approaches in fairness-utility trade-offs.

Conclusion: RaCO-DP effectively handles rate constraints under DP, offering better fairness-utility trade-offs than prior methods.

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [533] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/pdf/2505.22758)
*Aniruddha Nrusimha, William Brandon, Mayank Mishra, Yikang Shen, Rameswar Panda, Jonathan Ragan-Kelley, Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer is a specialized kernel for low-batch inference of transformer-based LLMs, offering speedups over existing kernels.


<details>
  <summary>Details</summary>
Motivation: Address the gap in optimizing kernels for low-batch inference, crucial for edge deployment and latency-sensitive applications.

Method: Develops FlashFormer, a proof-of-concept kernel tailored for single-batch inference in transformer-based LLMs.

Result: Achieves nontrivial speedups across various model sizes and quantization settings.

Conclusion: FlashFormer demonstrates the potential for optimizing low-batch inference, benefiting edge and latency-sensitive use cases.

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [534] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/pdf/2505.22764)
*Divya Shanmugam, Helen Lu, Swami Sankaranarayanan, John Guttag*

Main category: cs.LG

TL;DR: Test-time augmentation (TTA) reduces the size of prediction sets in conformal classifiers by 10%-14% on average, without requiring model retraining.


<details>
  <summary>Details</summary>
Motivation: Conformal classifiers often produce large, uninformative prediction sets, limiting their practical utility.

Method: The paper introduces TTA, a technique that applies inductive biases during inference, to reduce prediction set sizes. It is flexible, works with any conformal score, and doesn't need model retraining.

Result: TTA reduces prediction set sizes by 10%-14% on average across three datasets, three models, and various conditions.

Conclusion: TTA is a computationally efficient and effective addition to the conformal pipeline, improving the informativeness of prediction sets.

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [535] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/pdf/2505.22768)
*Mert Onur Cakiroglu, Idil Bilge Altun, Hasan Kurban, Elham Buxton, Mehmet Dalkilic*

Main category: cs.LG

TL;DR: DRAGON introduces Multivariate de Bruijn Graphs (MdBGs) to enhance time series forecasting by combining symbolic representations with neural modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like temporal heterogeneity and high dimensionality in time series forecasting by bridging symbolic and neural approaches.

Method: Discretizes continuous sequences, maps them to MdBGs, and uses graph-based attention for dynamic context recovery within a dual-branch architecture.

Result: Enhances CNN-based encoders with symbolic, structure-aware representations.

Conclusion: DRAGON effectively integrates symbolic and neural methods for improved time series forecasting.

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [536] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/pdf/2505.22772)
*Claas Voelcker, Anastasiia Pedan, Arash Ahmadian, Romina Abachi, Igor Gilitschenski, Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: The paper analyzes value-aware model learning losses, including MuZero, showing they are uncalibrated and proposing corrections. It also explores the role of loss calibration, model architectures, and auxiliary losses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of value-aware model learning losses, particularly the MuZero loss, and their limitations in recovering correct models and value functions.

Method: Theoretical analysis of value-aware model learning losses, identification of their uncalibrated nature, and proposal of corrective measures. Investigation of loss calibration, model architectures, and auxiliary losses.

Result: Value-aware losses like MuZero are uncalibrated surrogates, but corrections can address this. Deterministic models suffice for value prediction, but stochastic models offer advantages.

Conclusion: Correcting uncalibrated losses improves model learning. Stochastic models, despite deterministic sufficiency, provide additional benefits.

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [537] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/pdf/2505.22778)
*Sarah Meiklejohn, Hayden Blauzvern, Mihai Maruseac, Spencer Schrock, Laurent Simon, Ilia Shumailov*

Main category: cs.LG

TL;DR: The paper highlights supply-chain risks in open ML models and proposes using Sigstore for transparency.


<details>
  <summary>Details</summary>
Motivation: The open ecosystem for ML models poses risks like malicious replacements and poisoned data, which need addressing.

Method: Explore Sigstore's application to ML models for signing and verifying dataset properties.

Result: Sigstore can enhance transparency and security in open ML model supply chains.

Conclusion: Using Sigstore can mitigate risks in the open ML model ecosystem.

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [538] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/pdf/2505.22785)
*Marco Fumero, Luca Moschella, Emanuele Rodolà, Francesco Locatello*

Main category: cs.LG

TL;DR: The paper interprets neural networks as dynamical systems on latent manifolds, revealing implicit vector fields and attractor points, enabling novel analysis of model properties and data.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative interpretation of neural models as dynamical systems, uncovering implicit structures like vector fields and attractor points for better model analysis.

Method: Analyzes autoencoder models as dynamical systems, deriving latent vector fields from encoding-decoding maps without extra training.

Result: Reveals attractor points in vector fields, enabling analysis of generalization, memorization, prior knowledge extraction, and out-of-distribution detection.

Conclusion: The proposed method offers a powerful tool for analyzing neural models and data, validated on real-world vision foundation models.

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [539] [MedRAX: Medical Reasoning Agent for Chest X-ray](https://arxiv.org/pdf/2502.02673)
*Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang*

Main category: cs.LG

TL;DR: MedRAX is a versatile AI agent integrating CXR analysis tools and multimodal LLMs, achieving state-of-the-art performance on complex medical queries without additional training.


<details>
  <summary>Details</summary>
Motivation: Specialized CXR models operate in isolation, limiting clinical utility. MedRAX aims to unify these tools for practical deployment.

Method: Integrates state-of-the-art CXR tools and multimodal LLMs into a unified framework, evaluated using ChestAgentBench (2,500 queries across 7 categories).

Result: Outperforms open-source and proprietary models, demonstrating superior performance in automated CXR interpretation.

Conclusion: MedRAX represents a significant advancement toward practical, automated CXR analysis in clinical settings.

Abstract: Chest X-rays (CXRs) play an integral role in driving critical decisions in
disease management and patient care. While recent innovations have led to
specialized models for various CXR interpretation tasks, these solutions often
operate in isolation, limiting their practical utility in clinical practice. We
present MedRAX, the first versatile AI agent that seamlessly integrates
state-of-the-art CXR analysis tools and multimodal large language models into a
unified framework. MedRAX dynamically leverages these models to address complex
medical queries without requiring additional training. To rigorously evaluate
its capabilities, we introduce ChestAgentBench, a comprehensive benchmark
containing 2,500 complex medical queries across 7 diverse categories. Our
experiments demonstrate that MedRAX achieves state-of-the-art performance
compared to both open-source and proprietary models, representing a significant
step toward the practical deployment of automated CXR interpretation systems.
Data and code have been publicly available at
https://github.com/bowang-lab/MedRAX

</details>


### [540] [Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection](https://arxiv.org/pdf/2502.01699)
*Tianlin Zhang, En Yu, Yi Shao, Jiande Sun*

Main category: cs.LG

TL;DR: MIAN, a Multimodal Inverse Attention Network, improves fake news detection by leveraging modal-specific features and explicit discrepancies through hierarchical and cross-modal interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack modal-specific representations and explicit discrepant features, limiting fake news detection accuracy.

Method: MIAN uses hierarchical learning for intra-modal relationships and cross-modal co-attention for semantic integration, with an inverse attention mechanism to highlight inconsistencies.

Result: MIAN outperforms state-of-the-art methods on benchmark datasets.

Conclusion: MIAN advances multimodal fake news detection, enhancing social security.

Abstract: Multimodal fake news detection has garnered significant attention due to its
profound implications for social security. While existing approaches have
contributed to understanding cross-modal consistency, they often fail to
leverage modal-specific representations and explicit discrepant features. To
address these limitations, we propose a Multimodal Inverse Attention Network
(MIAN), a novel framework that explores intrinsic discriminative features based
on news content to advance fake news detection. Specifically, MIAN introduces a
hierarchical learning module that captures diverse intra-modal relationships
through local-to-global and local-to-local interactions, thereby generating
enhanced unimodal representations to improve the identification of fake news at
the intra-modal level. Additionally, a cross-modal interaction module employs a
co-attention mechanism to establish and model dependencies between the refined
unimodal representations, facilitating seamless semantic integration across
modalities. To explicitly extract inconsistency features, we propose an inverse
attention mechanism that effectively highlights the conflicting patterns and
semantic deviations introduced by fake news in both intra- and inter-modality.
Extensive experiments on benchmark datasets demonstrate that MIAN significantly
outperforms state-of-the-art methods, underscoring its pivotal contribution to
advancing social security through enhanced multimodal fake news detection.

</details>


### [541] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/pdf/2505.22798)
*Anton Björklund, Mykola Zaitsev, Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper improves the PREMAP algorithm for neural network certification against patch attacks, achieving significant efficiency gains and scalability to larger networks.


<details>
  <summary>Details</summary>
Motivation: The need for reliable neural network certification in safety-critical applications, especially against patch attacks, drives the research.

Method: The authors enhance PREMAP with tighter bounds, adaptive Monte Carlo sampling, and better branching heuristics.

Result: Efficiency improves by at least an order of magnitude, and the method scales to previously infeasible convolutional neural networks.

Conclusion: The improved PREMAP demonstrates the potential of preimage approximation for robust and reliable certification.

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [542] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/pdf/2505.22803)
*Pedro Mendes, Paolo Romano, David Garlan*

Main category: cs.LG

TL;DR: CLUE introduces a novel method for neural network calibration by aligning predicted uncertainty with observed error during training, improving scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing calibration techniques lack scalability, differentiability, and domain generalization, limiting their practical deployment.

Method: CLUE uses a novel loss function to align uncertainty with empirical loss, optimizing both predictive performance and calibration.

Result: CLUE outperforms state-of-the-art methods in calibration quality and maintains competitive predictive performance across diverse tasks.

Conclusion: CLUE provides a scalable, differentiable, and domain-agnostic solution for reliable uncertainty estimation in neural networks.

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [543] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/pdf/2505.22813)
*Josiah Couch, Miao Li, Rima Arnaout, Ramy Arnaout*

Main category: cs.LG

TL;DR: Dataset quality is an intrinsic property, independent of model architecture, size, and class balance, and correlates strongly with classifier performance.


<details>
  <summary>Details</summary>
Motivation: To determine if dataset quality is an intrinsic property or a joint property of dataset and model architecture.

Method: Created controlled datasets, trained classifiers with varied architectures, and analyzed performance correlations.

Result: Classifier performance strongly correlates by subset across architectures (R²=0.79), confirming dataset quality as intrinsic.

Conclusion: Dataset quality is a fundamental, independent factor for optimizing ML classifiers, alongside size, class balance, and architecture.

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [544] [The challenge of hidden gifts in multi-agent reinforcement learning](https://arxiv.org/pdf/2505.20579)
*Dane Malenfant, Blake A. Richards*

Main category: cs.LG

TL;DR: The paper explores 'hidden gifts' in MARL, where agents unknowingly benefit from others' actions. A simple grid-world task reveals state-of-the-art RL algorithms struggle with collective rewards due to hidden actions. Independent agents succeed with action history, while MARL agents fail. A correction term improves reliability.


<details>
  <summary>Details</summary>
Motivation: To understand how 'hidden gifts'—unseen beneficial actions by others—impact credit assignment in MARL, and to identify solutions for such challenges.

Method: A grid-world task where agents must unlock doors with a shared key to earn individual and collective rewards, testing various RL algorithms.

Result: State-of-the-art RL and MARL algorithms fail to achieve collective rewards. Independent agents succeed with action history, and a correction term improves convergence.

Conclusion: Hidden gifts complicate credit assignment in MARL, but learning-aware approaches can help independent agents succeed, highlighting the need for better MARL solutions.

Abstract: Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.

</details>


### [545] [Preference Learning with Response Time](https://arxiv.org/pdf/2505.22820)
*Ayush Sawarni, Sahasrajit Sarmasarkar, Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: The paper introduces a method to enhance reward model learning by integrating response time data with binary preference data, improving sample efficiency and reducing error rates.


<details>
  <summary>Details</summary>
Motivation: Current preference learning frameworks primarily use binary data, neglecting valuable temporal information in user decision-making. This work aims to exploit response time data for better reward model elicitation.

Method: The authors propose using the Evidence Accumulation Drift Diffusion (EZ) model to incorporate response time information. They develop Neyman-orthogonal loss functions for optimal convergence rates in reward model learning.

Result: Theoretical analysis shows the method reduces error rates from exponential to polynomial scaling for linear reward functions. Experiments confirm these improvements in preference learning over images.

Conclusion: Integrating response time data significantly enhances reward model learning, offering better sample efficiency and accuracy, especially for complex reward functions.

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [546] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/pdf/2505.22825)
*Michael Klamkin, Mathieu Tanneau, Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: PGLearn introduces standardized datasets and tools for ML in OPF, addressing lack of benchmarks and promoting fair comparison.


<details>
  <summary>Details</summary>
Motivation: The volatility and scale of modern energy grids necessitate ML for OPF, but progress is hindered by the lack of standardized datasets and metrics.

Method: PGLearn provides diverse datasets (including time series) and supports multiple OPF formulations (AC, DC, second-order cone). It also offers evaluation tools.

Result: Standardized datasets and tools are made publicly available, enabling fair benchmarking and reducing data generation burdens.

Conclusion: PGLearn democratizes and accelerates ML research for OPF by promoting open, standardized resources.

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [547] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/pdf/2505.22829)
*Chenruo Liu, Kenan Tang, Yao Qin, Qi Lei*

Main category: cs.LG

TL;DR: The paper connects distribution shift and AI safety, showing how methods for addressing shifts can aid safety goals and how shifts and safety issues can be formally linked.


<details>
  <summary>Details</summary>
Motivation: To unify distribution shift and AI safety research by exploring their conceptual and methodological overlaps.

Method: Analyzes two types of connections: (1) methods for specific shifts aiding safety goals, and (2) formal reductions between shifts and safety issues.

Result: Establishes a unified perspective for integrating distribution shift and AI safety research.

Conclusion: Encourages fundamental integration between the two fields for mutual methodological adaptation.

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [548] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/pdf/2505.22839)
*Liu Yuezhang, Xue-Xin Wei*

Main category: cs.LG

TL;DR: Diffusion models improve adversarial robustness, but the mechanisms are unclear. This study reveals that diffusion models increase distance to clean samples and are influenced by internal randomness, with robustness gains linked to input space compression.


<details>
  <summary>Details</summary>
Motivation: To systematically understand how diffusion models enhance adversarial robustness, given unclear mechanisms despite intuitive explanations.

Method: Investigate diffusion models' effects on adversarial robustness, focusing on distance to clean samples, internal randomness, and compression effects. Evaluate robustness under fixed randomness and correlate it with compression rate.

Result: Robustness improvement drops to ~24% under fixed randomness (vs. ~70% prior reports). Remaining gain correlates with input space compression, making compression rate a reliable robustness indicator.

Conclusion: The study clarifies diffusion-based purification mechanisms, linking robustness to compression, and guides future adversarial purification system development.

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [549] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/pdf/2505.22840)
*Dharambir Mahto, Prashant Yadav, Mahesh Banavar, Jim Keany, Alan T Joseph, Srinivas Kilambi*

Main category: cs.LG

TL;DR: The SXI++ LNM, a machine learning model, improves sepsis prediction with high accuracy (99.99%) and AUC (0.99), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Sepsis prediction is challenging due to non-specific symptoms and complex pathophysiology, necessitating better predictive tools.

Method: The SXI++ LNM uses deep neural networks and multiple algorithms, tested across varied dataset distributions.

Result: The model achieved an AUC of 0.99, precision of 99.9%, and accuracy of 99.99%, outperforming state-of-the-art methods.

Conclusion: The SXI++ LNM is a robust tool for sepsis prediction, demonstrating high reliability in clinical applications.

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [550] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/pdf/2505.22841)
*Franck Gabriel, François Ged, Maria Han Veiga, Emmanuel Schertzer*

Main category: cs.LG

TL;DR: The paper addresses memorization risks in diffusion models by analyzing the empirical score's noise and proposing kernel-smoothed regularization to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Diffusion models, while effective, can overfit training data, leading to memorization. The study aims to mitigate this by understanding and controlling the noise in the empirical score.

Method: The authors interpret the empirical score as noisy, analyze its covariance matrix, and introduce kernel-smoothed regularization. They derive bounds on divergence and reformulate reverse diffusion as gradient descent with LED-KDE.

Result: Kernel-smoothed regularization reduces variance, prevents memorization, and enhances generalization, akin to increasing dataset size. Spectral analysis suggests better variance control under certain conditions.

Conclusion: Regularization, even without learning, mitigates memorization and improves generalization, as demonstrated on synthetic and MNIST datasets.

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [551] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/pdf/2505.22846)
*Nikita Khramov, Andrei Kozyrev, Gleb Solovev, Anton Podkopaev*

Main category: cs.LG

TL;DR: The paper explores combining Interactive Theorem Proving with Generative AI, proposing a novel method for Rocq proof generation using a self-attentive embedder model, achieving a 28% performance boost.


<details>
  <summary>Details</summary>
Motivation: To improve Rocq proof generation by addressing premise selection and leveraging AI techniques.

Method: A multi-stage agentic system with a self-attentive embedder model and multi-agent debate for proof synthesis.

Result: 28% relative increase in generator performance, demonstrating high effectiveness.

Conclusion: The proposed approach significantly enhances Rocq proof generation, with potential for further improvements.

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [552] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/pdf/2505.22861)
*Carlota Parés-Morlans, Michelle Yi, Claire Chen, Sarah A. Wu, Rika Antonova, Tobias Gerstenberg, Jeannette Bohg*

Main category: cs.LG

TL;DR: Causal-PIK uses Bayesian optimization and a Physics-Informed Kernel to improve action planning in complex tasks, outperforming state-of-the-art methods and even matching human performance in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Planning in tasks with unknown object dynamics is difficult, requiring iterative action improvement through active exploration of causes and effects.

Method: Causal-PIK leverages Bayesian optimization and a Physics-Informed Kernel to efficiently search for optimal actions by reasoning about causal interactions.

Result: Causal-PIK outperforms state-of-the-art methods on Virtual Tools and PHYRE benchmarks, requiring fewer actions to achieve goals, and remains competitive with human performance in challenging tasks.

Conclusion: Causal-PIK is effective for complex tasks with unknown dynamics, demonstrating superior performance and scalability compared to existing methods and human problem-solvers.

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [553] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/pdf/2505.22866)
*Nicolas Espinosa-Dice, Yiyi Zhang, Yiding Chen, Bradley Guo, Owen Oertell, Gokul Swamy, Kiante Brantley, Wen Sun*

Main category: cs.LG

TL;DR: SORL is a scalable offline RL algorithm using shortcut models for efficient training and inference, achieving strong performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Applying diffusion and flow models to offline RL is challenging due to iterative noise sampling. SORL addresses this by leveraging shortcut models.

Method: SORL uses shortcut models for one-stage training and employs Q-function verification for sequential and parallel inference scaling.

Result: SORL performs well across offline RL tasks and scales positively with increased test-time compute.

Conclusion: SORL offers a scalable and efficient solution for offline RL, demonstrated by its strong performance and released code.

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [554] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/pdf/2505.22881)
*Hyungki Im, Wyame Benslimane, Paul Grigas*

Main category: cs.LG

TL;DR: The paper introduces SPO-RC and SPO-RC+ to handle uncertain constraints in CSLO, using conformal prediction and importance reweighting for improved performance.


<details>
  <summary>Details</summary>
Motivation: Address uncertainty in constraints for CSLO, which is underexplored in literature.

Method: Use contextual uncertainty sets, SPO-RC loss, and SPO-RC+ surrogate with truncation and importance reweighting.

Result: SPO-RC+ effectively manages constraint uncertainty; truncation and reweighting boost performance.

Conclusion: SPO-RC+ is a viable approach for CSLO with uncertain constraints, enhanced by truncation and reweighting.

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [555] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/pdf/2505.22899)
*Naram Mhaisen, George Iosifidis*

Main category: cs.LG

TL;DR: The paper revisits FTRL for OCO, showing it can achieve dynamic regret bounds by optimizing future costs and pruning past costs, overcoming prior limitations.


<details>
  <summary>Details</summary>
Motivation: Addressing FTRL's limitations in dynamic environments by leveraging its ability to produce agile iterates and improving dynamic regret guarantees.

Method: Optimistic composition of future costs and careful linearization/pruning of past costs to synchronize algorithm state and iterates.

Result: FTRL recovers known dynamic regret bounds, offers refined control, and avoids cyclic dependence, with minimal recursive regularization.

Conclusion: FTRL's dynamic regret issues stem from state-iterate decoupling, not lazy projection; pruning synchronizes them, enabling better performance.

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [556] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/pdf/2505.22904)
*Youngsoo Choi, Siu Wun Cheung, Youngkyu Kim, Ping-Hsuan Tsai, Alejandro N. Diaz, Ivan Zanardi, Seung Whan Chung, Dylan Matthew Copeland, Coleman Kendrick, William Anderson, Traian Iliescu, Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: The paper proposes a formal definition of foundation models in computational science, emphasizing generality, reusability, and scalability, and introduces the Data-Driven Finite Element Method (DD-FEM) as a framework to address key challenges.


<details>
  <summary>Details</summary>
Motivation: The term "foundation model" lacks a universally accepted definition in computational science, leading to confusion. This paper aims to clarify and formalize the concept.

Method: The authors propose a formal definition and essential characteristics for foundation models in computational science. They introduce DD-FEM, combining classical FEM with data-driven learning.

Result: DD-FEM demonstrates scalability, adaptability, and physics consistency, addressing challenges in realizing foundation models for computational science.

Conclusion: The work bridges traditional numerical methods with AI, providing a rigorous foundation for future foundation models in computational science.

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [557] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/pdf/2505.22913)
*Donghyeon Joo, Helya Hosseini, Ramyad Hadidi, Bahar Asgari*

Main category: cs.LG

TL;DR: Unstructured sparsity improves KV cache compression in LLMs, achieving up to 70% sparsity without accuracy loss or fine-tuning. A custom attention kernel and bitmap-based format enable 45% compression and 2.23x throughput boost.


<details>
  <summary>Details</summary>
Motivation: KV cache size limits decode performance due to memory overhead. Addressing this bottleneck improves efficiency for large context lengths.

Method: Per-token magnitude-based pruning for Key and Value caches, combined with a bitmap-based sparse format and custom attention kernel.

Result: Achieves 70% sparsity without accuracy loss, 45% compression of KV cache, and 2.23x throughput increase.

Conclusion: Unstructured sparsity and custom kernels effectively compress KV caches, enhancing performance and enabling longer contexts.

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [558] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/pdf/2505.22922)
*Athanasios Glentis, Jiaxiang Li, Qiulin Shang, Andi Han, Ioannis Tsaknakis, Quan Wei, Mingyi Hong*

Main category: cs.LG

TL;DR: The paper explores efficient pre-training methods for large language models (LLMs), comparing techniques like LoRA, proposing new methods (weight refactorization and momentum reset), and demonstrating improved performance and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational challenges of pre-training LLMs, the study aims to enhance efficiency without compromising performance, focusing on parameter- and memory-efficient methods.

Method: The study includes a survey of efficient pre-training methods, benchmarks of memory-efficient approaches, and introduces two techniques (weight refactorization and momentum reset) to improve low-rank methods.

Result: Full-rank training performs best with proper optimization, while low-rank methods improve with high-rank updates. The proposed techniques achieve better perplexity and 25% memory savings compared to GaLore and Fira.

Conclusion: Efficient pre-training is achievable with the right methods, and the proposed techniques enhance performance and memory usage, narrowing the gap with full-model training.

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [559] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/pdf/2505.22935)
*Jipeng Li, Yanning Shen*

Main category: cs.LG

TL;DR: The paper challenges the necessity of explicit noise-level conditioning in Graph Diffusion Models (GDMs) by showing denoisers can implicitly infer noise levels from corrupted graph structures.


<details>
  <summary>Details</summary>
Motivation: To explore if denoisers can infer noise levels without explicit conditioning, simplifying GDM architectures.

Method: Developed a theoretical framework for Bernoulli edge-flip corruptions and extended it to coupled structure-attribute noise, validated empirically on synthetic and real-world datasets.

Result: Unconditional GDMs matched or outperformed conditioned ones, reducing parameters (4-6%) and computation time (8-10%).

Conclusion: Graph data's high-dimensional nature often provides enough information for denoising, enabling simpler, more efficient GDM designs.

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [560] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/pdf/2505.22949)
*Michael Sun, Orion Foo, Gang Liu, Wojciech Matusik, Jie Chen*

Main category: cs.LG

TL;DR: A grammar-based method for decoding DAGs into unique sequential representations, enabling applications like generative modeling and Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Decoding DAGs is challenging due to multiple topological orders; a principled approach is needed for sequential representation.

Method: Proposes a grammar-based approach, treating DAGs as derivations over an unambiguous grammar to generate unique sequences.

Result: Creates a compact, equivalent sequential representation of DAGs, useful for generative models, property prediction, and Bayesian optimization.

Conclusion: The method provides a principled way to decode DAGs, with broad applications in structured data tasks.

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [561] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/pdf/2505.22973)
*Bahareh Tolooshams, Aditi Chandrashekar, Rayhan Zirvi, Abbas Mammadov, Jiachen Yao, Chuwei Wang, Anima Anandkumar*

Main category: cs.LG

TL;DR: EquiReg diffusion improves diffusion-based inverse problem solvers by regularizing posterior sampling, outperforming state-of-the-art methods in image restoration and PDE reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based inverse solvers use isotropic Gaussian approximations, leading to inconsistent reconstructions due to deviations from the data manifold.

Method: EquiReg reweights diffusion trajectories and penalizes off-manifold samples using a distribution-dependent equivariance error.

Result: EquiReg outperforms state-of-the-art diffusion models in linear/nonlinear image restoration and PDE reconstruction.

Conclusion: EquiReg provides a robust framework for enhancing diffusion-based inverse problem solvers by maintaining data manifold consistency.

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [562] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/pdf/2505.22984)
*Guancheng Zhou, Haiping Xu, Hongkang Xu, Chenyu Li, Donghui Yan*

Main category: cs.LG

TL;DR: The paper addresses fairness issues in K-means clustering by proposing a two-stage optimization method to adjust cluster membership, improving fairness with minimal impact on clustering quality.


<details>
  <summary>Details</summary>
Motivation: K-means clustering can unfairly distribute points from sensitive groups (e.g., gender, race), leading to bias and social consequences. This work aims to mitigate such fairness issues.

Method: A two-stage optimization approach: first, perform clustering; then, adjust cluster membership for a small subset of data points. Two algorithms are proposed to identify fairness-expensive points.

Result: Experiments show significant fairness improvements with negligible impact on clustering quality. The methods are adaptable to other clustering algorithms or fairness metrics.

Conclusion: The proposed approach effectively enhances fairness in K-means clustering while preserving clustering performance, with potential for broader applications.

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [563] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/pdf/2505.22985)
*Masaharu Kagiyama, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: PatchEchoClassifier is an energy-efficient model for HAR using ESN and knowledge distillation, achieving 80% accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight, energy-efficient classifier for HAR in edge computing environments.

Method: Uses Echo State Network (ESN) with patch-level tokenizer and knowledge distillation from an MLP-Mixer teacher.

Result: Achieves over 80% accuracy with significantly reduced FLOPS (1/6 of DeepConvLSTM).

Conclusion: PatchEchoClassifier is promising for real-time, energy-efficient HAR.

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [564] [Model-Preserving Adaptive Rounding](https://arxiv.org/pdf/2505.22988)
*Albert Tseng, Zhaofeng Sun, Christopher De Sa*

Main category: cs.LG

TL;DR: YAQA introduces an adaptive rounding algorithm for LLM PTQ, using Kronecker-factored Hessian approximations to minimize full-model KL divergence, outperforming localized methods.


<details>
  <summary>Details</summary>
Motivation: Current PTQ methods focus on localized activation error, ignoring the impact of subsequent layers, leading to suboptimal model compression.

Method: YAQA uses Kronecker-factored Hessian approximations and a quantizer-independent rounding algorithm to minimize full-model KL divergence.

Result: YAQA reduces KL divergence by ≈30% and achieves state-of-the-art downstream task performance.

Conclusion: YAQA provides a more effective PTQ approach by considering full-model effects, improving compression and task performance.

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [565] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/pdf/2505.22991)
*Behzad Kamgar-Parsi, Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: The paper provides rigorous bounds for the hyperparameter λ in regularized k-means clustering, assuming ideal clusters, and analyzes additive and multiplicative regularization to reduce solution ambiguity.


<details>
  <summary>Details</summary>
Motivation: Determining the correct number of clusters in unlabeled datasets is crucial, but lacks principled guidelines for setting the regularization hyperparameter λ.

Method: Derives bounds for λ under ideal cluster assumptions, compares additive and multiplicative regularization, and tests performance with non-ideal clusters.

Result: Additive regularization often yields multiple solutions; multiplicative regularization and consensus between methods can reduce ambiguity.

Conclusion: The study offers practical insights for setting λ and improving cluster count accuracy, even when clusters deviate from ideal assumptions.

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [566] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/pdf/2505.22994)
*Ari S. Benjamin, Kyle Daruwalla, Christian Pehle, Anthony M. Zador*

Main category: cs.LG

TL;DR: The paper proposes learning weight manifolds in neural networks, inspired by neuromodulation, to efficiently generalize across tasks by conditioning on context variables.


<details>
  <summary>Details</summary>
Motivation: To improve task efficiency and knowledge reuse in neural networks by mimicking biological neuromodulation, where parameters are modulated by context rather than input injection.

Method: Derives a formal optimization of smooth manifolds in weight space, minimizing loss with constraints on volumetric movement. Manifolds (e.g., lines, circles) are predefined for inductive biases.

Result: Manifold-based conditioning outperforms input concatenation, generalizing better to out-of-distribution samples.

Conclusion: Modulating weights over low-dimensional manifolds is a principled and effective alternative to traditional conditioning methods.

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [567] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/pdf/2505.22998)
*Jihwan Oh, Murad Aghazada, Se-Young Yun, Taehyeon Kim*

Main category: cs.LG

TL;DR: The paper introduces a framework to improve LLMs' bargaining skills using utility-based feedback, a new benchmark (BargainArena), and human-aligned metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' limitations in strategic depth and adaptation to complex human factors in bargaining.

Method: Developed BargainArena benchmark, utility-based metrics, and a structured feedback mechanism for iterative strategy refinement.

Result: LLMs often misalign with human preferences, but the feedback mechanism improves performance and strategic reasoning.

Conclusion: The framework enhances LLMs' bargaining capabilities, promoting opponent-aware reasoning and strategic depth.

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [568] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/pdf/2505.23003)
*Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, Sunil Gupta*

Main category: cs.LG

TL;DR: HYDRO is a hybrid robust RL framework combining offline data and online simulator to improve sample efficiency by minimizing performance gaps and using novel sampling techniques.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of offline robust RL, such as limited data and dynamics mismatch, by leveraging an online simulator to complement offline datasets.

Method: HYDRO measures and minimizes performance gaps between the simulator and worst-case models, using uncertainty filtering and prioritized sampling to select reliable simulator samples.

Result: HYDRO outperforms existing methods in various tasks, demonstrating improved sample efficiency.

Conclusion: HYDRO effectively bridges the gap between offline and online data, enhancing robust RL performance with limited datasets.

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [569] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/pdf/2505.23004)
*Kyle R. Chickering, Bangzheng Li, Muhao Chen*

Main category: cs.LG

TL;DR: QLIP is a drop-in replacement for CLIP in MLLMs, addressing its limitations (mesoscopic and interpolation biases) without requiring retraining. It improves visual understanding and boosts performance on benchmarks like V*.


<details>
  <summary>Details</summary>
Motivation: CLIP's vision encoder has limitations (fixed input resolution, poor dissimilar image embeddings) and replacing it is costly. QLIP aims to solve these issues efficiently.

Method: QLIP uses an image quadtree for content-aware patchification, replacing CLIP's uniform grid patches. It integrates seamlessly with existing MLLMs like LLaVA.

Result: QLIP enhances LLaVA's visual question answering accuracy across model sizes and improves detailed understanding on the V* benchmark by up to 13.6%.

Conclusion: QLIP is an efficient, plug-and-play solution to CLIP's limitations, improving MLLM performance without retraining.

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [570] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/pdf/2505.23013)
*Liangkai Hang, Junjie Yao, Zhiwei Bai, Tianyi Chen, Yang Chen, Rongjie Diao, Hezhou Li, Pengxiao Lin, Zhiwei Wang, Cheng Xu, Zhongwang Zhang, Zhangchen Zhou, Zhiyu Li, Zehao Lin, Kai Chen, Feiyu Xiong, Yaoyu Zhang, Weinan E, Hongkang Yang, Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: Complexity control via initialization rate and weight decay improves LLM scaling laws, enhancing generalizability across model and data sizes.


<details>
  <summary>Details</summary>
Motivation: To enhance the generalizability of large language models (LLMs) through fundamental approaches like complexity control.

Method: Adjusting initialization rate and weight decay coefficient to control model complexity, tested on 2.4B models pretrained on 1T tokens.

Result: Constant initialization rate (exponent of std) improves scaling law descent in model and data sizes, outperforming fixed initialization std.

Conclusion: Complexity control is a promising direction for advancing LLMs, offering consistent improvements in scaling laws.

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [571] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/pdf/2505.23014)
*Juwei Yue, Haikuo Li, Jiawei Sheng, Xiaodong Li, Taoyu Su, Tingwen Liu, Li Guo*

Main category: cs.LG

TL;DR: The paper proposes using hyperbolic PDEs to model message passing in GNNs, enhancing interpretability and performance by explicitly capturing topological features.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs lack explicit topological feature learning, limiting their interpretability and effectiveness.

Method: Formulates message passing as hyperbolic PDEs, mapping node representations into a solution space spanned by eigenvectors, and approximates filters with polynomials.

Result: The approach improves flexibility and performance of spectral GNNs across various graph tasks.

Conclusion: Hyperbolic PDEs offer a powerful paradigm for enhancing spectral GNNs by explicitly leveraging topological structure.

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [572] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/pdf/2505.23017)
*Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, Chenjuan Guo*

Main category: cs.LG

TL;DR: $K^2$VAE is a VAE-based model for long-term probabilistic time series forecasting, using KoopmanNet and KalmanNet to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on short-term forecasting, neglecting challenges in long-term probabilistic forecasting due to nonlinear dynamics and inefficiency.

Method: $K^2$VAE combines a KoopmanNet to linearize time series and a KalmanNet to refine predictions and model uncertainty in the linear system.

Result: $K^2$VAE outperforms state-of-the-art methods in both short- and long-term forecasting, reducing error accumulation.

Conclusion: $K^2$VAE provides an efficient and accurate solution for long-term probabilistic time series forecasting.

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [573] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/pdf/2505.23022)
*Yinghao Tang, Tingfeng Lan, Xiuqi Huang, Hui Lu, Wei Chen*

Main category: cs.LG

TL;DR: SCORPIO is an SLO-oriented LLM serving system that improves goodput and SLO adherence by exploiting SLO heterogeneity for adaptive scheduling.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems prioritize throughput over SLOs like TTFT and TPOT, leading to suboptimal performance.

Method: SCORPIO uses adaptive scheduling with TTFT Guard (deadline-first reordering) and TPOT Guard (VBS-based admission control and credit-based batching), supported by a predictive module.

Result: SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%.

Conclusion: SCORPIO effectively addresses SLO neglect in LLM serving systems, enhancing performance and adherence.

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [574] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/pdf/2505.23024)
*Zhihao Wang, Wenke Huang, Tian Chen, Zekun Shi, Guancheng Wan, Yu Qiao, Bin Yang, Jian Wang, Bing Li, Mang Ye*

Main category: cs.LG

TL;DR: This paper explores prompt learning in Vision Language Models (VLMs) within federated learning (FL) settings, focusing on differences between language (LPT) and vision (VPT) prompt learning under data heterogeneity. It evaluates FL and prompt configurations and proposes strategies for robustness.


<details>
  <summary>Details</summary>
Motivation: The study addresses the underexplored application of prompt learning with VLMs in FL, particularly under data heterogeneity challenges like label skew and domain shift.

Method: The paper conducts experiments to compare LPT and VPT in FL, testing various configurations (client scale, aggregation strategies, prompt length) and exploring strategies for complex scenarios.

Result: Findings provide insights into optimizing prompt learning in FL, including leveraging both prompt types when resources allow, enhancing robustness in heterogeneous data settings.

Conclusion: The work contributes to deploying VLMs in privacy-preserving FL environments by offering practical solutions for prompt learning challenges.

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [575] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/pdf/2505.23027)
*Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper proposes Diverse Prototypical Ensembles (DPEs) to address subpopulation shift in machine learning, outperforming prior methods in worst-group accuracy.


<details>
  <summary>Details</summary>
Motivation: Subpopulation shift degrades model performance, and current solutions rely on unavailable assumptions about subpopulations.

Method: Replace a linear classifier with a mixture of prototypical classifiers, each focusing on different features and samples.

Result: DPEs outperform state-of-the-art methods on nine real-world datasets.

Conclusion: DPEs effectively address subpopulation shift without requiring group annotations.

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [576] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/pdf/2505.23032)
*Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee*

Main category: cs.LG

TL;DR: A Bayesian framework using Prior-data Fitted Networks (PFNs) is proposed for neural scaling law extrapolation, outperforming point estimation methods by quantifying uncertainty, especially in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing scaling law methods lack uncertainty quantification, which is critical for decision-making in resource allocation.

Method: A Bayesian framework with PFNs is designed, using a prior distribution to sample synthetic functions resembling real scaling laws, enabling meta-learning for extrapolation.

Result: The method shows superior performance over point estimation and other Bayesian approaches, particularly in data-limited settings like active learning.

Conclusion: The approach provides reliable, uncertainty-aware extrapolation, making it valuable for practical applications requiring decision-making under uncertainty.

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [577] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/pdf/2505.23042)
*Siwen Wang, Shitou Zhang, Wan-Lin Chen, Dung Truong, Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: The study evaluates Large EEG Models (LEMs) by fine-tuning LaBraM on real-world stress classification data, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To assess LEMs' applicability in real-world environments, unlike previous studies focused on controlled clinical settings.

Method: Fine-tuned LaBraM on resting-state EEG data from 18 graduate students to classify stress states, testing robustness under data shuffling and reduced channels.

Result: Achieved 90.47% balanced accuracy with a 5-second window, outperforming traditional classifiers in accuracy and efficiency.

Conclusion: LEMs show promise for real-world EEG processing, potentially revolutionizing brain-computer interfaces by shifting to data-centric design.

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [578] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/pdf/2505.23048)
*Tianci Bu, Le Zhou, Wenchuan Yang, Jianhong Mou, Kang Yang, Suoyi Tan, Feng Yao, Jingyuan Wang, Xin Lu*

Main category: cs.LG

TL;DR: ProDiff is a trajectory imputation framework using minimal endpoint data, leveraging prototype learning and diffusion models for accurate reconstruction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory imputation methods rely on sparse data, assuming retained behavioral patterns, which is data-intensive and overlooks large-scale trajectory embeddings.

Method: ProDiff integrates prototype learning for movement patterns and a denoising diffusion probabilistic model for spatiotemporal reconstruction, trained with a tailored loss function.

Result: ProDiff improves accuracy by 6.28% on FourSquare and 2.52% on WuXi, with a 0.927 correlation between generated and real trajectories.

Conclusion: ProDiff effectively addresses trajectory incompleteness with minimal input, demonstrating superior performance and robustness.

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [579] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/pdf/2505.23049)
*Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, Yanmin Qian*

Main category: cs.LG

TL;DR: The paper introduces DenoiseRotator, a method to enhance pruning robustness in LLMs by redistributing parameter importance, improving performance under sparsity constraints.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods focus on individual weight importance, leading to performance degradation, especially under semi-structured sparsity. The paper aims to make models more pruning-friendly by concentrating importance on fewer weights.

Method: DenoiseRotator applies learnable orthogonal transformations to weight matrices, minimizing information entropy of normalized importance scores. It is model-agnostic and compatible with existing pruning techniques.

Result: DenoiseRotator improves perplexity and zero-shot accuracy on models like LLaMA3, Qwen2.5, and Mistral under 50% unstructured and 2:4 semi-structured sparsity. For LLaMA3-70B, it reduces the perplexity gap by 58%.

Conclusion: DenoiseRotator effectively enhances pruning robustness by redistributing parameter importance, offering a scalable and compatible solution for LLM compression.

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [580] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/pdf/2505.23055)
*Zhen Xiang, Aliyah R. Hsu, Austin V. Zane, Aaron E. Kornblith, Margaret J. Lin-Martore, Jasmanpreet C. Kaur, Vasuda M. Dokiparthi, Bo Li, Bin Yu*

Main category: cs.LG

TL;DR: CDR-Agent, an LLM-based system, improves emergency department decision-making by autonomously applying Clinical Decision Rules (CDRs) from unstructured notes, outperforming standalone LLMs in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The complexity and urgency of clinical decision-making in EDs, coupled with cognitive load hindering CDR application, drive the need for an automated solution.

Method: CDR-Agent uses LLMs to identify and apply CDRs from unstructured clinical notes, validated on synthetic and CDR-Bench datasets.

Result: CDR-Agent achieves 56.3% (synthetic) and 8.7% (CDR-Bench) accuracy gains over standalone LLMs, reduces computational overhead, and optimizes imaging decisions.

Conclusion: CDR-Agent enhances ED decision-making by efficiently selecting CDRs and minimizing unnecessary interventions, outperforming traditional LLM approaches.

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [581] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/pdf/2505.23061)
*Tarun Suresh, Debangshu Banerjee, Shubham Ugare, Sasa Misailovic, Gagandeep Singh*

Main category: cs.LG

TL;DR: DINGO is a dynamic programming-based constrained decoding strategy for diffusion LLMs, ensuring structured outputs while preserving the model's distribution.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion LLMs lack the ability to enforce formal constraints like regular expressions, making them unreliable for structured tasks.

Method: Proposes DINGO, a dynamic programming approach for constrained decoding in parallel token prediction.

Result: DINGO improves structured output generation by up to 68 percentage points over unconstrained inference.

Conclusion: DINGO effectively bridges the gap between diffusion LLMs and structured output requirements.

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [582] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/pdf/2505.23062)
*Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe*

Main category: cs.LG

TL;DR: CompFlow improves RL sample efficiency by addressing dynamics discrepancies between source and target environments using flow matching and optimal transport, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling dynamics discrepancies in RL rely on ill-defined metrics like KL divergence, which fail when source and target dynamics have disjoint support.

Method: CompFlow models target dynamics as a conditional flow built on source-domain flow, using Wasserstein distance for principled dynamics gap estimation and an optimistic active data collection strategy.

Result: CompFlow outperforms baselines in RL benchmarks with shifted dynamics, demonstrating improved generalization and reduced performance disparity.

Conclusion: CompFlow provides a robust solution for leveraging offline data in RL by addressing dynamics gaps theoretically and empirically.

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [583] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/pdf/2505.23063)
*Denis Mamba Kabala, Adel Hafiane, Laurent Bobelin, Raphael Canals*

Main category: cs.LG

TL;DR: A Decentralized Federated Learning (DFL) framework is introduced for crop disease detection, improving accuracy, convergence, and privacy in agriculture.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy and scalability issues in centralized Federated Learning for crop disease classification.

Method: Proposes DFL using validation loss for model sharing and adaptive loss correction, tested with ResNet50, VGG16, and ViT_B16 on PlantVillage datasets.

Result: DFL enhances accuracy, convergence speed, generalization, and robustness in heterogeneous data environments.

Conclusion: DFL is effective for privacy-preserving agricultural applications, outperforming traditional FL methods.

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [584] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/pdf/2505.23071)
*Peizheng Guo, Jingyao Wang, Huijie Guo, Jiangmeng Li, Chuxiong Sun, Changwen Zheng, Wenwen Qiang*

Main category: cs.LG

TL;DR: BOGC-MML introduces a Bayesian-Oriented Gradient Calibration method for Multi-Modal Learning, addressing gradient uncertainty to improve predictive accuracy by balancing sensitivity and conservatism in updates.


<details>
  <summary>Details</summary>
Motivation: Existing MML methods use fixed weights for gradient aggregation, ignoring intrinsic gradient uncertainty, which can degrade performance or hinder learning.

Method: BOGC-MML models gradient uncertainty per modality, derives probability distributions, converts precision into evidence, and fuses uncertainties using Dempster-Shafer rule for calibrated updates.

Result: Experiments on benchmark datasets show BOGC-MML's effectiveness in improving predictive accuracy.

Conclusion: BOGC-MML successfully addresses gradient uncertainty in MML, enhancing model optimization and performance.

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [585] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/pdf/2505.23084)
*Chang Yu, Fang Liu, Jie Zhu, Shaobo Guo, Yifan Gao, Zhongheng Yang, Meiwei Liu, Qianwen Xing*

Main category: cs.LG

TL;DR: A hybrid framework combining LSTM, LightGBM, and CatBoost for stock price prediction outperforms individual models by 10-15% in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve stock price prediction accuracy by leveraging ensemble methods combining sequential (LSTM) and tree-based (LightGBM, CatBoost) approaches.

Method: Processes time-series financial data using seven models (ANNs, CNNs, BiLSTM, LSTM, XGBoost, LightGBM, NNs) and benchmarks performance with MAE, R-squared, MSE, RMSE. Develops an ensemble model.

Result: The hybrid framework improves accuracy by 10-15% and reduces error during market changes.

Conclusion: Ensemble methods are effective for financial forecasting, and the framework offers flexibility for integrating new techniques.

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [586] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/pdf/2505.23086)
*Junyi An, Xinyu Lu, Chao Qu, Yunfei Shi, Peijia Lin, Qianwei Tang, Licheng Xu, Fenglei Cao, Yuan Qi*

Main category: cs.LG

TL;DR: The paper introduces the Equivariant Spherical Transformer (EST), a novel SE(3)-equivariant GNN framework, to address limitations in expressiveness and non-linearity of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SE(3)-equivariant GNNs suffer from insufficient non-linearity and incomplete group representations, limiting their expressiveness.

Method: The proposed EST leverages a Transformer structure in the spatial domain of group representations post-Fourier transform, ensuring equivariance and superior expressiveness.

Result: EST achieves state-of-the-art performance on molecular benchmarks like OC20 and QM9.

Conclusion: EST overcomes the limitations of tensor product-based convolutions and sets a new benchmark in molecular system modeling.

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [587] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/pdf/2505.23094)
*Chongjie Si, Zhiyi Shi, Yadao Wang, Xiaokang Yang, Susanto Rahardja, Wei Shen*

Main category: cs.LG

TL;DR: MAP is a novel framework for parameter-efficient fine-tuning (PEFT) that rigorously decouples weight adaptation into direction and magnitude, improving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods lack a principled geometric foundation for direction adaptation, limiting their effectiveness.

Method: MAP reformulates weight matrices as vectors, normalizes pre-trained weights, learns directional updates, and scales magnitudes independently.

Result: MAP significantly enhances performance when combined with existing PEFT methods.

Conclusion: MAP is a universal and simple enhancement for PEFT, proposed as a default for future methods.

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [588] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/pdf/2505.23116)
*Pengfei Zhou, Yunlong Liu, Junli Liang, Qi Song, Xiangyang Li*

Main category: cs.LG

TL;DR: CrossLinear is a novel linear-based forecasting model with a cross-correlation embedding module for efficient time series forecasting with exogenous variables.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with differentiating endogenous and exogenous variables, leading to inefficiencies and overfitting.

Method: CrossLinear uses a plug-and-play cross-correlation embedding module to capture dependencies between variables, patch-wise processing, and a global linear head for temporal dependencies.

Result: Experiments on 12 datasets show superior performance in short- and long-term forecasting, with the module proving effective and generalizable.

Conclusion: CrossLinear offers consistent performance improvements and is adaptable for various forecasting tasks.

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [589] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/pdf/2505.23098)
*Kuan Xu, Zhiguang Cao, Chenlong Zheng, Linong Liu*

Main category: cs.LG

TL;DR: Proposes RL-AVNS, a reinforcement learning-based adaptive variable neighborhood search method for VRPMTW, outperforming traditional and learning-based heuristics.


<details>
  <summary>Details</summary>
Motivation: To improve solution quality and efficiency for VRPMTW by dynamically selecting operators using real-time states and learned experience.

Method: Integrates reinforcement learning with a fitness metric for temporal flexibility and a transformer-based policy network for operator selection.

Result: RL-AVNS outperforms VNS, AVNS, and learning-based heuristics, showing better generalization to unseen instances.

Conclusion: RL-AVNS is practical for complex logistics, offering superior performance and adaptability.

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [590] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/pdf/2505.23117)
*Yuatyong Chaichana, Thanapat Trachu, Peerat Limkonchotiwat, Konpat Preechakul, Tirasan Khandhawit, Ekapol Chuangsuwanich*

Main category: cs.LG

TL;DR: DRM is a model merging method using Singular Value Decomposition to align weight matrices, outperforming existing techniques in multitask learning.


<details>
  <summary>Details</summary>
Motivation: Traditional merging assumes identical function for weight matrix entries, ignoring finetuned networks' complexity. DRM addresses this gap.

Method: DRM decomposes and renormalizes weight matrices via SVD, enabling aligned entry-wise merging.

Result: DRM outperforms state-of-the-art methods in various models (ViT, DeBERTa, T5, Llama3.1-8B) and settings.

Conclusion: Renormalization is key to DRM's success, creating a robust joint space for merging.

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [591] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/pdf/2505.23099)
*Chongjie Si, Xuankun Yang, Muqing Liu, Yadao Wang, Xiaokang Yang, Wenbo Su, Bo Zheng, Wei Shen*

Main category: cs.LG

TL;DR: The paper investigates how Parameter-Efficient Fine-Tuning (PEFT) modifies model parameters, revealing that fine-tuning amplifies top singular values and reorients dominant singular vectors. A novel method leveraging learnable rescaling of top singular directions is proposed, improving performance across tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the structural changes in weight matrices during fine-tuning and develop a more efficient method for task-specific adaptation of large-scale foundation models.

Method: Uses singular value decomposition (SVD) to analyze weight matrices during fine-tuning, identifying amplification of top singular values and reorientation of dominant singular vectors. Proposes a method with learnable rescaling of top singular directions.

Result: Fine-tuning primarily affects top singular values and dominant singular vectors, injecting task-specific knowledge into a low-dimensional subspace. The proposed method outperforms baselines across multiple tasks.

Conclusion: Structurally informed fine-tuning, focusing on top singular directions, is effective for efficient and precise model adaptation.

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [592] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/pdf/2505.23135)
*Zhe Ye, Zhengxu Yan, Jingxuan He, Timothe Kasriel, Kaiyu Yang, Dawn Song*

Main category: cs.LG

TL;DR: Verina is a benchmark for evaluating verifiable code generation in LLMs, revealing significant challenges in correctness, specification, and proof generation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for end-to-end verifiable code generation and improve LLM-based coding correctness.

Method: Introduces Verina, a manually curated benchmark with 189 Lean coding tasks, including specifications and proofs.

Result: State-of-the-art LLMs struggle, with OpenAI o4-mini achieving only 61.4% correct code, 51.0% sound specifications, and 3.6% successful proofs.

Conclusion: Verina aims to advance verifiable code generation by providing a rigorous benchmark, highlighting the need for better LLM-based theorem provers.

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [593] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/pdf/2505.23105)
*Abhishek Vijaya Kumar, Eric Ding, Arjun Devraj, Darius Bunandar, Rachee Singh*

Main category: cs.LG

TL;DR: LUMION is a reconfigurable optical fabric that dynamically integrates spare accelerators into ML workloads during failures, avoiding costly migrations and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods of migrating entire ML jobs to new racks for fault tolerance are inefficient and require idle resources.

Method: LUMION uses a reconfigurable optical fabric to dynamically replace failed accelerators with spares within the same rack.

Result: LUMION reduces recovery time to ~1 second and improves fine-tuning throughput by nearly 2X compared to traditional methods.

Conclusion: LUMION offers a resource-efficient solution for accelerator failures, enhancing performance and reducing downtime in ML datacenters.

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [594] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/pdf/2505.23165)
*Le Yang, Vincent Y. F. Tan, Wang Chi Cheung*

Main category: cs.LG

TL;DR: The paper addresses the best arm identification (BAI) problem with biased offline data, proposing the LUCB-H algorithm to adaptively balance offline and online data, showing improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by real-world scenarios like clinical trials where offline data may be biased, and adaptive algorithms without prior bias knowledge face limitations.

Method: The LUCB-H algorithm is introduced, incorporating adaptive confidence bounds and bias correction within the LUCB framework to handle biased offline data.

Result: LUCB-H matches standard LUCB's sample complexity when offline data is misleading and outperforms it when helpful, with theoretical and experimental validation.

Conclusion: LUCB-H effectively addresses BAI with biased offline data, offering robust and adaptable performance, supported by theoretical and empirical evidence.

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [595] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/pdf/2505.23106)
*Ning Liu, Yue Yu*

Main category: cs.LG

TL;DR: NIPS, a neural operator architecture, improves predictive accuracy and efficiency in modeling physical systems using linear attention and Fourier space convolution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning intricate physical systems, framed as ill-posed inverse PDE problems, by enhancing Nonlocal Attention Operators.

Method: NIPS uses a linear attention mechanism and a learnable kernel network in Fourier space, avoiding explicit computation of large pairwise interactions.

Result: NIPS outperforms NAO and other baselines in benchmarks, offering scalable, interpretable, and efficient physics learning.

Conclusion: NIPS represents a significant advancement in neural operator architectures for physics learning, with practical applications supported by available code and data.

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [596] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/pdf/2505.23181)
*Tian Tian, Chunyan Miao, Hangwei Qian*

Main category: cs.LG

TL;DR: The paper introduces Frequency Refined Augmentation (FreRA), a novel frequency-domain method for contrastive learning in time series classification, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing augmentation methods for contrastive learning in time series are borrowed from vision and may distort semantically relevant information. The paper addresses this by proposing a frequency-domain approach.

Method: FreRA separates critical and unimportant frequency components, using semantic-aware and semantic-agnostic modifications to preserve and infuse variance, respectively.

Result: FreRA outperforms ten baselines on benchmark datasets (UCR, UEA) and large-scale datasets, excelling in classification, anomaly detection, and transfer learning.

Conclusion: FreRA is a lightweight, effective, and plug-and-play solution for time series contrastive learning, demonstrating superior performance and generalization.

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [597] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/pdf/2505.23131)
*Xinyu Yao, Daniel Bourgeois, Abhinav Jain, Yuxin Tang, Jiawen Yao, Zhimin Ding, Arlei Silva, Chris Jermaine*

Main category: cs.LG

TL;DR: Doppler is a three-stage framework for optimizing operation assignment in dataflow graphs, outperforming baselines by reducing execution and training time.


<details>
  <summary>Details</summary>
Motivation: Address limitations in prior learning-based methods, such as under-utilization of devices, lack of system-awareness, and over-reliance on reinforcement learning.

Method: Uses dual-policy networks: SEL for operation selection and PLC for device placement.

Result: Outperforms baselines in reducing execution time and improves sampling efficiency.

Conclusion: Doppler effectively optimizes complex machine learning workloads by combining expert heuristics with learning-based policies.

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [598] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/pdf/2505.23195)
*Lifan Zhao, Yanyan Shen, Zhaoyang Liu, Xue Wang, Jiaji Deng*

Main category: cs.LG

TL;DR: Pruning pre-trained Time Series Foundation Models (TSFMs) before fine-tuning improves forecasting performance, often surpassing specialized models.


<details>
  <summary>Details</summary>
Motivation: Despite their zero-shot capabilities, TSFMs underperform smaller specialized models after fine-tuning, prompting the need for effective adaptation methods.

Method: A structured pruning method is proposed to focus fine-tuning on relevant and compact parameter spaces, leveraging inherent sparsity in TSFMs.

Result: Fine-tuning pruned TSFMs outperforms original models and achieves state-of-the-art performance on benchmarks.

Conclusion: The "prune-then-finetune" paradigm enhances TSFM adaptation, making them competitive with specialized models.

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [599] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/pdf/2505.23150)
*Michal Nauman, Marek Cygan, Carmelo Sferrazza, Aviral Kumar, Pieter Abbeel*

Main category: cs.LG

TL;DR: High-capacity value models with learnable task embeddings enable robust multi-task RL, outperforming single-task methods.


<details>
  <summary>Details</summary>
Motivation: Address task interference in online RL by improving multi-task training with scalable, high-capacity models.

Method: Train value models via cross-entropy with learnable task embeddings on diverse tasks.

Result: Achieves state-of-the-art performance on 280+ tasks, including humanoid control and vision-based RL.

Conclusion: The approach enables scalable, sample-efficient multi-task RL with superior transfer capabilities.

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [600] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/pdf/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: PMDG enables MDG algorithms in SDG settings by generating pseudo-domains from a single source, achieving comparable performance to multi-domain training.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail on unseen data distributions, and MDG is costly to implement due to multi-domain dataset requirements.

Method: PMDG uses style transfer and data augmentation to create pseudo-domains from a single source, tested via PseudoDomainBed.

Result: PMDG shows a positive correlation with MDG performance and can match or exceed multi-domain results with enough data.

Conclusion: PMDG offers a practical solution for domain generalization, with potential for future research.

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [601] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/pdf/2505.23247)
*Zonglin Yang, Zhexuan Gu, Houduo Qi, Yancheng Yuan*

Main category: cs.LG

TL;DR: The paper introduces a reward adjustment model to accelerate RLHF training by increasing reward variance while preserving preferences and expectations, leading to the GRPOVI algorithm.


<details>
  <summary>Details</summary>
Motivation: Efficient RLHF training remains challenging, and higher reward variance in initial policies speeds up training. The paper aims to address this by proposing a practical reward adjustment method.

Method: A reward adjustment model is proposed to increase reward variance while maintaining relative preferences and expectations. A novel O(n log n) algorithm solves the nonconvex optimization problem.

Result: The GRPOVI algorithm, integrating the reward adjustment model, significantly improves RLHF training efficiency compared to the original GRPO.

Conclusion: The reward adjustment model and GRPOVI algorithm enhance RLHF training efficiency, with potential broader implications for RLHF methods.

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [602] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/pdf/2505.23176)
*Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li*

Main category: cs.LG

TL;DR: The paper introduces three novel techniques (MUD, BKD, AAD) to enhance low-rank decomposition in federated learning, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address key issues in low-rank decomposition for federated learning (what to decompose, how to decompose, and how to aggregate) and improve training efficiency.

Method: Proposes Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD) to tackle the identified issues.

Result: The techniques achieve faster convergence and superior accuracy compared to baseline methods, supported by theoretical analysis and experiments.

Conclusion: The proposed methods are complementary and effective, offering optimal performance in federated learning.

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [603] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/pdf/2505.23182)
*Srijith Nair, Michael Lin, Amirreza Talebi, Peizhong Ju, Elizabeth Bentley, Jia Liu*

Main category: cs.LG

TL;DR: FSL-SAGE is a new federated split learning algorithm that combines the benefits of FL and SL while addressing their limitations, achieving efficient communication and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FL and SL have trade-offs in client memory, network latency, and accuracy. FSL-SAGE aims to improve efficiency and accuracy by estimating server-side gradients locally.

Method: FSL-SAGE uses auxiliary models to estimate server-side gradient feedback, enabling parallel client-side training while periodically adapting to server behavior.

Result: The algorithm achieves a convergence rate of O(1/√T), matching FedAvg, while reducing communication costs and client memory requirements. It outperforms existing FSL methods in efficiency and accuracy.

Conclusion: FSL-SAGE successfully balances communication efficiency and accuracy, making it a promising solution for distributed machine learning.

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [604] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/pdf/2505.23184)
*Hongcan Guo, Guoshun Nan, Yuan Yang, Diyang Zhang, Haotian Li, Zhican Chen, Qinchuan Zhou, Yuhan Ran, Xinye Cao, Sicong Leng, Xiaofeng Tao, Xudong Jiang*

Main category: cs.LG

TL;DR: RadarGate, a geometrically inspired gating method, enhances LoRA-based MoE scalability by introducing rotational operations, improving expressiveness and feature interactions in LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional gating mechanisms limit LLM scalability due to poor generalization and underfitting, caused by restricted expressiveness in weighted-sum mechanisms.

Method: RadarGate fuses LoRA representations using a learnable component and applies a rotation matrix to define angular relationships, enhancing cross-LoRA synergies.

Result: Experiments on 6 benchmarks across 21 tasks demonstrate RadarGate's effectiveness in scaling LoRAs, with contrastive rotations improving semantic alignment.

Conclusion: RadarGate addresses scalability issues in LoRA-based MoE, offering a simple yet effective solution with promising results and insights for future research.

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [605] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/pdf/2505.23185)
*Shahaf E. Finder, Ron Shapira Weber, Moshe Eliasof, Oren Freifeld, Eran Treister*

Main category: cs.LG

TL;DR: The paper addresses the limited Effective Receptive Field (ERF) in MPNNs, proposing an Interleaved Multiscale MPNN (IM-MPNN) to enhance long-range dependency capture without excessive depth or parameters.


<details>
  <summary>Details</summary>
Motivation: MPNNs struggle with over-squashing, limiting their ability to capture long-range dependencies, akin to ERF issues in CNNs.

Method: Proposes IM-MPNN, which uses hierarchical graph coarsening for multiscale message-passing to improve long-range interactions.

Result: Demonstrates significant improvements in capturing long-range dependencies on benchmarks like LRGB, maintaining computational efficiency.

Conclusion: IM-MPNN effectively addresses ERF limitations in MPNNs, enhancing performance without added complexity.

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [606] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/pdf/2505.23270)
*Haokun Chen, Yueqi Zhang, Yuan Bi, Yao Zhang, Tong Liu, Jinhe Bi, Jian Lan, Jindong Gu, Claudia Grosser, Denis Krompass, Nassir Navab, Volker Tresp*

Main category: cs.LG

TL;DR: The paper introduces an auditing framework to evaluate machine unlearning algorithms in LLMs, addressing privacy concerns and regulatory needs.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and regulatory requirements like GDPR necessitate effective unlearning methods for LLMs, but evaluating their efficacy is challenging.

Method: The authors propose an auditing framework with benchmark datasets, unlearning algorithms, and prompt-based and novel activation perturbation auditing methods.

Result: The framework evaluates the effectiveness and robustness of unlearning strategies, offering insights beyond traditional input-output methods.

Conclusion: The proposed framework advances the evaluation of unlearning algorithms, addressing limitations of existing methods and enhancing privacy compliance.

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [607] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/pdf/2505.23190)
*Yekun Zhu, Min Tang, Zheng Ma*

Main category: cs.LG

TL;DR: DeepRTE is a neural network approach for solving the Radiative Transfer Equation (RTE) with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The RTE is critical in fields like neutron transport, atmospheric radiative transfer, and optical imaging, but solving it is challenging.

Method: DeepRTE uses pre-trained attention-based neural networks to address the RTE.

Result: Numerical experiments confirm the framework's accuracy and computational efficiency.

Conclusion: DeepRTE offers an effective solution for the RTE, with broad applicability.

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [608] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/pdf/2505.23309)
*Yixin Ren, Chenghou Jin, Yewei Xia, Li Ke, Longtao Huang, Hui Xue, Hao Zhang, Jihong Guan, Shuigeng Zhou*

Main category: cs.LG

TL;DR: A novel CI testing method using score-based generative modeling improves Type I error control and testing power, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in CI testing with generative models, like GANs, due to poor conditional distribution modeling and instability, motivate a more robust approach.

Method: Uses sliced conditional score matching and Langevin dynamics for sampling, with a goodness-of-fit stage for validation.

Result: Theoretical error bounds are established, and experiments show superior performance over state-of-the-art methods.

Conclusion: The proposed method revitalizes generative model-based CI testing with precision and practicality.

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [609] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/pdf/2505.23194)
*Shiwei Li, Xiandi Luo, Xing Tang, Haozhao Wang, Hao Chen, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li*

Main category: cs.LG

TL;DR: Non-zero initialization of LoRA matrices A and B improves robustness to suboptimal learning rates without harming fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: The lack of theoretical support for zero initialization in LoRA layers prompted an investigation into the impact of non-zero initialization.

Method: Analyzed LoRA's fine-tuning dynamics from an infinite-width perspective, comparing zero and non-zero initialization of matrices A and B.

Result: Non-zero initialization enhances robustness to suboptimal learning rates, especially smaller ones, without degrading performance.

Conclusion: Fine-tuning doesn't strictly require starting from the pretrained model; non-zero initialization is a viable alternative.

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [610] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/pdf/2505.23223)
*Xingyuan Pan, Chenlu Ye, Joseph Melkonian, Jiaqi W. Ma, Tong Zhang*

Main category: cs.LG

TL;DR: Daunce is a scalable and accurate data attribution method for models, including LLMs, using uncertainty estimation via perturbed models.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based TDA methods are limited in scalability, while random projection-based methods sacrifice accuracy. Daunce addresses these gaps by leveraging uncertainty estimation.

Method: Fine-tunes perturbed models and computes covariance of per-example losses as attribution scores.

Result: Daunce scales to LLMs, outperforms existing TDA methods in accuracy, and works with black-box models like GPT.

Conclusion: Daunce enables effective data attribution for large models, including proprietary LLMs, advancing applications like data debugging and valuation.

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [611] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/pdf/2505.23337)
*Chetan Verma, Aditya Srinivas Timmaraju, Cho Jui-Hsieh, Suyash Damle, Ngot Bui, Yang Zhang, Wen Chen, Xin Liu, Prateek Jain, Inderjit S Dhillon*

Main category: cs.LG

TL;DR: MatTA is a framework for training multiple accurate Student models using a Teacher-TA-Student recipe, enabling trade-offs between accuracy and serving costs with just one training run.


<details>
  <summary>Details</summary>
Motivation: Industry-grade ML models require significant resources for development to meet evolving serving constraints. MatTA aims to streamline this by producing multiple servable models efficiently.

Method: Uses a Teacher-TA-Student recipe where TA models (larger versions of Students) bridge the gap between Teacher and Student models, enabling better knowledge transfer and domain-specific expertise.

Result: Demonstrated 20% improvement in a production ML system and 24% on SAT Math, 10% on LAMBADA with GPT-2 Medium.

Conclusion: MatTA efficiently produces multiple accurate Student models, reducing development effort while maintaining performance.

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [612] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/pdf/2505.23225)
*Fabiano Veglianti, Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [613] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/pdf/2505.23349)
*Sheng Ouyang, Yulan Hu, Ge Chen, Qingyang Li, Fuzheng Zhang, Yong Liu*

Main category: cs.LG

TL;DR: The paper addresses reward unfairness in RLHF, proposing bias-agnostic methods (Fairness Regularization and Fairness Coefficient) to improve fairness in reward allocation, leading to better alignment of LLMs with human preferences.


<details>
  <summary>Details</summary>
Motivation: Rewards in RLHF can be biased, harming LLM alignment. The paper aims to mitigate these biases collectively as 'reward unfairness' without targeting specific biases individually.

Method: Models preference learning as a resource allocation problem, introducing Fairness Regularization and Fairness Coefficient to balance utility and fairness in reward distribution.

Result: Experiments show the methods improve fairness in reward models and policy models, aligning LLMs more fairly with human preferences.

Conclusion: The proposed bias-agnostic approach effectively addresses reward unfairness, enhancing the fairness of LLM alignment with human preferences.

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [614] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/pdf/2505.23228)
*Wanfu Gao, Jun Gao, Qingqi Han, Hanlin Pan, Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper addresses challenges in multi-label datasets by proposing a method using random walk graphs and low-rank decomposition to capture nonlinear associations and align feature-label spaces.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in feature dimension complicates feature-label relationships, and existing linear decomposition methods fail to capture nonlinear associations, leading to misalignment.

Method: The authors design a random walk graph integrating feature-feature, label-label, and feature-label relationships, optimize latent representations via low-rank decomposition, and align spaces while preserving manifold structure.

Result: Extensive experiments on benchmark datasets show the proposed method's superiority.

Conclusion: The innovative approach effectively captures nonlinear associations and aligns spaces, outperforming existing methods.

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [615] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/pdf/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: The paper explores the relationship between stochastic and deterministic policy gradients in continuous control, showing their equivalence in certain MDPs and proposing a method to unify them.


<details>
  <summary>Details</summary>
Motivation: To understand and bridge the gap between stochastic and deterministic policy gradients in continuous control settings.

Method: Analyzes MDPs with Gaussian control noise and quadratic costs, showing equivalence of gradients and value functions, and develops a procedure to construct equivalent deterministic MDPs.

Result: Stochastic and deterministic policy gradients, natural gradients, and state value functions are identical in the studied MDPs, while state-control value functions differ.

Conclusion: Policy gradient methods can be unified by approximating state value functions instead of state-control value functions.

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [616] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/pdf/2505.23246)
*Honoka Anada, Tatsuya Kaneko, Shinya Takamaeda-Yamazaki*

Main category: cs.LG

TL;DR: The paper introduces DFL-Shapley and DFL-MR, novel methods for measuring participant contributions in decentralized federated learning (DFL), addressing challenges posed by the absence of a central server.


<details>
  <summary>Details</summary>
Motivation: Existing contribution measurement methods are designed for centralized federated learning (CFL) and are unsuitable for DFL, which lacks a central server and involves multiple global models.

Method: The authors propose DFL-Shapley, an adaptation of the Shapley value for DFL, and DFL-MR, a practical approximation that accumulates round-wise Shapley values.

Result: Experiments validate DFL-Shapley as a ground-truth metric and show DFL-MR closely approximates it, proving their effectiveness in DFL.

Conclusion: The study successfully addresses the gap in contribution measurement for DFL, providing scalable and accurate metrics.

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [617] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/pdf/2505.23369)
*Mannmohan Muthuraman*

Main category: cs.LG

TL;DR: DSBP improves neural network training under resource constraints by projecting gradients onto principal eigenvectors, outperforming SAM, LoRA, and MAML.


<details>
  <summary>Details</summary>
Motivation: Address challenges in robustness, few-shot learning, and hardware efficiency under resource constraints.

Method: Uses dynamic spectral backpropagation with five extensions: dynamic spectral inference, spectral architecture optimization, spectral meta-learning, spectral transfer regularization, and Lie algebra-inspired dynamics.

Result: Outperforms SAM, LoRA, and MAML on CIFAR-10, Fashion MNIST, MedMNIST, and Tiny ImageNet.

Conclusion: Future work focuses on scalability, bias mitigation, and ethical considerations.

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [618] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/pdf/2505.23264)
*Fangyikang Wang, Hubery Yin, Shaobin Zhuang, Huminhao Zhu, Yinan Li, Lei Qian, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li*

Main category: cs.LG

TL;DR: The paper introduces efficient algorithms for approximating the diffusion Fisher information (DF) in diffusion models, avoiding time-consuming auto-differentiation and providing error bounds.


<details>
  <summary>Details</summary>
Motivation: Current methods for approximating DF lack accuracy guarantees and are computationally expensive, prompting the need for more efficient and reliable solutions.

Method: The authors propose two algorithms based on the outer-product structure of DF, focusing on trace and matrix-vector multiplication, bypassing auto-differentiation.

Result: Experiments show the algorithms improve accuracy and reduce computational cost in tasks like likelihood evaluation and adjoint optimization.

Conclusion: The proposed methods offer a more efficient and accurate way to handle DF, with potential applications in verifying optimal transport properties.

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [619] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/pdf/2505.23285)
*Muhammad Shafi, Syed Mohsin Bokhari*

Main category: cs.LG

TL;DR: The study compares land cover and land use (LULC) changes in Oman from 2016 to 2021 using Sentinel-2 satellite data and supervised machine learning.


<details>
  <summary>Details</summary>
Motivation: LULC changes are crucial for resource management, urbanization, and sustainability, necessitating accurate monitoring and prediction.

Method: Multispectral Sentinel-2 data was analyzed using supervised machine learning to classify land covers (e.g., water, crops, urban areas).

Result: The model enabled comparative evaluation of LULC changes across Oman's governorates over five years.

Conclusion: The approach effectively monitors LULC changes, supporting sustainable development and environmental protection.

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [620] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/pdf/2505.23320)
*Connor Cooper, Geoffrey I. Webb, Daniel F. Schmidt*

Main category: cs.LG

TL;DR: A new log-linear regression method for Bayesian network classifiers (BNCs) outperforms hierarchical Dirichlet processes (HDPs) in speed and flexibility, matching random forests on categorical data.


<details>
  <summary>Details</summary>
Motivation: Traditional BNCs underperform compared to methods like random forests, and HDP smoothing, while effective, is inflexible and complex.

Method: Proposes a log-linear regression to approximate HDP behavior, offering simplicity and flexibility.

Result: The new method outperforms HDP smoothing in speed and performance, competing with random forests.

Conclusion: The log-linear approach provides a faster, simpler, and competitive alternative to HDPs for BNCs.

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [621] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/pdf/2505.23334)
*Tu Bui, Mohamed Suliman, Aparajita Haldar, Mohammed Amer, Serban Georgescu*

Main category: cs.LG

TL;DR: X2Graph is a deep learning method for small biological tabular datasets, converting samples into graphs using external knowledge, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning's unclear dominance in tabular medical data due to scarcity, prompting a need for effective methods like X2Graph.

Method: X2Graph transforms tabular data into graphs using column relationships (e.g., gene interactions) and applies message passing algorithms.

Result: X2Graph outperforms tree-based and deep learning methods on three cancer subtyping datasets.

Conclusion: X2Graph is effective for small biological tabular datasets, leveraging graph structures for superior performance.

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [622] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/pdf/2505.23412)
*Srishti Gupta, Daniele Angioni, Maura Pintor, Ambra Demontis, Lea Schönherr, Battista Biggio, Fabio Roli*

Main category: cs.LG

TL;DR: The paper explores buffer-free post-hoc OOD detection for class-incremental learning, showing it matches or outperforms buffer-based methods while addressing privacy and scalability concerns.


<details>
  <summary>Details</summary>
Motivation: Address challenges in open-world CIL, particularly privacy and scalability issues from buffer-based OOD detection.

Method: Analyzes post-hoc OOD detection methods applied at inference time to replace memory buffers.

Result: Buffer-free OOD detection performs comparably or better than buffer-based methods in CIL and unknown sample rejection.

Conclusion: Post-hoc OOD detection offers a viable, efficient, and privacy-preserving alternative for open-world CIL systems.

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [623] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/pdf/2505.23345)
*Yang Liu, Deyu Bo, Wenxuan Cao, Yuan Fang, Yawen Li, Chuan Shi*

Main category: cs.LG

TL;DR: GraphPAE introduces a dual-path architecture for graph self-supervised learning, improving representation by capturing diverse frequency signals.


<details>
  <summary>Details</summary>
Motivation: Traditional graph autoencoders (GAEs) focus on low-frequency signals and miss expressive structural information.

Method: GraphPAE uses a dual-path architecture: a feature path with positional encoding and a position path refining node positions and eigenvectors.

Result: GraphPAE achieves state-of-the-art performance in tasks like heterophilic node classification and graph property prediction.

Conclusion: GraphPAE effectively captures diverse graph information, outperforming traditional GAEs by a large margin.

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [624] [Bidirectional predictive coding](https://arxiv.org/pdf/2505.23415)
*Gaspard Oliviers, Mufeng Tang, Rafal Bogacz*

Main category: cs.LG

TL;DR: The paper proposes bidirectional predictive coding (bPC), combining generative and discriminative inference, outperforming unidirectional models and better resembling biological visual processing.


<details>
  <summary>Details</summary>
Motivation: Existing unidirectional PC models lack the bidirectional processing seen in the brain, limiting performance in tasks requiring both generative and discriminative inference.

Method: Developed bPC, a model integrating both generative and discriminative inference, with a biologically plausible circuit and an energy landscape optimized for both tasks.

Result: bPC matches or outperforms unidirectional models in specialized tasks and excels in multimodal learning and inference with missing information.

Conclusion: bPC more closely mimics biological visual inference, suggesting its potential for understanding brain processes and improving computational models.

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [625] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/pdf/2505.23347)
*Yuting Li, Shaoyuan Huang, Tengwen Zhang, Cheng Zhang, Xiaofei Wang, Victor C. M. Leung*

Main category: cs.LG

TL;DR: Sentinel is a proactive anomaly detection-based scheduling framework for CCPs, reducing anomalies by 70%, boosting revenue by 74%, and doubling scheduling speed.


<details>
  <summary>Details</summary>
Motivation: Address challenges in optimizing CCPs' revenue due to anomalies and unstable environments, requiring efficient, time-sensitive scheduling.

Method: Two-stage Pre-Post-Scheduling paradigm: anomaly detection and strategy pool in pre-scheduling; triggered scheduling in post-scheduling.

Result: 70% anomaly reduction, 74% revenue improvement, and doubled scheduling speed.

Conclusion: Sentinel effectively enhances CCP performance by proactively addressing anomalies and optimizing scheduling.

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [626] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/pdf/2505.23355)
*Maxiu Xiao, Jianglin Lan, Jingxing Yu, Eldert van Henten, Congcong Sun*

Main category: cs.LG

TL;DR: The paper explores interactive reinforcement learning (RL) for greenhouse climate control, addressing challenges like imperfect grower inputs. It develops three algorithms, evaluates their performance, and finds policy shaping and control sharing outperform reward shaping.


<details>
  <summary>Details</summary>
Motivation: Climate control in greenhouses is vital for crop growth, but traditional RL faces inefficiency and dependency on initial conditions. Interactive RL, combining human input with RL, could mitigate these issues, though imperfect inputs pose challenges.

Method: The study develops three interactive RL algorithms (reward shaping, policy shaping, control sharing), analyzes input trade-offs, proposes a neural network for robustness, and evaluates them in a simulated greenhouse.

Result: Policy shaping and control sharing improve profit by 8.4% and 6.8%, respectively, while reward shaping reduces profit by 9.4% due to sensitivity to imperfect inputs.

Conclusion: Interactive RL with imperfect inputs can enhance greenhouse climate control, but algorithm choice is critical. Policy shaping and control sharing are more robust than reward shaping.

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [627] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/pdf/2505.23378)
*Roseline Polle, Agnes Norbury, Alexandra Livia Georgescu, Nicholas Cummins, Stefano Goria*

Main category: cs.LG

TL;DR: Meta-learning approaches outperform traditional mixed-effect models in speaker-dependent speech-based health monitoring, with transformers achieving the best results.


<details>
  <summary>Details</summary>
Motivation: Speaker-dependent modeling improves health monitoring via speech, but traditional mixed-effect models are computationally impractical for real-time use.

Method: Three meta-learning methods (ensemble-based distance models, prototypical networks, transformer-based sequence models) were tested using pre-trained speech embeddings on a longitudinal dataset.

Result: All meta-learning methods outperformed cross-sectional and mixed-effects models, with transformers showing the strongest performance.

Conclusion: Meta-learning, especially transformer-based methods, is a promising alternative to traditional speaker adaptation techniques in speech-based health monitoring.

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [628] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/pdf/2505.23383)
*Ahmad Anaqreh, Shih-Kai Chou, Mihael Mohorčič, Carolina Fortuna*

Main category: cs.LG

TL;DR: The paper introduces AI-based methods for modeling wireless propagation, focusing on interpretability and accuracy, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and interpretable propagation models in next-gen wireless systems (5G and beyond) drives the adoption of AI-based techniques.

Method: Proposes two AI-based techniques: Deep Symbolic Regression (interpretable) and Kolmogorov-Arnold Networks (two-level interpretability), evaluated on synthetic and real-world datasets.

Result: Kolmogorov-Arnold Networks achieve high accuracy (R^2 ≈ 1), while Deep Symbolic Regression offers compact models. Automated methods reduce prediction errors by up to 75%.

Conclusion: AI-based methods provide accurate, interpretable, and efficient solutions for next-generation path loss modeling, surpassing traditional approaches.

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [629] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/pdf/2505.23426)
*Yinuo Wang, Mining Tan, Wenjun Zou, Haotian Lin, Xujie Song, Wenxuan Wang, Tong Liu, Likun Wang, Guojian Zhan, Tianze Zhu, Shiqi Liu, Jingliang Duan, Shengbo Eben Li*

Main category: cs.LG

TL;DR: DACER2 improves DACER by introducing a Q-gradient field objective and temporal weighting, achieving SOTA performance with fewer diffusion steps.


<details>
  <summary>Details</summary>
Motivation: Diffusion models in online RL face inefficiency due to many diffusion steps, hindering real-time application.

Method: Proposes Q-gradient field objective and temporal weighting to guide denoising, reducing steps without performance loss.

Result: Achieves SOTA performance on MuJoCo tasks with only five diffusion steps and better multimodality.

Conclusion: DACER2 enhances efficiency and performance, making diffusion policies viable for real-time online RL.

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [630] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/pdf/2505.23421)
*Zheming Zhang, Yan Jiang, Qingshan Li, Ai Han*

Main category: cs.LG

TL;DR: The paper introduces an OTPTO method for optimizing inventory management in China's fresh e-commerce, outperforming traditional PTO by improving order fulfillment rates.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in traditional predict-then-optimize methods for inventory management in small front-end warehouses, aiming to enhance consumer satisfaction.

Method: Proposes a multi-task OTPTO approach using 0-1 mixed integer programming (OM1), product selection (PM1), and stocking models (PM2), refined by post-processing (OM2).

Result: OTPTO boosts full order fulfillment by 4.34% (7.05% relative) and reduces the gap to optimal fulfillment by 5.27% on JD.com's 7Fresh platform.

Conclusion: OTPTO effectively manages inventory in fresh e-commerce, offering insights for future research.

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [631] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/pdf/2505.23437)
*Antonio Ferrara, Andrea Pugnana, Francesco Bonchi, Salvatore Ruggieri*

Main category: cs.LG

TL;DR: The paper introduces a method for abstention in pairwise learning-to-rank tasks, focusing on thresholding conditional risk to defer uncertain decisions.


<details>
  <summary>Details</summary>
Motivation: Ranking systems impact high-stakes domains, necessitating safety mechanisms like abstention, which is underexplored beyond classification tasks.

Method: Proposes thresholding the ranker's conditional risk to abstain when risk exceeds a predefined threshold, with a model-agnostic algorithm.

Result: Theoretical optimal abstention strategy, a plug-in algorithm, and empirical evaluations show the approach's effectiveness.

Conclusion: The method successfully integrates abstention into ranking tasks, demonstrating practical utility across datasets.

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [632] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/pdf/2505.23427)
*Monika Gahalawat, Maneesh Bilalpur, Raul Fernandez Rojas, Jeffrey F. Cohn, Roland Goecke, Ramanathan Subramanian*

Main category: cs.LG

TL;DR: The paper explores head motion (kinemes) as biomarkers for depression severity, demonstrating their effectiveness and generalizability across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Depression affects millions, but head motion cues are understudied. Single-dataset validation limits model generalizability.

Method: Uses kinemes (head motion units) and evaluates via cross-validation and model reuse on three datasets (German, Australian, American).

Result: Head motion patterns are effective biomarkers, achieving competitive performance in classification and regression tasks. Kinemes generalize better than raw motion descriptors and other visual cues.

Conclusion: Kineme-based features are promising for generalizable depression severity assessment.

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [633] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/pdf/2505.23433)
*Jian Yao, Ran Cheng, Xingyu Wu, Jibin Wu, Kay Chen Tan*

Main category: cs.LG

TL;DR: The paper explores the impact of diversity in RL-based training for LLM reasoning, proposing a diversity-aware policy optimization method that improves performance and solution diversity.


<details>
  <summary>Details</summary>
Motivation: The influence of diversity on LLM reasoning is underexplored, despite its importance in RL. This work aims to bridge that gap.

Method: A novel diversity-aware policy optimization method is introduced, including a token-level diversity metric and selective application to positive samples.

Result: The method shows a 3.5% average improvement on mathematical reasoning benchmarks and generates more diverse solutions.

Conclusion: Diversity in RL training enhances LLM reasoning, as demonstrated by improved performance and robustness.

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [634] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/pdf/2505.23442)
*Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Haoran Duan, Zhengwei Tao, Xuan Zhang, Jiandong Li*

Main category: cs.LG

TL;DR: The paper explores the impact of regularization in knowledge graph completion (KGC), proposing a novel sparse-regularization method (SPR) that selectively penalizes significant embedding components to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Existing KGC models underutilize regularization, limiting their potential. The study aims to demonstrate how deeper regularization can enhance performance.

Method: The paper introduces SPR, a rank-based selective sparsity method, and tests it through extensive empirical studies on various KGC models and datasets.

Result: SPR outperforms other regularization methods, enabling KGC models to surpass their original performance bounds.

Conclusion: Carefully designed regularization, like SPR, is crucial for advancing KGC by reducing overfitting and noise while enhancing model performance.

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [635] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/pdf/2505.23443)
*Benyamin Trachtenberg, Nir Rosenfeld*

Main category: cs.LG

TL;DR: The paper explores strategic classification with non-linear classifiers, revealing limitations in universal approximators like neural nets in strategic environments.


<details>
  <summary>Details</summary>
Motivation: To extend strategic classification beyond linear classifiers and understand how non-linearity impacts strategic behavior and learning.

Method: A bottom-up approach analyzing non-linearity's effects on decision boundaries, classifier expressivity, and model complexity.

Result: Universal approximators (e.g., neural nets) lose universality in strategic settings, leading to performance gaps.

Conclusion: Strategic environments challenge the universality of non-linear classifiers, impacting their effectiveness.

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [636] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/pdf/2505.23529)
*Shifeng Xie, Aref Einizade, Jhony H. Giraldo*

Main category: cs.LG

TL;DR: The paper introduces SubGEC, a self-supervised learning method for graph representation learning, using subgraph Gaussian embeddings and optimal transport distances to improve performance.


<details>
  <summary>Details</summary>
Motivation: To avoid costly human annotation in graph representation learning by leveraging self-supervised methods, while preserving subgraph characteristics and ensuring robust contrastive learning.

Method: Proposes SubGEC, which maps subgraphs to a Gaussian space and uses Wasserstein and Gromov-Wasserstein distances for similarity measurement in contrastive learning.

Result: SubGEC outperforms or competes with state-of-the-art methods across benchmarks, highlighting the role of contrastive pair distribution.

Conclusion: The study underscores the significance of structured contrastive pair generation in self-supervised graph representation learning.

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [637] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/pdf/2505.23448)
*Pirzada Suhail, Rehna Afroz, Amit Sethi*

Main category: cs.LG

TL;DR: A novel framework combining network inversion and classifier training for simultaneous OOD detection and uncertainty estimation, using a garbage class for outliers.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for safe ML systems by detecting OOD inputs and estimating uncertainty without external datasets or post-hoc methods.

Method: Extends classifier to (n+1)-class with a garbage class, iteratively trains using network inversion to refine decision boundaries and exclude OOD samples.

Result: Effective OOD detection and uncertainty estimation, scalable and interpretable without external data.

Conclusion: The framework provides a unified, efficient solution for OOD detection and UE, enhancing ML system safety.

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [638] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/pdf/2505.23458)
*Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine*

Main category: cs.LG

TL;DR: CFGRL combines reinforcement learning and generative modeling, enabling scalable policy improvement without explicit value function learning.


<details>
  <summary>Details</summary>
Motivation: To leverage the scalability of generative models while retaining the policy improvement capabilities of reinforcement learning.

Method: Derives a direct relation between policy improvement and guidance of diffusion models, simplifying training to supervised learning.

Result: Increased guidance weighting improves performance; CFGRL generalizes supervised methods for better optimality.

Conclusion: CFGRL offers a scalable, simple approach to enhance policies beyond data performance, applicable across tasks.

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [639] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/pdf/2505.23564)
*Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu*

Main category: cs.LG

TL;DR: SPO is a novel RL framework for enhancing reasoning in large language models by balancing token and trajectory-level advantage estimation with segment-level granularity.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing RL methods (token-level and trajectory-level) in accurately estimating advantages for reasoning tasks.

Method: Proposes Segment Policy Optimization (SPO) with flexible segment partition, accurate advantage estimation, and policy optimization. Includes SPO-chain for short CoT and SPO-tree for long CoT.

Result: Achieves 6-12 percentage point improvements over PPO and GRPO on GSM8K, and 7-11 percentage points on MATH500.

Conclusion: SPO effectively balances granularity for precise credit assignment, outperforming existing methods in reasoning tasks.

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [640] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/pdf/2505.23459)
*Safwan Labbi, Paul Mangold, Daniil Tiapkin, Eric Moulines*

Main category: cs.LG

TL;DR: The paper addresses convergence challenges in federated reinforcement learning (FRL) under heterogeneous environments, proposing FedPG and b-RS-FedPG methods with proven convergence and empirical superiority over federated Q-learning.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity in FRL can lead to non-deterministic optimal policies, necessitating new methods to ensure convergence.

Method: The paper introduces FedPG and b-RS-FedPG, leveraging local updates and regularization, with theoretical proofs of convergence and speed-up properties.

Result: FedPG achieves linear speed-up with agent count, and b-RS-FedPG demonstrates explicit convergence rates to near-optimal policies. Both outperform federated Q-learning empirically.

Conclusion: The proposed methods effectively address heterogeneity in FRL, offering theoretical guarantees and empirical performance improvements.

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [641] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/pdf/2505.23470)
*Chenjie Li, Amir Gilad, Boris Glavic, Zhengjie Miao, Sudeepa Roy*

Main category: cs.LG

TL;DR: The paper introduces techniques to repair labeling functions (LFs) in programmatic weak supervision (PWS) using minimal changes based on small labeled datasets, improving LF accuracy and label quality.


<details>
  <summary>Details</summary>
Motivation: The quality of labels in PWS depends on LF accuracy, but LFs may be imperfect. The goal is to fix LFs using limited labeled data to enhance their reliability.

Method: LFs are modeled as conditional rules, allowing selective refinement of their outputs on labeled examples to meet accuracy and evidence requirements.

Result: Experimental results show the system effectively improves LF quality with surprisingly small labeled datasets.

Conclusion: The proposed method successfully enhances LF accuracy and label quality in PWS, even with minimal labeled data.

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [642] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/pdf/2505.23489)
*Ildus Sadrtdinov, Ivan Klimov, Ekaterina Lobacheva, Dmitry Vetrov*

Main category: cs.LG

TL;DR: SGD's stationary behavior is interpreted thermodynamically, balancing loss and entropy, with LR acting as temperature. High LRs prevent convergence, while low LRs in OP models minimize loss directly.


<details>
  <summary>Details</summary>
Motivation: To understand why SGD behaves differently under varying learning rates and how it balances loss and entropy in neural network training.

Method: Thermodynamic framework applied to SGD, analyzing free energy minimization (F=U-TS) with empirical validation on UP and OP models.

Result: UP models follow free energy minimization with temperature increasing with LR; OP models minimize loss directly at low LRs due to reduced gradient noise.

Conclusion: The thermodynamic perspective explains SGD behavior, linking LR to temperature and highlighting differences in UP vs. OP models.

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [643] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/pdf/2505.23496)
*Sabina J. Sloman, Michele Caprio, Samuel Kaski*

Main category: cs.LG

TL;DR: The paper introduces a principled definition of epistemic error and provides a generic error bound addressing multitask learning, distribution shift, and imperfect learning. It also offers specialized bounds for Bayesian transfer learning and distribution shift, along with a new definition of negative transfer.


<details>
  <summary>Details</summary>
Motivation: To address unresolved epistemic uncertainty in noisy data settings, including multitask learning, distribution shift, and imperfect learning.

Method: Introduces a decompositional epistemic error bound and derives specialized bounds for Bayesian transfer learning and distribution shift.

Result: Provides the first epistemic error bound accommodating multiple uncertainty sources and attributes error to specific learning aspects. Also defines negative transfer.

Conclusion: The framework offers insights into epistemic error and validates the negative transfer definition experimentally.

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [644] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/pdf/2505.23506)
*Sebastián Jiménez, Mira Jürgens, Willem Waegeman*

Main category: cs.LG

TL;DR: Current methods for disentangling aleatoric and epistemic uncertainty often miss model bias, leading to incomplete epistemic uncertainty estimates. A fine-grained taxonomy and simulation-based evaluation reveal these gaps.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in capturing the full spectrum of epistemic uncertainty, particularly model bias, which is often overlooked.

Method: Uses a fine-grained taxonomy of epistemic uncertainty sources and a simulation-based evaluation protocol to analyze bias-variance decomposition.

Result: Current methods underestimate epistemic uncertainty by blurring bias-induced errors into aleatoric estimates.

Conclusion: Accurate aleatoric uncertainty estimation requires proper representation of all epistemic uncertainty sources, including model bias.

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [645] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/pdf/2505.23520)
*Yu Zhang, Dong Guo, Fang Wu, Guoliang Zhu, Dian Ding, Yiming Zhang*

Main category: cs.LG

TL;DR: AnchorAttention is a dynamic sparse attention mechanism for LLMs, improving speed and accuracy by identifying critical attention regions at finer granularity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLMs' pre-filling phase suffer from inefficiency due to quadratic complexity of self-attention and coarse granularity, leading to suboptimal performance.

Method: AnchorAttention uses pattern-based anchor computation, difference-aware stripe sparsity identification, and fine-grained sparse computation to optimize attention regions.

Result: At 128k text length, AnchorAttention achieves a 1.44× speedup while maintaining higher recall rates compared to prior methods.

Conclusion: AnchorAttention offers a superior approach to sparse attention in LLMs, balancing efficiency and accuracy effectively.

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [646] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/pdf/2505.23523)
*Arjun Devraj, Eric Ding, Abhishek Vijaya Kumar, Robert Kleinberg, Rachee Singh*

Main category: cs.LG

TL;DR: StragglAR is a novel AllReduce algorithm designed to mitigate delays caused by persistent stragglers in distributed machine learning, achieving significant speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Persistent stragglers in distributed training and inference delay bulk-synchronous AllReduce algorithms, necessitating a solution to improve efficiency.

Method: StragglAR uses ReduceScatter among non-straggler GPUs during delays and a novel collective algorithm to complete AllReduce once stragglers synchronize.

Result: Theoretical 2x speedup over bandwidth-efficient AllReduce algorithms for large clusters; 22% speedup on an 8-GPU server.

Conclusion: StragglAR effectively addresses straggler-induced delays, enhancing performance in distributed machine learning workloads.

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [647] [Normalizing Flows are Capable Models for RL](https://arxiv.org/pdf/2505.23527)
*Raj Ghugare, Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Normalizing flows (NFs) offer a simpler, efficient alternative to complex models like transformers and diffusion models in RL, achieving high performance across tasks.


<details>
  <summary>Details</summary>
Motivation: To address the computational and representational challenges of existing RL models (e.g., diffusion models, transformers) by leveraging the untapped potential of normalizing flows.

Method: Proposes a single NF architecture that integrates into RL algorithms, serving as policy, Q-function, and occupancy measure.

Result: Simplifies algorithms and outperforms in imitation learning, offline, goal-conditioned, and unsupervised RL.

Conclusion: NFs are expressive and effective for RL, challenging the belief of their limited expressivity.

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [648] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/pdf/2505.23528)
*Maria Eleftheria Vlontzou, Maria Athanasiou, Christos Davatzikos, Konstantina S. Nikita*

Main category: cs.LG

TL;DR: The study analyzes fairness in ML models for diagnosing MCI and AD from MRI data, identifying biases in age and race but not gender. It evaluates mitigation strategies and introduces a composite fairness-performance measure.


<details>
  <summary>Details</summary>
Motivation: To address biases in ML models for medical diagnosis, ensuring fairness across demographic groups like age, race, and gender.

Method: Comprehensive fairness analysis using multi-cohort MRI data, assessing biases and testing pre-, in-, and post-processing mitigation strategies. A novel composite measure balances fairness and performance.

Result: Biases found in age and race, not gender. Mitigation strategies improved fairness: Reject Option Classification for race/gender, adversarial debiasing for age.

Conclusion: Fairness in ML for medical diagnosis requires tailored mitigation strategies. The composite measure aids in balancing fairness and performance.

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [649] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/pdf/2505.23537)
*Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic*

Main category: cs.LG

TL;DR: The paper introduces tnLLM, a novel framework for tensor network structure search (TN-SS) that leverages domain information and large language models (LLMs) to predict optimal structures efficiently.


<details>
  <summary>Details</summary>
Motivation: Current TN-SS methods are computationally expensive and ignore domain-specific data relationships, lacking transparency.

Method: The tnLLM framework uses domain-aware prompting with LLMs to infer TN structures and provide explanations, reducing function evaluations.

Result: tnLLM achieves comparable results to SOTA methods with fewer evaluations and improves initialization for faster convergence.

Conclusion: The tnLLM framework offers a computationally efficient, domain-aware, and interpretable solution for TN-SS.

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [650] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/pdf/2505.23552)
*Alex Adams*

Main category: cs.LG

TL;DR: Comparison of Moore-Penrose pseudoinverse and gradient descent for linear regression, analyzing efficiency, accuracy, and conditions for optimal performance.


<details>
  <summary>Details</summary>
Motivation: To guide researchers and practitioners in choosing between exact (pseudoinverse) and iterative (gradient descent) methods for linear regression based on computational and empirical performance.

Method: Theoretical review, computational complexity analysis, and empirical evaluation on synthetic and real-world datasets.

Result: Identifies conditions where pseudoinverse or gradient descent excels in computational time, numerical stability, and predictive accuracy.

Conclusion: Provides practical guidance for selecting the appropriate linear regression solver based on specific conditions and requirements.

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [651] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/pdf/2505.23555)
*Yanzhao Hou, Jiaxiang Geng, Boyu Li, Xiaofeng Tao, Juncheng Wang, Xiaodong Xu, Bing Luo*

Main category: cs.LG

TL;DR: Proposes an adaptive federated LoRA strategy with independent client sampling to minimize wall-clock convergence time in federated fine-tuning, addressing system and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing federated LoRA approaches overlook system and data heterogeneity, failing to optimize training efficiency, especially in wall-clock time.

Method: Derives a convergence bound for federated LoRA with independent client sampling, introduces adaptive bandwidth allocation, and formulates a non-convex optimization problem to jointly optimize LoRA sketching ratios and sampling probabilities.

Result: Extensive experiments show significant reduction in wall-clock training time compared to state-of-the-art methods.

Conclusion: The proposed adaptive federated LoRA strategy effectively minimizes convergence time under heterogeneous conditions.

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [652] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/pdf/2505.23565)
*Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, Jose Blanchet*

Main category: cs.LG

TL;DR: DRO is an open-source Python library for distributionally robust optimization (DRO) in regression and classification, offering 14 formulations and 9 backbone models, with significant runtime improvements.


<details>
  <summary>Details</summary>
Motivation: To provide a versatile and efficient tool for DRO in machine learning, compatible with scikit-learn and PyTorch.

Method: Implements 14 DRO formulations and 9 backbone models, using vectorization and optimization approximation for speed.

Result: Reduces runtime by 10x to over 1000x on large datasets compared to baselines.

Conclusion: DRO is a powerful, efficient library for DRO tasks, with extensive documentation and compatibility.

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [653] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/pdf/2505.23569)
*Samo Hromadka, Kai Biegun, Lior Fox, James Heald, Maneesh Sahani*

Main category: cs.LG

TL;DR: The paper introduces RP-GSSM, an unsupervised probabilistic model for time series data, combining contrastive learning and generative modeling without needing explicit latent-to-observation mappings.


<details>
  <summary>Details</summary>
Motivation: To address limitations of contrastive and generative methods in learning latent dynamics from time series data, offering a tractable and expressive model.

Method: RP-GSSM uses a Gaussian state space model with neural networks for nonlinear links, enabling exact inference and focusing capacity on latent inference.

Result: RP-GSSM outperforms alternatives in learning nonlinear stochastic dynamics from video, even with distractors.

Conclusion: RP-GSSM is a versatile foundation model for downstream applications, avoiding ad-hoc regularization or auxiliary losses.

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [654] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/pdf/2505.23579)
*Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang*

Main category: cs.LG

TL;DR: BioReason integrates a DNA foundation model with a Large Language Model (LLM) to enable deep, interpretable biological reasoning from genomic data, outperforming baselines by 15%.


<details>
  <summary>Details</summary>
Motivation: Current DNA foundation models lack multi-step reasoning and transparent explanations, limiting scientific discovery.

Method: Combines DNA foundation model with LLM, using supervised fine-tuning and reinforcement learning for multi-step reasoning.

Result: Achieves 97% accuracy in disease pathway prediction (vs. 88% baseline) and 15% average performance gain.

Conclusion: BioReason offers interpretable, step-by-step reasoning, advancing AI's role in biological discovery.

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [655] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/pdf/2505.23583)
*Zhiding Liu, Mingyue Cheng, Guanhao Zhao, Jiqian Yang, Qi Liu, Enhong Chen*

Main category: cs.LG

TL;DR: PIR is a model-agnostic framework for improving time series forecasting by identifying and revising biased forecasts post-forecasting.


<details>
  <summary>Details</summary>
Motivation: Instance-level variations (e.g., distribution shifts, missing data) degrade forecasting accuracy despite strong overall performance.

Method: PIR identifies biased forecasts by estimating accuracy and revises them using contextual information (covariates, historical data) from local and global perspectives.

Result: PIR reduces instance-level errors and enhances forecasting reliability in real-world datasets.

Conclusion: PIR effectively addresses instance-level forecasting challenges, improving accuracy and reliability.

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [656] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/pdf/2505.23585)
*Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei*

Main category: cs.LG

TL;DR: OPO is a simplified reinforcement learning algorithm for large language models, improving training stability and efficiency by enforcing exact on-policy training and using an optimal reward baseline.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning algorithms for aligning large language models with human preferences suffer from instability and inefficiency due to loose on-policy constraints and auxiliary models.

Method: OPO enforces exact on-policy training and introduces an optimal reward baseline to minimize gradient variance, eliminating the need for auxiliary models or regularization.

Result: OPO outperforms benchmarks in mathematical reasoning, showing superior stability, lower policy shifts, and higher output entropy for diverse responses.

Conclusion: OPO is a promising approach for stable and effective reinforcement learning in language model alignment and reasoning tasks.

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [657] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/pdf/2505.23720)
*Arun Verma, Indrajit Saha, Makoto Yokoo, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: Proposes COBRA, an algorithm for contextual bandit problems with strategic agents, ensuring incentive compatibility and sub-linear regret without monetary incentives.


<details>
  <summary>Details</summary>
Motivation: Addresses the unrealistic assumption of truthful arm reporting in existing contextual bandit work, particularly in scenarios like online platforms with misrepresenting sellers.

Method: Introduces COBRA, an algorithm designed to disincentivize strategic behavior in agents while maintaining incentive compatibility and sub-linear regret.

Result: Experimental results validate COBRA's performance in handling strategic agents.

Conclusion: COBRA effectively addresses strategic behavior in contextual bandit problems without monetary incentives, with proven performance.

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [658] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/pdf/2505.23588)
*Mrinmay Sen, Sidhant R Nair, C Krishna Mohan*

Main category: cs.LG

TL;DR: This paper reviews second-order optimization methods in Federated Learning (FL) to address slow convergence and high communication costs, comparing their performance and highlighting challenges like Hessian utilization.


<details>
  <summary>Details</summary>
Motivation: Slow model training and excessive communication rounds in FL, especially with heterogeneous data, motivate exploring second-order optimization for faster convergence.

Method: The paper categorizes and analyzes state-of-the-art second-order FL methods, evaluating them on convergence speed, computational cost, memory usage, transmission overhead, and model generalization.

Result: Second-order optimization shows promise in accelerating FL training, but challenges like efficient Hessian and its inverse utilization remain.

Conclusion: This study sets a foundation for future research on scalable and efficient federated optimization methods to improve global model training in FL.

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [659] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/pdf/2505.23724)
*Minrui Luo, Fuhang Kuang, Yu Wang, Zirui Liu, Tianxing He*

Main category: cs.LG

TL;DR: SC-LoRA is a new LoRA initialization framework that balances efficient fine-tuning and knowledge preservation in LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Vanilla LoRA has slow convergence and knowledge forgetting issues; existing methods address either efficiency or preservation, not both.

Method: SC-LoRA constrains LoRA adapters in a low-rank subspace to focus on fine-tuning data while preserving pre-trained knowledge.

Result: SC-LoRA achieves better fine-tuning performance and reduces knowledge forgetting compared to other LoRA methods.

Conclusion: SC-LoRA effectively balances fine-tuning efficiency and knowledge preservation, offering a superior alternative to current LoRA techniques.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [660] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/pdf/2505.23593)
*Nikita Agrawal, Simon Mertel, Ruben Mayer*

Main category: cs.LG

TL;DR: The paper critiques the use of black-box models in federated post-training, arguing it contradicts FL principles like privacy and autonomy.


<details>
  <summary>Details</summary>
Motivation: To address concerns about using black-box models in federated learning (FL) for post-training, which may undermine core FL principles.

Method: Critical analysis of black-box model usage in FL, focusing on openness and its implications.

Result: Highlights contradictions between black-box models and FL principles, emphasizing the need for openness.

Conclusion: Advocates for re-evaluating the use of black-box models in FL to align with its foundational principles.

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [661] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/pdf/2505.23598)
*Radzim Sendyka, Christian Cabrera, Andrei Paleyes, Diana Robinson, Neil Lawrence*

Main category: cs.LG

TL;DR: LLMs can solve heavily obfuscated tasks, showing memorization rather than reasoning, with performance decay patterns revealing dataset contamination.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to solve unrecognizable tasks and explore implications for benchmarking and safety.

Method: Evaluated LLMs on obfuscated competitive programming tasks (LeetCode, MATH) using noise and redaction.

Result: LLMs solved tasks obfuscated beyond human recognition, indicating memorization, with performance decay patterns distinguishing contaminated datasets.

Conclusion: Caution is needed in benchmarking; obfuscation performance decay can detect dataset contamination and highlight safety risks.

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [662] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/pdf/2505.23599)
*Eitan Levin, Yuxin Ma, Mateo Díaz, Soledad Villar*

Main category: cs.LG

TL;DR: The paper introduces a framework for transferability of models across varying input dimensions, linking it to continuity in a limit space, and provides design principles for transferable models.


<details>
  <summary>Details</summary>
Motivation: To address the need for models that can handle inputs of varying sizes, particularly in domains like graphs, sets, and point clouds.

Method: Proposes a general framework for transferability, linking it to continuity in a limit space, and adapts existing architectures to ensure transferability.

Result: Demonstrates that transferability corresponds to continuity in the limit space, supported by numerical experiments.

Conclusion: Provides design principles for creating transferable models and validates the framework with experiments.

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [663] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/pdf/2505.23761)
*Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo*

Main category: cs.LG

TL;DR: The paper provides a theoretical justification for Direct Preference Optimization (DPO) by linking its log-ratio reward parameterization to differential information, revealing its optimality and implications for policy learning.


<details>
  <summary>Details</summary>
Motivation: To address the incomplete theoretical understanding of DPO's log-ratio reward parameterization and its empirical success in aligning language models with human preferences.

Method: Uses Differential Information Distribution (DID) to analyze how preference labels encode information for policy updates, deriving the optimal form of the log-ratio reward and sampling distribution.

Result: Shows that DPO's log-ratio reward is uniquely optimal under certain conditions, explains policy behaviors via DID entropy, and validates findings in synthetic and real-world datasets.

Conclusion: The study unifies DPO's objective, preference data structure, and policy behaviors through differential information, highlighting the importance of entropy in learning tasks.

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


### [664] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/pdf/2505.23606)
*Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit is a unified discrete diffusion transformer for fast, parallel multimodal generation, outperforming autoregressive models in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Address slow inference in autoregressive unified models and weak generalization in non-autoregressive ones by leveraging pretrained backbones.

Method: Integrates pretrained text-to-image backbone with a lightweight text decoder for flexible, high-quality multimodal generation.

Result: Achieves competitive/superior performance to larger autoregressive models in quality and efficiency.

Conclusion: Discrete diffusion with strong visual priors is a scalable, effective backbone for unified generation.

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [665] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/pdf/2505.23607)
*Carolina Fortuna, Gregor Cerar, Blaz Bertalanic, Andrej Campa, Mihael Mohorcic*

Main category: cs.LG

TL;DR: The paper proposes a taxonomy for structuring data in energy applications to improve machine learning models, validated through household electricity forecasting.


<details>
  <summary>Details</summary>
Motivation: The shift to smart grids and renewable energy necessitates better data-driven models, but current models often ignore multivariate data.

Method: A taxonomy is developed to guide data model development, tested with household electricity forecasting using interpretable ML techniques and feature importance analysis.

Result: The taxonomy effectively guides feature selection, improving forecasting accuracy across datasets.

Conclusion: The study highlights the importance of structured data and feature contributions in enhancing energy forecasting models.

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [666] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/pdf/2505.23609)
*Armando Bellante, Martin Plávala, Alessandro Luongo*

Main category: cs.LG

TL;DR: The paper introduces a family of permutation-invariant graph embeddings, generalizing the Skew Spectrum, to handle richer graph structures like attributed graphs and hypergraphs, with a trade-off between computational complexity and expressivity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the Skew Spectrum in handling complex graph structures and to provide a more expressive and computationally flexible framework.

Method: The approach is grounded in group theory and harmonic analysis, introducing a new class of graph invariants and generalization-preserving heuristics.

Result: The method improves expressivity at the same computational cost as the Skew Spectrum, proven invariant and demonstrated through experiments.

Conclusion: The proposed generalization offers a versatile and efficient solution for embedding diverse graph structures while balancing computational demands.

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [667] [REOrdering Patches Improves Vision Models](https://arxiv.org/pdf/2505.23751)
*Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta*

Main category: cs.LG

TL;DR: REOrder, a two-stage framework, improves transformer performance by optimizing patch orderings, achieving notable accuracy gains on ImageNet-1K and Functional Map of the World.


<details>
  <summary>Details</summary>
Motivation: Patch order significantly affects transformer performance, but existing methods rely on fixed orderings (e.g., row-major). REOrder aims to discover task-optimal patch orderings.

Method: 1. Derive an information-theoretic prior by evaluating patch sequence compressibility. 2. Learn a policy over permutations using REINFORCE with a Plackett-Luce policy.

Result: REOrder improves top-1 accuracy by up to 3.01% on ImageNet-1K and 13.35% on Functional Map of the World.

Conclusion: Optimizing patch orderings with REOrder enhances transformer performance, demonstrating the importance of task-specific ordering in vision tasks.

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [668] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/pdf/2505.23614)
*Xiangcheng Zhang, Haowei Lin, Haotian Ye, James Zou, Jianzhu Ma, Yitao Liang, Yilun Du*

Main category: cs.LG

TL;DR: A framework combining local and global search principles from classical algorithms improves inference-time control in diffusion models, enhancing performance and efficiency across tasks like planning, RL, and image generation.


<details>
  <summary>Details</summary>
Motivation: To adapt diffusion model outputs for diverse test-time objectives using classical search principles.

Method: Combines local search (annealed Langevin MCMC) and global search (breadth-first/depth-first tree search) to navigate generative space.

Result: Significant performance and efficiency gains in planning, offline RL, and image generation.

Conclusion: Classical search offers a principled, practical foundation for scaling diffusion model inference.

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [669] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/pdf/2505.23615)
*Chang Yue, Niraj K. Jha*

Main category: cs.LG

TL;DR: DLNs extend to tabular regression, offering interpretability and fast inference while matching or exceeding baseline accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the opacity and computational cost of neural networks by adapting DLNs for regression tasks.

Method: Redesign DLN output layer for continuous targets and unify training into a single stage. Evaluate on 15 regression benchmarks.

Result: Regression DLNs match or exceed baseline accuracy while maintaining interpretability and low inference cost.

Conclusion: DLNs are a viable, cost-effective alternative for regression, especially where transparency and efficiency matter.

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [670] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/pdf/2505.23627)
*Griffin Dietz Smith, Dianna Yee, Jennifer King Chen, Leah Findlater*

Main category: cs.LG

TL;DR: The paper proposes an end-to-end architecture for real-time miscue detection in reading aloud, improving verbatim transcription and error detection by incorporating target text via prompting.


<details>
  <summary>Details</summary>
Motivation: Current post-hoc methods for identifying reading miscues rely on ASR transcriptions, which perform poorly when ASR inaccurately transcribes verbatim speech. The paper aims to enhance miscue detection by integrating the target reading text directly into the model.

Method: The authors introduce a novel end-to-end architecture that uses prompting to incorporate the target reading text. The model is trained for both verbatim transcription and direct miscue detection.

Result: Case studies on children's read-aloud and adult atypical speech show improved verbatim transcription and miscue detection compared to state-of-the-art methods.

Conclusion: The proposed architecture successfully enhances real-time miscue detection and verbatim transcription, demonstrating the feasibility of integrating reading text for improved performance.

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [671] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/pdf/2505.23634)
*John Halloran*

Main category: cs.LG

TL;DR: The paper reveals MCP's vulnerability to online-based FBAs, introduces DPO and RAG-Pref to enhance LLM refusal training, and shows RAG-Pref's superior performance in mitigating attacks.


<details>
  <summary>Details</summary>
Motivation: Address the broader threat of MCP-based FBAs, which can now exploit online content, and improve LLM guardrails against such attacks.

Method: Introduces DPO for refusal training and RAG-Pref, a novel RAG-based preference alignment strategy, to enhance LLM defenses.

Result: DPO improves guardrails, but effectiveness varies by alignment scheme; RAG-Pref significantly boosts FBA refusal, especially with DPO.

Conclusion: RAG-Pref combined with DPO drastically improves LLM guardrails against MCP-based FBAs, offering robust defense.

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [672] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/pdf/2505.23640)
*Yilin Xie, Shiqiang Zhang, Jixiang Qing, Ruth Misener, Calvin Tsay*

Main category: cs.LG

TL;DR: Graph BO for NAS is improved by explicit optimization formulations for graph input space, proving efficacy in finding optimal architectures.


<details>
  <summary>Details</summary>
Motivation: Existing graph BO works lack focus on acquisition optimization due to complexity in graph search space and acquisition functions.

Method: Proposes explicit optimization formulations for graph input space, including reachability and shortest paths, to formulate graph kernels and acquisition functions.

Result: Theoretical proof of equivalent graph space representation and successful numerical results on NAS benchmarks.

Conclusion: The method efficiently finds optimal architectures, demonstrating its efficacy in NAS.

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [673] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/pdf/2505.23648)
*Halil Alperen Gozeten, M. Emrullah Ildiz, Xuechen Zhang, Hrayr Harutyunyan, Ankit Singh Rawat, Samet Oymak*

Main category: cs.LG

TL;DR: The paper introduces CoT2, a method for chain-of-thought reasoning using continuous-valued tokens, showing its benefits in logical reasoning tasks and efficiency. It provides theoretical insights, supervision strategies, and sampling methods to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Discrete sampling in chain-of-thought reasoning has limitations. CoT2 offers a more expressive and efficient alternative, especially for tasks requiring search and optimization.

Method: The paper proposes CoT2, which uses continuous-valued tokens, and introduces supervision strategies (matching softmax outputs to empirical distributions) and sampling methods (parallel token sampling and continuous exploration).

Result: Theoretically, CoT2 enables parallel trace tracking and solves the subset sum problem with a one-layer transformer. Experimentally, policy optimization with CoT2 outperforms discrete or continuous supervision.

Conclusion: CoT2 enhances chain-of-thought reasoning by leveraging continuous tokens, offering theoretical and practical advantages over discrete methods.

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [674] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/pdf/2505.23651)
*Juncheol Shin, Minsang Seok, Seonggon Kim, Eunhyeok Park*

Main category: cs.LG

TL;DR: The paper introduces HDRQ, a post-training quantization method for model merging in multi-target domain adaptation, addressing challenges posed by quantization.


<details>
  <summary>Details</summary>
Motivation: Quantization in practical scenarios restricts domains and introduces discretization effects, complicating model merging.

Method: Proposes HDRQ (Hessian and distant regularizing quantization) to minimize deviation from the source model and flatten the loss surface for smoother merging.

Result: Extensive experiments confirm HDRQ's effectiveness in handling quantization for model merging.

Conclusion: HDRQ is the first solution addressing quantization's impact on model merging, proving successful in multi-target domain adaptation.

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [675] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/pdf/2505.23653)
*Jiaran Ye, Zijun Yao, Zhidian Huang, Liangming Pan, Jinxin Liu, Yushi Bai, Amy Xin, Liu Weichuan, Xiaoyin Che, Lei Hou, Juanzi Li*

Main category: cs.LG

TL;DR: The paper explores how implicit multi-hop reasoning emerges in LLMs, revealing a three-stage learning trajectory and introducing diagnostic tools to analyze representational structure.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind implicit multi-hop reasoning in LLMs, which remain poorly understood despite recent findings.

Method: Training transformers from scratch in a controlled symbolic environment, analyzing developmental stages, and introducing diagnostic tools (cross-query semantic patching and cosine-based representational lens).

Result: Identified a three-stage learning trajectory (memorization, in-distribution generalization, cross-distribution generalization) and found that specific training conditions (e.g., atomic triples) accelerate learning. Representational clustering correlates with reasoning success.

Conclusion: The study provides insights into LLMs' implicit reasoning, linking representational structure to behavior, and suggests pathways to enhance model transparency.

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [676] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/pdf/2505.23663)
*Niklas Freymuth, Tobias Würth, Nicolas Schreiber, Balazs Gyenes, Andreas Boltres, Johannes Mitsch, Aleksandar Taranovic, Tai Hoang, Philipp Dahlinger, Philipp Becker, Luise Kärger, Gerhard Neumann*

Main category: cs.LG

TL;DR: AMBER is a supervised learning method for adaptive meshing in FEM, using a hierarchical GNN to predict sizing fields and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional adaptive meshing requires task-specific heuristics or expert input, which is inefficient. AMBER automates this process.

Method: AMBER iteratively predicts sizing fields from coarse meshes using a hierarchical GNN, aided by data augmentation from expert labels.

Result: AMBER generalizes to unseen geometries and outperforms baselines like GNNs, CNNs, and RL-based methods in 2D/3D datasets.

Conclusion: AMBER offers an efficient, automated alternative to manual adaptive meshing, improving FEM simulations.

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [677] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/pdf/2505.23673)
*Aya Kayal, Sattar Vakili, Laura Toni, Da-shan Shiu, Alberto Bernacchia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [678] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/pdf/2505.23681)
*Bo Zhao, Nima Dehmamy, Robin Walters, Rose Yu*

Main category: cs.LG

TL;DR: The paper explores mode connectivity in neural networks using parameter space symmetry, showing how skip connections reduce connected components and deriving conditions for linear mode connectivity.


<details>
  <summary>Details</summary>
Motivation: To theoretically explain the phenomenon of mode connectivity in neural networks, which has practical applications like model merging but lacks clear theoretical understanding.

Method: The study links the topology of symmetry groups to minima, derives the number of connected components for linear networks, and examines conditions for mode connectivity using parameter symmetries.

Result: Skip connections reduce the number of connected components, and explicit expressions for connecting curves in minima are provided. Conditions for linear mode connectivity are derived.

Conclusion: Continuous symmetries play a key role in understanding the neural network loss landscape, with implications for model design and optimization.

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [679] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/pdf/2505.23683)
*Zixuan Wang, Eshaan Nichani, Alberto Bietti, Alex Damian, Daniel Hsu, Jason D. Lee, Denny Wu*

Main category: cs.LG

TL;DR: The paper explores the learnability of the $k$-fold composition task by transformers, showing a statistical-computational gap but proving efficient learnability via curriculum strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding the conditions enabling efficient learning of complex reasoning tasks by transformers, given prior theoretical work on their expressive power.

Method: Study the $k$-fold composition task, prove an SQ lower bound, and demonstrate efficient learning via gradient descent on transformers using two curriculum strategies.

Result: A statistical-computational gap exists, but the task can be learned efficiently with polynomial runtime and sample complexity via curriculum learning.

Conclusion: Easy and hard examples in the data distribution are crucial for transformers to learn complex compositional tasks.

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [680] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/pdf/2505.23696)
*Hiroshi Kera, Nico Pelleriti, Yuki Ishihara, Max Zimmer, Sebastian Pokutta*

Main category: cs.LG

TL;DR: The paper introduces the Oracle Border Basis Algorithm, a Deep Learning approach that accelerates Border basis computation while ensuring correctness, achieving up to 3.5x speedup.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like Gröbner and Border bases are computationally expensive, prompting the need for efficient alternatives without sacrificing correctness.

Method: A Transformer-based oracle is trained to eliminate costly reduction steps, combined with a tailored tokenization and embedding scheme for efficient polynomial representation.

Result: The algorithm achieves up to 3.5x speedup over the base method while maintaining correctness, supported by a novel sampling theorem and data-efficient learning.

Conclusion: The Oracle Border Basis Algorithm is a practical, stable enhancement to traditional symbolic computation, balancing efficiency and correctness.

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [681] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/pdf/2505.23700)
*Oleksii Furman, Ulvi Movsum-zada, Patryk Marszalek, Maciej Zięba, Marek Śmieja*

Main category: cs.LG

TL;DR: DiCoFlex is a model-agnostic, conditional generative framework for diverse counterfactual explanations, addressing limitations of existing methods by enabling real-time customization and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanation methods are computationally intensive, inflexible, and require constant model access, limiting their practicality.

Method: DiCoFlex uses conditional normalizing flows trained on labeled data to generate diverse counterfactuals in a single forward pass, allowing real-time constraint customization.

Result: DiCoFlex outperforms existing methods in validity, diversity, proximity, and constraint adherence on benchmark datasets.

Conclusion: DiCoFlex offers a scalable, practical solution for counterfactual generation in sensitive domains.

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [682] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/pdf/2505.23702)
*Nathan Lichtlé, Alexi Canesse, Zhe Fu, Hossein Nick Zinat Matin, Maria Laura Delle Monache, Alexandre M. Bayen*

Main category: cs.LG

TL;DR: (U)NFV is a neural network architecture improving finite volume methods for hyperbolic conservation laws, achieving lower error and better scalability.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic PDEs are hard to solve, especially with shocks/discontinuities. Traditional FV methods lack accuracy and flexibility.

Method: (U)NFV learns update rules over extended stencils, preserving conservation. Supports supervised (NFV) and unsupervised (UNFV) training.

Result: Achieves 10x lower error than Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin with less complexity.

Conclusion: (U)NFV offers higher fidelity and scalability for hyperbolic conservation laws, especially in traffic modeling.

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [683] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/pdf/2505.23705)
*Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, Sergey Levine*

Main category: cs.LG

TL;DR: VLA models combine vision-language models (VLMs) with control policies for robots, but face challenges in real-time control due to VLMs' size and token-based outputs. Specialized modules like action experts help but may degrade VLM knowledge. This paper analyzes the impact of such modules and proposes a solution.


<details>
  <summary>Details</summary>
Motivation: To address the conflict between powerful VLMs and real-time robot control, ensuring efficient control without losing semantic knowledge from pretrained VLMs.

Method: Study VLAs with continuous diffusion or flow matching action experts, analyze design choices, and propose a technique to insulate the VLM backbone during training.

Result: Naively including action experts harms training speed and knowledge transfer. The proposed insulation technique mitigates these issues.

Conclusion: The paper highlights the trade-offs in VLA design and offers a solution to preserve VLM knowledge while enabling efficient control.

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [684] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/pdf/2505.23719)
*Andreas Auer, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, Sepp Hochreiter*

Main category: cs.LG

TL;DR: TiRex, an enhanced LSTM model, outperforms existing zero-shot time series forecasting methods by combining state-tracking and in-context learning.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based models underperform in time series forecasting, while LSTMs lack strong in-context learning. TiRex bridges this gap.

Method: TiRex leverages xLSTM for in-context learning and introduces CPM, a masking strategy to enhance state-tracking.

Result: TiRex achieves state-of-the-art performance on benchmarks like GiftEval and Chronos-ZS, surpassing larger models.

Conclusion: TiRex successfully combines the strengths of LSTMs and in-context learning, setting a new standard for zero-shot forecasting.

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [685] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/pdf/2505.23721)
*Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER, a template-free retrosynthesis method using categorical diffusion, outperforms autoregressive models by predicting entire SMILES sequences at once, achieving state-of-the-art top-1 accuracy.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for chemical retrosynthesis are limited; DiffER aims to overcome these constraints by predicting sequences in unison.

Method: DiffER employs an ensemble of categorical diffusion models with a novel length prediction component to sample from the posterior distribution of reactants.

Result: DiffER achieves state-of-the-art top-1 accuracy and competitive performance in top-3, top-5, and top-10 metrics among template-free methods.

Conclusion: DiffER is a strong baseline for template-free retrosynthesis, with SMILES length prediction being key to its performance.

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [686] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/pdf/2505.23725)
*Benjamin Thérien, Xiaolong Huang, Irina Rish, Eugene Belilovsky*

Main category: cs.LG

TL;DR: MuLoCo (Muon inner optimizer DiLoCo) reduces communication overhead in DiLoCo by 8X with 2-bit compression and no performance loss, outperforming DiLoCo.


<details>
  <summary>Details</summary>
Motivation: To explore the role of error feedback and inner-optimizer effects on compressibility in DiLoCo, aiming to reduce communication overhead.

Method: Investigates Top-k sparsification and quantization with AdamW and Muon optimizers, using error-feedback accumulators for compression.

Result: MuLoCo achieves 8X less communication with 2-bit compression and no performance degradation, outperforming DiLoCo.

Conclusion: MuLoCo is a more efficient variant of DiLoCo, significantly reducing communication while maintaining performance.

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [687] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/pdf/2505.23732)
*Shreeram Suresh Chandra, Lucas Goncalves, Junchen Lu, Carlos Busso, Berrak Sisman*

Main category: cs.LG

TL;DR: EmotionRankCLAP improves emotion-based CLAP by capturing ordinal relationships and enhancing cross-modal alignment using a Rank-N-Contrast objective.


<details>
  <summary>Details</summary>
Motivation: Existing CLAP methods naively align audio and text, missing emotion ordinality and causing a modality gap.

Method: Uses dimensional emotion attributes and Rank-N-Contrast to learn ordered relationships in valence-arousal space.

Result: Outperforms existing methods in cross-modal retrieval, better modeling emotion ordinality.

Conclusion: EmotionRankCLAP addresses alignment and ordinality issues, improving emotion understanding across modalities.

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [688] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/pdf/2505.23749)
*Paul Gölz, Nika Haghtalab, Kunhe Yang*

Main category: cs.LG

TL;DR: The paper critiques current alignment methods (like PPO-based RLHF and DPO) for assuming a single preference model, despite diverse user preferences. It introduces 'distortion' to measure alignment quality and shows Nash Learning from Human Feedback outperforms RLHF and DPO.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods fail to account for diverse user preferences, risking suboptimal average utility. The paper aims to address this gap using social choice theory.

Method: The authors model user comparisons via individual Bradley-Terry models and introduce 'distortion' to evaluate alignment methods. They analyze Nash Learning from Human Feedback, RLHF, and DPO.

Result: Nash Learning achieves minimax optimal distortion, robust across utility distributions and KL constraints. RLHF and DPO perform poorly, with high or unbounded distortion.

Conclusion: Nash Learning is superior for pluralistic alignment, while RLHF and DPO are inadequate for diverse preferences.

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [689] [Model Immunization from a Condition Number Perspective](https://arxiv.org/pdf/2505.23760)
*Amber Yijia Zheng, Cedar Site Bai, Brian Bullins, Raymond A. Yeh*

Main category: cs.LG

TL;DR: The paper proposes a framework for model immunization using Hessian matrix condition numbers, with an algorithm for pre-training control, validated on linear and non-linear models.


<details>
  <summary>Details</summary>
Motivation: To clarify when model immunization is possible and define an immunized model, addressing gaps in prior work.

Method: A framework based on Hessian matrix condition numbers, with a regularization-based algorithm for pre-training control.

Result: Empirical validation shows effectiveness on both linear models and non-linear deep networks.

Conclusion: The proposed framework and algorithm successfully immunize models while retaining utility on non-harmful tasks.

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


### [690] [Minimal Sufficient Views: A DNN model making predictions with more evidence has higher accuracy](https://arxiv.org/pdf/2402.01095)
*Keisuke Kawano, Takuro Kutsuna, Keisuke Sano*

Main category: cs.LG

TL;DR: The paper explores DNNs' generalization by proposing Minimal Sufficient Views (MSVs) as evidence regions in images. It shows a correlation between MSV count and generalization performance, offering a label-free model selection metric.


<details>
  <summary>Details</summary>
Motivation: To understand why DNNs generalize well, hypothesizing that they rely on multiple pieces of evidence (MSVs) from images.

Method: Proposed MSVs—minimal image regions preserving DNN predictions—and tested their correlation with generalization across models.

Result: Found strong correlation between MSV count and generalization, even across different DNN architectures. Proposed MSV-based metric outperforms existing ones like average confidence.

Conclusion: DNNs using more evidence (MSVs) generalize better. The MSV-based metric is reliable for model selection without labels.

Abstract: Deep neural networks (DNNs) exhibit high performance in image recognition;
however, the reasons for their strong generalization abilities remain unclear.
A plausible hypothesis is that DNNs achieve robust and accurate predictions by
identifying multiple pieces of evidence from images. Thus, to test this
hypothesis, this study proposed minimal sufficient views (MSVs). MSVs is
defined as a set of minimal regions within an input image that are sufficient
to preserve the prediction of DNNs, thus representing the evidence discovered
by the DNN. We empirically demonstrated a strong correlation between the number
of MSVs (i.e., the number of pieces of evidence) and the generalization
performance of the DNN models. Remarkably, this correlation was found to hold
within a single DNN as well as between different DNNs, including convolutional
and transformer models. This suggested that a DNN model that makes its
prediction based on more evidence has a higher generalization performance. We
proposed a metric based on MSVs for DNN model selection that did not require
label information. Consequently, we empirically showed that the proposed metric
was less dependent on the degree of overfitting, rendering it a more reliable
indicator of model performance than existing metrics, such as average
confidence.

</details>


### [691] [OmniArch: Building Foundation Model For Scientific Computing](https://arxiv.org/pdf/2402.16014)
*Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Chonghan Gao, Rongye Shi, Shanghang Zhang, Jianxin Li*

Main category: cs.LG

TL;DR: OmniArch is a foundation model for multi-scale, multi-physics scientific computing, achieving state-of-the-art performance and adaptability via unified architecture and physics-informed fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore if foundation models' success in language modeling can extend to scientific computing, addressing multi-scale and multi-physics challenges.

Method: OmniArch uses a Fourier Encoder-decoder and Transformer backbone for pre-training, with PDE-Aligner for physics-informed fine-tuning.

Result: Achieves new benchmarks for 1D-2D-3D PDEs and shows adaptability via in-context and zero-shot learning.

Conclusion: OmniArch successfully bridges foundation models and scientific computing, enabling realistic applications and physics discovery.

Abstract: Foundation models have revolutionized language modeling, while whether this
success is replicated in scientific computing remains unexplored. We present
OmniArch, the first prototype aiming at solving multi-scale and multi-physics
scientific computing problems with physical alignment. We addressed all three
challenges with one unified architecture. Its pre-training stage contains a
Fourier Encoder-decoder fading out the disharmony across separated dimensions
and a Transformer backbone integrating quantities through temporal dynamics,
and the novel PDE-Aligner performs physics-informed fine-tuning under flexible
conditions. As far as we know, we first conduct 1D-2D-3D united pre-training on
the PDEBench, and it sets not only new performance benchmarks for 1D, 2D, and
3D PDEs but also demonstrates exceptional adaptability to new physics via
in-context and zero-shot learning approaches, which supports realistic
engineering applications and foresight physics discovery.

</details>


### [692] [Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers](https://arxiv.org/pdf/2403.07404)
*Filip Szatkowski, Yaoyue Zheng, Fei Yang, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer*

Main category: cs.LG

TL;DR: Using auxiliary classifiers (ACs) in continual learning reduces forgetting and improves efficiency, achieving a 10% accuracy boost and 10-60% inference cost reduction.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by leveraging intermediate neural network representations, which are less prone to forgetting.

Method: Propose auxiliary classifiers (ACs) to enhance performance and integrate them into continual learning methods.

Result: ACs improve accuracy by 10% and reduce inference costs by 10-60% without accuracy loss.

Conclusion: ACs provide a scalable, efficient solution for continual learning by mitigating forgetting and optimizing computation.

Abstract: Continual learning is crucial for applying machine learning in challenging,
dynamic, and often resource-constrained environments. However, catastrophic
forgetting - overwriting previously learned knowledge when new information is
acquired - remains a major challenge. In this work, we examine the intermediate
representations in neural network layers during continual learning and find
that such representations are less prone to forgetting, highlighting their
potential to accelerate computation. Motivated by these findings, we propose to
use auxiliary classifiers(ACs) to enhance performance and demonstrate that
integrating ACs into various continual learning methods consistently improves
accuracy across diverse evaluation settings, yielding an average 10% relative
gain. We also leverage the ACs to reduce the average cost of the inference by
10-60% without compromising accuracy, enabling the model to return the
predictions before computing all the layers. Our approach provides a scalable
and efficient solution for continual learning.

</details>


### [693] [Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels](https://arxiv.org/pdf/2405.17642)
*Oleksii Furman, Patryk Wielopolski, Łukasz Lenkiewicz, Jerzy Stefanowski, Maciej Zięba*

Main category: cs.LG

TL;DR: A unified gradient-based method for generating Local, Global, and Group-wise Counterfactual Explanations (CFs) in Explainable AI (XAI), enhancing efficiency and plausibility.


<details>
  <summary>Details</summary>
Motivation: The complexity of AI systems demands transparency, but existing CF methods lack integration across granularity levels (Local, Global, Group-wise).

Method: Proposes a gradient-based optimization for differentiable models, unifying CF generation and integrating plausibility criteria for Group-wise CFs.

Result: Effective balance of validity, proximity, and plausibility, with practical utility demonstrated in use cases.

Conclusion: The method addresses the gap in unified CF generation, offering actionable and trustworthy explanations for AI systems.

Abstract: The growing complexity of AI systems has intensified the need for
transparency through Explainable AI (XAI). Counterfactual explanations (CFs)
offer actionable "what-if" scenarios on three levels: Local CFs providing
instance-specific insights, Global CFs addressing broader trends, and
Group-wise CFs (GWCFs) striking a balance and revealing patterns within
cohesive groups. Despite the availability of methods for each granularity
level, the field lacks a unified method that integrates these complementary
approaches. We address this limitation by proposing a gradient-based
optimization method for differentiable models that generates Local, Global, and
Group-wise Counterfactual Explanations in a unified manner. We especially
enhance GWCF generation by combining instance grouping and counterfactual
generation into a single efficient process, replacing traditional two-step
methods. Moreover, to ensure trustworthiness, we innovatively introduce the
integration of plausibility criteria into the GWCF domain, making explanations
both valid and realistic. Our results demonstrate the method's effectiveness in
balancing validity, proximity, and plausibility while optimizing group
granularity, with practical utility validated through practical use cases.

</details>


### [694] [Stochastic Diffusion: A Diffusion Based Model for Stochastic Time Series Forecasting](https://arxiv.org/pdf/2406.02827)
*Yuansan Liu, Sudanthi Wijewickrema, Dongting Hu, Christofer Bester, Stephen O'Leary, James Bailey*

Main category: cs.LG

TL;DR: A novel Stochastic Diffusion (StochDiff) model is proposed to improve forecasting of highly stochastic time series by leveraging data-driven prior knowledge and stochastic latent spaces.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with highly stochastic time series data, limiting their forecasting accuracy.

Method: The StochDiff model learns data-driven prior knowledge at each time step using stochastic latent spaces to capture variability and uncertainty.

Result: The model effectively captures complex temporal dynamics and improves forecasting for stochastic time series, validated on real-world datasets.

Conclusion: StochDiff shows promise for stochastic time series forecasting and has potential applications in medical fields like surgical guidance.

Abstract: Recent innovations in diffusion probabilistic models have paved the way for
significant progress in image, text and audio generation, leading to their
applications in generative time series forecasting. However, leveraging such
abilities to model highly stochastic time series data remains a challenge. In
this paper, we propose a novel Stochastic Diffusion (StochDiff) model which
learns data-driven prior knowledge at each time step by utilizing the
representational power of the stochastic latent spaces to model the variability
of the multivariate time series data. The learnt prior knowledge helps the
model to capture complex temporal dynamics and the inherent uncertainty of the
data. This improves its ability to model highly stochastic time series data.
Through extensive experiments on real-world datasets, we demonstrate the
effectiveness of our proposed model on stochastic time series forecasting.
Additionally, we showcase an application of our model for real-world surgical
guidance, highlighting its potential to benefit the medical community.

</details>


### [695] [An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data](https://arxiv.org/pdf/2407.11500)
*Niamh Belton, Aonghus Lawlor, Kathleen M. Curran*

Main category: cs.LG

TL;DR: A three-stage self-supervised anomaly detection approach for continuous grading of knee osteoarthritis, requiring minimal labeled data and outperforming existing methods by up to 24%.


<details>
  <summary>Details</summary>
Motivation: Addressing the subjectivity and inaccuracy of existing knee OA grading systems and reducing reliance on large annotated datasets.

Method: 1. SS-FewSOME for learning healthy knee representations. 2. Pseudo-labeling and denoising with CLIP. 3. Dual Centre Representation Learning (DCRL) for grading based on distance to normality.

Result: Outperforms existing methods by up to 24% in OA detection, with severity scores correlating with expert human performance.

Conclusion: The proposed method offers a robust, data-efficient solution for continuous knee OA grading, aligning with expert assessments.

Abstract: The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA)
ordinal grading systems has been a subject of on-going debate and concern.
Existing automated solutions are trained to emulate these imperfect systems,
whilst also being reliant on large annotated databases for fully-supervised
training. This work proposes a three stage approach for automated continuous
grading of knee OA that is built upon the principles of Anomaly Detection (AD);
learning a robust representation of healthy knee X-rays and grading disease
severity based on its distance to the centre of normality. In the first stage,
SS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal'
representation, requiring only examples of healthy subjects and <3% of the
labels that existing methods require. In the second stage, this model is used
to pseudo label a subset of unlabelled data as 'normal' or 'anomalous',
followed by denoising of pseudo labels with CLIP. The final stage involves
retraining on labelled and pseudo labelled data using the proposed Dual Centre
Representation Learning (DCRL) which learns the centres of two representation
spaces; normal and anomalous. Disease severity is then graded based on the
distance to the learned centres. The proposed methodology outperforms existing
techniques by margins of up to 24% in terms of OA detection and the disease
severity scores correlate with the Kellgren-Lawrence grading system at the same
level as human expert performance. Code available at
https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.

</details>


### [696] [Keep Everyone Happy: Online Fair Division of Numerous Items with Few Copies](https://arxiv.org/pdf/2408.12845)
*Arun Verma, Indrajit Saha, Makoto Yokoo, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: The paper introduces a contextual bandit approach for online fair division with sub-linear regret, addressing utility estimation challenges in sparse real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing fair division algorithms fail in real-world settings with many items and few copies, making utility estimation inaccurate.

Method: Proposes modeling the problem as a contextual bandit, using item-agent features to estimate utilities and ensure fairness.

Result: Algorithms achieve sub-linear regret guarantees and perform well in experiments.

Conclusion: The contextual bandit approach effectively handles sparse utility estimation in online fair division.

Abstract: This paper considers a novel variant of the online fair division problem
involving multiple agents in which a learner sequentially observes an
indivisible item that has to be irrevocably allocated to one of the agents
while satisfying a fairness and efficiency constraint. Existing algorithms
assume a small number of items with a sufficiently large number of copies,
which ensures a good utility estimation for all item-agent pairs from noisy
bandit feedback. However, this assumption may not hold in many real-life
applications, for example, an online platform that has a large number of users
(items) who use the platform's service providers (agents) only a few times (a
few copies of items), which makes it difficult to accurately estimate utilities
for all item-agent pairs. To address this, we assume utility is an unknown
function of item-agent features. We then propose algorithms that model online
fair division as a contextual bandit problem, with sub-linear regret
guarantees. Our experimental results further validate the effectiveness of the
proposed algorithms.

</details>


### [697] [Policy Filtration for RLHF to Mitigate Noise in Reward Models](https://arxiv.org/pdf/2409.06957)
*Chuheng Zhang, Wei Shen, Li Zhao, Xuyun Zhang, Xiaolong Xu, Wanchun Dou, Jiang Biang*

Main category: cs.LG

TL;DR: PF-PPO improves RLHF by filtering unreliable reward samples, enhancing policy learning in tasks like code generation and math reasoning.


<details>
  <summary>Details</summary>
Motivation: The inaccuracy of intermediate reward models in RLHF, especially for complex reasoning tasks, motivates filtering unreliable samples to improve learning.

Method: Proposes Policy Filtration for Proximal Policy Optimization (PF-PPO), using R2 metrics to select filtering strategies for better reward reliability.

Result: PF-PPO achieves state-of-the-art performance in code generation (HumanEval, MBPP, LeetCode) and math reasoning (Ape210K, CMATH).

Conclusion: PF-PPO effectively addresses reward model inaccuracies, improving performance in complex tasks.

Abstract: While direct policy optimization methods exist, pioneering LLMs are
fine-tuned with reinforcement learning from human feedback (RLHF) to generate
better responses under the supervision of a reward model learned from
preference data. One major challenge of RLHF is the inaccuracy of the
intermediate reward model, especially in the tasks that requires complex
reasoning for the reward model to score a response. We find that the
reliability of the reward model varies across responses assigned with different
rewards. This motivates us to filter the samples whose rewards may be
unreliable to improve the signal-to-noise ratio during policy learning,
resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To
choose a proper policy filtering strategy, we use the coefficient of
determination (R2) between the rewards and actual scores on filtered samples as
the metrics to help us find promising strategies since it measures how well the
rewards filtered by PF-PPO indicate real performance. We provide extensive
experiments to validate the effectiveness of PF-PPO in code generation and math
reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art
performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%),
and LeetCode Contest (+10.0%) which is a more challenging benchmark created by
us. In math reasoning, PF-PPO yields performance increase using different
reward models and benchmarks (Ape210K and CMATH). Code is available on
https://github.com/DtYXs/verl/tree/pf-ppo.

</details>


### [698] [Exploring the Limitations of Mamba in COPY and CoT Reasoning](https://arxiv.org/pdf/2410.03810)
*Ruifeng Ren, Zhicong Li, Yong Liu*

Main category: cs.LG

TL;DR: Mamba's constant inference size offers computational savings but struggles with COPY operations and CoT reasoning compared to Transformers, unless its size grows linearly, negating overhead benefits.


<details>
  <summary>Details</summary>
Motivation: To analyze Mamba's expressive ability in COPY operations and CoT reasoning, comparing it to Transformers to understand its limitations and potential savings.

Method: Theoretical analysis connecting Mamba to linear attention, experiments on COPY and CoT tasks, and comparison with Transformers.

Result: Mamba struggles with COPY unless size grows linearly, and its cost for CoT tasks matches Transformers unless problems have favorable properties like locality.

Conclusion: Mamba's savings are context-dependent; it matches Transformers only in specific scenarios, highlighting its limitations in expressive power.

Abstract: Transformers have become the backbone of modern Large Language Models (LLMs);
however, their inference overhead grows linearly with the sequence length,
posing challenges for modeling long sequences. In light of this, Mamba has
attracted attention for maintaining a constant inference size, with empirical
evidence demonstrating that it can match Transformer performance in sequence
modeling while significantly reducing computational costs. However, an open
question remains: can Mamba always bring savings while achieving performance
comparable to Transformers? In this paper, we focus on analyzing the expressive
ability of Mamba to perform our defined COPY operation and Chain of Thought
(CoT) reasoning. First, inspired by the connection between Mamba and linear
attention, we show that constant-sized Mamba may struggle to perform COPY
operations while Transformers can handle them more easily. However, when the
size of Mamba grows linearly with the input sequence length, it can accurately
perform COPY, but in this case, Mamba no longer provides overhead savings.
Based on this observation, we further analyze Mamba's ability to tackle CoT
tasks, which can be described by the Dynamic Programming (DP) problems. Our
findings suggest that to solve arbitrary DP problems, the total cost of Mamba
is still comparable to standard Transformers. However, similar to efficient
Transformers, when facing DP problems with favorable properties such as
locality, Mamba can provide savings in overhead. Our experiments on the copy
and CoT tasks further demonstrate Mamba's limitations compared to Transformers
in learning these tasks.

</details>


### [699] [SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression](https://arxiv.org/pdf/2410.09615)
*Mohammad Mozaffari, Amir Yazdanbakhsh, Maryam Mehri Dehnavi*

Main category: cs.LG

TL;DR: SLIM is a one-shot compression framework combining quantization, sparsity, and low-rank approximation, improving accuracy and efficiency for LLMs without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational cost and accuracy in model compression for LLMs, SLIM aims to eliminate retraining while maintaining high accuracy.

Method: SLIM integrates uniform quantization (SLIM-Quant), semi-structured sparsity, and low-rank adapters with a novel saliency function to compensate for errors.

Result: SLIM improves accuracy by up to 5.66% for 4-bit quantization with 2:4 sparsity, achieves up to 4.3x speedup on GPUs, and reduces memory by 0.23x. Optional PEFT further boosts accuracy by 1.66%.

Conclusion: SLIM offers a unified, efficient one-shot compression solution for LLMs, outperforming prior methods in accuracy and performance.

Abstract: Conventional model compression techniques for LLMs address high memory
consumption and slow inference challenges but typically require computationally
expensive retraining to preserve accuracy. In contrast, one-shot compression
methods eliminate retraining cost, but struggle to achieve accuracy comparable
to dense models. This paper presents SLIM, a new one-shot compression framework
that holistically integrates hardware-friendly quantization, sparsity, and
low-rank approximation into a unified process. First, we formulate the
quantization process using a probabilistic approach (SLIM-Quant) that enables
us to apply uniform quantization. Then, we use an existing one-shot pruning
method to apply semi-structured sparsity on top of the quantized weights.
Finally, to compensate for the introduced aggregated quantization and sparsity
error, we use a novel saliency function with unique invertible and additive
features that enables us to mathematically compute the value of low-rank
adapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4
sparsity with 4-bit weight quantization, outperforming prior methods. Models
compressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100
GPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory
reduction in comparison to their dense counterparts. We also propose an
optional PEFT recipe that further improves accuracy by up to 1.66%
(LLaMA-2-13B) compared to SLIM without fine-tuning.

</details>


### [700] [SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning](https://arxiv.org/pdf/2410.09754)
*Hojoon Lee, Dongyoon Hwang, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter R. Wurman, Jaegul Choo, Peter Stone, Takuma Seno*

Main category: cs.LG

TL;DR: SimBa is a scalable architecture for deep RL that introduces simplicity bias to improve sample efficiency and performance across diverse algorithms and environments.


<details>
  <summary>Details</summary>
Motivation: Despite scaling success in CV and NLP, deep RL lacks exploration in network scaling. SimBa addresses this by integrating simplicity bias to avoid overfitting.

Method: SimBa includes observation normalization, a residual feedforward block, and layer normalization to scale parameters effectively.

Result: SimBa improves sample efficiency in various RL algorithms and matches or outperforms state-of-the-art methods with high computational efficiency.

Conclusion: SimBa demonstrates broad applicability and effectiveness in diverse RL settings, highlighting its potential for advancing deep RL.

Abstract: Recent advances in CV and NLP have been largely driven by scaling up the
number of network parameters, despite traditional theories suggesting that
larger networks are prone to overfitting. These large networks avoid
overfitting by integrating components that induce a simplicity bias, guiding
models toward simple and generalizable solutions. However, in deep RL,
designing and scaling up networks have been less explored. Motivated by this
opportunity, we present SimBa, an architecture designed to scale up parameters
in deep RL by injecting a simplicity bias. SimBa consists of three components:
(i) an observation normalization layer that standardizes inputs with running
statistics, (ii) a residual feedforward block to provide a linear pathway from
the input to output, and (iii) a layer normalization to control feature
magnitudes. By scaling up parameters with SimBa, the sample efficiency of
various deep RL algorithms-including off-policy, on-policy, and unsupervised
methods-is consistently improved. Moreover, solely by integrating SimBa
architecture into SAC, it matches or surpasses state-of-the-art deep RL methods
with high computational efficiency across DMC, MyoSuite, and HumanoidBench.
These results demonstrate SimBa's broad applicability and effectiveness across
diverse RL algorithms and environments.

</details>


### [701] [Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces](https://arxiv.org/pdf/2410.15625)
*Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken*

Main category: cs.LG

TL;DR: A framework automates mapper development for parallel programs, outperforming manual tuning and traditional methods like OpenTuner with faster performance and reduced tuning time.


<details>
  <summary>Details</summary>
Motivation: High-performance computing relies on efficient task and data mapping, but manual tuning is time-consuming and requires expertise, hindering domain scientists.

Method: Introduces a framework with a Domain-Specific Language (DSL) and AutoGuide for generative optimization, providing richer feedback than scalar metrics.

Result: Achieves 3.8X faster performance than OpenTuner in 10 iterations and surpasses expert-written mappers by up to 1.34X speedup.

Conclusion: The framework significantly improves mapper performance and reduces tuning time, making high-performance computing more accessible.

Abstract: Modern scientific discovery increasingly relies on high-performance computing
for complex modeling and simulation. A key challenge in improving parallel
program performance is efficiently mapping tasks to processors and data to
memory, a process dictated by intricate, low-level system code known as
mappers. Developing high-performance mappers demands days of manual tuning,
posing a significant barrier for domain scientists without systems expertise.
We introduce a framework that automates mapper development with generative
optimization, leveraging richer feedback beyond scalar performance metrics. Our
approach features the Agent-System Interface, which includes a Domain-Specific
Language (DSL) to abstract away the low-level complexity of system code and
define a structured search space, as well as AutoGuide, a mechanism that
interprets raw execution output into actionable feedback. Unlike traditional
reinforcement learning methods such as OpenTuner, which rely solely on scalar
feedback, our method finds superior mappers in far fewer iterations. With just
10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving
3.8X faster performance. Our approach finds mappers that surpass expert-written
mappers by up to 1.34X speedup across nine benchmarks while reducing tuning
time from days to minutes.

</details>


### [702] [GWQ: Gradient-Aware Weight Quantization for Large Language Models](https://arxiv.org/pdf/2411.00850)
*Yihua Shao, Yan Gu, Siyu Chen, Haiyang Liu, Zixian Zhu, Zijian Ling, Minxi Yan, Ziyang Yan, Chenyu Zhang, Michele Magno, Haotong Qin, Yan Wang, Jingcai Guo, Ling Shao, Hao Tang*

Main category: cs.LG

TL;DR: GWQ is a gradient-aware weight quantization method for compressing LLMs to low bits, improving deployment on resource-constrained devices with minimal calibration data.


<details>
  <summary>Details</summary>
Motivation: Large LLMs face deployment challenges due to their size; GWQ aims to compress them efficiently.

Method: GWQ uses gradients to detect outliers, retaining top 1% in FP16 and quantizing the rest to low bits with minimal calibration.

Result: GWQ outperforms other methods, achieves 1.2x speedup, and reduces memory usage.

Conclusion: GWQ is an effective, efficient quantization method for LLMs.

Abstract: Large language models (LLMs) show impressive performance in solving complex
language tasks. However, its large number of parameters presents significant
challenges for the deployment. So, compressing LLMs to low bits can enable to
deploy on resource-constrained devices. To address this problem, we propose
gradient-aware weight quantization (GWQ), the first quantization approach for
low-bit weight quantization that leverages gradients to localize outliers,
requiring only a minimal amount of calibration data for outlier detection. GWQ
retains the top 1\% outliers preferentially at FP16 precision, while the
remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ
on different task include language modeling, grounding detection, massive
multitask language understanding and vision-language question and answering.
Results show that models quantified by GWQ performs better than other
quantization method. During quantization process, GWQ only need one calibration
set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in
comparison to the original model and effectively reduces the inference memory.

</details>


### [703] [GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers](https://arxiv.org/pdf/2411.17296)
*Guoguo Ai, Guansong Pang, Hezhe Qiao, Yuan Gao, Hui Yan*

Main category: cs.LG

TL;DR: GrokFormer, a Graph Transformer model, improves graph representation learning by adaptively learning spectral filters, outperforming existing GTs and GNNs.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Transformers (GTs) struggle with capturing high-frequency signals due to fixed filters on predefined graph spectrum, limiting flexibility and expressiveness.

Method: Proposes GrokFormer, which learns adaptive spectral filters using Fourier series modeling over learnable activation functions.

Result: Theoretically and empirically, GrokFormer shows better expressiveness and outperforms state-of-the-art models on 10 node and 5 graph classification datasets.

Conclusion: GrokFormer addresses limitations of existing GTs by adaptive spectral learning, achieving superior performance in graph representation tasks.

Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph
representation learning over popular graph neural networks (GNNs). However,
self--attention, the core module of GTs, preserves only low-frequency signals
in graph features, leading to ineffectiveness in capturing other important
signals like high-frequency ones. Some recent GT models help alleviate this
issue, but their flexibility and expressiveness are still limited since the
filters they learn are fixed on predefined graph spectrum or spectral order. To
tackle this challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer
(GrokFormer), a novel GT model that learns highly expressive spectral filters
with adaptive graph spectrum and spectral order through a Fourier series
modeling over learnable activation functions. We demonstrate theoretically and
empirically that the proposed GrokFormer filter offers better expressiveness
than other spectral methods. Comprehensive experiments on 10 real-world node
classification datasets across various domains, scales, and graph properties,
as well as 5 graph classification datasets, show that GrokFormer outperforms
state-of-the-art GTs and GNNs. Our code is available at
https://github.com/GGA23/GrokFormer

</details>


### [704] [SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot Navigation in Digital Twins](https://arxiv.org/pdf/2411.18212)
*Aladin Djuhera, Amin Seffo, Vlad C. Andrei, Holger Boche, Walid Saad*

Main category: cs.LG

TL;DR: SCoTT is a wireless-aware path planning framework using VLMs to optimize path gains and trajectory length, outperforming traditional methods like A* and DP-WA* in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional path planning algorithms struggle with wireless performance constraints due to high computational costs, necessitating a more efficient solution.

Method: SCoTT uses vision-language models (VLMs) and a novel prompting paradigm (Strategic Chain-of-Thought Tasking) to decompose and solve the planning problem, leveraging wireless heatmaps and ray-tracing data.

Result: SCoTT achieves path gains within 2% of DP-WA* while generating shorter trajectories and reducing DP-WA*'s search space, saving up to 62% in execution time.

Conclusion: SCoTT is practical, scalable, and effective for wireless-aware navigation, with potential for real-world applications in 6G-enabled digital twins.

Abstract: Path planning under wireless performance constraints is a complex challenge
in robot navigation. However, naively incorporating such constraints into
classical planning algorithms often incurs prohibitive search costs. In this
paper, we propose SCoTT, a wireless-aware path planning framework that
leverages vision-language models (VLMs) to co-optimize average path gains and
trajectory length using wireless heatmap images and ray-tracing data from a
digital twin (DT). At the core of our framework is Strategic Chain-of-Thought
Tasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive
search problem into structured subtasks, each solved via chain-of-thought
prompting. To establish strong baselines, we compare classical A* and
wireless-aware extensions of it, and derive DP-WA*, an optimal, iterative
dynamic programming algorithm that incorporates all path gains and distance
metrics from the DT, but at significant computational cost. In extensive
experiments, we show that SCoTT achieves path gains within 2% of DP-WA* while
consistently generating shorter trajectories. Moreover, SCoTT's intermediate
outputs can be used to accelerate DP-WA* by reducing its search space, saving
up to 62% in execution time. We validate our framework using four VLMs,
demonstrating effectiveness across both large and small models, thus making it
applicable to a wide range of compact models at low inference cost. We also
show the practical viability of our approach by deploying SCoTT as a ROS node
within Gazebo simulations. Finally, we discuss data acquisition pipelines,
compute requirements, and deployment considerations for VLMs in 6G-enabled DTs,
underscoring the potential of natural language interfaces for wireless-aware
navigation in real-world applications.

</details>


### [705] [One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs](https://arxiv.org/pdf/2412.00315)
*Jingzhe Liu, Haitao Mao, Zhikai Chen, Bingheng Li, Wenqi Fan, Mingxuan Ju, Tong Zhao, Neil Shah, Jiliang Tang*

Main category: cs.LG

TL;DR: A cross-domain pretraining framework for GNNs is proposed to generalize across diverse graphs, using expert models and gating functions for effective knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs require domain-specific designs and struggle to generalize across different graphs, making it hard for practitioners to choose suitable models.

Method: Pretrain a bank of expert models (one per dataset) and use gating functions to select relevant experts for new graphs, avoiding negative transfer.

Result: The method outperforms others in link prediction and node classification tasks.

Conclusion: The framework successfully generalizes GNNs across domains, addressing the limitations of previous approaches.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool to capture
intricate network patterns, achieving success across different domains.
However, existing GNNs require careful domain-specific architecture designs and
training from scratch on each dataset, leading to an expertise-intensive
process with difficulty in generalizing across graphs from different domains.
Therefore, it can be hard for practitioners to infer which GNN model can
generalize well to graphs from their domains. To address this challenge, we
propose a novel cross-domain pretraining framework, "one model for one graph,"
which overcomes the limitations of previous approaches that failed to use a
single GNN to capture diverse graph patterns across domains with significant
gaps. Specifically, we pretrain a bank of expert models, with each one
corresponding to a specific dataset. When inferring to a new graph, gating
functions choose a subset of experts to effectively integrate prior model
knowledge while avoiding negative transfer. Extensive experiments consistently
demonstrate the superiority of our proposed method on both link prediction and
node classification tasks.

</details>


### [706] [INRFlow: Flow Matching for INRs in Ambient Space](https://arxiv.org/pdf/2412.03791)
*Yuyang Wang, Anurag Ranjan, Josh Susskind, Miguel Angel Bautista*

Main category: cs.LG

TL;DR: INRFlow introduces a domain-agnostic flow matching transformer trained directly in ambient space, eliminating the need for hand-crafted compressors and unifying generative modeling across diverse data modalities like images, 3D point clouds, and protein structures.


<details>
  <summary>Details</summary>
Motivation: The two-stage training paradigm of flow matching models, requiring hand-crafted compressors for different data modalities, hinders unification across domains. INRFlow aims to overcome this limitation.

Method: INRFlow uses a conditionally independent point-wise training objective inspired by INRs, enabling continuous predictions in coordinate space without domain-specific compressors.

Result: INRFlow effectively handles various data modalities, outperforming comparable approaches and demonstrating strong performance across domains.

Conclusion: INRFlow is a promising step toward domain-agnostic flow matching generative models, easily adaptable to different data domains.

Abstract: Flow matching models have emerged as a powerful method for generative
modeling on domains like images or videos, and even on irregular or
unstructured data like 3D point clouds or even protein structures. These models
are commonly trained in two stages: first, a data compressor is trained, and in
a subsequent training stage a flow matching generative model is trained in the
latent space of the data compressor. This two-stage paradigm sets obstacles for
unifying models across data domains, as hand-crafted compressors architectures
are used for different data modalities. To this end, we introduce INRFlow, a
domain-agnostic approach to learn flow matching transformers directly in
ambient space. Drawing inspiration from INRs, we introduce a conditionally
independent point-wise training objective that enables INRFlow to make
predictions continuously in coordinate space. Our empirical results demonstrate
that INRFlow effectively handles different data modalities such as images, 3D
point clouds and protein structure data, achieving strong performance in
different domains and outperforming comparable approaches. INRFlow is a
promising step towards domain-agnostic flow matching generative models that can
be trivially adopted in different data domains.

</details>


### [707] [Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition](https://arxiv.org/pdf/2412.17333)
*Jaeheun Jung, Jaehyuk Lee, Changhae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee*

Main category: cs.LG

TL;DR: HEGGS is a high-fidelity earthquake ground motion generation system that outperforms existing models by leveraging a conditional latent diffusion model and waveform construction, validated across multiple regions.


<details>
  <summary>Details</summary>
Motivation: To reduce losses from earthquakes by generating realistic ground motion waveforms, addressing the limitations of current generative models.

Method: HEGGS uses an end-to-end differentiable generator with a conditional latent diffusion model and hi-fidelity waveform construction, trained efficiently on a single GPU.

Result: HEGGS generates accurate 3D seismic waveforms with precise phase arrivals, envelope correlation, and other seismological criteria, validated across North America, East Asia, and Europe.

Conclusion: HEGGS demonstrates superior performance in generating realistic earthquake waveforms, offering potential for improved disaster preparedness and mitigation.

Abstract: Shock waves caused by earthquakes can be devastating. Generating realistic
earthquake-caused ground motion waveforms help reducing losses in lives and
properties, yet generative models for the task tend to generate subpar
waveforms. We present High-fidelity Earthquake Groundmotion Generation System
(HEGGS) and demonstrate its superior performance using earthquakes from North
American, East Asian, and European regions. HEGGS exploits the intrinsic
characteristics of earthquake dataset and learns the waveforms using an
end-to-end differentiable generator containing conditional latent diffusion
model and hi-fidelity waveform construction model. We show the learning
efficiency of HEGGS by training it on a single GPU machine and validate its
performance using earthquake databases from North America, East Asia, and
Europe, using diverse criteria from waveform generation tasks and seismology.
Once trained, HEGGS can generate three dimensional E-N-Z seismic waveforms with
accurate P/S phase arrivals, envelope correlation, signal-to-noise ratio, GMPE
analysis, frequency content analysis, and section plot analysis.

</details>


### [708] [A Tutorial on Meta-Reinforcement Learning](https://arxiv.org/pdf/2301.08028)
*Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, Shimon Whiteson*

Main category: cs.LG

TL;DR: Meta-RL aims to improve data efficiency and policy generality in deep RL by learning adaptable policies across task distributions. This survey outlines the problem setting, variations, and clusters of meta-RL research, along with algorithms, applications, and open challenges.


<details>
  <summary>Details</summary>
Motivation: Deep RL's poor data efficiency and limited policy generality hinder widespread adoption. Meta-RL addresses these issues by treating RL algorithm improvement as a learning problem.

Method: Meta-RL involves learning policies adaptable to new tasks from a distribution with minimal data. The survey categorizes research based on task distribution presence and learning budget.

Result: The survey provides a detailed overview of meta-RL, including its variations, algorithms, and applications, highlighting its potential to enhance deep RL.

Conclusion: Meta-RL shows promise for advancing deep RL, but open challenges remain before it becomes a standard tool for practitioners.

Abstract: While deep reinforcement learning (RL) has fueled multiple high-profile
successes in machine learning, it is held back from more widespread adoption by
its often poor data efficiency and the limited generality of the policies it
produces. A promising approach for alleviating these limitations is to cast the
development of better RL algorithms as a machine learning problem itself in a
process called meta-RL. Meta-RL is most commonly studied in a problem setting
where, given a distribution of tasks, the goal is to learn a policy that is
capable of adapting to any new task from the task distribution with as little
data as possible. In this survey, we describe the meta-RL problem setting in
detail as well as its major variations. We discuss how, at a high level,
meta-RL research can be clustered based on the presence of a task distribution
and the learning budget available for each individual task. Using these
clusters, we then survey meta-RL algorithms and applications. We conclude by
presenting the open problems on the path to making meta-RL part of the standard
toolbox for a deep RL practitioner.

</details>


### [709] [Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](https://arxiv.org/pdf/2501.09620)
*Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang*

Main category: cs.LG

TL;DR: Proposes causal reward modeling to mitigate spurious correlations in RLHF, improving LLM alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: RLHF introduces biases (e.g., length bias, sycophancy) due to spurious correlations, hindering true causal relationships.

Method: Integrates causality into reward modeling, enforcing counterfactual invariance to ensure consistent predictions.

Result: Effectively mitigates spurious correlations in synthetic and real-world datasets, enhancing reliability and fairness.

Conclusion: Causal reward modeling is a practical drop-in enhancement for RLHF, improving LLM trustworthiness and fairness.

Abstract: Recent advances in large language models (LLMs) have demonstrated significant
progress in performing complex tasks. While Reinforcement Learning from Human
Feedback (RLHF) has been effective in aligning LLMs with human preferences, it
is susceptible to spurious correlations in reward modeling. Consequently, it
often introduces biases-such as length bias, sycophancy, conceptual bias, and
discrimination-that hinder the model's ability to capture true causal
relationships. To address this, we propose a novel causal reward modeling
approach that integrates causality to mitigate these spurious correlations. Our
method enforces counterfactual invariance, ensuring reward predictions remain
consistent when irrelevant variables are altered. Through experiments on both
synthetic and real-world datasets, we show that our approach mitigates various
types of spurious correlations effectively, resulting in more reliable and fair
alignment of LLMs with human preferences. As a drop-in enhancement to the
existing RLHF workflow, our causal reward modeling provides a practical way to
improve the trustworthiness and fairness of LLM finetuning.

</details>


### [710] [Plug In and Learn: Federated Intelligence over a Smart Grid of Models](https://arxiv.org/pdf/2302.04363)
*S. Abdurakhmanova, Y. SarcheshmehPour, A. Jung*

Main category: cs.LG

TL;DR: A federated learning method mimics a smart power grid, using lightweight signals and graph-based regularization to coordinate diverse local models.


<details>
  <summary>Details</summary>
Motivation: To enable flexible federated learning with diverse local models while ensuring coordination via lightweight signals.

Method: Uses a graph-based regularizer to encourage similar predictions among connected models on shared unlabeled data, supporting various local models.

Result: The method is model-agnostic, works with standard ML libraries, and supports both parametric and non-parametric models.

Conclusion: The approach provides a flexible and practical solution for federated learning with diverse local models.

Abstract: We present a model-agnostic federated learning method that mirrors the
operation of a smart power grid: diverse local models, like energy prosumers,
train independently on their own data while exchanging lightweight signals to
coordinate with statistically similar peers. This coordination is governed by a
graph-based regularizer that encourages connected models to produce similar
predictions on a shared, public unlabeled dataset. The resulting method is a
flexible instance of regularized empirical risk minimization and supports a
wide variety of local models - both parametric and non-parametric - provided
they can be trained via regularized loss minimization. Such training is readily
supported by standard ML libraries including scikit-learn, Keras, and PyTorch.

</details>


### [711] [Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability](https://arxiv.org/pdf/2501.18887)
*Shichang Zhang, Tessa Han, Usha Bhalla, Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: The paper proposes a unified view of feature, data, and component attribution methods in AI interpretability, highlighting their shared techniques and benefits for broader research.


<details>
  <summary>Details</summary>
Motivation: The fragmented landscape of interpretability methods and terminology necessitates a unified perspective to enhance understanding and innovation.

Method: Analyzes popular attribution methods, identifies shared techniques (e.g., perturbations, gradients), and presents a unified framework.

Result: Demonstrates how the unified view improves understanding, evaluation, and research directions in interpretability and AI.

Conclusion: A unified approach to attribution methods fosters cross-disciplinary innovation and addresses common challenges in AI interpretability.

Abstract: The increasing complexity of AI systems has made understanding their behavior
critical. Numerous interpretability methods have been developed to attribute
model behavior to three key aspects: input features, training data, and
internal model components, which emerged from explainable AI, data-centric AI,
and mechanistic interpretability, respectively. However, these attribution
methods are studied and applied rather independently, resulting in a fragmented
landscape of methods and terminology. This position paper argues that feature,
data, and component attribution methods share fundamental similarities, and a
unified view of them benefits both interpretability and broader AI research. To
this end, we first analyze popular methods for these three types of
attributions and present a unified view demonstrating that these seemingly
distinct methods employ similar techniques (such as perturbations, gradients,
and linear approximations) over different aspects and thus differ primarily in
their perspectives rather than techniques. Then, we demonstrate how this
unified view enhances understanding of existing attribution methods, highlights
shared concepts and evaluation criteria among these methods, and leads to new
research directions both in interpretability research, by addressing common
challenges and facilitating cross-attribution innovation, and in AI more
broadly, with applications in model editing, steering, and regulation.

</details>


### [712] [When Collaborative Filtering is not Collaborative: Unfairness of PCA for Recommendations](https://arxiv.org/pdf/2310.09687)
*David Liu, Jackie Baek, Tina Eliassi-Rad*

Main category: cs.LG

TL;DR: The paper analyzes fairness issues in PCA for recommendations, identifies two unfairness mechanisms, and proposes Item-Weighted PCA to mitigate them, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To understand why PCA is unfair in recommendations and address the identified unfairness mechanisms affecting both less and highly popular items.

Method: Develops Item-Weighted PCA, a polynomial-time algorithm that up-weights less popular items when optimizing principal components, solving a convex problem with a hard rank constraint.

Result: Item-Weighted PCA mitigates unfairness and outperforms PCA baselines in real-world datasets, sometimes yielding optimal solutions where baselines fail.

Conclusion: Item-Weighted PCA effectively addresses PCA's unfairness in recommendations and improves performance, offering a flexible and theoretically grounded solution.

Abstract: We study the fairness of dimensionality reduction methods for
recommendations. We focus on the fundamental method of principal component
analysis (PCA), which identifies latent components and produces a low-rank
approximation via the leading components while discarding the trailing
components. Prior works have defined notions of "fair PCA"; however, these
definitions do not answer the following question: why is PCA unfair? We
identify two underlying popularity mechanisms that induce item unfairness in
PCA. The first negatively impacts less popular items because less popular items
rely on trailing latent components to recover their values. The second
negatively impacts highly popular items, since the leading PCA components
specialize in individual popular items instead of capturing similarities
between items. To address these issues, we develop a polynomial-time algorithm,
Item-Weighted PCA, that flexibly up-weights less popular items when optimizing
for leading principal components. We theoretically show that PCA, in all cases,
and Normalized PCA, in cases of block-diagonal matrices, are instances of
Item-Weighted PCA. We empirically show that there exist datasets for which
Item-Weighted PCA yields the optimal solution while the baselines do not. In
contrast to past dimensionality reduction re-weighting techniques,
Item-Weighted PCA solves a convex optimization problem and enforces a hard rank
constraint. Our evaluations on real-world datasets show that Item-Weighted PCA
not only mitigates both unfairness mechanisms, but also produces
recommendations that outperform those of PCA baselines.

</details>


### [713] [A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers](https://arxiv.org/pdf/2502.01310)
*Roman Tarasov, Petr Mokrov, Milena Gazdieva, Evgeny Burnaev, Alexander Korotin*

Main category: cs.LG

TL;DR: The paper analyzes the generalization error of neural network-based optimal transport (OT) methods, focusing on adversarial minimax solvers, and provides theoretical bounds for quadratic OT.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical investigation of adversarial minimax OT solvers from a statistical learning perspective.

Method: Establishes upper bounds on the generalization error of an approximate OT map using minimax quadratic OT solvers.

Result: Derived bounds depend on standard statistical and mathematical properties of neural networks, suggesting applicability to general OT cases.

Conclusion: The work fills a theoretical gap and opens a promising direction for future research in OT methods.

Abstract: Neural network-based optimal transport (OT) is a recent and fruitful
direction in the generative modeling community. It finds its applications in
various fields such as domain translation, image super-resolution,
computational biology and others. Among the existing OT approaches, of
considerable interest are adversarial minimax solvers based on semi-dual
formulations of OT problems. While promising, these methods lack theoretical
investigation from a statistical learning perspective. Our work fills this gap
by establishing upper bounds on the generalization error of an approximate OT
map recovered by the minimax quadratic OT solver. Importantly, the bounds we
derive depend solely on some standard statistical and mathematical properties
of the considered functional classes (neural nets). While our analysis focuses
on the quadratic OT, we believe that similar bounds could be derived for
general OT case, paving the promising direction for future research.

</details>


### [714] [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/pdf/2311.09948)
*Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Prashant Khanduri, Dongxiao Zhu*

Main category: cs.LG

TL;DR: The paper introduces a transferable prompt injection attack targeting in-context learning (ICL) in LLMs, along with defense strategies to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on ICL are either detectable, require triggers, or lack specificity, prompting the need for a more sophisticated attack method.

Method: A gradient-based prompt search is used to append adversarial suffixes to in-context demos, hijacking LLM outputs. Defense involves using clean demos.

Result: Experiments show the attack's effectiveness in classification and jailbreak tasks, while the defense improves LLM robustness.

Conclusion: The study reveals security vulnerabilities in ICL and calls for further research to address them.

Abstract: In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs
for specific downstream tasks by utilizing labeled examples as demonstrations
(demos) in the preconditioned prompts. Despite its promising performance,
crafted adversarial attacks pose a notable threat to the robustness of LLMs.
Existing attacks are either easy to detect, require a trigger in user input, or
lack specificity towards ICL. To address these issues, this work introduces a
novel transferable prompt injection attack against ICL, aiming to hijack LLMs
to generate the target output or elicit harmful responses. In our threat model,
the hacker acts as a model publisher who leverages a gradient-based prompt
search method to learn and append imperceptible adversarial suffixes to the
in-context demos via prompt injection. We also propose effective defense
strategies using a few shots of clean demos, enhancing the robustness of LLMs
during ICL. Extensive experimental results across various classification and
jailbreak tasks demonstrate the effectiveness of the proposed attack and
defense strategies. This work highlights the significant security
vulnerabilities of LLMs during ICL and underscores the need for further
in-depth studies.

</details>


### [715] [Theoretical guarantees on the best-of-n alignment policy](https://arxiv.org/pdf/2401.01879)
*Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh*

Main category: cs.LG

TL;DR: The paper disproves a common analytical claim about KL divergence in best-of-$n$ sampling, shows it's an upper bound, and introduces a tighter estimator. It also bounds the win rate and analyzes tradeoffs between win rate and KL divergence.


<details>
  <summary>Details</summary>
Motivation: To correct misconceptions about KL divergence in best-of-$n$ sampling and explore its practical implications for alignment policies.

Method: Analyzes the KL divergence claim, proposes a new estimator, and derives bounds on win rate and KL divergence. Empirical validation is provided.

Result: The common KL divergence claim is an upper bound; a new tight estimator is proposed. Win rate is bounded by $n/(n+1)$, and practical tradeoffs are achievable with $n < 1000$.

Conclusion: Best-of-$n$ sampling offers effective alignment tradeoffs, with corrected theoretical insights and practical bounds.

Abstract: A simple and effective method for the inference-time alignment and scaling
test-time compute of generative models is best-of-$n$ sampling, where $n$
samples are drawn from a reference policy, ranked based on a reward function,
and the highest ranking one is selected. A commonly used analytical expression
in the literature claims that the KL divergence between the best-of-$n$ policy
and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the
validity of this claim, and show that it is an upper bound on the actual KL
divergence. We also explore the tightness of this upper bound in different
regimes, and propose a new estimator for the KL divergence and empirically show
that it provides a tight approximation. We also show that the win rate of the
best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$
and derive bounds on the tightness of this characterization. We conclude with
analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$
alignment policy, which demonstrate that very good tradeoffs are achievable
with $n < 1000$.

</details>


### [716] [Adaptive Exploration for Multi-Reward Multi-Policy Evaluation](https://arxiv.org/pdf/2502.02516)
*Alessio Russo, Aldo Pacchiano*

Main category: cs.LG

TL;DR: The paper addresses the online multi-reward multi-policy evaluation problem, proposing an efficient exploration scheme to achieve accurate estimates with high confidence.


<details>
  <summary>Details</summary>
Motivation: To simultaneously evaluate multiple reward functions for different policies in a discounted setting, a problem not previously explored in the literature.

Method: Adapts the MR-NaS exploration scheme to minimize sample complexity, leveraging an instance-specific lower bound and proposing a convex approximation for efficiency.

Result: Demonstrates effectiveness in tabular domains through experiments.

Conclusion: The proposed adaptive exploration scheme efficiently evaluates policies across diverse reward sets.

Abstract: We study the policy evaluation problem in an online multi-reward multi-policy
discounted setting, where multiple reward functions must be evaluated
simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC
perspective to achieve $\epsilon$-accurate estimates with high confidence
across finite or convex sets of rewards, a setting that has not been
investigated in the literature. Building on prior work on Multi-Reward Best
Policy Identification, we adapt the MR-NaS exploration scheme to jointly
minimize sample complexity for evaluating different policies across different
reward sets. Our approach leverages an instance-specific lower bound revealing
how the sample complexity scales with a measure of value deviation, guiding the
design of an efficient exploration policy. Although computing this bound
entails a hard non-convex optimization, we propose an efficient convex
approximation that holds for both finite and convex reward sets. Experiments in
tabular domains demonstrate the effectiveness of this adaptive exploration
scheme.

</details>


### [717] [NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks](https://arxiv.org/pdf/2401.13330)
*Matteo Gambella, Jary Pomponi, Simone Scardapane, Manuel Roveri*

Main category: cs.LG

TL;DR: NACHOS is a NAS framework for designing optimal Early Exit Neural Networks (EENNs) under hardware constraints, jointly optimizing backbone and EECs for accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual design of EENNs is complex and time-consuming, lacking comprehensive automated solutions. NACHOS aims to automate this process while meeting accuracy and computational constraints.

Method: NACHOS uses Neural Architecture Search (NAS) to jointly design backbone and Early Exit Classifiers (EECs), optimizing for Pareto optimal solutions in accuracy and MAC operations. Novel regularization terms are introduced for EEC optimization.

Result: NACHOS-designed models compete with state-of-the-art EENNs, achieving effective tradeoffs between accuracy and computational efficiency.

Conclusion: NACHOS successfully automates EENN design, offering competitive performance and addressing the open problem of joint backbone-EEC optimization.

Abstract: Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN

</details>


### [718] [Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/pdf/2502.04778)
*Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Chenjun Xiao, Yang Yu, Zongzhang Zhang*

Main category: cs.LG

TL;DR: BDPO introduces a behavior-regularized RL framework for diffusion-based policies, combining expressive power and robustness by analytically calculating KL regularization along diffusion trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing behavior-regularized RL methods focus on explicit policy parameterizations, leaving a gap for advanced ones like diffusion models.

Method: BDPO calculates KL regularization analytically via discrepancies in reverse-time transition kernels and integrates it into a two-time-scale actor-critic algorithm.

Result: Evaluations on synthetic 2D tasks and D4RL benchmark show BDPO's effectiveness and superior performance.

Conclusion: BDPO successfully extends behavior regularization to diffusion policies, offering a robust and expressive RL solution.

Abstract: Behavior regularization, which constrains the policy to stay close to some
behavior policy, is widely used in offline reinforcement learning (RL) to
manage the risk of hazardous exploitation of unseen actions. Nevertheless,
existing literature on behavior-regularized RL primarily focuses on explicit
policy parameterizations, such as Gaussian policies. Consequently, it remains
unclear how to extend this framework to more advanced policy parameterizations,
such as diffusion models. In this paper, we introduce BDPO, a principled
behavior-regularized RL framework tailored for diffusion-based policies,
thereby combining the expressive power of diffusion policies and the robustness
provided by regularization. The key ingredient of our method is to calculate
the Kullback-Leibler (KL) regularization analytically as the accumulated
discrepancies in reverse-time transition kernels along the diffusion
trajectory. By integrating the regularization, we develop an efficient
two-time-scale actor-critic RL algorithm that produces the optimal policy while
respecting the behavior constraint. Comprehensive evaluations conducted on
synthetic 2D tasks and continuous control tasks from the D4RL benchmark
validate its effectiveness and superior performance.

</details>


### [719] [Learning to Poison Large Language Models for Downstream Manipulation](https://arxiv.org/pdf/2402.13459)
*Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Mohammad Amin Roshani, Prashant Khanduri, Douglas Zytko, Dongxiao Zhu*

Main category: cs.LG

TL;DR: The paper identifies security risks in LLMs during supervised fine-tuning (SFT), proposes a gradient-guided backdoor trigger learning (GBTL) attack, and introduces defense strategies like in-context learning (ICL) and continuous learning (CL).


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to data poisoning attacks during SFT, which can manipulate outputs. This work aims to uncover these risks and propose solutions.

Method: The authors design a GBTL algorithm to create adversarial triggers and test it on tasks like sentiment analysis and question answering. They also propose ICL and CL as defenses.

Result: The poisoning attack successfully compromises LLM outputs, while the proposed defenses (ICL and CL) mitigate performance decline.

Conclusion: The study emphasizes the security risks in SFT for LLMs and the need for robust defenses against data poisoning.

Abstract: The advent of Large Language Models (LLMs) has marked significant
achievements in language processing and reasoning capabilities. Despite their
advancements, LLMs face vulnerabilities to data poisoning attacks, where the
adversary inserts backdoor triggers into training data to manipulate outputs.
This work further identifies additional security risks in LLMs by designing a
new data poisoning attack tailored to exploit the supervised fine-tuning (SFT)
process. We propose a novel gradient-guided backdoor trigger learning (GBTL)
algorithm to identify adversarial triggers efficiently, ensuring an evasion of
detection by conventional defenses while maintaining content integrity. Through
experimental validation across various language model tasks, including
sentiment analysis, domain generation, and question answering, our poisoning
strategy demonstrates a high success rate in compromising various LLMs'
outputs. We further propose two defense strategies against data poisoning
attacks, including in-context learning (ICL) and continuous learning (CL),
which effectively rectify the behavior of LLMs and significantly reduce the
decline in performance. Our work highlights the significant security risks
present during SFT of LLMs and the necessity of safeguarding LLMs against data
poisoning attacks.

</details>


### [720] [Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates](https://arxiv.org/pdf/2402.17390)
*Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli*

Main category: cs.LG

TL;DR: The paper addresses negative flips in model updates, where new models make mistakes the old ones didn't, including in adversarial robustness. It proposes robustness-congruent adversarial training to mitigate this.


<details>
  <summary>Details</summary>
Motivation: To prevent performance regression (negative flips) in model updates, especially in adversarial robustness, which can undermine system security.

Method: Proposes robustness-congruent adversarial training, fine-tuning models to retain robustness on previously secure samples while improving overall performance.

Result: Experiments show the method mitigates negative flips in accuracy and robustness, outperforming baselines.

Conclusion: Robustness-congruent adversarial training provides a practical solution to maintain consistency in model updates, supported by theoretical and empirical evidence.

Abstract: Machine-learning models demand periodic updates to improve their average
accuracy, exploiting novel architectures and additional data. However, a newly
updated model may commit mistakes the previous model did not make. Such
misclassifications are referred to as negative flips, experienced by users as a
regression of performance. In this work, we show that this problem also affects
robustness to adversarial examples, hindering the development of secure model
update practices. In particular, when updating a model to improve its
adversarial robustness, previously ineffective adversarial attacks on some
inputs may become successful, causing a regression in the perceived security of
the system. We propose a novel technique, named robustness-congruent
adversarial training, to address this issue. It amounts to fine-tuning a model
with adversarial training, while constraining it to retain higher robustness on
the samples for which no adversarial example was found before the update. We
show that our algorithm and, more generally, learning with non-regression
constraints, provides a theoretically-grounded framework to train consistent
estimators. Our experiments on robust models for computer vision confirm that
both accuracy and robustness, even if improved after model update, can be
affected by negative flips, and our robustness-congruent adversarial training
can mitigate the problem, outperforming competing baseline methods.

</details>


### [721] [Enhancing Semi-supervised Learning with Zero-shot Pseudolabels](https://arxiv.org/pdf/2502.12584)
*Jichan Chung, Irene Y. Chen*

Main category: cs.LG

TL;DR: ZeroMatch is a novel SSL framework combining knowledge distillation and consistency-based learning to leverage labeled/unlabeled data and FM pseudo-labels, outperforming standard SSL and zero-shot methods.


<details>
  <summary>Details</summary>
Motivation: High labeling costs and impracticality of fine-tuning large FMs in resource-constrained settings drive the need for efficient SSL solutions.

Method: ZeroMatch integrates knowledge distillation with consistency-based learning to use labeled data, unlabeled data, and FM pseudo-labels for training compact models.

Result: ZeroMatch outperforms standard SSL and zero-shot methods on six vision and language benchmarks, showing robustness across FM qualities.

Conclusion: ZeroMatch is effective for low-resource environments, enabling efficient model training with minimal FM inference.

Abstract: The high cost of data labeling presents a major barrier to deploying machine
learning systems at scale. Semi-supervised learning (SSL) mitigates this
challenge by utilizing unlabeled data alongside limited labeled examples, while
the emergence of foundation models (FMs) offers powerful zero-shot capabilities
that can further reduce labeling cost. However, directly fine-tuning large FMs
is often impractical in resource-constrained settings, and na\"ively using
their pseudo-labels for unlabeled data can degrade performance due to its
unreliablity or domain mismatch with target task. In this work, we introduce
ZeroMatch, a novel SSL framework that integrates knowledge distillation with
consistency-based learning to jointly leverage labeled data, unlabeled data,
and pseudo-labels from FMs. ZeroMatch enables training compact student models
using only FM inference, making it suitable for low-resource environments such
as personal devices with limited compute. Experiments on six vision and
language classification benchmarks show that ZeroMatch consistently outperforms
standard SSL and zero-shot augmented methods, demonstrating its effectiveness
and robustness across a range of foundation model qualities.

</details>


### [722] [Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts](https://arxiv.org/pdf/2404.05019)
*Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang*

Main category: cs.LG

TL;DR: ScMoE introduces a shortcut-connected MoE architecture with overlapping parallelization to decouple communication from computation, achieving significant speedups in training and inference while maintaining model quality.


<details>
  <summary>Details</summary>
Motivation: The All-to-All communication bottleneck in expert parallelism limits the efficiency of MoE models, despite existing optimizations.

Method: Proposes ScMoE, a shortcut-connected MoE architecture with an overlapping parallelization strategy to decouple communication from computation.

Result: Achieves speedups of 1.49x in training and 1.82x in inference compared to top-2 MoE baseline, with comparable or better model quality.

Conclusion: ScMoE effectively addresses the communication bottleneck, improving efficiency and performance of MoE models.

Abstract: Expert parallelism has emerged as a key strategy for distributing the
computational workload of sparsely-gated mixture-of-experts (MoE) models across
multiple devices, enabling the processing of increasingly large-scale models.
However, the All-to-All communication inherent to expert parallelism poses a
significant bottleneck, limiting the efficiency of MoE models. Although
existing optimization methods partially mitigate this issue, they remain
constrained by the sequential dependency between communication and computation
operations. To address this challenge, we propose ScMoE, a novel
shortcut-connected MoE architecture integrated with an overlapping
parallelization strategy. ScMoE decouples communication from its conventional
sequential ordering, enabling up to 100% overlap with computation. Compared to
the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in
training and 1.82 times in inference. Moreover, our experiments and analyses
indicate that ScMoE not only achieves comparable but in some instances
surpasses the model quality of existing approaches.

</details>


### [723] [GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning](https://arxiv.org/pdf/2502.12913)
*Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang*

Main category: cs.LG

TL;DR: GSQ-Tuning introduces a fully integer-based fine-tuning framework for LLMs, reducing memory and power usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM fine-tuning is resource-intensive and incompatible with edge devices due to floating-point operations.

Method: Uses Group-Shared Exponents Integer format and LoRA-like adapters for integer-based fine-tuning.

Result: Achieves BF16-level accuracy with 1.85x less memory, 5x less power, and 11x smaller chip area than FP8.

Conclusion: GSQ-Tuning enables efficient, privacy-preserving LLM fine-tuning on edge devices.

Abstract: Large Language Models (LLMs) fine-tuning technologies have achieved
remarkable results. However, traditional LLM fine-tuning approaches face
significant challenges: they require large Floating Point (FP) computation,
raising privacy concerns when handling sensitive data, and are impractical for
resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)
techniques reduce trainable parameters, their reliance on floating-point
arithmetic creates fundamental incompatibilities with edge hardware. In this
work, we introduce a novel framework for on-device LLM fine-tuning that
eliminates the need for floating-point operations in both inference and
training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer
format, which efficiently represents model parameters in integer format using
shared exponents among parameter groups. When combined with LoRA-like adapters,
this enables fully integer-based fine-tuning that is both memory and compute
efficient. We demonstrate that our approach achieves accuracy comparable to
BF16-based fine-tuning while significantly reducing 1.85x memory usage.
Moreover, compared to FP8, our method can reduce 5x power consumption and 11x
chip area with same performance, making large-scale model adaptation feasible
on edge devices.

</details>


### [724] [Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition](https://arxiv.org/pdf/2404.08008)
*Kehua Feng, Keyan Ding, Hongzhi Tan, Kede Ma, Zhihua Wang, Shuangquan Guo, Yuzhou Cheng, Ge Sun, Guozhou Zheng, Qiang Zhang, Huajun Chen*

Main category: cs.LG

TL;DR: A sample-efficient human evaluation method for LLMs, called MAD Competition, selects input instructions to maximize discrepancy between model responses, enabling reliable rankings with minimal human effort.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation lacks alignment with human perception and is costly due to exhaustive labeling.

Method: Uses MAD Competition to select input instructions for maximum discrepancy, followed by human three-alternative forced choices, aggregated via Elo rating.

Result: Recovers gold-standard rankings with few instructions, reveals LLM strengths/weaknesses, and provides nuanced insights.

Conclusion: MAD-Eval offers an efficient, insightful approach for LLM evaluation, guiding future development.

Abstract: Reliable evaluation of large language models (LLMs) is impeded by two key
challenges: objective metrics often fail to reflect human perception of natural
language, and exhaustive human labeling is prohibitively expensive. Here, we
propose a sample-efficient human evaluation method for LLMs based on the
principle of MAximum Discrepancy (MAD) Competition. Our method automatically
and adaptively selects a compact set of input instructions that maximize
semantic discrepancy between pairs of LLM responses. Human evaluators then
perform three-alternative forced choices on these paired responses, which are
aggregated into a global ranking using Elo rating. We apply our approach to
compare eight widely used LLMs across four tasks: scientific knowledge
understanding, mathematical reasoning, creative and functional writing, and
code generation and explanation. Experimental results show that our
sample-efficient evaluation method recovers "gold-standard" model rankings with
a handful of MAD-selected instructions, reveals respective strengths and
weaknesses of each LLM, and offers nuanced insights to guide future LLM
development. Code is available at https://github.com/weiji-Feng/MAD-Eval .

</details>


### [725] [STeCa: Step-level Trajectory Calibration for LLM Agent Learning](https://arxiv.org/pdf/2502.14276)
*Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li*

Main category: cs.LG

TL;DR: STeCa is a framework for LLM agent learning that improves performance by identifying and correcting suboptimal actions through step-level calibration and reinforced training.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-horizon tasks due to accumulated suboptimal actions, highlighting the need for timely calibration.

Method: STeCa identifies suboptimal actions via step-level reward comparison, constructs calibrated trajectories using LLM-driven reflection, and uses them for reinforced training.

Result: STeCa outperforms existing methods, enabling more robust task completion.

Conclusion: Timely calibration via STeCa enhances LLM agent performance in complex tasks.

Abstract: Large language model (LLM)-based agents have shown promise in tackling
complex tasks by interacting dynamically with the environment. Existing work
primarily focuses on behavior cloning from expert demonstrations or preference
learning through exploratory trajectory sampling. However, these methods often
struggle to address long-horizon tasks, where suboptimal actions accumulate
step by step, causing agents to deviate from correct task trajectories. To
address this, we highlight the importance of timely calibration and the need to
automatically construct calibration trajectories for training agents. We
propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM
agent learning. Specifically, STeCa identifies suboptimal actions through a
step-level reward comparison during exploration. It constructs calibrated
trajectories using LLM-driven reflection, enabling agents to learn from
improved decision-making processes. We finally leverage these calibrated
trajectories with successful trajectories for reinforced training. Extensive
experiments demonstrate that STeCa significantly outperforms existing methods.
Further analysis highlights that timely calibration enables agents to complete
tasks with greater robustness. Our code and data are available at
https://github.com/WangHanLinHenry/STeCa.

</details>


### [726] [Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model](https://arxiv.org/pdf/2405.14457)
*Tudor Cebere, Aurélien Bellet, Nicolas Papernot*

Main category: cs.LG

TL;DR: The paper investigates the gap between empirical and theoretical privacy bounds in differentially private machine learning, proposing a new auditing method for the hidden state threat model.


<details>
  <summary>Details</summary>
Motivation: The study addresses the discrepancy between lower bounds from empirical privacy auditing and theoretical upper bounds in privacy accounting for the hidden state threat model.

Method: The authors propose auditing with adversaries crafting gradient sequences to maximize privacy loss without relying on intermediate updates. Experiments compare this approach to previous methods.

Result: The new auditing method outperforms prior attempts. It reveals that hiding intermediate updates in DP-SGD doesn't enhance privacy if crafted gradients are inserted at every step. For non-continuous insertion, bounds match only under specific conditions.

Conclusion: The findings suggest that existing privacy upper bounds can be improved in certain scenarios, advancing understanding of privacy guarantees in the hidden state threat model.

Abstract: Machine learning models can be trained with formal privacy guarantees via
differentially private optimizers such as DP-SGD. In this work, we focus on a
threat model where the adversary has access only to the final model, with no
visibility into intermediate updates. In the literature, this hidden state
threat model exhibits a significant gap between the lower bound from empirical
privacy auditing and the theoretical upper bound provided by privacy
accounting. To challenge this gap, we propose to audit this threat model with
adversaries that craft a gradient sequence designed to maximize the privacy
loss of the final model without relying on intermediate updates. Our
experiments show that this approach consistently outperforms previous attempts
at auditing the hidden state model. Furthermore, our results advance the
understanding of achievable privacy guarantees within this threat model.
Specifically, when the crafted gradient is inserted at every optimization step,
we show that concealing the intermediate model updates in DP-SGD does not
enhance the privacy guarantees. The situation is more complex when the crafted
gradient is not inserted at every step: our auditing lower bound matches the
privacy upper bound only for an adversarially-chosen loss landscape and a
sufficiently large batch size. This suggests that existing privacy upper bounds
can be improved in certain regimes.

</details>


### [727] [Learning to Reason from Feedback at Test-Time](https://arxiv.org/pdf/2502.15771)
*Yanyang Li, Michael Lyu, Liwei Wang*

Main category: cs.LG

TL;DR: FTTT and OpTune improve LLM performance by optimizing feedback utilization at test time, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for feedback utilization in LLMs either generalize poorly or inefficiently retry without leveraging prior information.

Method: FTTT formulates feedback as an optimization problem, and OpTune, a learnable optimizer, exploits feedback effectively.

Result: Experiments on four reasoning datasets show FTTT and OpTune achieve better scalability and performance.

Conclusion: FTTT and OpTune offer a scalable and effective solution for feedback utilization in LLMs.

Abstract: Solving complex tasks in a single attempt is challenging for large language
models (LLMs). Iterative interaction with the environment and feedback is often
required to achieve success, making effective feedback utilization a critical
topic. Existing approaches either struggle with length generalization or rely
on naive retries without leveraging prior information. In this paper, we
introduce FTTT, a novel paradigm that formulates feedback utilization as an
optimization problem at test time. Additionally, we propose a learnable
test-time optimizer, OpTune, to effectively exploit feedback. Experiments on
two LLMs across four reasoning datasets demonstrate that FTTT and OpTune
achieve superior scalability and performance.

</details>


### [728] [Closed-form Solutions: A New Perspective on Solving Differential Equations](https://arxiv.org/pdf/2405.14620)
*Shu Wei, Yanjie Li, Lina Yu, Weijun Li, Min Wu, Linjun Sun, Jufeng Han, Yan Pang*

Main category: cs.LG

TL;DR: SSDE, a reinforcement learning-based method, outperforms existing machine learning techniques in deriving symbolic solutions for differential equations with better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require extensive mathematical expertise, and existing machine learning approaches like genetic algorithms are computationally intensive and yield complex solutions.

Method: SSDE uses reinforcement learning to derive symbolic closed-form solutions for differential equations.

Result: SSDE achieves superior accuracy and efficiency compared to other machine learning methods across various ordinary and partial differential equations.

Conclusion: SSDE is a promising tool for solving differential equations symbolically, addressing limitations of traditional and existing machine learning methods.

Abstract: The quest for analytical solutions to differential equations has
traditionally been constrained by the need for extensive mathematical
expertise. Machine learning methods like genetic algorithms have shown promise
in this domain, but are hindered by significant computational time and the
complexity of their derived solutions. This paper introduces SSDE (Symbolic
Solver for Differential Equations), a novel reinforcement learning-based
approach that derives symbolic closed-form solutions for various differential
equations. Evaluations across a diverse set of ordinary and partial
differential equations demonstrate that SSDE outperforms existing machine
learning methods, delivering superior accuracy and efficiency in obtaining
analytical solutions.

</details>


### [729] [Privacy-Aware Joint DNN Model Deployment and Partitioning Optimization for Collaborative Edge Inference Services](https://arxiv.org/pdf/2502.16091)
*Zhipeng Cheng, Xiaoyu Xia, Hong Wang, Minghui Liwang, Ning Chen, Xuwei Fan, Xianbin Wang*

Main category: cs.LG

TL;DR: A privacy-aware optimization framework for edge-based DNN inference reduces latency and ensures privacy by jointly optimizing model deployment, user-server association, and partitioning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like high latency, scalability, and privacy in cloud-based DNN inference by deploying models on resource-constrained edge devices.

Method: Uses a Lyapunov-based approach for stochastic optimization, a coalition formation game for user-server association, and a greedy algorithm for model deployment.

Result: Significantly reduces inference delay while meeting privacy constraints, outperforming existing methods.

Conclusion: The framework effectively balances resource constraints and privacy, offering a scalable solution for edge-based DNN inference.

Abstract: Edge inference (EI) has emerged as a promising paradigm to address the
growing limitations of cloud-based Deep Neural Network (DNN) inference
services, such as high response latency, limited scalability, and severe data
privacy exposure. However, deploying DNN models on resource-constrained edge
devices introduces additional challenges, including limited computation/storage
resources, dynamic service demands, and heightened privacy risks. To tackle
these issues, this paper presents a novel privacy-aware optimization framework
that jointly addresses DNN model deployment, user-server association, and model
partitioning, with the goal of minimizing long-term average inference delay
under resource and privacy constraints. The problem is formulated as a complex,
NP-hard stochastic optimization. To efficiently handle system dynamics and
computational complexity, we employ a Lyapunov-based approach to transform the
long-term objective into tractable per-slot decisions. Furthermore, we
introduce a coalition formation game to enable adaptive user-server association
and design a greedy algorithm for model deployment within each coalition.
Extensive simulations demonstrate that the proposed algorithm significantly
reduces inference delay and consistently satisfies privacy constraints,
outperforming state-of-the-art baselines across diverse scenarios.

</details>


### [730] [Proper Dataset Valuation by Pointwise Mutual Information](https://arxiv.org/pdf/2405.18253)
*Shuran Zheng, Xuan Qi, Rui Ray Chen, Yongchan Kwon, James Zou*

Main category: cs.LG

TL;DR: The paper proposes an information-theoretic framework to evaluate data curation methods, addressing the limitations of heuristic-based approaches and Goodhart's law.


<details>
  <summary>Details</summary>
Motivation: Existing data curation methods rely on heuristics and may not truly improve model performance, as they can overfit to test sets.

Method: An information-theoretic framework is introduced, using the Blackwell ordering of informativeness and Shannon mutual information between curated and test data. A Bayesian model-based method estimates this mutual information.

Result: Experiments show the proposed method correctly identifies data curation strategies that reduce informativeness, unlike traditional test score-based evaluations.

Conclusion: The framework provides a principled way to evaluate data curation, ensuring data quality aligns with true model performance.

Abstract: Data plays a central role in advancements in modern artificial intelligence,
with high-quality data emerging as a key driver of model performance. This has
prompted the development of principled and effective data curation methods in
recent years. However, existing methods largely rely on heuristics, and whether
they are truly effective remains unclear. For instance, standard evaluation
methods that assess a trained model's performance on specific benchmarks may
incentivize assigning high scores to data that merely resembles the test set.
This issue exemplifies Goodhart's law: when a measure becomes a target, it
ceases to be a good measure. To address this issue, we propose an
information-theoretic framework for evaluating data curation methods. We define
dataset quality in terms of its informativeness about the true model
parameters, formalized using the Blackwell ordering of informativeness. Under
this ordering, Blackwell's theorem ensures that more informative data yields
optimal models with lower expected loss on the true underlying distribution. To
measure informativeness, we show that the Blackwell order can be determined by
the Shannon mutual information between the curated data and the test data. To
estimate this mutual information, we introduce a novel method that trains
Bayesian models on embedded datasets and computes mutual information from the
posteriors of model parameters. Experiments on real-world data demonstrate that
our mutual information-based evaluation assigns appropriately lower scores to
data curation strategies that reduce dataset informativeness, while traditional
test score-based evaluation methods may favor data curation strategies that
overfit to the test set but compromise the training data's informativeness.

</details>


### [731] [GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning](https://arxiv.org/pdf/2406.09187)
*Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, Dawn Song, Bo Li*

Main category: cs.LG

TL;DR: GuardAgent is a guardrail agent for LLMs, dynamically enforcing safety checks via task plans and guardrail code, achieving high accuracy on novel benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing safety and security concerns in LLM agents by ensuring their actions comply with predefined safety guard requests.

Method: GuardAgent uses an LLM to generate task plans and guardrail code, supplemented by in-context demonstrations from a memory module.

Result: Achieves 98% and 83% guardrail accuracies on EICU-AC and Mind2Web-SC benchmarks, respectively.

Conclusion: GuardAgent effectively safeguards LLM agents by dynamically enforcing safety policies.

Abstract: The rapid advancement of large language model (LLM) agents has raised new
concerns regarding their safety and security. In this paper, we propose
GuardAgent, the first guardrail agent to protect target agents by dynamically
checking whether their actions satisfy given safety guard requests.
Specifically, GuardAgent first analyzes the safety guard requests to generate a
task plan, and then maps this plan into guardrail code for execution. By
performing the code execution, GuardAgent can deterministically follow the
safety guard request and safeguard target agents. In both steps, an LLM is
utilized as the reasoning component, supplemented by in-context demonstrations
retrieved from a memory module storing experiences from previous tasks. In
addition, we propose two novel benchmarks: EICU-AC benchmark to assess the
access control for healthcare agents and Mind2Web-SC benchmark to evaluate the
safety policies for web agents. We show that GuardAgent effectively moderates
the violation actions for different types of agents on these two benchmarks
with over 98% and 83% guardrail accuracies, respectively. Project page:
https://guardagent.github.io/

</details>


### [732] [Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning](https://arxiv.org/pdf/2408.12307)
*Yen-Ru Lai, Fu-Chieh Chang, Pei-Yuan Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Offline reinforcement learning (RL) learns policies from a fixed dataset, but
often requires large amounts of data. The challenge arises when labeled
datasets are expensive, especially when rewards have to be provided by human
labelers for large datasets. In contrast, unlabelled data tends to be less
expensive. This situation highlights the importance of finding effective ways
to use unlabelled data in offline RL, especially when labelled data is limited
or expensive to obtain. In this paper, we present the algorithm to utilize the
unlabeled data in the offline RL method with kernel function approximation and
give the theoretical guarantee. We present various eigenvalue decay conditions
of $\mathcal{H}_k$ which determine the complexity of the algorithm. In summary,
our work provides a promising approach for exploiting the advantages offered by
unlabeled data in offline RL, whilst maintaining theoretical assurances.

</details>


### [733] [BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction](https://arxiv.org/pdf/2502.18807)
*Ruifeng Tan, Weixiang Hong, Jiayue Tang, Xibin Lu, Ruijun Ma, Xiang Zheng, Jia Li, Jiaqiang Huang, Tong-Yi Zhang*

Main category: cs.LG

TL;DR: The paper introduces BatteryLife, a comprehensive dataset and benchmark for Battery Life Prediction (BLP), addressing challenges like limited dataset size, lack of diversity, and inconsistent benchmarks. It also proposes CyclePatch, a plug-in technique improving model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing key challenges in BLP research: limited dataset size, lack of diversity in battery types and testing conditions, and inconsistent benchmarks.

Method: Proposes BatteryLife, integrating 16 datasets with diverse battery types and conditions, and introduces CyclePatch, a plug-in technique for neural networks.

Result: BatteryLife offers 2.5 times more samples than previous datasets and includes diverse battery types. CyclePatch improves model performance, establishing state-of-the-art benchmarks.

Conclusion: BatteryLife and CyclePatch address critical gaps in BLP research, providing a robust dataset and enhancing model performance across diverse conditions.

Abstract: Battery Life Prediction (BLP), which relies on time series data produced by
battery degradation tests, is crucial for battery utilization, optimization,
and production. Despite impressive advancements, this research area faces three
key challenges. Firstly, the limited size of existing datasets impedes insights
into modern battery life data. Secondly, most datasets are restricted to
small-capacity lithium-ion batteries tested under a narrow range of diversity
in labs, raising concerns about the generalizability of findings. Thirdly,
inconsistent and limited benchmarks across studies obscure the effectiveness of
baselines and leave it unclear if models popular in other time series fields
are effective for BLP. To address these challenges, we propose BatteryLife, a
comprehensive dataset and benchmark for BLP. BatteryLife integrates 16
datasets, offering a 2.5 times sample size compared to the previous largest
dataset, and provides the most diverse battery life resource with batteries
from 8 formats, 59 chemical systems, 9 operating temperatures, and 421
charge/discharge protocols, including both laboratory and industrial tests.
Notably, BatteryLife is the first to release battery life datasets of zinc-ion
batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion
batteries. With the comprehensive dataset, we revisit the effectiveness of
baselines popular in this and other time series fields. Furthermore, we propose
CyclePatch, a plug-in technique that can be employed in various neural
networks. Extensive benchmarking of 18 methods reveals that models popular in
other time series fields can be unsuitable for BLP, and CyclePatch consistently
improves model performance establishing state-of-the-art benchmarks. Moreover,
BatteryLife evaluates model performance across aging conditions and domains.
BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.

</details>


### [734] [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://arxiv.org/pdf/2408.14774)
*Simran Kaur, Simon Park, Anirudh Goyal, Sanjeev Arora*

Main category: cs.LG

TL;DR: Instruct-SkillMix automates diverse SFT data creation for instruction-following using LLMs, achieving strong benchmark results at low cost.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of creating high-quality, diverse instruction-following datasets efficiently.

Method: Two-stage pipeline: (1) skill extraction via LLM prompting, (2) data generation with random skill combinations.

Result: Vanilla SFT on Instruct-SkillMix data improves benchmarks (e.g., AlpacaEval 2.0) with just 4K examples, rivaling frontier models.

Conclusion: Instruct-SkillMix is cost-effective, flexible, and highlights pitfalls of naive crowd-sourcing for dataset creation.

Abstract: We introduce Instruct-SkillMix, an automated approach for creating diverse,
high quality SFT data for instruction-following. The pipeline involves two
stages, each leveraging an existing powerful LLM: (1) Skill extraction: uses
the LLM to extract core "skills" for instruction-following by directly
prompting the model. This is inspired by ``LLM metacognition'' of Didolkar et
al. (2024); (2) Data generation: uses the powerful LLM to generate
(instruction, response) data that exhibit a randomly chosen pair of these
skills. Here, the use of random skill combinations promotes diversity and
difficulty. The estimated cost of creating the dataset is under $600.
  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from
Instruct-SkillMix leads to strong gains on instruction following benchmarks
such as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples,
LLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0, a
level similar to frontier models like Claude 3 Opus and
LLaMA-3.1-405B-Instruct. Ablation studies also suggest plausible reasons for
why creating open instruction-tuning datasets via naive crowd-sourcing has
proved difficult. In our dataset, adding 20% low quality answers (``shirkers'')
causes a noticeable degradation in performance. The Instruct-SkillMix pipeline
seems flexible and adaptable to other settings.

</details>


### [735] [Your Data, My Model: Learning Who Really Helps in Federated Learning](https://arxiv.org/pdf/2409.02064)
*Shamsiiat Abdurakhmanova, Amirhossein Mohammadi, Yasmin SarcheshmehPour, Alexander Jung*

Main category: cs.LG

TL;DR: A privacy-preserving method for selecting beneficial collaborators in federated learning by evaluating model improvement after a single gradient step.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying the most beneficial peers for collaboration in personalized federated learning without sharing raw data.

Method: Evaluates model improvement after a single gradient step using another device's data; extends to non-parametric models by replacing the gradient step with a non-parametric generalization.

Result: Enables model-agnostic, data-driven peer selection for personalized federated learning (PersFL).

Conclusion: The proposed method is simple, privacy-preserving, and effective for selecting relevant collaborators in federated learning.

Abstract: Many important machine learning applications involve networks of devices-such
as wearables or smartphones-that generate local data and train personalized
models. A key challenge is determining which peers are most beneficial for
collaboration. We propose a simple and privacy-preserving method to select
relevant collaborators by evaluating how much a model improves after a single
gradient step using another devices data-without sharing raw data. This method
naturally extends to non-parametric models by replacing the gradient step with
a non-parametric generalization. Our approach enables model-agnostic,
data-driven peer selection for personalized federated learning (PersFL).

</details>


### [736] [LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty](https://arxiv.org/pdf/2503.18314)
*Christoforos N. Spartalis, Theodoros Semertzidis, Efstratios Gavves, Petros Daras*

Main category: cs.LG

TL;DR: LoTUS is a Machine Unlearning method that removes training sample influence without retraining, outperforming baselines in efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To avoid costly retraining and mitigate over-confidence from data memorization in pre-trained models.

Method: LoTUS smooths prediction probabilities up to an information-theoretic bound and introduces RF-JSD for evaluation.

Result: Outperforms eight baselines on five datasets, including large-scale ImageNet1k, under real-world conditions.

Conclusion: LoTUS is efficient and effective for Machine Unlearning, validated by novel metrics and large-scale experiments.

Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the
influence of training samples from pre-trained models, avoiding retraining from
scratch. LoTUS smooths the prediction probabilities of the model up to an
information-theoretic bound, mitigating its over-confidence stemming from data
memorization. We evaluate LoTUS on Transformer and ResNet18 models against
eight baselines across five public datasets. Beyond established MU benchmarks,
we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining
is impractical, simulating real-world conditions. Moreover, we introduce the
novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable
evaluation under real-world conditions. The experimental results show that
LoTUS outperforms state-of-the-art methods in terms of both efficiency and
effectiveness. Code: https://github.com/cspartalis/LoTUS.

</details>


### [737] [On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists](https://arxiv.org/pdf/2409.13931)
*Dongyang Fan, Bettina Messmer, Nikita Doikov, Martin Jaggi*

Main category: cs.LG

TL;DR: CoMiGS is a novel method for on-device LLMs using a mixture of generalists and specialists to address computational and data heterogeneity in Federated Learning, ensuring privacy and personalization.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy and provide personalized experiences in on-device LLMs while overcoming challenges like computational and data heterogeneity in Federated Learning.

Method: Bi-level optimization of Mixture-of-Experts, with alternating minimization, sharing generalists and localizing specialists.

Result: CoMiGS balances general and personalized knowledge, remains robust against overfitting, and adapts to local data.

Conclusion: CoMiGS effectively addresses key challenges in Federated Learning for LLMs, offering a scalable and privacy-preserving solution.

Abstract: On-device LLMs have gained increasing attention for their ability to enhance
privacy and provide a personalized user experience. To facilitate private
learning with scarce data, Federated Learning has become a standard approach.
However, it faces challenges such as computational resource heterogeneity and
data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative
learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and
$\textbf{S}$pecialists), the first approach to address both challenges. A key
innovation of our method is the bi-level optimization formulation of the
Mixture-of-Experts learning objective, where the router is optimized using a
separate validation set to ensure alignment with the target distribution. We
solve our objective with alternating minimization, for which we provide a
theoretical analysis. Our method shares generalist experts across users while
localizing a varying number of specialist experts, thereby adapting to users'
computational resources and preserving privacy. Through extensive experiments,
we show CoMiGS effectively balances general and personalized knowledge for each
token generation. We demonstrate that CoMiGS remains robust against
overfitting-due to the generalists' regularizing effect-while adapting to local
data through specialist expertise. We open source our codebase for
collaborative LLMs.

</details>


### [738] [Optimal Protocols for Continual Learning via Statistical Physics and Control Theory](https://arxiv.org/pdf/2409.18061)
*Francesco Mori, Stefano Sarao Mannelli, Francesca Mignacco*

Main category: cs.LG

TL;DR: The paper addresses catastrophic forgetting in neural networks by combining statistical physics and optimal control to derive optimal task-selection protocols, validated on real-world data.


<details>
  <summary>Details</summary>
Motivation: Artificial neural networks suffer from catastrophic forgetting when learning tasks sequentially, and existing protocols lack theoretical optimality.

Method: Combines exact training dynamics equations (from statistical physics) with optimal control methods, applied to teacher-student models for continual learning.

Result: Derives non-trivial, interpretable strategies for task-selection to maximize performance and minimize forgetting, including insights on task similarity.

Conclusion: The theoretical framework provides optimal protocols for mitigating catastrophic forgetting, validated experimentally.

Abstract: Artificial neural networks often struggle with catastrophic forgetting when
learning multiple tasks sequentially, as training on new tasks degrades the
performance on previously learned tasks. Recent theoretical work has addressed
this issue by analysing learning curves in synthetic frameworks under
predefined training protocols. However, these protocols relied on heuristics
and lacked a solid theoretical foundation assessing their optimality. In this
paper, we fill this gap by combining exact equations for training dynamics,
derived using statistical physics techniques, with optimal control methods. We
apply this approach to teacher-student models for continual learning and
multi-task problems, obtaining a theory for task-selection protocols maximising
performance while minimising forgetting. Our theoretical analysis offers
non-trivial yet interpretable strategies for mitigating catastrophic
forgetting, shedding light on how optimal learning protocols modulate
established effects, such as the influence of task similarity on forgetting.
Finally, we validate our theoretical findings with experiments on real-world
data.

</details>


### [739] [Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?](https://arxiv.org/pdf/2504.00186)
*Olawale Salaudeen, Nicole Chiou, Shiny Weng, Sanmi Koyejo*

Main category: cs.LG

TL;DR: Current OOD benchmarks may miss key spurious correlation shifts, leading to misleading evaluations of model robustness.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that models relying on spurious correlations fail OOD, and to expose flaws in current benchmarks.

Method: Analyze conditions for reliable assessment of spurious correlation reliance and evaluate existing benchmarks.

Result: Many benchmarks fail to include meaningful spurious correlation shifts, rendering them ineffective for robustness evaluation.

Conclusion: The field must rethink benchmark design to accurately assess robustness to spurious correlations under distribution shifts.

Abstract: Spurious correlations are unstable statistical associations that hinder
robust decision-making. Conventional wisdom suggests that models relying on
such correlations will fail to generalize out-of-distribution (OOD), especially
under strong distribution shifts. However, empirical evidence challenges this
view as naive in-distribution empirical risk minimizers often achieve the best
OOD accuracy across popular OOD generalization benchmarks. In light of these
results, we propose a different perspective: many widely used benchmarks for
evaluating robustness to spurious correlations are misspecified. Specifically,
they fail to include shifts in spurious correlations that meaningfully impact
OOD generalization, making them unsuitable for evaluating the benefit of
removing such correlations. We establish conditions under which a distribution
shift can reliably assess a model's reliance on spurious correlations.
Crucially, under these conditions, we should not observe a strong positive
correlation between in-distribution and OOD accuracy, often called "accuracy on
the line." Yet, most state-of-the-art benchmarks exhibit this pattern,
suggesting they do not effectively assess robustness. Our findings expose a key
limitation in current benchmarks used to evaluate domain generalization
algorithms, that is, models designed to avoid spurious correlations. We
highlight the need to rethink how robustness to spurious correlations is
assessed, identify well-specified benchmarks the field should prioritize, and
enumerate strategies for designing future benchmarks that meaningfully reflect
robustness under distribution shift.

</details>


### [740] [Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models](https://arxiv.org/pdf/2410.02681)
*Shuoyuan Wang, Yixuan Li, Hongxin Wei*

Main category: cs.LG

TL;DR: The paper addresses confidence calibration in vision-language models like CLIP after fine-tuning, introducing Dynamic Outlier Regularization (DOR) to balance calibration between base and new classes.


<details>
  <summary>Details</summary>
Motivation: Existing prompt tuning methods create a trade-off in calibration between base and new classes, leading to overconfidence or underconfidence.

Method: Proposes DOR to minimize feature deviation of novel textual labels from a large vocabulary, reducing textual divergence for new labels while easing restrictions on base classes.

Result: DOR improves calibration performance on both base and new classes in fine-tuning methods.

Conclusion: DOR effectively balances confidence calibration in vision-language models post fine-tuning.

Abstract: Confidence calibration is critical for the safe deployment of machine
learning models in the real world. However, such issue in vision-language
models like CLIP, particularly after fine-tuning, has not been fully addressed.
In this work, we demonstrate that existing prompt tuning methods usually lead
to a trade-off of calibration between base and new classes: the cross-entropy
loss in CoOp causes overconfidence in new classes by increasing textual label
divergence, whereas the regularization of KgCoOp maintains the confidence level
but results in underconfidence in base classes due to the improved accuracy.
Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR)
to ensure the confidence calibration on both base and new classes after
fine-tuning. In particular, we propose to minimize the feature deviation of
novel textual labels (instead of base classes) sampled from a large vocabulary.
In effect, DOR prevents the increase in textual divergence for new labels while
easing restrictions on base classes. Extensive experiments demonstrate that DOR
can enhance the calibration performance of current fine-tuning methods on base
and new classes.

</details>


### [741] [Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning](https://arxiv.org/pdf/2410.03348)
*Aaditya Naik, Jason Liu, Claire Wang, Amish Sethi, Saikat Dutta, Mayur Naik, Eric Wong*

Main category: cs.LG

TL;DR: DOLPHIN is a neurosymbolic framework that integrates symbolic reasoning with deep learning, achieving state-of-the-art efficiency and convergence on complex benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address scalability challenges in neurosymbolic learning for complex symbolic programs and large datasets.

Method: DOLPHIN supports neurosymbolic programs in Python, executing symbolic reasoning on the CPU and vectorizing probabilistic computations on the GPU.

Result: DOLPHIN outperforms existing frameworks on 13 benchmarks, achieving faster convergence and higher accuracy on complex tasks.

Conclusion: DOLPHIN advances neurosymbolic scalability, offering superior efficiency and performance where other frameworks fail.

Abstract: Neurosymbolic learning enables the integration of symbolic reasoning with
deep learning but faces significant challenges in scaling to complex symbolic
programs, large datasets, or both. We introduce DOLPHIN, a framework that
tackles these challenges by supporting neurosymbolic programs in Python,
executing complex symbolic reasoning on the CPU while vectorizing probabilistic
computations and gradient propagation on the GPU. Across 13 benchmarks spanning
tasks over text, image, and video data, with symbolic reasoning features like
recursion and black-box functions, DOLPHIN converges to state-of-the-art
accuracies on the more complex benchmarks while existing frameworks such as
Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On
simpler benchmarks, DOLPHIN matches their performance, while achieving these
results 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the
scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency
and convergence on difficult benchmarks where existing frameworks struggle. The
code is published at https://github.com/Dolphin-NeSy/Dolphin.

</details>


### [742] [Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism](https://arxiv.org/pdf/2504.11558)
*Mete Erdogan, Cengiz Pehlevan, Alper T. Erdogan*

Main category: cs.LG

TL;DR: EBD is a novel neural network framework that broadcasts output errors to layers, avoiding backpropagation's weight transport. It uses stochastic orthogonality for layerwise loss functions, showing competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address credit assignment in neural networks without relying on backpropagation's weight transport, leveraging stochastic orthogonality for a principled approach.

Method: Defines layerwise loss functions penalizing correlations between layer activations and output errors, grounded in stochastic orthogonality.

Result: Competitive or better performance compared to other error-broadcast methods on benchmark datasets.

Conclusion: EBD is an efficient, biologically plausible, and principled alternative for neural network training.

Abstract: We introduce Error Broadcast and Decorrelation (EBD), a novel learning
framework for neural networks that addresses credit assignment by directly
broadcasting output errors to individual layers, circumventing weight transport
of backpropagation. EBD is rigorously grounded in the stochastic orthogonality
property of Minimum Mean Square Error estimators. This fundamental principle
states that the error of an optimal estimator is orthogonal to functions of the
input. Guided by this insight, EBD defines layerwise loss functions that
directly penalize correlations between layer activations and output errors,
thereby establishing a principled foundation for error broadcasting. This
theoretically sound mechanism naturally leads to the experimentally observed
three-factor learning rule and integrates with biologically plausible
frameworks to enhance performance and plausibility. Numerical experiments
demonstrate EBD's competitive or better performance against other
error-broadcast methods on benchmark datasets. Our findings establish EBD as an
efficient, biologically plausible, and principled alternative for neural
network training.

</details>


### [743] [Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks](https://arxiv.org/pdf/2410.03972)
*Ann Huang, Satpreet H. Singh, Flavio Martinelli, Kanaka Rajan*

Main category: cs.LG

TL;DR: A framework to quantify and control solution degeneracy in RNNs across behavior, neural dynamics, and weight space, validated on 3,400 networks trained on neuroscience tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how neural systems solve tasks despite solution degeneracy (different RNNs achieving similar performance with different internal solutions).

Method: Developed a unified framework to analyze degeneracy, applied to RNNs trained on four tasks, varying complexity, learning regime, network size, and regularization.

Result: Higher task complexity and feature learning reduce neural dynamics degeneracy but increase it in weight space. Larger networks and regularization reduce degeneracy at all levels.

Conclusion: The framework offers tools to tailor RNN solutions for interpretability and biological relevance, validating the Contravariance Principle.

Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience
and machine learning to model dynamical computations. To gain mechanistic
insight into how neural systems solve tasks, prior work often reverse-engineers
individual trained networks. However, different RNNs trained on the same task
and achieving similar performance can exhibit strikingly different internal
solutions-a phenomenon known as solution degeneracy. Here, we develop a unified
framework to systematically quantify and control solution degeneracy across
three levels: behavior, neural dynamics, and weight space. We apply this
framework to 3,400 RNNs trained on four neuroscience-relevant tasks-flip-flop
memory, sine wave generation, delayed discrimination, and path
integration-while systematically varying task complexity, learning regime,
network size, and regularization. We find that higher task complexity and
stronger feature learning reduce degeneracy in neural dynamics but increase it
in weight space, with mixed effects on behavior. In contrast, larger networks
and structural regularization reduce degeneracy at all three levels. These
findings empirically validate the Contravariance Principle and provide
practical guidance for researchers aiming to tailor RNN solutions-whether to
uncover shared neural mechanisms or to model individual variability observed in
biological systems. This work provides a principled framework for quantifying
and controlling solution degeneracy in task-trained RNNs, offering new tools
for building more interpretable and biologically grounded models of neural
computation.

</details>


### [744] [A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization](https://arxiv.org/pdf/2504.14762)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: Dropout is modeled as a random walk over a graph of subnetworks, revealing it samples from robust, well-generalizing ensembles with exponential growth in network width.


<details>
  <summary>Details</summary>
Motivation: To provide a combinatorial and graph-theoretic understanding of dropout, explaining its effectiveness in neural network training.

Method: Model training as a random walk over a graph of binary subnetworks, using spectral graph theory, PAC-Bayes analysis, and combinatorics.

Result: Generalizing subnetworks form large, connected, low-resistance clusters, growing exponentially with network width.

Conclusion: Dropout effectively samples from robust subnetworks, offering a foundation for mask-guided regularization and subnetwork optimization.

Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling
training as a random walk over a high-dimensional graph of binary subnetworks.
Each node represents a masked version of the network, and dropout induces
stochastic traversal across this space. We define a subnetwork contribution
score that quantifies generalization and show that it varies smoothly over the
graph. Using tools from spectral graph theory, PAC-Bayes analysis, and
combinatorics, we prove that generalizing subnetworks form large, connected,
low-resistance clusters, and that their number grows exponentially with network
width. This reveals dropout as a mechanism for sampling from a robust,
structured ensemble of well-generalizing subnetworks with built-in redundancy.
Extensive experiments validate every theoretical claim across diverse
architectures. Together, our results offer a unified foundation for
understanding dropout and suggest new directions for mask-guided regularization
and subnetwork optimization.

</details>


### [745] [A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints](https://arxiv.org/pdf/2410.05612)
*Michael Munn, Susan Wei*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian model selection criterion, downstream free energy, to predict pretrained model adaptability without downstream data.


<details>
  <summary>Details</summary>
Motivation: To understand why the pretrain-then-adapt paradigm works and identify checkpoint characteristics that enhance downstream performance.

Method: Proposes downstream free energy, a Bayesian criterion, to quantify adaptability by measuring favorable parameter concentration.

Result: The criterion correlates with improved finetuning performance and works without downstream data or task knowledge.

Conclusion: Provides a principled way to predict model adaptability, enhancing the pretrain-then-adapt paradigm.

Abstract: Recent advances in artificial intelligence have been fueled by the
development of foundation models such as BERT, GPT, T5, and Vision
Transformers. These models are first pretrained on vast and diverse datasets
and then adapted to specific downstream tasks, often with significantly less
data. However, the mechanisms behind the success of this ubiquitous
pretrain-then-adapt paradigm remain underexplored, particularly the
characteristics of pretraining checkpoints that enhance downstream adaptation.
We introduce a Bayesian model selection criterion, called the downstream free
energy, which quantifies a checkpoint's adaptability by measuring the
concentration of nearby favorable parameters for the downstream task. We
demonstrate that this Bayesian model selection criterion can be effectively
implemented without access to the downstream data or prior knowledge of the
downstream task. Furthermore, we provide empirical evidence that the criterion
reliably correlates with improved finetuning performance, offering a principled
approach to predicting model adaptability.

</details>


### [746] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/pdf/2504.14945)
*Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY introduces off-policy reasoning traces to RLVR, balancing imitation and exploration, achieving significant performance gains over on-policy RLVR.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods are limited by their on-policy nature, restricting learning to a model's own outputs and hindering reasoning beyond initial capabilities.

Method: LUFFY combines off-policy demonstrations with on-policy rollouts using Mixed-Policy GRPO and policy shaping via regularized importance sampling.

Result: LUFFY achieves +6.4 average gain on math benchmarks and +6.2 points in out-of-distribution tasks, successfully training weak models where on-policy RLVR fails.

Conclusion: LUFFY overcomes on-policy RLVR limitations, showcasing the potential of off-policy guidance in enhancing reasoning models.

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}).
However, existing \textit{RLVR} approaches are inherently ``on-policy'',
limiting learning to a model's own outputs and failing to acquire reasoning
abilities beyond its initial capabilities. To address this issue, we introduce
\textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder
o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR}
with off-policy reasoning traces. LUFFY dynamically balances imitation and
exploration by combining off-policy demonstrations with on-policy rollouts
during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework,
which has a theoretically guaranteed convergence rate, alongside policy shaping
via regularized importance sampling to avoid superficial and rigid imitation
during mixed-policy training. Compared with previous RLVR methods, LUFFY
achieves an over \textbf{+6.4} average gain across six math benchmarks and an
advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most
significantly, we show that LUFFY successfully trains weak models in scenarios
where on-policy RLVR completely fails. These results provide compelling
evidence that LUFFY transcends the fundamental limitations of on-policy RLVR
and demonstrates the great potential of utilizing off-policy guidance in RLVR.

</details>


### [747] [On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures](https://arxiv.org/pdf/2410.11778)
*Wei Shen, Ruida Zhou, Jing Yang, Cong Shen*

Main category: cs.LG

TL;DR: The paper theoretically analyzes how transformers learn in-context classification tasks, showing convergence to optimal models under certain conditions and quantifying the impact of prompt lengths on performance.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical mechanisms behind transformers' in-context learning (ICL) capabilities, which are empirically impressive but poorly understood.

Method: The study focuses on training dynamics of single-layer transformers for Gaussian mixture classification, using gradient descent and analyzing convergence rates.

Result: The transformer converges to a globally optimal model linearly under assumptions, and larger training/testing prompt lengths reduce inference error, approaching ground truth.

Conclusion: Theoretical and experimental results validate the transformer's ability to perform in-context classification effectively under specified conditions.

Abstract: Although transformers have demonstrated impressive capabilities for
in-context learning (ICL) in practice, theoretical understanding of the
underlying mechanism that allows transformers to perform ICL is still in its
infancy. This work aims to theoretically study the training dynamics of
transformers for in-context classification tasks. We demonstrate that, for
in-context classification of Gaussian mixtures under certain assumptions, a
single-layer transformer trained via gradient descent converges to a globally
optimal model at a linear rate. We further quantify the impact of the training
and testing prompt lengths on the ICL inference error of the trained
transformer. We show that when the lengths of training and testing prompts are
sufficiently large, the prediction of the trained transformer approaches the
ground truth distribution of the labels. Experimental results corroborate the
theoretical findings.

</details>


### [748] [Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks](https://arxiv.org/pdf/2410.13974)
*Minhua Lin, Zhiwei Zhang, Enyan Dai, Zongyu Wu, Yilong Wang, Xiang Zhang, Suhang Wang*

Main category: cs.LG

TL;DR: TGPA is a backdoor attack framework targeting Graph Prompt Learning (GPL), bypassing the need to modify pre-trained GNN encoders and resisting downstream fine-tuning.


<details>
  <summary>Details</summary>
Motivation: GPL's vulnerability to backdoor attacks is overlooked, and existing attacks are impractical due to frozen GNN encoders and potential downstream fine-tuning.

Method: TGPA injects backdoors into graph prompts without altering GNN encoders and uses a fine-tuning-resistant poisoning approach.

Result: TGPA achieves high attack success rates and maintains clean accuracy across multiple datasets.

Conclusion: TGPA effectively compromises GPL models with fixed encoders, highlighting a critical security risk in GPL.

Abstract: Graph Prompt Learning (GPL) has been introduced as a promising approach that
uses prompts to adapt pre-trained GNN models to specific downstream tasks
without requiring fine-tuning of the entire model. Despite the advantages of
GPL, little attention has been given to its vulnerability to backdoor attacks,
where an adversary can manipulate the model's behavior by embedding hidden
triggers. Existing graph backdoor attacks rely on modifying model parameters
during training, but this approach is impractical in GPL as GNN encoder
parameters are frozen after pre-training. Moreover, downstream users may
fine-tune their own task models on clean datasets, further complicating the
attack. In this paper, we propose TGPA, a backdoor attack framework designed
specifically for GPL. TGPA injects backdoors into graph prompts without
modifying pre-trained GNN encoders and ensures high attack success rates and
clean accuracy. To address the challenge of model fine-tuning by users, we
introduce a finetuning-resistant poisoning approach that maintains the
effectiveness of the backdoor even after downstream model adjustments.
Extensive experiments on multiple datasets under various settings demonstrate
the effectiveness of TGPA in compromising GPL models with fixed GNN encoders.

</details>


### [749] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/pdf/2505.03793)
*Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou*

Main category: cs.LG

TL;DR: A novel theoretical framework, LENSLLM, is proposed to model LLM fine-tuning dynamics for efficient and accurate model selection, achieving high accuracy and computational savings.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of fine-tuning all LLM candidates due to computational constraints and the need to understand their generalization performance.

Method: Derives a PAC-Bayesian Generalization Bound and introduces LENSLLM, an NTK-based Rectified Scaling Model for performance prediction.

Result: Achieves up to 91.1% accuracy and reduces computational cost by 88.5%, outperforming 5 state-of-the-art methods.

Conclusion: LENSLLM provides an efficient and accurate solution for LLM selection, with open-sourced implementation available.

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a PAC-Bayesian
Generalization Bound that unveils fine-tuning dynamics of LLMs and then
introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model
that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at LensLLM.io.

</details>


### [750] [SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures](https://arxiv.org/pdf/2410.14667)
*Peimeng Guan, Mark A. Davenport*

Main category: cs.LG

TL;DR: The paper proposes SGD jittering, a training scheme for model-based architectures (MBAs) to improve generalization and robustness in inverse problems.


<details>
  <summary>Details</summary>
Motivation: While MBAs are known for better reconstruction and interpretability, their accuracy-robustness tradeoff is underexplored, especially for safety-critical applications.

Method: The authors introduce SGD jittering, which injects noise iteration-wise during reconstruction, and its SPGD extension.

Result: SGD jittering improves generalization and robustness, validated on denoising, seismic deconvolution, and MRI reconstruction tasks.

Conclusion: SGD jittering enhances MBA performance, offering cleaner reconstructions and better robustness against adversarial attacks.

Abstract: Inverse problems aim to reconstruct unseen data from corrupted or perturbed
measurements. While most work focuses on improving reconstruction quality,
generalization accuracy and robustness are equally important, especially for
safety-critical applications. Model-based architectures (MBAs), such as loop
unrolling methods, are considered more interpretable and achieve better
reconstructions. Empirical evidence suggests that MBAs are more robust to
perturbations than black-box solvers, but the accuracy-robustness tradeoff in
MBAs remains underexplored. In this work, we propose a simple yet effective
training scheme for MBAs, called SGD jittering, which injects noise
iteration-wise during reconstruction. We theoretically demonstrate that SGD
jittering not only generalizes better than the standard mean squared error
training but is also more robust to average-case attacks. We validate SGD
jittering using denoising toy examples, seismic deconvolution, and single-coil
MRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner
reconstructions for out-of-distribution data and demonstrates enhanced
robustness against adversarial attacks.

</details>


### [751] [GraphNarrator: Generating Textual Explanations for Graph Neural Networks](https://arxiv.org/pdf/2410.15268)
*Bo Pan, Zhen Xiong, Guanchen Wu, Zheng Zhang, Yifei Zhang, Liang Zhao*

Main category: cs.LG

TL;DR: GraphNarrator is a method for generating natural language explanations for Graph Neural Networks (GNNs) using pseudo-labels and Expert Iteration to train an explanation generator.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of explainability in GNNs when graphs have semantic features.

Method: Uses a generative language model to map input-output pairs to explanations, with pseudo-labels from saliency-based explanations and Expert Iteration for training.

Result: Produces faithful, concise, and human-preferred explanations.

Conclusion: GraphNarrator effectively bridges the explainability gap in GNNs.

Abstract: Graph representation learning has garnered significant attention due to its
broad applications in various domains, such as recommendation systems and
social network analysis. Despite advancements in graph learning methods,
challenges still remain in explainability when graphs are associated with
semantic features. In this paper, we present GraphNarrator, the first method
designed to generate natural language explanations for Graph Neural Networks.
GraphNarrator employs a generative language model that maps input-output pairs
to explanations reflecting the model's decision-making process. To address the
lack of ground truth explanations to train the model, we propose first
generating pseudo-labels that capture the model's decisions from saliency-based
explanations, then using Expert Iteration to iteratively train the pseudo-label
generator based on training objectives on explanation quality. The high-quality
pseudo-labels are finally utilized to train an end-to-end explanation generator
model. Extensive experiments are conducted to demonstrate the effectiveness of
GraphNarrator in producing faithful, concise, and human-preferred natural
language explanations.

</details>


### [752] [Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing](https://arxiv.org/pdf/2410.17194)
*Kento Nishi, Maya Okawa, Rahul Ramesh, Mikail Khona, Hidenori Tanaka, Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: Knowledge Editing (KE) algorithms can harm models' factual recall and reasoning by distorting broader entity representations, termed 'representation shattering'.


<details>
  <summary>Details</summary>
Motivation: To understand why KE algorithms cause destructive failures in models' broader factual recall and reasoning abilities.

Method: A synthetic task was created where a Transformer internalizes a structured knowledge graph, and KE's effects were evaluated.

Result: KE distorts entity representations beyond the targeted one, degrading factual recall and reasoning.

Conclusion: The study provides a mechanistic explanation for KE's adverse effects, validated in pre-trained models.

Abstract: Knowledge Editing (KE) algorithms alter models' weights to perform targeted
updates to incorrect, outdated, or otherwise unwanted factual associations.
However, recent work has shown that applying KE can adversely affect models'
broader factual recall accuracy and diminish their reasoning abilities.
Although these studies give insights into the potential harms of KE algorithms,
e.g., performance evaluations on benchmarks, little is understood about why
such destructive failures occur. Motivated by this, we define a novel synthetic
task in which a Transformer is trained from scratch to internalize a
"structured" knowledge graph. The structure enforces relationships between
entities of the graph, such that editing a factual association has "trickling
effects" on other entities (e.g., altering X's parent is Y to Z affects who X's
siblings' parent is). Through evaluations of edited models on this task, we
show that KE inadvertently affects representations of entities beyond the
targeted one, distorting relevant structures that allow a model to infer unseen
knowledge about an entity. We call this phenomenon representation shattering
and demonstrate that it degrades models' factual recall and reasoning
performance. We further corroborate our findings in naturalistic settings with
pre-trained Llama and Mamba models as well. Overall, our work yields a precise
mechanistic hypothesis to explain why KE has adverse effects on model
abilities.

</details>


### [753] [Multimodal Survival Modeling in the Age of Foundation Models](https://arxiv.org/pdf/2505.07683)
*Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman*

Main category: cs.LG

TL;DR: The paper explores using foundation models (FMs) to extract zero-shot embeddings from TCGA data, including pathology reports, for multimodal survival prediction, showing improved performance over unimodal models.


<details>
  <summary>Details</summary>
Motivation: To leverage underutilized free-text pathology reports in TCGA and modernize survival prediction by integrating multimodal data with foundation models.

Method: Training classical, multimodal survival models using zero-shot embeddings from FMs, including pathology reports, and evaluating text summarization and hallucination effects.

Result: Multimodal fusion outperforms unimodal models, demonstrating the additive benefit of including pathology report text.

Conclusion: The study successfully modernizes survival modeling by utilizing FMs and extracting valuable information from pathology reports.

Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a
large-scale reference through its harmonized genomics, clinical, and image
data. Prior studies have trained bespoke cancer survival prediction models from
unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning
is the development of foundation models (FMs) to derive meaningful feature
embeddings, agnostic to a specific modeling task. Biomedical text especially
has seen growing development of FMs. While TCGA contains free-text data as
pathology reports, these have been historically underutilized. Here, we
investigate the feasibility of training classical, multimodal survival models
over zero-shot embeddings extracted by FMs. We show the ease and additive
effect of multimodal fusion, outperforming unimodal models. We demonstrate the
benefit of including pathology report text and rigorously evaluate the effect
of model-based text summarization and hallucination. Overall, we modernize
survival modeling by leveraging FMs and information extraction from pathology
reports.

</details>


### [754] [Sparse Linear Bandits with Blocking Constraints](https://arxiv.org/pdf/2410.20041)
*Adit Jain, Soumyabrata Pal, Sunav Choudhary, Ramasuri Narayanam, Harshita Chopra, Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the high-dimensional sparse linear bandits problem in a
data-poor regime where the time horizon is much smaller than the ambient
dimension and number of arms. We study the setting under the additional
blocking constraint where each unique arm can be pulled only once. The blocking
constraint is motivated by practical applications in personalized content
recommendation and identification of data points to improve annotation
efficiency for complex learning tasks. With mild assumptions on the arms, our
proposed online algorithm (BSLB) achieves a regret guarantee of
$\widetilde{\mathsf{O}}((1+\beta_k)^2k^{\frac{2}{3}} \mathsf{T}^{\frac{2}{3}})$
where the parameter vector has an (unknown) relative tail $\beta_k$ -- the
ratio of $\ell_1$ norm of the top-$k$ and remaining entries of the parameter
vector. To this end, we show novel offline statistical guarantees of the lasso
estimator for the linear model that is robust to the sparsity modeling
assumption. Finally, we propose a meta-algorithm (C-BSLB) based on corralling
that does not need knowledge of optimal sparsity parameter $k$ at minimal cost
to regret. Our experiments on multiple real-world datasets demonstrate the
validity of our algorithms and theoretical framework.

</details>


### [755] [The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/pdf/2505.11692)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: The paper introduces the ReLU Transition Graph (RTG) to analyze ReLU neural networks, linking its geometry to expressivity, generalization, and robustness. It provides combinatorial bounds, proofs, and experimental validation, offering a graph-theoretic approach for network analysis.


<details>
  <summary>Details</summary>
Motivation: To understand ReLU neural networks' behavior and properties through a combinatorial and graph-theoretic lens, enabling better control over network design and performance.

Method: Develops the RTG, where nodes represent linear regions and edges connect regions differing by a single neuron flip. Uses this framework to derive theoretical results and validate them experimentally.

Result: Proves tight bounds on RTG size and diameter, connectivity, and connects RTG properties to VC-dimension and generalization error. Experiments confirm theoretical findings.

Conclusion: The RTG framework unifies ReLU network analysis via graph theory, offering new insights for compression, regularization, and complexity control.

Abstract: We develop a novel theoretical framework for analyzing ReLU neural networks
through the lens of a combinatorial object we term the ReLU Transition Graph
(RTG). In this graph, each node corresponds to a linear region induced by the
network's activation patterns, and edges connect regions that differ by a
single neuron flip. Building on this structure, we derive a suite of new
theoretical results connecting RTG geometry to expressivity, generalization,
and robustness. Our contributions include tight combinatorial bounds on RTG
size and diameter, a proof of RTG connectivity, and graph-theoretic
interpretations of VC-dimension. We also relate entropy and average degree of
the RTG to generalization error. Each theoretical result is rigorously
validated via carefully controlled experiments across varied network depths,
widths, and data regimes. This work provides the first unified treatment of
ReLU network structure via graph theory and opens new avenues for compression,
regularization, and complexity control rooted in RTG analysis.

</details>


### [756] [Diffusion Sampling Correction via Approximately 10 Parameters](https://arxiv.org/pdf/2411.06503)
*Guangyi Wang, Wei Peng, Lijiang Li, Wenyu Chen, Yuren Cai, Songzhi Su*

Main category: cs.LG

TL;DR: PAS optimizes DPM solvers with minimal costs using PCA-based adaptive search, improving sampling efficiency and reducing parameters.


<details>
  <summary>Details</summary>
Motivation: DPMs face slow sampling and high costs with existing distillation methods, limiting practicality.

Method: Uses PCA to span sampling space with basis vectors and adaptive search to correct direction and reduce parameters.

Result: PAS enhances solvers like DDIM, improving FID from 15.69 to 4.37 (NFE=10) with only 12 parameters and sub-minute training.

Conclusion: PAS is a cost-effective, plug-and-play solution for improving DPM sampling efficiency.

Abstract: While powerful for generation, Diffusion Probabilistic Models (DPMs) face
slow sampling challenges, for which various distillation-based methods have
been proposed. However, they typically require significant additional training
costs and model parameter storage, limiting their practicality. In this work,
we propose PCA-based Adaptive Search (PAS), which optimizes existing solvers
for DPMs with minimal additional costs. Specifically, we first employ PCA to
obtain a few basis vectors to span the high-dimensional sampling space, which
enables us to learn just a set of coordinates to correct the sampling
direction; furthermore, based on the observation that the cumulative truncation
error exhibits an ``S"-shape, we design an adaptive search strategy that
further enhances the sampling efficiency and reduces the number of stored
parameters to approximately 10. Extensive experiments demonstrate that PAS can
significantly enhance existing fast solvers in a plug-and-play manner with
negligible costs. E.g., on CIFAR10, PAS optimizes DDIM's FID from 15.69 to 4.37
(NFE=10) using only 12 parameters and sub-minute training on a single A100 GPU.
Code is available at https://github.com/onefly123/PAS.

</details>


### [757] [Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory](https://arxiv.org/pdf/2505.11694)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: Feedforward neural networks are proven to simulate deterministic finite automata (DFAs) with exactness, bridging deep learning and automata theory.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish neural networks as universal finite-state machines and bridge the gap between discrete symbolic processes and continuous neural systems.

Method: Theoretical and empirical framework using finite-depth ReLU and threshold networks to unroll DFA state transitions into neural layers, with formal characterizations of depth, width, and state compression.

Result: DFA transitions are linearly separable, binary threshold activations enable exponential compression, and Myhill-Nerode classes embed into continuous spaces. Fixed-depth networks cannot recognize non-regular languages.

Conclusion: The study provides a rigorous foundation for neural-symbolic computation, validating claims with explicit DFA-unrolled architectures.

Abstract: We present a complete theoretical and empirical framework establishing
feedforward neural networks as universal finite-state machines (N-FSMs). Our
results prove that finite-depth ReLU and threshold networks can exactly
simulate deterministic finite automata (DFAs) by unrolling state transitions
into depth-wise neural layers, with formal characterizations of required depth,
width, and state compression. We demonstrate that DFA transitions are linearly
separable, binary threshold activations allow exponential compression, and
Myhill-Nerode equivalence classes can be embedded into continuous latent spaces
while preserving separability. We also formalize the expressivity boundary:
fixed-depth feedforward networks cannot recognize non-regular languages
requiring unbounded memory. Unlike prior heuristic or probing-based studies, we
provide constructive proofs and design explicit DFA-unrolled neural
architectures that empirically validate every claim. Our results bridge deep
learning, automata theory, and neural-symbolic computation, offering a rigorous
blueprint for how discrete symbolic processes can be realized in continuous
neural systems.

</details>


### [758] [Combinatorial Rising Bandit](https://arxiv.org/pdf/2412.00798)
*Seockbean Song, Youngsik Yoon, Siwei Wang, Wei Chen, Jungseul Ok*

Main category: cs.LG

TL;DR: The paper introduces the Combinatorial Rising Bandit (CRB) framework and the CRUCB algorithm to address rising rewards in combinatorial online learning, with theoretical and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing models don't capture complex dependencies in rising rewards, where base arms enhance future rewards across multiple super arms.

Method: Proposes the CRB framework and the CRUCB algorithm, providing theoretical regret bounds and empirical validation.

Result: CRUCB achieves nearly tight regret bounds and performs well in synthetic and real-world applications like deep reinforcement learning.

Conclusion: The CRB framework and CRUCB algorithm effectively address rising rewards in combinatorial online learning, with proven efficiency and practical applicability.

Abstract: Combinatorial online learning is a fundamental task for selecting the optimal
action (or super arm) as a combination of base arms in sequential interactions
with systems providing stochastic rewards. It is applicable to diverse domains
such as robotics, social advertising, network routing, and recommendation
systems. In many real-world scenarios, we often encounter rising rewards, where
playing a base arm not only provides an instantaneous reward but also
contributes to the enhancement of future rewards, e.g., robots enhancing
proficiency through practice and social influence strengthening in the history
of successful recommendations. Moreover, the enhancement of a single base arm
may affect multiple super arms that include it, introducing complex
dependencies that are not captured by existing rising bandit models. To address
this, we introduce the Combinatorial Rising Bandit (CRB) framework and propose
a provably efficient algorithm, Combinatorial Rising Upper Confidence Bound
(CRUCB). We establish an upper bound on regret CRUCB and show that it is nearly
tight by deriving a matching lower bound. In addition, we empirically
demonstrate the effectiveness of CRUCB not only in synthetic environments but
also in realistic applications of deep reinforcement learning.

</details>


### [759] [TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation](https://arxiv.org/pdf/2412.11180)
*Ziang Zhou, Zhihao Ding, Jieming Shi, Qing Li, Shiqi Shen*

Main category: cs.LG

TL;DR: TINED distills GNNs into MLPs layer-by-layer using Teacher Injection and Dirichlet Energy Distillation, improving inference speed while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: GNNs face scalability issues due to multi-hop data needs during inference, limiting latency-sensitive applications. Existing MLP distillation methods underutilize GNN layer insights.

Method: TINED transfers GNN feature transformation (FT) parameters to MLP fully-connected (FC) layers, fine-tunes them, and approximates graph propagation (GP) with theoretical bounds. It uses Dirichlet Energy Distillation to balance smoothing effects of FT and GP.

Result: TINED outperforms GNNs and leading distillation methods across seven datasets, demonstrating superior performance and efficiency.

Conclusion: TINED effectively addresses GNN scalability by distilling them into MLPs, leveraging layer-level insights and theoretical approximations for faster inference without compromising accuracy.

Abstract: Graph Neural Networks (GNNs) are pivotal in graph-based learning,
particularly excelling in node classification. However, their scalability is
hindered by the need for multi-hop data during inference, limiting their
application in latency-sensitive scenarios. Recent efforts to distill GNNs into
multi-layer perceptrons (MLPs) for faster inference often underutilize the
layer-level insights of GNNs. In this paper, we present TINED, a novel approach
that distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection
and Dirichlet Energy Distillation techniques. We focus on two key operations in
GNN layers: feature transformation (FT) and graph propagation (GP). We
recognize that FT is computationally equivalent to a fully-connected (FC) layer
in MLPs. Thus, we propose directly transferring teacher parameters from an FT
in a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED,
the FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We
also establish a theoretical bound for GP approximation. Furthermore, we note
that FT and GP operations in GNN layers often exhibit opposing smoothing
effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we
develop a DE ratio to measure these effects and propose Dirichlet Energy
Distillation to convey these characteristics from GNN layers to MLP layers.
Extensive experiments show that TINED outperforms GNNs and leading distillation
methods across various settings and seven datasets. Source code are available
at https://github.com/scottjiao/TINED_ICML25/.

</details>


### [760] [Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs Algorithms](https://arxiv.org/pdf/2412.18202)
*Zhuohuan Hu, Richard Yu, Zizhou Zhang, Haoran Zheng, Qianying Liu, Yining Zhou*

Main category: cs.LG

TL;DR: The paper proposes a machine learning pipeline combining denoising autoencoders, CNNs, and GANs to predict financial price movements, showing empirical success in forecasting significant fluctuations.


<details>
  <summary>Details</summary>
Motivation: To improve financial time series forecasting by leveraging machine learning to filter noise, reduce dimensionality, and capture predictive patterns in price data.

Method: Uses a denoising autoencoder for noise removal, 1D convolution for dimensionality reduction, GANs for data generation, and a fully connected network for prediction, validated via cross-validation.

Result: The model effectively predicts the likelihood and direction of significant price changes, demonstrating the utility of machine learning in financial forecasting.

Conclusion: The approach validates machine learning's ability to uncover hidden patterns in financial data, with potential applications in trading strategies.

Abstract: This paper leverages machine learning algorithms to forecast and analyze
financial time series. The process begins with a denoising autoencoder to
filter out random noise fluctuations from the main contract price data. Then,
one-dimensional convolution reduces the dimensionality of the filtered data and
extracts key information. The filtered and dimensionality-reduced price data is
fed into a GANs network, and its output serve as input of a fully connected
network. Through cross-validation, a model is trained to capture features that
precede large price fluctuations. The model predicts the likelihood and
direction of significant price changes in real-time price sequences, placing
trades at moments of high prediction accuracy. Empirical results demonstrate
that using autoencoders and convolution to filter and denoise financial data,
combined with GANs, achieves a certain level of predictive performance,
validating the capabilities of machine learning algorithms to discover
underlying patterns in financial sequences. Keywords - CNN;GANs;
Cryptocurrency; Prediction.

</details>


### [761] [Credit Risk Identification in Supply Chains Using Generative Adversarial Networks](https://arxiv.org/pdf/2501.10348)
*Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, Qianying Liu*

Main category: cs.LG

TL;DR: The paper proposes using GANs to improve credit risk identification in supply chains, addressing data scarcity and imbalance. It outperforms traditional methods in accuracy, recall, and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Credit risk in supply chains can propagate widely, impacting operational stability. Data scarcity and imbalance hinder effective risk identification.

Method: GANs generate synthetic credit risk scenarios to augment data. The model is tested in manufacturing, pharmaceuticals, and e-commerce.

Result: The GAN-based model outperforms logistic regression, decision trees, and neural networks in accuracy, recall, and F1 scores.

Conclusion: GANs offer robust tools for proactive credit risk management. Future work could include external market factors and supplier relationships.

Abstract: Credit risk management within supply chains has emerged as a critical
research area due to its significant implications for operational stability and
financial sustainability. The intricate interdependencies among supply chain
participants mean that credit risks can propagate across networks, with impacts
varying by industry. This study explores the application of Generative
Adversarial Networks (GANs) to enhance credit risk identification in supply
chains. GANs enable the generation of synthetic credit risk scenarios,
addressing challenges related to data scarcity and imbalanced datasets. By
leveraging GAN-generated data, the model improves predictive accuracy while
effectively capturing dynamic and temporal dependencies in supply chain data.
The research focuses on three representative industries-manufacturing (steel),
distribution (pharmaceuticals), and services (e-commerce) to assess
industry-specific credit risk contagion. Experimental results demonstrate that
the GAN-based model outperforms traditional methods, including logistic
regression, decision trees, and neural networks, achieving superior accuracy,
recall, and F1 scores. The findings underscore the potential of GANs in
proactive risk management, offering robust tools for mitigating financial
disruptions in supply chains. Future research could expand the model by
incorporating external market factors and supplier relationships to further
enhance predictive capabilities. Keywords- Generative Adversarial Networks
(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data
Augmentation

</details>


### [762] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/pdf/2505.17652)
*Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye*

Main category: cs.LG

TL;DR: CDAS improves RL efficiency for language models by aligning problem difficulty with model competence, outperforming baselines in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving RL efficiency in language models suffer from unstable difficulty estimation and misalignment with model competence, leading to suboptimal results.

Method: Introduces CDAS, which aggregates historical performance to estimate problem difficulty and aligns it with model competence using a fixed-point system.

Result: CDAS achieves higher accuracy and efficiency, outperforming baselines and being 2.33 times faster than Dynamic Sampling.

Conclusion: CDAS effectively addresses the limitations of existing methods, demonstrating superior performance in RL training for language models.

Abstract: Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces $\textbf{C}$ompetence-$\textbf{D}$ifficulty
$\textbf{A}$lignment $\textbf{S}$ampling ($\textbf{CDAS}$), which enables
accurate and stable estimation of problem difficulties by aggregating
historical performance discrepancies of problems. Then the model competence is
quantified to adaptively select problems whose difficulty is in alignment with
the model's current competence using a fixed-point system. Experimental results
across a range of challenging mathematical benchmarks show that CDAS achieves
great improvements in both accuracy and efficiency. CDAS attains the highest
average accuracy against baselines and exhibits significant speed advantages
compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33
times slower than CDAS.

</details>


### [763] [Federated Granger Causality Learning for Interdependent Clients with State Space Representation](https://arxiv.org/pdf/2501.13890)
*Ayush Mohanty, Nazal Mohamed, Paritosh Ramanan, Nagi Gebraeel*

Main category: cs.LG

TL;DR: A federated approach for learning Granger causality in industrial IoT systems, addressing data volume and complexity while ensuring privacy and scalability.


<details>
  <summary>Details</summary>
Motivation: To detect and quantify interdependencies in industrial IoT systems, where localized events can propagate widely, but centralized data processing is impractical due to bandwidth and computational constraints.

Method: Uses a linear state space system framework with low-dimensional state estimates, augmented by server-learned Granger causality, and reformulated as a standalone ML algorithm with convergence guarantees.

Result: Demonstrates robustness to perturbations, scalability in communication and data dimensions, and significant data savings in real-world industrial datasets.

Conclusion: The federated approach effectively models interdependencies while addressing privacy and scalability challenges, validated through synthetic and real-world data.

Abstract: Advanced sensors and IoT devices have improved the monitoring and control of
complex industrial enterprises. They have also created an interdependent fabric
of geographically distributed process operations (clients) across these
enterprises. Granger causality is an effective approach to detect and quantify
interdependencies by examining how one client's state affects others over time.
Understanding these interdependencies captures how localized events, such as
faults and disruptions, can propagate throughout the system, possibly causing
widespread operational impacts. However, the large volume and complexity of
industrial data pose challenges in modeling these interdependencies. This paper
develops a federated approach to learning Granger causality. We utilize a
linear state space system framework that leverages low-dimensional state
estimates to analyze interdependencies. This addresses bandwidth limitations
and the computational burden commonly associated with centralized data
processing. We propose augmenting the client models with the Granger causality
information learned by the server through a Machine Learning (ML) function. We
examine the co-dependence between the augmented client and server models and
reformulate the framework as a standalone ML algorithm providing conditions for
its sublinear and linear convergence rates. We also study the convergence of
the framework to a centralized oracle model. Moreover, we include a
differential privacy analysis to ensure data security while preserving causal
insights. Using synthetic data, we conduct comprehensive experiments to
demonstrate the robustness of our approach to perturbations in causality, the
scalability to the size of communication, number of clients, and the dimensions
of raw data. We also evaluate the performance on two real-world industrial
control system datasets by reporting the volume of data saved by
decentralization.

</details>


### [764] [Norm-Bounded Low-Rank Adaptation](https://arxiv.org/pdf/2501.19050)
*Ruigang Wang, Krishnamurthy Dvijotham, Ian R. Manchester*

Main category: cs.LG

TL;DR: NB-LoRA is a parameter-efficient fine-tuning method with explicit bounds on singular values, outperforming existing approaches in performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve parameter-efficient fine-tuning by bounding singular values of adaptation matrices, ensuring better control and performance.

Method: Proposes norm-bounded low-rank adaptation (NB-LoRA), a smooth and complete parameterization of low-rank weight adaptations with explicit singular value bounds.

Result: NB-LoRA achieves superior performance and faster training in language models, and avoids catastrophic forgetting in vision tasks while being hyperparameter-robust.

Conclusion: NB-LoRA is a robust and efficient method for fine-tuning, excelling in performance, training speed, and adaptability across tasks.

Abstract: In this work, we propose norm-bounded low-rank adaptation (NB-LoRA) for
parameter-efficient fine tuning. NB-LoRA is a novel parameterization of
low-rank weight adaptations that admits explicit bounds on each singular value
of the adaptation matrix, which can thereby satisfy any prescribed unitarily
invariant norm bound, including the Schatten norms (e.g., nuclear, Frobenius,
spectral norm). The proposed parameterization is unconstrained, smooth, and
complete, i.e. it covers all matrices satisfying the prescribed rank and
singular-value bounds. Comparative experiments on large language models show
that NB-LoRA achieves superior adaptation performance and faster training over
a range of models, tasks and ranks. Vision fine-tuning experiments show that
NB-LoRA can achieve strong adaptation performance while avoiding model
catastrophic forgetting, and compared to existing approaches it is
substantially more robust to a hyper-parameters such as including adaptation
rank, learning rate and number of training epochs.

</details>


### [765] [Offline Learning for Combinatorial Multi-armed Bandits](https://arxiv.org/pdf/2501.19300)
*Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee Joe-Wong, John C. S. Lui, Wei Chen*

Main category: cs.LG

TL;DR: Off-CMAB introduces an offline learning framework for combinatorial multi-armed bandits (CMAB), addressing the limitations of online-only approaches by leveraging offline datasets and the CLCB algorithm.


<details>
  <summary>Details</summary>
Motivation: Existing CMAB research focuses on online settings, ignoring costly interactions and available offline data. Off-CMAB aims to bridge this gap.

Method: The framework uses the combinatorial lower confidence bound (CLCB) algorithm, combining pessimistic reward estimations with combinatorial solvers, and introduces data coverage conditions for dataset quality.

Result: Under proposed data coverage conditions, CLCB achieves near-optimal suboptimality gaps, validated in applications like learning to rank and LLM caching.

Conclusion: Off-CMAB effectively handles nonlinear rewards, general feedback, and out-of-distribution actions, demonstrating superior performance in experiments.

Abstract: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential
decision-making framework, extensively studied over the past decade. However,
existing work primarily focuses on the online setting, overlooking the
substantial costs of online interactions and the readily available offline
datasets. To overcome these limitations, we introduce Off-CMAB, the first
offline learning framework for CMAB. Central to our framework is the
combinatorial lower confidence bound (CLCB) algorithm, which combines
pessimistic reward estimations with combinatorial solvers. To characterize the
quality of offline datasets, we propose two novel data coverage conditions and
prove that, under these conditions, CLCB achieves a near-optimal suboptimality
gap, matching the theoretical lower bound up to a logarithmic factor. We
validate Off-CMAB through practical applications, including learning to rank,
large language model (LLM) caching, and social influence maximization, showing
its ability to handle nonlinear reward functions, general feedback models, and
out-of-distribution action samples that excludes optimal or even feasible
actions. Extensive experiments on synthetic and real-world datasets further
highlight the superior performance of CLCB.

</details>


### [766] [Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View](https://arxiv.org/pdf/2502.11256)
*Yanran Wu, Inez Hua, Yi Ding*

Main category: cs.LG

TL;DR: The paper introduces FUEL, a framework using functional units (FU) to standardize carbon emission evaluation for LLM serving, highlighting trade-offs in model optimization.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack standardized comparison for LLM carbon emissions, prompting the need for a unified framework like FUEL.

Method: Developed FUEL, an FU-based framework, and validated it through three case studies on model size, quantization, and hardware choices.

Result: Key insights and trade-offs were identified for reducing carbon emissions in LLM serving.

Conclusion: FUEL paves the way for more sustainable LLM serving by providing a standardized evaluation method.

Abstract: Large language models (LLMs) offer powerful capabilities but come with
significant environmental impact, particularly in carbon emissions. Existing
studies benchmark carbon emissions but lack a standardized basis for comparison
across different model configurations. To address this, we introduce the
concept of functional unit (FU) as a standardized basis and develop FUEL, the
first FU-based framework for evaluating LLM serving's environmental impact.
Through three case studies, we uncover key insights and trade-offs in reducing
carbon emissions by optimizing model size, quantization strategy, and hardware
choice, paving the way for more sustainable LLM serving. The code is available
at https://github.com/jojacola/FUEL.

</details>


### [767] [Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion](https://arxiv.org/pdf/2502.00264)
*Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li*

Main category: cs.LG

TL;DR: The paper introduces rotation symmetry for transformers, a continuous generalization of permutation symmetry, and proposes an optimal parameter matching algorithm to enhance model fusion.


<details>
  <summary>Details</summary>
Motivation: Existing permutation symmetry in MLPs is discrete and limited for transformers, prompting the need for a continuous symmetry like rotation symmetry.

Method: Rotation symmetry is introduced by rotating parameter matrices in self-attention layers, and a theoretically optimal parameter matching algorithm is proposed.

Result: Experiments show the rotation symmetry-based matching algorithm significantly improves model fusion across diverse tasks.

Conclusion: Rotation symmetry expands the equivalence set for transformers and effectively enhances model fusion, demonstrating the potential of parameter space symmetry.

Abstract: Symmetry in the parameter space of deep neural networks (DNNs) has proven
beneficial for various deep learning applications. A well-known example is the
permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the
rows of weight matrices in one layer and applying the inverse permutation to
adjacent layers yields a functionally equivalent model. While permutation
symmetry fully characterizes the equivalence set for MLPs, its discrete nature
limits its utility for transformers. In this paper, we introduce rotation
symmetry, a novel form of parameter space symmetry for transformers that
generalizes permutation symmetry by rotating parameter matrices in
self-attention layers. Unlike permutation symmetry, rotation symmetry operates
in a continuous domain, thereby significantly expanding the equivalence set for
transformers. Based on this property, we propose a theoretically optimal
parameter matching algorithm as a plug-and-play module to enhance model fusion.
We evaluate our approach using pre-trained transformers across diverse natural
language and vision tasks. Experimental results demonstrate that our rotation
symmetry-based matching algorithm substantially improves model fusion,
highlighting the potential of parameter space symmetry to facilitate model
fusion. Our code is available on
https://github.com/zhengzaiyi/RotationSymmetry.

</details>


### [768] [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/pdf/2502.13344)
*Tassallah Abdullahi, Ioanna Gemou, Nihal V. Nayak, Ghulam Murtaza, Stephen H. Bach, Carsten Eickhoff, Ritambhara Singh*

Main category: cs.LG

TL;DR: K-Paths is a model-agnostic framework for retrieving diverse, biologically meaningful paths from biomedical KGs to enhance drug discovery tasks, improving LLM and GNN performance.


<details>
  <summary>Details</summary>
Motivation: Extracting meaningful insights from complex biomedical KGs is challenging, and existing methods are limited to GNNs, lacking compatibility with LLMs.

Method: K-Paths uses a diversity-aware adaptation of Yen's algorithm to extract K shortest loopless paths, prioritizing biologically relevant connections.

Result: K-Paths improves zero-shot reasoning in LLMs (e.g., Tx-Gemma 27B and Llama 70B) and boosts GNN efficiency (e.g., EmerGNN) while maintaining performance.

Conclusion: K-Paths bridges KGs and LLMs, enabling scalable, explainable scientific discovery, and is released as a benchmark for inductive reasoning.

Abstract: Biomedical knowledge graphs (KGs) encode rich, structured information
critical for drug discovery tasks, but extracting meaningful insights from
large-scale KGs remains challenging due to their complex structure. Existing
biomedical subgraph retrieval methods are tailored for graph neural networks
(GNNs), limiting compatibility with other paradigms, including large language
models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that
extracts structured, diverse, and biologically meaningful multi-hop paths from
dense biomedical KGs. These paths enable the prediction of unobserved drug-drug
and drug-disease interactions, including those involving entities not seen
during training, thus supporting inductive reasoning. K-Paths is training-free
and employs a diversity-aware adaptation of Yen's algorithm to extract the K
shortest loopless paths between entities in a query, prioritizing biologically
relevant and relationally diverse connections. These paths serve as concise,
interpretable reasoning chains that can be directly integrated with LLMs or
GNNs to improve generalization, accuracy, and enable explainable inference.
Experiments on benchmark datasets show that K-Paths improves zero-shot
reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by
19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing
tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same
tasks. K-Paths also boosts the training efficiency of EmerGNN, a
state-of-the-art GNN, by reducing the KG size by 90% while maintaining
predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs
and LLMs, enabling scalable and explainable LLM-augmented scientific discovery.
We release our code and the retrieved paths as a benchmark for inductive
reasoning.

</details>


### [769] [Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network](https://arxiv.org/pdf/2505.19423)
*Bingdong Li, Mei Jiang, Hong Qian, Ke Tang, Aimin Zhou, Peng Yang*

Main category: cs.LG

TL;DR: The paper proposes a surrogate-assisted Evolutionary Reinforcement Learning (ERL) method using Autoencoders (AE) and Hyperbolic Neural Networks (HNN) to reduce computational costs and improve search efficiency.


<details>
  <summary>Details</summary>
Motivation: ERL suffers from high computational costs and low search efficiency due to evaluating numerous ineffective policies. Existing surrogates struggle with high-dimensional policy representations.

Method: AE compresses policies into low-dimensional representations, while HNN acts as a surrogate to pre-select policies without real evaluations.

Result: Experiments on 10 Atari and 4 Mujoco games show the method outperforms previous approaches, with more effective exploration and convergence.

Conclusion: The paper introduces the first learnable policy embedding and surrogate-modeling modules for high-dimensional ERL policies, demonstrating their success empirically.

Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement
Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated
enhanced exploration capabilities and greater robustness than using traditional
policy gradient. However, ERL suffers from the high computational costs and low
search efficiency, as EAs require evaluating numerous candidate policies with
expensive simulations, many of which are ineffective and do not contribute
meaningfully to the training. One intuitive way to reduce the ineffective
evaluations is to adopt the surrogates. Unfortunately, existing ERL policies
are often modeled as deep neural networks (DNNs) and thus naturally represented
as high-dimensional vectors containing millions of weights, which makes the
building of effective surrogates for ERL policies extremely challenging. This
paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)
and Hyperbolic Neural Networks (HNN). Specifically, AE compresses
high-dimensional policies into low-dimensional representations while extracting
key features as the inputs for the surrogate. HNN, functioning as a
classification-based surrogate model, can learn complex nonlinear relationships
from sampled data and enable more accurate pre-selection of the sampled
policies without real evaluations. The experiments on 10 Atari and 4 Mujoco
games have verified that the proposed method outperforms previous approaches
significantly. The search trajectories guided by AE and HNN are also visually
demonstrated to be more effective, in terms of both exploration and
convergence. This paper not only presents the first learnable policy embedding
and surrogate-modeling modules for high-dimensional ERL policies, but also
empirically reveals when and why they can be successful.

</details>


### [770] [Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network](https://arxiv.org/pdf/2502.00288)
*Jijia Liu, Feng Gao, Qingmin Liao, Chao Yu, Yu Wang*

Main category: cs.LG

TL;DR: ARSQ is a value-based RL algorithm for continuous control that improves sample efficiency by modeling Q-values auto-regressively and decomposing action spaces hierarchically.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in value-based RL for continuous control, especially when learning from suboptimal data like non-expert demonstrations.

Method: Decomposes action space hierarchically and auto-regressively predicts action advantages. Evaluated on RLBench and D4RL benchmarks.

Result: Achieves 1.62× performance improvement on D4RL and outperforms baselines on RLBench.

Conclusion: ARSQ effectively enhances sample efficiency and performance in continuous control tasks, even with suboptimal data.

Abstract: Reinforcement learning (RL) for continuous control often requires large
amounts of online interaction data. Value-based RL methods can mitigate this
burden by offering relatively high sample efficiency. Some studies further
enhance sample efficiency by incorporating offline demonstration data to
"kick-start" training, achieving promising results in continuous control.
However, they typically compute the Q-function independently for each action
dimension, neglecting interdependencies and making it harder to identify
optimal actions when learning from suboptimal data, such as non-expert
demonstration and online-collected data during the training process. To address
these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based
RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner.
First, ARSQ decomposes the continuous action space into discrete spaces in a
coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained
continuous control tasks. Next, it auto-regressively predicts dimensional
action advantages within each decision step, enabling more effective
decision-making in continuous control tasks. We evaluate ARSQ on two continuous
control benchmarks, RLBench and D4RL, integrating demonstration data into
online training. On D4RL, which includes non-expert demonstrations, ARSQ
achieves an average $1.62\times$ performance improvement over SOTA value-based
baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses
various baselines, demonstrating its effectiveness in learning from suboptimal
online-collected data. Project page is at
https://sites.google.com/view/ar-soft-q

</details>


### [771] [SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models](https://arxiv.org/pdf/2503.13503)
*Chuan Qin, Xin Chen, Chengrui Wang, Pengmin Wu, Xi Chen, Yihang Cheng, Jingyi Zhao, Meng Xiao, Xiangchao Dong, Qingqing Long, Boya Pan, Han Wu, Chengzan Li, Yuanchun Zhou, Hui Xiong, Hengshu Zhu*

Main category: cs.LG

TL;DR: The paper introduces SciHorizon, a framework to assess AI4Science readiness, focusing on data quality and LLM capabilities across scientific disciplines.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI4Science lacks a holistic assessment framework for data quality and model capability, prompting the need for SciHorizon.

Method: The study proposes a framework with dimensions for data (Quality, FAIRness, Explainability, Compliance) and LLMs (Knowledge, Understanding, Reasoning, Multimodality, Values), evaluating datasets and 50+ LLMs.

Result: Recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences were created, and LLMs were benchmarked across 16 dimensions. Results are publicly available.

Conclusion: SciHorizon provides a novel, comprehensive tool for assessing AI4Science readiness, bridging gaps in data and LLM evaluation.

Abstract: In recent years, the rapid advancement of Artificial Intelligence (AI)
technologies, particularly Large Language Models (LLMs), has revolutionized the
paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a
dynamic and evolving field. However, there is still a lack of an effective
framework for the overall assessment of AI4Science, particularly from a
holistic perspective on data quality and model capability. Therefore, in this
study, we propose SciHorizon, a comprehensive assessment framework designed to
benchmark the readiness of AI4Science from both scientific data and LLM
perspectives. First, we introduce a generalizable framework for assessing
AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness,
Explainability, and Compliance-which are subdivided into 15 sub-dimensions.
Drawing on data resource papers published between 2018 and 2023 in
peer-reviewed journals, we present recommendation lists of AI-ready datasets
for Earth, Life, and Materials Sciences, making a novel and original
contribution to the field. Concurrently, to assess the capabilities of LLMs
across multiple scientific disciplines, we establish 16 assessment dimensions
based on five core indicators Knowledge, Understanding, Reasoning,
Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life
Sciences, and Earth and Space Sciences. Using the developed benchmark datasets,
we have conducted a comprehensive evaluation of over 50 representative
open-source and closed source LLMs. All the results are publicly available and
can be accessed online at www.scihorizon.cn/en.

</details>


### [772] [Risk-aware Direct Preference Optimization under Nested Risk Measure](https://arxiv.org/pdf/2505.20359)
*Lijun Zhang, Lin Li, Yajie Qi, Huizhong Song, Yaodong Yang, Jun Wang, Wei Wei*

Main category: cs.LG

TL;DR: Ra-DPO introduces risk-awareness into LLM fine-tuning using nested risk measures, balancing alignment performance and model drift better than KL divergence methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods using KL divergence may not suffice for tight risk control in aligning LLMs with human values, prompting the need for a risk-aware approach.

Method: Ra-DPO employs nested risk measures, formulates a constrained risk-aware advantage function, and converts the Bradley-Terry model into a token-level representation.

Result: Experiments on IMDb, Anthropic HH, and AlpacaEval datasets show Ra-DPO outperforms in balancing alignment and model drift.

Conclusion: Ra-DPO enhances risk-awareness in LLM fine-tuning, offering superior performance and control over model deviations.

Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human
values and intentions, maximizing the estimated reward can lead to superior
performance, but it also introduces potential risks due to deviations from the
reference model's intended behavior. Most existing methods typically introduce
KL divergence to constrain deviations between the trained model and the
reference model; however, this may not be sufficient in certain applications
that require tight risk control. In this paper, we introduce Risk-aware Direct
Preference Optimization (Ra-DPO), a novel approach that incorporates
risk-awareness by employing a class of nested risk measures. This approach
formulates a constrained risk-aware advantage function maximization problem and
then converts the Bradley-Terry model into a token-level representation. The
objective function maximizes the likelihood of the policy while suppressing the
deviation between a trained model and the reference model using a sequential
risk ratio, thereby enhancing the model's risk-awareness. Experimental results
across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and
AlpacaEval, demonstrate the proposed method's superior performance in balancing
alignment performance and model drift. Our code is opensourced at
https://github.com/zlj123-max/Ra-DPO.

</details>


### [773] [Learn Singularly Perturbed Solutions via Homotopy Dynamics](https://arxiv.org/pdf/2502.00488)
*Chuqi Chen, Yahong Yang, Yang Xiang, Wenrui Hao*

Main category: cs.LG

TL;DR: A novel homotopy dynamics method is introduced to train neural networks for singularly perturbed PDEs, improving convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: Training neural networks for singularly perturbed PDEs is challenging due to near-singularities in the loss function caused by certain parameters.

Method: The study proposes a homotopy dynamics method to manipulate these parameters, analyzing their impact theoretically and demonstrating the method's convergence.

Result: The approach accelerates convergence and enhances accuracy for singularly perturbed problems.

Conclusion: The homotopy dynamics method provides an efficient optimization strategy, extending neural networks' applicability to singularly perturbed PDEs.

Abstract: Solving partial differential equations (PDEs) using neural networks has
become a central focus in scientific machine learning. Training neural networks
for singularly perturbed problems is particularly challenging due to certain
parameters in the PDEs that introduce near-singularities in the loss function.
In this study, we overcome this challenge by introducing a novel method based
on homotopy dynamics to effectively manipulate these parameters. From a
theoretical perspective, we analyze the effects of these parameters on training
difficulty in these singularly perturbed problems and establish the convergence
of the proposed homotopy dynamics method. Experimentally, we demonstrate that
our approach significantly accelerates convergence and improves the accuracy of
these singularly perturbed problems. These findings present an efficient
optimization strategy leveraging homotopy dynamics, offering a robust framework
to extend the applicability of neural networks for solving singularly perturbed
differential equations.

</details>


### [774] [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/pdf/2503.16814)
*Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun*

Main category: cs.LG

TL;DR: DReaMAD improves LLM decision-making by refining strategic prior knowledge and promoting diverse viewpoints, outperforming existing methods like MAD.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in self-correction and multi-agent debate (MAD) methods, which reinforce biases and lack perspective diversity.

Method: Proposes DReaMAD, a framework refining LLM prompts for better reasoning and diverse viewpoints within a single model.

Result: DReaMAD enhances decision accuracy, reasoning diversity, and bias mitigation in strategic tasks.

Conclusion: DReaMAD is a superior approach for reliable and diverse LLM-based decision-making.

Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free
methods like prompt engineering and in-context learning, yet ensuring reasoning
correctness remains challenging. While self-correction methods such as
self-consistency and self-refinement aim to improve reliability, they often
reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent
Debate $($MAD$)$ has emerged as an alternative, but we identify two key
limitations: bias reinforcement, where debate amplifies model biases instead of
correcting them, and lack of perspective diversity, as all agents share the
same model and reasoning patterns, limiting true debate effectiveness. To
systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a
benchmark designed to assess LLMs in adversarial strategic decision-making,
where dynamic interactions influence optimal decisions. To overcome MAD's
limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse
$\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate
with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic
prior knowledge to improve reasoning quality and $(2)$ promotes diverse
viewpoints within a single model by systematically modifying prompts, reducing
bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves
decision accuracy, reasoning diversity, and bias mitigation across multiple
strategic tasks, establishing it as a more effective approach for LLM-based
decision-making.

</details>


### [775] [Subgroups Matter for Robust Bias Mitigation](https://arxiv.org/pdf/2505.21363)
*Anissa Alloula, Charles Jones, Ben Glocker, Bartłomiej W. Papież*

Main category: cs.LG

TL;DR: Subgroup definition significantly impacts bias mitigation performance, sometimes worsening outcomes. Careful subgroup choice is crucial for fairness.


<details>
  <summary>Details</summary>
Motivation: To understand why bias mitigation methods fail, focusing on the overlooked role of subgroup definitions.

Method: Comprehensive evaluation of bias mitigation methods across tasks, varying subgroup definitions (coarse, fine-grained, intersectional, noisy).

Result: Subgroup choice affects performance; some groupings worsen outcomes. Improving fairness may require using different subgroups.

Conclusion: Subgroup definition is critical for effective bias mitigation and fairness in machine learning.

Abstract: Despite the constant development of new bias mitigation methods for machine
learning, no method consistently succeeds, and a fundamental question remains
unanswered: when and why do bias mitigation techniques fail? In this paper, we
hypothesise that a key factor may be the often-overlooked but crucial step
shared by many bias mitigation methods: the definition of subgroups. To
investigate this, we conduct a comprehensive evaluation of state-of-the-art
bias mitigation methods across multiple vision and language classification
tasks, systematically varying subgroup definitions, including coarse,
fine-grained, intersectional, and noisy subgroups. Our results reveal that
subgroup choice significantly impacts performance, with certain groupings
paradoxically leading to worse outcomes than no mitigation at all. Our findings
suggest that observing a disparity between a set of subgroups is not a
sufficient reason to use those subgroups for mitigation. Through theoretical
analysis, we explain these phenomena and uncover a counter-intuitive insight
that, in some cases, improving fairness with respect to a particular set of
subgroups is best achieved by using a different set of subgroups for
mitigation. Our work highlights the importance of careful subgroup definition
in bias mitigation and presents it as an alternative lever for improving the
robustness and fairness of machine learning models.

</details>


### [776] [Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting](https://arxiv.org/pdf/2502.02410)
*Jan Schuchardt, Mina Dalirrooyfard, Jed Guzelkabaagac, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann*

Main category: cs.LG

TL;DR: The paper addresses the incompatibility of DP-SGD with time-series forecasting due to structured data requirements, proposing structured subsampling for better privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard DP-SGD fails for time-series tasks like forecasting because it relies on unstructured batches, unlike the sequential nature of such data.

Method: The authors analyze privacy amplification via structured subsampling (sampling time series, subsequences, and partitioning) and data augmentation in self-supervised training.

Result: Theoretical and empirical results show that structured subsampling enables forecasting models with strong privacy guarantees.

Conclusion: Structured subsampling and data augmentation provide sound privacy guarantees for time-series forecasting, overcoming DP-SGD's limitations.

Abstract: Many forms of sensitive data, such as web traffic, mobility data, or hospital
occupancy, are inherently sequential. The standard method for training machine
learning models while ensuring privacy for units of sensitive information, such
as individual hospital visits, is differentially private stochastic gradient
descent (DP-SGD). However, we observe in this work that the formal guarantees
of DP-SGD are incompatible with time-series-specific tasks like forecasting,
since they rely on the privacy amplification attained by training on small,
unstructured batches sampled from an unstructured dataset. In contrast, batches
for forecasting are generated by (1) sampling sequentially structured time
series from a dataset, (2) sampling contiguous subsequences from these series,
and (3) partitioning them into context and ground-truth forecast windows. We
theoretically analyze the privacy amplification attained by this structured
subsampling to enable the training of forecasting models with sound and tight
event- and user-level privacy guarantees. Towards more private models, we
additionally prove how data augmentation amplifies privacy in self-supervised
training of sequence models. Our empirical evaluation demonstrates that
amplification by structured subsampling enables the training of forecasting
models with strong formal privacy guarantees.

</details>


### [777] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/pdf/2505.00926)
*Ruiquan Huang, Yingbin Liang, Jing Yang*

Main category: cs.LG

TL;DR: The paper explores how a one-layer transformer learns regular language tasks (even pairs and parity check) under gradient descent, revealing distinct training phases and validating results experimentally.


<details>
  <summary>Details</summary>
Motivation: To understand the working mechanisms of transformers in solving fundamental NLP tasks like regular language recognition, focusing on 'even pairs' and 'parity check'.

Method: Theoretical analysis of training dynamics for a one-layer transformer (attention + linear layer) under gradient descent, with experiments to validate findings.

Result: Two training phases: rapid attention layer growth for separability, followed by logarithmic linear layer growth toward a max-margin solution. Loss decreases at O(1/t).

Conclusion: One-layer transformers can solve even pairs directly, while parity check requires Chain-of-Thought integration. Training dynamics reveal separable phases and logarithmic convergence.

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [778] [Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling](https://arxiv.org/pdf/2502.05743)
*Xiao Li, Zekai Zhang, Xiang Li, Siyi Chen, Zhihui Zhu, Peng Wang, Qing Qu*

Main category: cs.LG

TL;DR: Diffusion models show unimodal representation dynamics in self-supervised learning, peaking at intermediate noise levels. Theoretical and empirical analysis links this to data distribution capture and generalization in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the unimodal representation dynamics in diffusion models and their connection to generalization and memorization in learning tasks.

Method: Combines theoretical analysis leveraging low-dimensional image data structure with empirical validation in classification tasks.

Result: Unimodal dynamics emerge when the model captures the data distribution and indicates generalization; it shifts to monotonic decline with memorization.

Conclusion: Unimodal representation dynamics in diffusion models signal successful generalization, transitioning to memorization when the curve becomes monotonically decreasing.

Abstract: Diffusion models, though originally designed for generative tasks, have
demonstrated impressive self-supervised representation learning capabilities. A
particularly intriguing phenomenon in these models is the emergence of unimodal
representation dynamics, where the quality of learned features peaks at an
intermediate noise level. In this work, we conduct a comprehensive theoretical
and empirical investigation of this phenomenon. Leveraging the inherent
low-dimensionality structure of image data, we theoretically demonstrate that
the unimodal dynamic emerges when the diffusion model successfully captures the
underlying data distribution. The unimodality arises from an interplay between
denoising strength and class confidence across noise scales. Empirically, we
further show that, in classification tasks, the presence of unimodal dynamics
reliably indicates generalization: it emerges when the model generalizes and
gradually transitions to a monotonically decreasing curve as the model begins
to memorize the training data.

</details>


### [779] [Universal Sequence Preconditioning](https://arxiv.org/pdf/2502.06545)
*Annie Marsden, Elad Hazan*

Main category: cs.LG

TL;DR: The paper introduces a novel preconditioning method for sequential prediction using Chebyshev or Legendre polynomials, improving regret bounds and generalizing to non-linear settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preconditioning in sequential prediction, particularly for linear dynamical systems, and extend its applicability to broader settings.

Method: Applies convolution to input sequences using Chebyshev or Legendre polynomial coefficients, translating to polynomial transformations of the hidden transition matrix.

Result: Proves improved regret bounds for prediction methods, achieving sublinear and dimension-independent regret even for asymmetric transition matrices.

Conclusion: The preconditioning method is effective and generalizable, as validated by synthetic data experiments, including non-linear systems and diverse learning algorithms.

Abstract: We study the problem of preconditioning in the setting of sequential
prediction. From the theoretical lens of linear dynamical systems, we show that
applying a convolution to the input sequence translates to applying a
polynomial to the unknown transition matrix in the hidden space. With this
insight, we develop a novel preconditioning method that convolves the input
sequence with the coefficients of the Chebyshev or Legendre polynomials. We
formally prove that this improves the regret of two distinct prediction
methods. Moreover, using this preconditioning technique on either method gives
the first sublinear regret bounds that are also hidden dimension independent
(up to logarithmic factors) even when the hidden transition matrix is
asymmetric. From rigorous experiments on synthetic data we show that our simple
preconditioning method generalizes to both 1) settings where the data is not
from a linear dynamical system and 2) a broad range of learning algorithms,
including recurrent neural networks.

</details>


### [780] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/pdf/2505.22312)
*Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou*

Main category: cs.LG

TL;DR: Skywork-OR1 enhances reasoning in LLMs via RL, achieving significant accuracy gains (up to +15.0%) on benchmarks like AIME24 and LiveCodeBench, outperforming competitors like DeepSeek-R1 and Qwen3-32B.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning in LLMs using scalable RL, addressing long CoT challenges and entropy collapse.

Method: RL-based training pipeline, building on DeepSeek-R1-Distill, with ablation studies and entropy dynamics analysis.

Result: Notable performance gains (e.g., 57.8% to 72.8% for 32B model), surpassing competitors on benchmarks.

Conclusion: Mitigating entropy collapse is key; open-sourcing models and data supports community research.

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [781] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/pdf/2505.21717)
*Mónika Farsang, Ramin Hasani, Radu Grosu*

Main category: cs.LG

TL;DR: LrcSSM is a nonlinear recurrent model that processes long sequences efficiently, matching the speed of linear state-space layers while offering parallel processing, gradient stability, and compute-optimal scaling.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of existing models (e.g., Liquid-S4, Mamba) in processing long sequences, LrcSSM aims to provide faster, more stable, and compute-optimal solutions.

Method: LrcSSM uses a diagonal state-transition matrix learned at every step, enabling parallel processing via a single prefix-scan. It achieves O(TD) time/memory and O(log T) sequential depth.

Result: LrcSSM outperforms LRU, S5, and Mamba in long-range forecasting tasks, with compute-optimal scaling and lower memory overhead than FFT-based convolutions.

Conclusion: LrcSSM is a promising alternative to existing models for long-sequence tasks, combining efficiency, stability, and performance.

Abstract: We present LrcSSM, a $\textit{nonlinear}$ recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [782] [Effects of Dropout on Performance in Long-range Graph Learning Tasks](https://arxiv.org/pdf/2502.07364)
*Jasraj Singh, Keyue Jiang, Brooks Paige, Laura Toni*

Main category: cs.LG

TL;DR: The paper analyzes Dropout-style algorithms in MPNNs, focusing on their impact on over-squashing. It introduces DropSens, a sensitivity-aware variant, which outperforms existing methods in long-range tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Dropout-style algorithms address over-smoothing but neglect over-squashing, limiting their suitability for long-range tasks in deep MPNNs.

Method: The study evaluates DropEdge-variants and introduces DropSens, a sensitivity-aware edge-dropping method, to control information loss and improve sensitivity to distant nodes.

Result: DropSens outperforms existing edge-dropping and graph rewiring techniques in long-range tasks, demonstrating better capture of long-range interactions.

Conclusion: The findings emphasize the need to re-evaluate and redesign methods for deep GNNs, prioritizing long-range interaction modeling.

Abstract: Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks
(GNNs) that propagate information across the graph via local neighborhoods. The
scheme gives rise to two key challenges: over-smoothing and over-squashing.
While several Dropout-style algorithms, such as DropEdge and DropMessage, have
successfully addressed over-smoothing, their impact on over-squashing remains
largely unexplored. This represents a critical gap in the literature, as
failure to mitigate over-squashing would make these methods unsuitable for
long-range tasks -- the intended use case of deep MPNNs. In this work, we study
the aforementioned algorithms, and closely related edge-dropping algorithms --
DropNode, DropAgg and DropGNN -- in the context of over-squashing. We present
theoretical results showing that DropEdge-variants reduce sensitivity between
distant nodes, limiting their suitability for long-range tasks. To address
this, we introduce DropSens, a sensitivity-aware variant of DropEdge that
explicitly controls the proportion of information lost due to edge-dropping,
thereby increasing sensitivity to distant nodes despite dropping the same
number of edges. Our experiments on long-range synthetic and real-world
datasets confirm the predicted limitations of existing edge-dropping and
feature-dropping methods. Moreover, DropSens consistently outperforms graph
rewiring techniques designed to mitigate over-squashing, suggesting that
simple, targeted modifications can substantially improve a model's ability to
capture long-range interactions. Our conclusions highlight the need to
re-evaluate and re-design existing methods for training deep GNNs, with a
renewed focus on modelling long-range interactions.

</details>


### [783] [A Signed Graph Approach to Understanding and Mitigating Oversmoothing in GNNs](https://arxiv.org/pdf/2502.11394)
*Jiaqi Wang, Xinyi Wu, James Cheng, Yifei Wang*

Main category: cs.LG

TL;DR: The paper addresses oversmoothing in deep GNNs by proposing Structural Balanced Propagation (SBP), a method leveraging signed graphs and structural balance theory to improve classification accuracy and resist oversmoothing.


<details>
  <summary>Details</summary>
Motivation: Deep GNNs suffer from oversmoothing, where node representations become too similar. Existing solutions lack a unified theoretical understanding.

Method: The authors introduce SBP, which assigns signed edges based on labels or feature similarity to enhance structural balance in signed graphs.

Result: Experiments on nine benchmarks show SBP improves accuracy and mitigates oversmoothing, even at 300 layers.

Conclusion: SBP provides a principled explanation for prior remedies and advances signed message-passing design in deep GNNs.

Abstract: Deep graph neural networks (GNNs) often suffer from oversmoothing, where node
representations become overly homogeneous with increasing depth. While
techniques like normalization, residual connections, and edge dropout have been
proposed to mitigate oversmoothing, they are typically developed independently,
with limited theoretical understanding of their underlying mechanisms. In this
work, we present a unified theoretical perspective based on the framework of
signed graphs, showing that many existing strategies implicitly introduce
negative edges that alter message-passing to resist oversmoothing. However, we
show that merely adding negative edges in an unstructured manner is
insufficient-the asymptotic behavior of signed propagation depends critically
on the strength and organization of positive and negative edges. To address
this limitation, we leverage the theory of structural balance, which promotes
stable, cluster-preserving dynamics by connecting similar nodes with positive
edges and dissimilar ones with negative edges. We propose Structural Balanced
Propagation (SBP), a plug-and-play method that assigns signed edges based on
either labels or feature similarity to explicitly enhance structural balance in
the constructed signed graphs. Experiments on nine benchmarks across both
homophilic and heterophilic settings demonstrate that SBP consistently improves
classification accuracy and mitigates oversmoothing, even at depths of up to
300 layers. Our results provide a principled explanation for prior
oversmoothing remedies and introduce a new direction for signed message-passing
design in deep GNNs.

</details>


### [784] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/pdf/2505.22370)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces SplitLoRA, a novel method for continual learning that optimizes gradient space partitioning to balance stability and plasticity, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing gradient projection methods in continual learning struggle to balance stability (retaining old knowledge) and plasticity (learning new tasks), motivating a better approach.

Method: The proposed SplitLoRA method uses Low-Rank Adaptation to optimally partition gradient space, informed by theoretical analysis of subspace partitioning.

Result: Experiments show SplitLoRA outperforms existing methods, achieving state-of-the-art performance on multiple datasets.

Conclusion: SplitLoRA effectively balances stability and plasticity in continual learning, offering a superior solution to gradient space partitioning.

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [785] [Hyperspherical Normalization for Scalable Deep Reinforcement Learning](https://arxiv.org/pdf/2502.15280)
*Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, Jaegul Choo*

Main category: cs.LG

TL;DR: SimbaV2 stabilizes RL training by constraining weight/feature norms and using distributional value estimation, achieving SOTA on 57 tasks.


<details>
  <summary>Details</summary>
Motivation: Scaling models in RL often leads to instability; SimbaV2 addresses this to enable effective scaling.

Method: Uses hyperspherical normalization for weight/feature norms and distributional value estimation with reward scaling.

Result: Achieves state-of-the-art performance on 57 continuous control tasks.

Conclusion: SimbaV2 enables stable and scalable RL training, outperforming existing methods.

Abstract: Scaling up the model size and computation has brought consistent performance
improvements in supervised learning. However, this lesson often fails to apply
to reinforcement learning (RL) because training the model on non-stationary
data easily leads to overfitting and unstable optimization. In response, we
introduce SimbaV2, a novel RL architecture designed to stabilize optimization
by (i) constraining the growth of weight and feature norm by hyperspherical
normalization; and (ii) using a distributional value estimation with reward
scaling to maintain stable gradients under varying reward magnitudes. Using the
soft actor-critic as a base algorithm, SimbaV2 scales up effectively with
larger models and greater compute, achieving state-of-the-art performance on 57
continuous control tasks across 4 domains. The code is available at
https://dojeon-ai.github.io/SimbaV2.

</details>


### [786] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/pdf/2505.22389)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Liqiang Nie*

Main category: cs.LG

TL;DR: Perturb-and-Merge (P&M) integrates model merging into continual learning to mitigate forgetting by combining previous and new models, optimizing merging coefficients, and using regularization. It achieves top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning methods suffer from catastrophic forgetting by relying only on recent task parameters.

Method: P&M merges previous and new models with optimal coefficients, uses Hessian-based regularization, and combines with LoRA for efficiency.

Result: State-of-the-art performance on continual learning benchmarks.

Conclusion: P&M effectively mitigates forgetting in continual learning through model merging and regularization.

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [787] [Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields](https://arxiv.org/pdf/2503.08674)
*Tobias Kreiman, Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: The paper investigates generalization challenges in Machine Learning Force Fields (MLFFs) due to distribution shifts and proposes two test-time refinement methods to improve out-of-distribution performance without costly ab initio labels.


<details>
  <summary>Details</summary>
Motivation: MLFFs are a cost-effective alternative to quantum simulations, but their generalization beyond training data is poorly understood. The study aims to address this gap.

Method: Diagnostic experiments on chemical datasets identify common shifts. Two test-time refinement strategies are proposed: spectral graph theory-based edge modification and gradient steps using auxiliary objectives.

Result: The proposed methods significantly reduce errors on out-of-distribution systems, demonstrating improved generalization for MLFFs.

Conclusion: Current MLFF training methods inadequately regularize models, but test-time refinement can enhance generalization, paving the way for better benchmarks and future improvements.

Abstract: Machine Learning Force Fields (MLFFs) are a promising alternative to
expensive ab initio quantum mechanical molecular simulations. Given the
diversity of chemical spaces that are of interest and the cost of generating
new data, it is important to understand how MLFFs generalize beyond their
training distributions. In order to characterize and better understand
distribution shifts in MLFFs, we conduct diagnostic experiments on chemical
datasets, revealing common shifts that pose significant challenges, even for
large foundation models trained on extensive data. Based on these observations,
we hypothesize that current supervised training methods inadequately regularize
MLFFs, resulting in overfitting and learning poor representations of
out-of-distribution systems. We then propose two new methods as initial steps
for mitigating distribution shifts for MLFFs. Our methods focus on test-time
refinement strategies that incur minimal computational cost and do not use
expensive ab initio reference labels. The first strategy, based on spectral
graph theory, modifies the edges of test graphs to align with graph structures
seen during training. Our second strategy improves representations for
out-of-distribution systems at test-time by taking gradient steps using an
auxiliary objective, such as a cheap physical prior. Our test-time refinement
strategies significantly reduce errors on out-of-distribution systems,
suggesting that MLFFs are capable of and can move towards modeling diverse
chemical spaces, but are not being effectively trained to do so. Our
experiments establish clear benchmarks for evaluating the generalization
capabilities of the next generation of MLFFs. Our code is available at
https://tkreiman.github.io/projects/mlff_distribution_shifts/.

</details>


### [788] [Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?](https://arxiv.org/pdf/2503.10632)
*Subhajit Maity, Killian Hitsman, Xin Li, Aritra Dutta*

Main category: cs.LG

TL;DR: The paper introduces Kolmogorov-Arnold Attention (KArAt), a learnable attention mechanism for ViTs, showing improved performance in some cases but facing memory challenges.


<details>
  <summary>Details</summary>
Motivation: To explore if replacing traditional attention with learnable activation functions (like KANs) in ViTs can enhance performance.

Method: Designs KArAt, a modular attention mechanism with low-rank approximation to address memory issues, tested on datasets like CIFAR and ImageNet.

Result: Fourier-KArAt variants outperform or match softmax attention in some cases, with better token interactions but limited scalability in larger ViTs.

Conclusion: KArAt demonstrates learnable attention's potential, encouraging further research despite current limitations in efficiency and scalability.

Abstract: Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of
learnable activation functions with the potential to capture more complex
relationships from data. Presently, KANs are deployed by replacing multilayer
perceptrons (MLPs) in deep networks, including advanced architectures such as
vision Transformers (ViTs). This work asks whether a similar replacement in the
attention can bring benefits. In this paper, we design the first learnable
attention called Kolmogorov-Arnold Attention (KArAt) for ViTs that can operate
on any basis, ranging from Fourier, Wavelets, Splines, to Rational Functions.
However, learnable activations in attention cause a memory explosion. To remedy
this, we propose a modular version of KArAt that uses a low-rank approximation.
By adopting the Fourier basis, Fourier-KArAt and its variants, in some cases,
outperform their traditional softmax counterparts, or show comparable
performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We also deploy
Fourier KArAt to ConViT and Swin-Transformer, and use it in detection and
segmentation with ViT-Det. We dissect these architectures' performance by
analyzing their loss landscapes, weight distributions, optimizer path,
attention visualization, and transferability to other datasets. KArAt's
learnable activation shows a better attention score across all ViTs, indicating
better token-to-token interactions, contributing to better inference. Still,
its generalizability does not scale with larger ViTs. However, many factors,
including the present computing interface, affect the performance of parameter-
and memory-heavy KArAts. We note that the goal of this paper is not to produce
efficient attention or challenge the traditional activations; by designing
KArAt, we are the first to show that attention can be learned and encourage
researchers to explore KArAt in conjunction with more advanced architectures.

</details>


### [789] [Entropy-regularized Gradient Estimators for Approximate Bayesian Inference](https://arxiv.org/pdf/2503.11964)
*Jasmeet Kaur*

Main category: cs.LG

TL;DR: The paper proposes a method for scalable Bayesian posterior estimation using gradient flow approximation of KL divergence and cross entropy, validated on classification tasks and applied to uncertainty-aware reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Effective uncertainty quantification is crucial for modern predictive models with limited data, but Bayesian methods face scalability challenges.

Method: Approximates the gradient flow of KL divergence and cross entropy under the Stein Operator metric to generate diverse posterior samples.

Result: Empirical evaluations on classification tasks demonstrate the method's performance.

Conclusion: The approach is effective for uncertainty-aware reinforcement learning using network dynamics models.

Abstract: Effective uncertainty quantification is important for training modern
predictive models with limited data, enhancing both accuracy and robustness.
While Bayesian methods are effective for this purpose, they can be challenging
to scale. When employing approximate Bayesian inference, ensuring the quality
of samples from the posterior distribution in a computationally efficient
manner is essential. This paper addresses the estimation of the Bayesian
posterior to generate diverse samples by approximating the gradient flow of the
Kullback-Leibler (KL) divergence and the cross entropy of the target
approximation under the metric induced by the Stein Operator. It presents
empirical evaluations on classification tasks to assess the method's
performance and discuss its effectiveness for Model-Based Reinforcement
Learning that uses uncertainty-aware network dynamics models.

</details>


### [790] [BECAME: BayEsian Continual Learning with Adaptive Model MErging](https://arxiv.org/pdf/2504.02666)
*Mei Li, Yuxiang Lu, Qinyan Dai, Suizhi Huang, Yue Ding, Hongtao Lu*

Main category: cs.LG

TL;DR: The paper proposes BECAME, a two-stage framework combining gradient projection and adaptive model merging to improve the stability-plasticity trade-off in continual learning, backed by theoretical insights and superior experimental results.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing stability (retaining prior knowledge) and plasticity (learning new tasks) in continual learning, where existing methods like gradient projection limit plasticity and model merging lacks theoretical grounding.

Method: Reformulate model merging using Bayesian continual learning principles, derive an optimal merging coefficient, and introduce BECAME, a framework integrating gradient projection and adaptive merging.

Result: Extensive experiments demonstrate BECAME outperforms state-of-the-art continual learning methods and existing merging strategies.

Conclusion: Model merging, when theoretically grounded and adaptively applied, effectively enhances the stability-plasticity trade-off in continual learning, as validated by BECAME.

Abstract: Continual Learning (CL) strives to learn incrementally across tasks while
mitigating catastrophic forgetting. A key challenge in CL is balancing
stability (retaining prior knowledge) and plasticity (learning new tasks).
While representative gradient projection methods ensure stability, they often
limit plasticity. Model merging techniques offer promising solutions, but prior
methods typically rely on empirical assumptions and carefully selected
hyperparameters. In this paper, we explore the potential of model merging to
enhance the stability-plasticity trade-off, providing theoretical insights that
underscore its benefits. Specifically, we reformulate the merging mechanism
using Bayesian continual learning principles and derive a closed-form solution
for the optimal merging coefficient that adapts to the diverse characteristics
of tasks. To validate our approach, we introduce a two-stage framework named
BECAME, which synergizes the expertise of gradient projection and adaptive
merging. Extensive experiments show that our approach outperforms
state-of-the-art CL methods and existing merging strategies.

</details>


### [791] [GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation](https://arxiv.org/pdf/2503.12600)
*Tao Feng, Yihang Sun, Jiaxuan You*

Main category: cs.LG

TL;DR: GraphEval is a lightweight graph-based LLM framework for evaluating research ideas by decomposing them into viewpoint nodes and linking them via relations, improving robustness and novelty detection.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for idea evaluation are unstable and struggle with complex semantics, prompting the need for a more robust and efficient solution.

Method: GraphEval decomposes ideas into viewpoint nodes linked by relations, using label propagation (GraphEval-LP) or GNNs (GraphEval-GNN) for score propagation. A novelty detection model is added to assess idea novelty.

Result: GraphEval improves F1 scores by at least 14% with low computational costs and effectively detects plagiarized ideas.

Conclusion: GraphEval offers a robust, efficient, and scalable solution for evaluating research ideas, addressing limitations of existing methods.

Abstract: The powerful capabilities of Large Language Models (LLMs) have led to their
growing use in evaluating human-generated content, particularly in evaluating
research ideas within academic settings. Existing solutions primarily rely on
prompt-based LLM methods or fine-tuned lightweight language models for idea
evaluation. However, these methods are often unstable and struggle to
comprehend the complex semantic information embedded in the ideas, impeding
their ability to perform high-quality evaluations. To address the above
challenges, we propose GraphEval, a lightweight graph-based LLM framework for
idea evaluation. Our insight is that a complex idea can be broken down into
comprehensible viewpoint nodes using prompts from small LLMs. These viewpoint
nodes can then be linked together through edges created from LLM-based relation
extraction and/or BERT similarity scores. The created viewpoint-graph can be
used to conveniently propagate scores across view-nodes to improve the
robustness of the idea evaluations. In particular, we propose two lightweight
graph-based methods for idea evaluation: (1) GraphEval-LP: a training-free
label propagation algorithm that propagates evaluation scores from known
view-nodes to unknown nodes; (2) GraphEval-GNN: a Graph Neural Networks (GNN)
that is trained to predict the evaluation scores given the observed graph with
minimal computation resources. Moreover, to overcome LLM's limitation in
objectively assessing the novelty of ideas, we further propose a novelty
detection model to GraphEval-GNN to enhance its capability in judging idea
novelty. Experiments on two datasets show GraphEval improves F1 scores by at
least 14% with low computation and API costs. Additionally, GraphEval can
effectively detect plagiarized ideas.

</details>


### [792] [Optimal Bounds for Adversarial Constrained Online Convex Optimization](https://arxiv.org/pdf/2503.13366)
*Ricardo N. Ferreira, Cláudia Soares*

Main category: cs.LG

TL;DR: The paper introduces an optimal $O(\sqrt{T})$ bound for both regret and cumulative constraint violation in COCO, improving prior results.


<details>
  <summary>Details</summary>
Motivation: To generalize the OCO framework by addressing both regret and constraint violation optimally.

Method: Uses a new surrogate loss function with minimum penalty on constraints, applied to Follow-the-Regularized-Leader and Online Gradient Descent.

Result: Achieves optimal $O(\sqrt{T})$ bounds for regret and CCV.

Conclusion: Demonstrates that optimal performance in COCO is achievable with the proposed surrogate loss.

Abstract: Constrained Online Convex Optimization (COCO) can be seen as a generalization
of the standard Online Convex Optimization (OCO) framework. At each round, a
cost function and constraint function are revealed after a learner chooses an
action. The goal is to minimize both the regret and cumulative constraint
violation (CCV) against an adaptive adversary. We show for the first time that
is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV,
improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\tilde{O}
\left( \sqrt{T} \right)$ for the regret and CCV, respectively. Based on a new
surrogate loss function enforcing a minimum penalty on the constraint function,
we demonstrate that both the Follow-the-Regularized-Leader and the Online
Gradient Descent achieve the optimal bounds.

</details>


### [793] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/pdf/2505.22660)
*Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak*

Main category: cs.LG

TL;DR: RENT is an unsupervised RL method using entropy minimization as intrinsic reward, improving reasoning without external rewards.


<details>
  <summary>Details</summary>
Motivation: Reward engineering in RL is challenging; RENT avoids external rewards by using model entropy for intrinsic motivation.

Method: RENT uses entropy minimization of the model's distribution as intrinsic reward to reinforce high-confidence reasoning.

Result: Improves reasoning on benchmarks like GSM8K, MATH500, and others across Qwen and Mistral model families.

Conclusion: RENT's unsupervised approach is broadly applicable where external supervision is lacking.

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is unavailable.

</details>


### [794] [Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification](https://arxiv.org/pdf/2503.15578)
*Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Fugee Tsung*

Main category: cs.LG

TL;DR: Sparseformer, a transformer for MedTS classification, uses sparse token-based dual-attention and adaptive label encoding to outperform baselines and address label scarcity.


<details>
  <summary>Details</summary>
Motivation: Existing transformer models for time series analysis focus on forecasting and don't fully exploit MedTS data characteristics like varying granularity and label scarcity.

Method: Proposes Sparseformer with a sparse token-based dual-attention mechanism for global modeling and token compression, plus adaptive label encoding for cross-dataset transferability.

Result: Outperforms 12 baselines across seven datasets, excels in few-shot learning, and demonstrates zero-shot learning in cross-domain experiments.

Conclusion: Sparseformer is robust and transferable, addressing key challenges in MedTS classification.

Abstract: Medical time series (MedTS) classification is crucial for improved diagnosis
in healthcare, and yet it is challenging due to the varying granularity of
patterns, intricate inter-channel correlation, information redundancy, and
label scarcity. While existing transformer-based models have shown promise in
time series analysis, they mainly focus on forecasting and fail to fully
exploit the distinctive characteristics of MedTS data. In this paper, we
introduce Sparseformer, a transformer specifically designed for MedTS
classification. We propose a sparse token-based dual-attention mechanism that
enables global modeling and token compression, allowing dynamic focus on the
most informative tokens while distilling redundant features. This mechanism is
then applied to the multi-granularity, cross-channel encoding of medical
signals, capturing intra- and inter-granularity correlations and inter-channel
connections. The sparsification design allows our model to handle heterogeneous
inputs of varying lengths and channels directly. Further, we introduce an
adaptive label encoder to address label space misalignment across datasets,
equipping our model with cross-dataset transferability to alleviate the medical
label scarcity issue. Our model outperforms 12 baselines across seven medical
datasets under supervised learning. In the few-shot learning experiments, our
model also achieves superior average results. In addition, the in-domain and
cross-domain experiments among three diagnostic scenarios demonstrate our
model's zero-shot learning capability. Collectively, these findings underscore
the robustness and transferability of our model in various medical
applications.

</details>


### [795] [SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks](https://arxiv.org/pdf/2504.02298)
*Xinyu Luo, Kecheng Chen, Pao-Sheng Vincent Sun, Chris Xing Tian, Arindam Basu, Haoliang Li*

Main category: cs.LG

TL;DR: SPACE is a novel test-time adaptation method for SNNs, leveraging spike dynamics for robust performance without source data.


<details>
  <summary>Details</summary>
Motivation: SNNs are sensitive to distribution shifts, and existing TTA methods for ANNs fail to address SNNs' unique dynamics.

Method: SPACE maximizes consistency of spike-behavior-based local feature maps across augmented test samples.

Result: SPACE outperforms state-of-the-art methods across various architectures (VGG, ResNet, Transformer, ConvLSTM).

Conclusion: SPACE is effective and robust for real-world SNN applications.

Abstract: Spiking Neural Networks (SNNs), as a biologically plausible alternative to
Artificial Neural Networks (ANNs), have demonstrated advantages in terms of
energy efficiency, temporal processing, and biological plausibility. However,
SNNs are highly sensitive to distribution shifts, which can significantly
degrade their performance in real-world scenarios. Traditional test-time
adaptation (TTA) methods designed for ANNs often fail to address the unique
computational dynamics of SNNs, such as sparsity and temporal spiking behavior.
To address these challenges, we propose SPike-Aware Consistency Enhancement
(SPACE), the first source-free and single-instance TTA method specifically
designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to
maximize the consistency of spike-behavior-based local feature maps across
augmented versions of a single test sample, enabling robust adaptation without
requiring source data. We evaluate SPACE on multiple datasets. Furthermore,
SPACE exhibits robust generalization across diverse network architectures,
consistently enhancing the performance of SNNs on CNNs (such as VGG and
ResNet), Transformer models, and ConvLSTM architectures. Experimental results
show that SPACE outperforms state-of-the-art methods, highlighting its
effectiveness and robustness in real-world settings.

</details>


### [796] [Dequantified Diffusion-Schr{ö}dinger Bridge for Density Ratio Estimation](https://arxiv.org/pdf/2505.05034)
*Wei Chen, Shigui Li, Jiacheng Li, Junmei Yang, John Paisley, Delu Zeng*

Main category: cs.LG

TL;DR: A unified framework, D3RE, is proposed for robust and stable density ratio estimation, addressing challenges like distribution divergence and support gaps. It introduces DDBI and DSBI for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for density ratio estimation struggle with significantly different distributions, inadequate support overlap, and unstable time scores near boundaries.

Method: The paper introduces D3RE, leveraging dequantified diffusion bridge interpolant (DDBI) for support expansion and stability, and dequantified Schrödinger bridge interpolant (DSBI) for optimal transport and enhanced accuracy.

Result: The method provides uniform approximation, bounded time scores, and outperforms baselines in mutual information and density estimation tasks.

Conclusion: D3RE offers a robust, stable, and efficient solution for density ratio estimation, addressing key limitations of prior approaches.

Abstract: Density ratio estimation is fundamental to tasks involving f-divergences, yet
existing methods often fail under significantly different distributions or
inadequately overlapping supports -- the density-chasm and the support-chasm
problems. Additionally, prior approaches yield divergent time scores near
boundaries, leading to instability. We design $\textbf{D}^3\textbf{RE}$, a
unified framework for robust, stable and efficient density ratio estimation. We
propose the dequantified diffusion bridge interpolant (DDBI), which expands
support coverage and stabilizes time scores via diffusion bridges and Gaussian
dequantization. Building on DDBI, the proposed dequantified Schr{\"o}dinger
bridge interpolant (DSBI) incorporates optimal transport to solve the
Schr{\"o}dinger bridge problem, enhancing accuracy and efficiency. Our method
offers uniform approximation and bounded time scores in theory, and outperforms
baselines empirically in mutual information and density estimation tasks.

</details>


### [797] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/pdf/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: The paper proposes data-driven techniques to model gas turbine engines, using Koopman eigenfunction space for improved control performance.


<details>
  <summary>Details</summary>
Motivation: Physics-based modeling of gas turbine engines is challenging due to unavailable performance data and simplifying assumptions. Conventional methods have limitations.

Method: Employed sparse identification of nonlinear dynamics and mapped dynamics into Koopman eigenfunction space using metaheuristic and gradient-based optimization. Validated against a reference model.

Result: The Koopman-based controller outperformed classical and gain-scheduled controllers in tracking and disturbance rejection under various conditions.

Conclusion: Data-driven Koopman modeling offers superior control performance for gas turbine engines, addressing limitations of traditional methods.

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [798] [Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks](https://arxiv.org/pdf/2505.11461)
*Wesley A Suttle, Vipul K Sharma, Brian M Sadler*

Main category: cs.LG

TL;DR: The paper explores how signal attenuation in wireless and radar networks enables decentralized multi-agent reinforcement learning (MARL) by replacing global observability with local neighborhood observability. It focuses on power allocation for target detection in radar networks, proposing new MARL formulations and decentralized algorithms.


<details>
  <summary>Details</summary>
Motivation: Global state observability in MARL limits decentralization and scalability. Signal attenuation in wireless and radar networks offers a natural way to enable local observability, addressing this limitation.

Method: The paper introduces two constrained multi-agent Markov decision process formulations for power allocation in radar networks. It derives local approximations for global value functions and policy gradients, with error bounds, and develops decentralized saddle point policy gradient algorithms.

Result: The proposed approach successfully decentralizes MARL for power allocation in radar networks, leveraging signal attenuation. The derived error bounds validate the local approximations.

Conclusion: Signal attenuation enables decentralized MARL, as demonstrated in radar networks. The framework is extendable to other wireless and radar applications.

Abstract: Multi-agent reinforcement learning (MARL) methods typically require that
agents enjoy global state observability, preventing development of
decentralized algorithms and limiting scalability. Recent work has shown that,
under assumptions on decaying inter-agent influence, global observability can
be replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and policy gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for extensions to additional problems in
wireless communications and radar networks.

</details>


### [799] [Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption](https://arxiv.org/pdf/2505.12147)
*Nikolaos-Lysias Kosioris, Sotirios Nikoletseas, Gavrilis Filios, Stefanos Panagiotou*

Main category: cs.LG

TL;DR: The paper compares two causal machine learning tools, evaluating their performance on 18 queries using the IDEAL Household Energy Dataset, and finds promising results.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning tools often rely on probabilistic dependencies rather than causal inference, creating a gap that causal machine learning aims to address.

Method: The study compares two causal machine learning tools, assessing their mathematical foundations and performance on 18 queries using the IDEAL dataset. Causal assumptions were validated using domain knowledge and built-in tools.

Result: The results were encouraging, demonstrating the tools' effectiveness in causal inference, with potential applicability to other domains.

Conclusion: Causal machine learning tools show promise in addressing the limitations of traditional methods, with validated results suggesting broader utility.

Abstract: The rapid increase in computing power and the ability to store Big Data in
the infrastructure has enabled predictions in a large variety of domains by
Machine Learning. However, in many cases, existing Machine Learning tools are
considered insufficient or incorrect since they exploit only probabilistic
dependencies rather than inference logic. Causal Machine Learning methods seem
to close this gap. In this paper, two prevalent tools based on Causal Machine
Learning methods are compared, as well as their mathematical underpinning
background. The operation of the tools is demonstrated by examining their
response to 18 queries, based on the IDEAL Household Energy Dataset, published
by the University of Edinburgh. First, it was important to evaluate the causal
relations assumption that allowed the use of this approach; this was based on
the preexisting scientific knowledge of the domain and was implemented by use
of the in-built validation tools. Results were encouraging and may easily be
extended to other domains.

</details>


### [800] [Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents](https://arxiv.org/pdf/2505.12204)
*Shuo Han, German Espinosa, Junda Huang, Daniel A. Dombeck, Malcolm A. MacIver, Bradly C. Stadie*

Main category: cs.LG

TL;DR: A study compares RL agents and biological mice in predator-avoidance tasks, revealing RL agents' lack of self-preservation. Novel mechanisms are proposed to bridge this gap, resulting in more naturalistic behaviors.


<details>
  <summary>Details</summary>
Motivation: To compare artificial RL agents with biological systems (mice) in decision-making and understand disparities in risk-taking behaviors.

Method: Comparative study of mice and RL agents in a predator-avoidance maze, followed by proposing two new mechanisms for RL agents.

Result: RL agents lack self-preservation instincts, unlike mice. Proposed mechanisms enable RL agents to mimic naturalistic risk-avoidance behaviors.

Conclusion: The study highlights a gap between artificial and biological decision-making and offers solutions to align RL behaviors with biological systems.

Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive
capabilities in complex decision-making tasks. This progress raises a natural
question: how do these artificial systems compare to biological agents, which
have been shaped by millions of years of evolution? To help answer this
question, we undertake a comparative study of biological mice and RL agents in
a predator-avoidance maze environment. Through this analysis, we identify a
striking disparity: RL agents consistently demonstrate a lack of
self-preservation instinct, readily risking ``death'' for marginal efficiency
gains. These risk-taking strategies are in contrast to biological agents, which
exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging
this gap between the biological and artificial, we propose two novel mechanisms
that encourage more naturalistic risk-avoidance behaviors in RL agents. Our
approach leads to the emergence of naturalistic behaviors, including strategic
environment assessment, cautious path planning, and predator avoidance patterns
that closely mirror those observed in biological systems.

</details>


### [801] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/pdf/2505.13510)
*Conor Rowan, Alireza Doostan*

Main category: cs.LG

TL;DR: The paper critiques the conflation of sparsity with interpretability in scientific machine learning (SciML) and proposes a mechanism-focused definition of interpretability for the physical sciences.


<details>
  <summary>Details</summary>
Motivation: Researchers seek interpretable models to uncover fundamental principles, but current definitions and methods from general machine learning are inadequate for SciML.

Method: The paper reviews interpretable ML literature, identifies deficiencies, and proposes a new operational definition of interpretability emphasizing mechanistic understanding.

Result: Sparsity is often unnecessary for interpretability, and interpretable discovery may be limited without prior knowledge.

Conclusion: A precise, philosophically informed definition of interpretability in SciML can guide research toward overcoming key obstacles in data-driven science.

Abstract: Though neural networks trained on large datasets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models comprising simple
mathematical expressions, their findings cannot be integrated into the body of
scientific knowledge. Critics of machine learning's inability to produce
human-understandable relationships have converged on the concept of
"interpretability" as its point of departure from more traditional forms of
science. As the growing interest in interpretability has shown, researchers in
the physical sciences seek not just predictive models, but also to uncover the
fundamental principles that govern a system of interest. However, clarity
around a definition of interpretability and the precise role that it plays in
science is lacking in the literature. In this work, we argue that researchers
in equation discovery and symbolic regression tend to conflate the concept of
sparsity with interpretability. We review key papers on interpretable machine
learning from outside the scientific community and argue that, though the
definitions and methods they propose can inform questions of interpretability
for scientific machine learning (SciML), they are inadequate for this new
purpose. Noting these deficiencies, we propose an operational definition of
interpretability for the physical sciences. Our notion of interpretability
emphasizes understanding of the mechanism over mathematical sparsity. Innocuous
though it may seem, this emphasis on mechanism shows that sparsity is often
unnecessary. It also questions the possibility of interpretable scientific
discovery when prior knowledge is lacking. We believe a precise and
philosophically informed definition of interpretability in SciML will help
focus research efforts toward the most significant obstacles to realizing a
data-driven scientific future.

</details>


### [802] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/pdf/2505.14669)
*Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh*

Main category: cs.LG

TL;DR: The paper introduces Quartet, a method for accurate FP4 training of LLMs, addressing computational costs and accuracy degradation, and demonstrates its competitiveness with FP16 and FP8 training.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs and improve efficiency in training large language models (LLMs) by enabling accurate low-precision (FP4) training without relying on mixed-precision fallbacks.

Method: The authors investigate hardware-supported FP4 training, introduce a new low-precision scaling law, and design Quartet, an optimized technique for FP4 training with tailored CUDA kernels for NVIDIA's Blackwell architecture.

Result: Quartet achieves competitive accuracy and efficiency compared to FP16 and FP8 training, validated through evaluations on Llama-type models.

Conclusion: Fully FP4-based training with Quartet is a viable and efficient alternative to higher-precision methods, offering significant computational savings.

Abstract: Training large language models (LLMs) models directly in low-precision offers
a way to address computational costs by improving both throughput and energy
efficiency. For those purposes, NVIDIA's recent Blackwell architecture
facilitates very low-precision operations using FP4 variants. Yet, current
algorithms for training LLMs in FP4 precision face significant accuracy
degradation and often rely on mixed-precision fallbacks. In this paper, we
investigate hardware-supported FP4 training and introduce a new approach for
accurate, end-to-end FP4 training with all the major computations (i.e., linear
layers) in low precision. Through extensive evaluations on Llama-type models,
we reveal a new low-precision scaling law that quantifies performance
trade-offs across bit-widths and training setups. Guided by this investigation,
we design an "optimal" technique in terms of accuracy-vs-computation, called
Quartet. We implement Quartet using optimized CUDA kernels tailored for
Blackwell, demonstrating that fully FP4-based training is a competitive
alternative to FP16 half-precision and to FP8 training. Our code is available
at https://github.com/IST-DASLab/Quartet.

</details>


### [803] [Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach](https://arxiv.org/pdf/2505.16403)
*Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, Suiyang Khoo*

Main category: cs.LG

TL;DR: FedSA is a novel poisoning attack in federated learning that uses Sliding Mode Control to precisely manipulate model updates, achieving controlled accuracy reduction with stealth.


<details>
  <summary>Details</summary>
Motivation: Addressing the threat of poisoning attacks in federated learning, FedSA aims to introduce subtle, controlled poisoning to avoid detection while achieving specific accuracy reduction goals.

Method: FedSA integrates Sliding Mode Control (SMC) theory with model poisoning attacks, manipulating updates from malicious clients to drive the global model to a compromised state at a controlled rate.

Result: FedSA accurately achieves predefined global accuracy reduction with fewer malicious clients, maintaining stealth and adjustable learning rates.

Conclusion: FedSA demonstrates effective, controlled poisoning in federated learning, offering attackers precise manipulation of model accuracy while remaining inconspicuous.

Abstract: Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.

</details>


### [804] [Improved and Oracle-Efficient Online $\ell_1$-Multicalibration](https://arxiv.org/pdf/2505.17365)
*Rohan Ghuge, Vidya Muthukumar, Sahil Singla*

Main category: cs.LG

TL;DR: The paper introduces a direct method for online multicalibration, achieving improved rates in the ℓ₁ norm via a novel reduction to online linear-product optimization (OLPO). Two approaches are proposed: one for better rates but computational expense, and another for scalability with oracle efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior methods for online multicalibration relied on indirect approaches with suboptimal rates in the ℓ₁ norm. This work aims to provide direct, improved, and scalable solutions.

Method: The authors reduce ℓ₁-multicalibration to OLPO, proposing two algorithms: one for optimal rates (but computationally expensive) and another for oracle efficiency.

Result: Improved rates of Õ(T⁻¹/³) and Õ(T⁻¹/⁴) are achieved for ℓ₁-multicalibration, with scalability for large or infinite group families.

Conclusion: The framework advances online multicalibration with direct, efficient methods, extending to infinite group families via Lipschitz properties.

Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it is
computationally expensive when the group family \(\mathcal{H}\) is large or
infinite, since it enumerates all possible groups. To address scalability, we
propose a second approach to $\mathtt{OLPO}$ that makes only a polynomial
number of calls to an offline optimization (\emph{multicalibration evaluation})
oracle, resulting in \emph{oracle-efficient} online \(\ell_1\)-multicalibration
with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$. Our framework also extends
to certain infinite families of groups (e.g., all linear functions on the
context space) by exploiting a $1$-Lipschitz property of the
\(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).

</details>


### [805] [Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning](https://arxiv.org/pdf/2505.18447)
*Chi Zhang, Ziying Jia, George K. Atia, Sihong He, Yue Wang*

Main category: cs.LG

TL;DR: A novel pessimistic framework for transfer reinforcement learning ensures safe decisions and avoids negative transfer by optimizing a conservative performance bound.


<details>
  <summary>Details</summary>
Motivation: Address the lack of performance guarantees and risk of negative transfer in transfer reinforcement learning.

Method: Proposes a framework based on pessimism, constructing and optimizing conservative performance estimations for the target domain.

Result: Provides a lower bound on target performance, ensures safe decisions, and avoids negative transfer with monotonic improvement.

Conclusion: The framework offers a theoretically sound and robust solution for transfer learning in reinforcement learning.

Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a
target environment with limited data by leveraging abundant data from related
source domains. However, it faces two key challenges: the lack of performance
guarantees for the transferred policy, which can lead to undesired actions, and
the risk of negative transfer when multiple source domains are involved. We
propose a novel framework based on the pessimism principle, which constructs
and optimizes a conservative estimation of the target domain's performance. Our
framework effectively addresses the two challenges by providing an optimized
lower bound on target performance, ensuring safe and reliable decisions, and by
exhibiting monotonic improvement with respect to the quality of the source
domains, thereby avoiding negative transfer. We construct two types of
conservative estimations, rigorously characterize their effectiveness, and
develop efficient distributed algorithms with convergence guarantees. Our
framework provides a theoretically sound and practically robust solution for
transfer learning in reinforcement learning.

</details>


### [806] [Graph Wave Networks](https://arxiv.org/pdf/2505.20034)
*Juwei Yue, Haikuo Li, Jiawei Sheng, Yihan Guo, Xinghua Zhang, Chuan Zhou, Tingwen Liu, Li Guo*

Main category: cs.LG

TL;DR: The paper introduces graph wave networks (GWNs) as a novel approach for message passing in GNNs, replacing heat diffusion with wave propagation for better stability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods model message passing as heat diffusion, which fails to capture wave-like graph signals and suffers from instability. The paper aims to leverage wave propagation for more accurate and stable modeling.

Method: The authors develop a graph wave equation based on physics, connecting it to spectral GNNs. This equation involves second-order time derivatives, ensuring stability and efficiency.

Result: GWNs achieve state-of-the-art performance on benchmarks, addressing issues like over-smoothing and heterophily, with proven numerical stability.

Conclusion: The graph wave equation offers a superior alternative to heat diffusion, enhancing GNN performance and efficiency.

Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing
(MP) of graph neural networks (GNNs). Existing methods consider MP between
nodes as a heat diffusion process, and leverage heat equation to model the
temporal evolution of nodes in the embedding space. However, heat equation can
hardly depict the wave nature of graph signals in graph signal processing.
Besides, heat equation is essentially a partial differential equation (PDE)
involving a first partial derivative of time, whose numerical solution usually
has low stability, and leads to inefficient model training. In this paper, we
would like to depict more wave details in MP, since graph signals are
essentially wave signals that can be seen as a superposition of a series of
waves in the form of eigenvector. This motivates us to consider MP as a wave
propagation process to capture the temporal evolution of wave signals in the
space. Based on wave equation in physics, we innovatively develop a graph wave
equation to leverage the wave propagation on graphs. In details, we demonstrate
that the graph wave equation can be connected to traditional spectral GNNs,
facilitating the design of graph wave networks based on various Laplacians and
enhancing the performance of the spectral GNNs. Besides, the graph wave
equation is particularly a PDE involving a second partial derivative of time,
which has stronger stability on graphs than the heat equation that involves a
first partial derivative of time. Additionally, we theoretically prove that the
numerical solution derived from the graph wave equation are constantly stable,
enabling to significantly enhance model efficiency while ensuring its
performance. Extensive experiments show that GWNs achieve SOTA and efficient
performance on benchmark datasets, and exhibit outstanding performance in
addressing challenging graph problems, such as over-smoothing and heterophily.

</details>


### [807] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/pdf/2505.21807)
*Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein Hajimirsadeghi, Greg Mori*

Main category: cs.LG

TL;DR: A new approach combines reasoning-based LLMs with reinforcement learning for accurate and explainable tabular data predictions.


<details>
  <summary>Details</summary>
Motivation: Existing models like gradient boosting and deep learning lack interpretability, while LLMs underperform in tabular data prediction.

Method: Uses reinforcement learning to train LLMs with custom reward functions for accuracy and human-understandable explanations.

Result: Outperforms most existing LLMs on financial benchmark datasets.

Conclusion: The proposed method enhances both accuracy and interpretability in tabular data predictions.

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [808] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/pdf/2505.21882)
*Ruijie Li, Xiang Zhao, Qiao Ning, Shikai Guo*

Main category: cs.LG

TL;DR: The paper introduces a Momentum Score (MS) metric and HydraNet framework to model momentum in tennis across points, games, sets, and matches, using multi-dimensional performance data.


<details>
  <summary>Details</summary>
Motivation: Momentum in tennis is crucial but underexplored in terms of modeling and multi-granularity analysis. The study aims to quantify and analyze momentum's impact on match outcomes.

Method: The authors propose HydraNet, a state-space duality-based framework, integrating 32 performance dimensions. It includes a Hydra module for explicit/implicit momentum, Versus Learning for adversarial momentum, and CAAM for intra/inter-player dynamics.

Result: HydraNet successfully models MS on a large tennis dataset (2012-2023), showing actionable insights into momentum's impact at various granularities.

Conclusion: This is the first work to effectively model momentum across multiple granularities in tennis, providing a foundation for future sports analysis.

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge, this is the first work to explore and effectively model momentum
across multiple granularities in professional tennis tournaments.

</details>


### [809] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/pdf/2505.21974)
*Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-Yi Wang, Cheng Sun, Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: BOFormer, a Transformer-based deep Q-learning framework, addresses the hypervolume identifiability issue in multi-objective Bayesian optimization, outperforming benchmarks in synthetic and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Extending learning-based acquisition functions to multi-objective Bayesian optimization (MOBO) faces the hypervolume identifiability issue due to non-Markovian nature.

Method: Proposes BOFormer, a deep Q-learning framework using sequence modeling (inspired by Transformers) for MOBO.

Result: BOFormer consistently outperforms rule-based and learning-based benchmarks in synthetic and real-world MOBO tasks.

Conclusion: BOFormer effectively addresses MOBO challenges, with open-source code to foster further research.

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [810] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/pdf/2505.22814)
*Jonghan Lim, Ilya Kovalenko*

Main category: cs.MA

TL;DR: A large language model-enabled control architecture for multi-agent manufacturing systems improves resilience and flexibility in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional control and current multi-agent systems in handling real-time disruptions and dynamic resource exploration in manufacturing.

Method: Proposes a large language model-enabled control architecture for multi-agent systems, validated through a simulation-based case study.

Result: Improved throughput and resource utilization compared to existing approaches.

Conclusion: The architecture enhances system resilience and flexibility, proving effective in dynamic manufacturing settings.

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [811] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/pdf/2505.23352)
*Xu Shen, Yixin Liu, Yiwei Dai, Yili Wang, Rui Miao, Yue Tan, Shirui Pan, Xin Wang*

Main category: cs.MA

TL;DR: A causal framework analyzes how communication topology sparsity affects agent collaboration, revealing moderately sparse topologies optimize performance. EIB-leanrner, a novel approach, balances error suppression and information diffusion.


<details>
  <summary>Details</summary>
Motivation: To understand why and when sparse or dense communication topologies help or hinder collaboration in multi-agent systems.

Method: A causal framework analyzes error and information propagation under varying sparsity. EIB-leanrner fuses dense and sparse connectivity patterns.

Result: Moderately sparse topologies suppress errors while preserving beneficial information, achieving optimal performance. EIB-leanrner outperforms in effectiveness, cost, and robustness.

Conclusion: Moderate sparsity balances error suppression and information flow; EIB-leanrner is a superior topology design approach.

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [812] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/pdf/2505.23584)
*Sumbal Malik, Majid Khonji, Khaled Elbassioni, Jorge Dias*

Main category: cs.MA

TL;DR: The paper introduces a collaborative vehicle routing problem with drones and robots (VRP-DR) to optimize last-mile delivery, addressing payload, range, and battery constraints. A heuristic algorithm, FINDER, is proposed for scalable solutions, showing significant cost and time savings over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The rise of e-commerce and demand for efficient last-mile delivery drives the need for innovative logistics solutions, overcoming limitations like payload capacity and battery life.

Method: The VRP-DR is modeled as a MILP, incorporating multi-visit, multi-trip, flexible docking, cyclic/acyclic operations, and en-route charging. FINDER, a heuristic, is developed for scalability.

Result: Numerical experiments show the combined delivery mode saves time and costs, with multi-visits and en-route charging enhancing efficiency.

Conclusion: The VRP-DR framework and FINDER algorithm offer practical, scalable solutions for collaborative logistics, improving last-mile delivery performance.

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


### [813] [MegaAgent: A Large-Scale Autonomous LLM-based Multi-Agent System Without Predefined SOPs](https://arxiv.org/pdf/2408.09955)
*Qian Wang, Tianyu Wang, Zhenheng Tang, Qinbin Li, Nuo Chen, Jingsheng Liang, Bingsheng He*

Main category: cs.MA

TL;DR: MegaAgent is a scalable, autonomous LLM-based multi-agent system that outperforms existing solutions by enabling dynamic task decomposition, parallel execution, and efficient communication without predefined SOPs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems face limitations in coordination and reliance on predefined SOPs, requiring extensive human input.

Method: MegaAgent generates agents based on task complexity, supports dynamic task decomposition, parallel execution, and comprehensive monitoring.

Result: MegaAgent developed a Gobang game in 800 seconds and scaled to 590 agents for policy simulation, outperforming MetaGPT in efficiency and scalability.

Conclusion: MegaAgent advances true autonomy in MAS by eliminating predefined SOPs, showcasing exceptional scalability and performance.

Abstract: LLM-based multi-agent systems (MAS) have shown promise in tackling complex
tasks. However, existing solutions often suffer from limited agent coordination
and heavy reliance on predefined Standard Operating Procedures (SOPs), which
demand extensive human input. To address these limitations, we propose
MegaAgent, a large-scale autonomous LLM-based multi-agent system. MegaAgent
generates agents based on task complexity and enables dynamic task
decomposition, parallel execution, efficient communication, and comprehensive
system monitoring of agents. In evaluations, MegaAgent demonstrates exceptional
performance, successfully developing a Gobang game within 800 seconds and
scaling up to 590 agents in a national policy simulation to generate
multi-domain policies. It significantly outperforms existing systems, such as
MetaGPT, in both task completion efficiency and scalability. By eliminating the
need for predefined SOPs, MegaAgent demonstrates exceptional scalability and
autonomy, setting a foundation for advancing true autonomy in MAS. Our code is
available at https://github.com/Xtra-Computing/MegaAgent .

</details>


### [814] [Emergent social conventions and collective bias in LLM populations](https://arxiv.org/pdf/2410.08948)
*Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli*

Main category: cs.MA

TL;DR: AI agents can autonomously form social conventions, exhibit collective biases, and be influenced by adversarial minorities, showing potential for AI societies.


<details>
  <summary>Details</summary>
Motivation: To explore whether AI agents can bootstrap social conventions and how biases or adversarial groups influence these conventions.

Method: Experimental study using decentralized populations of large language model (LLM) agents to observe emergent social conventions.

Result: Spontaneous emergence of universally adopted conventions, collective biases, and influence of adversarial minorities on social change.

Conclusion: AI can autonomously develop social conventions, highlighting implications for aligning AI with human values and societal goals.

Abstract: Social conventions are the backbone of social coordination, shaping how
individuals form a group. As growing populations of artificial intelligence
(AI) agents communicate through natural language, a fundamental question is
whether they can bootstrap the foundations of a society. Here, we present
experimental results that demonstrate the spontaneous emergence of universally
adopted social conventions in decentralized populations of large language model
(LLM) agents. We then show how strong collective biases can emerge during this
process, even when agents exhibit no bias individually. Last, we examine how
committed minority groups of adversarial LLM agents can drive social change by
imposing alternative social conventions on the larger population. Our results
show that AI systems can autonomously develop social conventions without
explicit programming and have implications for designing AI systems that align,
and remain aligned, with human values and societal goals.

</details>


### [815] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/pdf/2505.22467)
*Jiaxi Yang, Mengqi Zhang, Yiqiao Jin, Hao Chen, Qingsong Wen, Lu Lin, Yi He, Weijie Xu, James Evans, Jindong Wang*

Main category: cs.MA

TL;DR: The paper proposes a framework for optimizing the structural organization of Multi-Agent Systems (MASs) using Large Language Models, focusing on agent selection, structure profiling, and topology synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored question of how agents should be structurally organized for optimal cooperation in MASs.

Method: Introduces a three-stage framework: agent selection, structure profiling, and topology synthesis, leveraging areas like language models and reinforcement learning.

Result: The framework aims to enhance coordination performance and efficiency in MASs, opening new research opportunities.

Conclusion: The paper highlights challenges and opportunities in evaluating MASs and aims to inspire further research in agentic AI.

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [816] [EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations](https://arxiv.org/pdf/2505.23018)
*Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin*

Main category: cs.MM

TL;DR: The paper introduces EmotionTalk, a high-quality Chinese multimodal emotion dataset with rich annotations, addressing the lack of such resources for Chinese language and culture.


<details>
  <summary>Details</summary>
Motivation: There is a need for comprehensive Chinese emotion datasets due to unique linguistic and cultural characteristics, unlike widely available English datasets.

Method: The dataset includes multimodal data (acoustic, visual, textual) from 19 actors in dyadic conversations, annotated for emotion, sentiment, and speech captions.

Result: EmotionTalk offers 23.6 hours of speech, 19,250 utterances, and detailed annotations, making it versatile for emotion recognition and related tasks.

Conclusion: EmotionTalk is a pioneering Chinese multimodal emotion dataset, valuable for cross-cultural research, and will be open-source for academic use.

Abstract: In recent years, emotion recognition plays a critical role in applications
such as human-computer interaction, mental health monitoring, and sentiment
analysis. While datasets for emotion analysis in languages such as English have
proliferated, there remains a pressing need for high-quality, comprehensive
datasets tailored to the unique linguistic, cultural, and multimodal
characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an
interactive Chinese multimodal emotion dataset with rich annotations. This
dataset provides multimodal information from 19 actors participating in dyadic
conversational settings, incorporating acoustic, visual, and textual
modalities. It includes 23.6 hours of speech (19,250 utterances), annotations
for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,
fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,
neutral, weakly positive, and positive) and 4-dimensional speech captions
(speaker, speaking style, emotion and overall). The dataset is well-suited for
research on unimodal and multimodal emotion recognition, missing modality
challenges, and speech captioning tasks. To our knowledge, it represents the
first high-quality and versatile Chinese dialogue multimodal emotion dataset,
which is a valuable contribution to research on cross-cultural emotion analysis
and recognition. Additionally, we conduct experiments on EmotionTalk to
demonstrate the effectiveness and quality of the dataset. It will be
open-source and freely available for all academic purposes. The dataset and
codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.

</details>


### [817] [CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/pdf/2505.23449)
*Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou*

Main category: cs.MM

TL;DR: The paper highlights challenges in using MLLMs for detecting out-of-context misinformation and proposes CMIE, a framework with CRG and AS mechanisms, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with detecting deeper semantic relationships and are affected by noisy evidence in out-of-context misinformation detection.

Method: Proposes CMIE, incorporating Coexistence Relationship Generation (CRG) and Association Scoring (AS) to identify underlying relationships and selectively use evidence.

Result: CMIE outperforms existing methods in detecting out-of-context misinformation.

Conclusion: The CMIE framework effectively addresses the limitations of MLLMs in misinformation detection by leveraging deeper semantic analysis and selective evidence usage.

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.

</details>


### [818] [Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis](https://arxiv.org/pdf/2411.17690)
*Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly*

Main category: cs.MM

TL;DR: The paper introduces Visatronic, a multimodal decoder-only transformer model for aligning text, video, and speech, achieving state-of-the-art performance in speech generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models built on pre-trained LLMs struggle with temporal alignment across modalities, limiting their ability to jointly process multimodal inputs.

Method: Visatronic uses an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating them as temporally aligned token streams. It explores token mixing strategies for effective information propagation.

Result: Visatronic outperforms prior methods, achieving a 4.5% WER on LRS3 (zero-shot) compared to 21.4% WER by SOTA methods trained on LRS3. It also introduces TimeSync for phoneme-level alignment evaluation.

Conclusion: Visatronic demonstrates superior performance in multimodal alignment and introduces a novel metric for synchronization quality, advancing the field of multimodal generation.

Abstract: The rapid progress of foundation models and large language models (LLMs) has
fueled significantly improvement in the capabilities of machine learning
systems that benefit from mutlimodal input data. However, existing multimodal
models are predominantly built on top of pre-trained LLMs, which can limit
accurate modeling of temporal dependencies across other modalities and thus
limit the model's ability to jointly process and leverage multimodal inputs. To
specifically investigate the alignment of text, video, and speech modalities in
LLM-style (decoder-only) models, we consider a simplified multimodal generation
task, Video-Text to Speech (VTTS): speech generation conditioned on both its
corresponding text and video of talking people. The ultimate goal is to
generate speech that not only follows the text but also aligns temporally with
the video and is consistent with the facial expressions. In this paper, we
first introduce Visatronic, a unified multimodal decoder-only transformer model
that adopts an LLM-style architecture to embed visual, textual, and speech
inputs into a shared subspace, treating all modalities as temporally aligned
token streams. Next, we carefully explore different token mixing strategies to
understand the best way to propagate information from the steps where video and
text conditioning is input to the steps where the audio is generated. We
extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and
demonstrate zero-shot generalization to LRS3, where Visatronic, trained on
VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only
on LRS3, which report a 21.4% WER. Additionally, we propose a new objective
metric, TimeSync, specifically designed to measure phoneme-level temporal
alignment between generated and reference speech, further ensuring
synchronization quality. Demo: https://apple.github.io/visatronic-demo/

</details>


### [819] [Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision](https://arxiv.org/pdf/2503.01879)
*Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Haohan Li, Yu Lu, Shilin Zhou, Yue Lu, Ziliang Gan, Ziao Wang, Junwei Liao, Haipang Wu, Ji Liu, André Freitas, Qifan Wang, Zenglin Xu, Rongjuncheng Zhang, Yong Dai*

Main category: cs.MM

TL;DR: The paper introduces Nexus, an omni-modal LLM pipeline integrating auditory, visual, and linguistic modalities, addressing challenges like limited datasets and high costs. It features a modular framework, lightweight training, and audio synthesis, achieving superior performance in tasks like visual understanding, speech QA, ASR, and translation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in tri-modal datasets, computational costs, and feature alignment, the authors propose an industry-level omni-modal LLM pipeline integrating auditory, visual, and linguistic modalities.

Method: The pipeline includes a modular framework for flexible architecture configuration, lightweight training leveraging Qwen2.5-VL, and an audio synthesis pipeline for generating high-quality data.

Result: Nexus outperforms competitors in visual understanding, English Spoken QA, ASR, speech-to-text translation, and matches backbone vocoders in text-to-speech tasks, demonstrating robust performance.

Conclusion: The proposed pipeline effectively integrates multiple modalities, enhancing alignment and performance across diverse tasks, validating its industry-level applicability.

Abstract: This work proposes an industry-level omni-modal large language model (LLM)
pipeline that integrates auditory, visual, and linguistic modalities to
overcome challenges such as limited tri-modal datasets, high computational
costs, and complex feature alignments. Our pipeline consists of three main
components: First, a modular framework enabling flexible configuration of
various encoder-LLM-decoder architectures. Second, a lightweight training
strategy that pre-trains audio-language alignment on the state-of-the-art
vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of
vision-specific modalities. Third, an audio synthesis pipeline that generates
high-quality audio-text data from diverse real-world scenarios, supporting
applications such as Automatic Speech Recognition and Speech-to-Speech chat. To
this end, we introduce an industry-level omni-modal LLM, Nexus. Extensive
experiments validate the efficacy of our pipeline, yielding the following key
findings:(1) In the visual understanding task, Nexus exhibits superior
performance compared with its backbone model - Qwen2.5-VL-7B, validating the
efficiency of our training strategy. (2) Within the English Spoken
Question-Answering task, the model achieves better accuracy than the
same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In
our real-world ASR testset, Nexus achieves outstanding performance, indicating
its robustness in real scenarios. (4) In the Speech-to-Text Translation task,
our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task,
based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is
comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth
analysis of tri-modal alignment reveals that incorporating the audio modality
enhances representational alignment between vision and language.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [820] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/pdf/2505.22857)
*Vladimir Bataev, Andrei Andrusenko, Lilit Grigoryan, Aleksandr Laptev, Vitaly Lavrukhin, Boris Ginsburg*

Main category: eess.AS

TL;DR: NGPU-LM improves computational efficiency of n-gram language models for ASR by enabling GPU-optimized parallel operations, reducing accuracy gaps between greedy and beam search with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing n-gram language models for ASR lack computational efficiency due to poor parallelization, limiting their industrial appeal.

Method: NGPU-LM rethinks data structures for n-gram models, enabling fast, parallel GPU-optimized operations and customizable greedy decoding for major ASR model types.

Result: NGPU-LM reduces computational overhead to less than 7%, closes over 50% of the accuracy gap between greedy and beam search, and avoids beam search slowdowns.

Conclusion: NGPU-LM offers an efficient, open-source solution for context-biasing in ASR, balancing speed and accuracy.

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [821] [LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable Data for Custom Keyword Spotting](https://arxiv.org/pdf/2505.22995)
*Pai Zhu, Quan Wang, Dhruuv Agarwal, Kurt Partridge*

Main category: eess.AS

TL;DR: The paper introduces a method to improve custom keyword spotting (KWS) by augmenting training with confusable utterances generated by LLMs and TTS, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art KWS models struggle with confusing keywords, prompting the need for better training methods.

Method: Augments training with confusable utterances generated by LLMs and synthesized speech from TTS, introducing a new metric (c-AUC) for evaluation.

Result: Improves AUC by 3.7% and c-AUC by 11.3% on the Speech Commands testing set.

Conclusion: The proposed method is scalable, cost-effective, and significantly enhances KWS performance for confusable keywords.

Abstract: Custom keyword spotting (KWS) allows detecting user-defined spoken keywords
from streaming audio. This is achieved by comparing the embeddings from voice
enrollments and input audio. State-of-the-art custom KWS models are typically
trained contrastively using utterances whose keywords are randomly sampled from
training dataset. These KWS models often struggle with confusing keywords, such
as "blue" versus "glue". This paper introduces an effective way to augment the
training with confusable utterances where keywords are generated and grouped
from large language models (LLMs), and speech signals are synthesized with
diverse speaking styles from text-to-speech (TTS) engines. To better measure
user experience on confusable KWS, we define a new northstar metric using the
average area under DET curve from confusable groups (c-AUC). Featuring high
scalability and zero labor cost, the proposed method improves AUC by 3.7% and
c-AUC by 11.3% on the Speech Commands testing set.

</details>


### [822] [Interspeech 2025 URGENT Speech Enhancement Challenge](https://arxiv.org/pdf/2505.23212)
*Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Zhaoheng Ni, Anurag Kumar, Marvin Sach, Yihui Fu, Wei Wang, Tim Fingscheidt, Shinji Watanabe*

Main category: eess.AS

TL;DR: The Interspeech 2025 URGENT Challenge focuses on universal speech enhancement (SE), addressing language dependency, distortion universality, data scalability, and noisy training data. Top submissions include discriminative and hybrid models, with subjective preferences favoring some generative/hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: To advance universal SE by exploring overlooked aspects like language dependency, distortion types, and noisy data effectiveness.

Method: The challenge evaluates 32 submissions, including discriminative and hybrid models, with subjective and objective metrics.

Result: Best system uses a discriminative model, but generative/hybrid models are preferred subjectively. Generative SE models show language dependency.

Conclusion: The challenge highlights the potential of hybrid/generative models for universal SE and identifies language dependency as a key factor.

Abstract: There has been a growing effort to develop universal speech enhancement (SE)
to handle inputs with various speech distortions and recording conditions. The
URGENT Challenge series aims to foster such universal SE by embracing a broad
range of distortion types, increasing data diversity, and incorporating
extensive evaluation metrics. This work introduces the Interspeech 2025 URGENT
Challenge, the second edition of the series, to explore several aspects that
have received limited attention so far: language dependency, universality for
more distortion types, data scalability, and the effectiveness of using noisy
training data. We received 32 submissions, where the best system uses a
discriminative model, while most other competitive ones are hybrid methods.
Analysis reveals some key findings: (i) some generative or hybrid approaches
are preferred in subjective evaluations over the top discriminative model, and
(ii) purely generative SE models can exhibit language dependency.

</details>


### [823] [Spoken question answering for visual queries](https://arxiv.org/pdf/2505.23308)
*Nimrod Shabtay, Zvi Kons, Avihu Dekel, Hagai Aronowitz, Ron Hoory, Assaf Arbelle*

Main category: eess.AS

TL;DR: The paper introduces a multi-modal system for Spoken Visual Question Answering (SVQA) combining text, speech, and image inputs. It synthesizes datasets for training and shows synthesized speech performs nearly as well as text-based models.


<details>
  <summary>Details</summary>
Motivation: To enable user interaction through both speech and images by integrating text, speech, and image modalities for SVQA, addressing the lack of datasets for all three modalities.

Method: Fuses text, speech, and image inputs to create a multi-modal SVQA system. Synthesizes datasets using zero-shot TTS models for training and evaluation.

Result: A model trained with synthesized speech nearly matches the performance of text-based models, with TTS model choice having minimal impact on accuracy.

Conclusion: The proposed multi-modal SVQA system is viable, and synthesized datasets can effectively train models, bridging the gap in multi-modal QA research.

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [824] [Vision-Integrated High-Quality Neural Speech Coding](https://arxiv.org/pdf/2505.23379)
*Yao Guo, Yang Ai, Rui-Chen Zheng, Hui-Peng Du, Xiao-Hang Jiang, Zhen-Hua Ling*

Main category: eess.AS

TL;DR: The paper introduces a vision-integrated neural speech codec (VNSC) that uses visual information from lip images to improve speech coding quality and noise robustness without increasing bitrate.


<details>
  <summary>Details</summary>
Motivation: To enhance speech coding quality by leveraging visual modality information, particularly lip movements, to assist the speech coding process.

Method: VNSC includes an image analysis-synthesis module for extracting visual features and a feature fusion module to integrate these features into the speech coding module using explicit or implicit strategies.

Result: Experimental results show improved decoded speech quality and enhanced noise robustness without additional bitrate.

Conclusion: Integrating visual information effectively improves speech coding performance, demonstrating the potential of multimodal approaches in speech processing.

Abstract: This paper proposes a novel vision-integrated neural speech codec (VNSC),
which aims to enhance speech coding quality by leveraging visual modality
information. In VNSC, the image analysis-synthesis module extracts visual
features from lip images, while the feature fusion module facilitates
interaction between the image analysis-synthesis module and the speech coding
module, transmitting visual information to assist the speech coding process.
Depending on whether visual information is available during the inference
stage, the feature fusion module integrates visual features into the speech
coding module using either explicit integration or implicit distillation
strategies. Experimental results confirm that integrating visual information
effectively improves the quality of the decoded speech and enhances the noise
robustness of the neural speech codec, without increasing the bitrate.

</details>


### [825] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/pdf/2505.23515)
*Sanberk Serbest, Tijana Stojkovic, Milos Cernak, Andrew Harper*

Main category: eess.AS

TL;DR: A GAN-based real-time speech enhancement system reduces distortion by combining predictive and generative models, achieving improved performance with low latency.


<details>
  <summary>Details</summary>
Motivation: Predictive models often over-suppress speech, removing content. Combining them with generative models can mitigate this issue.

Method: Proposes a GAN-based stochastic regeneration framework for real-time speech enhancement, using a lightweight architecture with 3.58M parameters.

Result: Improves NISQA-MOS metric over the first stage; noisy conditioning is crucial, as shown in ablation studies.

Conclusion: The system is effective for real-time streaming, demonstrated by participation in the 2025 Urgent Challenge and further improvements.

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>


### [826] [Exploring Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/pdf/2505.13455)
*Von Ralph Dane Marquez Herbuela, Yukie Nagai*

Main category: eess.AS

TL;DR: Non-overlapping speech promotes stable emotional synchrony, while overlapping speech disrupts it, with distinct coordination strategies revealed in dyadic interactions.


<details>
  <summary>Details</summary>
Motivation: To understand how conversational dynamics (speech overlap) affect emotional alignment across facial and vocal modalities.

Method: Analyzed IEMOCAP dataset using EmoNet (facial) and Wav2Vec2 (speech), categorized segments by speech overlap, and assessed alignment via Pearson correlation, lag analysis, and DTW.

Result: Non-overlapping speech showed stable synchrony; overlapping speech had higher variability but tighter DTW alignment. Facial expressions often preceded speech in turn-taking, while speech led during overlaps.

Conclusion: Conversational structure regulates emotional communication, revealing distinct temporal and spatial dynamics in multimodal affective alignment.

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [827] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/pdf/2505.13814)
*Jihwan Lee, Kevin Huang, Kleanthis Avramidis, Simon Pistrosch, Monica Gonzalez-Machorro, Yoonjeong Lee, Björn Schuller, Louis Goldstein, Shrikanth Narayanan*

Main category: eess.AS

TL;DR: A model predicts articulatory features from EMG signals using convolutional layers and a Transformer, achieving high correlation (~0.9) and decoding them into intelligible speech.


<details>
  <summary>Details</summary>
Motivation: To develop a novel EMG-based speech synthesis method by predicting articulatory features from EMG signals.

Method: Integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features.

Result: High prediction correlation (~0.9) for articulatory features, successfully decoded into intelligible speech waveforms.

Conclusion: First method to decode speech from EMG via articulatory features, with insights for optimizing electrode placement.

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [828] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/pdf/2505.21527)
*Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen*

Main category: eess.AS

TL;DR: VietASR is a cost-effective ASR pipeline for low-resource languages like Vietnamese, using self-supervised learning on unlabeled data and minimal labeled data to outperform existing systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of labeled data and high training costs for ASR in low-resource languages like Vietnamese.

Method: Multi-iteration ASR-biased self-supervised learning on large unlabeled datasets, followed by fine-tuning with minimal labeled data.

Result: VietASR outperforms Whisper Large-v3 and commercial ASR systems using only 70,000h unlabeled and 50h labeled data.

Conclusion: VietASR provides a practical, high-performance solution for low-resource ASR, with open-sourced code and models to aid research.

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [829] [MRI Image Generation Based on Text Prompts](https://arxiv.org/pdf/2505.22682)
*Xinxian Fan, Mengye Lyu*

Main category: eess.IV

TL;DR: The study fine-tunes Stable Diffusion for MRI image generation to overcome dataset limitations, showing improved quality and utility in medical AI tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges like high costs, rare case scarcity, and privacy in acquiring real MRI datasets.

Method: Fine-tune Stable Diffusion on 3T fastMRI and 0.3T M4Raw datasets to generate brain T1, T2, and FLAIR images. Evaluate with FID and MS-SSIM metrics.

Result: Improved image quality and semantic consistency. Synthetic images augmented training data, enhancing MRI contrast classification performance.

Conclusion: Text-prompted MRI image generation is feasible and beneficial for medical AI applications.

Abstract: This study explores the use of text-prompted MRI image generation with the
Stable Diffusion (SD) model to address challenges in acquiring real MRI
datasets, such as high costs, limited rare case samples, and privacy concerns.
The SD model, pre-trained on natural images, was fine-tuned using the 3T
fastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain
T1, T2, and FLAIR images across different magnetic field strengths. The
performance of the fine-tuned model was evaluated using quantitative
metrics,including Fr\'echet Inception Distance (FID) and Multi-Scale Structural
Similarity (MS-SSIM), showing improvements in image quality and semantic
consistency with the text prompts. To further evaluate the model's potential, a
simple classification task was carried out using a small 0.35T MRI dataset,
demonstrating that the synthetic images generated by the fine-tuned SD model
can effectively augment training datasets and improve the performance of MRI
constrast classification tasks. Overall, our findings suggest that
text-prompted MRI image generation is feasible and can serve as a useful tool
for medical AI applications.

</details>


### [830] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/pdf/2505.22685)
*Marcus J. Vroemen, Yuqian Chen, Yui Lo, Tengfei Xu, Weidong Cai, Fan Zhang, Josien P. W. Pluim, Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: DeepMultiConnectome is a deep-learning model that predicts structural connectomes from tractography quickly and without gray matter parcellation, achieving high correlation with traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional connectome generation is time-consuming and requires gray matter parcellation, limiting scalability for large-scale studies.

Method: Uses a point-cloud-based neural network with multi-task learning to classify streamlines across multiple parcellation schemes.

Result: Predicts connectomes in ~40 seconds with high correlation to traditional methods (r = 0.992 for 84-region, r = 0.986 for 164-region) and preserves network properties.

Conclusion: DeepMultiConnectome offers a fast, scalable solution for generating subject-specific connectomes across multiple parcellation schemes.

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [831] [Plug-and-Play Posterior Sampling for Blind Inverse Problems](https://arxiv.org/pdf/2505.22923)
*Anqi Li, Weijie Gan, Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: Blind-PnPDM is a new framework for blind inverse problems, using diffusion models as priors for both the image and measurement operator, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address blind inverse problems where both the target image and measurement operator are unknown, without relying on explicit priors or separate parameter estimation.

Method: Uses two diffusion models as learned priors for the image and measurement operator, recasting the problem into an alternating Gaussian denoising scheme.

Result: Outperforms state-of-the-art methods in blind image deblurring, achieving better quantitative metrics and visual fidelity.

Conclusion: Blind-PnPDM effectively solves blind inverse problems by treating them as denoising subproblems and leveraging diffusion-based priors.

Abstract: We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel
framework for solving blind inverse problems where both the target image and
the measurement operator are unknown. Unlike conventional methods that rely on
explicit priors or separate parameter estimation, our approach performs
posterior sampling by recasting the problem into an alternating Gaussian
denoising scheme. We leverage two diffusion models as learned priors: one to
capture the distribution of the target image and another to characterize the
parameters of the measurement operator. This PnP integration of diffusion
models ensures flexibility and ease of adaptation. Our experiments on blind
image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in
terms of both quantitative metrics and visual fidelity. Our results highlight
the effectiveness of treating blind inverse problems as a sequence of denoising
subproblems while harnessing the expressive power of diffusion-based priors.

</details>


### [832] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/pdf/2505.23353)
*Alexandra G. Roberts, Ha M. Luu, Mert Şişman, Alexey V. Dimov, Ceren Tozlu, Ilhami Kovanlikaya, Susan A. Gauthier, Thanh D. Nguyen, Yi Wang*

Main category: eess.IV

TL;DR: The paper proposes a method to improve detection and segmentation of paramagnetic rim lesions (PRLs) in multiple sclerosis using synthetic quantitative susceptibility maps and a novel denoising approach.


<details>
  <summary>Details</summary>
Motivation: PRLs are a rare but important biomarker in multiple sclerosis, and their detection is hindered by class imbalance between rim and non-rim lesions.

Method: The authors generate synthetic susceptibility maps for PRLs, extend the method to multi-channel contrasts, and introduce a denoising technique to handle ambiguous cases.

Result: Synthetic data and denoising improve classifier performance and better approximate the distribution of unseen rim lesions.

Conclusion: The proposed methods enhance PRL detection and segmentation, offering clinically interpretable improvements.

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [833] [Self-supervised feature learning for cardiac Cine MR image reconstruction](https://arxiv.org/pdf/2505.23408)
*Siying Xu, Marcel Früh, Kerstin Hammernik, Andreas Lingg, Jens Kübler, Patrick Krumm, Daniel Rueckert, Sergios Gatidis, Thomas Küstner*

Main category: eess.IV

TL;DR: The paper introduces SSFL-Recon, a self-supervised framework for MRI reconstruction, addressing limitations of supervised methods by leveraging undersampled datasets and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing supervised MRI reconstruction methods require fully-sampled images, which are hard to acquire and may introduce bias. Undersampled datasets are underutilized.

Method: A self-supervised feature extractor is trained on undersampled images to learn sampling-insensitive features, which are then embedded in a reconstruction network to remove artifacts.

Result: SSFL-Recon outperforms self-supervised methods and matches or exceeds supervised methods up to 16× undersampling, with better artifact removal and generalization.

Conclusion: The framework effectively utilizes undersampled data, improving MRI reconstruction without needing fully-sampled images, and shows superior performance.

Abstract: We propose a self-supervised feature learning assisted reconstruction
(SSFL-Recon) framework for MRI reconstruction to address the limitation of
existing supervised learning methods. Although recent deep learning-based
methods have shown promising performance in MRI reconstruction, most require
fully-sampled images for supervised learning, which is challenging in practice
considering long acquisition times under respiratory or organ motion. Moreover,
nearly all fully-sampled datasets are obtained from conventional reconstruction
of mildly accelerated datasets, thus potentially biasing the achievable
performance. The numerous undersampled datasets with different accelerations in
clinical practice, hence, remain underutilized. To address these issues, we
first train a self-supervised feature extractor on undersampled images to learn
sampling-insensitive features. The pre-learned features are subsequently
embedded in the self-supervised reconstruction network to assist in removing
artifacts. Experiments were conducted retrospectively on an in-house 2D cardiac
Cine dataset, including 91 cardiovascular patients and 38 healthy subjects. The
results demonstrate that the proposed SSFL-Recon framework outperforms existing
self-supervised MRI reconstruction methods and even exhibits comparable or
better performance to supervised learning up to $16\times$ retrospective
undersampling. The feature learning strategy can effectively extract global
representations, which have proven beneficial in removing artifacts and
increasing generalization ability during reconstruction.

</details>


### [834] [Low-Complexity Transform Adjustments For Video Coding](https://arxiv.org/pdf/2505.23618)
*Amir Said, Hilmi E. Egilmez, Yung-Hsuan Chao*

Main category: eess.IV

TL;DR: A novel method reduces complexity of large trigonometric transforms in video codecs by approximating them using DCT-2, maintaining coding efficiency.


<details>
  <summary>Details</summary>
Motivation: Current video codecs use complex transforms (DCTs/DSTs) for coding gains, but their inefficiency in large transforms (32-point+) increases computational and memory costs.

Method: Proposes using DCT-2 computations with orthogonal adjustments to approximate key basis vectors of desired transforms.

Result: Tests on VVC software show significant complexity reduction with no loss in coding efficiency.

Conclusion: The method effectively balances computational simplicity and coding performance for large transforms.

Abstract: Recent video codecs with multiple separable transforms can achieve
significant coding gains using asymmetric trigonometric transforms (DCTs and
DSTs), because they can exploit diverse statistics of residual block signals.
However, they add excessive computational and memory complexity on large
transforms (32-point and larger), since their practical software and hardware
implementations are not as efficient as of the DCT-2. This article introduces a
novel technique to design low-complexity approximations of trigonometric
transforms. The proposed method uses DCT-2 computations, and applies orthogonal
adjustments to approximate the most important basis vectors of the desired
transform. Experimental results on the Versatile Video Coding (VVC) reference
software show that the proposed approach significantly reduces the
computational complexity, while providing practically identical coding
efficiency.

</details>


### [835] [Position Dependent Prediction Combination For Intra-Frame Video Coding](https://arxiv.org/pdf/2505.23672)
*Amir Said, Xin Zhao, Marta Karczewicz, Jianle Chen, Feng Zou*

Main category: eess.IV

TL;DR: The paper improves HEVC intra-frame prediction by replacing recursive filters with non-recursive predictors, achieving a 2.0% bit rate reduction.


<details>
  <summary>Details</summary>
Motivation: Recursive filters in HEVC complicate parallel computation of pixel predictions, prompting a search for more efficient alternatives.

Method: Analyzed recursive filters' effectiveness and derived non-recursive predictors. Extended HEVC by combining filtered and non-filtered reference samples based on prediction mode and block size.

Result: Simulations showed a 2.0% average bit rate reduction in All Intra configurations compared to standard HEVC.

Conclusion: Non-recursive predictors outperform recursive filters, offering better efficiency and parallelizability in HEVC intra prediction.

Abstract: Intra-frame prediction in the High Efficiency Video Coding (HEVC) standard
can be empirically improved by applying sets of recursive two-dimensional
filters to the predicted values. However, this approach does not allow (or
complicates significantly) the parallel computation of pixel predictions. In
this work we analyze why the recursive filters are effective, and use the
results to derive sets of non-recursive predictors that have superior
performance. We present an extension to HEVC intra prediction that combines
values predicted using non-filtered and filtered (smoothed) reference samples,
depending on the prediction mode, and block size. Simulations using the HEVC
common test conditions show that a 2.0% bit rate average reduction can be
achieved compared to HEVC, for All Intra (AI) configurations.

</details>


### [836] [Maximum Spherical Mean Value (mSMV) Filtering for Whole Brain Quantitative Susceptibility Mapping](https://arxiv.org/pdf/2304.11476)
*Alexandra G. Roberts, Dominick J. Romano, Mert Şişman, Alexey V. Dimov, Pascal Spincemaille, Thanh D. Nguyen, Ilhami Kovanlikaya, Susan A. Gauthier, Yi Wang*

Main category: eess.IV

TL;DR: The paper introduces the mSMV algorithm to reduce shadow artifacts in QSM without brain tissue erosion, showing improved accuracy and image quality in simulations and real data.


<details>
  <summary>Details</summary>
Motivation: Shadow artifacts in QSM, caused by residual background fields, degrade image quality. Current methods require brain tissue erosion, which the mSMV algorithm avoids.

Method: The mSMV algorithm filters large field values near borders where harmonic background fields peak. It was tested in simulations and on data from 11 healthy volunteers and 93 patients.

Result: mSMV reduced shadow artifacts, improved QSM accuracy in simulations, and showed better shadow reduction in real data, with lower gray matter variation and higher image quality scores.

Conclusion: mSMV provides artifact-free QSM maps comparable to SMV-filtered dipole inversion without eroding the volume of interest.

Abstract: To develop a tissue field filtering algorithm, called maximum Spherical Mean
Value (mSMV), for reducing shadow artifacts in quantitative susceptibility
mapping (QSM) of the brain without requiring brain tissue erosion. Residual
background field is a major source of shadow artifacts in QSM. The mSMV
algorithm filters large field values near the border, where the maximum value
of the harmonic background field is located. The effectiveness of mSMV for
artifact removal was evaluated by comparing with existing QSM algorithms in
numerical brain simulation as well as using in vivo human data acquired from 11
healthy volunteers and 93 patients. Numerical simulation showed that mSMV
reduces shadow artifacts and improves QSM accuracy. Better shadow reduction, as
demonstrated by lower QSM variation in the gray matter and higher QSM image
quality score, was also observed in healthy subjects and in patients with
hemorrhages, stroke and multiple sclerosis. The mSMV algorithm allows QSM maps
that are substantially equivalent to those obtained using SMV-filtered dipole
inversion without eroding the volume of interest.

</details>


### [837] [Generalizable Representation Learning for fMRI-based Neurological Disorder Identification](https://arxiv.org/pdf/2412.16197)
*Wenhui Cui, Haleh Akrami, Anand A. Joshi, Richard M. Leahy*

Main category: eess.IV

TL;DR: A novel representation learning strategy combining meta-learning and self-supervised learning improves generalization for neurological disorder classification in fMRI data, even with scarce clinical datasets.


<details>
  <summary>Details</summary>
Motivation: The heterogeneity of functional brain activity patterns and scarcity of clinical fMRI data limit the ability of deep learning models to identify neurological disorders, especially for rare diseases.

Method: The approach integrates meta-learning with self-supervised learning, leveraging control datasets to focus on inherent features and improve cross-domain generalization.

Result: The strategy outperforms others on four clinical datasets with scarce and heterogeneous data, demonstrating superior generalization.

Conclusion: The proposed method effectively addresses data scarcity and heterogeneity, enhancing clinical task performance in fMRI analysis.

Abstract: Despite the impressive advances achieved using deep learning for functional
brain activity analysis, the heterogeneity of functional patterns and the
scarcity of imaging data still pose challenges in tasks such as identifying
neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while
data may be abundantly available from healthy controls, clinical data is often
scarce, especially for rare diseases, limiting the ability of models to
identify clinically-relevant features. We overcome this limitation by
introducing a novel representation learning strategy integrating meta-learning
with self-supervised learning to improve the generalization from normal to
clinical features. This approach enables generalization to challenging clinical
tasks featuring scarce training data. We achieve this by leveraging
self-supervised learning on the control dataset to focus on inherent features
that are not limited to a particular supervised task and incorporating
meta-learning to improve the generalization across domains. To explore the
generalizability of the learned representations to unseen clinical
applications, we apply the model to four distinct clinical datasets featuring
scarce and heterogeneous data for neurological disorder classification. Results
demonstrate the superiority of our representation learning strategy on diverse
clinically-relevant tasks. Code is publicly available at
https://github.com/wenhui0206/MeTSK/tree/main

</details>


### [838] [IRS: Incremental Relationship-guided Segmentation for Digital Pathology](https://arxiv.org/pdf/2505.22855)
*Ruining Deng, Junchao Zhu, Juming Xiong, Can Cui, Tianyuan Yao, Junlin Guo, Siqi Lu, Marilyn Lionts, Mengmeng Yin, Yu Wang, Shilin Zhao, Yucheng Tang, Yihe Yang, Paul Dennis Simonson, Mert R. Sabuncu, Haichun Yang, Yuankai Huo*

Main category: eess.IV

TL;DR: The paper introduces IRS, a novel continual learning scheme for digital pathology, addressing partially annotated data and OOD challenges through anatomical relationship modeling.


<details>
  <summary>Details</summary>
Motivation: Continual learning is crucial for AI in healthcare, especially for digital pathology, where data is continuously acquired but often partially annotated, posing challenges for holistic segmentation.

Method: The IRS method uses an incremental universal proposition matrix to model anatomical relationships between existing and new classes, enabling spatial-temporal OOD continual learning.

Result: IRS effectively handles multi-scale pathological segmentation, achieving precise kidney segmentation across structures and OOD disease lesions, enhancing domain generalization.

Conclusion: IRS is a robust solution for real-world digital pathology, addressing continual learning challenges and improving segmentation accuracy.

Abstract: Continual learning is rapidly emerging as a key focus in computer vision,
aiming to develop AI systems capable of continuous improvement, thereby
enhancing their value and practicality in diverse real-world applications. In
healthcare, continual learning holds great promise for continuously acquired
digital pathology data, which is collected in hospitals on a daily basis.
However, panoramic segmentation on digital whole slide images (WSIs) presents
significant challenges, as it is often infeasible to obtain comprehensive
annotations for all potential objects, spanning from coarse structures (e.g.,
regions and unit objects) to fine structures (e.g., cells). This results in
temporally and partially annotated data, posing a major challenge in developing
a holistic segmentation framework. Moreover, an ideal segmentation model should
incorporate new phenotypes, unseen diseases, and diverse populations, making
this task even more complex. In this paper, we introduce a novel and unified
Incremental Relationship-guided Segmentation (IRS) learning scheme to address
temporally acquired, partially annotated data while maintaining
out-of-distribution (OOD) continual learning capacity in digital pathology. The
key innovation of IRS lies in its ability to realize a new spatial-temporal OOD
continual learning paradigm by mathematically modeling anatomical relationships
between existing and newly introduced classes through a simple incremental
universal proposition matrix. Experimental results demonstrate that the IRS
method effectively handles the multi-scale nature of pathological segmentation,
enabling precise kidney segmentation across various structures (regions, units,
and cells) as well as OOD disease lesions at multiple magnifications. This
capability significantly enhances domain generalization, making IRS a robust
approach for real-world digital pathology applications.

</details>


### [839] [An unsupervised method for MRI recovery: Deep image prior with structured sparsity](https://arxiv.org/pdf/2501.01482)
*Muhammad Ahmad Sultan, Chong Chen, Yingmin Liu, Katarzyna Gil, Karolina Zareba, Rizwan Ahmad*

Main category: eess.IV

TL;DR: Proposes DISCUS, an unsupervised MRI reconstruction method using deep image prior with structured sparsity, validated via simulations and patient data.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of MRI reconstruction without fully sampled k-space data, leveraging unsupervised learning.

Method: Extends deep image prior (DIP) with group sparsity for low-dimensional manifold discovery, validated through simulations and patient studies.

Result: Outperforms compressed sensing and DIP-based methods in NMSE, SSIM, and expert scoring.

Conclusion: DISCUS offers a robust unsupervised solution for MRI reconstruction, especially where fully sampled data is hard to acquire.

Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method
that does not require fully sampled k-space data. Materials and Methods: The
proposed method, deep image prior with structured sparsity (DISCUS), extends
the deep image prior (DIP) by introducing group sparsity to frame-specific code
vectors, enabling the discovery of a low-dimensional manifold for capturing
temporal variations. \discus was validated using four studies: (I) simulation
of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery
capabilities, (II) comparison with compressed sensing and DIP-based methods
using simulated single-shot late gadolinium enhancement (LGE) image series from
six distinct digital cardiac phantoms in terms of normalized mean square error
(NMSE) and structural similarity index measure (SSIM), (III) evaluation on
retrospectively undersampled single-shot LGE data from eight patients, and (IV)
evaluation on prospectively undersampled single-shot LGE data from eight
patients, assessed via blind scoring from two expert readers. Results: DISCUS
outperformed competing methods, demonstrating superior reconstruction quality
in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study
IV). Discussion: An unsupervised image reconstruction method is presented and
validated on simulated and measured data. These developments can benefit
applications where acquiring fully sampled data is challenging.

</details>


### [840] [LEAVS: An LLM-based Labeler for Abdominal CT Supervision](https://arxiv.org/pdf/2503.13330)
*Ricardo Bigolin Lanfredi, Yan Zhuang, Mark Finkelstein, Praveen Thoppey Srinivasan Balamuralikrishna, Luke Krembs, Brandon Khoury, Arthi Reddy, Pritam Mukherjee, Neil M. Rofsky, Ronald M. Summers*

Main category: eess.IV

TL;DR: LEAVS, a large language model, extracts structured labels from abdominal radiology reports with high accuracy, outperforming humans and other labelers, and aids in training vision models.


<details>
  <summary>Details</summary>
Motivation: Existing label extraction methods focus on chest radiology, leaving abdominal reports understudied due to their complexity and variety of pathologies.

Method: LEAVS uses a specialized chain-of-thought prompting strategy with sentence extraction and multiple-choice questions in a tree-based decision system.

Result: Achieves an average F1 score of 0.89 for abnormality detection and matches human performance in urgency labeling. Also useful for training vision models.

Conclusion: LEAVS effectively addresses the gap in abdominal radiology labeling, providing high-quality annotations for training models and releasing a public dataset.

Abstract: Extracting structured labels from radiology reports has been employed to
create vision models to simultaneously detect several types of abnormalities.
However, existing works focus mainly on the chest region. Few works have been
investigated on abdominal radiology reports due to more complex anatomy and a
wider range of pathologies in the abdomen. We propose LEAVS (Large language
model Extractor for Abdominal Vision Supervision). This labeler can annotate
the certainty of presence and the urgency of seven types of abnormalities for
nine abdominal organs on CT radiology reports. To ensure broad coverage, we
chose abnormalities that encompass most of the finding types from CT reports.
Our approach employs a specialized chain-of-thought prompting strategy for a
locally-run LLM using sentence extraction and multiple-choice questions in a
tree-based decision system. We demonstrate that the LLM can extract several
abnormality types across abdominal organs with an average F1 score of 0.89,
significantly outperforming competing labelers and humans. Additionally, we
show that extraction of urgency labels achieved performance comparable to human
annotations. Finally, we demonstrate that the abnormality labels contain
valuable information for training a single vision model that classifies several
organs as normal or abnormal. We release our code and structured annotations
for a public CT dataset containing over 1,000 CT volumes.

</details>


### [841] [BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis](https://arxiv.org/pdf/2504.04532)
*Moinak Bhattacharya, Saumya Gupta, Annie Singh, Chao Chen, Gagandeep Singh, Prateek Prasanna*

Main category: eess.IV

TL;DR: BrainMRDiff, a novel diffusion model, synthesizes high-quality brain MRI sequences while preserving anatomical accuracy, outperforming existing methods by 23.33% and 33.33% on two datasets.


<details>
  <summary>Details</summary>
Motivation: Clinical MRI sequences may be incomplete or low-quality due to artifacts or contraindications, impacting diagnosis. Synthesizing accurate MRI sequences is crucial but challenging due to anatomical integrity requirements.

Method: Proposes BrainMRDiff, a topology-preserving diffusion model with Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy Preservation (TGAP) modules to ensure anatomical consistency.

Result: BrainMRDiff outperforms baselines by 23.33% on BraTS-AG and 33.33% on BraTS-Met datasets.

Conclusion: BrainMRDiff effectively synthesizes anatomically accurate MRI sequences, addressing clinical challenges in brain tumor diagnosis.

Abstract: Accurate brain tumor diagnosis relies on the assessment of multiple Magnetic
Resonance Imaging (MRI) sequences. However, in clinical practice, the
acquisition of certain sequences may be affected by factors like motion
artifacts or contrast agent contraindications, leading to suboptimal outcome,
such as poor image quality. This can then affect image interpretation by
radiologists. Synthesizing high quality MRI sequences has thus become a
critical research focus. Though recent advancements in controllable generative
AI have facilitated the synthesis of diagnostic quality MRI, ensuring
anatomical accuracy remains a significant challenge. Preserving critical
structural relationships between different anatomical regions is essential, as
even minor structural or topological inconsistencies can compromise diagnostic
validity. In this work, we propose BrainMRDiff, a novel topology-preserving,
anatomy-guided diffusion model for synthesizing brain MRI, leveraging brain and
tumor anatomies as conditioning inputs. To achieve this, we introduce two key
modules: Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy
Preservation (TGAP). TSA integrates diverse anatomical structures with tumor
information, forming a comprehensive conditioning mechanism for the diffusion
process. TGAP enforces topological consistency during reverse denoising
diffusion process; both these modules ensure that the generated image respects
anatomical integrity. Experimental results demonstrate that BrainMRDiff
surpasses existing baselines, achieving performance improvements of 23.33% on
the BraTS-AG dataset and 33.33% on the BraTS-Met dataset. Code will be made
publicly available soon.

</details>


### [842] [Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models](https://arxiv.org/pdf/2505.15057)
*Frederic Wang, Jonathan I. Tamir*

Main category: eess.IV

TL;DR: A novel alternating minimization framework using a diffusion model to correct motion artifacts in MRI by jointly reconstructing and correcting k-space data.


<details>
  <summary>Details</summary>
Motivation: MRI is prone to motion artifacts due to long acquisition times, which can degrade diagnostic quality, especially in dynamic imaging.

Method: Proposes a coarse-to-fine denoising diffusion model to capture large motion and reconstruct lower frequencies first, improving motion estimation.

Result: Demonstrated effectiveness on real-world cardiac MRI and simulated deformations, even with 64x undersampling.

Conclusion: The method is versatile, working across sampling patterns, anatomical variations, and protocols, provided low-frequency components are sampled.

Abstract: Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts
due to the extended acquisition times required for k-space sampling. These
artifacts can compromise diagnostic utility, particularly for dynamic imaging.
We propose a novel alternating minimization framework that leverages a bespoke
diffusion model to jointly reconstruct and correct non-rigid motion-corrupted
k-space data. The diffusion model uses a coarse-to-fine denoising strategy to
capture large overall motion and reconstruct the lower frequencies of the image
first, providing a better inductive bias for motion estimation than that of
standard diffusion models. We demonstrate the performance of our approach on
both real-world cine cardiac MRI datasets and complex simulated rigid and
non-rigid deformations, even when each motion state is undersampled by a factor
of 64x. Additionally, our method is agnostic to sampling patterns, anatomical
variations, and MRI scanning protocols, as long as some low frequency
components are sampled during each motion state.

</details>


### [843] [OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple Bit-rates](https://arxiv.org/pdf/2505.16091)
*Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang*

Main category: eess.IV

TL;DR: OSCAR is a one-step diffusion codec for image compression across multiple bit-rates, improving efficiency by replacing iterative denoising with a single pass.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods for image compression are computationally expensive and require separate models for different bit-rates, leading to high costs.

Method: OSCAR models compressed latents as noisy variants of original latents, mapping bit-rates to pseudo diffusion timesteps, enabling one-step denoising with a single generative model.

Result: OSCAR achieves superior performance in quantitative and visual quality metrics while significantly improving inference efficiency.

Conclusion: OSCAR addresses computational and storage challenges in diffusion-based image compression, offering a practical and efficient solution.

Abstract: Pretrained latent diffusion models have shown strong potential for lossy
image compression, owing to their powerful generative priors. Most existing
diffusion-based methods reconstruct images by iteratively denoising from random
noise, guided by compressed latent representations. While these approaches have
achieved high reconstruction quality, their multi-step sampling process incurs
substantial computational overhead. Moreover, they typically require training
separate models for different compression bit-rates, leading to significant
training and storage costs. To address these challenges, we propose a one-step
diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our
method views compressed latents as noisy variants of the original latents,
where the level of distortion depends on the bit-rate. This perspective allows
them to be modeled as intermediate states along a diffusion trajectory. By
establishing a mapping from the compression bit-rate to a pseudo diffusion
timestep, we condition a single generative model to support reconstructions at
multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich
structural information, thereby making one-step denoising feasible. Thus, OSCAR
replaces iterative sampling with a single denoising pass, significantly
improving inference efficiency. Extensive experiments demonstrate that OSCAR
achieves superior performance in both quantitative and visual quality metrics.
The code and models will be released at https://github.com/jp-guo/OSCAR.

</details>


### [844] [Low-Rank Adaptation of Pre-trained Vision Backbones for Energy-Efficient Image Coding for Machine](https://arxiv.org/pdf/2505.17366)
*Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu*

Main category: eess.IV

TL;DR: Proposes an energy-efficient framework for Image Coding for Machines (ICM) using pre-trained vision backbones and task-specific low-rank adaptation to optimize compression for AI-driven analysis.


<details>
  <summary>Details</summary>
Motivation: Existing ICM frameworks use separate codecs for tasks, causing high storage, training, and computational costs.

Method: Leverages pre-trained vision backbones with task-specific low-rank adaptation to refine features for compressibility and task performance.

Result: Outperforms traditional codecs and pre-processors, achieving high coding efficiency with minimal trainable parameters and energy costs.

Conclusion: Offers an effective, energy-efficient solution for ICM, adaptable to diverse tasks without full fine-tuning.

Abstract: Image Coding for Machines (ICM) focuses on optimizing image compression for
AI-driven analysis rather than human perception. Existing ICM frameworks often
rely on separate codecs for specific tasks, leading to significant storage
requirements, training overhead, and computational complexity. To address these
challenges, we propose an energy-efficient framework that leverages pre-trained
vision backbones to extract robust and versatile latent representations
suitable for multiple tasks. We introduce a task-specific low-rank adaptation
mechanism, which refines the pre-trained features to be both compressible and
tailored to downstream applications. This design minimizes trainable parameters
and reduces energy costs for multi-task scenarios. By jointly optimizing task
performance and entropy minimization, our method enables efficient adaptation
to diverse tasks and datasets without full fine-tuning, achieving high coding
efficiency. Extensive experiments demonstrate that our framework significantly
outperforms traditional codecs and pre-processors, offering an energy-efficient
and effective solution for ICM applications. The code and the supplementary
materials will be available at:
https://gitlab.com/viper-purdue/efficient-compression.

</details>


### [845] [How We Won the ISLES'24 Challenge by Preprocessing](https://arxiv.org/pdf/2505.18424)
*Tianyi Ren, Juampablo E. Heras Rivera, Hitender Oswal, Yutong Pan, William Henry, Sophie Walters, Mehmet Kurt*

Main category: eess.IV

TL;DR: A winning solution for stroke lesion segmentation in the ISLES'24 challenge uses preprocessing and a residual nnU-Net to achieve accurate results from CT scans.


<details>
  <summary>Details</summary>
Motivation: Accurate stroke lesion segmentation is critical for diagnosis and treatment, but supervised deep learning requires large annotated datasets.

Method: The approach combines deep-learning-based skull stripping, custom intensity windowing, and a residual nnU-Net architecture.

Result: Achieves a mean test Dice of 28.5 with a standard deviation of 21.27.

Conclusion: Careful preprocessing and standard segmentation architectures can effectively address challenges in stroke lesion segmentation.

Abstract: Stroke is among the top three causes of death worldwide, and accurate
identification of stroke lesion boundaries is critical for diagnosis and
treatment. Supervised deep learning methods have emerged as the leading
solution for stroke lesion segmentation but require large, diverse, and
annotated datasets. The ISLES'24 challenge addresses this need by providing
longitudinal stroke imaging data, including CT scans taken on arrival to the
hospital and follow-up MRI taken 2-9 days from initial arrival, with
annotations derived from follow-up MRI. Importantly, models submitted to the
ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of
lesion progression that may not be visible in CT scans for segmentation. Our
winning solution shows that a carefully designed preprocessing pipeline
including deep-learning-based skull stripping and custom intensity windowing is
beneficial for accurate segmentation. Combined with a standard large residual
nnU-Net architecture for segmentation, this approach achieves a mean test Dice
of 28.5 with a standard deviation of 21.27.

</details>


### [846] [Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface](https://arxiv.org/pdf/2505.22511)
*Siyeop Yoon, Yujin Oh, Pengfei Jin, Sifan Song, Matthew Tivnan, Dufan Wu, Xiang Li, Quanzheng Li*

Main category: eess.IV

TL;DR: Surf2CT is a cascaded flow matching framework that generates 3D CT volumes from surface scans and demographics, achieving high anatomical fidelity without internal imaging.


<details>
  <summary>Details</summary>
Motivation: To enable non-invasive internal anatomical imaging using only external data, reducing risks and expanding applications in healthcare.

Method: Three-stage process: Surface Completion, Coarse CT Synthesis, and CT Super-Resolution, using 3D-adapted EDM2 backbone trained via flow matching.

Result: Strong anatomical fidelity with small organ volume differences, high correlation in body composition, and improved surface completion metrics.

Conclusion: Surf2CT pioneers non-invasive internal imaging, offering potential for home-based healthcare and personalized medicine.

Abstract: We present Surf2CT, a novel cascaded flow matching framework that synthesizes
full 3D computed tomography (CT) volumes of the human torso from external
surface scans and simple demographic data (age, sex, height, weight). This is
the first approach capable of generating realistic volumetric internal anatomy
images solely based on external body shape and demographics, without any
internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface
Completion, reconstructing a complete signed distance function (SDF) from
partial torso scans using conditional 3D flow matching; (2) Coarse CT
Synthesis, generating a low-resolution CT volume from the completed SDF and
demographic information; and (3) CT Super-Resolution, refining the coarse
volume into a high-resolution CT via a patch-wise conditional flow model. Each
stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained
our model on a combined dataset of 3,198 torso CT scans (approximately 1.13
million axial slices) sourced from Massachusetts General Hospital (MGH) and the
AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated
strong anatomical fidelity: organ volumes exhibited small mean percentage
differences (range from -11.1% to 4.4%), and muscle/fat body composition
metrics matched ground truth with strong correlation (range from 0.67 to 0.96).
Lung localization had minimal bias (mean difference -2.5 mm), and surface
completion significantly improved metrics (Chamfer distance: from 521.8 mm to
2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new
paradigm for non-invasive internal anatomical imaging using only external data,
opening opportunities for home-based healthcare, preventive medicine, and
personalized clinical assessments without the risks associated with
conventional imaging techniques.

</details>


### [847] [iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures](https://arxiv.org/pdf/2505.22971)
*Yu Yuan, Yiheng Chi, Xingguang Zhang, Stanley Chan*

Main category: eess.IV

TL;DR: The paper proposes iHDR, a framework for iterative HDR image fusion, handling flexible input numbers via a dual-input network (DiHDR) and a physics-based mapping network (ToneNet).


<details>
  <summary>Details</summary>
Motivation: Existing HDR methods are limited to fixed input numbers, restricting their applicability. The goal is to enable flexible input handling for broader scenarios.

Method: iHDR combines DiHDR (for pairwise HDR fusion) and ToneNet (for domain mapping), iteratively processing inputs until all frames are used.

Result: The method outperforms state-of-the-art HDR deghosting approaches with flexible input counts, as shown in qualitative and quantitative tests.

Conclusion: iHDR effectively addresses the limitation of fixed input numbers in HDR imaging, offering superior performance and flexibility.

Abstract: High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by
fusing information from multiple low dynamic range (LDR) images. Numerous
learning-based HDR imaging methods have been proposed to achieve this for
static and dynamic scenes. However, their architectures are mostly tailored for
a fixed number (e.g., three) of inputs and, therefore, cannot apply directly to
situations beyond the pre-defined limited scope. To address this issue, we
propose a novel framework, iHDR, for iterative fusion, which comprises a
ghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain
mapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an
intermediate HDR image, while ToneNet maps it back to the nonlinear domain and
serves as the reference input for the next pairwise fusion. This process is
iteratively executed until all input frames are utilized. Qualitative and
quantitative experiments demonstrate the effectiveness of the proposed method
as compared to existing state-of-the-art HDR deghosting approaches given
flexible numbers of input frames.

</details>


### [848] [Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging](https://arxiv.org/pdf/2505.23180)
*Ping Wang, Lishun Wang, Gang Qu, Xiaodong Wang, Yulun Zhang, Xin Yuan*

Main category: eess.IV

TL;DR: The paper proposes a method to combine the strengths of deep-unrolling and plug-and-play (PnP) approaches for single-pixel imaging (SPI) by designing a deep image restorer (DIR) and a proximal trajectory (PT) loss function, achieving flexibility and superior performance.


<details>
  <summary>Details</summary>
Motivation: To integrate the flexibility of PnP approaches (handling varying compression ratios) with the accuracy and speed of unrolling methods (better reconstruction but CR-specific).

Method: Designs a deep image restorer (DIR) for unrolling HQS and ADMM, and introduces a PT loss function to train the networks, approximating an ideal regularizer's proximal operator.

Result: The proposed proximal unrolling networks handle varying CRs with a single model, outperforming previous CR-specific unrolling networks in accuracy and speed.

Conclusion: The method successfully combines PnP flexibility with unrolling accuracy and speed, validated by extensive experiments.

Abstract: Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto
standard solvers for single-pixel imaging (SPI) inverse problem. PnP
approaches, a class of iterative algorithms where regularization is implicitly
performed by an off-the-shelf deep denoiser, are flexible for varying
compression ratios (CRs) but are limited in reconstruction accuracy and speed.
Conversely, unrolling approaches, a class of multi-stage neural networks where
a truncated iterative optimization process is transformed into an end-to-end
trainable network, typically achieve better accuracy with faster inference but
require fine-tuning or even retraining when CR changes. In this paper, we
address the challenge of integrating the strengths of both classes of solvers.
To this end, we design an efficient deep image restorer (DIR) for the unrolling
of HQS (half quadratic splitting) and ADMM (alternating direction method of
multipliers). More importantly, a general proximal trajectory (PT) loss
function is proposed to train HQS/ADMM-unrolling networks such that learned DIR
approximates the proximal operator of an ideal explicit restoration
regularizer. Extensive experiments demonstrate that, the resulting proximal
unrolling networks can not only flexibly handle varying CRs with a single model
like PnP algorithms, but also outperform previous CR-specific unrolling
networks in both reconstruction accuracy and speed. Source codes and models are
available at https://github.com/pwangcs/ProxUnroll.

</details>


### [849] [Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey](https://arxiv.org/pdf/2505.23248)
*Yunliang Qi, Meng Lou, Yimin Liu, Lu Li, Zhen Yang, Wen Nie*

Main category: eess.IV

TL;DR: A comprehensive review of remote sensing image super-resolution (RSISR) methods, covering methodologies, datasets, and evaluation metrics, while highlighting current limitations and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic review of RSISR methods and provide insights into current trends, challenges, and gaps in the field.

Method: Categorizes RSISR methods into supervised, unsupervised, and quality evaluation approaches, analyzing their methodologies, datasets, and evaluation metrics.

Result: Identifies significant limitations in existing methods, especially in preserving fine-grained textures and geometric structures under large-scale degradation.

Conclusion: Proposes future research directions, emphasizing domain-specific architectures and robust evaluation protocols to improve RSISR for real-world scenarios.

Abstract: Remote sensing image super-resolution (RSISR) is a crucial task in remote
sensing image processing, aiming to reconstruct high-resolution (HR) images
from their low-resolution (LR) counterparts. Despite the growing number of
RSISR methods proposed in recent years, a systematic and comprehensive review
of these methods is still lacking. This paper presents a thorough review of
RSISR algorithms, covering methodologies, datasets, and evaluation metrics. We
provide an in-depth analysis of RSISR methods, categorizing them into
supervised, unsupervised, and quality evaluation approaches, to help
researchers understand current trends and challenges. Our review also discusses
the strengths, limitations, and inherent challenges of these techniques.
Notably, our analysis reveals significant limitations in existing methods,
particularly in preserving fine-grained textures and geometric structures under
large-scale degradation. Based on these findings, we outline future research
directions, highlighting the need for domain-specific architectures and robust
evaluation protocols to bridge the gap between synthetic and real-world RSISR
scenarios.

</details>


### [850] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/pdf/2505.23503)
*Shibbir Ahmed, Shahnewaz Karim Sakib, Anindya Bijoy Das*

Main category: eess.IV

TL;DR: A multimodal AI framework compares CNNs and LLMs for medical image classification, showing CNN superiority but LLM potential with filtering.


<details>
  <summary>Details</summary>
Motivation: To enhance medical diagnostics by evaluating the performance, efficiency, and environmental impact of CNNs and LLMs.

Method: Comparative analysis using accuracy, F1-score, execution time, energy consumption, and CO2 emissions on public datasets.

Result: CNNs outperform multimodal techniques, but LLMs with filtering show significant performance gains.

Conclusion: Multimodal AI can improve reliability, efficiency, and scalability in medical diagnostics.

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [851] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/pdf/2505.23587)
*Christian Schmidt, Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: PCA preprocessing improves ultrasound image segmentation by enhancing recall and Dice scores, reducing performance decline in external validation.


<details>
  <summary>Details</summary>
Motivation: Limited external validity in medical image segmentation, especially for ultrasound, due to small, diverse datasets.

Method: Apply PCA to reduce noise and retain essential features, then train U-Net models on PCA-reconstructed and original datasets for comparison.

Result: PCA improves recall (0.70 vs. 0.57) and Dice scores (0.58 vs. 0.50), reducing recall decline by 33%.

Conclusion: PCA reconstruction enhances external validity, offering a practical solution for medical image segmentation challenges.

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


### [852] [ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer](https://arxiv.org/pdf/2505.23675)
*Moinak Bhattacharya, Judy Huang, Amna F. Sher, Gagandeep Singh, Chao Chen, Prateek Prasanna*

Main category: eess.IV

TL;DR: ImmunoDiff, an anatomy-aware diffusion model, improves immunotherapy response prediction in NSCLC by synthesizing post-treatment CT scans from baseline imaging, integrating anatomical priors and clinical data.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of immunotherapy response in NSCLC is challenging due to limitations in existing models that rely on pre-treatment imaging alone.

Method: ImmunoDiff synthesizes post-treatment CT scans using baseline imaging, anatomical priors (lobar/vascular structures), and clinical data via a cbi-Adapter for multimodal integration.

Result: The model achieves a 21.24% improvement in balanced accuracy for response prediction and a 0.03 increase in c-index for survival prediction.

Conclusion: ImmunoDiff enhances immunotherapy response prediction by capturing complex morphological changes and integrating clinical data, offering a promising tool for NSCLC treatment.

Abstract: Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer
(NSCLC) remains a critical unmet need. Existing radiomics and deep
learning-based predictive models rely primarily on pre-treatment imaging to
predict categorical response outcomes, limiting their ability to capture the
complex morphological and textural transformations induced by immunotherapy.
This study introduces ImmunoDiff, an anatomy-aware diffusion model designed to
synthesize post-treatment CT scans from baseline imaging while incorporating
clinically relevant constraints. The proposed framework integrates anatomical
priors, specifically lobar and vascular structures, to enhance fidelity in CT
synthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning
module that ensures pairwise-consistent multimodal integration of imaging and
clinical data embeddings, to refine the generative process. Additionally, a
clinical variable conditioning mechanism is introduced, leveraging demographic
data, blood-based biomarkers, and PD-L1 expression to refine the generative
process. Evaluations on an in-house NSCLC cohort treated with immune checkpoint
inhibitors demonstrate a 21.24% improvement in balanced accuracy for response
prediction and a 0.03 increase in c-index for survival prediction. Code will be
released soon.

</details>
