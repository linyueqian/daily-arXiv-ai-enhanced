{"id": "2506.03530", "pdf": "https://arxiv.org/pdf/2506.03530", "abs": "https://arxiv.org/abs/2506.03530", "authors": ["Guanzhou Ke", "Yi Xie", "Xiaoli Wang", "Guoqing Chao", "Bo Wang", "Shengfeng He"], "title": "How Far Are We from Predicting Missing Modalities with Foundation Models?", "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal foundation models have demonstrated impressive capabilities across\ndiverse tasks. However, their potential as plug-and-play solutions for missing\nmodality prediction remains underexplored. To investigate this, we categorize\nexisting approaches into three representative paradigms, encompassing a total\nof 42 model variants, and conduct a comprehensive evaluation in terms of\nprediction accuracy and adaptability to downstream tasks. Our analysis reveals\nthat current foundation models often fall short in two critical aspects: (i)\nfine-grained semantic extraction from the available modalities, and (ii) robust\nvalidation of generated modalities. These limitations lead to suboptimal and,\nat times, misaligned predictions. To address these challenges, we propose an\nagentic framework tailored for missing modality prediction. This framework\ndynamically formulates modality-aware mining strategies based on the input\ncontext, facilitating the extraction of richer and more discriminative semantic\nfeatures. In addition, we introduce a \\textit{self-refinement mechanism}, which\niteratively verifies and enhances the quality of generated modalities through\ninternal feedback. Experimental results show that our method reduces FID for\nmissing image prediction by at least 14% and MER for missing text prediction by\nat least 10% compared to baselines."}
{"id": "2506.03378", "pdf": "https://arxiv.org/pdf/2506.03378", "abs": "https://arxiv.org/abs/2506.03378", "authors": ["Orchid Chetia Phukan", "Mohd Mujtaba Akhtar", "Girish", "Swarup Ranjan Behera", "Abu Osama Siddiqui", "Sarthak Jain", "Priyabrata Mallick", "Jaya Sai Kiran Patibandla", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer", "categories": ["eess.AS", "cs.CV", "cs.MM"], "comment": "Accepted to INTERSPEECH 2025", "summary": "As video-sharing platforms have grown over the past decade, child viewership\nhas surged, increasing the need for precise detection of harmful content like\nviolence or explicit scenes. Malicious users exploit moderation systems by\nembedding unsafe content in minimal frames to evade detection. While prior\nresearch has focused on visual cues and advanced such fine-grained detection,\naudio features remain underexplored. In this study, we embed audio cues with\nvisual for fine-grained child harmful content detection and introduce SNIFR, a\nnovel framework for effective alignment. SNIFR employs a transformer encoder\nfor intra-modality interaction, followed by a cascaded cross-transformer for\ninter-modality alignment. Our approach achieves superior performance over\nunimodal and baseline fusion methods, setting a new state-of-the-art."}
{"id": "2506.03594", "pdf": "https://arxiv.org/pdf/2506.03594", "abs": "https://arxiv.org/abs/2506.03594", "authors": ["Shengjie Lin", "Jiading Fang", "Muhammad Zubair Irshad", "Vitor Campagnolo Guizilini", "Rares Andrei Ambrus", "Greg Shakhnarovich", "Matthew R. Walter"], "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.MM", "cs.RO"], "comment": "https://github.com/ripl/splart", "summary": "Reconstructing articulated objects prevalent in daily environments is crucial\nfor applications in augmented/virtual reality and robotics. However, existing\nmethods face scalability limitations (requiring 3D supervision or costly\nannotations), robustness issues (being susceptible to local optima), and\nrendering shortcomings (lacking speed or photorealism). We introduce SplArt, a\nself-supervised, category-agnostic framework that leverages 3D Gaussian\nSplatting (3DGS) to reconstruct articulated objects and infer kinematics from\ntwo sets of posed RGB images captured at different articulation states,\nenabling real-time photorealistic rendering for novel viewpoints and\narticulations. SplArt augments 3DGS with a differentiable mobility parameter\nper Gaussian, achieving refined part segmentation. A multi-stage optimization\nstrategy is employed to progressively handle reconstruction, part segmentation,\nand articulation estimation, significantly enhancing robustness and accuracy.\nSplArt exploits geometric self-supervision, effectively addressing challenging\nscenarios without requiring 3D annotations or category-specific priors.\nEvaluations on established and newly proposed benchmarks, along with\napplications to real-world scenarios using a handheld RGB camera, demonstrate\nSplArt's state-of-the-art performance and real-world practicality. Code is\npublicly available at https://github.com/ripl/splart."}
{"id": "2506.03831", "pdf": "https://arxiv.org/pdf/2506.03831", "abs": "https://arxiv.org/abs/2506.03831", "authors": ["Ibrahim Ibrahimov", "Zainkó Csaba", "Gábor Gosztolya"], "title": "Conformer-based Ultrasound-to-Speech Conversion", "categories": ["cs.SD", "cs.MM", "eess.AS"], "comment": "accepted to Interspeech 2025", "summary": "Deep neural networks have shown promising potential for ultrasound-to-speech\nconversion task towards Silent Speech Interfaces. In this work, we applied two\nConformer-based DNN architectures (Base and one with bi-LSTM) for this task.\nSpeaker-specific models were trained on the data of four speakers from the\nUltrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized\nto audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN\nbaseline, objective measurements (MSE and mel cepstral distortion) showed no\nstatistically significant improvement for either model. However, a MUSHRA\nlistening test revealed that Conformer with bi-LSTM provided better perceptual\nquality, while Conformer Base matched the performance of the baseline along\nwith a 3x faster training time due to its simpler architecture. These findings\nsuggest that Conformer-based models, especially the Conformer with bi-LSTM,\noffer a promising alternative to CNNs for ultrasound-to-speech conversion."}
{"id": "2506.03550", "pdf": "https://arxiv.org/pdf/2506.03550", "abs": "https://arxiv.org/abs/2506.03550", "authors": ["Kanami Imamura", "Tomohiko Nakamura", "Norihiro Takamune", "Kohei Yatabe", "Hiroshi Saruwatari"], "title": "Local Equivariance Error-Based Metrics for Evaluating Sampling-Frequency-Independent Property of Neural Network", "categories": ["cs.SD", "eess.AS"], "comment": "5 pages, 4 figures, accepted for European Signal Processing\n  Conference 2025 (EUSIPCO 2025)", "summary": "Audio signal processing methods based on deep neural networks (DNNs) are\ntypically trained only at a single sampling frequency (SF) and therefore\nrequire signal resampling to handle untrained SFs. However, recent studies have\nshown that signal resampling can degrade performance with untrained SFs. This\nproblem has been overlooked because most studies evaluate only the performance\nat trained SFs. In this paper, to assess the robustness of DNNs to SF changes,\nwhich we refer to as the SF-independent (SFI) property, we propose three\nmetrics to quantify the SFI property on the basis of local equivariance error\n(LEE). LEE measures the robustness of DNNs to input transformations. By using\nsignal resampling as input transformation, we extend LEE to measure the\nrobustness of audio source separation methods to signal resampling. The\nproposed metrics are constructed to quantify the SFI property in specific\nnetwork components responsible for predicting time-frequency masks. Experiments\non music source separation demonstrated a strong correlation between the\nproposed metrics and performance degradation at untrained SFs."}
{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259", "abs": "https://arxiv.org/abs/2506.03259", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs."}
{"id": "2506.03162", "pdf": "https://arxiv.org/pdf/2506.03162", "abs": "https://arxiv.org/abs/2506.03162", "authors": ["Damith Chamalke Senadeera", "Xiaoyun Yang", "Dimitrios Kollias", "Gregory Slabaugh"], "title": "Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid proliferation of surveillance cameras has increased the demand for\nautomated violence detection. While CNNs and Transformers have shown success in\nextracting spatio-temporal features, they struggle with long-term dependencies\nand computational efficiency. We propose Dual Branch VideoMamba with Gated\nClass Token Fusion (GCTF), an efficient architecture combining a dual-branch\ndesign and a state-space model (SSM) backbone where one branch captures spatial\nfeatures, while the other focuses on temporal dynamics, with continuous fusion\nvia a gating mechanism. We also present a new benchmark by merging RWF-2000,\nRLVS, and VioPeru datasets in video violence detection, ensuring strict\nseparation between training and testing sets. Our model achieves\nstate-of-the-art performance on this benchmark offering an optimal balance\nbetween accuracy and computational efficiency, demonstrating the promise of\nSSMs for scalable, real-time surveillance violence detection."}
{"id": "2506.03205", "pdf": "https://arxiv.org/pdf/2506.03205", "abs": "https://arxiv.org/abs/2506.03205", "authors": ["Umberto Gonçalves de Sousa"], "title": "Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments", "categories": ["cs.AI"], "comment": "17 pages, 5 figures", "summary": "This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum\nreinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,\nwhere Q-ARDNS-Multi stands for \"Quantum Adaptive Reward-Driven Neural Simulator\n- Multi-Agent\". It integrates quantum circuits with RY gates, meta-cognitive\nadaptation, and multi-agent coordination mechanisms for complex 3D\nenvironments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action\nselection, a dual-memory system inspired by human cognition, a shared memory\nmodule for agent cooperation, and adaptive exploration strategies modulated by\nreward variance and intrinsic motivation. Evaluated in a $10 \\times 10 \\times\n3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi\nachieves success rates of 99.6\\% and 99.5\\% for Agents 0 and 1, respectively,\noutperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft\nActor-Critic (SAC) in terms of success rate, stability, navigation efficiency,\nand collision avoidance. The framework records mean rewards of $-304.2891 \\pm\n756.4636$ and $-295.7622 \\pm 752.7103$, averaging 210 steps to goal,\ndemonstrating its robustness in dynamic settings. Comprehensive analyses,\nincluding learning curves, reward distributions, statistical tests, and\ncomputational efficiency evaluations, highlight the contributions of quantum\ncircuits and meta-cognitive adaptation. By bridging quantum computing,\ncognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,\nhuman-like approach for applications in robotics, autonomous navigation, and\ndecision-making under uncertainty."}
{"id": "2506.03154", "pdf": "https://arxiv.org/pdf/2506.03154", "abs": "https://arxiv.org/abs/2506.03154", "authors": ["Zhaoyang Chen", "Cody Fleming"], "title": "Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL", "categories": ["cs.LG"], "comment": null, "summary": "Classifier free guidance has shown strong potential in diffusion-based\nreinforcement learning. However, existing methods rely on joint training of the\nguidance module and the diffusion model, which can be suboptimal during the\nearly stages when the guidance is inaccurate and provides noisy learning\nsignals. In offline RL, guidance depends solely on offline data: observations,\nactions, and rewards, and is independent of the policy module's behavior,\nsuggesting that joint training is not required. This paper proposes modular\ntraining methods that decouple the guidance module from the diffusion model,\nbased on three key findings:\n  Guidance Necessity: We explore how the effectiveness of guidance varies with\nthe training stage and algorithm choice, uncovering the roles of guidance and\ndiffusion. A lack of good guidance in the early stage presents an opportunity\nfor optimization.\n  Guidance-First Diffusion Training: We introduce a method where the guidance\nmodule is first trained independently as a value estimator, then frozen to\nguide the diffusion model using classifier-free reward guidance. This\nmodularization reduces memory usage, improves computational efficiency, and\nenhances both sample efficiency and final performance.\n  Cross-Module Transferability: Applying two independently trained guidance\nmodels, one during training and the other during inference, can significantly\nreduce normalized score variance (e.g., reducing IQR by 86%). We show that\nguidance modules trained with one algorithm (e.g., IDQL) can be directly reused\nwith another (e.g., DQL), with no additional training required, demonstrating\nbaseline-level performance as well as strong modularity and transferability.\n  We provide theoretical justification and empirical validation on bullet D4RL\nbenchmarks. Our findings suggest a new paradigm for offline RL: modular,\nreusable, and composable training pipelines."}
{"id": "2506.04215", "pdf": "https://arxiv.org/pdf/2506.04215", "abs": "https://arxiv.org/abs/2506.04215", "authors": ["Alex DeWeese", "Guannan Qu"], "title": "Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs", "categories": ["cs.MA", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are\nknown to be NEXP-Complete and intractable to solve. However, for problems such\nas cooperative navigation, obstacle avoidance, and formation control, basic\nassumptions can be made about local visibility and local dependencies. The work\nDeWeese and Qu 2024 formalized these assumptions in the construction of the\nLocally Interdependent Multi-Agent MDP. In this setting, it establishes three\nclosed-form policies that are tractable to compute in various situations and\nare exponentially close to optimal with respect to visibility. However, it is\nalso shown that these solutions can have poor performance when the visibility\nis small and fixed, often getting stuck during simulations due to the so called\n\"Penalty Jittering\" phenomenon. In this work, we establish the Extended Cutoff\nPolicy Class which is, to the best of our knowledge, the first non-trivial\nclass of near optimal closed-form partially observable policies that are\nexponentially close to optimal with respect to the visibility for any Locally\nInterdependent Multi-Agent MDP. These policies are able to remember agents\nbeyond their visibilities which allows them to perform significantly better in\nmany small and fixed visibility settings, resolve Penalty Jittering\noccurrences, and under certain circumstances guarantee fully observable joint\noptimal behavior despite the partial observability. We also propose a\ngeneralized form of the Locally Interdependent Multi-Agent MDP that allows for\ntransition dependence and extended reward dependence, then replicate our\ntheoretical results in this setting."}
{"id": "2506.03364", "pdf": "https://arxiv.org/pdf/2506.03364", "abs": "https://arxiv.org/abs/2506.03364", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Swarup Ranjan Behera", "Priyabrata Mallick", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models", "categories": ["eess.AS", "cs.MM", "cs.SD"], "comment": "Accepted to INTERSPEECH 2025", "summary": "In this work, we introduce the task of singing voice deepfake source\nattribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs)\nsuch as ImageBind, LanguageBind will be most effective for SVDSA as they are\nbetter equipped for capturing subtle source-specific characteristics-such as\nunique timbre, pitch manipulation, or synthesis artifacts of each singing voice\ndeepfake source due to their cross-modality pre-training. Our experiments with\nMMFMs, speech foundation models and music foundation models verify the\nhypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired\nfrom related research, we also explore fusion of foundation models (FMs) for\nimproved SVDSA. To this end, we propose a novel framework, COFFE which employs\nChernoff Distance as novel loss function for effective fusion of FMs. Through\nCOFFE with the symphony of MMFMs, we attain the topmost performance in\ncomparison to all the individual FMs and baseline fusion methods."}
{"id": "2506.04070", "pdf": "https://arxiv.org/pdf/2506.04070", "abs": "https://arxiv.org/abs/2506.04070", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}."}
{"id": "2506.03554", "pdf": "https://arxiv.org/pdf/2506.03554", "abs": "https://arxiv.org/abs/2506.03554", "authors": ["Reo Yoneyama", "Masaya Kawamura", "Ryo Terashima", "Ryuichi Yamamoto", "Tomoki Toda"], "title": "Comparative Analysis of Fast and High-Fidelity Neural Vocoders for Low-Latency Streaming Synthesis in Resource-Constrained Environments", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In real-time speech synthesis, neural vocoders often require low-latency\nsynthesis through causal processing and streaming. However, streaming\nintroduces inefficiencies absent in batch synthesis, such as limited\nparallelism, inter-frame dependency management, and parameter loading overhead.\nThis paper proposes multi-stream Wavehax (MS-Wavehax), an efficient neural\nvocoder for low-latency streaming, by extending the aliasing-free neural\nvocoder Wavehax with multi-stream decomposition. We analyze the\nlatency-throughput trade-off in a CPU-only environment and identify key\nbottlenecks in streaming neural vocoders. Our findings provide practical\ninsights for optimizing chunk sizes and designing vocoders tailored to specific\napplication demands and hardware constraints. Furthermore, our subjective\nevaluations show that MS-Wavehax delivers high speech quality under causal and\nnon-causal conditions while being remarkably compact and easily deployable in\nresource-constrained environments."}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268", "abs": "https://arxiv.org/abs/2506.03268", "authors": ["Cristiano Chesi"], "title": "A conclusive remark on linguistic theorizing and language modeling", "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics"}
{"id": "2506.03168", "pdf": "https://arxiv.org/pdf/2506.03168", "abs": "https://arxiv.org/abs/2506.03168", "authors": ["Dawen Jiang", "Zhishu Shen", "Qiushi Zheng", "Tiehua Zhang", "Wei Xiang", "Jiong Jin"], "title": "Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE Internet of Things Magazine", "summary": "Amid the challenges posed by global population growth and climate change,\ntraditional agricultural Internet of Things (IoT) systems is currently\nundergoing a significant digital transformation to facilitate efficient big\ndata processing. While smart agriculture utilizes artificial intelligence (AI)\ntechnologies to enable precise control, it still encounters significant\nchallenges, including excessive reliance on agricultural expert knowledge,\ndifficulties in fusing multimodal data, poor adaptability to dynamic\nenvironments, and bottlenecks in real-time decision-making at the edge. Large\nlanguage models (LLMs), with their exceptional capabilities in knowledge\nacquisition and semantic understanding, provide a promising solution to address\nthese challenges. To this end, we propose Farm-LightSeek, an edge-centric\nmultimodal agricultural IoT data analytics framework that integrates LLMs with\nedge computing. This framework collects real-time farmland multi-source data\n(images, weather, geographic information) via sensors, performs cross-modal\nreasoning and disease detection at edge nodes, conducts low-latency management\ndecisions, and enables cloud collaboration for model updates. The main\ninnovations of Farm-LightSeek include: (1) an agricultural\n\"perception-decision-action\" closed-loop architecture; (2) cross-modal adaptive\nmonitoring; and (3)a lightweight LLM deployment strategy balancing performance\nand efficiency. Experiments conducted on two real-world datasets demonstrate\nthat Farm-LightSeek consistently achieves reliable performance in\nmission-critical tasks, even under the limitations of edge computing resources.\nThis work advances intelligent real-time agricultural solutions and highlights\nthe potential for deeper integration of agricultural IoT with LLMs."}
{"id": "2506.03233", "pdf": "https://arxiv.org/pdf/2506.03233", "abs": "https://arxiv.org/abs/2506.03233", "authors": ["Andrea Ferrario"], "title": "A Trustworthiness-based Metaphysics of Artificial Intelligence Systems", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "To appear in the proceedings of 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25)", "summary": "Modern AI systems are man-made objects that leverage machine learning to\nsupport our lives across a myriad of contexts and applications. Despite\nextensive epistemological and ethical debates, their metaphysical foundations\nremain relatively under explored. The orthodox view simply suggests that AI\nsystems, as artifacts, lack well-posed identity and persistence conditions --\ntheir metaphysical kinds are no real kinds. In this work, we challenge this\nperspective by introducing a theory of metaphysical identity of AI systems. We\ndo so by characterizing their kinds and introducing identity criteria -- formal\nrules that answer the questions \"When are two AI systems the same?\" and \"When\ndoes an AI system persist, despite change?\" Building on Carrara and Vermaas'\naccount of fine-grained artifact kinds, we argue that AI trustworthiness\nprovides a lens to understand AI system kinds and formalize the identity of\nthese artifacts by relating their functional requirements to their physical\nmake-ups. The identity criteria of AI systems are determined by their\ntrustworthiness profiles -- the collection of capabilities that the systems\nmust uphold over time throughout their artifact histories, and their\neffectiveness in maintaining these capabilities. Our approach suggests that the\nidentity and persistence of AI systems is sensitive to the socio-technical\ncontext of their design and utilization via their trustworthiness, providing a\nsolid metaphysical foundation to the epistemological, ethical, and legal\ndiscussions about these artifacts."}
{"id": "2506.03155", "pdf": "https://arxiv.org/pdf/2506.03155", "abs": "https://arxiv.org/abs/2506.03155", "authors": ["Yu Zheng"], "title": "Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence has enabled a diversity of\napplications that bridge the gap between digital and physical worlds. As\nphysical environments are too complex to model through a single information\nacquisition approach, it is crucial to fuse multimodal data generated by\ndifferent sources, such as sensors, devices, systems, and people, to solve a\nproblem in the real world. Unfortunately, it is neither applicable nor\nsustainable to deploy new resources to collect original data from scratch for\nevery problem. Thus, when data is inadequate in the domain of problem, it is\nvital to fuse knowledge from multimodal data that is already available in other\ndomains. We call this cross-domain knowledge fusion. Existing research focus on\nfusing multimodal data in a single domain, supposing the knowledge from\ndifferent datasets is intrinsically aligned; however, this assumption may not\nhold in the scenarios of cross-domain knowledge fusion. In this paper, we\nformally define the cross-domain multimodal data fusion problem, discussing its\nunique challenges, differences and advantages beyond data fusion in a single\ndomain. We propose a four-layer framework, consisting of Domains, Links, Models\nand Data layers, answering three key questions: \"what to fuse\", \"why can be\nfused\", and \"how to fuse\". The Domains Layer selects relevant data from\ndifferent domains for a given problem. The Links Layer reveals the philosophy\nof knowledge alignment beyond specific model structures. The Models Layer\nprovides two knowledge fusion paradigms based on the fundamental mechanisms for\nprocessing data. The Data Layer turns data of different structures,\nresolutions, scales and distributions into a consistent representation that can\nbe fed into an AI model. With this framework, we can design end-to-end\nsolutions that fuse cross-domain multimodal data effectively for solving\nreal-world problems."}
{"id": "2506.03543", "pdf": "https://arxiv.org/pdf/2506.03543", "abs": "https://arxiv.org/abs/2506.03543", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Hanzhang Qin", "Ang Li"], "title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "categories": ["cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Current large language model (LLM) agents lack authentic human psychological\nprocesses necessary for genuine digital twins and social AI applications. To\naddress this limitation, we present a computational implementation of Global\nWorkspace Theory (GNWT) that integrates human cognitive architecture principles\ninto LLM agents, creating specialized sub-agents for emotion, memory, social\nnorms, planning, and goal-tracking coordinated through a global workspace\nmechanism. However, authentic digital twins require accurate personality\ninitialization. We therefore develop a novel adventure-based personality test\nthat evaluates true personality through behavioral choices within interactive\nscenarios, bypassing self-presentation bias found in traditional assessments.\nBuilding on these innovations, our CogniPair platform enables digital twins to\nengage in realistic simulated dating interactions and job interviews before\nreal encounters, providing bidirectional cultural fit assessment for both\nromantic compatibility and workplace matching. Validation using 551 GNWT-Agents\nand Columbia University Speed Dating dataset demonstrates 72% correlation with\nhuman attraction patterns, 77.8% match prediction accuracy, and 74% agreement\nin human validation studies. This work advances psychological authenticity in\nLLM agents and establishes a foundation for intelligent dating platforms and HR\ntechnology solutions."}
{"id": "2506.03403", "pdf": "https://arxiv.org/pdf/2506.03403", "abs": "https://arxiv.org/abs/2506.03403", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Swarup Ranjan Behera", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition", "categories": ["eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Compression-based representations (CBRs) from neural audio codecs such as\nEnCodec capture intricate acoustic features like pitch and timbre, while\nrepresentation-learning-based representations (RLRs) from pre-trained models\ntrained for speech representation learning such as WavLM encode high-level\nsemantic and prosodic information. Previous research on Speech Emotion\nRecognition (SER) has explored both, however, fusion of CBRs and RLRs haven't\nbeen explored yet. In this study, we solve this gap and investigate the fusion\nof RLRs and CBRs and hypothesize they will be more effective by providing\ncomplementary information. To this end, we propose, HYFuse, a novel framework\nthat fuses the representations by transforming them to hyperbolic space. With\nHYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the\ntop performance in comparison to individual representations as well as the\nhomogeneous fusion of RLRs and CBRs and report SOTA."}
{"id": "2506.04214", "pdf": "https://arxiv.org/pdf/2506.04214", "abs": "https://arxiv.org/abs/2506.04214", "authors": ["Tingle Li", "Baihe Huang", "Xiaobin Zhuang", "Dongya Jia", "Jiawei Chen", "Yuping Wang", "Zhuo Chen", "Gopala Anumanchipalli", "Yuxuan Wang"], "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "ICML 2025", "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/"}
{"id": "2506.03959", "pdf": "https://arxiv.org/pdf/2506.03959", "abs": "https://arxiv.org/abs/2506.03959", "authors": ["Jacob de Nobel", "Jeroen J. Briaire", "Thomas H. W. Baeck", "Anna V. Kononova", "Johan H. M. Frijns"], "title": "From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder Framework for Auditory Perception and Cochlear Implant Simulation", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "comment": "43 Pages, 11 Figures, 2 Tables", "summary": "We present NeuroVoc, a flexible model-agnostic vocoder framework that\nreconstructs acoustic waveforms from simulated neural activity patterns using\nan inverse Fourier transform. The system applies straightforward signal\nprocessing to neurogram representations, time-frequency binned outputs from\nauditory nerve fiber models. Crucially, the model architecture is modular,\nallowing for easy substitution or modification of the underlying auditory\nmodels. This flexibility eliminates the need for\nspeech-coding-strategy-specific vocoder implementations when simulating\nauditory perception in cochlear implant (CI) users. It also allows direct\ncomparisons between normal hearing (NH) and electrical hearing (EH) models, as\ndemonstrated in this study. The vocoder preserves distinctive features of each\nmodel; for example, the NH model retains harmonic structure more faithfully\nthan the EH model. We evaluated perceptual intelligibility in noise using an\nonline Digits-in-Noise (DIN) test, where participants completed three test\nconditions: one with standard speech, and two with vocoded speech using the NH\nand EH models. Both the standard DIN test and the EH-vocoded groups were\nstatistically equivalent to clinically reported data for NH and CI listeners.\nOn average, the NH and EH vocoded groups increased SRT compared to the standard\ntest by 2.4 dB and 7.1 dB, respectively. These findings show that, although\nsome degradation occurs, the vocoder can reconstruct intelligible speech under\nboth hearing models and accurately reflects the reduced speech-in-noise\nperformance experienced by CI users."}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278", "abs": "https://arxiv.org/abs/2506.03278", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ."}
{"id": "2506.03169", "pdf": "https://arxiv.org/pdf/2506.03169", "abs": "https://arxiv.org/abs/2506.03169", "authors": ["Arindam Chaudhuri"], "title": "Improvement of human health lifespan with hybrid group pose estimation methods", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human beings rely heavily on estimation of poses in order to access their\nbody movements. Human pose estimation methods take advantage of computer vision\nadvances in order to track human body movements in real life applications. This\ncomes from videos which are recorded through available devices. These\npara-digms provide potential to make human movement measurement more accessible\nto users. The consumers of pose estimation movements believe that human poses\ncontent tend to supplement available videos. This has increased pose estimation\nsoftware usage to estimate human poses. In order to address this problem, we\ndevelop hybrid-ensemble-based group pose estimation method to improve human\nhealth. This proposed hybrid-ensemble-based group pose estimation method aims\nto detect multi-person poses using modified group pose estimation and modified\nreal time pose estimation. This ensemble allows fusion of performance of stated\nmethods in real time. The input poses from images are fed into individual\nmeth-ods. The pose transformation method helps to identify relevant features\nfor en-semble to perform training effectively. After this, customized\npre-trained hybrid ensemble is trained on public benchmarked datasets which is\nbeing evaluated through test datasets. The effectiveness and viability of\nproposed method is estab-lished based on comparative analysis of group pose\nestimation methods and ex-periments conducted on benchmarked datasets. It\nprovides best optimized results in real-time pose estimation. It makes pose\nestimation method more robust to oc-clusion and improves dense regression\naccuracy. These results have affirmed po-tential application of this method in\nseveral real-time situations with improvement in human health life span"}
{"id": "2506.03315", "pdf": "https://arxiv.org/pdf/2506.03315", "abs": "https://arxiv.org/abs/2506.03315", "authors": ["Kai Sauerwald", "Kenneth Skiba", "Eduardo Fermé", "Thomas Meyer"], "title": "Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback", "categories": ["cs.AI", "cs.LO", "03E99, 91B14", "I.2.4"], "comment": null, "summary": "We study how linear orders can be employed to realise choice functions for\nwhich the set of potential choices is restricted, i.e., the possible choice is\nnot possible among the full powerset of all alternatives. In such restricted\nsettings, constructing a choice function via a relation on the alternatives is\nnot always possible. However, we show that one can always construct a choice\nfunction via a linear order on sets of alternatives, even when a fallback value\nis encoded as the minimal element in the linear order. The axiomatics of such\nchoice functions are presented for the general case and the case of\nunion-closed input restrictions. Restricted choice structures have applications\nin knowledge representation and reasoning, and here we discuss their\napplications for theory change and abstract argumentation."}
{"id": "2506.03158", "pdf": "https://arxiv.org/pdf/2506.03158", "abs": "https://arxiv.org/abs/2506.03158", "authors": ["Jiahao Qin", "Bei Peng", "Feng Liu", "Guangliang Cheng", "Lu Zong"], "title": "DUAL: Dynamic Uncertainty-Aware Learning", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 3 figures", "summary": "Deep learning models frequently encounter feature uncertainty in diverse\nlearning scenarios, significantly impacting their performance and reliability.\nThis challenge is particularly complex in multi-modal scenarios, where models\nmust integrate information from different sources with inherent uncertainties.\nWe propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that\neffectively handles feature uncertainty in both single-modal and multi-modal\nscenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty\nModeling, which continuously refines uncertainty estimates through joint\nconsideration of feature characteristics and learning dynamics; Adaptive\nDistribution-Aware Modulation, which maintains balanced feature distributions\nthrough dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal\nRelationship Learning, which explicitly models uncertainties in cross-modal\ninteractions. Through extensive experiments, we demonstrate DUAL's\neffectiveness across multiple domains: in computer vision tasks, it achieves\nsubstantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on\nCIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it\ndemonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy\non CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements\non MISR. The code will be available on GitHub soon."}
{"id": "2506.03546", "pdf": "https://arxiv.org/pdf/2506.03546", "abs": "https://arxiv.org/abs/2506.03546", "authors": ["Yuanchen Bai", "Zijian Ding", "Angelique Taylor"], "title": "From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Advancements in generative models have enabled multi-agent systems (MAS) to\nperform complex virtual tasks such as writing and code generation, which do not\ngeneralize well to physical multi-agent robotic teams. Current frameworks often\ntreat agents as conceptual task executors rather than physically embodied\nentities, and overlook critical real-world constraints such as spatial context,\nrobotic capabilities (e.g., sensing and navigation). To probe this gap, we\nreconfigure and stress-test a hierarchical multi-agent robotic team built on\nthe CrewAI framework in a simulated emergency department onboarding scenario.\nWe identify five persistent failure modes: role misalignment; tool access\nviolations; lack of in-time handling of failure reports; noncompliance with\nprescribed workflows; bypassing or false reporting of task completion. Based on\nthis analysis, we propose three design guidelines emphasizing process\ntransparency, proactive failure recovery, and contextual grounding. Our work\ninforms the development of more resilient and robust multi-agent robotic\nsystems (MARS), including opportunities to extend virtual multi-agent\nframeworks to the real world."}
{"id": "2506.03425", "pdf": "https://arxiv.org/pdf/2506.03425", "abs": "https://arxiv.org/abs/2506.03425", "authors": ["Petr Grinberg", "Ankur Kumar", "Surya Koppisetti", "Gaurav Bharaj"], "title": "A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": "5 pages, 3 figures, accepted at Interspeech 2025", "summary": "Evaluating explainability techniques, such as SHAP and LRP, in the context of\naudio deepfake detection is challenging due to lack of clear ground truth\nannotations. In the cases when we are able to obtain the ground truth, we find\nthat these methods struggle to provide accurate explanations. In this work, we\npropose a novel data-driven approach to identify artifact regions in deepfake\naudio. We consider paired real and vocoded audio, and use the difference in\ntime-frequency representation as the ground-truth explanation. The difference\nsignal then serves as a supervision to train a diffusion model to expose the\ndeepfake artifacts in a given vocoded audio. Experimental results on the VocV4\nand LibriSeVoc datasets demonstrate that our method outperforms traditional\nexplainability techniques, both qualitatively and quantitatively."}
{"id": "2411.16331", "pdf": "https://arxiv.org/pdf/2411.16331", "abs": "https://arxiv.org/abs/2411.16331", "authors": ["Xiaozhong Ji", "Xiaobin Hu", "Zhihong Xu", "Junwei Zhu", "Chuming Lin", "Qingdong He", "Jiangning Zhang", "Donghao Luo", "Yi Chen", "Qin Lin", "Qinglin Lu", "Chengjie Wang"], "title": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.SD", "eess.AS"], "comment": "refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}", "summary": "The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity."}
{"id": "2506.04013", "pdf": "https://arxiv.org/pdf/2506.04013", "abs": "https://arxiv.org/abs/2506.04013", "authors": ["Seymanur Akti", "Tuan Nam Nguyen", "Alexander Waibel"], "title": "Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Expressive voice conversion aims to transfer both speaker identity and\nexpressive attributes from a target speech to a given source speech. In this\nwork, we improve over a self-supervised, non-autoregressive framework with a\nconditional variational autoencoder, focusing on reducing source timbre leakage\nand improving linguistic-acoustic disentanglement for better style transfer. To\nminimize style leakage, we use multilingual discrete speech units for content\nrepresentation and reinforce embeddings with augmentation-based similarity loss\nand mix-style layer normalization. To enhance expressivity transfer, we\nincorporate local F0 information via cross-attention and extract style\nembeddings enriched with global pitch and energy features. Experiments show our\nmodel outperforms baselines in emotion and speaker similarity, demonstrating\nsuperior style adaptation and reduced source style leakage."}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292", "abs": "https://arxiv.org/abs/2506.03292", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting."}
{"id": "2506.03170", "pdf": "https://arxiv.org/pdf/2506.03170", "abs": "https://arxiv.org/abs/2506.03170", "authors": ["Murthy L", "Subarna Tripathi"], "title": "PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n$100\\%$ attribution accuracy. However, any model with less than \\emph{perfect}\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory."}
{"id": "2506.03332", "pdf": "https://arxiv.org/pdf/2506.03332", "abs": "https://arxiv.org/abs/2506.03332", "authors": ["Yifei Ming", "Zixuan Ke", "Xuan-Phi Nguyen", "Jiayu Wang", "Shafiq Joty"], "title": "Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows", "categories": ["cs.AI"], "comment": null, "summary": "Agentic workflows -- where multiple large language model (LLM) instances\ninteract to solve tasks -- are increasingly built on feedback mechanisms, where\none model evaluates and critiques another. Despite the promise of\nfeedback-driven improvement, the stability of agentic workflows rests on the\nreliability of the judge. However, judges may hallucinate information, exhibit\nbias, or act adversarially -- introducing critical vulnerabilities into the\nworkflow. In this work, we present a systematic analysis of agentic workflows\nunder deceptive or misleading feedback. We introduce a two-dimensional\nframework for analyzing judge behavior, along axes of intent (from constructive\nto malicious) and knowledge (from parametric-only to retrieval-augmented\nsystems). Using this taxonomy, we construct a suite of judge behaviors and\ndevelop WAFER-QA, a new benchmark with critiques grounded in retrieved web\nevidence to evaluate robustness of agentic workflows against factually\nsupported adversarial feedback. We reveal that even strongest agents are\nvulnerable to persuasive yet flawed critiques -- often switching correct\nanswers after a single round of misleading feedback. Taking a step further, we\nstudy how model predictions evolve over multiple rounds of interaction,\nrevealing distinct behavioral patterns between reasoning and non-reasoning\nmodels. Our findings highlight fundamental vulnerabilities in feedback-based\nworkflows and offer guidance for building more robust agentic systems."}
{"id": "2506.03159", "pdf": "https://arxiv.org/pdf/2506.03159", "abs": "https://arxiv.org/abs/2506.03159", "authors": ["Lesley Wheat", "Martin v. Mohrenschildt", "Saeid Habibi"], "title": "Bayes Error Rate Estimation in Difficult Situations", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "21 pages, 13 figures, 20 tables", "summary": "The Bayes Error Rate (BER) is the fundamental limit on the achievable\ngeneralizable classification accuracy of any machine learning model due to\ninherent uncertainty within the data. BER estimators offer insight into the\ndifficulty of any classification problem and set expectations for optimal\nclassification performance. In order to be useful, the estimators must also be\naccurate with a limited number of samples on multivariate problems with unknown\nclass distributions. To determine which estimators meet the minimum\nrequirements for \"usefulness\", an in-depth examination of their accuracy is\nconducted using Monte Carlo simulations with synthetic data in order to obtain\ntheir confidence bounds for binary classification. To examine the usability of\nthe estimators on real-world applications, new test scenarios are introduced\nupon which 2500 Monte Carlo simulations per scenario are run over a wide range\nof BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized\nHenze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,\nresults show that kNN is overwhelmingly the more accurate non-parametric\nestimator. In order to reach the target of an under 5 percent range for the 95\npercent confidence bounds, the minimum number of required samples per class is\n1000. As more features are added, more samples are needed, so that 2500 samples\nper class are required at only 4 features. Other estimators do become more\naccurate than kNN as more features are added, but continuously fail to meet the\ntarget range."}
{"id": "2506.03801", "pdf": "https://arxiv.org/pdf/2506.03801", "abs": "https://arxiv.org/abs/2506.03801", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": "Accepted to the Next Gen Data and Process Management: Large Language\n  Models and Beyond workshop at SIGMOD 2025", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments."}
{"id": "2506.03515", "pdf": "https://arxiv.org/pdf/2506.03515", "abs": "https://arxiv.org/abs/2506.03515", "authors": ["Masaya Kawamura", "Takuya Hasumi", "Yuma Shirahata", "Ryuichi Yamamoto"], "title": "BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": "Accepted to INTERSPEECH 2025", "summary": "This paper proposes a highly compact, lightweight text-to-speech (TTS) model\nfor on-device applications. To reduce the model size, the proposed model\nintroduces two techniques. First, we introduce quantization-aware training\n(QAT), which quantizes model parameters during training to as low as 1.58-bit.\nIn this case, most of 32-bit model parameters are quantized to ternary values\n{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,\nwe save a group of 1.58-bit weights as a single int8 index. This allows for\nefficient storage of model parameters, even on hardware that treats values in\nunits of 8-bit. Experimental results demonstrate that the proposed method\nachieved 83 % reduction in model size, while outperforming the baseline of\nsimilar model size without quantization in synthesis quality."}
{"id": "2506.03152", "pdf": "https://arxiv.org/pdf/2506.03152", "abs": "https://arxiv.org/abs/2506.03152", "authors": ["Robert Bayer", "Julian Priest", "Daniel Kjellberg", "Jeppe Lindhard", "Nikolaj Sørenesen", "Nicolaj Valsted", "Ívar Óli", "Pınar Tözün"], "title": "Adaptive and Robust Image Processing on CubeSats", "categories": ["eess.IV", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "CubeSats offer a low-cost platform for space research, particularly for Earth\nobservation. However, their resource-constrained nature and being in space,\nchallenge the flexibility and complexity of the deployed image processing\npipelines and their orchestration. This paper introduces two novel systems,\nDIPP and DISH, to address these challenges. DIPP is a modular and configurable\nimage processing pipeline framework that allows for adaptability to changing\nmission goals even after deployment, while preserving robustness. DISH is a\ndomain-specific language (DSL) and runtime system designed to schedule complex\nimaging workloads on low-power and memory-constrained processors.\n  Our experiments demonstrate that DIPP's decomposition of the processing\npipelines adds negligible overhead, while significantly reducing the network\nrequirements of updating pipelines and being robust against erroneous module\nuploads. Furthermore, we compare DISH to Lua, a general purpose scripting\nlanguage, and demonstrate its comparable expressiveness and lower memory\nrequirement."}
{"id": "2505.14151", "pdf": "https://arxiv.org/pdf/2505.14151", "abs": "https://arxiv.org/abs/2505.14151", "authors": ["Jiaming Li", "Sheng Wang", "Xin Wang", "Yitao Zhu", "Honglin Xiong", "Zixu Zhuang", "Qian Wang"], "title": "ReactDiff: Latent Diffusion for Facial Reaction Generation", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by Neural Networks", "summary": "Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}."}
{"id": "2506.04073", "pdf": "https://arxiv.org/pdf/2506.04073", "abs": "https://arxiv.org/abs/2506.04073", "authors": ["Esteban Gutiérrez", "Frederic Font", "Xavier Serra", "Lonce Wyse"], "title": "A Statistics-Driven Differentiable Approach for Sound Texture Synthesis and Analysis", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted to the 28th International Conference on Digital Audio\n  Effects (DAFx 2025) to be held in Ancona, Italy. 8 pages, one diagram and 5\n  tables", "summary": "In this work, we introduce TexStat, a novel loss function specifically\ndesigned for the analysis and synthesis of texture sounds characterized by\nstochastic structure and perceptual stationarity. Drawing inspiration from the\nstatistical and perceptual framework of McDermott and Simoncelli, TexStat\nidentifies similarities between signals belonging to the same texture category\nwithout relying on temporal structure. We also propose using TexStat as a\nvalidation metric alongside Frechet Audio Distances (FAD) to evaluate texture\nsound synthesis models. In addition to TexStat, we present TexEnv, an\nefficient, lightweight and differentiable texture sound synthesizer that\ngenerates audio by imposing amplitude envelopes on filtered noise. We further\nintegrate these components into TexDSP, a DDSP-inspired generative model\ntailored for texture sounds. Through extensive experiments across various\ntexture sound types, we demonstrate that TexStat is perceptually meaningful,\ntime-invariant, and robust to noise, features that make it effective both as a\nloss function for generative tasks and as a validation metric. All tools and\ncode are provided as open-source contributions and our PyTorch implementations\nare efficient, differentiable, and highly configurable, enabling its use in\nboth generative tasks and as a perceptually grounded evaluation metric."}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs."}
{"id": "2506.03171", "pdf": "https://arxiv.org/pdf/2506.03171", "abs": "https://arxiv.org/abs/2506.03171", "authors": ["Ghulam Mujtaba", "Eun-Seok Ryu"], "title": "EdgeVidSum: Real-Time Personalized Video Summarization at the Edge", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "EdgeVidSum is a lightweight method that generates personalized, fast-forward\nsummaries of long-form videos directly on edge devices. The proposed approach\nenables real-time video summarization while safeguarding user privacy through\nlocal data processing using innovative thumbnail-based techniques and efficient\nneural architectures. Unlike conventional methods that process entire videos\nframe by frame, the proposed method uses thumbnail containers to significantly\nreduce computational complexity without sacrificing semantic relevance. The\nframework employs a hierarchical analysis approach, where a lightweight 2D CNN\nmodel identifies user-preferred content from thumbnails and generates\ntimestamps to create fast-forward summaries. Our interactive demo highlights\nthe system's ability to create tailored video summaries for long-form videos,\nsuch as movies, sports events, and TV shows, based on individual user\npreferences. The entire computation occurs seamlessly on resource-constrained\ndevices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical\nchallenges of computational efficiency, personalization, and privacy in modern\nvideo consumption environments."}
{"id": "2506.03469", "pdf": "https://arxiv.org/pdf/2506.03469", "abs": "https://arxiv.org/abs/2506.03469", "authors": ["Tuan Le", "Risal Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, 7 figures, European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes\nenvironments requires not only formal verification but also interpretability\nand targeted falsification. While model checking provides formal guarantees,\nits effectiveness is limited by abstraction quality and the completeness of the\nunderlying trajectory dataset. We propose a hybrid framework that integrates\n(1) explainability, (2) model checking, and (3) risk-guided falsification to\nachieve both rigor and coverage. Our approach begins by constructing a\nhuman-interpretable abstraction of the RL policy using Comprehensible Abstract\nPolicy Summarization (CAPS). This abstract graph, derived from offline\ntrajectories, is both verifier-friendly, semantically meaningful, and can be\nused as input to Storm probabilistic model checker to verify satisfaction of\ntemporal safety specifications. If the model checker identifies a violation, it\nwill return an interpretable counterexample trace by which the policy fails the\nsafety requirement. However, if no violation is detected, we cannot conclude\nsatisfaction due to potential limitation in the abstraction and coverage of the\noffline dataset. In such cases, we estimate associated risk during model\nchecking to guide a falsification strategy that prioritizes searching in\nhigh-risk states and regions underrepresented in the trajectory dataset. We\nfurther provide PAC-style guarantees on the likelihood of uncovering undetected\nviolations. Finally, we incorporate a lightweight safety shield that switches\nto a fallback policy at runtime when such a risk exceeds a threshold,\nfacilitating failure mitigation without retraining."}
{"id": "2506.03160", "pdf": "https://arxiv.org/pdf/2506.03160", "abs": "https://arxiv.org/abs/2506.03160", "authors": ["Shriyank Somvanshi", "Anannya Ghosh Tusti", "Mahmuda Sultana Mimi", "Md Monzurul Islam", "Sazzad Bin Bashar Polock", "Anandi Dutta", "Subasish Das"], "title": "Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes", "categories": ["cs.LG"], "comment": null, "summary": "The increasing presence of automated vehicles (AVs) presents new challenges\nfor crash classification and safety analysis. Accurately identifying the SAE\nautomation level involved in each crash is essential to understanding crash\ndynamics and system accountability. However, existing approaches often overlook\nautomation-specific factors and lack model sophistication to capture\ndistinctions between different SAE levels. To address this gap, this study\nevaluates the performance of three advanced tabular deep learning models\nMambaAttention, TabPFN, and TabTransformer for classifying SAE automation\nlevels using structured crash data from Texas (2024), covering 4,649 cases\ncategorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level\n2), and Advanced Automation (SAE Levels 3-5 combined). Following class\nbalancing using SMOTEENN, the models were trained and evaluated on a unified\ndataset of 7,300 records. MambaAttention demonstrated the highest overall\nperformance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),\nwhile TabPFN excelled in zero-shot inference with high robustness for rare\ncrash categories. In contrast, TabTransformer underperformed, particularly in\ndetecting Partial Automation crashes (F1-score: 55%), suggesting challenges in\nmodeling shared human-system control dynamics. These results highlight the\ncapability of deep learning models tailored for tabular data to enhance the\naccuracy and efficiency of automation-level classification. Integrating such\nmodels into crash analysis frameworks can support policy development, AV safety\nevaluation, and regulatory decisions, especially in distinguishing high-risk\nconditions for mid- and high-level automation technologies."}
{"id": "2506.03828", "pdf": "https://arxiv.org/pdf/2506.03828", "abs": "https://arxiv.org/abs/2506.03828", "authors": ["Dhaval Patel", "Shuxin Lin", "James Rayfield", "Nianjun Zhou", "Roman Vaculin", "Natalia Martinez", "Fearghal O'donncha", "Jayant Kalagnanam"], "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance", "categories": ["cs.AI", "cs.MA"], "comment": "39 pages, 18 figures", "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench."}
{"id": "2506.03606", "pdf": "https://arxiv.org/pdf/2506.03606", "abs": "https://arxiv.org/abs/2506.03606", "authors": ["Parismita Gogoi", "Sishir Kalita", "Wendy Lalhminghlui", "Viyazonuo Terhiija", "Moakala Tzudir", "Priyankoo Sarmah", "S. R. M. Prasanna"], "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "comment": "Accepted in Interspeech2025", "summary": "This study explores the use of self-supervised learning (SSL) models for tone\nrecognition in three low-resource languages from North Eastern India: Angami,\nAo, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on\nboth tonal and non-tonal languages. We analyze tone-wise performance across the\nlayers for all three languages and compare the different models. Our results\nshow that tone recognition works best for Mizo and worst for Angami. The middle\nlayers of the SSL models are the most important for tone recognition,\nregardless of the pre-training language, i.e. tonal or non-tonal. We have also\nfound that the tone inventory, tone types, and dialectal variations affect tone\nrecognition. These findings provide useful insights into the strengths and\nweaknesses of SSL-based embeddings for tonal languages and highlight the\npotential for improving tone recognition in low-resource settings. The source\ncode is available at GitHub 1 ."}
{"id": "2506.03175", "pdf": "https://arxiv.org/pdf/2506.03175", "abs": "https://arxiv.org/abs/2506.03175", "authors": ["Youshen Xiao", "Yiling Shi", "Ruixi Sun", "Hongjiang Wei", "Fei Gao", "Yuyao Zhang"], "title": "Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging\ntechnique for monitoring physiological processes, capable of providing\nhigh-contrast images of optical absorption at much greater depths than\ntraditional optical imaging methods. However, practical instrumentation and\ngeometric constraints limit the number of acoustic sensors available around the\nimaging target, leading to sparsity in sensor data. Traditional photoacoustic\n(PA) image reconstruction methods, when directly applied to sparse PA data,\nproduce severe artifacts. Additionally, these traditional methods do not\nconsider the inter-frame relationships in dynamic imaging. Temporal resolution\nis crucial for dynamic photoacoustic imaging, which is fundamentally limited by\nthe low repetition rate (e.g., 20 Hz) and high cost of high-power laser\ntechnology. Recently, Implicit Neural Representation (INR) has emerged as a\npowerful deep learning tool for solving inverse problems with sparse data, by\ncharacterizing signal properties as continuous functions of their coordinates\nin an unsupervised manner. In this work, we propose an INR-based method to\nimprove dynamic photoacoustic image reconstruction from sparse-views and\nenhance temporal resolution, using only spatiotemporal coordinates as input.\nSpecifically, the proposed INR represents dynamic photoacoustic images as\nimplicit functions and encodes them into a neural network. The weights of the\nnetwork are learned solely from the acquired sparse sensor data, without the\nneed for external training datasets or prior images. Benefiting from the strong\nimplicit continuity regularization provided by INR, as well as explicit\nregularization for low-rank and sparsity, our proposed method outperforms\ntraditional reconstruction methods under two different sparsity conditions,\neffectively suppressing artifacts and ensuring image quality."}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681", "abs": "https://arxiv.org/abs/2506.03681", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English."}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301", "abs": "https://arxiv.org/abs/2506.03301", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph."}
{"id": "2506.03173", "pdf": "https://arxiv.org/pdf/2506.03173", "abs": "https://arxiv.org/abs/2506.03173", "authors": ["Xiaoyi Liu", "Hao Tang"], "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Physical intelligence -- anticipating and shaping the world from partial,\nmultisensory observations -- is critical for next-generation world models. We\npropose FOLIAGE, a physics-informed multimodal world model for unbounded\naccretive surface growth. In its Action-Perception loop, a unified context\nencoder maps images, mesh connectivity, and point clouds to a shared latent\nstate. A physics-aware predictor, conditioned on physical control actions,\nadvances this latent state in time to align with the target latent of the\nsurface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces\nwith critic heads for downstream objectives. FOLIAGE's Accretive Graph Network\n(AGN) captures dynamic connectivity through Age Positional Encoding and\nEnergy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch\nMasking enhance MAGE's expressiveness, while Hierarchical Pooling balances\nglobal context with local dynamics. We create SURF-GARDEN, a world model\nlearning platform comprising a Counterfactual Physics Simulator, a Multimodal\nCorrespondence Extractor, and Evolution Tracing, which generates 7,200 diverse\nsurface-growth sequences. SURF-BENCH, our physical-intelligence evaluation\nsuite, evaluates six core tasks -- topology recognition, inverse material\nestimation, growth-stage classification, latent roll-out, cross-modal\nretrieval, and dense correspondence -- and four stress tests -- sensor dropout,\nzero-shot modality transfer, long-horizon prediction, and physics ablation --\nto probe resilience. FOLIAGE outperforms specialized baselines while remaining\nrobust across dynamic environments, establishing a new world-model based,\nmultimodal pathway to physical intelligence."}
{"id": "2506.03503", "pdf": "https://arxiv.org/pdf/2506.03503", "abs": "https://arxiv.org/abs/2506.03503", "authors": ["Shan Shan"], "title": "Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis", "categories": ["cs.AI"], "comment": null, "summary": "The quantification of social science remains a longstanding challenge,\nlargely due to the philosophical nature of its foundational theories. Although\nquantum computing has advanced rapidly in recent years, its relevance to social\ntheory remains underexplored. Most existing research focuses on micro-cognitive\nmodels or philosophical analogies, leaving a gap in system-level applications\nof quantum principles to the analysis of social systems. This study addresses\nthat gap by proposing a theoretical and computational framework that combines\nquantum mechanics with Generative AI to simulate the emergence and evolution of\nsocial norms. Drawing on core quantum concepts--such as superposition,\nentanglement, and probabilistic measurement--this research models society as a\ndynamic, uncertain system and sets up five ideal-type experiments. These\nscenarios are simulated using 25 generative agents, each assigned evolving\nroles as compliers, resistors, or enforcers. Within a simulated environment\nmonitored by a central observer (the Watcher), agents interact, respond to\nsurveillance, and adapt to periodic normative disruptions. These interactions\nallow the system to self-organize under external stress and reveal emergent\npatterns. Key findings show that quantum principles, when integrated with\ngenerative AI, enable the modeling of uncertainty, emergence, and\ninterdependence in complex social systems. Simulations reveal patterns\nincluding convergence toward normative order, the spread of resistance, and the\nspontaneous emergence of new equilibria in social rules. In conclusion, this\nstudy introduces a novel computational lens that lays the groundwork for a\nquantum-informed social theory. It offers interdisciplinary insights into how\nsociety can be understood not just as a structure to observe but as a dynamic\nsystem to simulate and redesign through quantum technologies."}
{"id": "2506.03161", "pdf": "https://arxiv.org/pdf/2506.03161", "abs": "https://arxiv.org/abs/2506.03161", "authors": ["Mira Nuthakki"], "title": "Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "18 pages, figures at end, methods at end. Format/order can be changed\n  if necessary", "summary": "Traffic congestion and collisions represent significant economic,\nenvironmental, and social challenges worldwide. Traditional traffic management\napproaches have shown limited success in addressing these complex, dynamic\nproblems. To address the current research gaps, three potential tools are\ndeveloped: a comprehensive 3D city-wide simulation environment that integrates\nboth macroscopic and microscopic traffic dynamics; a collision model; and a\nreinforcement learning framework with custom reward functions prioritizing\nsafety over efficiency. Unity game engine-based simulation is used for direct\ncollision modeling. A custom reward enabled reinforcement learning method,\nproximal policy optimization (PPO) model, yields substantial improvements over\nbaseline results, reducing the number of serious collisions, number of\nvehicle-vehicle collisions, and total distance travelled by over 3 times the\nbaseline values. The model also improves fuel efficiency by 39% and reduces\ncarbon emissions by 88%. Results establish feasibility for city-wide 3D traffic\nsimulation applications incorporating the vision-zero safety principles of the\nDepartment of Transportation, including physics-informed, adaptable, realistic\ncollision modeling, as well as appropriate reward modeling for real-world\ntraffic signal light control towards reducing collisions, optimizing traffic\nflow and reducing greenhouse emissions."}
{"id": "2411.07161", "pdf": "https://arxiv.org/pdf/2411.07161", "abs": "https://arxiv.org/abs/2411.07161", "authors": ["Young-Min Cho", "Raphael Shu", "Nilaksh Das", "Tamer Alkhouli", "Yi-An Lai", "Jason Cai", "Monica Sunkara", "Yi Zhang", "Dan Roth"], "title": "RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration", "categories": ["cs.MA", "cs.AI"], "comment": "preprint", "summary": "Effective group decision-making is critical in Multi-Agent Systems (MAS).\nYet, how different mechanisms for reaching consensus impact collaboration\nquality and efficiency remains understudied. We conduct a systematic study on\ngroup decision-making mechanisms in a decentralized setting. Through controlled\nexperiments, we analyze how different voting rules affect decision quality and\nefficiency in a multi-round collaboration. Results reveal that majority voting\noften cause inefficient collaboration due to its strict acceptance criteria. At\nthe extreme, unanimous voting gives 87% lower initial performance than the\nbest-performing method. Our qualitative analysis of cross-agent communication\nshows that messages become longer and more repetitive over time: while message\nlength increases by 84%, similarity to the previous round increases to 90%.\nBased on these insights, language-based early stopping methods make the\nperformance 13% closer to oracle while reducing rounds by 50%. Our findings\nhighlight the crucial role of group decision-making in optimizing MAS\ncollaboration."}
{"id": "2506.03917", "pdf": "https://arxiv.org/pdf/2506.03917", "abs": "https://arxiv.org/abs/2506.03917", "authors": ["Stefano Damiano", "Toon van Waterschoot"], "title": "Sound Field Reconstruction Using Physics-Informed Boundary Integral Networks", "categories": ["eess.AS"], "comment": "Accepted for publication at EUSIPCO 2025", "summary": "Sound field reconstruction refers to the problem of estimating the acoustic\npressure field over an arbitrary region of space, using only a limited set of\nmeasurements. Physics-informed neural networks have been adopted to solve the\nproblem by incorporating in the training loss function the governing partial\ndifferential equation, either the Helmholtz or the wave equation. In this work,\nwe introduce a boundary integral network for sound field reconstruction.\nRelying on the Kirchhoff-Helmholtz boundary integral equation to model the\nsound field in a given region of space, we employ a shallow neural network to\nretrieve the pressure distribution on the boundary of the considered domain,\nenabling to accurately retrieve the acoustic pressure inside of it. Assuming\nthe positions of measurement microphones are known, we train the model by\nminimizing the mean squared error between the estimated and measured pressure\nat those locations. Experimental results indicate that the proposed model\noutperforms existing physics-informed data-driven techniques."}
{"id": "2506.03177", "pdf": "https://arxiv.org/pdf/2506.03177", "abs": "https://arxiv.org/abs/2506.03177", "authors": ["Isarun Chamveha", "Supphanut Chaiyungyuen", "Sasinun Worakriangkrai", "Nattawadee Prasawang", "Warasinee Chaisangmongkon", "Pornpim Korpraphong", "Voraparee Suvannarerg", "Shanigarn Thiravit", "Chalermdej Kannawat", "Kewalin Rungsinaporn", "Suwara Issaragrisil", "Payia Chadbunchachai", "Pattiya Gatechumpol", "Chawiporn Muktabhant", "Patarachai Sereerat"], "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice."}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications."}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303", "abs": "https://arxiv.org/abs/2506.03303", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks."}
{"id": "2506.03174", "pdf": "https://arxiv.org/pdf/2506.03174", "abs": "https://arxiv.org/abs/2506.03174", "authors": ["Koki Matsuishi", "Kosuke Ukita", "Tsuyoshi Okita"], "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures", "summary": "In recent years, the widespread adoption of wearable devices has highlighted\nthe growing importance of behavior analysis using IMU. While applications span\ndiverse fields such as healthcare and robotics, recent studies have\nincreasingly focused on multimodal analysis, in addition to unimodal analysis.\nSeveral studies have proposed multimodal foundation models that incorporate\nfirst-person video and text data; however, these models still fall short in\nproviding a detailed analysis of full-body human activity. To address this\nlimitation, we propose Activity Understanding and Representations Alignment -\nMultimodal Foundation Model (AURA-MFM), a foundational model integrating four\nmodalities: third-person video, motion capture, IMU, and text. By incorporating\nthird-person video and motion capture data, the model enables a detailed and\nmultidimensional understanding of human activity, which first-person\nperspectives alone fail to capture. Additionally, a Transformer-based IMU\nencoder is employed to enhance the model's overall performance. Experimental\nevaluations on retrieval and activity recognition tasks demonstrate that our\nmodel surpasses existing methods. Notably, in the zero-shot classification for\naction recognition, our method achieved significantly higher performance, with\nan F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method\nrecorded an F1-score of 0.0747 and an accuracy of 0.1961."}
{"id": "2506.03548", "pdf": "https://arxiv.org/pdf/2506.03548", "abs": "https://arxiv.org/abs/2506.03548", "authors": ["Chenglong Ye", "Gang Xiong", "Junyou Shang", "Xingyuan Dai", "Xiaoyan Gong", "Yisheng Lv"], "title": "SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Traffic simulation tools, such as SUMO, are essential for urban mobility\nresearch. However, such tools remain challenging for users due to complex\nmanual workflows involving network download, demand generation, simulation\nsetup, and result analysis. In this paper, we introduce SUMO-MCP, a novel\nplatform that not only wraps SUMO' s core utilities into a unified tool suite\nbut also provides additional auxiliary utilities for common preprocessing and\npostprocessing tasks. Using SUMO-MCP, users can issue simple natural-language\nprompts to generate traffic scenarios from OpenStreetMap data, create demand\nfrom origin-destination matrices or random patterns, run batch simulations with\nmultiple signal-control strategies, perform comparative analyses with automated\nreporting, and detect congestion for signal-timing optimization. Furthermore,\nthe platform allows flexible custom workflows by dynamically combining exposed\nSUMO tools without additional coding. Experiments demonstrate that SUMO-MCP\nsignificantly makes traffic simulation more accessible and reliable for\nresearchers. We will release code for SUMO-MCP at\nhttps://github.com/ycycycl/SUMO-MCP in the future."}
{"id": "2506.03163", "pdf": "https://arxiv.org/pdf/2506.03163", "abs": "https://arxiv.org/abs/2506.03163", "authors": ["Oluwaseyi Giwa"], "title": "Causal Discovery in Dynamic Fading Wireless Networks", "categories": ["cs.LG", "eess.SP", "stat.ME"], "comment": "5 pages, 3 figures", "summary": "Dynamic causal discovery in wireless networks is essential due to evolving\ninterference, fading, and mobility, which complicate traditional static causal\nmodels. This paper addresses causal inference challenges in dynamic fading\nwireless environments by proposing a sequential regression-based algorithm with\na novel application of the NOTEARS acyclicity constraint, enabling efficient\nonline updates. We derive theoretical lower and upper bounds on the detection\ndelay required to identify structural changes, explicitly quantifying their\ndependence on network size, noise variance, and fading severity. Monte Carlo\nsimulations validate these theoretical results, demonstrating linear increases\nin detection delay with network size, quadratic growth with noise variance, and\ninverse-square dependence on the magnitude of structural changes. Our findings\nprovide rigorous theoretical insights and practical guidelines for designing\nrobust online causal inference mechanisms to maintain network reliability under\nnonstationary wireless conditions."}
{"id": "2412.09429", "pdf": "https://arxiv.org/pdf/2412.09429", "abs": "https://arxiv.org/abs/2412.09429", "authors": ["Yi Luo", "Linghang Shi", "Yihao Li", "Aobo Zhuang", "Yeyun Gong", "Ling Liu", "Chen Lin"], "title": "From Intention To Implementation: Automating Biomedical Research via LLMs", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": "The paper involves material for which we have not yet obtained proper\n  copyright permissions", "summary": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems."}
{"id": "2506.04152", "pdf": "https://arxiv.org/pdf/2506.04152", "abs": "https://arxiv.org/abs/2506.04152", "authors": ["Ryan Langman", "Xuesong Yang", "Paarth Neekhara", "Shehzeen Hussain", "Edresson Casanova", "Evelina Bakhturina", "Jason Li"], "title": "HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset", "categories": ["eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This paper introduces HiFiTTS-2, a large-scale speech dataset designed for\nhigh-bandwidth speech synthesis. The dataset is derived from LibriVox\naudiobooks, and contains approximately 36.7k hours of English speech for 22.05\nkHz training, and 31.7k hours for 44.1 kHz training. We present our data\nprocessing pipeline, including bandwidth estimation, segmentation, text\npreprocessing, and multi-speaker detection. The dataset is accompanied by\ndetailed utterance and audiobook metadata generated by our pipeline, enabling\nresearchers to apply data quality filters to adapt the dataset to various use\ncases. Experimental results demonstrate that our data pipeline and resulting\ndataset can facilitate the training of high-quality, zero-shot text-to-speech\n(TTS) models at high bandwidths."}
{"id": "2506.03178", "pdf": "https://arxiv.org/pdf/2506.03178", "abs": "https://arxiv.org/abs/2506.03178", "authors": ["Md. Zihad Bin Jahangir", "Muhammad Ashad Kabir", "Sumaiya Akter", "Israt Jahan", "Minh Chau"], "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "25 pages", "summary": "Automated radiology report generation holds significant potential to reduce\nradiologists' workload and enhance diagnostic accuracy. However, generating\nprecise and clinically meaningful reports from chest radiographs remains\nchallenging due to the complexity of medical language and the need for\ncontextual understanding. Existing models often struggle with maintaining both\naccuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel\nframework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings\nand Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves\nimproved coherence and clinical accuracy while maintaining computational\nefficiency. This efficiency is driven by an optimization strategy that enhances\nparameter utilization and reduces memory overhead, enabling faster report\ngeneration with lower computational resource demands. Extensive experiments\nconducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR\noutperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L\nscore of 0.433 and a METEOR score of 0.336, establishing new performance\nbenchmarks in the domain. These results underscore LLaMA-XR's potential as an\neffective and efficient AI system for automated radiology reporting, offering\nenhanced clinical utility and reliability."}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing."}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "Víctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation."}
{"id": "2506.03179", "pdf": "https://arxiv.org/pdf/2506.03179", "abs": "https://arxiv.org/abs/2506.03179", "authors": ["Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate remarkable capabilities\nin handling complex multimodal tasks and are increasingly adopted in video\nunderstanding applications. However, their rapid advancement raises serious\ndata privacy concerns, particularly given the potential inclusion of sensitive\nvideo content, such as personal recordings and surveillance footage, in their\ntraining datasets. Determining improperly used videos during training remains a\ncritical and unresolved challenge. Despite considerable progress on membership\ninference attacks (MIAs) for text and image data in MLLMs, existing methods\nfail to generalize effectively to the video domain. These methods suffer from\npoor scalability as more frames are sampled and generally achieve negligible\ntrue positive rates at low false positive rates (TPR@Low FPR), mainly due to\ntheir failure to capture the inherent temporal variations of video frames and\nto account for model behavior differences as the number of frames varies. To\naddress these challenges, we introduce Vid-SME, the first membership inference\nmethod tailored for video data used in video understanding LLMs (VULLMs).\nVid-SME leverages the confidence of model output and integrates adaptive\nparameterization to compute Sharma-Mittal entropy (SME) for video inputs. By\nleveraging the SME difference between natural and temporally-reversed video\nframes, Vid-SME derives robust membership scores to determine whether a given\nvideo is part of the model's training set. Experiments on various self-trained\nand open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME."}
{"id": "2506.03586", "pdf": "https://arxiv.org/pdf/2506.03586", "abs": "https://arxiv.org/abs/2506.03586", "authors": ["Yu Ma", "Chongtao Guo", "Le Liang", "Xiao Li", "Shi Jin"], "title": "Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "This paper investigates a joint phase design and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to optimize average delay, where\ndata packets for each user arrive at the base station stochastically. The\nsequential optimization problem is inherently a Markov decision process (MDP),\nmaking it fall within the scope of reinforcement learning. To effectively\nhandle the mixed action space and reduce the state space dimensionality, a\nhybrid deep reinforcement learning (DRL) approach is proposed. Specifically,\nproximal policy optimization (PPO)-$\\Theta$ is employed to optimize RIS phase\nshift design, while PPO-N is responsible for subcarrier allocation decisions.\nTo further mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize subcarrier\nallocation indicater more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture network dynamics, key factors\nclosely related to average delay, including the number of backlogged packets in\nbuffers and the current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance training\nefficiency and accelerate convergence. Simulation results demonstrate that the\nproposed algorithm significantly reduces average delay, enhances resource\nallocation efficiency, and achieves superior system robustness and fairness\ncompared to baseline methods."}
{"id": "2506.03164", "pdf": "https://arxiv.org/pdf/2506.03164", "abs": "https://arxiv.org/abs/2506.03164", "authors": ["Vignav Ramesh", "Morteza Mardani"], "title": "Test-Time Scaling of Diffusion Models via Noise Trajectory Search", "categories": ["cs.LG"], "comment": null, "summary": "The iterative and stochastic nature of diffusion models enables test-time\nscaling, whereby spending additional compute during denoising generates\nhigher-fidelity samples. Increasing the number of denoising steps is the\nprimary scaling axis, but this yields quickly diminishing returns. Instead\noptimizing the noise trajectory--the sequence of injected noise vectors--is\npromising, as the specific noise realizations critically affect sample quality;\nbut this is challenging due to a high-dimensional search space, complex\nnoise-outcome interactions, and costly trajectory evaluations. We address this\nby first casting diffusion as a Markov Decision Process (MDP) with a terminal\nreward, showing tree-search methods such as Monte Carlo tree search (MCTS) to\nbe meaningful but impractical. To balance performance and efficiency, we then\nresort to a relaxation of MDP, where we view denoising as a sequence of\nindependent contextual bandits. This allows us to introduce an\n$\\epsilon$-greedy search algorithm that globally explores at extreme timesteps\nand locally exploits during the intermediate steps where de-mixing occurs.\nExperiments on EDM and Stable Diffusion reveal state-of-the-art scores for\nclass-conditioned/text-to-image generation, exceeding baselines by up to\n$164\\%$ and matching/exceeding MCTS performance. To our knowledge, this is the\nfirst practical method for test-time noise trajectory optimization of arbitrary\n(non-differentiable) rewards."}
{"id": "2503.02077", "pdf": "https://arxiv.org/pdf/2503.02077", "abs": "https://arxiv.org/abs/2503.02077", "authors": ["Ziyan Wang", "Zhicheng Zhang", "Fei Fang", "Yali Du"], "title": "M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Designing effective reward functions in multi-agent reinforcement learning\n(MARL) is a significant challenge, often leading to suboptimal or misaligned\nbehaviors in complex, coordinated environments. We introduce Multi-agent\nReinforcement Learning from Multi-phase Human Feedback of Mixed Quality\n($\\text{M}^3\\text{HF}$), a novel framework that integrates multi-phase human\nfeedback of mixed quality into the MARL training process. By involving humans\nwith diverse expertise levels to provide iterative guidance,\n$\\text{M}^3\\text{HF}$ leverages both expert and non-expert feedback to\ncontinuously refine agents' policies. During training, we strategically pause\nagent learning for human evaluation, parse feedback using large language models\nto assign it appropriately and update reward functions through predefined\ntemplates and adaptive weights by using weight decay and performance-based\nadjustments. Our approach enables the integration of nuanced human insights\nacross various levels of quality, enhancing the interpretability and robustness\nof multi-agent cooperation. Empirical results in challenging environments\ndemonstrate that $\\text{M}^3\\text{HF}$ significantly outperforms\nstate-of-the-art methods, effectively addressing the complexities of reward\ndesign in MARL and enabling broader human participation in the training\nprocess."}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037", "abs": "https://arxiv.org/abs/2506.04037", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs"}
{"id": "2506.03181", "pdf": "https://arxiv.org/pdf/2506.03181", "abs": "https://arxiv.org/abs/2506.03181", "authors": ["Wangting Zhou", "Jiangshan He", "Tong Cai", "Lin Wang", "Zhen Yuan", "Xunbin Wei", "Xueli Chen"], "title": "Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Photoacoustic microscopy holds the potential to measure biomarkers'\nstructural and functional status without labels, which significantly aids in\ncomprehending pathophysiological conditions in biomedical research. However,\nconventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered\nby a limited depth-of-field (DoF) due to the narrow depth range focused on a\nGaussian beam. Consequently, it fails to resolve sufficient details in the\ndepth direction. Herein, we propose a decision-level constrained end-to-end\nmulti-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method\nis a lightweight siamese network that incorporates an artifact-resistant\nchannel-wise spatial frequency as its feature fusion rule. The meticulously\ncrafted U-Net-based perceptual loss function for decision-level focus\nproperties in end-to-end fusion seamlessly integrates the complementary\nadvantages of spatial domain and transform domain methods within Dc-EEMF. This\napproach can be trained end-to-end without necessitating post-processing\nprocedures. Experimental results and numerical analyses collectively\ndemonstrate our method's robust performance, achieving an impressive fusion\nresult for PAM images without a substantial sacrifice in lateral resolution.\nThe utilization of Dc-EEMF-powered PAM has the potential to serve as a\npractical tool in preclinical and clinical studies requiring extended DoF for\nvarious applications."}
{"id": "2506.04076", "pdf": "https://arxiv.org/pdf/2506.04076", "abs": "https://arxiv.org/abs/2506.04076", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription."}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312", "abs": "https://arxiv.org/abs/2506.03312", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media."}
{"id": "2506.03182", "pdf": "https://arxiv.org/pdf/2506.03182", "abs": "https://arxiv.org/abs/2506.03182", "authors": ["Shivani Chiranjeevi", "Hossein Zaremehrjerdi", "Zi K. Deng", "Talukder Z. Jubery", "Ari Grele", "Arti Singh", "Asheesh K Singh", "Soumik Sarkar", "Nirav Merchant", "Harold F. Greeney", "Baskar Ganapathysubramanian", "Chinmay Hegde"], "title": "TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The rapid global loss of biodiversity, particularly among insects, represents\nan urgent ecological crisis. Current methods for insect species discovery are\nmanual, slow, and severely constrained by taxonomic expertise, hindering timely\nconservation actions. We introduce TerraIncognita, a dynamic benchmark designed\nto evaluate state-of-the-art multimodal models for the challenging problem of\nidentifying unknown, potentially undescribed insect species from image data.\nOur benchmark dataset combines a mix of expertly annotated images of insect\nspecies likely known to frontier AI models, and images of rare and poorly known\nspecies, for which few/no publicly available images exist. These images were\ncollected from underexplored biodiversity hotspots, realistically mimicking\nopen-world discovery scenarios faced by ecologists. The benchmark assesses\nmodels' proficiency in hierarchical taxonomic classification, their capability\nto detect and abstain from out-of-distribution (OOD) samples representing novel\nspecies, and their ability to generate explanations aligned with expert\ntaxonomic knowledge. Notably, top-performing models achieve over 90\\% F1 at the\nOrder level on known species, but drop below 2\\% at the Species level,\nhighlighting the sharp difficulty gradient from coarse to fine taxonomic\nprediction (Order $\\rightarrow$ Family $\\rightarrow$ Genus $\\rightarrow$\nSpecies). TerraIncognita will be updated regularly, and by committing to\nquarterly dataset expansions (of both known and novel species), will provide an\nevolving platform for longitudinal benchmarking of frontier AI methods. All\nTerraIncognita data, results, and future updates are available\n\\href{https://baskargroup.github.io/TerraIncognita/}{here}."}
{"id": "2506.03610", "pdf": "https://arxiv.org/pdf/2506.03610", "abs": "https://arxiv.org/abs/2506.03610", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak."}
{"id": "2506.03176", "pdf": "https://arxiv.org/pdf/2506.03176", "abs": "https://arxiv.org/abs/2506.03176", "authors": ["Bin Wang", "Yongqi Han", "Minbo Ma", "Tianrui Li", "Junbo Zhang", "Feng Hong", "Yanwei Yu"], "title": "Non-collective Calibrating Strategy for Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Deep learning-based approaches have demonstrated significant advancements in\ntime series forecasting. Despite these ongoing developments, the complex\ndynamics of time series make it challenging to establish the rule of thumb for\ndesigning the golden model architecture. In this study, we argue that refining\nexisting advanced models through a universal calibrating strategy can deliver\nsubstantial benefits with minimal resource costs, as opposed to elaborating and\ntraining a new model from scratch. We first identify a multi-target learning\nconflict in the calibrating process, which arises when optimizing variables\nacross time steps, leading to the underutilization of the model's learning\ncapabilities. To address this issue, we propose an innovative calibrating\nstrategy called Socket+Plug (SoP). This approach retains an exclusive optimizer\nand early-stopping monitor for each predicted target within each Plug while\nkeeping the fully trained Socket backbone frozen. The model-agnostic nature of\nSoP allows it to directly calibrate the performance of any trained deep\nforecasting models, regardless of their specific architectures. Extensive\nexperiments on various time series benchmarks and a spatio-temporal\nmeteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up\nto a 22% improvement even when employing a simple MLP as the Plug (highlighted\nin Figure 1)"}
{"id": "2404.19564", "pdf": "https://arxiv.org/pdf/2404.19564", "abs": "https://arxiv.org/abs/2404.19564", "authors": ["Michael Amir", "Alfred M. Bruckstein"], "title": "Time, Travel, and Energy in the Uniform Dispersion Problem", "categories": ["cs.RO", "cs.DM", "cs.MA", "68T40", "I.2.9"], "comment": "Accepted to IEEE T-RO. Includes and expands results from \"Minimizing\n  Travel in the Uniform Dispersal Problem for Robotic Sensors\" (AAMAS 2019,\n  arXiv:1903.03259)", "summary": "We investigate the algorithmic problem of uniformly dispersing a swarm of\nrobots in an unknown, gridlike environment. In this setting, our goal is to\nstudy the relationships between performance metrics and robot capabilities. We\nintroduce a formal model comparing dispersion algorithms based on makespan,\ntraveled distance, energy consumption, sensing, communication, and memory.\nUsing this framework, we classify uniform dispersion algorithms according to\ntheir capability requirements and performance. We prove that while makespan and\ntravel can be minimized in all environments, energy cannot, if the swarm's\nsensing range is bounded. In contrast, we show that energy can be minimized by\n``ant-like'' robots in synchronous settings and asymptotically minimized in\nasynchronous settings, provided the environment is topologically simply\nconnected, by using our ``Find-Corner Depth-First Search'' (FCDFS) algorithm.\nOur theoretical and experimental results show that FCDFS significantly\noutperforms known algorithms. Our findings reveal key limitations in designing\nswarm robotics systems for unknown environments, emphasizing the role of\ntopology in energy-efficient dispersion."}
{"id": "2506.04077", "pdf": "https://arxiv.org/pdf/2506.04077", "abs": "https://arxiv.org/abs/2506.04077", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information."}
{"id": "2506.03183", "pdf": "https://arxiv.org/pdf/2506.03183", "abs": "https://arxiv.org/abs/2506.03183", "authors": ["Yaşar Utku Alçalar", "Yu Cao", "Mehmet Akçakaya"], "title": "Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Conference on Future Internet of Things and Cloud\n  (FiCloud), 2025", "summary": "Physics-driven artificial intelligence (PD-AI) reconstruction methods have\nemerged as the state-of-the-art for accelerating MRI scans, enabling higher\nspatial and temporal resolutions. However, the high resolution of these scans\ngenerates massive data volumes, leading to challenges in transmission, storage,\nand real-time processing. This is particularly pronounced in functional MRI,\nwhere hundreds of volumetric acquisitions further exacerbate these demands.\nEdge computing with FPGAs presents a promising solution for enabling PD-AI\nreconstruction near the MRI sensors, reducing data transfer and storage\nbottlenecks. However, this requires optimization of PD-AI models for hardware\nefficiency through quantization and bypassing traditional FFT-based approaches,\nwhich can be a limitation due to their computational demands. In this work, we\npropose a novel PD-AI computational MRI approach optimized for FPGA-based edge\ncomputing devices, leveraging 8-bit complex data quantization and eliminating\nredundant FFT/IFFT operations. Our results show that this strategy improves\ncomputational efficiency while maintaining reconstruction quality comparable to\nconventional PD-AI methods, and outperforms standard clinical methods. Our\napproach presents an opportunity for high-resolution MRI reconstruction on\nresource-constrained devices, highlighting its potential for real-world\ndeployment."}
{"id": "2506.04134", "pdf": "https://arxiv.org/pdf/2506.04134", "abs": "https://arxiv.org/abs/2506.04134", "authors": ["Jinting Wang", "Shan Yang", "Li Liu"], "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "10 pages, 10 figures", "summary": "Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our\nUniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech\nsynchronization by 32% compared to the single CSV2S."}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357", "abs": "https://arxiv.org/abs/2506.03357", "authors": ["Aldan Creo", "Héctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano Hormazábal-Lagos"], "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection."}
{"id": "2506.03184", "pdf": "https://arxiv.org/pdf/2506.03184", "abs": "https://arxiv.org/abs/2506.03184", "authors": ["Mahe Zabin", "Ho-Jin Choi", "Md. Monirul Islam", "Jia Uddin"], "title": "Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages, 2 figures, published at Proceedings of the 15th KIPS\n  International Conference on Ubiquitous Information Technologies and\n  Applications (CUTE 2021), Jeju, Repubilc of Korea", "summary": "The performance of a classifier depends on the tuning of its parame ters. In\nthis paper, we have experimented the impact of various tuning parameters on the\nperformance of a deep convolutional neural network (DCNN). In the ex perimental\nevaluation, we have considered a DCNN classifier that consists of 2\nconvolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.\nTo observe the impact of pooling, activation function, and optimizer tuning pa\nrameters, we utilized a crack image dataset having two classes: negative and\npos itive. The experimental results demonstrate that with the maxpooling, the\nDCNN demonstrates its better performance for adam optimizer and tanh activation\nfunc tion."}
{"id": "2506.03613", "pdf": "https://arxiv.org/pdf/2506.03613", "abs": "https://arxiv.org/abs/2506.03613", "authors": ["Shaoshan Liu", "Fan Wang", "Hongjun Zhou", "Yuanfeng Wang"], "title": "Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations", "categories": ["cs.AI", "cs.CC"], "comment": null, "summary": "While theory and practice are often seen as separate domains, this article\nshows that theoretical insight is essential for overcoming real-world\nengineering barriers. We begin with a practical challenge: training a\ncross-morphology embodied AI policy that generalizes across diverse robot\nmorphologies. We formalize this as the Heterogeneous Embodied Agent Training\n(HEAT) problem and prove it reduces to a structured Partially Observable Markov\nDecision Process (POMDP) that is PSPACE-complete. This result explains why\ncurrent reinforcement learning pipelines break down under morphological\ndiversity, due to sequential training constraints, memory-policy coupling, and\ndata incompatibility. We further explore Collective Adaptation, a distributed\nlearning alternative inspired by biological systems. Though NEXP-complete in\ntheory, it offers meaningful scalability and deployment benefits in practice.\nThis work illustrates how computational theory can illuminate system design\ntrade-offs and guide the development of more robust, scalable embodied AI. For\npractitioners and researchers to explore this problem, the implementation code\nof this work has been made publicly available at\nhttps://github.com/airs-admin/HEAT"}
{"id": "2506.03206", "pdf": "https://arxiv.org/pdf/2506.03206", "abs": "https://arxiv.org/abs/2506.03206", "authors": ["Nadav Timor", "Jonathan Mamou", "Oren Pereg", "Hongyang Zhang", "David Harel"], "title": "Out-of-Vocabulary Sampling Boosts Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Speculative decoding relies on fast and accurate drafters. Recent\nstate-of-the-art language models employ larger and larger vocabularies, which\nsignificantly slows down drafters. One promising approach to boost the\nefficiency of speculative decoding is to use drafters with smaller\nvocabularies. However, existing sampling methods cannot draw out-of-vocabulary\ntokens, creating a tradeoff between drafters' vocabulary size and acceptance\nrates. This paper introduces Redistributing Drafter Kernels (RDK), the first\nout-of-vocabulary sampler that effectively recovers acceptance rates by\nvirtually restoring pruned target tokens. RDK leverages token-affinity priors\nto reallocate drafter mass towards high-overlap regions. We prove\nmathematically that RDK can achieve higher acceptance rates than vanilla and\nstate-of-the-art samplers. We provide an efficient first-order approximation of\nRDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,\nenabling lightweight implementations for large vocabularies. Our experiments\ndemonstrate that this linear-time RDK significantly boosts acceptance rates\neven after extreme pruning (removing more than 75% of the drafter's\nvocabulary), where existing samplers fail. RDK opens the door to extremely\npruned drafters, which were previously impractical."}
{"id": "2502.17721", "pdf": "https://arxiv.org/pdf/2502.17721", "abs": "https://arxiv.org/abs/2502.17721", "authors": ["Xiangwen Wang", "Yibo Jacky Zhang", "Zhoujie Ding", "Katherine Tsai", "Haolun Wu", "Sanmi Koyejo"], "title": "Aligning Compound AI Systems via System-level DPO", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "Accepted to workshops MARW and WMAC (Oral) at AAAI25", "summary": "Compound AI systems, comprising multiple interacting components such as LLMs,\nfoundation models, and external tools, have demonstrated remarkable\nimprovements compared to single models in various tasks. To ensure their\neffective deployment in real-world applications, aligning these systems with\nhuman preferences is crucial. However, aligning the compound system via policy\noptimization, unlike the alignment of a single model, is challenging for two\nmain reasons: (i) non-differentiable interactions between components make\nend-to-end gradient-based optimization method inapplicable, and (ii)\nsystem-level preferences cannot be directly transformed into component-level\npreferences. To address these challenges, we first formulate compound AI\nsystems as Directed Acyclic Graphs (DAGs), explicitly modeling both component\ninteractions and the associated data flows. Building on this formulation, we\nintroduce $\\textbf{SysDPO}$, a framework that extends Direct Preference\nOptimization (DPO) to enable joint system-level alignment. We propose two\nvariants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending\non whether we construct a system-specific preference dataset. We empirically\ndemonstrate the effectiveness of our approach across two applications: the\njoint alignment of a language model and a diffusion model, and the joint\nalignment of an LLM collaboration system."}
{"id": "2402.12208", "pdf": "https://arxiv.org/pdf/2402.12208", "abs": "https://arxiv.org/abs/2402.12208", "authors": ["Shengpeng Ji", "Minghui Fang", "Jialong Zuo", "Ziyue Jiang", "Dingdong Wang", "Hanting Wang", "Hai Huang", "Zhou Zhao"], "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models", "categories": ["eess.AS", "cs.SD"], "comment": "ACL 2025 Main", "summary": "In recent years, large language models have achieved significant success in\ngenerative tasks related to speech, audio, music, and other signal domains. A\ncrucial element of these models is the discrete acoustic codecs, which serve as\nan intermediate representation replacing the mel-spectrogram. However, there\nexist several gaps between discrete codecs and downstream speech language\nmodels. Specifically, 1) Due to the reconstruction paradigm of the Codec model\nand the structure of residual vector quantization, the initial channel of the\ncodebooks contains excessive information, making it challenging to directly\ngenerate acoustic tokens from weakly supervised signals such as text in\ndownstream tasks. 2) numerous codebooks increases the burden on downstream\nspeech language models. Consequently, leveraging the characteristics of speech\nlanguage models, we propose Language-Codec. In the Language-Codec, we introduce\na Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with\nimproved fourier transform structures and attention blocks, refined\ndiscriminator design to address the aforementioned gaps. We compare our method\nwith competing audio compression algorithms and observe significant\noutperformance across extensive evaluations. Furthermore, we also validate the\nefficiency of the Language-Codec on downstream speech language models. The\nsource code and pre-trained models can be accessed at\nhttps://github.com/jishengpeng/languagecodec ."}
{"id": "2506.03185", "pdf": "https://arxiv.org/pdf/2506.03185", "abs": "https://arxiv.org/abs/2506.03185", "authors": ["Liangrui Pan", "Xingchen Li", "Zhongyi Chen", "Ling Chu", "Shaoliang Peng"], "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Submit to ACM MM2025", "summary": "Pathologists comprehensive evaluation of donor liver biopsies provides\ncrucial information for accepting or discarding potential grafts. However,\nrapidly and accurately obtaining these assessments intraoperatively poses a\nsignificant challenge for pathologists. Features in donor liver biopsies, such\nas portal tract fibrosis, total steatosis, macrovesicular steatosis, and\nhepatocellular ballooning are correlated with transplant outcomes, yet\nquantifying these indicators suffers from substantial inter- and intra-observer\nvariability. To address this, we introduce DLiPath, the first benchmark for\ncomprehensive donor liver assessment based on a histopathology image dataset.\nWe collected and publicly released 636 whole slide images from 304 donor liver\npatients at the Department of Pathology, the Third Xiangya Hospital, with\nexpert annotations for key pathological features (including cholestasis, portal\ntract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,\nand hepatocellular ballooning). We selected nine state-of-the-art\nmultiple-instance learning (MIL) models based on the DLiPath dataset as\nbaselines for extensive comparative analysis. The experimental results\ndemonstrate that several MIL models achieve high accuracy across donor liver\nassessment indicators on DLiPath, charting a clear course for future automated\nand intelligent donor liver assessment research. Data and code are available at\nhttps://github.com/panliangrui/ACM_MM_2025."}
{"id": "2410.16428", "pdf": "https://arxiv.org/pdf/2410.16428", "abs": "https://arxiv.org/abs/2410.16428", "authors": ["Wan Lin", "Junhui Chen", "Tianhao Wang", "Zhenyu Zhou", "Lantian Li", "Dong Wang"], "title": "Neural Scoring: A Refreshed End-to-End Approach for Speaker Recognition in Complex Conditions", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Modern speaker verification systems primarily rely on speaker embeddings and\ncosine similarity. While effective, these methods struggle with multi-talker\nspeech due to the unidentifiability of embedding vectors. We propose Neural\nScoring (NS), a novel end-to-end framework that directly estimates verification\nposterior probabilities without relying on test-side embeddings, making it more\npowerful and robust to complex conditions, e.g., with multiple talkers. To\naddress the challenge of training such end-to-end models, we introduce a\nmulti-enrollment training strategy, which pairs each test utterance with\nmultiple enrolled speakers and proves essential to the model's success.\nExperiments on the VoxCeleb dataset demonstrate that NS consistently\noutperforms both the baseline and several competitive methods, achieving an\noverall 70.36% reduction in Equal Error Rate (EER) compared to the baseline."}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360", "abs": "https://arxiv.org/abs/2506.03360", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake"}
{"id": "2506.03189", "pdf": "https://arxiv.org/pdf/2506.03189", "abs": "https://arxiv.org/abs/2506.03189", "authors": ["Ghada Sokar", "Gintare Karolina Dziugaite", "Anurag Arnab", "Ahmet Iscen", "Pablo Samuel Castro", "Cordelia Schmid"], "title": "Continual Learning in Vision-Language Models via Aligned Model Merging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual learning is conventionally tackled through sequential fine-tuning,\na process that, while enabling adaptation, inherently favors plasticity over\nthe stability needed to retain prior knowledge. While existing approaches\nattempt to mitigate catastrophic forgetting, a bias towards recent tasks\npersists as they build upon this sequential nature. In this work we present a\nnew perspective based on model merging to maintain stability while still\nretaining plasticity. Rather than just sequentially updating the model weights,\nwe propose merging newly trained task parameters with previously learned ones,\npromoting a better balance. To maximize the effectiveness of the merging\nprocess, we propose a simple mechanism that promotes learning aligned weights\nwith previous ones, thereby avoiding interference when merging. We evaluate\nthis approach on large Vision-Language Models (VLMs), and demonstrate its\neffectiveness in reducing forgetting, increasing robustness to various task\norders and similarities, and improving generalization."}
{"id": "2506.03673", "pdf": "https://arxiv.org/pdf/2506.03673", "abs": "https://arxiv.org/abs/2506.03673", "authors": ["Yinlong Xu", "Yanzhao Zheng", "Shuoshuo Sun", "Shuaihan Huang", "Baohua Dong", "Hangcheng Zhu", "Ruohui Huang", "Gang Yu", "Hongxia Xu", "Jian Wu"], "title": "Reason from Future: Reverse Thought Chain Enhances LLM Reasoning", "categories": ["cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "It has been demonstrated that carefully designed reasoning paradigms, like\nChain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning\ncapabilities of small language models by detailed thinking and extensive\nthought searching, unbounded branching factors in the searching space create\nprohibitive reasoning consumption. However these methods fall into the trap of\nlocal optimum reasoning, which means the model lacks a global perspective while\nsolving problems. We propose a novel reasoning paradigm called Reason from\nFuture (RFF), which generates reasoning paths by bidirectional reasoning that\ncombines top-down planning with bottom-up reasoning accumulation. The essence\nof RFF lies in its reverse reasoning mechanism, which prioritizes core logical\nrelationships and imposes goal-oriented constraints on intermediate steps,\nthereby reducing the searching space and mitigating error accumulation inherent\nin sequential forward reasoning. Empirical evaluations across diverse\nexperiments demonstrate that RFF outperforms conventional paradigms with higher\naccuracy and less searching space to solve complex tasks."}
{"id": "2506.03207", "pdf": "https://arxiv.org/pdf/2506.03207", "abs": "https://arxiv.org/abs/2506.03207", "authors": ["Md Nahid Hasan Shuvo", "Moinul Hossain"], "title": "Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "7 pages, 4 Figures, Accepted to publish in Proceedings of the 2025\n  ACM Workshop on Wireless Security and Machine Learning (WiseML 2025), July 3,\n  2025, Arlington, VA, USA", "summary": "Federated Learning (FL) is increasingly adopted as a decentralized machine\nlearning paradigm due to its capability to preserve data privacy by training\nmodels without centralizing user data. However, FL is susceptible to indirect\nprivacy breaches via network traffic analysis-an area not explored in existing\nresearch. The primary objective of this research is to study the feasibility of\nfingerprinting deep learning models deployed within FL environments by\nanalyzing their network-layer traffic information. In this paper, we conduct an\nexperimental evaluation using various deep learning architectures (i.e., CNN,\nRNN) within a federated learning testbed. We utilize machine learning\nalgorithms, including Support Vector Machines (SVM), Random Forest, and\nGradient-Boosting, to fingerprint unique patterns within the traffic data. Our\nexperiments show high fingerprinting accuracy, achieving 100% accuracy using\nRandom Forest and around 95.7% accuracy using SVM and Gradient Boosting\nclassifiers. This analysis suggests that we can identify specific architectures\nrunning within the subsection of the network traffic. Hence, if an adversary\nknows about the underlying DL architecture, they can exploit that information\nand conduct targeted attacks. These findings suggest a notable security\nvulnerability in FL systems and the necessity of strengthening it at the\nnetwork level."}
{"id": "2505.02945", "pdf": "https://arxiv.org/pdf/2505.02945", "abs": "https://arxiv.org/abs/2505.02945", "authors": ["Egil Diau"], "title": "The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": "This version updates the position paper with clearer language and\n  improved structure. It also corrects minor mistakes in wording and\n  formatting. There is no change in framing, scope, or modeling domain. The\n  core contribution remains a simulateable, agent-based framework intended for\n  cs.CE / cs.MA", "summary": "The origins of economic behavior remain unresolved-not only in the social\nsciences but also in AI, where dominant theories often rely on predefined\nincentives or institutional assumptions. Contrary to the longstanding myth of\nbarter as the foundation of exchange, converging evidence from early human\nsocieties suggests that reciprocity-not barter-was the foundational economic\nlogic, enabling communities to sustain exchange and social cohesion long before\nformal markets emerged. Yet despite its centrality, reciprocity lacks a\nsimulateable and cognitively grounded account. Here, we introduce a minimal\nbehavioral framework based on three empirically supported cognitive\nprimitives-individual recognition, reciprocal credence, and cost--return\nsensitivity-that enable agents to participate in and sustain reciprocal\nexchange, laying the foundation for scalable economic behavior. These\nmechanisms scaffold the emergence of cooperation, proto-economic exchange, and\ninstitutional structure from the bottom up. By bridging insights from\nprimatology, developmental psychology, and economic anthropology, this\nframework offers a unified substrate for modeling trust, coordination, and\neconomic behavior in both human and artificial systems."}
{"id": "2406.01205", "pdf": "https://arxiv.org/pdf/2406.01205", "abs": "https://arxiv.org/abs/2406.01205", "authors": ["Shengpeng Ji", "Qian Chen", "Wen Wang", "Jialong Zuo", "Minghui Fang", "Ziyue Jiang", "Hai Huang", "Zehan Wang", "Xize Cheng", "Siqi Zheng", "Zhou Zhao"], "title": "ControlSpeech: Towards Simultaneous and Independent Zero-shot Speaker Cloning and Zero-shot Language Style Control", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "ACL 2025 Main", "summary": "In this paper, we present ControlSpeech, a text-to-speech (TTS) system\ncapable of fully cloning the speaker's voice and enabling arbitrary control and\nadjustment of speaking style. Prior zero-shot TTS models only mimic the\nspeaker's voice without further control and adjustment capabilities while prior\ncontrollable TTS models cannot perform speaker-specific voice generation.\nTherefore, ControlSpeech focuses on a more challenging task: a TTS system with\ncontrollable timbre, content, and style at the same time. ControlSpeech takes\nspeech prompts, content prompts, and style prompts as inputs and utilizes\nbidirectional attention and mask-based parallel decoding to capture codec\nrepresentations corresponding to timbre, content, and style in a discrete\ndecoupling codec space. Moreover, we analyze the many-to-many issue in textual\nstyle control and propose the Style Mixture Semantic Density (SMSD) module,\nwhich is based on Gaussian mixture density networks, to resolve this problem.\nTo facilitate empirical validations, we make available a new style controllable\ndataset called VccmDataset. Our experimental results demonstrate that\nControlSpeech exhibits comparable or state-of-the-art (SOTA) performance in\nterms of controllability, timbre similarity, audio quality, robustness, and\ngeneralizability. The relevant code and demo are available at\nhttps://github.com/jishengpeng/ControlSpeech ."}
{"id": "2506.03186", "pdf": "https://arxiv.org/pdf/2506.03186", "abs": "https://arxiv.org/abs/2506.03186", "authors": ["Duaa Kareem Qasim", "Sabah Abdulazeez Jebur", "Lafta Raheem Ali", "Abdul Jalil M. Khalaf", "Abir Jaafar Hussain"], "title": "Lightweight Convolutional Neural Networks for Retinal Disease Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)\nsignificantly impact vision and affect millions worldwide. Early detection is\ncrucial, as DR, a complication of diabetes, damages retinal blood vessels,\npotentially leading to blindness, while MH disrupts central vision, affecting\ntasks like reading and facial recognition. This paper employed two lightweight\nand efficient Convolution Neural Network architectures, MobileNet and\nNASNetMobile, for the classification of Normal, DR, and MH retinal images. The\nmodels were trained on the RFMiD dataset, consisting of 3,200 fundus images,\nafter undergoing preprocessing steps such as resizing, normalization, and\naugmentation. To address data scarcity, this study leveraged transfer learning\nand data augmentation techniques, enhancing model generalization and\nperformance. The experimental results demonstrate that MobileNetV2 achieved the\nhighest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%\naccuracy. These findings highlight the effectiveness of CNNs in retinal disease\nclassification, providing a foundation for AI-assisted ophthalmic diagnosis and\nearly intervention."}
{"id": "2502.14627", "pdf": "https://arxiv.org/pdf/2502.14627", "abs": "https://arxiv.org/abs/2502.14627", "authors": ["Yuguo Yin", "Yuxin Xie", "Wenyuan Yang", "Dongchao Yang", "Jinghan Ru", "Xianwei Zhuang", "Liming Liang", "Yuexian Zou"], "title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to\nretrieve audio clips or multilingual texts from databases. However, existing\nML-ATR schemes suffer from inconsistencies for instance similarity matching\nacross languages. We theoretically analyze the inconsistency in terms of both\nmultilingual modal alignment direction error and weight error, and propose the\ntheoretical weight error upper bound for quantifying the inconsistency. Based\non the analysis of the weight error upper bound, we find that the inconsistency\nproblem stems from the data distribution error caused by random sampling of\nlanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastive\nlearning and audio-English co-anchor contrastive learning, aiming to mitigate\nthe negative impact of data distribution error on recall and consistency in\nML-ATR. Experimental results on the translated AudioCaps and Clotho datasets\nshow that our scheme achieves state-of-the-art performance on recall and\nconsistency metrics for eight mainstream languages, including English. Our code\nwill be available at https://github.com/ATRI-ACL/ATRI-ACL."}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction."}
{"id": "2506.03190", "pdf": "https://arxiv.org/pdf/2506.03190", "abs": "https://arxiv.org/abs/2506.03190", "authors": ["Jiaming Yi", "Ruirui Pan", "Jishen Yang", "Xiulong Yang"], "title": "MINT: Memory-Infused Prompt Tuning at Test-time for CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Improving the generalization ability of Vision-Language Pre-trained Models\n(VLMs) under test-time data distribution shifts remains a critical challenge.\nThe existing Test-Time Adaptation (TTA) methods fall short in fully leveraging\nthe model's internal knowledge, particularly in dynamically adapting to complex\nand hierarchical visual semantic information. In this paper, we propose\nMemory-Infused Prompt Tuning (MINT), a novel framework to address this issue.\nInspired by human associative memory theory, MINT introduces a Memory Prompt\nBank (MPB), which stores learnable key-value prompt pairs that work as a memory\nof previously seen samples. During the test time, relevant prompt pairs in the\nMPB are retrieved by the hierarchical visual features of test images to\ndynamically assemble Associative Prompts. The associative prompts are then\ninjected into the image encoder for fine-grained, customized visual contextual\nguidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,\nprecise VLM adaptation at test time by leveraging this MPB-acquired memory,\nwithout source data or retraining. The code is available at\nhttps://github.com/Jamieyi2004/MINT."}
{"id": "2506.03915", "pdf": "https://arxiv.org/pdf/2506.03915", "abs": "https://arxiv.org/abs/2506.03915", "authors": ["Sebastian Rödling", "Matej Zečević", "Devendra Singh Dhami", "Kristian Kersting"], "title": "Causal Explanations Over Time: Articulated Reasoning for Interactive Environments", "categories": ["cs.AI"], "comment": "Main paper: 9 pages, References: 2 pages, Supplementary: 9 pages.\n  Number of figures: 10, number of tables: 3", "summary": "Structural Causal Explanations (SCEs) can be used to automatically generate\nexplanations in natural language to questions about given data that are\ngrounded in a (possibly learned) causal model. Unfortunately they work for\nsmall data only. In turn they are not attractive to offer reasons for events,\ne.g., tracking causal changes over multiple time steps, or a behavioral\ncomponent that involves feedback loops through actions of an agent. To this\nend, we generalize SCEs to a (recursive) formulation of explanation trees to\ncapture the temporal interactions between reasons. We show the benefits of this\nmore general SCE algorithm on synthetic time-series data and a 2D grid game,\nand further compare it to the base SCE and other existing methods for causal\nexplanations."}
{"id": "2506.03210", "pdf": "https://arxiv.org/pdf/2506.03210", "abs": "https://arxiv.org/abs/2506.03210", "authors": ["Qiusheng Huang", "Yuan Niu", "Xiaohui Zhong", "Anboyu Guo", "Lei Chen", "Dianjun Zhang", "Xuefeng Zhang", "Hao Li"], "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate, high-resolution ocean forecasting is crucial for maritime\noperations and environmental monitoring. While traditional numerical models are\ncapable of producing sub-daily, eddy-resolving forecasts, they are\ncomputationally intensive and face challenges in maintaining accuracy at fine\nspatial and temporal scales. In contrast, recent data-driven approaches offer\nimproved computational efficiency and emerging potential, yet typically operate\nat daily resolution and struggle with sub-daily predictions due to error\naccumulation over time. We introduce FuXi-Ocean, the first data-driven global\nocean forecasting model achieving six-hourly predictions at eddy-resolving\n1/12{\\deg} spatial resolution, reaching depths of up to 1500 meters. The model\narchitecture integrates a context-aware feature extraction module with a\npredictive network employing stacked attention blocks. The core innovation is\nthe Mixture-of-Time (MoT) module, which adaptively integrates predictions from\nmultiple temporal contexts by learning variable-specific reliability ,\nmitigating cumulative errors in sequential forecasting. Through comprehensive\nexperimental evaluation, FuXi-Ocean demonstrates superior skill in predicting\nkey variables, including temperature, salinity, and currents, across multiple\ndepths."}
{"id": "2406.05298", "pdf": "https://arxiv.org/pdf/2406.05298", "abs": "https://arxiv.org/abs/2406.05298", "authors": ["Ryan Langman", "Ante Jukić", "Kunal Dhawan", "Nithin Rao Koluguri", "Jason Li"], "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs", "categories": ["eess.AS"], "comment": null, "summary": "Historically, most speech models in machine-learning have used the\nmel-spectrogram as a speech representation. Recently, discrete audio tokens\nproduced by neural audio codecs have become a popular alternate speech\nrepresentation for speech synthesis tasks such as text-to-speech (TTS).\nHowever, the data distribution produced by such codecs is too complex for some\nTTS models to predict, typically requiring large autoregressive models to get\ngood quality. Most existing audio codecs use Residual Vector Quantization (RVQ)\nto compress and reconstruct the time-domain audio signal. We propose a spectral\ncodec which uses Finite Scalar Quantization (FSQ) to compress the\nmel-spectrogram and reconstruct the time-domain audio signal. A study of\nobjective audio quality metrics and subjective listening tests suggests that\nour spectral codec has comparable perceptual quality to equivalent audio\ncodecs. We show that FSQ, and the use of spectral speech representations, can\nboth improve the performance of parallel TTS models."}
{"id": "2506.03188", "pdf": "https://arxiv.org/pdf/2506.03188", "abs": "https://arxiv.org/abs/2506.03188", "authors": ["Madhu Babu Sikha", "Lalith Appari", "Gurudatt Nanjanagudu Ganesh", "Amay Bandodkar", "Imon Banerjee"], "title": "Multi-Analyte, Swab-based Automated Wound Monitor with AI", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "comment": "4 pages conference paper", "summary": "Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000\nindividuals every year in the US alone and identifying non-healing DFUs that\ndevelop to chronic wounds early can drastically reduce treatment costs and\nminimize risks of amputation. There is therefore a pressing need for diagnostic\ntools that can detect non-healing DFUs early. We develop a low cost,\nmulti-analyte 3D printed assays seamlessly integrated on swabs that can\nidentify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile\napplication developed for the controlled acquisition and automated analysis of\nwound sensor data. By comparing both the original base image (before exposure\nto the wound) and the wound-exposed image, we developed automated computer\nvision techniques to compare density changes between the two assay images,\nwhich allow us to automatically determine the severity of the wound. The iOS\napp ensures accurate data collection and presents actionable insights, despite\nchallenges such as variations in camera configurations and ambient conditions.\nThe proposed integrated sensor and iOS app will allow healthcare professionals\nto monitor wound conditions real-time, track healing progress, and assess\ncritical parameters related to wound care."}
{"id": "2503.02769", "pdf": "https://arxiv.org/pdf/2503.02769", "abs": "https://arxiv.org/abs/2503.02769", "authors": ["Dingdong Wang", "Jin Xu", "Ruihang Chu", "Zhifang Guo", "Xiong Wang", "Jincenzi Wu", "Dongchao Yang", "Shengpeng Ji", "Junyang Lin"], "title": "InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "comment": "Accepted to ACL 2025; Data is available at:\n  https://huggingface.co/datasets/ddwang2000/SpeechInstructBench", "summary": "Recent advancements in speech large language models (SpeechLLMs) have\nattracted considerable attention. Nonetheless, current methods exhibit\nsuboptimal performance in adhering to speech instructions. Notably, the\nintelligence of models significantly diminishes when processing speech-form\ninput as compared to direct text-form input. Prior work has attempted to\nmitigate this semantic inconsistency between speech and text representations\nthrough techniques such as representation and behavior alignment, which involve\nthe meticulous design of data pairs during the post-training phase. In this\npaper, we introduce a simple and scalable training method called InSerter,\nwhich stands for Interleaved Speech-Text Representation Pre-training. InSerter\nis designed to pre-train large-scale unsupervised speech-text sequences, where\nthe speech is synthesized from randomly selected segments of an extensive text\ncorpus using text-to-speech conversion. Consequently, the model acquires the\nability to generate textual continuations corresponding to the provided speech\nsegments, obviating the need for intensive data design endeavors. To\nsystematically evaluate speech instruction-following capabilities, we introduce\nSpeechInstructBench, the first comprehensive benchmark specifically designed\nfor speech-oriented instruction-following tasks. Our proposed InSerter achieves\nSOTA performance in SpeechInstructBench and demonstrates superior or\ncompetitive results across diverse speech processing tasks."}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424", "abs": "https://arxiv.org/abs/2506.03424", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs."}
{"id": "2506.03191", "pdf": "https://arxiv.org/pdf/2506.03191", "abs": "https://arxiv.org/abs/2506.03191", "authors": ["Muhammad Islam", "Tao Huang", "Euijoon Ahn", "Usman Naseem"], "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an in-depth survey on the use of multimodal Generative\nArtificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)\nfor human motion understanding and generation, offering insights into emerging\nmethods, architectures, and their potential to advance realistic and versatile\nmotion synthesis. Focusing exclusively on text and motion modalities, this\nresearch investigates how textual descriptions can guide the generation of\ncomplex, human-like motion sequences. The paper explores various generative\napproaches, including autoregressive models, diffusion models, Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and\ntransformer-based models, by analyzing their strengths and limitations in terms\nof motion quality, computational efficiency, and adaptability. It highlights\nrecent advances in text-conditioned motion generation, where textual inputs are\nused to control and refine motion outputs with greater precision. The\nintegration of LLMs further enhances these models by enabling semantic\nalignment between instructions and motion, improving coherence and contextual\nrelevance. This systematic survey underscores the transformative potential of\ntext-to-motion GenAI and LLM architectures in applications such as healthcare,\nhumanoids, gaming, animation, and assistive technologies, while addressing\nongoing challenges in generating efficient and realistic human motion."}
{"id": "2506.03939", "pdf": "https://arxiv.org/pdf/2506.03939", "abs": "https://arxiv.org/abs/2506.03939", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git."}
{"id": "2506.03225", "pdf": "https://arxiv.org/pdf/2506.03225", "abs": "https://arxiv.org/abs/2506.03225", "authors": ["Waël Doulazmi", "Auguste Lehuger", "Marin Toromanoff", "Valentin Charraut", "Thibault Buhet", "Fabien Moutarde"], "title": "Multiple-Frequencies Population-Based Training", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "Accepted at RLC25", "summary": "Reinforcement Learning's high sensitivity to hyperparameters is a source of\ninstability and inefficiency, creating significant challenges for\npractitioners. Hyperparameter Optimization (HPO) algorithms have been developed\nto address this issue, among them Population-Based Training (PBT) stands out\nfor its ability to generate hyperparameters schedules instead of fixed\nconfigurations. PBT trains a population of agents, each with its own\nhyperparameters, frequently ranking them and replacing the worst performers\nwith mutations of the best agents. These intermediate selection steps can cause\nPBT to focus on short-term improvements, leading it to get stuck in local\noptima and eventually fall behind vanilla Random Search over longer timescales.\nThis paper studies how this greediness issue is connected to the choice of\nevolution frequency, the rate at which the selection is done. We propose\nMultiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm\nthat addresses greediness by employing sub-populations, each evolving at\ndistinct frequencies. MF-PBT introduces a migration process to transfer\ninformation between sub-populations, with an asymmetric design to balance short\nand long-term optimization. Extensive experiments on the Brax suite demonstrate\nthat MF-PBT improves sample efficiency and long-term performance, even without\nactually tuning hyperparameters."}
{"id": "2504.12423", "pdf": "https://arxiv.org/pdf/2504.12423", "abs": "https://arxiv.org/abs/2504.12423", "authors": ["Haohan Shi", "Xiyu Shi", "Safak Dogan", "Saif Alzubi", "Tianjin Huang", "Yunxiao Zhang"], "title": "Benchmarking Audio Deepfake Detection Robustness in Real-world Communication Scenarios", "categories": ["eess.AS", "eess.SP"], "comment": "Accepted by EUSIPCO 2025", "summary": "Existing Audio Deepfake Detection (ADD) systems often struggle to generalise\neffectively due to the significantly degraded audio quality caused by audio\ncodec compression and channel transmission effects in real-world communication\nscenarios. To address this challenge, we developed a rigorous benchmark to\nevaluate the performance of the ADD system under such scenarios. We introduced\nADD-C, a new test dataset to evaluate the robustness of ADD systems under\ndiverse communication conditions, including different combinations of audio\ncodecs for compression and packet loss rates. Benchmarking three baseline ADD\nmodels on the ADD-C dataset demonstrated a significant decline in robustness\nunder such conditions. A novel Data Augmentation (DA) strategy was proposed to\nimprove the robustness of ADD systems. Experimental results demonstrated that\nthe proposed approach significantly enhances the performance of ADD systems on\nthe proposed ADD-C dataset. Our benchmark can assist future efforts towards\nbuilding practical and robustly generalisable ADD systems."}
{"id": "2506.03192", "pdf": "https://arxiv.org/pdf/2506.03192", "abs": "https://arxiv.org/abs/2506.03192", "authors": ["Basudha Pal", "Rama Chellappa", "Muhammad Umair"], "title": "Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "While echocardiography and MRI are clinical standards for evaluating cardiac\nstructure, their use is limited by cost and accessibility.We introduce a direct\nclassification framework that predicts severe left ventricular hypertrophy from\nchest X-rays, without relying on anatomical measurements or demographic inputs.\nOur approach achieves high AUROC and AUPRC, and employs Mutual Information\nNeural Estimation to quantify feature expressivity. This reveals clinically\nmeaningful attribute encoding and supports transparent model interpretation."}
{"id": "2505.14470", "pdf": "https://arxiv.org/pdf/2505.14470", "abs": "https://arxiv.org/abs/2505.14470", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST"}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434", "abs": "https://arxiv.org/abs/2506.03434", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich Schütze"], "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs."}
{"id": "2506.03193", "pdf": "https://arxiv.org/pdf/2506.03193", "abs": "https://arxiv.org/abs/2506.03193", "authors": ["Ekram Alam", "Abu Sufian", "Paramartha Dutta", "Marco Leo"], "title": "Human Fall Detection using Transfer Learning-based 3D CNN", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unintentional or accidental falls are one of the significant health issues in\nsenior persons. The population of senior persons is increasing steadily. So,\nthere is a need for an automated fall detection monitoring system. This paper\nintroduces a vision-based fall detection system using a pre-trained 3D CNN.\nUnlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The\nproposed model leverages the original learned weights of a 3D CNN model\npre-trained on the Sports1M dataset to extract the spatio-temporal features.\nOnly the SVM classifier was trained, which saves the time required to train the\n3D CNN. Stratified shuffle five split cross-validation has been used to split\nthe dataset into training and testing data. Extracted features from the\nproposed 3D CNN model were fed to an SVM classifier to classify the activity as\nfall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the\nexperiment. The source code for this work can be accessed via the following\nlink: https://github.com/ekramalam/HFD_3DCNN."}
{"id": "2506.03997", "pdf": "https://arxiv.org/pdf/2506.03997", "abs": "https://arxiv.org/abs/2506.03997", "authors": ["Mario Alviano", "Laura Giordano", "Daniele Theseider Dupré"], "title": "A framework for Conditional Reasoning in Answer Set Programming", "categories": ["cs.AI", "cs.LO", "I.2.4"], "comment": "19 pages", "summary": "In this paper we introduce a Conditional Answer Set Programming framework\n(Conditional ASP) for the definition of conditional extensions of Answer Set\nProgramming (ASP). The approach builds on a conditional logic with typicality,\nand on the combination of a conditional knowledge base with an ASP program, and\nallows for conditional reasoning over the answer sets of the program. The\nformalism relies on a multi-preferential semantics (and on the KLM preferential\nsemantics, as a special case) to provide an interpretation of conditionals."}
{"id": "2506.03227", "pdf": "https://arxiv.org/pdf/2506.03227", "abs": "https://arxiv.org/abs/2506.03227", "authors": ["Abdelrahman Sayed Sayed", "Pierre-Jean Meyer", "Mohamed Ghazel"], "title": "Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 5 figures, Accepted for publication in the proceedings of\n  the 8th International Symposium on AI Verification SAIV 2025", "summary": "A neural ordinary differential equation (neural ODE) is a machine learning\nmodel that is commonly described as a continuous depth generalization of a\nresidual network (ResNet) with a single residual block, or conversely, the\nResNet can be seen as the Euler discretization of the neural ODE. These two\nmodels are therefore strongly related in a way that the behaviors of either\nmodel are considered to be an approximation of the behaviors of the other. In\nthis work, we establish a more formal relationship between these two models by\nbounding the approximation error between two such related models. The obtained\nerror bound then allows us to use one of the models as a verification proxy for\nthe other, without running the verification tools twice: if the reachable\noutput set expanded by the error bound satisfies a safety property on one of\nthe models, this safety property is then guaranteed to be also satisfied on the\nother model. This feature is fully reversible, and the initial safety\nverification can be run indifferently on either of the two models. This novel\napproach is illustrated on a numerical example of a fixed-point attractor\nsystem modeled as a neural ODE."}
{"id": "2505.15965", "pdf": "https://arxiv.org/pdf/2505.15965", "abs": "https://arxiv.org/abs/2505.15965", "authors": ["Gowtham Premananth", "Vinith Kugathasan", "Carol Espy-Wilson"], "title": "Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives", "categories": ["eess.AS", "eess.SP"], "comment": "Accepted to be presented at Interspeech 2025", "summary": "Advancements in AI-driven speech-based applications have transformed diverse\nindustries ranging from healthcare to customer service. However, the increasing\nprevalence of non-native accented speech in global interactions poses\nsignificant challenges for speech-processing systems, which are often trained\non datasets dominated by native speech. This study investigates accented\nEnglish speech through articulatory and acoustic analysis, identifying simpler\ncoordination patterns and higher average pitch than native speech. Using\neigenspectra and Vocal Tract Variable-based coordination features, we establish\nan efficient method for quantifying accent strength without relying on\nresource-intensive phonetic transcriptions. Our findings provide a new avenue\nfor research on the impacts of accents on speech intelligibility and offer\ninsights for developing inclusive, robust speech processing systems that\naccommodate diverse linguistic communities."}
{"id": "2506.03202", "pdf": "https://arxiv.org/pdf/2506.03202", "abs": "https://arxiv.org/abs/2506.03202", "authors": ["Itxasne Antúnez Sáenz", "Ane Alberdi Aramendi", "David Dunaway", "Juling Ong", "Lara Deliège", "Amparo Sáenz", "Anita Ahmadi Birjandi", "Noor UI Owase Jeelani", "Silvia Schievano", "Alessandro Borghi"], "title": "A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "11 pages, 16 figures", "summary": "Craniosynostosis is a medical condition that affects the growth of babies'\nheads, caused by an early fusion of cranial sutures. In recent decades,\nsurgical treatments for craniosynostosis have significantly improved, leading\nto reduced invasiveness, faster recovery, and less blood loss. At Great Ormond\nStreet Hospital (GOSH), the main surgical treatment for patients diagnosed with\nsagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This\nprocedure involves a 15x15 mm2 osteotomy, where two springs are inserted to\ninduce distraction. Despite the numerous advantages of this surgical technique\nfor patients, the outcome remains unpredictable due to the lack of efficient\npreoperative planning tools. The surgeon's experience and the baby's age are\ncurrently relied upon to determine the osteotomy location and spring selection.\nPrevious tools for predicting the surgical outcome of SC relied on finite\nelement modeling (FEM), which involved computed tomography (CT) imaging and\nrequired engineering expertise and lengthy calculations. The main goal of this\nresearch is to develop a real-time prediction tool for the surgical outcome of\npatients, eliminating the need for CT scans to minimise radiation exposure\nduring preoperative planning. The proposed methodology involves creating\npersonalised synthetic skulls based on three-dimensional (3D) photographs,\nincorporating population average values of suture location, skull thickness,\nand soft tissue properties. A machine learning (ML) surrogate model is employed\nto achieve the desired surgical outcome. The resulting multi-output support\nvector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.\nFurthermore, in the future, this model could not only simulate various surgical\nscenarios but also provide optimal parameters for achieving a maximum cranial\nindex (CI)."}
{"id": "2303.11607", "pdf": "https://arxiv.org/pdf/2303.11607", "abs": "https://arxiv.org/abs/2303.11607", "authors": ["Siddique Latif", "Aun Zaidi", "Heriberto Cuayahuitl", "Fahad Shamshad", "Moazzam Shoukat", "Muhammad Usama", "Junaid Qadir"], "title": "Transformers in Speech Processing: A Survey", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Computer Science Review 2025", "summary": "The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues."}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458", "abs": "https://arxiv.org/abs/2506.03458", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "title": "Culture Matters in Toxic Language Detection in Persian", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection."}
{"id": "2506.03194", "pdf": "https://arxiv.org/pdf/2506.03194", "abs": "https://arxiv.org/abs/2506.03194", "authors": ["Rynaa Grover", "Jayant Sravan Tamarapalli", "Sahiti Yerramilli", "Nilay Pande"], "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel at high-level visual\nreasoning, but their performance on nuanced perceptual tasks remains\nsurprisingly limited. We present HueManity, a benchmark designed to assess\nvisual perception in MLLMs. The dataset comprises 83,850 images featuring\ntwo-character alphanumeric strings embedded in Ishihara test style dot\npatterns, challenging models on precise pattern recognition. Our evaluation of\nnine state-of-the-art MLLMs on HueManity demonstrates a significant performance\ndeficit compared to human and traditional computer vision baselines. The\nbest-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a\nstriking 3% on the alphanumeric `hard' task. In contrast, human participants\nachieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model\nreached accuracies of 96.5% and 94.5%. These results highlight a critical gap\nin the visual capabilities of current MLLMs. Our analysis further explores\npotential architectural and training-paradigm factors contributing to this\nperceptual gap in MLLMs. We open-source HueManity dataset and code to foster\nfurther research in improving perceptual robustness of MLLMs."}
{"id": "2506.04018", "pdf": "https://arxiv.org/pdf/2506.04018", "abs": "https://arxiv.org/abs/2506.04018", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Gouné", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "comment": "Prepint, under review for NeurIPS 2025", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent."}
{"id": "2506.03230", "pdf": "https://arxiv.org/pdf/2506.03230", "abs": "https://arxiv.org/abs/2506.03230", "authors": ["Selcuk Gurses", "Aozhong Zhang", "Yanxia Deng", "Xun Dong", "Xin Li", "Naigang Wang", "Penghang Yin", "Zi Yang"], "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Finetuning is a critical step for adapting large language models (LLMs) to\ndomain-specific downstream tasks. To mitigate the substantial computational and\nmemory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)\nmethods have been proposed to update only a small subset of model parameters.\nHowever, performance gaps between PEFT approaches and full-model fine-tuning\nstill exist. In this work, we present DiaBlo, a simple yet effective PEFT\napproach that updates only the diagonal blocks of selected model weight\nmatrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates\nthe need for low rank matrix products, thereby avoiding the reliance on\nauxiliary initialization schemes or customized optimization strategies to\nimprove convergence. This design leads to stable and robust convergence while\nmaintaining comparable memory efficiency and training speed to LoRA. We conduct\nextensive experiments across a range of tasks, including commonsense reasoning,\narithmetic reasoning, code generation, and safety alignment, to evaluate the\neffectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo\ndemonstrates strong and consistent performance while maintaining high memory\nefficiency and fast finetuning speed. Codes are available at\nhttps://github.com/ziyangjoy/DiaBlo."}
{"id": "2505.16044", "pdf": "https://arxiv.org/pdf/2505.16044", "abs": "https://arxiv.org/abs/2505.16044", "authors": ["Gowtham Premananth", "Philip Resnik", "Sonia Bansal", "Deanna L. Kelly", "Carol Espy-Wilson"], "title": "Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation", "categories": ["eess.AS", "cs.LG", "eess.IV", "eess.SP"], "comment": "Accepted to be presented at Interspeech 2025", "summary": "Studies on schizophrenia assessments using deep learning typically treat it\nas a classification task to detect the presence or absence of the disorder,\noversimplifying the condition and reducing its clinical applicability. This\ntraditional approach overlooks the complexity of schizophrenia, limiting its\npractical value in healthcare settings. This study shifts the focus to\nindividual symptom severity estimation using a multimodal approach that\nintegrates speech, video, and text inputs. We develop unimodal models for each\nmodality and a multimodal framework to improve accuracy and robustness. By\ncapturing a more detailed symptom profile, this approach can help in enhancing\ndiagnostic precision and support personalized treatment, offering a scalable\nand objective tool for mental health assessment."}
{"id": "2506.03216", "pdf": "https://arxiv.org/pdf/2506.03216", "abs": "https://arxiv.org/abs/2506.03216", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter Eklund", "Sunil Aryal"], "title": "A Survey of Deep Learning Video Super-Resolution", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "This paper has been published in IEEE Transactions on Emerging Topics\n  in Computational Intelligence, vol. 8, no. 4, pp. 2655-2676, Aug. 2024, doi:\n  10.1109/TETCI.2024.3398015", "summary": "Video super-resolution (VSR) is a prominent research topic in low-level\ncomputer vision, where deep learning technologies have played a significant\nrole. The rapid progress in deep learning and its applications in VSR has led\nto a proliferation of tools and techniques in the literature. However, the\nusage of these methods is often not adequately explained, and decisions are\nprimarily driven by quantitative improvements. Given the significance of VSR's\npotential influence across multiple domains, it is imperative to conduct a\ncomprehensive analysis of the elements and deep learning methodologies employed\nin VSR research. This methodical analysis will facilitate the informed\ndevelopment of models tailored to specific application needs. In this paper, we\npresent an overarching overview of deep learning-based video super-resolution\nmodels, investigating each component and discussing its implications.\nFurthermore, we provide a synopsis of key components and technologies employed\nby state-of-the-art and earlier VSR models. By elucidating the underlying\nmethodologies and categorising them systematically, we identified trends,\nrequirements, and challenges in the domain. As a first-of-its-kind survey of\ndeep learning-based VSR models, this work also establishes a multi-level\ntaxonomy to guide current and future VSR research, enhancing the maturation and\ninterpretation of VSR practices for various practical applications."}
{"id": "2505.19931", "pdf": "https://arxiv.org/pdf/2505.19931", "abs": "https://arxiv.org/abs/2505.19931", "authors": ["Qixi Zheng", "Yushen Chen", "Zhikang Niu", "Ziyang Ma", "Xiaofei Wang", "Kai Yu", "Xie Chen"], "title": "Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling", "categories": ["eess.AS", "cs.SD"], "comment": null, "summary": "Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS,\nand F5-TTS, have attracted significant attention in recent years. These models\nrequire multiple sampling steps to reconstruct speech from noise, making\ninference speed a key challenge. Reducing the number of sampling steps can\ngreatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a\ntraining-free approach to accelerate the inference of flow-matching-based TTS\nmodels. By inspecting the sampling trajectory of F5-TTS, we identify redundant\nsteps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform\ntime-step sampling strategy that effectively reduces the number of sampling\nsteps. Our approach achieves a 7-step generation with an inference RTF of 0.030\non an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS\nwhile maintaining comparable performance. Furthermore, EPSS performs well on E2\nTTS models, demonstrating its strong generalization ability."}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476", "abs": "https://arxiv.org/abs/2506.03476", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers."}
{"id": "2506.03195", "pdf": "https://arxiv.org/pdf/2506.03195", "abs": "https://arxiv.org/abs/2506.03195", "authors": ["Yunqi Hong", "Sohyun An", "Andrew Bai", "Neil Y. C. Lin", "Cho-Jui Hsieh"], "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite Multimodal Large Language Models (MLLMs) showing promising results on\ngeneral zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to\ndistinguish between visually similar subcategories--details that MLLMs may\neasily overlook without explicit guidance. To address this, we introduce\nAutoSEP, an iterative self-supervised prompt learning framework designed to\nenhance MLLM fine-grained classification capabilities in a fully unsupervised\nmanner. Our core idea is to leverage unlabeled data to learn a description\nprompt that guides MLLMs in identifying crucial discriminative features within\nan image, and boosts classification accuracy. We developed an automatic\nself-enhancing prompt learning framework called AutoSEP to iteratively improve\nthe description prompt using unlabeled data, based on instance-level\nclassification scoring function. AutoSEP only requires black-box access to\nMLLMs, eliminating the need for any training or fine-tuning. We evaluate our\napproach on multiple fine-grained classification datasets. It consistently\noutperforms other unsupervised baselines, demonstrating the effectiveness of\nour self-supervised optimization framework. Notably, AutoSEP on average\nimproves 13 percent over standard zero-shot classification and 5 percent over\nthe best-performing baselines. Code is available at:\nhttps://github.com/yq-hong/AutoSEP"}
{"id": "2506.04022", "pdf": "https://arxiv.org/pdf/2506.04022", "abs": "https://arxiv.org/abs/2506.04022", "authors": ["Qiyue Xia", "J. Michael Herrmann"], "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-objective reinforcement learning (MORL) aims at optimising several,\noften conflicting goals in order to improve flexibility and reliability of RL\nin practical tasks. This can be achieved by finding diverse policies that are\noptimal for some objective preferences and non-dominated by optimal policies\nfor other preferences so that they form a Pareto front in the multi-objective\nperformance space. The relation between the multi-objective performance space\nand the parameter space that represents the policies is generally non-unique.\nUsing a training scheme that is based on a locally linear map between the\nparameter space and the performance space, we show that an approximate Pareto\nfront can provide an interpretation of the current parameter vectors in terms\nof the objectives which enables an effective search within contiguous solution\ndomains. Experiments are conducted with and without retraining across different\ndomains, and the comparison with previous methods demonstrates the efficiency\nof our approach."}
{"id": "2506.03234", "pdf": "https://arxiv.org/pdf/2506.03234", "abs": "https://arxiv.org/abs/2506.03234", "authors": ["Kaiwen Duan", "Hongwei Yao", "Yufei Chen", "Ziyun Li", "Tong Qiao", "Zhan Qin", "Cong Wang"], "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\ntext-to-image (T2I) models with human preferences. However, RLHF's feedback\nmechanism also opens new pathways for adversaries. This paper demonstrates the\nfeasibility of hijacking T2I models by poisoning a small fraction of preference\ndata with natural-appearing examples. Specifically, we propose BadReward, a\nstealthy clean-label poisoning attack targeting the reward model in multi-modal\nRLHF. BadReward operates by inducing feature collisions between visually\ncontradicted preference data instances, thereby corrupting the reward model and\nindirectly compromising the T2I model's integrity. Unlike existing alignment\npoisoning techniques focused on single (text) modality, BadReward is\nindependent of the preference annotation process, enhancing its stealth and\npractical threat. Extensive experiments on popular T2I models show that\nBadReward can consistently guide the generation towards improper outputs, such\nas biased or violent imagery, for targeted concepts. Our findings underscore\nthe amplified threat landscape for RLHF in multi-modal systems, highlighting\nthe urgent need for robust defenses. Disclaimer. This paper contains uncensored\ntoxic content that might be offensive or disturbing to the readers."}
{"id": "2503.04721", "pdf": "https://arxiv.org/pdf/2503.04721", "abs": "https://arxiv.org/abs/2503.04721", "authors": ["Guan-Ting Lin", "Jiachen Lian", "Tingle Li", "Qirui Wang", "Gopala Anumanchipalli", "Alexander H. Liu", "Hung-yi Lee"], "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities", "categories": ["cs.CL", "eess.AS"], "comment": "Work in Progress", "summary": "Spoken dialogue modeling poses challenges beyond text-based language\nmodeling, requiring real-time interaction, turn-taking, and backchanneling.\nWhile most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing\none turn at a time - emerging full-duplex SDMs can listen and speak\nsimultaneously, enabling more natural conversations. However, current\nevaluations remain limited, focusing mainly on turn-based metrics or coarse\ncorpus-level analyses. To address this, we introduce Full-Duplex-Bench, a\nbenchmark that systematically evaluates key interactive behaviors: pause\nhandling, backchanneling, turn-taking, and interruption management. Our\nframework uses automatic metrics for consistent, reproducible assessment and\nprovides a fair, fast evaluation setup. By releasing our benchmark and code, we\naim to advance spoken dialogue modeling and foster the development of more\nnatural and engaging SDMs."}
{"id": "2506.03217", "pdf": "https://arxiv.org/pdf/2506.03217", "abs": "https://arxiv.org/abs/2506.03217", "authors": ["Pierrick Coupé", "Boris Mansencal", "Floréal Morandat", "Sergio Morell-Ortega", "Nicolas Villain", "Jose V. Manjón", "Vincent Planche"], "title": "petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles\n(T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's\ndisease (AD) diagnosis and prognosis. Existing pipelines face limitations\nregarding processing time, variability in tracer types, and challenges in\nmultimodal integration.\n  METHODS: We developed petBrain, a novel end-to-end processing pipeline for\namyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based\nsegmentation, standardized biomarker quantification (Centiloid, CenTauR,\nHAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is\nimplemented as a web-based platform, requiring no local computational\ninfrastructure or specialized software knowledge.\n  RESULTS: petBrain provides reliable and rapid biomarker quantification, with\nresults comparable to existing pipelines for A and T2. It shows strong\nconcordance with data processed in ADNI databases. The staging and\nquantification of A/T2/N by petBrain demonstrated good agreement with\nCSF/plasma biomarkers, clinical status, and cognitive performance.\n  DISCUSSION: petBrain represents a powerful and openly accessible platform for\nstandardized AD biomarker analysis, facilitating applications in clinical\nresearch."}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability."}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding."}
{"id": "2506.04133", "pdf": "https://arxiv.org/pdf/2506.04133", "abs": "https://arxiv.org/abs/2506.04133", "authors": ["Shaina Raza", "Ranjan Sapkota", "Manoj Karkee", "Christos Emmanouilidis"], "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment."}
{"id": "2506.03267", "pdf": "https://arxiv.org/pdf/2506.03267", "abs": "https://arxiv.org/abs/2506.03267", "authors": ["Shahbaz Rezaei", "Avishai Halev", "Xin Liu"], "title": "On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "A prevailing approach to explain time series models is to generate\nattribution in time domain. A recent development in time series XAI is the\nconcept of explanation spaces, where any model trained in the time domain can\nbe interpreted with any existing XAI method in alternative domains, such as\nfrequency. The prevailing approach is to present XAI attributions either in the\ntime domain or in the domain where the attribution is most sparse. In this\npaper, we demonstrate that in certain cases, XAI methods can generate\nattributions that highlight fundamentally different features in the time and\nfrequency domains that are not direct counterparts of one another. This\nsuggests that both domains' attributions should be presented to achieve a more\ncomprehensive interpretation. Thus it shows the necessity of multi-domain\nexplanation. To quantify when such cases arise, we introduce the uncertainty\nprinciple (UP), originally developed in quantum mechanics and later studied in\nharmonic analysis and signal processing, to the XAI literature. This principle\nestablishes a lower bound on how much a signal can be simultaneously localized\nin both the time and frequency domains. By leveraging this concept, we assess\nwhether attributions in the time and frequency domains violate this bound,\nindicating that they emphasize distinct features. In other words, UP provides a\nsufficient condition that the time and frequency domain explanations do not\nmatch and, hence, should be both presented to the end user. We validate the\neffectiveness of this approach across various deep learning models, XAI\nmethods, and a wide range of classification and forecasting datasets. The\nfrequent occurrence of UP violations across various datasets and XAI methods\nhighlights the limitations of existing approaches that focus solely on\ntime-domain explanations. This underscores the need for multi-domain\nexplanations as a new paradigm."}
{"id": "2506.03238", "pdf": "https://arxiv.org/pdf/2506.03238", "abs": "https://arxiv.org/abs/2506.03238", "authors": ["Ziheng Zhao", "Lisong Dai", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics."}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484", "abs": "https://arxiv.org/abs/2506.03484", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training."}
{"id": "2506.03198", "pdf": "https://arxiv.org/pdf/2506.03198", "abs": "https://arxiv.org/abs/2506.03198", "authors": ["Hao Yin", "Lijun Gu", "Paritosh Parmar", "Lin Xu", "Tianxiao Guo", "Weiwei Fu", "Yang Zhang", "Tianyou Zheng"], "title": "FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the increasing awareness of health and the growing desire for aesthetic\nphysique, fitness has become a prevailing trend. However, the potential risks\nassociated with fitness training, especially with weight-loaded fitness\nactions, cannot be overlooked. Action Quality Assessment (AQA), a technology\nthat quantifies the quality of human action and provides feedback, holds the\npotential to assist fitness enthusiasts of varying skill levels in achieving\nbetter training outcomes. Nevertheless, current AQA methodologies and datasets\nare limited to single-view competitive sports scenarios and RGB modality and\nlack professional assessment and guidance of fitness actions. To address this\ngap, we propose the FLEX dataset, the first multi-modal, multi-action,\nlarge-scale dataset that incorporates surface electromyography (sEMG) signals\ninto AQA. FLEX utilizes high-precision MoCap to collect 20 different\nweight-loaded actions performed by 38 subjects across 3 different skill levels\nfor 10 repetitions each, containing 5 different views of the RGB video, 3D\npose, sEMG, and physiological information. Additionally, FLEX incorporates\nknowledge graphs into AQA, constructing annotation rules in the form of penalty\nfunctions that map weight-loaded actions, action keysteps, error types, and\nfeedback. We conducted various baseline methodologies on FLEX, demonstrating\nthat multimodal data, multiview data, and fine-grained annotations\nsignificantly enhance model performance. FLEX not only advances AQA\nmethodologies and datasets towards multi-modal and multi-action scenarios but\nalso fosters the integration of artificial intelligence within the fitness\ndomain. Dataset and code are available at\nhttps://haoyin116.github.io/FLEX_Dataset."}
{"id": "2506.04135", "pdf": "https://arxiv.org/pdf/2506.04135", "abs": "https://arxiv.org/abs/2506.04135", "authors": ["Pei Yang", "Hai Ci", "Mike Zheng Shou"], "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents show promising capabilities for\nautomating computer-use tasks and facilitating accessibility, but existing\ninteractive benchmarks are mostly English-only, covering web-use or Windows,\nLinux, and Android environments, but not macOS. macOS is a major OS with\ndistinctive GUI patterns and exclusive applications. To bridge the gaps, we\npresent macOSWorld, the first comprehensive benchmark for evaluating GUI agents\non macOS. macOSWorld features 202 multilingual interactive tasks across 30\napplications (28 macOS-exclusive), with task instructions and OS interfaces\noffered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As\nGUI agents are shown to be vulnerable to deception attacks, macOSWorld also\nincludes a dedicated safety benchmarking subset. Our evaluation on six GUI\nagents reveals a dramatic gap: proprietary computer-use agents lead at above\n30% success rate, while open-source lightweight research models lag at below\n2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks\nalso expose common weaknesses, especially in Arabic, with a 27.5% average\ndegradation compared to English. Results from safety benchmarking also\nhighlight that deception attacks are more general and demand immediate\nattention. macOSWorld is available at https://github.com/showlab/macosworld."}
{"id": "2506.03302", "pdf": "https://arxiv.org/pdf/2506.03302", "abs": "https://arxiv.org/abs/2506.03302", "authors": ["James Bagrow", "Josh Bongard"], "title": "Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony", "categories": ["cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "comment": "14 pages, 7 figures, 2 tables", "summary": "Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with\ninterpretability, making them valuable for scientific modeling. However, it is\nunclear a priori how deep a network needs to be for any given task, and deeper\nKANs can be difficult to optimize. Here we introduce multi-exit KANs, where\neach layer includes its own prediction branch, enabling the network to make\naccurate predictions at multiple depths simultaneously. This architecture\nprovides deep supervision that improves training while discovering the right\nlevel of model complexity for each task. Multi-exit KANs consistently\noutperform standard, single-exit versions on synthetic functions, dynamical\nsystems, and real-world datasets. Remarkably, the best predictions often come\nfrom earlier, simpler exits, revealing that these networks naturally identify\nsmaller, more parsimonious and interpretable models without sacrificing\naccuracy. To automate this discovery, we develop a differentiable \"learning to\nexit\" algorithm that balances contributions from exits during training. Our\napproach offers scientists a practical way to achieve both high performance and\ninterpretability, addressing a fundamental challenge in machine learning for\nscientific discovery."}
{"id": "2506.03420", "pdf": "https://arxiv.org/pdf/2506.03420", "abs": "https://arxiv.org/abs/2506.03420", "authors": ["Muhammad Zubair Hasan", "Fahmida Yasmin Rifat"], "title": "Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Written as per the requirements of CVPR 2025. It is a 8 page paper\n  without reference", "summary": "Skin cancer is among the most prevalent and life-threatening diseases\nworldwide, with early detection being critical to patient outcomes. This work\npresents a hybrid machine and deep learning-based approach for classifying\nmalignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,\nwhich comprises 401,059 cropped lesion images extracted from 3D Total Body\nPhotography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our\nmethod combines vision transformers (EVA02) and our designed convolutional ViT\nhybrid (EdgeNeXtSAC) to extract robust features, employing a\nsegmentation-assisted classification pipeline to enhance lesion localization.\nPredictions from these models are fused with a gradient-boosted decision tree\n(GBDT) ensemble enriched by engineered features and patient-specific relational\nmetrics. To address class imbalance and improve generalization, we augment\nmalignant cases with Stable Diffusion-generated synthetic lesions and apply a\ndiagnosis-informed relabeling strategy to harmonize external datasets into a\n3-class format. Using partial AUC (pAUC) above 80 percent true positive rate\n(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the\nhighest among all configurations. These results underscore the potential of\nhybrid, interpretable AI systems for skin cancer triage in telemedicine and\nresource-constrained settings."}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489", "abs": "https://arxiv.org/abs/2506.03489", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe."}
{"id": "2506.03211", "pdf": "https://arxiv.org/pdf/2506.03211", "abs": "https://arxiv.org/abs/2506.03211", "authors": ["Wanting Yang", "Zehui Xiong", "Qianqian Yang", "Ping Zhang", "Merouane Debbah", "Rahim Tafazolli"], "title": "Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission", "categories": ["cs.CV", "cs.NI"], "comment": null, "summary": "With the rapid development of autonomous driving and extended reality,\nefficient transmission of point clouds (PCs) has become increasingly important.\nIn this context, we propose a novel channel-adaptive cross-modal generative\nsemantic communication (SemCom) for PC transmission, called GenSeC-PC.\nGenSeC-PC employs a semantic encoder that fuses images and point clouds, where\nimages serve as non-transmitted side information. Meanwhile, the decoder is\nbuilt upon the backbone of PointDif. Such a cross-modal design not only ensures\nhigh compression efficiency but also delivers superior reconstruction\nperformance compared to PointDif. Moreover, to ensure robust transmission and\nreduce system complexity, we design a streamlined and asymmetric\nchannel-adaptive joint semantic-channel coding architecture, where only the\nencoder needs the feedback of average signal-to-noise ratio (SNR) and available\nbandwidth. In addition, rectified denoising diffusion implicit models is\nemployed to accelerate the decoding process to the millisecond level, enabling\nreal-time PC communication. Unlike existing methods, GenSeC-PC leverages\ngenerative priors to ensure reliable reconstruction even from noisy or\nincomplete source PCs. More importantly, it supports fully analog transmission,\nimproving compression efficiency by eliminating the need for error-free side\ninformation transmission common in prior SemCom approaches. Simulation results\nconfirm the effectiveness of cross-modal semantic extraction and dual-metric\nguided fine-tuning, highlighting the framework's robustness across diverse\nconditions, including low SNR, bandwidth limitations, varying numbers of 2D\nimages, and previously unseen objects."}
{"id": "2506.04210", "pdf": "https://arxiv.org/pdf/2506.04210", "abs": "https://arxiv.org/abs/2506.04210", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models."}
{"id": "2506.03307", "pdf": "https://arxiv.org/pdf/2506.03307", "abs": "https://arxiv.org/abs/2506.03307", "authors": ["Kristen Goebel", "William Solow", "Paola Pesantez-Cabrera", "Markus Keller", "Alan Fern"], "title": "Budgeted Online Active Learning with Expert Advice and Episodic Priors", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces a novel approach to budgeted online active learning\nfrom finite-horizon data streams with extremely limited labeling budgets. In\nagricultural applications, such streams might include daily weather data over a\ngrowing season, and labels require costly measurements of weather-dependent\nplant characteristics. Our method integrates two key sources of prior\ninformation: a collection of preexisting expert predictors and episodic\nbehavioral knowledge of the experts based on unlabeled data streams. Unlike\nprevious research on online active learning with experts, our work\nsimultaneously considers query budgets, finite horizons, and episodic\nknowledge, enabling effective learning in applications with severely limited\nlabeling capacity. We demonstrate the utility of our approach through\nexperiments on various prediction problems derived from both a realistic\nagricultural crop simulator and real-world data from multiple grape cultivars.\nThe results show that our method significantly outperforms baseline expert\npredictions, uniform query selection, and existing approaches that consider\nbudgets and limited horizons but neglect episodic knowledge, even under highly\nconstrained labeling budgets."}
{"id": "2506.03752", "pdf": "https://arxiv.org/pdf/2506.03752", "abs": "https://arxiv.org/abs/2506.03752", "authors": ["Gonçalo Mesquita", "Ana Rita Cóias", "Artur Dubrawski", "Alexandre Bernardino"], "title": "Frame-Level Real-Time Assessment of Stroke Rehabilitation Exercises from Video-Level Labeled Data: Task-Specific vs. Foundation Models", "categories": ["eess.IV"], "comment": null, "summary": "The growing demands of stroke rehabilitation have increased the need for\nsolutions to support autonomous exercising. Virtual coaches can provide\nreal-time exercise feedback from video data, helping patients improve motor\nfunction and keep engagement. However, training real-time motion analysis\nsystems demands frame-level annotations, which are time-consuming and costly to\nobtain. In this work, we present a framework that learns to classify individual\nframes from video-level annotations for real-time assessment of compensatory\nmotions in rehabilitation exercises. We use a gradient-based technique and a\npseudo-label selection method to create frame-level pseudo-labels for training\na frame-level classifier. We leverage pre-trained task-specific models - Action\nTransformer, SkateFormer - and a foundation model - MOMENT - for pseudo-label\ngeneration, aiming to improve generalization to new patients. To validate the\napproach, we use the \\textit{SERE} dataset with 18 post-stroke patients\nperforming five rehabilitation exercises annotated on compensatory motions.\nMOMENT achieves better video-level assessment results (AUC = $73\\%$),\noutperforming the baseline LSTM (AUC = $58\\%$). The Action Transformer, with\nthe Integrated Gradient technique, leads to better outcomes (AUC = $72\\%$) for\nframe-level assessment, outperforming the baseline trained with ground truth\nframe-level labeling (AUC = $69\\%$). We show that our proposed approach with\npre-trained models enhances model generalization ability and facilitates the\ncustomization to new patients, reducing the demands of data labeling."}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490", "abs": "https://arxiv.org/abs/2506.03490", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications."}
{"id": "2506.03213", "pdf": "https://arxiv.org/pdf/2506.03213", "abs": "https://arxiv.org/abs/2506.03213", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "ConMamba: Contrastive Vision Mamba for Plant Disease Detection", "categories": ["cs.CV"], "comment": null, "summary": "Plant Disease Detection (PDD) is a key aspect of precision agriculture.\nHowever, existing deep learning methods often rely on extensively annotated\ndatasets, which are time-consuming and costly to generate. Self-supervised\nLearning (SSL) offers a promising alternative by exploiting the abundance of\nunlabeled data. However, most existing SSL approaches suffer from high\ncomputational costs due to convolutional neural networks or transformer-based\narchitectures. Additionally, they struggle to capture long-range dependencies\nin visual representation and rely on static loss functions that fail to align\nlocal and global features effectively. To address these challenges, we propose\nConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates\nthe Vision Mamba Encoder (VME), which employs a bidirectional State Space Model\n(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce\na dual-level contrastive loss with dynamic weight adjustment to optimize\nlocal-global feature alignment. Experimental results on three benchmark\ndatasets demonstrate that ConMamba significantly outperforms state-of-the-art\nmethods across multiple evaluation metrics. This provides an efficient and\nrobust solution for PDD."}
{"id": "2209.01205", "pdf": "https://arxiv.org/pdf/2209.01205", "abs": "https://arxiv.org/abs/2209.01205", "authors": ["Han Wu", "Jie Yin", "Bala Rajaratnam", "Jianyuan Guo"], "title": "Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2"], "comment": "Published at ICLR 2023", "summary": "Knowledge graphs (KGs) are powerful in terms of their inference abilities,\nbut are also notorious for their incompleteness and long-tail distribution of\nrelations. To address these challenges and expand the coverage of KGs, few-shot\nKG completion aims to make predictions for triplets involving novel relations\nwhen only a few training triplets are provided as reference. Previous methods\nhave focused on designing local neighbor aggregators to learn entity-level\ninformation and/or imposing a potentially invalid sequential dependency\nassumption at the triplet level to learn meta relation information. However,\npairwise triplet-level interactions and context-level relational information\nhave been largely overlooked for learning meta representations of few-shot\nrelations. In this paper, we propose a hierarchical relational learning method\n(HiRe) for few-shot KG completion. By jointly capturing three levels of\nrelational information (entity-level, triplet-level and context-level), HiRe\ncan effectively learn and refine meta representations of few-shot relations,\nand thus generalize well to new unseen relations. Extensive experiments on\nbenchmark datasets validate the superiority of HiRe over state-of-the-art\nmethods. The code can be found in https://github.com/alexhw15/HiRe.git."}
{"id": "2506.03320", "pdf": "https://arxiv.org/pdf/2506.03320", "abs": "https://arxiv.org/abs/2506.03320", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 1 figure, accepted at TCAI workshop 2025", "summary": "Continual learning--the ability to acquire, retain, and refine knowledge over\ntime--has always been fundamental to intelligence, both human and artificial.\nHistorically, different AI paradigms have acknowledged this need, albeit with\nvarying priorities: early expert and production systems focused on incremental\nknowledge consolidation, while reinforcement learning emphasised dynamic\nadaptation. With the rise of deep learning, deep continual learning has\nprimarily focused on learning robust and reusable representations over time to\nsolve sequences of increasingly complex tasks. However, the emergence of Large\nLanguage Models (LLMs) and foundation models has raised the question: Do we\nstill need continual learning when centralised, monolithic models can tackle\ndiverse tasks with access to internet-scale knowledge? We argue that continual\nlearning remains essential for three key reasons: (i) continual pre-training is\nstill necessary to ensure foundation models remain up to date, mitigating\nknowledge staleness and distribution shifts while integrating new information;\n(ii) continual fine-tuning enables models to specialise and personalise,\nadapting to domain-specific tasks, user preferences, and real-world constraints\nwithout full retraining, avoiding the need for computationally expensive long\ncontext-windows; (iii) continual compositionality offers a scalable and modular\napproach to intelligence, enabling the orchestration of foundation models and\nagents to be dynamically composed, recombined, and adapted. While continual\npre-training and fine-tuning are explored as niche research directions, we\nargue it is continual compositionality that will mark the rebirth of continual\nlearning. The future of AI will not be defined by a single static model but by\nan ecosystem of continually evolving and interacting models, making continual\nlearning more relevant than ever."}
{"id": "2506.03890", "pdf": "https://arxiv.org/pdf/2506.03890", "abs": "https://arxiv.org/abs/2506.03890", "authors": ["Christian Tinauer", "Maximilian Sackl", "Stefan Ropele", "Christian Langkammer"], "title": "Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for the conference EUSIPCO2025 (https://eusipco2025.org/)", "summary": "Deep learning models have shown strong performance in classifying Alzheimer's\ndisease (AD) from R2* maps, but their decision-making remains opaque, raising\nconcerns about interpretability. Previous studies suggest biases in model\ndecisions, necessitating further analysis. This study uses Layer-wise Relevance\nPropagation (LRP) and spectral clustering to explore classifier decision\nstrategies across preprocessing and training configurations using R2* maps. We\ntrained a 3D convolutional neural network on R2* maps, generating relevance\nheatmaps via LRP and applied spectral clustering to identify dominant patterns.\nt-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess\nclustering structure. Spectral clustering revealed distinct decision patterns,\nwith the relevance-guided model showing the clearest separation between AD and\nnormal control (NC) cases. The t-SNE visualization confirmed that this model\naligned heatmap groupings with the underlying subject groups. Our findings\nhighlight the significant impact of preprocessing and training choices on deep\nlearning models trained on R2* maps, even with similar performance metrics.\nSpectral clustering offers a structured method to identify classification\nstrategy differences, emphasizing the importance of explainability in medical\nAI."}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501", "abs": "https://arxiv.org/abs/2506.03501", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector"}
{"id": "2506.03224", "pdf": "https://arxiv.org/pdf/2506.03224", "abs": "https://arxiv.org/abs/2506.03224", "authors": ["Jinwei Zeng", "Yu Liu", "Guozhen Zhang", "Jingtao Ding", "Yuming Lin", "Jian Yuan", "Yong Li"], "title": "OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data", "categories": ["cs.CV", "cs.AI", "physics.soc-ph"], "comment": "Accepted by IJCAI 2025", "summary": "Accurately estimating high-resolution carbon emissions is crucial for\neffective emission governance and mitigation planning. While conventional\nmethods for precise carbon accounting are hindered by substantial data\ncollection efforts, the rise of open data and advanced learning techniques\noffers a promising solution. Once an open data-based prediction model is\ndeveloped and trained, it can easily infer emissions for new areas based on\navailable open data. To address this, we incorporate two modalities of open\ndata, satellite images and point-of-interest (POI) data, to predict\nhigh-resolution urban carbon emissions, with satellite images providing\nmacroscopic and static and POI data offering fine-grained and relatively\ndynamic functionality information. However, estimating high-resolution carbon\nemissions presents two significant challenges: the intertwined and implicit\neffects of various functionalities on carbon emissions, and the complex spatial\ncontiguity correlations that give rise to the agglomeration effect. Our model,\nOpenCarbon, features two major designs that target the challenges: a\ncross-modality information extraction and fusion module to extract\ncomplementary functionality information from two modules and model their\ninteractions, and a neighborhood-informed aggregation module to capture the\nspatial contiguity correlations. Extensive experiments demonstrate our model's\nsuperiority, with a significant performance gain of 26.6\\% on R2. Further\ngeneralizability tests and case studies also show OpenCarbon's capacity to\ncapture the intrinsic relation between urban functionalities and carbon\nemissions, validating its potential to empower efficient carbon governance and\ntargeted carbon mitigation planning. Codes and data are available:\nhttps://github.com/JinweiZzz/OpenCarbon."}
{"id": "2506.03324", "pdf": "https://arxiv.org/pdf/2506.03324", "abs": "https://arxiv.org/abs/2506.03324", "authors": ["Ethan Che", "Hakan Ceylan", "James McInerney", "Nathan Kallus"], "title": "Optimization of Epsilon-Greedy Exploration", "categories": ["cs.LG"], "comment": null, "summary": "Modern recommendation systems rely on exploration to learn user preferences\nfor new items, typically implementing uniform exploration policies (e.g.,\nepsilon-greedy) due to their simplicity and compatibility with machine learning\n(ML) personalization models. Within these systems, a crucial consideration is\nthe rate of exploration - what fraction of user traffic should receive random\nitem recommendations and how this should evolve over time. While various\nheuristics exist for navigating the resulting exploration-exploitation\ntradeoff, selecting optimal exploration rates is complicated by practical\nconstraints including batched updates, time-varying user traffic, short time\nhorizons, and minimum exploration requirements. In this work, we propose a\nprincipled framework for determining the exploration schedule based on directly\nminimizing Bayesian regret through stochastic gradient descent (SGD), allowing\nfor dynamic exploration rate adjustment via Model-Predictive Control (MPC).\nThrough extensive experiments with recommendation datasets, we demonstrate that\nvariations in the batch size across periods significantly influence the optimal\nexploration strategy. Our optimization methods automatically calibrate\nexploration to the specific problem setting, consistently matching or\noutperforming the best heuristic for each setting."}
{"id": "2506.04030", "pdf": "https://arxiv.org/pdf/2506.04030", "abs": "https://arxiv.org/abs/2506.04030", "authors": ["Olivier Jaubert", "Salman Mohammadi", "Keith A. Goatman", "Shadia S. Mikhael", "Conor Bradley", "Rebecca Hughes", "Richard Good", "John H. Hipwell", "Sonia Dahdouh"], "title": "Conformal coronary calcification volume estimation with conditional coverage via histogram clustering", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE 22nd International Symposium on Biomedical Imaging (ISBI)", "summary": "Incidental detection and quantification of coronary calcium in CT scans could\nlead to the early introduction of lifesaving clinical interventions. However,\nover-reporting could negatively affect patient wellbeing and unnecessarily\nburden the medical system. Therefore, careful considerations should be taken\nwhen automatically reporting coronary calcium scores. A cluster-based\nconditional conformal prediction framework is proposed to provide score\nintervals with calibrated coverage from trained segmentation networks without\nretraining. The proposed method was tuned and used to calibrate predictive\nintervals for 3D UNet models (deterministic, MCDropout and deep ensemble)\nreaching similar coverage with better triage metrics compared to conventional\nconformal prediction. Meaningful predictive intervals of calcium scores could\nhelp triage patients according to the confidence of their risk category\nprediction."}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510", "abs": "https://arxiv.org/abs/2506.03510", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms."}
{"id": "2506.03229", "pdf": "https://arxiv.org/pdf/2506.03229", "abs": "https://arxiv.org/abs/2506.03229", "authors": ["Qian-Wei Wang", "Yuqiu Xie", "Letian Zhang", "Zimo Liu", "Shu-Tao Xia"], "title": "Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the context of noisy partial label learning (NPLL), each training sample\nis associated with a set of candidate labels annotated by multiple noisy\nannotators. With the emergence of high-performance pre-trained vision-language\nmodels (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these\nmodels to replace time-consuming manual annotation workflows and achieve\n\"manual-annotation-free\" training for downstream tasks has become a highly\npromising research avenue. This paper focuses on learning from noisy partial\nlabels annotated by pre-trained VLMs and proposes an innovative collaborative\nconsistency regularization (Co-Reg) method. Unlike the symmetric noise\nprimarily addressed in traditional noisy label learning, the noise generated by\npre-trained models is instance-dependent, embodying the underlying patterns of\nthe pre-trained models themselves, which significantly increases the learning\ndifficulty for the model. To address this, we simultaneously train two neural\nnetworks that implement collaborative purification of training labels through a\n\"Co-Pseudo-Labeling\" mechanism, while enforcing consistency regularization\nconstraints in both the label space and feature representation space. Our\nmethod can also leverage few-shot manually annotated valid labels to further\nenhance its performances. Comparative experiments with different denoising and\ndisambiguation algorithms, annotation manners, and pre-trained model\napplication schemes fully validate the effectiveness of the proposed method,\nwhile revealing the broad prospects of integrating weakly-supervised learning\ntechniques into the knowledge distillation process of pre-trained models."}
{"id": "2506.03333", "pdf": "https://arxiv.org/pdf/2506.03333", "abs": "https://arxiv.org/abs/2506.03333", "authors": ["Juan Sebastian Rojas", "Chi-Guhn Lee"], "title": "A Differential Perspective on Distributional Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To date, distributional reinforcement learning (distributional RL) methods\nhave exclusively focused on the discounted setting, where an agent aims to\noptimize a potentially-discounted sum of rewards over time. In this work, we\nextend distributional RL to the average-reward setting, where an agent aims to\noptimize the reward received per time-step. In particular, we utilize a\nquantile-based approach to develop the first set of algorithms that can\nsuccessfully learn and/or optimize the long-run per-step reward distribution,\nas well as the differential return distribution of an average-reward MDP. We\nderive proven-convergent tabular algorithms for both prediction and control, as\nwell as a broader family of algorithms that have appealing scaling properties.\nEmpirically, we find that these algorithms consistently yield competitive\nperformance when compared to their non-distributional equivalents, while also\ncapturing rich information about the long-run reward and return distributions."}
{"id": "2506.04058", "pdf": "https://arxiv.org/pdf/2506.04058", "abs": "https://arxiv.org/abs/2506.04058", "authors": ["Bulat Maksudov", "Kathleen Curran", "Alessandra Mileo"], "title": "Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge."}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks."}
{"id": "2506.03275", "pdf": "https://arxiv.org/pdf/2506.03275", "abs": "https://arxiv.org/abs/2506.03275", "authors": ["Austin Silveria", "Soham V. Govande", "Daniel Y. Fu"], "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."}
{"id": "2506.03337", "pdf": "https://arxiv.org/pdf/2506.03337", "abs": "https://arxiv.org/abs/2506.03337", "authors": ["Yide Ran", "Wentao Guo", "Jingwei Sun", "Yanzhou Pan", "Xiaodong Yu", "Hao Wang", "Jianwen Xie", "Yiran Chen", "Denghui Zhang", "Zhaozhuo Xu"], "title": "Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": "56 pages, 11 figures", "summary": "Federated Learning enables collaborative fine-tuning of Large Language Models\n(LLMs) across decentralized Non-Independent and Identically Distributed\n(Non-IID) clients, but such models' massive parameter sizes lead to significant\nmemory and communication challenges. This work introduces Meerkat, a sparse\nzeroth-order optimization (ZO) method designed for federated LLM fine-tuning.\nBy limiting fine-tuning to a transferable, static, extremely sparse subset of\nparameters, Meerkat achieves remarkable communication efficiency, enabling\ncost-effective high-frequency synchronization. With theoretical analysis and\nexperiments, we show that this high-frequency communication effectively\nmitigates Non-IID data challenges and leads to superior performance compared to\nfull-parameter ZO. Furthermore, experiment results show that Meerkat\noutperforms existing sparsity baselines with better performance at the same\ncommunication frequency. To further handle Non-IID drift, Meerkat leverages\ntraceable local updates and forms a virtual path for each client. This virtual\npath mechanism reveals the GradIP phenomenon: the inner products between LLM\npre-training gradients maintained by server and client gradients estimated via\nZO converges for extreme Non-IID clients but oscillates for IID ones. This\ndistinct behavior provides a signal for identifying clients with extreme data\nheterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP\ntrajectories to identify extreme Non-IID clients and applies early stopping to\nenhance aggregated model quality. Experiments confirm that Meerkat and\nMeerkat-vp significantly improve the efficiency and effectiveness of ZO\nfederated LLM fine-tuning."}
{"id": "2506.04116", "pdf": "https://arxiv.org/pdf/2506.04116", "abs": "https://arxiv.org/abs/2506.04116", "authors": ["Xuanru Zhou", "Jiarun Liu", "Shoujun Yu", "Hao Yang", "Cheng Li", "Tao Tan", "Shanshan Wang"], "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency."}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens."}
{"id": "2506.03290", "pdf": "https://arxiv.org/pdf/2506.03290", "abs": "https://arxiv.org/abs/2506.03290", "authors": ["Leyla Mirvakhabova", "Hong Cai", "Jisoo Jeong", "Hanno Ackermann", "Farhad Zanjani", "Fatih Porikli"], "title": "Learning Optical Flow Field via Neural Ordinary Differential Equation", "categories": ["cs.CV"], "comment": "CVPRW 2025", "summary": "Recent works on optical flow estimation use neural networks to predict the\nflow field that maps positions of one image to positions of the other. These\nnetworks consist of a feature extractor, a correlation volume, and finally\nseveral refinement steps. These refinement steps mimic the iterative\nrefinements performed by classical optimization algorithms and are usually\nimplemented by neural layers (e.g., GRU) which are recurrently executed for a\nfixed and pre-determined number of steps. However, relying on a fixed number of\nsteps may result in suboptimal performance because it is not tailored to the\ninput data. In this paper, we introduce a novel approach for predicting the\nderivative of the flow using a continuous model, namely neural ordinary\ndifferential equations (ODE). One key advantage of this approach is its\ncapacity to model an equilibrium process, dynamically adjusting the number of\ncompute steps based on the data at hand. By following a particular neural\narchitecture, ODE solver, and associated hyperparameters, our proposed model\ncan replicate the exact same updates as recurrent cells used in existing works,\noffering greater generality. Through extensive experimental analysis on optical\nflow benchmarks, we demonstrate that our approach achieves an impressive\nimprovement over baseline and existing models, all while requiring only a\nsingle refinement step."}
{"id": "2506.03209", "pdf": "https://arxiv.org/pdf/2506.03209", "abs": "https://arxiv.org/abs/2506.03209", "authors": ["Tinghuan Li", "Shuheng Chen", "Junyi Fan", "Elham Pishgar", "Kamiar Alaei", "Greg Placencia", "Maryam Pishgar"], "title": "Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Postoperative stroke remains a critical complication in elderly surgical\nintensive care unit (SICU) patients, contributing to prolonged hospitalization,\nelevated healthcare costs, and increased mortality. Accurate early risk\nstratification is essential to enable timely intervention and improve clinical\noutcomes. We constructed a combined cohort of 19,085 elderly SICU admissions\nfrom the MIMIC-III and MIMIC-IV databases and developed an interpretable\nmachine learning (ML) framework to predict in-hospital stroke using clinical\ndata from the first 24 hours of Intensive Care Unit (ICU) stay. The\npreprocessing pipeline included removal of high-missingness features, iterative\nSingular Value Decomposition (SVD) imputation, z-score normalization, one-hot\nencoding, and class imbalance correction via the Adaptive Synthetic Sampling\n(ADASYN) algorithm. A two-stage feature selection process-combining Recursive\nFeature Elimination with Cross-Validation (RFECV) and SHapley Additive\nexPlanations (SHAP)-reduced the initial 80 variables to 20 clinically\ninformative predictors. Among eight ML models evaluated, CatBoost achieved the\nbest performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP\nanalysis and ablation studies identified prior cerebrovascular disease, serum\ncreatinine, and systolic blood pressure as the most influential risk factors.\nOur results highlight the potential of interpretable ML approaches to support\nearly detection of postoperative stroke and inform decision-making in\nperioperative critical care."}
{"id": "2506.03355", "pdf": "https://arxiv.org/pdf/2506.03355", "abs": "https://arxiv.org/abs/2506.03355", "authors": ["Elias Abad Rocamora", "Christian Schlarmann", "Naman Deep Singh", "Yongtao Wu", "Matthias Hein", "Volkan Cevher"], "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization."}
{"id": "2506.04121", "pdf": "https://arxiv.org/pdf/2506.04121", "abs": "https://arxiv.org/abs/2506.04121", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Over the past decade, Medical Image Segmentation (MIS) using Deep Neural\nNetworks (DNNs) has achieved significant performance improvements and holds\ngreat promise for future developments. This paper presents a comprehensive\nstudy on MIS based on DNNs. Intelligent Vision Systems are often evaluated\nbased on their output levels, such as Data, Information, Knowledge,\nIntelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at\nthese levels are the focus of research. Additionally, Explainable Artificial\nIntelligence (XAI) has become an important research direction, as it aims to\nuncover the \"black box\" nature of previous DNN architectures to meet the\nrequirements of transparency and ethics. The study emphasizes the importance of\nMIS in disease diagnosis and early detection, particularly for increasing the\nsurvival rate of cancer patients through timely diagnosis. XAI and early\nprediction are considered two important steps in the journey from\n\"intelligence\" to \"wisdom.\" Additionally, the paper addresses existing\nchallenges and proposes potential solutions to enhance the efficiency of\nimplementing DNN-based MIS."}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks."}
{"id": "2506.03335", "pdf": "https://arxiv.org/pdf/2506.03335", "abs": "https://arxiv.org/abs/2506.03335", "authors": ["Dheeraj Khanna", "Jerrin Bright", "Yuhao Chen", "John S. Zelek"], "title": "SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports", "categories": ["cs.CV"], "comment": "Paper accepted at CVSports IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW'25). The paper has 8 pages, including 6\n  Figures and 5 Tables", "summary": "Multi-object tracking (MOT) in team sports is particularly challenging due to\nthe fast-paced motion and frequent occlusions resulting in motion blur and\nidentity switches, respectively. Predicting player positions in such scenarios\nis particularly difficult due to the observed highly non-linear motion\npatterns. Current methods are heavily reliant on object detection and\nappearance-based tracking, which struggle to perform in complex team sports\nscenarios, where appearance cues are ambiguous and motion patterns do not\nnecessarily follow a linear pattern. To address these challenges, we introduce\nSportMamba, an adaptive hybrid MOT technique specifically designed for tracking\nin dynamic team sports. The technical contribution of SportMamba is twofold.\nFirst, we introduce a mamba-attention mechanism that models non-linear motion\nby implicitly focusing on relevant embedding dependencies. Second, we propose a\nheight-adaptive spatial association metric to reduce ID switches caused by\npartial occlusions by accounting for scale variations due to depth changes.\nAdditionally, we extend the detection search space with adaptive buffers to\nimprove associations in fast-motion scenarios. Our proposed technique,\nSportMamba, demonstrates state-of-the-art performance on various metrics in the\nSportsMOT dataset, which is characterized by complex motion and severe\nocclusion. Furthermore, we demonstrate its generalization capability through\nzero-shot transfer to VIP-HTD, an ice hockey dataset."}
{"id": "2506.03214", "pdf": "https://arxiv.org/pdf/2506.03214", "abs": "https://arxiv.org/abs/2506.03214", "authors": ["Yi Guo", "Yihang Dong", "Michael Kwok-Po Ng", "Shuqiang Wang"], "title": "A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "comment": null, "summary": "Brain-computer interfaces (BCIs) with speech decoding from brain recordings\nhave broad application potential in fields such as clinical rehabilitation and\ncognitive neuroscience. However, current decoding methods remain limited to\nsingle-language, single-subject, and single neuroimaging modality settings,\nrestricting their clinical applicability and generalizability. Here we propose\na joint multilingual, multi-subject and multimodal decoding framework. It maps\ndiverse brain recordings into a unified semantic space defined by a pre-trained\nmultilingual model (PMM), enabling decoding across multiple languages, multiple\nsubjects and multiple neuroimaging modalities. The proposed framework is\nvalidated using non-invasive brain recordings from 159 participants across four\nlanguages. Experimental results show that it exhibits strong generalization\nacross multilingual, multi-subject, and multimodal settings. More importantly,\nthe proposed framework can promote linguistic fairness, which is vital for\nunderrepresented languages in BCI applications. The unified semantic space\nenables cross-lingual mapping enhancement, allowing the framework to boost the\ndecoding performance of underrepresented languages, thereby promoting\nlinguistic fairness. Overall, the proposed framework establishes a new\npotential paradigm for brain decoding, opening new paths for broader\napplications of BCI."}
{"id": "2506.03363", "pdf": "https://arxiv.org/pdf/2506.03363", "abs": "https://arxiv.org/abs/2506.03363", "authors": ["Divya Shyamal", "Jiaqi Zhang", "Caroline Uhler"], "title": "Probabilistic Factorial Experimental Design for Combinatorial Interventions", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "A combinatorial intervention, consisting of multiple treatments applied to a\nsingle unit with potentially interactive effects, has substantial applications\nin fields such as biomedicine, engineering, and beyond. Given $p$ possible\ntreatments, conducting all possible $2^p$ combinatorial interventions can be\nlaborious and quickly becomes infeasible as $p$ increases. Here we introduce\nprobabilistic factorial experimental design, formalized from how scientists\nperform lab experiments. In this framework, the experimenter selects a dosage\nfor each possible treatment and applies it to a group of units. Each unit\nindependently receives a random combination of treatments, sampled from a\nproduct Bernoulli distribution determined by the dosages. Additionally, the\nexperimenter can carry out such experiments over multiple rounds, adapting the\ndesign in an active manner. We address the optimal experimental design problem\nwithin an intervention model that imposes bounded-degree interactions between\ntreatments. In the passive setting, we provide a closed-form solution for the\nnear-optimal design. Our results prove that a dosage of $\\tfrac{1}{2}$ for each\ntreatment is optimal up to a factor of $1+O(\\tfrac{\\ln(n)}{n})$ for estimating\nany $k$-way interaction model, regardless of $k$, and imply that\n$O\\big(kp^{3k}\\ln(p)\\big)$ observations are required to accurately estimate\nthis model. For the multi-round setting, we provide a near-optimal acquisition\nfunction that can be numerically optimized. We also explore several extensions\nof the design problem and finally validate our findings through simulations."}
{"id": "2506.04129", "pdf": "https://arxiv.org/pdf/2506.04129", "abs": "https://arxiv.org/abs/2506.04129", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "Recent Advances in Medical Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image classification is crucial for diagnosis and treatment,\nbenefiting significantly from advancements in artificial intelligence. The\npaper reviews recent progress in the field, focusing on three levels of\nsolutions: basic, specific, and applied. It highlights advances in traditional\nmethods using deep learning models like Convolutional Neural Networks and\nVision Transformers, as well as state-of-the-art approaches with Vision\nLanguage Models. These models tackle the issue of limited labeled data, and\nenhance and explain predictive results through Explainable Artificial\nIntelligence."}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533", "abs": "https://arxiv.org/abs/2506.03533", "authors": ["Apurva Gandhi", "Graham Neubig"], "title": "Go-Browse: Training Web Agents with Structured Exploration", "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%."}
{"id": "2506.03340", "pdf": "https://arxiv.org/pdf/2506.03340", "abs": "https://arxiv.org/abs/2506.03340", "authors": ["Zihui Xue", "Mi Luo", "Kristen Grauman"], "title": "Seeing the Arrow of Time in Large Multimodal Models", "categories": ["cs.CV"], "comment": "Project website: https://vision.cs.utexas.edu/projects/SeeAoT", "summary": "The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is\nfundamental to video comprehension, yet remains a significant challenge for\nmodern large multimodal models (LMMs). Current LMMs struggle to perceive and\nutilize temporal directionality in video when responding to language queries,\nobstructing deeper temporal understanding. We tackle this deficiency by first\nproviding a critical analysis of existing benchmarks and models. We then\nintroduce ArrowRL, a reinforcement learning (RL)-based training strategy with\nan innovative reverse reward that instills AoT awareness by encouraging\ndivergent video interpretations between forward and reversed visual frames. For\nrigorous evaluation, we additionally develop AoTBench, a new multi-faceted\nbenchmark probing temporally challenging questions. Experiments show ArrowRL\ngreatly advances temporal perception: it not only achieves substantial\nimprovements on our challenging AoTBench but also demonstrably boosts\nperformance on standard video question answering (VQA) benchmarks (with peak\naccuracy gains reaching over 20% and 10% respectively). This validates\nArrowRL's effectiveness and highlights the critical need for dedicated AoT\nunderstanding in LMMs."}
{"id": "2506.03218", "pdf": "https://arxiv.org/pdf/2506.03218", "abs": "https://arxiv.org/abs/2506.03218", "authors": ["Alina Wernick", "Kristof Meding"], "title": "Beware! The AI Act Can Also Apply to Your AI Research Practices", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The EU has become one of the vanguards in regulating the digital age. A\nparticularly important regulation in the Artificial Intelligence (AI) domain is\nthe EU AI Act, which entered into force in 2024. The AI Act specifies -- due to\na risk-based approach -- various obligations for providers of AI systems. These\nobligations, for example, include a cascade of documentation and compliance\nmeasures, which represent a potential obstacle to science. But do these\nobligations also apply to AI researchers? This position paper argues that,\nindeed, the AI Act's obligations could apply in many more cases than the AI\ncommunity is aware of. In our analysis of the AI Act and its applicability, we\ncontribute the following: 1.) We give a high-level introduction to the AI Act\naimed at non-legal AI research scientists. 2.) We explain with everyday\nresearch examples why the AI Act applies to research. 3.) We analyse the\nexceptions of the AI Act's applicability and state that especially scientific\nresearch exceptions fail to account for current AI research practices. 4.) We\npropose changes to the AI Act to provide more legal certainty for AI\nresearchers and give two recommendations for AI researchers to reduce the risk\nof not complying with the AI Act. We see our paper as a starting point for a\ndiscussion between policymakers, legal scholars, and AI researchers to avoid\nunintended side effects of the AI Act on research."}
{"id": "2506.03370", "pdf": "https://arxiv.org/pdf/2506.03370", "abs": "https://arxiv.org/abs/2506.03370", "authors": ["Leonid Ryvkin"], "title": "Comparison of different Unique hard attention transformer models by the formal languages they can recognize", "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": null, "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity."}
{"id": "2506.04173", "pdf": "https://arxiv.org/pdf/2506.04173", "abs": "https://arxiv.org/abs/2506.04173", "authors": ["Savannah P. Hays", "Lianrui Zuo", "Anqi Feng", "Yihao Liu", "Blake E. Dewey", "Jiachen Zhuo", "Ellen M. Mowry", "Scott D. Newsome Jerry L. Prince", "Aaron Carass"], "title": "Synthetic multi-inversion time magnetic resonance images for visualization of subcortical structures", "categories": ["eess.IV"], "comment": "Under review at the Journal of Medical Imaging", "summary": "Purpose: Visualization of subcortical gray matter is essential in\nneuroscience and clinical practice, particularly for disease understanding and\nsurgical planning.While multi-inversion time (multi-TI) T$_1$-weighted\n(T$_1$-w) magnetic resonance (MR) imaging improves visualization, it is rarely\nacquired in clinical settings. Approach: We present SyMTIC (Synthetic Multi-TI\nContrasts), a deep learning method that generates synthetic multi-TI images\nusing routinely acquired T$_1$-w, T$_2$-weighted (T$_2$-w), and FLAIR images.\nOur approach combines image translation via deep neural networks with imaging\nphysics to estimate longitudinal relaxation time (T$_1$) and proton density\n(PD) maps. These maps are then used to compute multi-TI images with arbitrary\ninversion times. Results: SyMTIC was trained using paired MPRAGE and FGATIR\nimages along with T$_2$-w and FLAIR images. It accurately synthesized multi-TI\nimages from standard clinical inputs, achieving image quality comparable to\nthat from explicitly acquired multi-TI data.The synthetic images, especially\nfor TI values between 400-800 ms, enhanced visualization of subcortical\nstructures and improved segmentation of thalamic nuclei. Conclusion: SyMTIC\nenables robust generation of high-quality multi-TI images from routine MR\ncontrasts. It generalizes well to varied clinical datasets, including those\nwith missing FLAIR images or unknown parameters, offering a practical solution\nfor improving brain MR image visualization and analysis."}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin."}
{"id": "2506.03345", "pdf": "https://arxiv.org/pdf/2506.03345", "abs": "https://arxiv.org/abs/2506.03345", "authors": ["Chien-Fu", "Huang", "Katherine Sieg", "Leonid Karlinksy", "Nash Flores", "Rebekah Sheraw", "Xin Zhang"], "title": "Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers", "categories": ["cs.CV"], "comment": "Published at 36th Annual SEMI Advanced Semiconductor Manufacturing\n  Conference (ASMC) 2025", "summary": "Controlling defects in semiconductor processes is important for maintaining\nyield, improving production cost, and preventing time-dependent critical\ncomponent failures. Electron beam-based imaging has been used as a tool to\nsurvey wafers in the line and inspect for defects. However, manual\nclassification of images for these nano-scale defects is limited by time, labor\nconstraints, and human biases. In recent years, deep learning computer vision\nalgorithms have shown to be effective solutions for image-based inspection\napplications in industry. This work proposes application of vision transformer\n(ViT) neural networks for automatic defect classification (ADC) of scanning\nelectron microscope (SEM) images of wafer defects. We evaluated our proposed\nmethods on 300mm wafer semiconductor defect data from our fab in IBM Albany. We\nstudied 11 defect types from over 7400 total images and investigated the\npotential of transfer learning of DinoV2 and semi-supervised learning for\nimproved classification accuracy and efficient computation. We were able to\nachieve classification accuracies of over 90% with less than 15 images per\ndefect class. Our work demonstrates the potential to apply the proposed\nframework for a platform agnostic in-house classification tool with faster\nturnaround time and flexibility."}
{"id": "2506.03231", "pdf": "https://arxiv.org/pdf/2506.03231", "abs": "https://arxiv.org/abs/2506.03231", "authors": ["Yajie Zhou", "Jiajun Ruan", "Eric S. Wang", "Sadjad Fouladi", "Francis Y. Yan", "Kevin Hsieh", "Zaoxing Liu"], "title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite growing interest in domain-specific benchmarking of large language\nmodels (LLMs) and agents, current evaluations remain limited to static,\nsmall-scale datasets, especially in high-stakes tasks like network operations\nthat demand reliability for deployments. We present NetPress, an automated\nbenchmark generation framework for evaluating LLM agents in network\napplications. NetPress introduces a unified abstraction with state and action,\nenabling dynamic generation of diverse query sets along with corresponding\nground truths. At runtime, users can specify benchmark configurations to\ngenerate millions of queries on the fly. In addition to dynamic benchmark\nconstruction, NetPress integrates with network emulators to provide realistic\nenvironment feedback, supporting comprehensive evaluation across correctness,\nsafety, and latency. We instantiate NetPress on three representative\napplications, revealing interesting fine-grained differences in agent behavior\nthat static, correctness-only benchmarks often miss. NetPress moves LLM\nevaluation toward realistic, scalable testing in infrastructure-centric\ndomains, helping close the gap between benchmark performance and real-world\ndeployment readiness. Code is available at\nhttps://github.com/Froot-NetSys/NetPress."}
{"id": "2506.03374", "pdf": "https://arxiv.org/pdf/2506.03374", "abs": "https://arxiv.org/abs/2506.03374", "authors": ["Haley Dozier", "Althea Henslee", "Ashley Abraham", "Andrew Strelzoff", "Mark Chappell"], "title": "Product Quantization for Surface Soil Similarity", "categories": ["cs.LG"], "comment": "To be published in the CSCE 2022 proceedings", "summary": "The use of machine learning (ML) techniques has allowed rapid advancements in\nmany scientific and engineering fields. One of these problems is that of\nsurface soil taxonomy, a research area previously hindered by the reliance on\nhuman-derived classifications, which are mostly dependent on dividing a dataset\nbased on historical understandings of that data rather than data-driven,\nstatistically observable similarities. Using a ML-based taxonomy allows soil\nresearchers to move beyond the limitations of human visualization and create\nclassifications of high-dimension datasets with a much higher level of\nspecificity than possible with hand-drawn taxonomies. Furthermore, this\npipeline allows for the possibility of producing both highly accurate and\nflexible soil taxonomies with classes built to fit a specific application. The\nmachine learning pipeline outlined in this work combines product quantization\nwith the systematic evaluation of parameters and output to get the best\navailable results, rather than accepting sub-optimal results by using either\ndefault settings or best guess settings."}
{"id": "2506.03645", "pdf": "https://arxiv.org/pdf/2506.03645", "abs": "https://arxiv.org/abs/2506.03645", "authors": ["Hansen Feng", "Lizhi Wang", "Yiqi Huang", "Tong Li", "Lin Zhu", "Hua Huang"], "title": "YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency", "categories": ["cs.CV", "eess.IV"], "comment": "17 pages, 19 figures, TPAMI under review", "summary": "The rapid advancement of photography has created a growing demand for a\npractical blind raw image denoising method. Recently, learning-based methods\nhave become mainstream due to their excellent performance. However, most\nexisting learning-based methods suffer from camera-specific data dependency,\nresulting in performance drops when applied to data from unknown cameras. To\naddress this challenge, we introduce a novel blind raw image denoising method\nnamed YOND, which represents You Only Need a Denoiser. Trained solely on\nsynthetic data, YOND can generalize robustly to noisy raw images captured by\ndiverse unknown cameras. Specifically, we propose three key modules to\nguarantee the practicality of YOND: coarse-to-fine noise estimation (CNE),\nexpectation-matched variance-stabilizing transform (EM-VST), and SNR-guided\ndenoiser (SNR-Net). Firstly, we propose CNE to identify the camera noise\ncharacteristic, refining the estimated noise parameters based on the coarse\ndenoised image. Secondly, we propose EM-VST to eliminate camera-specific data\ndependency, correcting the bias expectation of VST according to the noisy\nimage. Finally, we propose SNR-Net to offer controllable raw image denoising,\nsupporting adaptive adjustments and manual fine-tuning. Extensive experiments\non unknown cameras, along with flexible solutions for challenging cases,\ndemonstrate the superior practicality of our method. The source code will be\npublicly available at the\n\\href{https://fenghansen.github.io/publication/YOND}{project homepage}."}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks."}
{"id": "2506.03371", "pdf": "https://arxiv.org/pdf/2506.03371", "abs": "https://arxiv.org/abs/2506.03371", "authors": ["Xiaonan Wang", "Bo Shao", "Hansaem Kim"], "title": "Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have enabled accurate\nimage-based geolocation, raising serious concerns about location privacy risks\nin everyday social media posts. However, current benchmarks remain\ncoarse-grained, linguistically biased, and lack multimodal and privacy-aware\nevaluations. To address these gaps, we present KoreaGEO Bench, the first\nfine-grained, multimodal geolocation benchmark for Korean street views. Our\ndataset comprises 1,080 high-resolution images sampled across four urban\nclusters and nine place types, enriched with multi-contextual annotations and\ntwo styles of Korean captions simulating real-world privacy exposure. We\nintroduce a three-path evaluation protocol to assess ten mainstream VLMs under\nvarying input modalities and analyze their accuracy, spatial bias, and\nreasoning behavior. Results reveal modality-driven shifts in localization\nprecision and highlight structural prediction biases toward core cities."}
{"id": "2506.03237", "pdf": "https://arxiv.org/pdf/2506.03237", "abs": "https://arxiv.org/abs/2506.03237", "authors": ["Jigang Fan", "Quanlin Wu", "Shengjie Luo", "Liwei Wang"], "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite."}
{"id": "2506.03392", "pdf": "https://arxiv.org/pdf/2506.03392", "abs": "https://arxiv.org/abs/2506.03392", "authors": ["Aref Ghoreishee", "Abhishek Mishra", "John Walsh", "Anup Das", "Nagarajan Kandasamy"], "title": "Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "comment": null, "summary": "We propose a new ternary spiking neuron model to improve the representation\ncapacity of binary spiking neurons in deep Q-learning. Although a ternary\nneuron model has recently been introduced to overcome the limited\nrepresentation capacity offered by the binary spiking neurons, we show that its\nperformance is worse than that of binary models in deep Q-learning tasks. We\nhypothesize gradient estimation bias during the training process as the\nunderlying potential cause through mathematical and empirical analysis. We\npropose a novel ternary spiking neuron model to mitigate this issue by reducing\nthe estimation bias. We use the proposed ternary spiking neuron as the\nfundamental computing unit in a deep spiking Q-learning network (DSQN) and\nevaluate the network's performance in seven Atari games from the Gym\nenvironment. Results show that the proposed ternary spiking neuron mitigates\nthe drastic performance degradation of ternary neurons in Q-learning tasks and\nimproves the network performance compared to the existing binary neurons,\nmaking DSQN a more practical solution for on-board autonomous decision-making\ntasks."}
{"id": "2506.03738", "pdf": "https://arxiv.org/pdf/2506.03738", "abs": "https://arxiv.org/abs/2506.03738", "authors": ["Francesca Borrelli", "Giusy Giugliano", "Emilie Houliez", "Jaromir Behal", "Daniele Pirone", "Leonilde Roselli", "Angela Sardo", "Valerio Zupo", "Maria Costantini", "Lisa Miccio", "Pasquale Memmolo", "Vittorio Bianco", "Pietro Ferraro"], "title": "3D Holographic Flow Cytometry Measurements of Microalgae: Strategies for Angle Recovery in Complex Rotation Patterns", "categories": ["physics.bio-ph", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "Marine ecosystems are in the spotlight, because environmental changes are\nthreatening biodiversity and ecological functions. In this context, microalgae\nplay key ecological roles both in planktonic and benthic ecosystems.\nConsequently, they are considered indispensable targets for global monitoring\nprograms. However, due to a high spatial and temporal variability and to\ndifficulties of species identification (still relying on microscopy\nobservations), the assessment of roles played by these components of marine\necosystems is demanding. In addition, technologies for a 3D assessment of their\ncomplex morphology are scarcely available. Here, we present a comprehensive\nworkflow for retrieving 3D information on microalgae with diverse geometries\nthrough holographic microscopy operating in flow-cytometry mode. Depending on\nthe rotation patterns of samples, a tailored approach is used to retrieve their\nrolling angles. We demonstrate the feasibility of measuring 3D data of various\nmicroalgae, contingent to the intrinsic optical properties of cells.\nSpecifically, we show that for quasi-transparent and low-scattering\nmicroorganisms, the retrieved angles permit to achieve quantitative 3D\ntomographic Refractive Index (RI) mapping, providing a full characterization of\nthe alga in terms of its inner structure and the outer shape. Moreover, even in\nthe most challenging scenarios, where microalgae exhibit high light absorption\nor strong scattering, quantitative 3D shape reconstructions of diatoms and\ndinoflagellates can be at least achieved. Finally, we compare our direct 3D\nmeasurements with 2D inferences of 3D properties, obtained using a commercially\navailable microscopy system. The ability to non-invasively obtain 3D\ninformation on microalgae marks a fundamental advancement in the field,\nunlocking a wealth of novel biological insights for characterizing aquatic\necosystems."}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558", "abs": "https://arxiv.org/abs/2506.03558", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets."}
{"id": "2506.03373", "pdf": "https://arxiv.org/pdf/2506.03373", "abs": "https://arxiv.org/abs/2506.03373", "authors": ["Muhammad Shaban", "Yuzhou Chang", "Huaying Qiu", "Yao Yu Yeo", "Andrew H. Song", "Guillaume Jaume", "Yuchen Wang", "Luca L. Weishaupt", "Tong Ding", "Anurag Vaidya", "Abdallah Lamane", "Daniel Shao", "Mohammed Zidane", "Yunhao Bai", "Paige McCallum", "Shuli Luo", "Wenrui Wu", "Yang Wang", "Precious Cramer", "Chi Ngai Chan", "Pierre Stephan", "Johanna Schaffenrath", "Jia Le Lee", "Hendrik A. Michel", "Caiwei Tian", "Cristina Almagro-Perez", "Sophia J. Wagner", "Sharifa Sahai", "Ming Y. Lu", "Richard J. Chen", "Andrew Zhang", "Mark Edward M. Gonzales", "Ahmad Makky", "Jia-Ying Joey Lee", "Hao Cheng", "Nourhan El Ahmar", "Sayed Matar", "Maximilian Haist", "Darci Phillips", "Yuqi Tan", "Garry P. Nolan", "W. Richard Burack", "Jacob D. Estes", "Jonathan T. C. Liu", "Toni K Choueiri", "Neeraj Agarwal", "Marc Barry", "Scott J. Rodig", "Long Phi Le", "Georg Gerber", "Christian M. Schürch", "Fabian J. Theis", "Youn H Kim", "Joe Yeong", "Sabina Signoretti", "Brooke E. Howitt", "Lit-Hsin Loo", "Qin Ma", "Sizun Jiang", "Faisal Mahmood"], "title": "A Foundation Model for Spatial Proteomics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models have begun to transform image analysis by acting as\npretrained generalist backbones that can be adapted to many tasks even when\npost-training data are limited, yet their impact on spatial proteomics, imaging\nthat maps proteins at single-cell resolution, remains limited. Here, we\nintroduce KRONOS, a foundation model built for spatial proteomics. KRONOS was\ntrained in a self-supervised manner on over 47 million image patches covering\n175 protein markers, 16 tissue types, and 8 fluorescence-based imaging\nplatforms. We introduce key architectural adaptations to address the\nhigh-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.\nWe demonstrate that KRONOS learns biologically meaningful representations\nacross multiple scales, ranging from cellular and microenvironment to tissue\nlevels, enabling it to address diverse downstream tasks, including cell\nphenotyping, region classification, and patient stratification. Evaluated\nacross 11 independent cohorts, KRONOS achieves state-of-the-art performance\nacross cell phenotyping, treatment response prediction, and retrieval tasks,\nand is highly data-efficient. KRONOS also introduces the paradigm of\nsegmentation-free patch-level processing for efficient and scalable spatial\nproteomics analysis, allowing cross-institutional comparisons, and as an image\nreverse search engine for spatial patterns. Together, these results position\nKRONOS as a flexible and scalable tool for spatial proteomics. The model is\npublicly accessible at https://github.com/mahmoodlab/KRONOS."}
{"id": "2506.03270", "pdf": "https://arxiv.org/pdf/2506.03270", "abs": "https://arxiv.org/abs/2506.03270", "authors": ["Jeremy Siburian", "Keisuke Shirai", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Michael Görner", "Atsushi Hashimoto"], "title": "Grounded Vision-Language Interpreter for Integrated Task and Motion Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Project website: https://omron-sinicx.github.io/ViLaIn-TAMP/", "summary": "While recent advances in vision-language models (VLMs) have accelerated the\ndevelopment of language-guided robot planners, their black-box nature often\nlacks safety guarantees and interpretability crucial for real-world deployment.\nConversely, classical symbolic planners offer rigorous safety verification but\nrequire significant expert knowledge for setup. To bridge the current gap, this\npaper proposes ViLaIn-TAMP, a hybrid planning framework for enabling\nverifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP\ncomprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A\nprior framework that converts multimodal inputs into structured problem\nspecifications using off-the-shelf VLMs without additional domain-specific\ntraining, (2) a modular Task and Motion Planning (TAMP) system that grounds\nthese specifications in actionable trajectory sequences through symbolic and\ngeometric constraint reasoning and can utilize learning-based skills for key\nmanipulation phases, and (3) a corrective planning module which receives\nconcrete feedback on failed solution attempts from the motion and task planning\ncomponents and can feed adapted logic and geometric feasibility constraints\nback to ViLaIn to improve and further refine the specification. We evaluate our\nframework on several challenging manipulation tasks in a cooking domain. We\ndemonstrate that the proposed closed-loop corrective architecture exhibits a\nmore than 30% higher mean success rate for ViLaIn-TAMP compared to without\ncorrective planning."}
{"id": "2506.03404", "pdf": "https://arxiv.org/pdf/2506.03404", "abs": "https://arxiv.org/abs/2506.03404", "authors": ["Walter Mayor", "Johan Obando-Ceron", "Aaron Courville", "Pablo Samuel Castro"], "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "The use of parallel actors for data collection has been an effective\ntechnique used in reinforcement learning (RL) algorithms. The manner in which\ndata is collected in these algorithms, controlled via the number of parallel\nenvironments and the rollout length, induces a form of bias-variance trade-off;\nthe number of training passes over the collected data, on the other hand, must\nstrike a balance between sample efficiency and overfitting. We conduct an\nempirical analysis of these trade-offs on PPO, one of the most popular RL\nalgorithms that uses parallel actors, and establish connections to network\nplasticity and, more generally, optimization stability. We examine its impact\non network architectures, as well as the hyper-parameter sensitivity when\nscaling data. Our analyses indicate that larger dataset sizes can increase\nfinal performance across a variety of settings, and that scaling parallel\nenvironments is more effective than increasing rollout lengths. These findings\nhighlight the critical role of data collection strategies in improving agent\nperformance."}
{"id": "2506.03979", "pdf": "https://arxiv.org/pdf/2506.03979", "abs": "https://arxiv.org/abs/2506.03979", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "categories": ["cs.LG", "cs.CV", "cs.NA", "eess.IV", "math.NA", "stat.ML"], "comment": "45 pages", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods."}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566", "abs": "https://arxiv.org/abs/2506.03566", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS."}
{"id": "2506.03388", "pdf": "https://arxiv.org/pdf/2506.03388", "abs": "https://arxiv.org/abs/2506.03388", "authors": ["Pengyu Chen", "Xiao Huang", "Teng Fei", "Sicheng Wang"], "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Environmental soundscapes convey substantial ecological and social\ninformation regarding urban environments; however, their potential remains\nlargely untapped in large-scale geographic analysis. In this study, we\ninvestigate the extent to which urban sounds correspond with visual scenes by\ncomparing various visual representation strategies in capturing acoustic\nsemantics. We employ a multimodal approach that integrates geo-referenced sound\nrecordings with both street-level and remote sensing imagery across three major\nglobal cities: London, New York, and Tokyo. Utilizing the AST model for audio,\nalong with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV\nfor semantic segmentation, we extract embeddings and class-level features to\nevaluate cross-modal similarity. The results indicate that street view\nembeddings demonstrate stronger alignment with environmental sounds compared to\nsegmentation outputs, whereas remote sensing segmentation is more effective in\ninterpreting ecological categories through a Biophony--Geophony--Anthrophony\n(BGA) framework. These findings imply that embedding-based models offer\nsuperior semantic alignment, while segmentation-based methods provide\ninterpretable links between visual structure and acoustic ecology. This work\nadvances the burgeoning field of multimodal urban sensing by offering novel\nperspectives for incorporating sound into geospatial analysis."}
{"id": "2506.03350", "pdf": "https://arxiv.org/pdf/2506.03350", "abs": "https://arxiv.org/abs/2506.03350", "authors": ["Eliot Krzysztof Jones", "Alexander Robey", "Andy Zou", "Zachary Ravichandran", "George J. Pappas", "Hamed Hassani", "Matt Fredrikson", "J. Zico Kolter"], "title": "Adversarial Attacks on Robotic Vision Language Action Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control\nis reshaping the field of robotics by enabling the fusion of multimodal sensory\ninputs at the billion-parameter scale. The capabilities of VLAs stem primarily\nfrom their architectures, which are often based on frontier large language\nmodels (LLMs). However, LLMs are known to be susceptible to adversarial misuse,\nand given the significant physical risks inherent to robotics, questions remain\nregarding the extent to which VLAs inherit these vulnerabilities. Motivated by\nthese concerns, in this work we initiate the study of adversarial attacks on\nVLA-controlled robots. Our main algorithmic contribution is the adaptation and\napplication of LLM jailbreaking attacks to obtain complete control authority\nover VLAs. We find that textual attacks, which are applied once at the\nbeginning of a rollout, facilitate full reachability of the action space of\ncommonly used VLAs and often persist over longer horizons. This differs\nsignificantly from LLM jailbreaking literature, as attacks in the real world do\nnot have to be semantically linked to notions of harm. We make all code\navailable at https://github.com/eliotjones1/robogcg ."}
{"id": "2506.03411", "pdf": "https://arxiv.org/pdf/2506.03411", "abs": "https://arxiv.org/abs/2506.03411", "authors": ["Melissa Dutz", "Han Shao", "Avrim Blum", "Aloni Cohen"], "title": "A Machine Learning Theory Perspective on Strategic Litigation", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Strategic litigation involves bringing a legal case to court with the goal of\nhaving a broader impact beyond resolving the case itself: for example, creating\nprecedent which will influence future rulings. In this paper, we explore\nstrategic litigation from the perspective of machine learning theory. We\nconsider an abstract model of a common-law legal system where a lower court\ndecides new cases by applying a decision rule learned from a higher court's\npast rulings. In this model, we explore the power of a strategic litigator, who\nstrategically brings cases to the higher court to influence the learned\ndecision rule, thereby affecting future cases. We explore questions including:\nWhat impact can a strategic litigator have? Which cases should a strategic\nlitigator bring to court? Does it ever make sense for a strategic litigator to\nbring a case when they are sure the court will rule against them?"}
{"id": "2502.03783", "pdf": "https://arxiv.org/pdf/2502.03783", "abs": "https://arxiv.org/abs/2502.03783", "authors": ["Luohong Wu", "Nicola A. Cavalcanti", "Matthias Seibold", "Giuseppe Loggia", "Lisa Reissner", "Jonas Hein", "Silvan Beeler", "Arnd Viehöfer", "Stephan Wirth", "Lilian Calvet", "Philipp Fürnstahl"], "title": "UltraBones100k: A reliable automated labeling method and large-scale dataset for ultrasound-based bone surface extraction", "categories": ["eess.IV", "cs.CV"], "comment": "accepted by Computers in Biology and Medicine", "summary": "Ultrasound-based bone surface segmentation is crucial in computer-assisted\northopedic surgery. However, ultrasound images have limitations, including a\nlow signal-to-noise ratio, and acoustic shadowing, which make interpretation\ndifficult. Existing deep learning models for bone segmentation rely primarily\non costly manual labeling by experts, limiting dataset size and model\ngeneralizability. Additionally, the complexity of ultrasound physics and\nacoustic shadow makes the images difficult for humans to interpret, leading to\nincomplete labels in anechoic regions and limiting model performance. To\nadvance ultrasound bone segmentation and establish effective model benchmarks,\nlarger and higher-quality datasets are needed.\n  We propose a methodology for collecting ex-vivo ultrasound datasets with\nautomatically generated bone labels, including anechoic regions. The proposed\nlabels are derived by accurately superimposing tracked bone CT models onto the\ntracked ultrasound images. These initial labels are refined to account for\nultrasound physics. A clinical evaluation is conducted by an expert physician\nspecialized on orthopedic sonography to assess the quality of the generated\nbone labels. A neural network for bone segmentation is trained on the collected\ndataset and its predictions are compared to expert manual labels, evaluating\naccuracy, completeness, and F1-score.\n  We collected the largest known dataset of 100k ultrasound images of human\nlower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank\ntest with Bonferroni correction confirmed that the bone alignment after our\nmethod significantly improved the quality of bone labeling (p < 0.001). The\nmodel trained on UltraBones100k consistently outperforms manual labeling in all\nmetrics, particularly in low-intensity regions (320% improvement in\ncompleteness at a distance threshold of 0.5 mm)."}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL."}
{"id": "2506.03394", "pdf": "https://arxiv.org/pdf/2506.03394", "abs": "https://arxiv.org/abs/2506.03394", "authors": ["Shafqaat Ahmad"], "title": "Temporal Vegetation Index-Based Unsupervised Crop Stress Detection via Eigenvector-Guided Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Early detection of crop stress is vital for minimizing yield loss and\nenabling timely intervention in precision agriculture. Traditional approaches\nusing NDRE often detect stress only after visible symptoms appear or require\nlabeled datasets, limiting scalability. This study introduces EigenCL, a novel\nunsupervised contrastive learning framework guided by temporal NDRE dynamics\nand biologically grounded eigen decomposition. Using over 10,000 Sentinel-2\nNDRE image patches from drought-affected Iowa cornfields, we constructed\nfive-point NDRE time series per patch and derived an RBF similarity matrix. The\nprincipal eigenvector explaining 76% of the variance and strongly correlated (r\n= 0.95) with raw NDRE values was used to define stress-aware similarity for\ncontrastive embedding learning. Unlike existing methods that rely on visual\naugmentations, EigenCL pulls embeddings together based on biologically similar\nstress trajectories and pushes apart divergent ones. The learned embeddings\nformed physiologically meaningful clusters, achieving superior clustering\nmetrics (Silhouette: 0.748, DBI: 0.35) and enabling 76% early stress detection\nup to 12 days before conventional NDRE thresholds. Downstream classification\nyielded 95% k-NN and 91% logistic regression accuracy. Validation on an\nindependent 2023 Nebraska dataset confirmed generalizability without\nretraining. EigenCL offers a label-free, scalable approach for early stress\ndetection that aligns with underlying plant physiology and is suitable for\nreal-world deployment in data-scarce agricultural environments."}
{"id": "2506.03381", "pdf": "https://arxiv.org/pdf/2506.03381", "abs": "https://arxiv.org/abs/2506.03381", "authors": ["Artur Grigorev", "Khaled Saleh", "Jiwon Kim", "Adriana-Simona Mihaita"], "title": "Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Traffic incidents remain a critical public safety concern worldwide, with\nAustralia recording 1,300 road fatalities in 2024, which is the highest toll in\n12 years. Similarly, the United States reports approximately 6 million crashes\nannually, raising significant challenges in terms of a fast reponse time and\noperational management. Traditional response protocols rely on human\ndecision-making, which introduces potential inconsistencies and delays during\ncritical moments when every minute impacts both safety outcomes and network\nperformance. To address this issue, we propose a novel Incident Response\nBenchmark that uses generative artificial intelligence to automatically\ngenerate response plans for incoming traffic incidents. Our approach aims to\nsignificantly reduce incident resolution times by suggesting\ncontext-appropriate actions such as variable message sign deployment, lane\nclosures, and emergency resource allocation adapted to specific incident\ncharacteristics. First, the proposed methodology uses real-world incident\nreports from the Performance Measurement System (PeMS) as training and\nevaluation data. We extract historically implemented actions from these reports\nand compare them against AI-generated response plans that suggest specific\nactions, such as lane closures, variable message sign announcements, and/or\ndispatching appropriate emergency resources. Second, model evaluations reveal\nthat advanced generative AI models like GPT-4o and Grok 2 achieve superior\nalignment with expert solutions, demonstrated by minimized Hamming distances\n(averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28).\nConversely, while Gemini 1.5 Pro records the lowest count of missed actions,\nits extremely high number of unnecessary actions (1547 compared to 225 for\nGPT-4o) indicates an over-triggering strategy that reduces the overall plan\nefficiency."}
{"id": "2506.03426", "pdf": "https://arxiv.org/pdf/2506.03426", "abs": "https://arxiv.org/abs/2506.03426", "authors": ["Joonseong Kang", "Soojeong Lee", "Subeen Park", "Sumin Park", "Taero Kim", "Jihee Kim", "Ryunyi Lee", "Kyungwoo Song"], "title": "Adaptive Task Vectors for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks without parameter updates by conditioning on a few demonstrations\nprovided in the prompt. Despite its success, ICL suffers from several\nlimitations, including sensitivity to demonstration order, context length\nconstraints, and computational inefficiency. To address these challenges, task\nvector-based approaches compress task information into a single vector.\nHowever, these methods typically construct task vectors from fixed sets of\ndemonstrations and reuse them across input queries, without conditioning on the\nspecific input. This limitation can lead models to struggle with effective\nadaptation when the input query is not well aligned with the underlying\ndemonstrations, consequently degrading their generalization performance on\nunseen tasks. To overcome this limitation, we propose Adaptive Task Vectors\n(ATV), a simple and effective framework that dynamically generates task vectors\nconditioned on each input query. ATV employs a small language model to generate\ntask vectors, which are then transformed to match the target LLM's architecture\nand applied to guide its output generation. In contrast to ICL and previous\nvector-based approaches, which rely on fixed demonstration sets and their\ncorresponding vectors, ATV dynamically generates task vectors tailored to each\nspecific input query and task. Consequently, ATV demonstrates strong\nperformance and generalization capabilities, even for unseen tasks.\nFurthermore, we provide a theoretical analysis indicating that ATV is\nexpressively equivalent to LoRA under equal rank budgets and more expressive\nthan Prefix-Tuning, thereby offering formal support for its representational\nadvantage."}
{"id": "2503.02321", "pdf": "https://arxiv.org/pdf/2503.02321", "abs": "https://arxiv.org/abs/2503.02321", "authors": ["Pengchen Liang", "Leijun Shi", "Huiping Yao", "Bin Pu", "Jianguo Chen", "Lei Zhao", "Haishan Huang", "Zhuangzhuang Chen", "Zhaozhao Xu", "Lite Xu", "Qing Chang", "Yiwei Li"], "title": "Rapid Bone Scintigraphy Enhancement via Semantic Prior Distillation from Segment Anything Model", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 9 figures, 8 tables", "summary": "Rapid bone scintigraphy is crucial for diagnosing skeletal disorders and\ndetecting tumor metastases in children, as it shortens scan duration and\nreduces discomfort. However, accelerated acquisition often degrades image\nquality, impairing the visibility of fine anatomical details and potentially\ncompromising diagnosis. To overcome this limitation, we introduce the first\napplication of SAM-based semantic priors for medical image restoration,\nutilizing the Segment Anything Model (SAM) to enhance pediatric rapid bone\nscintigraphy. Our approach employs two cascaded networks, $f^{IR1}$ and\n$f^{IR2}$, supported by three specialized modules: a Semantic Prior Integration\n(SPI) module, a Semantic Knowledge Distillation (SKD) module, and a Semantic\nConsistency Module (SCM). The SPI and SKD modules inject domain-specific\nsemantic cues from a fine-tuned SAM, while the SCM preserves coherent semantic\nfeature representations across both cascaded stages. Moreover, we present RBS,\na novel Rapid Bone Scintigraphy dataset comprising paired standard (20 cm/min)\nand rapid (40 cm/min) scans from 137 pediatric patients aged 0.5 - 16 years,\nmaking it the first dataset tailored for pediatric rapid bone scintigraphy\nrestoration. Extensive experiments on both a public endoscopic dataset and our\nRBS dataset demonstrate that our method consistently surpasses existing\ntechniques in PSNR, SSIM, FID, and LPIPS metrics."}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570", "abs": "https://arxiv.org/abs/2506.03570", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance."}
{"id": "2506.03433", "pdf": "https://arxiv.org/pdf/2506.03433", "abs": "https://arxiv.org/abs/2506.03433", "authors": ["Yifan Li", "Xin Li", "Tianqin Li", "Wenbin He", "Yu Kong", "Liu Ren"], "title": "ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads", "categories": ["cs.CV"], "comment": "The project is available:\n  https://jackyfl.github.io/vitsplit.github.io/", "summary": "Vision foundation models (VFMs) have demonstrated remarkable performance\nacross a wide range of downstream tasks. While several VFM adapters have shown\npromising results by leveraging the prior knowledge of VFMs, we identify two\ninefficiencies in these approaches. First, the interaction between\nconvolutional neural network (CNN) and VFM backbone triggers early layer\ngradient backpropagation. Second, existing methods require tuning all\ncomponents, adding complexity. Besides, these adapters alter VFM features,\nunderutilizing the prior knowledge. To tackle these challenges, we propose a\nnew approach called ViT-Split, based on a key observation: the layers of\nseveral VFMs, like DINOv2, can be divided into two distinct components: an\nextractor for learning low-level features and an adapter for learning\ntask-specific features. Leveraging this insight, we eliminate the CNN branch\nand introduce two heads, task head and prior head, to the frozen VFM. The task\nhead is designed to learn task-specific features, mitigating the early gradient\npropagation issue. The prior head is used to leverage the multi-scale prior\nfeatures from the frozen VFM, reducing tuning parameters and overfitting.\nExtensive experiments on various tasks (e.g., segmentation, detection, depth\nestimation, and visual question answering) validate the effectiveness and\nefficiency of ViT-Split. Specifically, ViT-Split reduces training time up to\n$4\\times$ while achieving comparable or even better results on ADE20K, compared\nto other VFM adapters."}
{"id": "2506.03391", "pdf": "https://arxiv.org/pdf/2506.03391", "abs": "https://arxiv.org/abs/2506.03391", "authors": ["Tri Kurniawan Wijaya", "Xinyang Shao", "Gonzalo Fiz Pontiveros", "Edoardo D'Amico"], "title": "Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks", "categories": ["cs.IR", "cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "Recommender systems are pivotal in delivering personalized experiences across\nindustries, yet their adoption and scalability remain hindered by the need for\nextensive dataset- and task-specific configurations. Existing systems often\nrequire significant manual intervention, domain expertise, and engineering\neffort to adapt to new datasets or tasks, creating barriers to entry and\nlimiting reusability. In contrast, recent advancements in large language models\n(LLMs) have demonstrated the transformative potential of reusable systems,\nwhere a single model can handle diverse tasks without significant\nreconfiguration. Inspired by this paradigm, we propose the Dataset- and\nTask-Independent Recommender System (DTIRS), a framework aimed at maximizing\nthe reusability of recommender systems while minimizing barriers to entry.\nUnlike LLMs, which achieve task generalization directly, DTIRS focuses on\neliminating the need to rebuild or reconfigure recommendation pipelines for\nevery new dataset or task, even though models may still need retraining on new\ndata. By leveraging the novel Dataset Description Language (DsDL), DTIRS\nenables standardized dataset descriptions and explicit task definitions,\nallowing autonomous feature engineering, model selection, and optimization.\nThis paper introduces the concept of DTIRS and establishes a roadmap for\ntransitioning from Level-1 automation (dataset-agnostic but task-specific\nsystems) to Level-2 automation (fully dataset- and task-independent systems).\nAchieving this paradigm would maximize code reusability and lower barriers to\nadoption. We discuss key challenges, including the trade-offs between\ngeneralization and specialization, computational overhead, and scalability,\nwhile presenting DsDL as a foundational tool for this vision."}
{"id": "2506.03444", "pdf": "https://arxiv.org/pdf/2506.03444", "abs": "https://arxiv.org/abs/2506.03444", "authors": ["Yue Gong", "Raul Castro Fernandez"], "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "As hypothesis generation becomes increasingly automated, a new bottleneck has\nemerged: hypothesis assessment. Modern systems can surface thousands of\nstatistical relationships-correlations, trends, causal links-but offer little\nguidance on which ones are novel, non-trivial, or worthy of expert attention.\nIn this work, we study the complementary problem to hypothesis generation:\nautomatic hypothesis assessment. Specifically, we ask: given a large set of\nstatistical relationships, can we automatically assess which ones are novel and\nworth further exploration? We focus on correlations as they are a common entry\npoint in exploratory data analysis that often serve as the basis for forming\ndeeper scientific or causal hypotheses.\n  To support automatic assessment, we propose to leverage the vast knowledge\nencoded in LLMs' weights to derive a prior distribution over the correlation\nvalue of a variable pair. If an LLM's prior expects the correlation value\nobserved, then such correlation is not surprising, and vice versa. We propose\nthe Logit-based Calibrated Prior, an LLM-elicited correlation prior that\ntransforms the model's raw output logits into a calibrated, continuous\npredictive distribution over correlation values. We evaluate the prior on a\nbenchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of\n78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of\n89.2% in predicting Pearson correlation coefficient. It also outperforms a\nfine-tuned RoBERTa classifier in binary correlation prediction and achieves\nhigher precision@K in hypothesis ranking. We further show that the prior\ngeneralizes to correlations not seen during LLM pretraining, reflecting\ncontext-sensitive reasoning rather than memorization."}
{"id": "2504.18268", "pdf": "https://arxiv.org/pdf/2504.18268", "abs": "https://arxiv.org/abs/2504.18268", "authors": ["Ana Matoso", "Catarina Passarinho", "Marta P. Loureiro", "José Maria Moreira", "Patrícia Figueiredo", "Rita G. Nunes"], "title": "Towards a deep learning approach for classifying treatment response in glioblastomas", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glioblastomas are the most aggressive type of glioma, having a 5-year\nsurvival rate of 6.9%. Treatment typically involves surgery, followed by\nradiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)\nscans to monitor disease progression. To assess treatment response,\nradiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to\ncategorize the tumor into one of four labels based on imaging and clinical\nfeatures: complete response, partial response, stable disease, and progressive\ndisease. This assessment is very complex and time-consuming. Since deep\nlearning (DL) has been widely used to tackle classification problems, this work\naimed to implement the first DL pipeline for the classification of RANO\ncriteria based on two consecutive MRI acquisitions. The models were trained and\ntested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction\nof input images, 2) different combinations of modalities, 3) different model\narchitectures, 4) different pretraining tasks, and 5) adding clinical data. The\npipeline that achieved the best performance used a Densenet264 considering only\nT1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)\nimages as input without any pretraining. A median Balanced Accuracy of 50.96%\nwas achieved. Additionally, explainability methods were applied. Using Saliency\nMaps, the tumor region was often successfully highlighted. In contrast,\nGrad-CAM typically failed to highlight the tumor region, with some exceptions\nobserved in the Complete Response and Progressive Disease classes, where it\neffectively identified the tumor region. These results set a benchmark for\nfuture studies on glioblastoma treatment response assessment based on the RANO\ncriteria while emphasizing the heterogeneity of factors that might play a role\nwhen assessing the tumor's response to treatment."}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573", "abs": "https://arxiv.org/abs/2506.03573", "authors": ["Lin Sun", "Can Zhang"], "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b."}
{"id": "2506.03440", "pdf": "https://arxiv.org/pdf/2506.03440", "abs": "https://arxiv.org/abs/2506.03440", "authors": ["Tanqiu Qiao", "Ruochen Li", "Frederick W. B. Li", "Yoshiki Kubotani", "Shigeo Morishima", "Hubert P. H. Shum"], "title": "Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos", "categories": ["cs.CV"], "comment": "Accepted by Expert Systems with Applications (ESWA)", "summary": "Human-Object Interaction (HOI) recognition in videos requires understanding\nboth visual patterns and geometric relationships as they evolve over time.\nVisual and geometric features offer complementary strengths. Visual features\ncapture appearance context, while geometric features provide structural\npatterns. Effectively fusing these multimodal features without compromising\ntheir unique characteristics remains challenging. We observe that establishing\nrobust, entity-specific representations before modeling interactions helps\npreserve the strengths of each modality. Therefore, we hypothesize that a\nbottom-up approach is crucial for effective multimodal fusion. Following this\ninsight, we propose the Geometric Visual Fusion Graph Neural Network\n(GeoVis-GNN), which uses dual-attention feature fusion combined with\ninterdependent entity graph learning. It progressively builds from\nentity-specific representations toward high-level interaction understanding. To\nadvance HOI recognition to real-world scenarios, we introduce the Concurrent\nPartial Interaction Dataset (MPHOI-120). It captures dynamic multi-person\ninteractions involving concurrent actions and partial engagement. This dataset\nhelps address challenges like complex human-object dynamics and mutual\nocclusions. Extensive experiments demonstrate the effectiveness of our method\nacross various HOI scenarios. These scenarios include two-person interactions,\nsingle-person activities, bimanual manipulations, and complex concurrent\npartial interactions. Our method achieves state-of-the-art performance."}
{"id": "2506.03399", "pdf": "https://arxiv.org/pdf/2506.03399", "abs": "https://arxiv.org/abs/2506.03399", "authors": ["Sean Steinle"], "title": "Sampling Preferences Yields Simple Trustworthiness Scores", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the onset of large language models (LLMs), the performance of artificial\nintelligence (AI) models is becoming increasingly multi-dimensional.\nAccordingly, there have been several large, multi-dimensional evaluation\nframeworks put forward to evaluate LLMs. Though these frameworks are much more\nrealistic than previous attempts which only used a single score like accuracy,\nmulti-dimensional evaluations can complicate decision-making since there is no\nobvious way to select an optimal model. This work introduces preference\nsampling, a method to extract a scalar trustworthiness score from\nmulti-dimensional evaluation results by considering the many characteristics of\nmodel performance which users value. We show that preference sampling improves\nupon alternate aggregation methods by using multi-dimensional trustworthiness\nevaluations of LLMs from TrustLLM and DecodingTrust. We find that preference\nsampling is consistently reductive, fully reducing the set of candidate models\n100% of the time whereas Pareto optimality never reduces the set by more than\n50%. Likewise, preference sampling is consistently sensitive to user\npriors-allowing users to specify the relative weighting and confidence of their\npreferences-whereas averaging scores is intransigent to the users' prior\nknowledge."}
{"id": "2506.03472", "pdf": "https://arxiv.org/pdf/2506.03472", "abs": "https://arxiv.org/abs/2506.03472", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Embeddings for MNIST", "categories": ["cs.LG", "20-XX, 08A02", "F.4.1; I.2"], "comment": null, "summary": "We present an empirical validation of the directional non-commutative\nmonoidal embedding framework recently introduced in prior\nwork~\\cite{Godavarti2025monoidal}. This framework defines learnable\ncompositional embeddings using distinct non-commutative operators per dimension\n(axis) that satisfy an interchange law, generalizing classical one-dimensional\ntransforms. Our primary goal is to verify that this framework can effectively\nmodel real data by applying it to a controlled, well-understood task: image\nclassification on the MNIST dataset~\\cite{lecun1998gradient}. A central\nhypothesis for why the proposed monoidal embedding works well is that it\ngeneralizes the Discrete Fourier Transform (DFT)~\\cite{oppenheim1999discrete}\nby learning task-specific frequency components instead of using fixed basis\nfrequencies. We test this hypothesis by comparing learned monoidal embeddings\nagainst fixed DFT-based embeddings on MNIST. The results show that as the\nembedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance\ngap between the learned monoidal embeddings and fixed DFT-based embeddings on\nMNIST grows increasingly large. This comparison is used as an analytic tool to\nexplain why the framework performs well: the learnable embeddings can capture\nthe most discriminative spectral components for the task. Overall, our\nexperiments confirm that directional non-commutative monoidal embeddings are\nhighly effective for representing image data, offering a compact learned\nrepresentation that retains high task performance. The code used in this work\nis available at\nhttps://github.com/mahesh-godavarti/directional_composition_mnist."}
{"id": "2506.00605", "pdf": "https://arxiv.org/pdf/2506.00605", "abs": "https://arxiv.org/abs/2506.00605", "authors": ["Ruiming Min", "Minghao Liu"], "title": "ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education", "categories": ["eess.IV", "cs.CV"], "comment": "All authors did not agree to submitting this work. This version of\n  the report contains misinformation and is not ready to share", "summary": "With the advancement of modern medicine and the development of technologies\nsuch as MRI, CT, and cellular analysis, it has become increasingly critical for\nclinicians to accurately interpret various diagnostic images. However, modern\nmedical education often faces challenges due to limited access to high-quality\nteaching materials, stemming from privacy concerns and a shortage of\neducational resources (Balogh et al., 2015). In this context, image data\ngenerated by machine learning models, particularly generative models, presents\na promising solution. These models can create diverse and comparable imaging\ndatasets without compromising patient privacy, thereby supporting modern\nmedical education. In this study, we explore the use of convolutional neural\nnetworks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic\nmedical images. The source code is available at\nhttps://github.com/mliuby/COMP4211-Project."}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics."}
{"id": "2506.03448", "pdf": "https://arxiv.org/pdf/2506.03448", "abs": "https://arxiv.org/abs/2506.03448", "authors": ["Bimsara Pathiraja", "Maitreya Patel", "Shivam Singh", "Yezhou Yang", "Chitta Baral"], "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions", "categories": ["cs.CV"], "comment": "Project page: \\url{http://refedit.vercel.app}", "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility."}
{"id": "2506.03407", "pdf": "https://arxiv.org/pdf/2506.03407", "abs": "https://arxiv.org/abs/2506.03407", "authors": ["Lukas Meyer", "Josef Grün", "Maximilian Weiherer", "Bernhard Egger", "Marc Stamminger", "Linus Franke"], "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)\nframework that is able to generate multi-view consistent novel views from\nimages of multiple, independent cameras with different spectral domains. In\ncontrast to previous approaches, our method does not require cross-modal camera\ncalibration and is versatile enough to model a variety of different spectra,\nincluding thermal and near-infra red, without any algorithmic changes.\n  Unlike existing 3DGS-based frameworks that treat each modality separately (by\noptimizing per-channel spherical harmonics) and therefore fail to exploit the\nunderlying spectral and spatial correlations, our method leverages a novel\nneural color representation that encodes multi-spectral information into a\nlearned, compact, per-splat feature embedding. A shallow multi-layer perceptron\n(MLP) then decodes this embedding to obtain spectral color values, enabling\njoint learning of all bands within a unified representation.\n  Our experiments show that this simple yet effective strategy is able to\nimprove multi-spectral rendering quality, while also leading to improved\nper-spectra rendering quality over state-of-the-art methods. We demonstrate the\neffectiveness of this new technique in agricultural applications to render\nvegetation indices, such as normalized difference vegetation index (NDVI)."}
{"id": "2506.03474", "pdf": "https://arxiv.org/pdf/2506.03474", "abs": "https://arxiv.org/abs/2506.03474", "authors": ["Yifeng Xiao", "Yurong Xu", "Ning Yan", "Masood Mortazavi", "Pierluigi Nuzzo"], "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "I.2.6; C.3"], "comment": "Preprint. 10 pages + appendix. Submitted to NeurIPS 2025", "summary": "Simulation-based design space exploration (DSE) aims to efficiently optimize\nhigh-dimensional structured designs under complex constraints and expensive\nevaluation costs. Existing approaches, including heuristic and multi-step\nreinforcement learning (RL) methods, struggle to balance sampling efficiency\nand constraint satisfaction due to sparse, delayed feedback, and large hybrid\naction spaces. In this paper, we introduce CORE, a constraint-aware, one-step\nRL method for simulationguided DSE. In CORE, the policy agent learns to sample\ndesign configurations by defining a structured distribution over them,\nincorporating dependencies via a scaling-graph-based decoder, and by reward\nshaping to penalize invalid designs based on the feedback obtained from\nsimulation. CORE updates the policy using a surrogate objective that compares\nthe rewards of designs within a sampled batch, without learning a value\nfunction. This critic-free formulation enables efficient learning by\nencouraging the selection of higher-reward designs. We instantiate CORE for\nhardware-mapping co-design of neural network accelerators, demonstrating that\nit significantly improves sample efficiency and achieves better accelerator\nconfigurations compared to state-of-the-art baselines. Our approach is general\nand applicable to a broad class of discrete-continuous constrained design\nproblems."}
{"id": "2506.02197", "pdf": "https://arxiv.org/pdf/2506.02197", "abs": "https://arxiv.org/abs/2506.02197", "authors": ["Marcos V. Conde", "Radu Timofte", "Zihao Lu", "Xiangyu Kong", "Xiaoxia Xing", "Fan Wang", "Suejin Han", "MinKyu Park", "Tianyu Zhang", "Xin Luo", "Yeda Chen", "Dong Liu", "Li Pang", "Yuhang Yang", "Hongzhong Wang", "Xiangyong Cao", "Ruixuan Jiang", "Senyan Xu", "Siyuan Jiang", "Xueyang Fu", "Zheng-Jun Zha", "Tianyu Hao", "Yuhong He", "Ruoqi Li", "Yueqi Yang", "Xiang Yu", "Guanlan Hong", "Minmin Yi", "Yuanjia Chen", "Liwen Zhang", "Zijie Jin", "Cheng Li", "Lian Liu", "Wei Song", "Heng Sun", "Yubo Wang", "Jinghua Wang", "Jiajie Lu", "Watchara Ruangsan"], "title": "NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "CVPR 2025 - New Trends in Image Restoration and Enhancement (NTIRE)", "summary": "This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution\nChallenge, highlighting the proposed solutions and results. New methods for RAW\nRestoration and Super-Resolution could be essential in modern Image Signal\nProcessing (ISP) pipelines, however, this problem is not as explored as in the\nRGB domain. The goal of this challenge is two fold, (i) restore RAW images with\nblur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering\nunknown noise and blur. In the challenge, a total of 230 participants\nregistered, and 45 submitted results during thee challenge period. This report\npresents the current state-of-the-art in RAW Restoration."}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580", "abs": "https://arxiv.org/abs/2506.03580", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey."}
{"id": "2506.03449", "pdf": "https://arxiv.org/pdf/2506.03449", "abs": "https://arxiv.org/abs/2506.03449", "authors": ["John W. Smutny"], "title": "The effects of using created synthetic images in computer vision training", "categories": ["cs.CV"], "comment": "Nine pages long. Main content in pages one through eight. References\n  start at page nine", "summary": "This paper investigates how rendering engines, like Unreal Engine 4 (UE), can\nbe used to create synthetic images to supplement datasets for deep computer\nvision (CV) models in image abundant and image limited use cases. Using\nrendered synthetic images from UE can provide developers and businesses with a\nmethod of accessing nearly unlimited, reproducible, agile, and cheap training\nsets for their customers and applications without the threat of poisoned images\nfrom the internet or the cost of collecting them. The validity of these\ngenerated images are examined by testing the change in model test accuracy in\ntwo different sized CV models across two binary classification cases (Cat vs\nDog and Weld Defect Detection). In addition, this paper provides an\nimplementation of how to measure the quality of synthetic images by using\npre-trained CV models as auditors. Results imply that for large (VGG16) and\nsmall (MobileNetV3-small) parameter deep CV models, adding >60% additional\nsynthetic images to a real image dataset during model training can narrow the\ntest-training accuracy gap to ~1-2% without a conclusive effect on test\naccuracy compared to using real world images alone. Likewise, adding <10%\nadditional real training images to synthetic only training sets decreased the\nclassification error rate in half, then decreasing further when adding more\nreal training images. For these cases tested, using synthetic images from\nrendering engines allow researchers to only use 10% of their real images during\ntraining, compared to the traditional 50-70%. This research serves as an\nexample of how to create synthetic images, guidelines on how to use the images,\npotential restrictions and possible performance improvements for data-scarce\nprojects."}
{"id": "2506.03511", "pdf": "https://arxiv.org/pdf/2506.03511", "abs": "https://arxiv.org/abs/2506.03511", "authors": ["Fangyi Cao", "Bin Ren", "Zihao Wang", "Shiwei Fu", "Youbin Mo", "Xiaoyang Liu", "Yuzhou Chen", "Weixin Yao"], "title": "POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.AI", "eess.IV"], "comment": "9 pages main text with 5 figures, 9 pages appendix with 9 figures.\n  Submitted to NeurIPS 2025", "summary": "With over 1,000,000 images from more than 10,000 exposures using\nstate-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)\nin the search for exoplanets, can artificial intelligence (AI) serve as a\ntransformative tool in imaging Earth-like exoplanets in the coming decade? In\nthis paper, we introduce a benchmark and explore this question from a\npolarimetric image representation learning perspective. Despite extensive\ninvestments over the past decade, only a few new exoplanets have been directly\nimaged. Existing imaging approaches rely heavily on labor-intensive labeling of\nreference stars, which serve as background to extract circumstellar objects\n(disks or exoplanets) around target stars. With our POLARIS (POlarized Light\ndAta for total intensity Representation learning of direct Imaging of\nexoplanetary Systems) dataset, we classify reference star and circumstellar\ndisk images using the full public SPHERE/IRDIS polarized-light archive since\n2014, requiring less than 10 percent manual labeling. We evaluate a range of\nmodels including statistical, generative, and large vision-language models and\nprovide baseline performance. We also propose an unsupervised generative\nrepresentation learning framework that integrates these models, achieving\nsuperior performance and enhanced representational power. To our knowledge,\nthis is the first uniformly reduced, high-quality exoplanet imaging dataset,\nrare in astrophysics and machine learning. By releasing this dataset and\nbaselines, we aim to equip astrophysicists with new tools and engage data\nscientists in advancing direct exoplanet imaging, catalyzing major\ninterdisciplinary breakthroughs."}
{"id": "2506.03522", "pdf": "https://arxiv.org/pdf/2506.03522", "abs": "https://arxiv.org/abs/2506.03522", "authors": ["Daniel Campa", "Mehdi Saeedi", "Ian Colbert", "Srinjoy Das"], "title": "Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach", "categories": ["cs.LG", "stat.ML"], "comment": "8 pages, 9 figures, Accepted at the IEEE Conference on Games 2025\n  (IEEE CoG)", "summary": "Navigation path traces play a crucial role in video game design, serving as a\nvital resource for both enhancing player engagement and fine-tuning\nnon-playable character behavior. Generating such paths with human-like realism\ncan enrich the overall gaming experience, and evaluating path traces can\nprovide game designers insights into player interactions. Despite the\nimpressive recent advancements in deep learning-based generative modeling, the\nvideo game industry hesitates to adopt such models for path generation, often\nciting their complex training requirements and interpretability challenges. To\naddress these problems, we propose a novel path generation and evaluation\napproach that is grounded in principled nonparametric statistics and provides\nprecise control while offering interpretable insights. Our path generation\nmethod fuses two statistical techniques: (1) nonparametric model-free\ntransformations that capture statistical characteristics of path traces through\ntime; and (2) copula models that capture statistical dependencies in space. For\npath evaluation, we adapt a nonparametric three-sample hypothesis test designed\nto determine if the generated paths are overfit (mimicking the original data\ntoo closely) or underfit (diverging too far from it). We demonstrate the\nprecision and reliability of our proposed methods with empirical analysis on\ntwo existing gaming benchmarks to showcase controlled generation of diverse\nnavigation paths. Notably, our novel path generator can be fine-tuned with user\ncontrollable parameters to create navigation paths that exhibit varying levels\nof human-likeness in contrast to those produced by neural network-based agents.\nThe code is available at https://github.com/daniel-campa/mf-copula."}
{"id": "2406.04158", "pdf": "https://arxiv.org/pdf/2406.04158", "abs": "https://arxiv.org/abs/2406.04158", "authors": ["Da Li", "Guoqiang Zhao", "Chen Yao", "Kaiqiang Zhu", "Houjun Sun", "Jiacheng Bao"], "title": "CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets with Sparse-Aspect Multi-Baseline Data", "categories": ["cs.CV", "eess.IV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Sparse-aspect multi-baseline Synthetic Aperture Radar (SAR) three-dimensional\n(3D) tomography is a crucial remote sensing technique. Compared to full-aspect\nobservation, it needs only a few observation aspects to achieve a sufficiently\nclear 3D scene reconstruction, providing a cost-effective alternative. In the\npast, compressive sensing (CS) was the mainstream approach for sparse 3D SAR\nimaging. Recently, deep learning (DL) revolutionizes this field through its\npowerful data-driven representation capabilities and efficient inference\ncharacteristics. However, existing DL methods primarily depend on\nhigh-resolution radar images for supervising the training of deep neural\nnetworks (DNNs). This unimodal approach precludes the incorporation of\ncomplementary information from other data sources, thereby limiting potential\nimprovements in imaging performance. In this paper, we propose a Cross-Modal\n3D-SAR Reconstruction Network (CMAR-Net) that enhances 3D SAR imaging by fusing\nheterogeneous information. Leveraging cross-modal supervision from 2D optical\nimages and error transfer guaranteed by differentiable rendering, CMAR-Net\nachieves efficient training and reconstructs highly sparse-aspect\nmulti-baseline SAR image into visually structured and accurate 3D images,\nparticularly for vehicle targets. Extensive experiments on simulated and\nreal-world datasets demonstrate that CMAR-Net significantly outperforms\nstate-of-the-art sparse reconstruction algorithms based on CS and DL, with\naverage improvements of 75.83% in PSNR and 47.85% in SSIM. Furthermore, our\nmethod eliminates the need for time-consuming full-aperture data preprocessing\nand relies solely on computer-rendered optical images, significantly reducing\ndataset construction costs. This work highlights the potential of cross-modal\nlearning for multi-baseline SAR 3D imaging and introduces a novel framework for\nradar imaging research."}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592", "abs": "https://arxiv.org/abs/2506.03592", "authors": ["Viktor Hangya", "Fabian Küch", "Darina Gold"], "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions."}
{"id": "2506.03461", "pdf": "https://arxiv.org/pdf/2506.03461", "abs": "https://arxiv.org/abs/2506.03461", "authors": ["Nan Xiang", "Lifeng Xing", "Dequan Jin"], "title": "RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels", "categories": ["cs.CV", "68Txx", "I.5.1"], "comment": "7 pages, 1 figure", "summary": "In few-shot learning (FSL), the labeled samples are scarce. Thus, label\nerrors can significantly reduce classification accuracy. Since label errors are\ninevitable in realistic learning tasks, improving the robustness of the model\nin the presence of label errors is critical. This paper proposes a new robust\nneural field-based image approach (RoNFA) for few-shot image classification\nwith noisy labels. RoNFA consists of two neural fields for feature and category\nrepresentation. They correspond to the feature space and category set. Each\nneuron in the field for category representation (FCR) has a receptive field\n(RF) on the field for feature representation (FFR) centered at the\nrepresentative neuron for its category generated by soft clustering. In the\nprediction stage, the range of these receptive fields adapts according to the\nneuronal activation in FCR to ensure prediction accuracy. These learning\nstrategies provide the proposed model with excellent few-shot learning\ncapability and strong robustness against label noises. The experimental results\non real-world FSL datasets with three different types of label noise\ndemonstrate that the proposed method significantly outperforms state-of-the-art\nFSL methods. Its accuracy obtained in the presence of noisy labels even\nsurpasses the results obtained by state-of-the-art FSL methods trained on clean\nsupport sets, indicating its strong robustness against noisy labels."}
{"id": "2506.03516", "pdf": "https://arxiv.org/pdf/2506.03516", "abs": "https://arxiv.org/abs/2506.03516", "authors": ["Arnab Debnath", "Gregory J. Stein", "Jana Kosecka"], "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted at CVPR 2025 workshop - Foundation Models Meet Embodied\n  Agents", "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent\nis instructed to locate a target object in an unexplored environment.\nTraditional learning-based methods rely heavily on large-scale annotated data\nor require extensive interaction with the environment in a reinforcement\nlearning setting, often failing to generalize to novel environments and\nlimiting scalability. To overcome these challenges, we explore a zero-shot\nsetting where the agent operates without task-specific training, enabling more\nscalable and adaptable solution. Recent advances in Vision Foundation Models\n(VFMs) offer powerful capabilities for visual understanding and reasoning,\nmaking them ideal for agents to comprehend scenes, identify relevant regions,\nand infer the likely locations of objects. In this work, we present a zero-shot\nobject goal navigation framework that integrates the perceptual strength of\nVFMs with a model-based planner that is capable of long-horizon decision making\nthrough frontier exploration. We evaluate our approach on the HM3D dataset\nusing the Habitat simulator and demonstrate that our method achieves\nstate-of-the-art performance in terms of success weighted by path length for\nzero-shot object goal navigation."}
{"id": "2506.03531", "pdf": "https://arxiv.org/pdf/2506.03531", "abs": "https://arxiv.org/abs/2506.03531", "authors": ["Daniel Ovalle", "Lorenz T. Biegler", "Ignacio E. Grossmann", "Carl D. Laird", "Mateo Dulce Rubio"], "title": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel\nframework that provides probabilistic feasibility guarantees for data-driven\nconstraints in optimization problems. While standard Mixed-Integer Constraint\nLearning methods often violate the true constraints due to model error or data\nlimitations, our C-MICL approach leverages conformal prediction to ensure\nfeasible solutions are ground-truth feasible. This guarantee holds with\nprobability at least $1{-}\\alpha$, under a conditional independence assumption.\nThe proposed framework supports both regression and classification tasks\nwithout requiring access to the true constraint function, while avoiding the\nscalability issues associated with ensemble-based heuristics. Experiments on\nreal-world applications demonstrate that C-MICL consistently achieves target\nfeasibility rates, maintains competitive objective performance, and\nsignificantly reduces computational cost compared to existing methods. Our work\nbridges mathematical optimization and machine learning, offering a principled\napproach to incorporate uncertainty-aware constraints into decision-making with\nrigorous statistical guarantees."}
{"id": "2503.17797", "pdf": "https://arxiv.org/pdf/2503.17797", "abs": "https://arxiv.org/abs/2503.17797", "authors": ["Chaoyu Liu", "Davide Murari", "Lihao Liu", "Yangming Li", "Chris Budd", "Carola-Bibiane Schönlieb"], "title": "Enhancing Fourier Neural Operators with Local Spatial Features", "categories": ["cs.LG", "eess.IV"], "comment": null, "summary": "Partial Differential Equation (PDE) problems often exhibit strong local\nspatial structures, and effectively capturing these structures is critical for\napproximating their solutions. Recently, the Fourier Neural Operator (FNO) has\nemerged as an efficient approach for solving these PDE problems. By using\nparametrization in the frequency domain, FNOs can efficiently capture global\npatterns. However, this approach inherently overlooks the critical role of\nlocal spatial features, as frequency-domain parameterized convolutions\nprimarily emphasize global interactions without encoding comprehensive\nlocalized spatial dependencies. Although several studies have attempted to\naddress this limitation, their extracted Local Spatial Features (LSFs) remain\ninsufficient, and computational efficiency is often compromised. To address\nthis limitation, we introduce a convolutional neural network (CNN)-based\nfeature pre-extractor to capture LSFs directly from input data, resulting in a\nhybrid architecture termed \\textit{Conv-FNO}. Furthermore, we introduce two\nnovel resizing schemes to make our Conv-FNO resolution invariant. In this work,\nwe focus on demonstrating the effectiveness of incorporating LSFs into FNOs by\nconducting both a theoretical analysis and extensive numerical experiments. Our\nfindings show that this simple yet impactful modification enhances the\nrepresentational capacity of FNOs and significantly improves performance on\nchallenging PDE benchmarks."}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution."}
{"id": "2506.03473", "pdf": "https://arxiv.org/pdf/2506.03473", "abs": "https://arxiv.org/abs/2506.03473", "authors": ["Xinru Ying", "Jiaqi Mo", "Jingyang Lin", "Canghong Jin", "Fangfang Wang", "Lina Wei"], "title": "MamFusion: Multi-Mamba with Temporal Fusion for Partially Relevant Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Partially Relevant Video Retrieval (PRVR) is a challenging task in the domain\nof multimedia retrieval. It is designed to identify and retrieve untrimmed\nvideos that are partially relevant to the provided query. In this work, we\ninvestigate long-sequence video content understanding to address information\nredundancy issues. Leveraging the outstanding long-term state space modeling\ncapability and linear scalability of the Mamba module, we introduce a\nmulti-Mamba module with temporal fusion framework (MamFusion) tailored for PRVR\ntask. This framework effectively captures the state-relatedness in long-term\nvideo content and seamlessly integrates it into text-video relevance\nunderstanding, thereby enhancing the retrieval process. Specifically, we\nintroduce Temporal T-to-V Fusion and Temporal V-to-T Fusion to explicitly model\ntemporal relationships between text queries and video moments, improving\ncontextual awareness and retrieval accuracy. Extensive experiments conducted on\nlarge-scale datasets demonstrate that MamFusion achieves state-of-the-art\nperformance in retrieval effectiveness. Code is available at the link:\nhttps://github.com/Vision-Multimodal-Lab-HZCU/MamFusion."}
{"id": "2506.03525", "pdf": "https://arxiv.org/pdf/2506.03525", "abs": "https://arxiv.org/abs/2506.03525", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project website: https://video-skill-cot.github.io/", "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains."}
{"id": "2506.03542", "pdf": "https://arxiv.org/pdf/2506.03542", "abs": "https://arxiv.org/abs/2506.03542", "authors": ["Yongxiang Tang", "Yanhua Cheng", "Xiaocheng Liu", "Chenchen Jiao", "Yanxiang Zeng", "Ning Luo", "Pengjia Yuan", "Xialong Liu", "Peng Jiang"], "title": "Learning Monotonic Probabilities with a Generative Cost Model", "categories": ["cs.LG"], "comment": null, "summary": "In many machine learning tasks, it is often necessary for the relationship\nbetween input and output variables to be monotonic, including both strictly\nmonotonic and implicitly monotonic relationships. Traditional methods for\nmaintaining monotonicity mainly rely on construction or regularization\ntechniques, whereas this paper shows that the issue of strict monotonic\nprobability can be viewed as a partial order between an observable revenue\nvariable and a latent cost variable. This perspective enables us to reformulate\nthe monotonicity challenge into modeling the latent cost variable. To tackle\nthis, we introduce a generative network for the latent cost variable, termed\nthe Generative Cost Model (GCM), which inherently addresses the strict\nmonotonic problem, and propose the Implicit Generative Cost Model (IGCM) to\naddress the implicit monotonic problem. We further validate our approach with a\nnumerical simulation of quantile regression and conduct multiple experiments on\npublic datasets, showing that our method significantly outperforms existing\nmonotonic modeling techniques. The code for our experiments can be found at\nhttps://github.com/tyxaaron/GCM."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum, ideal for extreme\nparameter-efficient scenarios. Evaluated on personalized text-to-image\ngeneration using Stable Diffusion XL as baseline, WaveFT significantly\noutperforms LoRA and other PEFT methods, especially at low parameter counts;\nachieving superior subject fidelity, prompt alignment, and image diversity."}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598", "abs": "https://arxiv.org/abs/2506.03598", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL."}
{"id": "2506.03481", "pdf": "https://arxiv.org/pdf/2506.03481", "abs": "https://arxiv.org/abs/2506.03481", "authors": ["Hongsong Wang", "Xiaoyan Ma", "Jidong Kuang", "Jie Gui"], "title": "Heterogeneous Skeleton-Based Action Representation Learning", "categories": ["cs.CV"], "comment": "To appear in CVPR 2025", "summary": "Skeleton-based human action recognition has received widespread attention in\nrecent years due to its diverse range of application scenarios. Due to the\ndifferent sources of human skeletons, skeleton data naturally exhibit\nheterogeneity. The previous works, however, overlook the heterogeneity of human\nskeletons and solely construct models tailored for homogeneous skeletons. This\nwork addresses the challenge of heterogeneous skeleton-based action\nrepresentation learning, specifically focusing on processing skeleton data that\nvaries in joint dimensions and topological structures. The proposed framework\ncomprises two primary components: heterogeneous skeleton processing and unified\nrepresentation learning. The former first converts two-dimensional skeleton\ndata into three-dimensional skeleton via an auxiliary network, and then\nconstructs a prompted unified skeleton using skeleton-specific prompts. We also\ndesign an additional modality named semantic motion encoding to harness the\nsemantic information within skeletons. The latter module learns a unified\naction representation using a shared backbone network that processes different\nheterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and\nPKU-MMD II datasets demonstrate the effectiveness of our method in various\ntasks of action understanding. Our approach can be applied to action\nrecognition in robots with different humanoid structures."}
{"id": "2506.03568", "pdf": "https://arxiv.org/pdf/2506.03568", "abs": "https://arxiv.org/abs/2506.03568", "authors": ["Li Zeqiao", "Wang Yijing", "Wang Haoyu", "Li Zheng", "Li Peng", "Zuo zhiqiang", "Hu Chuan"], "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous driving promises significant advancements in mobility, road safety\nand traffic efficiency, yet reinforcement learning and imitation learning face\nsafe-exploration and distribution-shift challenges. Although human-AI\ncollaboration alleviates these issues, it often relies heavily on extensive\nhuman intervention, which increases costs and reduces efficiency. This paper\ndevelops a confidence-guided human-AI collaboration (C-HAC) strategy to\novercome these limitations. First, C-HAC employs a distributional proxy value\npropagation method within the distributional soft actor-critic (DSAC)\nframework. By leveraging return distributions to represent human intentions\nC-HAC achieves rapid and stable learning of human-guided policies with minimal\nhuman interaction. Subsequently, a shared control mechanism is activated to\nintegrate the learned human-guided policy with a self-learning policy that\nmaximizes cumulative rewards. This enables the agent to explore independently\nand continuously enhance its performance beyond human guidance. Finally, a\npolicy confidence evaluation algorithm capitalizes on DSAC's return\ndistribution networks to facilitate dynamic switching between human-guided and\nself-learning policies via a confidence-based intervention function. This\nensures the agent can pursue optimal policies while maintaining safety and\nperformance guarantees. Extensive experiments across diverse driving scenarios\nreveal that C-HAC significantly outperforms conventional methods in terms of\nsafety, efficiency, and overall performance, achieving state-of-the-art\nresults. The effectiveness of the proposed method is further validated through\nreal-world road tests in complex traffic conditions. The videos and code are\navailable at: https://github.com/lzqw/C-HAC."}
{"id": "2506.03556", "pdf": "https://arxiv.org/pdf/2506.03556", "abs": "https://arxiv.org/abs/2506.03556", "authors": ["Wang WeiQuan", "Riaz-ul-Haque Mian"], "title": "Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "In semiconductor manufacturing, testing costs remain significantly high,\nespecially during wafer and FPGA testing. To reduce the number of required\ntests while maintaining predictive accuracy, this study investigates three\nbaseline sampling strategies: Random Sampling, Stratified Sampling, and k-means\nClustering Sampling. To further enhance these methods, this study proposes a\nnovel algorithm that improves the sampling quality of each approach. This\nresearch is conducted using real industrial production data from wafer-level\ntests and silicon measurements from various FPGAs. This study introduces two\nhybrid strategies: Stratified with Short Distance Elimination (S-SDE) and\nk-means with Short Distance Elimination (K-SDE). Their performance is evaluated\nwithin the framework of Gaussian Process Regression (GPR) for predicting wafer\nand FPGA test data. At the core of our proposed approach is the Short Distance\nElimination (SDE) algorithm, which excludes spatially proximate candidate\npoints during sampling, thereby ensuring a more uniform distribution of\ntraining data across the physical domain. A parameter sweep was conducted over\nthe (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,\n3, 4} and not both zero, to identify the optimal combination that minimizes\nRMSD. Experimental results on a randomly selected wafer file reveal that\n(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent\nexperiments adopt this parameter configuration. The results demonstrate that\nthe proposed SDE-based strategies enhance predictive accuracy: K-SDE improves\nupon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while\nS-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84\npercent (FPGA)."}
{"id": "2506.01224", "pdf": "https://arxiv.org/pdf/2506.01224", "abs": "https://arxiv.org/abs/2506.01224", "authors": ["John W. Smutny"], "title": "Dirty and Clean-Label attack detection using GAN discriminators", "categories": ["cs.CV", "eess.IV"], "comment": "13 pages total. Appendix starts on page 10", "summary": "Gathering enough images to train a deep computer vision model is a constant\nchallenge. Unfortunately, collecting images from unknown sources can leave your\nmodel s behavior at risk of being manipulated by a dirty-label or clean-label\nattack unless the images are properly inspected. Manually inspecting each\nimage-label pair is impractical and common poison-detection methods that\ninvolve re-training your model can be time consuming. This research uses GAN\ndiscriminators to protect a single class against mislabeled and different\nlevels of modified images. The effect of said perturbation on a basic\nconvolutional neural network classifier is also included for reference. The\nresults suggest that after training on a single class, GAN discriminator s\nconfidence scores can provide a threshold to identify mislabeled images and\nidentify 100% of the tested poison starting at a perturbation epsilon magnitude\nof 0.20, after decision threshold calibration using in-class samples.\nDevelopers can use this report as a basis to train their own discriminators to\nprotect high valued classes in their CV models."}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616", "abs": "https://arxiv.org/abs/2506.03616", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning."}
{"id": "2506.03502", "pdf": "https://arxiv.org/pdf/2506.03502", "abs": "https://arxiv.org/abs/2506.03502", "authors": ["Yuxuan Chen", "Haipeng Xie"], "title": "CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "The denoising diffusion probabilistic model has become a mainstream\ngenerative model, achieving significant success in various computer vision\ntasks. Recently, there has been initial exploration of applying diffusion\nmodels to time series tasks. However, existing studies still face challenges in\nmulti-scale feature alignment and generative capabilities across different\nentities and long-time scales. In this paper, we propose CHIME, a conditional\nhallucination and integrated multi-scale enhancement framework for time series\ndiffusion models. By employing multi-scale decomposition and adaptive\nintegration, CHIME captures the decomposed features of time series, achieving\nin-domain distribution alignment between generated and original samples. In\naddition, we introduce a feature hallucination module in the conditional\ndenoising process, enabling the transfer of temporal features through the\ntraining of category-independent transformation layers. Experimental results on\npublicly available real-world datasets demonstrate that CHIME achieves\nstate-of-the-art performance and exhibits excellent generative generalization\ncapabilities in few-shot scenarios."}
{"id": "2506.03571", "pdf": "https://arxiv.org/pdf/2506.03571", "abs": "https://arxiv.org/abs/2506.03571", "authors": ["Chong Hyun Lee", "Kibae Lee"], "title": "DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose DaigNet, a new approach to object detection with which we can\ndetect an object bounding box using diagonal constraints on adjacency matrix of\na graph convolutional network (GCN). We propose two diagonalization algorithms\nbased on hard and soft constraints on adjacency matrix and two loss functions\nusing diagonal constraint and complementary constraint. The DaigNet eliminates\nthe need for designing a set of anchor boxes commonly used. To prove\nfeasibility of our novel detector, we adopt detection head in YOLO models.\nExperiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than\nYOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%\nhigher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8."}
{"id": "2506.03588", "pdf": "https://arxiv.org/pdf/2506.03588", "abs": "https://arxiv.org/abs/2506.03588", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "title": "A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "The decision-making process significantly influences the predictions of\nmachine learning models. This is especially important in rule-based systems\nsuch as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and\napplication of rules directly determine prediction accuracy and reliability.\nLFCSs combine evolutionary algorithms with supervised learning to optimize\nfuzzy classification rules, offering enhanced interpretability and robustness.\nDespite these advantages, research on improving decision-making mechanisms\n(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use\nvoting-based or single-winner-based inference schemes. These schemes rely on\nclassification performance on training data and may not perform well on unseen\ndata, risking overfitting. To address these limitations, this article\nintroduces a novel class inference scheme for LFCSs based on the\nDempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles\nuncertainty well. By using the DS theory, the scheme calculates belief masses\n(i.e., measures of belief) for each specific class and the ``I don't know''\nstate from each fuzzy rule and infers a class from these belief masses. Unlike\nthe conventional schemes, the proposed scheme also considers the ``I don't\nknow'' state that reflects uncertainty, thereby improving the transparency and\nreliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the\nproposed scheme demonstrates statistically significant improvements in terms of\ntest macro F1 scores across 30 real-world datasets compared to conventional\nvoting-based and single-winner-based fuzzy inference schemes. It forms smoother\ndecision boundaries, provides reliable confidence measures, and enhances the\nrobustness and generalizability of LFCSs in real-world applications. Our\nimplementation is available at https://github.com/YNU-NakataLab/jUCS."}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619", "abs": "https://arxiv.org/abs/2506.03619", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval."}
{"id": "2506.03512", "pdf": "https://arxiv.org/pdf/2506.03512", "abs": "https://arxiv.org/abs/2506.03512", "authors": ["Daikun Liu", "Lei Cheng", "Teng Wang", "changyin Sun"], "title": "EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Recent learning-based methods for event-based optical flow estimation utilize\ncost volumes for pixel matching but suffer from redundant computations and\nlimited scalability to higher resolutions for flow refinement. In this work, we\ntake advantage of the complementarity between temporally dense feature\ndifferences of adjacent event frames and cost volume and present a lightweight\nevent-based optical flow network (EDCFlow) to achieve high-quality flow\nestimation at a higher resolution. Specifically, an attention-based multi-scale\ntemporal feature difference layer is developed to capture diverse motion\npatterns at high resolution in a computation-efficient manner. An adaptive\nfusion of high-resolution difference motion features and low-resolution\ncorrelation motion features is performed to enhance motion representation and\nmodel generalization. Notably, EDCFlow can serve as a plug-and-play refinement\nmodule for RAFT-like event-based methods to enhance flow details. Extensive\nexperiments demonstrate that EDCFlow achieves better performance with lower\ncomplexity compared to existing methods, offering superior generalization."}
{"id": "2506.03582", "pdf": "https://arxiv.org/pdf/2506.03582", "abs": "https://arxiv.org/abs/2506.03582", "authors": ["Rui Yann", "Xianglei Xing"], "title": "ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present ViTSGMM, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, while their\ngeneralization ability when dealing with extremely limited labeled data remains\nto be improved. To address these limitations, we construct a hierarchical\nmixture density classification decision mechanism by optimizing mutual\ninformation between feature representations and target classes, compressing\nredundant information while retaining crucial discriminative components.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on STL-10 and CIFAR-10/100 datasets when using negligible labeled\nsamples. Notably, this paper also reveals a long-overlooked data leakage issue\nin the STL-10 dataset for semi-supervised learning tasks and removes duplicates\nto ensure the reliability of experimental results. Code available at\nhttps://github.com/Shu1L0n9/ViTSGMM."}
{"id": "2506.03590", "pdf": "https://arxiv.org/pdf/2506.03590", "abs": "https://arxiv.org/abs/2506.03590", "authors": ["Minh Luu", "Surya Jasper", "Khoi Le", "Evan Pan", "Michael Quinn", "Aakash Tyagi", "Jiang Hu"], "title": "VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration", "categories": ["cs.LG"], "comment": null, "summary": "Failure triage in design functional verification is critical but\ntime-intensive, relying on manual specification reviews, log inspections, and\nwaveform analyses. While machine learning (ML) has improved areas like stimulus\ngeneration and coverage closure, its application to RTL-level simulation\nfailure triage, particularly for large designs, remains limited. VCDiag offers\nan efficient, adaptable approach using VCD data to classify failing waveforms\nand pinpoint likely failure locations. In the largest experiment, VCDiag\nachieves over 94% accuracy in identifying the top three most likely modules.\nThe framework introduces a novel signal selection and statistical compression\napproach, achieving over 120x reduction in raw data size while preserving\nfeatures essential for classification. It can also be integrated into diverse\nVerilog/SystemVerilog designs and testbenches."}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627", "abs": "https://arxiv.org/abs/2506.03627", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications."}
{"id": "2506.03517", "pdf": "https://arxiv.org/pdf/2506.03517", "abs": "https://arxiv.org/abs/2506.03517", "authors": ["Ziyi Wu", "Anil Kag", "Ivan Skorokhodov", "Willi Menapace", "Ashkan Mirzaei", "Igor Gilitschenski", "Sergey Tulyakov", "Aliaksandr Siarohin"], "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://snap-research.github.io/DenseDPO/", "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels."}
{"id": "2506.03589", "pdf": "https://arxiv.org/pdf/2506.03589", "abs": "https://arxiv.org/abs/2506.03589", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages, 14 figures", "summary": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks."}
{"id": "2506.03595", "pdf": "https://arxiv.org/pdf/2506.03595", "abs": "https://arxiv.org/abs/2506.03595", "authors": ["Runa Eschenhagen", "Aaron Defazio", "Tsung-Hsien Lee", "Richard E. Turner", "Hao-Jun Michael Shi"], "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed\ninterest in Kronecker-factorization-based optimization algorithms for training\nneural networks. Despite its success, Shampoo relies heavily on several\nheuristics such as learning rate grafting and stale preconditioning to achieve\nperformance at-scale. These heuristics increase algorithmic complexity,\nnecessitate further hyperparameter tuning, and lack theoretical justification.\nThis paper investigates these heuristics from the angle of Frobenius norm\napproximation to full-matrix Adam and decouples the preconditioner's\neigenvalues and eigenbasis updates. We show that grafting from Adam mitigates\nthe staleness and mis-scaling of the preconditioner's eigenvalues and how\ncorrecting the eigenvalues directly can eliminate the need for learning rate\ngrafting. To manage the error induced by infrequent eigenbasis computations, we\npropose an adaptive criterion for determining the eigenbasis computation\nfrequency motivated by terminating a warm-started QR algorithm. This criterion\ndecouples the update frequency of different preconditioner matrices and enables\nus to investigate the impact of approximation error on convergence. These\npractical techniques offer a principled angle towards removing Shampoo's\nheuristics and developing improved Kronecker-factorization-based training\nalgorithms."}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles."}
{"id": "2506.03521", "pdf": "https://arxiv.org/pdf/2506.03521", "abs": "https://arxiv.org/abs/2506.03521", "authors": ["Weinan He", "Zilei Wang", "Yixin Zhang"], "title": "Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation", "categories": ["cs.CV"], "comment": "Camera-ready version for AAAI 2025", "summary": "Universal Domain Adaptation (UniDA) focuses on transferring source domain\nknowledge to the target domain under both domain shift and unknown category\nshift. Its main challenge lies in identifying common class samples and aligning\nthem. Current methods typically obtain target domain semantics centers from an\nunconstrained continuous image representation space. Due to domain shift and\nthe unknown number of clusters, these centers often result in complex and less\nrobust alignment algorithm. In this paper, based on vision-language models, we\nsearch for semantic centers in a semantically meaningful and discrete text\nrepresentation space. The constrained space ensures almost no domain bias and\nappropriate semantic granularity for these centers, enabling a simple and\nrobust adaptation algorithm. Specifically, we propose TArget Semantics\nClustering (TASC) via Text Representations, which leverages information\nmaximization as a unified objective and involves two stages. First, with the\nfrozen encoders, a greedy search-based framework is used to search for an\noptimal set of text embeddings to represent target semantics. Second, with the\nsearch results fixed, encoders are refined based on gradient descent,\nsimultaneously achieving robust domain alignment and private class clustering.\nAdditionally, we propose Universal Maximum Similarity (UniMS), a scoring\nfunction tailored for detecting open-set samples in UniDA. Experimentally, we\nevaluate the universality of UniDA algorithms under four category shift\nscenarios. Extensive experiments on four benchmarks demonstrate the\neffectiveness and robustness of our method, which has achieved state-of-the-art\nperformance."}
{"id": "2506.03602", "pdf": "https://arxiv.org/pdf/2506.03602", "abs": "https://arxiv.org/abs/2506.03602", "authors": ["Hiroki Shiraishi", "Yohei Hayamizu", "Tomonori Hashiyama", "Keiki Takadama", "Hisao Ishibuchi", "Masaya Nakata"], "title": "Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Rule representations significantly influence the search capabilities and\ndecision boundaries within the search space of Learning Classifier Systems\n(LCSs), a family of rule-based machine learning systems that evolve\ninterpretable models through evolutionary processes. However, it is very\ndifficult to choose an appropriate rule representation for each problem.\nAdditionally, some problems benefit from using different representations for\ndifferent subspaces within the input space. Thus, an adaptive mechanism is\nneeded to choose an appropriate rule representation for each rule in LCSs. This\narticle introduces a flexible rule representation using a four-parameter beta\ndistribution and integrates it into a fuzzy-style LCS. The four-parameter beta\ndistribution can form various function shapes, and this flexibility enables our\nLCS to automatically select appropriate representations for different\nsubspaces. Our rule representation can represent crisp/fuzzy decision\nboundaries in various boundary shapes, such as rectangles and bells, by\ncontrolling four parameters, compared to the standard representations such as\ntrapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the\nappropriate rule representation for each subspace. Moreover, our LCS\nincorporates a generalization bias favoring crisp rules where feasible,\nenhancing model interpretability without compromising accuracy. Experimental\nresults on real-world classification tasks show that our LCS achieves\nsignificantly superior test accuracy and produces more compact rule sets. Our\nimplementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An\nextended abstract related to this work is available at\nhttps://doi.org/10.36227/techrxiv.174900805.59801248/v1."}
{"id": "2506.03618", "pdf": "https://arxiv.org/pdf/2506.03618", "abs": "https://arxiv.org/abs/2506.03618", "authors": ["Jiayi Wan", "Xiang Zhu", "Fanzhen Liu", "Wei Fan", "Xiaolong Xu"], "title": "GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget."}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA."}
{"id": "2506.03538", "pdf": "https://arxiv.org/pdf/2506.03538", "abs": "https://arxiv.org/abs/2506.03538", "authors": ["Chengqi Li", "Zhihao Shi", "Yangdi Lu", "Wenbo He", "Xiangyu Xu"], "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released."}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching."}
{"id": "2506.03674", "pdf": "https://arxiv.org/pdf/2506.03674", "abs": "https://arxiv.org/abs/2506.03674", "authors": ["Yidi Wang", "Jiawei Gu", "pei Xiaobing", "Xubin Zheng", "Xiao Luo", "Pengyang Wang", "Ziyue Qiao"], "title": "Out-of-Distribution Graph Models Merging", "categories": ["cs.LG"], "comment": null, "summary": "This paper studies a novel problem of out-of-distribution graph models\nmerging, which aims to construct a generalized model from multiple graph models\npre-trained on different domains with distribution discrepancy. This problem is\nchallenging because of the difficulty in learning domain-invariant knowledge\nimplicitly in model parameters and consolidating expertise from potentially\nheterogeneous GNN backbones. In this work, we propose a graph generation\nstrategy that instantiates the mixture distribution of multiple domains. Then,\nwe merge and fine-tune the pre-trained graph models via a MoE module and a\nmasking mechanism for generalized adaptation. Our framework is\narchitecture-agnostic and can operate without any source/target domain data.\nBoth theoretical analysis and experimental results demonstrate the\neffectiveness of our approach in addressing the model generalization problem."}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hernán Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge Sánchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model."}
{"id": "2506.03555", "pdf": "https://arxiv.org/pdf/2506.03555", "abs": "https://arxiv.org/abs/2506.03555", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui"], "title": "WIFE-Fusion:Wavelet-aware Intra-inter Frequency Enhancement for Multi-model Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal image fusion effectively aggregates information from diverse\nmodalities, with fused images playing a crucial role in vision systems.\nHowever, existing methods often neglect frequency-domain feature exploration\nand interactive relationships. In this paper, we propose wavelet-aware\nIntra-inter Frequency Enhancement Fusion (WIFE-Fusion), a multimodal image\nfusion framework based on frequency-domain components interactions. Its core\ninnovations include: Intra-Frequency Self-Attention (IFSA) that leverages\ninherent cross-modal correlations and complementarity through interactive\nself-attention mechanisms to extract enriched frequency-domain features, and\nInter-Frequency Interaction (IFI) that enhances enriched features and filters\nlatent features via combinatorial interactions between heterogeneous\nfrequency-domain components across modalities. These processes achieve precise\nsource feature extraction and unified modeling of feature\nextraction-aggregation. Extensive experiments on five datasets across three\nmultimodal fusion tasks demonstrate WIFE-Fusion's superiority over current\nspecialized and unified fusion methods. Our code is available at\nhttps://github.com/Lmmh058/WIFE-Fusion."}
{"id": "2506.03621", "pdf": "https://arxiv.org/pdf/2506.03621", "abs": "https://arxiv.org/abs/2506.03621", "authors": ["Chaehun Shin", "Jooyoung Choi", "Johan Barthelemy", "Jungbeom Lee", "Sungroh Yoon"], "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/"}
{"id": "2506.03696", "pdf": "https://arxiv.org/pdf/2506.03696", "abs": "https://arxiv.org/abs/2506.03696", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Predictive Business Process Monitoring (PBPM) aims to forecast future\noutcomes of ongoing business processes. However, existing methods often lack\nflexibility to handle real-world challenges such as simultaneous events, class\nimbalance, and multi-level attributes. While prior work has explored static\nencoding schemes and fixed LSTM architectures, they struggle to support\nadaptive representations and generalize across heterogeneous datasets. To\naddress these limitations, we propose a suite of dynamic LSTM HyperModels that\nintegrate two-level hierarchical encoding for event and sequence attributes,\ncharacter-based decomposition of event labels, and novel pseudo-embedding\ntechniques for durations and attribute correlations. We further introduce\nspecialized LSTM variants for simultaneous event modeling, leveraging\nmultidimensional embeddings and time-difference flag augmentation. Experimental\nvalidation on four public and real-world datasets demonstrates up to 100%\naccuracy on balanced datasets and F1 scores exceeding 86\\% on imbalanced ones.\nOur approach advances PBPM by offering modular and interpretable models better\nsuited for deployment in complex settings. Beyond PBPM, it contributes to the\nbroader AI community by improving temporal outcome prediction, supporting data\nheterogeneity, and promoting explainable process intelligence frameworks."}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}."}
{"id": "2506.03583", "pdf": "https://arxiv.org/pdf/2506.03583", "abs": "https://arxiv.org/abs/2506.03583", "authors": ["Zhigang Yang", "Huiguang Yao", "Linmao Tian", "Xuezhi Zhao", "Qiang Li", "Qi Wang"], "title": "A Large-Scale Referring Remote Sensing Image Segmentation Dataset and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Referring Remote Sensing Image Segmentation is a complex and challenging task\nthat integrates the paradigms of computer vision and natural language\nprocessing. Existing datasets for RRSIS suffer from critical limitations in\nresolution, scene diversity, and category coverage, which hinders the\ngeneralization and real-world applicability of refer segmentation models. To\nfacilitate the development of this field, we introduce NWPU-Refer, the largest\nand most diverse RRSIS dataset to date, comprising 15,003 high-resolution\nimages (1024-2048px) spanning 30+ countries with 49,745 annotated targets\nsupporting single-object, multi-object, and non-object segmentation scenarios.\nAdditionally, we propose the Multi-scale Referring Segmentation Network\n(MRSNet), a novel framework tailored for the unique demands of RRSIS. MRSNet\nintroduces two key innovations: (1) an Intra-scale Feature Interaction Module\n(IFIM) that captures fine-grained details within each encoder stage, and (2) a\nHierarchical Feature Interaction Module (HFIM) to enable seamless cross-scale\nfeature fusion, preserving spatial integrity while enhancing discriminative\npower. Extensive experiments conducte on the proposed NWPU-Refer dataset\ndemonstrate that MRSNet achieves state-of-the-art performance across multiple\nevaluation metrics, validating its effectiveness. The dataset and code are\npublicly available at https://github.com/CVer-Yang/NWPU-Refer."}
{"id": "2506.03642", "pdf": "https://arxiv.org/pdf/2506.03642", "abs": "https://arxiv.org/abs/2506.03642", "authors": ["Haoyu Zhang", "Meng Liu", "Zaijing Li", "Haokun Wen", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual-spatial understanding, the ability to infer object relationships and\nlayouts from visual input, is fundamental to downstream tasks such as robotic\nnavigation and embodied interaction. However, existing methods face spatial\nuncertainty and data scarcity, limiting the 3D spatial reasoning capability of\npre-trained vision-language models (VLMs). To address these challenges, we\npresent a unified framework for enhancing 3D spatial reasoning in pre-trained\nVLMs without modifying their architecture. This framework combines SpatialMind,\na structured prompting strategy that decomposes complex scenes and questions\ninto interpretable reasoning steps, with ScanForgeQA, a scalable\nquestion-answering dataset built from diverse 3D simulation scenes through an\nautomated construction process designed for fine-tuning. Extensive experiments\nacross multiple benchmarks demonstrate the individual and combined\neffectiveness of our prompting and fine-tuning strategies, and yield insights\nthat may inspire future research on visual-spatial understanding."}
{"id": "2506.03703", "pdf": "https://arxiv.org/pdf/2506.03703", "abs": "https://arxiv.org/abs/2506.03703", "authors": ["Xiansheng Cai", "Sihan Hu", "Tao Wang", "Yuan Huang", "Pan Zhang", "Youjin Deng", "Kun Chen"], "title": "Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cond-mat.str-el", "physics.comp-ph"], "comment": null, "summary": "Fundamental physics often confronts complex symbolic problems with few\nguiding exemplars or established principles. While artificial intelligence (AI)\noffers promise, its typical need for vast datasets to learn from hinders its\nuse in these information-scarce frontiers. We introduce learning at criticality\n(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models\n(LLMs) to a sharp learning transition, addressing this information scarcity. At\nthis transition, LLMs achieve peak generalization from minimal data,\nexemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic\nreasoning. To elucidate this peak, we analyze a minimal concept-network model\n(CoNet) designed to capture the essence of how LLMs might link tokens. Trained\non a single exemplar, this model also undergoes a sharp learning transition.\nThis transition exhibits hallmarks of a second-order phase transition, notably\npower-law distributed solution path lengths. At this critical point, the system\nmaximizes a ``critical thinking pattern\" crucial for generalization, enabled by\nthe underlying scale-free exploration. This suggests LLMs reach peak\nperformance by operating at criticality, where such explorative dynamics enable\nthe extraction of underlying operational rules. We demonstrate LaC in quantum\nfield theory: an 8B-parameter LLM, tuned to its critical point by LaC using a\nfew exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,\nsignificantly outperforming far larger models. LaC thus leverages critical\nphenomena, a physical principle, to empower AI for complex, data-sparse\nchallenges in fundamental physics."}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700", "abs": "https://arxiv.org/abs/2506.03700", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."}
{"id": "2506.03591", "pdf": "https://arxiv.org/pdf/2506.03591", "abs": "https://arxiv.org/abs/2506.03591", "authors": ["Jiaxing Zhang", "Xinyi Zeng", "Hao Tang"], "title": "Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts", "categories": ["cs.CV"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) based on end-to-end\nautoregressive (AR) transformers effectively integrate both understanding and\ngeneration tasks within a single framework. However, intrinsic Task Objective\nConflicts between high-level semantic abstraction in understanding and\nfine-grained detail preservation in generation pose significant challenges,\noften leading to suboptimal trade-offs and task interference. Existing\nsolutions, such as decoupling shared visual encoders, fall short of\nfundamentally resolving these conflicts due to inherent AR architecture. In\nthis paper, we propose a novel approach that decouples internal components of\nAR to resolve task objective conflicts. Specifically, we design UTAMoE, a\nUnified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal\nAR modules via a Task-Aware MoE Layer to create task-specific optimization\nsubpaths. To enhance task differentiation while maintaining overall\ncoordination, we introduce a novel Two-Stage Training Strategy. Extensive\nexperiments on multimodal benchmarks demonstrate that UTAMoE mitigates task\nobjective conflicts, achieving state-of-the-art performance across various\ntasks. Visualizations and ablation studies further validate the effectiveness\nof our approach."}
{"id": "2506.03654", "pdf": "https://arxiv.org/pdf/2506.03654", "abs": "https://arxiv.org/abs/2506.03654", "authors": ["Xiaochun Lei", "Siqi Wu", "Weilin Wu", "Zetao Jiang"], "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6\\% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX."}
{"id": "2506.03719", "pdf": "https://arxiv.org/pdf/2506.03719", "abs": "https://arxiv.org/abs/2506.03719", "authors": ["Quentin Bertrand", "Anne Gagneux", "Mathurin Massias", "Rémi Emonet"], "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modern deep generative models can now produce high-quality synthetic samples\nthat are often indistinguishable from real training data. A growing body of\nresearch aims to understand why recent methods -- such as diffusion and flow\nmatching techniques -- generalize so effectively. Among the proposed\nexplanations are the inductive biases of deep learning architectures and the\nstochastic nature of the conditional flow matching loss. In this work, we rule\nout the latter -- the noisy nature of the loss -- as a primary contributor to\ngeneralization in flow matching. First, we empirically show that in\nhigh-dimensional settings, the stochastic and closed-form versions of the flow\nmatching loss yield nearly equivalent losses. Then, using state-of-the-art flow\nmatching models on standard image datasets, we demonstrate that both variants\nachieve comparable statistical performance, with the surprising observation\nthat using the closed-form can even improve performance."}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704", "abs": "https://arxiv.org/abs/2506.03704", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG."}
{"id": "2506.03596", "pdf": "https://arxiv.org/pdf/2506.03596", "abs": "https://arxiv.org/abs/2506.03596", "authors": ["Feng Han", "Yang Jiao", "Shaoxiang Chen", "Junhao Xu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The field of controllable image generation has seen significant advancements,\nwith various architectures improving generation layout consistency with control\nsignals. However, contemporary methods still face challenges in bridging the\nsemantic gap between input text prompts with sparse semantics and the target\nimages, often over-relying on low-level control signals to infer regional\ndetails. To address this challenge, we propose ControlThinker, a novel\nframework that employs a \"comprehend-then-generate\" paradigm. Firstly, by\nincentivizing the visual reasoning capability of a MLLM, latent semantics from\ncontrol images are mined to enrich text prompts. This enriched semantic\nunderstanding then seamlessly aids in image generation without the need for\nadditional complex modifications. To further tackle the uncertainty arising\nfrom the ambiguity of control images, we encourage broader exploration of\nreasoning trajectories and select the optimal one using a metric-based output\nreward model (ORM). Extensive experimental results demonstrate that\nControlThinker effectively mitigates the semantic gap between raw text prompts\nand target images, resulting in improved visual quality and semantic\nconsistency across a wide range of benchmarks. The code and models are\navailable at https://github.com/Maplebb/ControlThinker."}
{"id": "2506.03667", "pdf": "https://arxiv.org/pdf/2506.03667", "abs": "https://arxiv.org/abs/2506.03667", "authors": ["Joji Joseph", "Bharadwaj Amrutur", "Shalabh Bhatnagar"], "title": "Accelerating SfM-based Pose Estimation with Dominating Set", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a preprocessing technique to speed up\nStructure-from-Motion (SfM) based pose estimation, which is critical for\nreal-time applications like augmented reality (AR), virtual reality (VR), and\nrobotics. Our method leverages the concept of a dominating set from graph\ntheory to preprocess SfM models, significantly enhancing the speed of the pose\nestimation process without losing significant accuracy. Using the OnePose\ndataset, we evaluated our method across various SfM-based pose estimation\ntechniques. The results demonstrate substantial improvements in processing\nspeed, ranging from 1.5 to 14.48 times, and a reduction in reference images and\npoint cloud size by factors of 17-23 and 2.27-4, respectively. This work offers\na promising solution for efficient and accurate 3D pose estimation, balancing\nspeed and accuracy in real-time applications."}
{"id": "2506.03725", "pdf": "https://arxiv.org/pdf/2506.03725", "abs": "https://arxiv.org/abs/2506.03725", "authors": ["Daniil Medyakov", "Sergey Stanko", "Gleb Molodtsov", "Philip Zmushko", "Grigoriy Evseev", "Egor Petrov", "Aleksandr Beznosikov"], "title": "Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization", "categories": ["cs.LG", "math.OC"], "comment": "58 pages, 5 figures, 5 tables", "summary": "Quite recently, large language models have made a significant breakthrough\nacross various disciplines. However, training them is an extremely\nresource-intensive task, even for major players with vast computing resources.\nOne of the methods gaining popularity in light of these challenges is Sign-SGD.\nThis method can be applied both as a memory-efficient approach in single-node\ntraining and as a gradient compression technique in the distributed learning.\nNevertheless, it is impossible to automatically determine the effective\nstepsize from the theoretical standpoint. Indeed, it depends on the parameters\nof the dataset to which we do not have access in the real-world learning\nparadigm. To address this issue, we design several variants of single-node\ndeterministic Sign-SGD. We extend our approaches to practical scenarios:\nstochastic single-node and multi-node learning, methods with incorporated\nmomentum. We conduct extensive experiments on real machine learning problems\nthat emphasize the practical applicability of our ideas."}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723", "abs": "https://arxiv.org/abs/2506.03723", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence."}
{"id": "2506.03605", "pdf": "https://arxiv.org/pdf/2506.03605", "abs": "https://arxiv.org/abs/2506.03605", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Learning to use tools or objects in common scenes, particularly handling them\nin various ways as instructed, is a key challenge for developing interactive\nrobots. Training models to generate such manipulation trajectories requires a\nlarge and diverse collection of detailed manipulation demonstrations for\nvarious objects, which is nearly unfeasible to gather at scale. In this paper,\nwe propose a framework that leverages large-scale ego- and exo-centric video\ndatasets -- constructed globally with substantial effort -- of Exo-Ego4D to\nextract diverse manipulation trajectories at scale. From these extracted\ntrajectories with the associated textual action description, we develop\ntrajectory generation models based on visual and point cloud-based language\nmodels. In the recently proposed egocentric vision-based in-a-quality\ntrajectory dataset of HOT3D, we confirmed that our models successfully generate\nvalid object trajectories, establishing a training dataset and baseline models\nfor the novel task of generating 6DoF manipulation trajectories from action\ndescriptions in egocentric vision."}
{"id": "2506.03682", "pdf": "https://arxiv.org/pdf/2506.03682", "abs": "https://arxiv.org/abs/2506.03682", "authors": ["Melika Ayoughi", "Samira Abnar", "Chen Huang", "Chris Sandino", "Sayeri Lala", "Eeshan Gunesh Dhekane", "Dan Busbridge", "Shuangfei Zhai", "Vimal Thilak", "Josh Susskind", "Pascal Mettes", "Paul Groth", "Hanlin Goh"], "title": "How PARTs assemble into wholes: Learning the relative composition of images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The composition of objects and their parts, along with object-object\npositional relationships, provides a rich source of information for\nrepresentation learning. Hence, spatial-aware pretext tasks have been actively\nexplored in self-supervised learning. Existing works commonly start from a grid\nstructure, where the goal of the pretext task involves predicting the absolute\nposition index of patches within a fixed grid. However, grid-based approaches\nfall short of capturing the fluid and continuous nature of real-world object\ncompositions. We introduce PART, a self-supervised learning approach that\nleverages continuous relative transformations between off-grid patches to\novercome these limitations. By modeling how parts relate to each other in a\ncontinuous space, PART learns the relative composition of images-an off-grid\nstructural relative positioning process that generalizes beyond occlusions and\ndeformations. In tasks requiring precise spatial understanding such as object\ndetection and time series prediction, PART outperforms strong grid-based\nmethods like MAE and DropPos, while also maintaining competitive performance on\nglobal classification tasks with minimal hyperparameter tuning. By breaking\nfree from grid constraints, PART opens up an exciting new trajectory for\nuniversal self-supervised pretraining across diverse datatypes-from natural\nimages to EEG signals-with promising potential in video, medical imaging, and\naudio."}
{"id": "2506.03757", "pdf": "https://arxiv.org/pdf/2506.03757", "abs": "https://arxiv.org/abs/2506.03757", "authors": ["Razvan-Andrei Lascu", "David Šiška", "Łukasz Szpruch"], "title": "PPO in the Fisher-Rao geometry", "categories": ["cs.LG", "math.OC"], "comment": "17 pages", "summary": "Proximal Policy Optimization (PPO) has become a widely adopted algorithm for\nreinforcement learning, offering a practical policy gradient method with strong\nempirical performance. Despite its popularity, PPO lacks formal theoretical\nguarantees for policy improvement and convergence. PPO is motivated by Trust\nRegion Policy Optimization (TRPO) that utilizes a surrogate loss with a KL\ndivergence penalty, which arises from linearizing the value function within a\nflat geometric space. In this paper, we derive a tighter surrogate in the\nFisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).\nOur proposed scheme provides strong theoretical guarantees, including monotonic\npolicy improvement. Furthermore, in the tabular setting, we demonstrate that\nFR-PPO achieves sub-linear convergence without any dependence on the\ndimensionality of the action or state spaces, marking a significant step toward\nestablishing formal convergence results for PPO-based algorithms."}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735", "abs": "https://arxiv.org/abs/2506.03735", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements."}
{"id": "2506.03607", "pdf": "https://arxiv.org/pdf/2506.03607", "abs": "https://arxiv.org/abs/2506.03607", "authors": ["Wing Man Casca Kwok", "Yip Chiu Tung", "Kunal Bhagchandani"], "title": "Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI", "categories": ["cs.CV"], "comment": null, "summary": "Edge computing decentralizes processing power to network edge, enabling\nreal-time AI-driven decision-making in IoT applications. In industrial\nautomation such as robotics and rugged edge AI, real-time perception and\nintelligence are critical for autonomous operations. Deploying\ntransformer-based image captioning models at the edge can enhance machine\nperception, improve scene understanding for autonomous robots, and aid in\nindustrial inspection.\n  However, these edge or IoT devices are often constrained in computational\nresources for physical agility, yet they have strict response time\nrequirements. Traditional deep learning models can be too large and\ncomputationally demanding for these devices. In this research, we present\nfindings of transformer-based models for image captioning that operate\neffectively on edge devices. By evaluating resource-effective transformer\nmodels and applying knowledge distillation techniques, we demonstrate inference\ncan be accelerated on resource-constrained devices while maintaining model\nperformance using these techniques."}
{"id": "2506.03710", "pdf": "https://arxiv.org/pdf/2506.03710", "abs": "https://arxiv.org/abs/2506.03710", "authors": ["Yisen Feng", "Haoyu Zhang", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "OSGNet @ Ego4D Episodic Memory Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solutions for the three egocentric video localization\n  tracks(Natural Language Queries, Goal Step, and Moment Queries tracks) of the\n  Ego4D Episodic Memory Challenge at CVPR EgoVis Workshop 2025", "summary": "In this report, we present our champion solutions for the three egocentric\nvideo localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.\nAll tracks require precise localization of the interval within an untrimmed\negocentric video. Previous unified video localization approaches often rely on\nlate fusion strategies, which tend to yield suboptimal results. To address\nthis, we adopt an early fusion-based video localization model to tackle all\nthree tasks, aiming to enhance localization accuracy. Ultimately, our method\nachieved first place in the Natural Language Queries, Goal Step, and Moment\nQueries tracks, demonstrating its effectiveness. Our code can be found at\nhttps://github.com/Yisen-Feng/OSGNet."}
{"id": "2506.03758", "pdf": "https://arxiv.org/pdf/2506.03758", "abs": "https://arxiv.org/abs/2506.03758", "authors": ["Daniel Palenicek", "Florian Vogt", "Jan Peters"], "title": "Scaling CrossQ with Weight Normalization", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.07523", "summary": "Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics which are\nemphasized by higher UTDs, particularly Q-bias explosion and the growing\nmagnitude of critic network weights. To address this, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nprevents potential loss of plasticity and keeps the effective learning rate\nconstant. Our proposed approach reliably scales with increasing UTD ratios,\nachieving competitive or superior performance across a range of challenging\ntasks on the DeepMind control benchmark, notably the complex dog and humanoid\nenvironments. This work eliminates the need for drastic interventions, such as\nnetwork resets, and offers a robust pathway for improving sample efficiency and\nscalability in model-free reinforcement learning."}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761", "abs": "https://arxiv.org/abs/2506.03761", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions."}
{"id": "2506.03608", "pdf": "https://arxiv.org/pdf/2506.03608", "abs": "https://arxiv.org/abs/2506.03608", "authors": ["Di Fan", "Heng Yu", "Zhiyuan Xu"], "title": "PDSE: A Multiple Lesion Detector for CT Images using PANet and Deformable Squeeze-and-Excitation Block", "categories": ["cs.CV"], "comment": "MIUA 2024", "summary": "Detecting lesions in Computed Tomography (CT) scans is a challenging task in\nmedical image processing due to the diverse types, sizes, and locations of\nlesions. Recently, various one-stage and two-stage framework networks have been\ndeveloped to focus on lesion localization. We introduce a one-stage lesion\ndetection framework, PDSE, by redesigning Retinanet to achieve higher accuracy\nand efficiency for detecting lesions in multimodal CT images. Specifically, we\nenhance the path aggregation flow by incorporating a low-level feature map.\nAdditionally, to improve model representation, we utilize the adaptive\nSqueeze-and-Excitation (SE) block and integrate channel feature map attention.\nThis approach has resulted in achieving new state-of-the-art performance. Our\nmethod significantly improves the detection of small and multiscaled objects.\nWhen evaluated against other advanced algorithms on the public DeepLesion\nbenchmark, our algorithm achieved an mAP of over 0.20."}
{"id": "2506.03737", "pdf": "https://arxiv.org/pdf/2506.03737", "abs": "https://arxiv.org/abs/2506.03737", "authors": ["Hao Yu", "Tangyu Jiang", "Shuning Jia", "Shannan Yan", "Shunning Liu", "Haolong Qian", "Guanghao Li", "Shuting Dong", "Huaisong Zhang", "Chun Yuan"], "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE"}
{"id": "2506.03777", "pdf": "https://arxiv.org/pdf/2506.03777", "abs": "https://arxiv.org/abs/2506.03777", "authors": ["Li Zhang", "Zhongxuan Han", "Chaochao chen", "Xiaohua Feng", "Jiaming Zhang", "Yuyuan Li"], "title": "FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "With emerging application of Federated Learning (FL) in decision-making\nscenarios, it is imperative to regulate model fairness to prevent disparities\nacross sensitive groups (e.g., female, male). Current research predominantly\nfocuses on two concepts of group fairness within FL: Global Fairness (overall\nmodel disparity across all clients) and Local Fairness (the disparity within\neach client). However, the non-decomposable, non-differentiable nature of\nfairness criteria pose two fundamental, unresolved challenges for fair FL: (i)\nHarmonizing global and local fairness in multi-class classification; (ii)\nEnabling a controllable, optimal accuracy-fairness trade-off. To tackle the\naforementioned challenges, we propose a novel controllable federated\ngroup-fairness calibration framework, named FedFACT. FedFACT identifies the\nBayes-optimal classifiers under both global and local fairness constraints in\nmulti-class case, yielding models with minimal performance decline while\nguaranteeing fairness. To effectively realize an adjustable, optimal\naccuracy-fairness balance, we derive specific characterizations of the\nBayes-optimal fair classifiers for reformulating fair FL as personalized\ncost-sensitive learning problem for in-processing, and bi-level optimization\nfor post-processing. Theoretically, we provide convergence and generalization\nguarantees for FedFACT to approach the near-optimal accuracy under given\nfairness levels. Extensive experiments on multiple datasets across various data\nheterogeneity demonstrate that FedFACT consistently outperforms baselines in\nbalancing accuracy and global-local fairness."}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762", "abs": "https://arxiv.org/abs/2506.03762", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."}
{"id": "2506.03615", "pdf": "https://arxiv.org/pdf/2506.03615", "abs": "https://arxiv.org/abs/2506.03615", "authors": ["Sarah Alyami", "Hamzah Luqman", "Sadam Al-Azani", "Maad Alowaifeer", "Yazeed Alharbi", "Yaser Alonaizan"], "title": "Isharah: A Large-Scale Multi-Scene Dataset for Continuous Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Current benchmarks for sign language recognition (SLR) focus mainly on\nisolated SLR, while there are limited datasets for continuous SLR (CSLR), which\nrecognizes sequences of signs in a video. Additionally, existing CSLR datasets\nare collected in controlled settings, which restricts their effectiveness in\nbuilding robust real-world CSLR systems. To address these limitations, we\npresent Isharah, a large multi-scene dataset for CSLR. It is the first dataset\nof its type and size that has been collected in an unconstrained environment\nusing signers' smartphone cameras. This setup resulted in high variations of\nrecording settings, camera distances, angles, and resolutions. This variation\nhelps with developing sign language understanding models capable of handling\nthe variability and complexity of real-world scenarios. The dataset consists of\n30,000 video clips performed by 18 deaf and professional signers. Additionally,\nthe dataset is linguistically rich as it provides a gloss-level annotation for\nall dataset's videos, making it useful for developing CSLR and sign language\ntranslation (SLT) systems. This paper also introduces multiple sign language\nunderstanding benchmarks, including signer-independent and unseen-sentence\nCSLR, along with gloss-based and gloss-free SLT. The Isharah dataset is\navailable on https://snalyami.github.io/Isharah_CSLR/."}
{"id": "2506.03740", "pdf": "https://arxiv.org/pdf/2506.03740", "abs": "https://arxiv.org/abs/2506.03740", "authors": ["Jianfeng Wu", "Nannan Xu"], "title": "SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single image super-resolution is a well-known downstream task which aims to\nrestore low-resolution images into high-resolution images. At present, models\nbased on Transformers have shone brightly in the field of super-resolution due\nto their ability to capture long-term dependencies in information. However,\ncurrent methods typically compute self-attention in nonoverlapping windows to\nsave computational costs, and the standard self-attention computation only\nfocuses on its results, thereby neglecting the useful information across\nchannels and the rich spatial structural information generated in the\nintermediate process. Channel attention and spatial attention have,\nrespectively, brought significant improvements to various downstream visual\ntasks in terms of extracting feature dependency and spatial structure\nrelationships, but the synergistic relationship between channel and spatial\nattention has not been fully explored yet.To address these issues, we propose a\nnovel model. Synergistic Alternating Aggregation Transformer (SAAT), which can\nbetter utilize the potential information of features. In SAAT, we introduce the\nEfficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial\n& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines\nefficient channel attention with shifted window attention, enhancing non-local\nfeature fusion, and producing more visually appealing results. On the other\nhand, SWSAG leverages spatial attention to capture rich structured feature\ninformation, thereby enabling SAAT to more effectively extract structural\nfeatures.Extensive experimental results and ablation studies demonstrate the\neffectiveness of SAAT in the field of super-resolution. SAAT achieves\nperformance comparable to that of the state-of-the-art (SOTA) under the same\nquantity of parameters."}
{"id": "2506.03784", "pdf": "https://arxiv.org/pdf/2506.03784", "abs": "https://arxiv.org/abs/2506.03784", "authors": ["Beatrix M. G. Nielsen", "Emanuele Marconato", "Andrea Dittadi", "Luigi Gresele"], "title": "When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When and why representations learned by different deep neural networks are\nsimilar is an active research topic. We choose to address these questions from\nthe perspective of identifiability theory, which suggests that a measure of\nrepresentational similarity should be invariant to transformations that leave\nthe model distribution unchanged. Focusing on a model family which includes\nseveral popular pre-training approaches, e.g., autoregressive language models,\nwe explore when models which generate distributions that are close have similar\nrepresentations. We prove that a small Kullback-Leibler divergence between the\nmodel distributions does not guarantee that the corresponding representations\nare similar. This has the important corollary that models arbitrarily close to\nmaximizing the likelihood can still learn dissimilar representations, a\nphenomenon mirrored in our empirical observations on models trained on\nCIFAR-10. We then define a distributional distance for which closeness implies\nrepresentational similarity, and in synthetic experiments, we find that wider\nnetworks learn distributions which are closer with respect to our distance and\nhave more similar representations. Our results establish a link between\ncloseness in distribution and representational similarity."}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763", "abs": "https://arxiv.org/abs/2506.03763", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach."}
{"id": "2506.03635", "pdf": "https://arxiv.org/pdf/2506.03635", "abs": "https://arxiv.org/abs/2506.03635", "authors": ["Yinfan Wang", "Jie Gui", "Baosheng Yu", "Qi Li", "Zhenan Sun", "Juho Kannala", "Guoying Zhao"], "title": "FingerVeinSyn-5M: A Million-Scale Dataset and Benchmark for Finger Vein Recognition", "categories": ["cs.CV"], "comment": null, "summary": "A major challenge in finger vein recognition is the lack of large-scale\npublic datasets. Existing datasets contain few identities and limited samples\nper finger, restricting the advancement of deep learning-based methods. To\naddress this, we introduce FVeinSyn, a synthetic generator capable of producing\ndiverse finger vein patterns with rich intra-class variations. Using FVeinSyn,\nwe created FingerVeinSyn-5M -- the largest available finger vein dataset --\ncontaining 5 million samples from 50,000 unique fingers, each with 100\nvariations including shift, rotation, scale, roll, varying exposure levels,\nskin scattering blur, optical blur, and motion blur. FingerVeinSyn-5M is also\nthe first to offer fully annotated finger vein images, supporting deep learning\napplications in this field. Models pretrained on FingerVeinSyn-5M and\nfine-tuned with minimal real data achieve an average 53.91\\% performance gain\nacross multiple benchmarks. The dataset is publicly available at:\nhttps://github.com/EvanWang98/FingerVeinSyn-5M."}
{"id": "2506.03755", "pdf": "https://arxiv.org/pdf/2506.03755", "abs": "https://arxiv.org/abs/2506.03755", "authors": ["Max Hellrigel-Holderbaum", "Leonard Dung"], "title": "Misalignment or misuse? The AGI alignment tradeoff", "categories": ["cs.CY", "cs.AI"], "comment": "Forthcoming in Philosophical Studies", "summary": "Creating systems that are aligned with our goals is seen as a leading\napproach to create safe and beneficial AI in both leading AI companies and the\nacademic field of AI safety. We defend the view that misaligned AGI - future,\ngenerally intelligent (robotic) AI agents - poses catastrophic risks. At the\nsame time, we support the view that aligned AGI creates a substantial risk of\ncatastrophic misuse by humans. While both risks are severe and stand in tension\nwith one another, we show that - in principle - there is room for alignment\napproaches which do not increase misuse risk. We then investigate how the\ntradeoff between misalignment and misuse looks empirically for different\ntechnical approaches to AI alignment. Here, we argue that many current\nalignment techniques and foreseeable improvements thereof plausibly increase\nrisks of catastrophic misuse. Since the impacts of AI depend on the social\ncontext, we close by discussing important social factors and suggest that to\nreduce the risk of a misuse catastrophe due to aligned AGI, techniques such as\nrobustness, AI control methods and especially good governance seem essential."}
{"id": "2506.03790", "pdf": "https://arxiv.org/pdf/2506.03790", "abs": "https://arxiv.org/abs/2506.03790", "authors": ["Peng Wang", "Yifu Lu", "Yaodong Yu", "Druv Pai", "Qing Qu", "Yi Ma"], "title": "Attention-Only Transformers via Unrolled Subspace Denoising", "categories": ["cs.LG"], "comment": "28 pages, 7 figures, 5 tables", "summary": "Despite the popularity of transformers in practice, their architectures are\nempirically designed and neither mathematically justified nor interpretable.\nMoreover, as indicated by many empirical studies, some components of\ntransformer architectures may be redundant. To derive a fully interpretable\ntransformer architecture with only necessary components, we contend that the\ngoal of representation learning is to compress a set of noisy initial token\nrepresentations towards a mixture of low-dimensional subspaces. To compress\nthese noisy token representations, an associated denoising operation naturally\ntakes the form of a multi-head (subspace) self-attention. By unrolling such\niterative denoising operations into a deep network, we arrive at a highly\ncompact architecture that consists of \\textit{only} self-attention operators\nwith skip connections at each layer. Moreover, we show that each layer performs\nhighly efficient denoising: it improves the signal-to-noise ratio of token\nrepresentations \\textit{at a linear rate} with respect to the number of layers.\nDespite its simplicity, extensive experiments on vision and language tasks\ndemonstrate that such a transformer achieves performance close to that of\nstandard transformer architectures such as GPT-2 and CRATE."}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781", "abs": "https://arxiv.org/abs/2506.03781", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark."}
{"id": "2506.03643", "pdf": "https://arxiv.org/pdf/2506.03643", "abs": "https://arxiv.org/abs/2506.03643", "authors": ["Lingjun Mao", "Rodolfo Corona", "Xin Liang", "Wenhao Yan", "Zineng Tang"], "title": "Images are Worth Variable Length of Representations", "categories": ["cs.CV"], "comment": null, "summary": "Most existing vision encoders map images into a fixed-length sequence of\ntokens, overlooking the fact that different images contain varying amounts of\ninformation. For example, a visually complex image (e.g., a cluttered room)\ninherently carries more information and thus deserves more tokens than a simple\nimage (e.g., a blank wall). To address this inefficiency, we propose DOVE, a\ndynamic vision encoder that produces a variable number of visual tokens (i.e.,\ncontinuous representation vectors) to reconstruct each image. Our results show\nthat DOVE significantly reduces the average number of tokens while maintaining\nhigh reconstruction quality. In several linear probing and downstream\nmultimodal tasks, it outperforms existing autoencoder-based tokenization\nmethods when using far fewer tokens, capturing more expressive semantic\nfeatures compared to fixed-length encoding. We further extend DOVE with\nquery-conditioned tokenization. By guiding the model to focus on query-relevant\nregions, it achieves more efficient and targeted semantic extraction. Our code\nand checkpoints are available at https://dove-encoder.github.io/dove-encoder."}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring."}
{"id": "2506.03802", "pdf": "https://arxiv.org/pdf/2506.03802", "abs": "https://arxiv.org/abs/2506.03802", "authors": ["Andreas Athanasopoulos", "Christos Dimitrakakis"], "title": "Learning Equilibria in Matching Games with Bandit Feedback", "categories": ["cs.LG"], "comment": "21 pages, 2 figures", "summary": "We investigate the problem of learning an equilibrium in a generalized\ntwo-sided matching market, where agents can adaptively choose their actions\nbased on their assigned matches. Specifically, we consider a setting in which\nmatched agents engage in a zero-sum game with initially unknown payoff\nmatrices, and we explore whether a centralized procedure can learn an\nequilibrium from bandit feedback. We adopt the solution concept of matching\nequilibrium, where a pair consisting of a matching $\\mathfrak{m}$ and a set of\nagent strategies $X$ forms an equilibrium if no agent has the incentive to\ndeviate from $(\\mathfrak{m}, X)$. To measure the deviation of a given pair\n$(\\mathfrak{m}, X)$ from the equilibrium pair $(\\mathfrak{m}^\\star, X^\\star)$,\nwe introduce matching instability that can serve as a regret measure for the\ncorresponding learning problem. We then propose a UCB algorithm in which agents\nform preferences and select actions based on optimistic estimates of the game\npayoffs, and prove that it achieves sublinear, instance-independent regret over\na time horizon $T$."}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793", "abs": "https://arxiv.org/abs/2506.03793", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale."}
{"id": "2506.03652", "pdf": "https://arxiv.org/pdf/2506.03652", "abs": "https://arxiv.org/abs/2506.03652", "authors": ["Cheng Zhang", "Hongxia xie", "Bin Wen", "Songhan Zuo", "Ruoxuan Zhang", "Wen-huang Cheng"], "title": "EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of diffusion models, text-to-image generation has\nachieved significant progress in image resolution, detail fidelity, and\nsemantic alignment, particularly with models like Stable Diffusion 3.5, Stable\nDiffusion XL, and FLUX 1. However, generating emotionally expressive and\nabstract artistic images remains a major challenge, largely due to the lack of\nlarge-scale, fine-grained emotional datasets. To address this gap, we present\nthe EmoArt Dataset -- one of the most comprehensive emotion-annotated art\ndatasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,\nImpressionism, Expressionism, Abstract Art), offering rich stylistic and\ncultural diversity. Each image includes structured annotations: objective scene\ndescriptions, five key visual attributes (brushwork, composition, color, line,\nlight), binary arousal-valence labels, twelve emotion categories, and potential\nart therapy effects. Using EmoArt, we systematically evaluate popular\ntext-to-image diffusion models for their ability to generate emotionally\naligned images from text. Our work provides essential data and benchmarks for\nemotion-driven image synthesis and aims to advance fields such as affective\ncomputing, multimodal learning, and computational art, enabling applications in\nart therapy and creative design. The dataset and more details can be accessed\nvia our project website."}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness."}
{"id": "2506.03813", "pdf": "https://arxiv.org/pdf/2506.03813", "abs": "https://arxiv.org/abs/2506.03813", "authors": ["Lili Chen", "Changyang She", "Jingge Zhu", "Jamie Evans"], "title": "Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "As the number of mobile devices continues to grow, interference has become a\nmajor bottleneck in improving data rates in wireless networks. Efficient joint\nchannel and power allocation (JCPA) is crucial for managing interference. In\nthis paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the\nJCPA problem in multi-channel wireless networks. To reduce the computational\ncomplexity of iterative optimization, we further introduce JCPGNN-M, a graph\nneural network-based solution that enables simultaneous multi-channel\nallocation for each user. We reformulate the problem as a Lagrangian function,\nwhich allows us to enforce the total power constraints systematically. Our\nsolution involves combining this Lagrangian framework with GNNs and iteratively\nupdating the Lagrange multipliers and resource allocation scheme. Unlike\nexisting GNN-based methods that limit each user to a single channel, JCPGNN-M\nsupports efficient spectrum reuse and scales well in dense network scenarios.\nSimulation results show that JCPGNN-M achieves better data rate compared to\neWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and\nit can generalize well to larger networks."}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820", "abs": "https://arxiv.org/abs/2506.03820", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages."}
{"id": "2506.03660", "pdf": "https://arxiv.org/pdf/2506.03660", "abs": "https://arxiv.org/abs/2506.03660", "authors": ["Wei Luo", "Haiming Yao", "Yunkang Cao", "Qiyu Chen", "Ang Gao", "Weiming Shen", "Weihang Zhang", "Wenyong Yu"], "title": "INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning", "categories": ["cs.CV"], "comment": "15 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) is essential for industrial inspection and medical\ndiagnosis, yet existing methods typically rely on ``comparing'' test images to\nnormal references from a training set. However, variations in appearance and\npositioning often complicate the alignment of these references with the test\nimage, limiting detection accuracy. We observe that most anomalies manifest as\nlocal variations, meaning that even within anomalous images, valuable normal\ninformation remains. We argue that this information is useful and may be more\naligned with the anomalies since both the anomalies and the normal information\noriginate from the same image. Therefore, rather than relying on external\nnormality from the training set, we propose INP-Former, a novel method that\nextracts Intrinsic Normal Prototypes (INPs) directly from the test image.\nSpecifically, we introduce the INP Extractor, which linearly combines normal\ntokens to represent INPs. We further propose an INP Coherence Loss to ensure\nINPs can faithfully represent normality for the testing image. These INPs then\nguide the INP-guided Decoder to reconstruct only normal tokens, with\nreconstruction errors serving as anomaly scores. Additionally, we propose a\nSoft Mining Loss to prioritize hard-to-optimize samples during training.\nINP-Former achieves state-of-the-art performance in single-class, multi-class,\nand few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a\nversatile and universal solution for AD. Remarkably, INP-Former also\ndemonstrates some zero-shot AD capability. Furthermore, we propose a soft\nversion of the INP Coherence Loss and enhance INP-Former by incorporating\nresidual learning, leading to the development of INP-Former++. The proposed\nmethod significantly improves detection performance across single-class,\nmulti-class, semi-supervised, few-shot, and zero-shot settings."}
{"id": "2506.03837", "pdf": "https://arxiv.org/pdf/2506.03837", "abs": "https://arxiv.org/abs/2506.03837", "authors": ["Xiao-Qi Han", "Ze-Feng Gao", "Xin-De Wang", "Zhenfeng Ouyang", "Peng-Jie Guo", "Zhong-Yi Lu"], "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction", "categories": ["cond-mat.supr-con", "cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": "7 pages, 2 figures", "summary": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like\nBCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB$_2$. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods."}
{"id": "2506.03817", "pdf": "https://arxiv.org/pdf/2506.03817", "abs": "https://arxiv.org/abs/2506.03817", "authors": ["Julius Gonsior", "Tim Rieß", "Anja Reusch", "Claudio Hartmann", "Maik Thiele", "Wolfgang Lehner"], "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid", "categories": ["cs.LG"], "comment": null, "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future."}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc."}
{"id": "2506.03662", "pdf": "https://arxiv.org/pdf/2506.03662", "abs": "https://arxiv.org/abs/2506.03662", "authors": ["Erhang Zhang", "Junyi Ma", "Yin-Dong Zheng", "Yixuan Zhou", "Hesheng Wang"], "title": "Zero-Shot Temporal Interaction Localization for Egocentric Videos", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc."}
{"id": "2506.03872", "pdf": "https://arxiv.org/pdf/2506.03872", "abs": "https://arxiv.org/abs/2506.03872", "authors": ["Yang Xiao", "Guoan Xu", "Qiang Wu", "Wenjing Jia"], "title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge\nwith wide applications. Recent advances in feed-forward 3D Gaussian sparse-view\nreconstruction methods provide an efficient solution for real-time novel view\nsynthesis by leveraging geometric priors learned from large-scale multi-view\ndatasets and computing 3D Gaussian centers via back-projection. Despite\noffering strong geometric cues, both feed-forward multi-view depth estimation\nand flow-depth joint estimation face key limitations: the former suffers from\nmislocation and artifact issues in low-texture or repetitive regions, while the\nlatter is prone to local noise and global inconsistency due to unreliable\nmatches when ground-truth flow supervision is unavailable. To overcome this, we\npropose JointSplat, a unified framework that leverages the complementarity\nbetween optical flow and depth via a novel probabilistic optimization\nmechanism. Specifically, this pixel-level mechanism scales the information\nfusion between depth and flow based on the matching probability of optical flow\nduring training. Building upon the above mechanism, we further propose a novel\nmulti-view depth-consistency loss to leverage the reliability of supervision\nwhile suppressing misleading gradients in uncertain areas. Evaluated on\nRealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art\n(SOTA) methods, demonstrating the effectiveness and robustness of our proposed\nprobabilistic joint flow-depth optimization approach for high-fidelity\nsparse-view 3D reconstruction."}
{"id": "2506.03835", "pdf": "https://arxiv.org/pdf/2506.03835", "abs": "https://arxiv.org/abs/2506.03835", "authors": ["Jianyuan Yin", "Qianxiao Li"], "title": "Learning task-specific predictive models for scientific computing", "categories": ["cs.LG"], "comment": null, "summary": "We consider learning a predictive model to be subsequently used for a given\ndownstream task (described by an algorithm) that requires access to the model\nevaluation. This task need not be prediction, and this situation is frequently\nencountered in machine-learning-augmented scientific computing. We show that\nthis setting differs from classical supervised learning, and in general it\ncannot be solved by minimizing the mean square error of the model predictions\nas is frequently performed in the literature. Instead, we find that the maximum\nprediction error on the support of the downstream task algorithm can serve as\nan effective estimate for the subsequent task performance. With this insight,\nwe formulate a task-specific supervised learning problem based on the given\nsampling measure, whose solution serves as a reliable surrogate model for the\ndownstream task. Then, we discretize the empirical risk based on training data,\nand develop an iterative algorithm to solve the task-specific supervised\nlearning problem. Three illustrative numerical examples on trajectory\nprediction, optimal control and minimum energy path computation demonstrate the\neffectiveness of the approach."}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861", "abs": "https://arxiv.org/abs/2506.03861", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia."}
{"id": "2506.03664", "pdf": "https://arxiv.org/pdf/2506.03664", "abs": "https://arxiv.org/abs/2506.03664", "authors": ["Valerie Krug", "Sebastian Stober"], "title": "Intersectional Bias in Pre-Trained Image Recognition Models", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG"], "comment": "Summary paper accepted at the 3rd TRR 318 Conference: Contextualizing\n  Explanations 2025", "summary": "Deep Learning models have achieved remarkable success. Training them is often\naccelerated by building on top of pre-trained models which poses the risk of\nperpetuating encoded biases. Here, we investigate biases in the representations\nof commonly used ImageNet classifiers for facial images while considering\nintersections of sensitive variables age, race and gender. To assess the\nbiases, we use linear classifier probes and visualize activations as\ntopographic maps. We find that representations in ImageNet classifiers\nparticularly allow differentiation between ages. Less strongly pronounced, the\nmodels appear to associate certain ethnicities and distinguish genders in\nmiddle-aged groups."}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880", "abs": "https://arxiv.org/abs/2506.03880", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential."}
{"id": "2506.03839", "pdf": "https://arxiv.org/pdf/2506.03839", "abs": "https://arxiv.org/abs/2506.03839", "authors": ["Tobias Pielok", "Bernd Bischl", "David Rügamer"], "title": "Revisiting Unbiased Implicit Variational Inference", "categories": ["cs.LG", "stat.ML", "62F15, 68T07", "I.2.6; G.3"], "comment": "Accepted to ICML 2025", "summary": "Recent years have witnessed growing interest in semi-implicit variational\ninference (SIVI) methods due to their ability to rapidly generate samples from\ncomplex distributions. However, since the likelihood of these samples is\nnon-trivial to estimate in high dimensions, current research focuses on finding\neffective SIVI training routines. Although unbiased implicit variational\ninference (UIVI) has largely been dismissed as imprecise and computationally\nprohibitive because of its inner MCMC loop, we revisit this method and show\nthat UIVI's MCMC loop can be effectively replaced via importance sampling and\nthe optimal proposal distribution can be learned stably by minimizing an\nexpected forward Kullback-Leibler divergence without bias. Our refined approach\ndemonstrates superior performance or parity with state-of-the-art methods on\nestablished SIVI benchmarks."}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867", "abs": "https://arxiv.org/abs/2506.03867", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages."}
{"id": "2506.03675", "pdf": "https://arxiv.org/pdf/2506.03675", "abs": "https://arxiv.org/abs/2506.03675", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Utilizing multi-modal data enhances scene understanding by providing\ncomplementary semantic and geometric information. Existing methods fuse\nfeatures or distill knowledge from multiple modalities into a unified\nrepresentation, improving robustness but restricting each modality's ability to\nfully leverage its strengths in different situations. We reformulate\nmulti-modal semantic segmentation as a mask-level classification task and\npropose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross\nModality Alignment (CMA) to maximize modality effectiveness and handle missing\nmodalities. Specifically, BiXFormer first categorizes multi-modal inputs into\nRGB and X, where X represents any non-RGB modalities, e.g., depth, allowing\nseparate processing for each. This design leverages the well-established\npretraining for RGB, while addressing the relative lack of attention to X\nmodalities. Then, we propose UMM, which includes Modality Agnostic Matching\n(MAM) and Complementary Matching (CM). MAM assigns labels to features from all\nmodalities without considering modality differences, leveraging each modality's\nstrengths. CM then reassigns unmatched labels to remaining unassigned features\nwithin their respective modalities, ensuring that each available modality\ncontributes to the final prediction and mitigating the impact of missing\nmodalities. Moreover, to further facilitate UMM, we introduce CMA, which\nenhances the weaker queries assigned in CM by aligning them with optimally\nmatched queries from MAM. Experiments on both synthetic and real-world\nmulti-modal benchmarks demonstrate the effectiveness of our method, achieving\nsignificant improvements in mIoU of +2.75% and +22.74% over the prior arts."}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation."}
{"id": "2506.03850", "pdf": "https://arxiv.org/pdf/2506.03850", "abs": "https://arxiv.org/abs/2506.03850", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines."}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages."}
{"id": "2506.03683", "pdf": "https://arxiv.org/pdf/2506.03683", "abs": "https://arxiv.org/abs/2506.03683", "authors": ["Qiang Fu", "Zonglei Jing", "Zonghao Ying", "Xiaoqian Li"], "title": "PRJ: Perception-Retrieval-Judgement for Generated Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of generative AI has enabled remarkable creative\ncapabilities, yet it also raises urgent concerns regarding the safety of\nAI-generated visual content in real-world applications such as content\nmoderation, platform governance, and digital media regulation. This includes\nunsafe material such as sexually explicit images, violent scenes, hate symbols,\npropaganda, and unauthorized imitations of copyrighted artworks. Existing image\nsafety systems often rely on rigid category filters and produce binary outputs,\nlacking the capacity to interpret context or reason about nuanced,\nadversarially induced forms of harm. In addition, standard evaluation metrics\n(e.g., attack success rate) fail to capture the semantic severity and dynamic\nprogression of toxicity. To address these limitations, we propose\nPerception-Retrieval-Judgement (PRJ), a cognitively inspired framework that\nmodels toxicity detection as a structured reasoning process. PRJ follows a\nthree-stage design: it first transforms an image into descriptive language\n(perception), then retrieves external knowledge related to harm categories and\ntraits (retrieval), and finally evaluates toxicity based on legal or normative\nrules (judgement). This language-centric structure enables the system to detect\nboth explicit and implicit harms with improved interpretability and categorical\ngranularity. In addition, we introduce a dynamic scoring mechanism based on a\ncontextual toxicity risk matrix to quantify harmfulness across different\nsemantic dimensions. Experiments show that PRJ surpasses existing safety\ncheckers in detection accuracy and robustness while uniquely supporting\nstructured category-level toxicity interpretation."}
{"id": "2506.03933", "pdf": "https://arxiv.org/pdf/2506.03933", "abs": "https://arxiv.org/abs/2506.03933", "authors": ["Jia Fu", "Yongtao Wu", "Yihang Chen", "Kunyu Peng", "Xiao Zhang", "Volkan Cevher", "Sepideh Pashami", "Anders Holst"], "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments."}
{"id": "2506.03857", "pdf": "https://arxiv.org/pdf/2506.03857", "abs": "https://arxiv.org/abs/2506.03857", "authors": ["Mingxuan Xia", "Haobo Wang", "Yixuan Li", "Zewei Yu", "Jindong Wang", "Junbo Zhao", "Runze Wu"], "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 (Main conference)", "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist."}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887", "abs": "https://arxiv.org/abs/2506.03887", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm."}
{"id": "2506.03684", "pdf": "https://arxiv.org/pdf/2506.03684", "abs": "https://arxiv.org/abs/2506.03684", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "title": "DSSAU-Net:U-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation", "categories": ["cs.CV"], "comment": "14 pages, 3 figures, 5 tables.Accepted by MICCAI Workshop on IUGC\n  2024", "summary": "In the childbirth process, traditional methods involve invasive vaginal\nexaminations, but research has shown that these methods are both subjective and\ninaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way\nto assess fetal head position via two key parameters: Angle of Progression\n(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal\nhead (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth\ndelivery process. Therefore, accurate segmentation of FH and PS is crucial. In\nthis work, we propose a sparse self-attention network architecture with good\nperformance and high computational efficiency, named DSSAU-Net, for the\nsegmentation of FH and PS. Specifically, we stack varying numbers of Dual\nSparse Selection Attention (DSSA) blocks at each stage to form a symmetric\nU-shaped encoder-decoder network architecture. For a given query, DSSA is\ndesigned to explicitly perform one sparse token selection at both the region\nand pixel levels, respectively, which is beneficial for further reducing\ncomputational complexity while extracting the most relevant features. To\ncompensate for the information loss during the upsampling process, skip\nconnections with convolutions are designed. Additionally, multiscale feature\nfusion is employed to enrich the model's global and local information. The\nperformance of DSSAU-Net has been validated using the Intrapartum Ultrasound\nGrand Challenge (IUGC) 2024 \\textit{test set} provided by the organizer in the\nMICCAI IUGC 2024\ncompetition\\footnote{\\href{https://codalab.lisn.upsaclay.fr/competitions/18413\\#learn\\_the\\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\\#learn\\_the\\_details}},\nwhere we win the fourth place on the tasks of classification and segmentation,\ndemonstrating its effectiveness. The codes will be available at\nhttps://github.com/XiaZunhui/DSSAU-Net."}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941", "abs": "https://arxiv.org/abs/2506.03941", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session."}
{"id": "2506.03870", "pdf": "https://arxiv.org/pdf/2506.03870", "abs": "https://arxiv.org/abs/2506.03870", "authors": ["Mohd. Farhan Israk Soumik", "Syed Mhamudul Hasan", "Abdur R. Shahid"], "title": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems."}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901", "abs": "https://arxiv.org/abs/2506.03901", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing."}
{"id": "2506.03698", "pdf": "https://arxiv.org/pdf/2506.03698", "abs": "https://arxiv.org/abs/2506.03698", "authors": ["Yuanlin Mo", "Haishan Huang", "Bocheng Liang", "Weibo Ma"], "title": "Advancements in Artificial Intelligence Applications for Cardiovascular Disease Research", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in artificial intelligence (AI) have revolutionized\ncardiovascular medicine, particularly through integration with computed\ntomography (CT), magnetic resonance imaging (MRI), electrocardiography (ECG)\nand ultrasound (US). Deep learning architectures, including convolutional\nneural networks and generative adversarial networks, enable automated analysis\nof medical imaging and physiological signals, surpassing human capabilities in\ndiagnostic accuracy and workflow efficiency. However, critical challenges\npersist, including the inability to validate input data accuracy, which may\npropagate diagnostic errors. This review highlights AI's transformative\npotential in precision diagnostics while underscoring the need for robust\nvalidation protocols to ensure clinical reliability. Future directions\nemphasize hybrid models integrating multimodal data and adaptive algorithms to\nrefine personalized cardiovascular care."}
{"id": "2506.03954", "pdf": "https://arxiv.org/pdf/2506.03954", "abs": "https://arxiv.org/abs/2506.03954", "authors": ["Jianqing Zhang", "Xinghao Wu", "Yanbing Zhou", "Xiaoting Sun", "Qiqi Cai", "Yang Liu", "Yang Hua", "Zhenzhe Zheng", "Jian Cao", "Qiang Yang"], "title": "HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted by KDD2025", "summary": "As AI evolves, collaboration among heterogeneous models helps overcome data\nscarcity by enabling knowledge transfer across institutions and devices.\nTraditional Federated Learning (FL) only supports homogeneous models, limiting\ncollaboration among clients with heterogeneous model architectures. To address\nthis, Heterogeneous Federated Learning (HtFL) methods are developed to enable\ncollaboration across diverse heterogeneous models while tackling the data\nheterogeneity issue at the same time. However, a comprehensive benchmark for\nstandardized evaluation and analysis of the rapidly growing HtFL methods is\nlacking. Firstly, the highly varied datasets, model heterogeneity scenarios,\nand different method implementations become hurdles to making easy and fair\ncomparisons among HtFL methods. Secondly, the effectiveness and robustness of\nHtFL methods are under-explored in various scenarios, such as the medical\ndomain and sensor signal modality. To fill this gap, we introduce the first\nHeterogeneous Federated Learning Library (HtFLlib), an easy-to-use and\nextensible framework that integrates multiple datasets and model heterogeneity\nscenarios, offering a robust benchmark for research and practical applications.\nSpecifically, HtFLlib integrates (1) 12 datasets spanning various domains,\nmodalities, and data heterogeneity scenarios; (2) 40 model architectures,\nranging from small to large, across three modalities; (3) a modularized and\neasy-to-extend HtFL codebase with implementations of 10 representative HtFL\nmethods; and (4) systematic evaluations in terms of accuracy, convergence,\ncomputation costs, and communication costs. We emphasize the advantages and\npotential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze\nadvancing HtFL research and enable its broader applications. The code is\nreleased at https://github.com/TsingZ0/HtFLlib."}
{"id": "2506.03889", "pdf": "https://arxiv.org/pdf/2506.03889", "abs": "https://arxiv.org/abs/2506.03889", "authors": ["Pau Vilimelis Aceituno", "Jack William Miller", "Noah Marti", "Youssef Farag", "Victor Boussange"], "title": "Temporal horizons in forecasting: a performance-learnability trade-off", "categories": ["cs.LG", "nlin.CD"], "comment": "33 pages, 12 figures", "summary": "When training autoregressive models for dynamical systems, a critical\nquestion arises: how far into the future should the model be trained to\npredict? Too short a horizon may miss long-term trends, while too long a\nhorizon can impede convergence due to accumulating prediction errors. In this\nwork, we formalize this trade-off by analyzing how the geometry of the loss\nlandscape depends on the training horizon. We prove that for chaotic systems,\nthe loss landscape's roughness grows exponentially with the training horizon,\nwhile for limit cycles, it grows linearly, making long-horizon training\ninherently challenging. However, we also show that models trained on long\nhorizons generalize well to short-term forecasts, whereas those trained on\nshort horizons suffer exponentially (resp. linearly) worse long-term\npredictions in chaotic (resp. periodic) systems. We validate our theory through\nnumerical experiments and discuss practical implications for selecting training\nhorizons. Our results provide a principled foundation for hyperparameter\noptimization in autoregressive forecasting models."}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902", "abs": "https://arxiv.org/abs/2506.03902", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "title": "The Harmonic Structure of Information Contours", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity."}
{"id": "2506.03706", "pdf": "https://arxiv.org/pdf/2506.03706", "abs": "https://arxiv.org/abs/2506.03706", "authors": ["Aditya Gandhamal", "Aniruddh Sikdar", "Suresh Sundaram"], "title": "OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop on Transformers for Vision\n  (Non-archival track)", "summary": "Open-vocabulary semantic segmentation (OVSS) entails assigning semantic\nlabels to each pixel in an image using textual descriptions, typically\nleveraging world models such as CLIP. To enhance out-of-domain generalization,\nwe propose Cost Aggregation with Optimal Transport (OV-COAST) for\nopen-vocabulary semantic segmentation. To align visual-language features within\nthe framework of optimal transport theory, we employ cost volume to construct a\ncost matrix, which quantifies the distance between two distributions. Our\napproach adopts a two-stage optimization strategy: in the first stage, the\noptimal transport problem is solved using cost volume via Sinkhorn distance to\nobtain an alignment solution; in the second stage, this solution is used to\nguide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS\nmodels on the MESS benchmark, where our approach notably improves the\nperformance of the cost-aggregation model CAT-Seg with ViT-B backbone,\nachieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %\nmIoU. The code is available at\nhttps://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ ."}
{"id": "2506.03964", "pdf": "https://arxiv.org/pdf/2506.03964", "abs": "https://arxiv.org/abs/2506.03964", "authors": ["HyunGi Kim", "Jisoo Mok", "Dongjun Lee", "Jaihyun Lew", "Sungjae Kim", "Sungroh Yoon"], "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS."}
{"id": "2506.03898", "pdf": "https://arxiv.org/pdf/2506.03898", "abs": "https://arxiv.org/abs/2506.03898", "authors": ["Pierre-François Massiani", "Christian Fiedler", "Lukas Haverbeck", "Friedrich Solowjow", "Sebastian Trimpe"], "title": "A kernel conditional two-sample test", "categories": ["cs.LG", "stat.ML"], "comment": "40 pages, 8 figures, 8 tables. Under review", "summary": "We propose a framework for hypothesis testing on conditional probability\ndistributions, which we then use to construct conditional two-sample\nstatistical tests. These tests identify the inputs -- called covariates in this\ncontext -- where two conditional expectations differ with high probability. Our\nkey idea is to transform confidence bounds of a learning method into a\nconditional two-sample test, and we instantiate this principle for kernel ridge\nregression (KRR) and conditional kernel mean embeddings. We generalize existing\npointwise-in-time or time-uniform confidence bounds for KRR to\npreviously-inaccessible yet essential cases such as infinite-dimensional\noutputs with non-trace-class kernels. These bounds enable circumventing the\nneed for independent data in our statistical tests, since they allow online\nsampling. We also introduce bootstrapping schemes leveraging the parametric\nform of testing thresholds identified in theory to avoid tuning inaccessible\nparameters, making our method readily applicable in practice. Such conditional\ntwo-sample tests are especially relevant in applications where data arrive\nsequentially or non-independently, or when output distributions vary with\noperational parameters. We demonstrate their utility through examples in\nprocess monitoring and comparison of dynamical systems. Overall, our results\nestablish a comprehensive foundation for conditional two-sample testing, from\ntheoretical guarantees to practical implementation, and advance the\nstate-of-the-art on the concentration of vector-valued least squares\nestimation."}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913", "abs": "https://arxiv.org/abs/2506.03913", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext."}
{"id": "2506.03709", "pdf": "https://arxiv.org/pdf/2506.03709", "abs": "https://arxiv.org/abs/2506.03709", "authors": ["Aniruddh Sikdar", "Aditya Gandhamal", "Suresh Sundaram"], "title": "AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives", "categories": ["cs.CV"], "comment": "Accepted at Workshop on Foundation Models Meet Embodied Agents at\n  CVPR 2025 (Non-archival Track)", "summary": "Open-vocabulary semantic segmentation (OVSS) involves assigning labels to\neach pixel in an image based on textual descriptions, leveraging world models\nlike CLIP. However, they encounter significant challenges in cross-domain\ngeneralization, hindering their practical efficacy in real-world applications.\nEmbodied AI systems are transforming autonomous navigation for ground vehicles\nand drones by enhancing their perception abilities, and in this study, we\npresent AetherVision-Bench, a benchmark for multi-angle segmentation across\naerial, and ground perspectives, which facilitates an extensive evaluation of\nperformance across different viewing angles and sensor modalities. We assess\nstate-of-the-art OVSS models on the proposed benchmark and investigate the key\nfactors that impact the performance of zero-shot transfer models. Our work\npioneers the creation of a robustness benchmark, offering valuable insights and\nestablishing a foundation for future research."}
{"id": "2506.04001", "pdf": "https://arxiv.org/pdf/2506.04001", "abs": "https://arxiv.org/abs/2506.04001", "authors": ["Han Ji", "Yuqi Feng", "Jiahao Fan", "Yanan Sun"], "title": "CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Performance predictors have emerged as a promising method to accelerate the\nevaluation stage of neural architecture search (NAS). These predictors estimate\nthe performance of unseen architectures by learning from the correlation\nbetween a small set of trained architectures and their performance. However,\nmost existing predictors ignore the inherent distribution shift between limited\ntraining samples and diverse test samples. Hence, they tend to learn spurious\ncorrelations as shortcuts to predictions, leading to poor generalization. To\naddress this, we propose a Causality-guided Architecture Representation\nLearning (CARL) method aiming to separate critical (causal) and redundant\n(non-causal) features of architectures for generalizable architecture\nperformance prediction. Specifically, we employ a substructure extractor to\nsplit the input architecture into critical and redundant substructures in the\nlatent space. Then, we generate multiple interventional samples by pairing\ncritical representations with diverse redundant representations to prioritize\ncritical features. Extensive experiments on five NAS search spaces demonstrate\nthe state-of-the-art accuracy and superior interpretability of CARL. For\ninstance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS."}
{"id": "2506.03910", "pdf": "https://arxiv.org/pdf/2506.03910", "abs": "https://arxiv.org/abs/2506.03910", "authors": ["Shyam Prabhu", "P Akshay Kumar", "Antov Selwinston", "Pavan Taduvai", "Shreya Bairi", "Rohit Batra"], "title": "Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods", "categories": ["cs.LG"], "comment": "7 pages, 3 figures", "summary": "Materials design problems often require optimizing multiple variables,\nrendering full factorial exploration impractical. Design of experiment (DOE)\nmethods, such as Taguchi technique, are commonly used to efficiently sample the\ndesign space but they inherently lack the ability to capture non-linear\ndependency of process variables. In this work, we demonstrate how machine\nlearning (ML) methods can be used to overcome these limitations. We compare the\nperformance of Taguchi method against an active learning based Gaussian process\nregression (GPR) model in a wire arc additive manufacturing (WAAM) process to\naccurately predict aspects of bead geometry, including penetration depth, bead\nwidth, and height. While Taguchi method utilized a three-factor, five-level L25\northogonal array to suggest weld parameters, the GPR model used an\nuncertainty-based exploration acquisition function coupled with latin hypercube\nsampling for initial training data. Accuracy and efficiency of both models was\nevaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.\nThis work applies to broader materials processing domain requiring efficient\nexploration of complex parameters."}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Björn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD."}
{"id": "2506.03713", "pdf": "https://arxiv.org/pdf/2506.03713", "abs": "https://arxiv.org/abs/2506.03713", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "PlückeRF: A Line-based 3D Representation for Few-view Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D reconstruction methods aim to predict the 3D structure of a\nscene directly from input images, providing a faster alternative to per-scene\noptimization approaches. Significant progress has been made in single-view and\nfew-view reconstruction using learned priors that infer object shape and\nappearance, even for unobserved regions. However, there is substantial\npotential to enhance these methods by better leveraging information from\nmultiple views when available. To address this, we propose a few-view\nreconstruction model that more effectively harnesses multi-view information.\nOur approach introduces a simple mechanism that connects the 3D representation\nwith pixel rays from the input views, allowing for preferential sharing of\ninformation between nearby 3D locations and between 3D locations and nearby\npixel rays. We achieve this by defining the 3D representation as a set of\nstructured, feature-augmented lines; the Pl\\\"uckeRF representation. Using this\nrepresentation, we demonstrate improvements in reconstruction quality over the\nequivalent triplane representation and state-of-the-art feedforward\nreconstruction methods."}
{"id": "2506.04006", "pdf": "https://arxiv.org/pdf/2506.04006", "abs": "https://arxiv.org/abs/2506.04006", "authors": ["Fernando de Meer Pardo", "Branka Hadji Misheva", "Martin Braschler", "Kurt Stockinger"], "title": "TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "We present TransClean, a method for detecting false positive predictions of\nentity matching algorithms under real-world conditions characterized by\nlarge-scale, noisy, and unlabeled multi-source datasets that undergo\ndistributional shifts. TransClean is explicitly designed to operate with\nmultiple data sources in an efficient, robust and fast manner while accounting\nfor edge cases and requiring limited manual labeling. TransClean leverages the\nTransitive Consistency of a matching, a measure of the consistency of a\npairwise matching model f_theta on the matching it produces G_f_theta, based\nboth on its predictions on directly evaluated record pairs and its predictions\non implied record pairs. TransClean iteratively modifies a matching through\ngradually removing false positive matches while removing as few true positive\nmatches as possible. In each of these steps, the estimation of the Transitive\nConsistency is exclusively done through model evaluations and produces\nquantities that can be used as proxies of the amounts of true and false\npositives in the matching while not requiring any manual labeling, producing an\nestimate of the quality of the matching and indicating which record groups are\nlikely to contain false positives. In our experiments, we compare combining\nTransClean with a naively trained pairwise matching model (DistilBERT) and with\na state-of-the-art end-to-end matching method (CLER) and illustrate the\nflexibility of TransClean in being able to detect most of the false positives\nof either setup across a variety of datasets. Our experiments show that\nTransClean induces an average +24.42 F1 score improvement for entity matching\nin a multi-source setting when compared to traditional pair-wise matching\nalgorithms."}
{"id": "2506.03911", "pdf": "https://arxiv.org/pdf/2506.03911", "abs": "https://arxiv.org/abs/2506.03911", "authors": ["Chamsi Hssaine", "Yichun Hu", "Ciara Pike-Burke"], "title": "Learning Fair And Effective Points-Based Rewards Programs", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Points-based rewards programs are a prevalent way to incentivize customer\nloyalty; in these programs, customers who make repeated purchases from a seller\naccumulate points, working toward eventual redemption of a free reward. These\nprograms have recently come under scrutiny due to accusations of unfair\npractices in their implementation. Motivated by these concerns, we study the\nproblem of fairly designing points-based rewards programs, with a focus on two\nobstacles that put fairness at odds with their effectiveness. First, due to\ncustomer heterogeneity, the seller should set different redemption thresholds\nfor different customers to generate high revenue. Second, the relationship\nbetween customer behavior and the number of accumulated points is typically\nunknown; this requires experimentation which may unfairly devalue customers'\npreviously earned points. We first show that an individually fair rewards\nprogram that uses the same redemption threshold for all customers suffers a\nloss in revenue of at most a factor of $1+\\ln 2$, compared to the optimal\npersonalized strategy that differentiates between customers. We then tackle the\nproblem of designing temporally fair learning algorithms in the presence of\ndemand uncertainty. Toward this goal, we design a learning algorithm that\nlimits the risk of point devaluation due to experimentation by only changing\nthe redemption threshold $O(\\log T)$ times, over a horizon of length $T$. This\nalgorithm achieves the optimal (up to polylogarithmic factors)\n$\\widetilde{O}(\\sqrt{T})$ regret in expectation. We then modify this algorithm\nto only ever decrease redemption thresholds, leading to improved fairness at a\ncost of only a constant factor in regret. Extensive numerical experiments show\nthe limited value of personalization in average-case settings, in addition to\ndemonstrating the strong practical performance of our proposed learning\nalgorithms."}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922", "abs": "https://arxiv.org/abs/2506.03922", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields."}
{"id": "2506.03714", "pdf": "https://arxiv.org/pdf/2506.03714", "abs": "https://arxiv.org/abs/2506.03714", "authors": ["Shuai Liu", "Mingyue Cui", "Boyang Li", "Quanmin Liang", "Tinghe Hong", "Kai Huang", "Yunxiao Shan", "Kai Huang"], "title": "FSHNet: Fully Sparse Hybrid Network for 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Fully sparse 3D detectors have recently gained significant attention due to\ntheir efficiency in long-range detection. However, sparse 3D detectors extract\nfeatures only from non-empty voxels, which impairs long-range interactions and\ncauses the center feature missing. The former weakens the feature extraction\ncapability, while the latter hinders network optimization. To address these\nchallenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet\nincorporates a proposed SlotFormer block to enhance the long-range feature\nextraction capability of existing sparse encoders. The SlotFormer divides\nsparse voxels using a slot partition approach, which, compared to traditional\nwindow partition, provides a larger receptive field. Additionally, we propose a\ndynamic sparse label assignment strategy to deeply optimize the network by\nproviding more high-quality positive samples. To further enhance performance,\nwe introduce a sparse upsampling module to refine downsampled voxels,\npreserving fine-grained details crucial for detecting small objects. Extensive\nexperiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the\neffectiveness of FSHNet. The code is available at\nhttps://github.com/Say2L/FSHNet."}
{"id": "2506.04036", "pdf": "https://arxiv.org/pdf/2506.04036", "abs": "https://arxiv.org/abs/2506.04036", "authors": ["Wei Wenying", "Zhao Kaifa", "Xue Lei", "Fan Ming"], "title": "Privacy and Security Threat for OpenAI GPTs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications."}
{"id": "2506.03914", "pdf": "https://arxiv.org/pdf/2506.03914", "abs": "https://arxiv.org/abs/2506.03914", "authors": ["Eduardo Santos Escriche", "Stefanie Jegelka"], "title": "Learning equivariant models by discovering symmetries with learnable augmentations", "categories": ["cs.LG"], "comment": null, "summary": "Recently, a trend has emerged that favors learning relevant symmetries from\ndata in geometric domains instead of designing constrained architectures. To do\nso, two popular options are (1) to modify the training protocol, e.g., with a\nspecific loss and data augmentations (soft equivariance), or (2) to ignore\nequivariance and infer it only implicitly. However, both options have\nlimitations: soft equivariance requires a priori knowledge about relevant\nsymmetries, while inferring symmetries merely via the task and larger data\nlacks interpretability. To address both limitations, we propose SEMoLA, an\nend-to-end approach that jointly (1) discovers a priori unknown symmetries in\nthe data via learnable data augmentations, and (2) softly encodes the\nrespective approximate equivariance into an arbitrary unconstrained model.\nHence, it does not need prior knowledge about symmetries, it offers\ninterpretability, and it maintains robustness to distribution shifts.\nEmpirically, we demonstrate the ability of SEMoLA to robustly discover relevant\nsymmetries while achieving high prediction accuracy across various datasets,\nencompassing multiple data modalities and underlying symmetry groups."}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs."}
{"id": "2506.03753", "pdf": "https://arxiv.org/pdf/2506.03753", "abs": "https://arxiv.org/abs/2506.03753", "authors": ["Caiyi Sun", "Yujing Sun", "Xiao Han", "Zemin Yang", "Jiawei Liu", "Xinge Zhu", "Siu Ming Yiu", "Yuexin Ma"], "title": "HUMOF: Human Motion Forecasting in Interactive Social Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Complex scenes present significant challenges for predicting human behaviour\ndue to the abundance of interaction information, such as human-human and\nhumanenvironment interactions. These factors complicate the analysis and\nunderstanding of human behaviour, thereby increasing the uncertainty in\nforecasting human motions. Existing motion prediction methods thus struggle in\nthese complex scenarios. In this paper, we propose an effective method for\nhuman motion forecasting in interactive scenes. To achieve a comprehensive\nrepresentation of interactions, we design a hierarchical interaction feature\nrepresentation so that high-level features capture the overall context of the\ninteractions, while low-level features focus on fine-grained details. Besides,\nwe propose a coarse-to-fine interaction reasoning module that leverages both\nspatial and frequency perspectives to efficiently utilize hierarchical\nfeatures, thereby enhancing the accuracy of motion predictions. Our method\nachieves state-of-the-art performance across four public datasets. Code will be\nreleased when this paper is published."}
{"id": "2506.04038", "pdf": "https://arxiv.org/pdf/2506.04038", "abs": "https://arxiv.org/abs/2506.04038", "authors": ["Sven Kirchner", "Alois C. Knoll"], "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements."}
{"id": "2506.03919", "pdf": "https://arxiv.org/pdf/2506.03919", "abs": "https://arxiv.org/abs/2506.03919", "authors": ["Lorenz Kummer", "Samir Moustafa", "Anatol Ehrlich", "Franka Bause", "Nikolaus Suess", "Wilfried N. Gansterer", "Nils M. Kriege"], "title": "Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The lottery ticket hypothesis (LTH) is well-studied for convolutional neural\nnetworks but has been validated only empirically for graph neural networks\n(GNNs), for which theoretical findings are largely lacking. In this paper, we\nidentify the expressivity of sparse subnetworks, i.e. their ability to\ndistinguish non-isomorphic graphs, as crucial for finding winning tickets that\npreserve the predictive performance. We establish conditions under which the\nexpressivity of a sparsely initialized GNN matches that of the full network,\nparticularly when compared to the Weisfeiler-Leman test, and in that context\nput forward and prove a Strong Expressive Lottery Ticket Hypothesis. We\nsubsequently show that an increased expressivity in the initialization\npotentially accelerates model convergence and improves generalization. Our\nfindings establish novel theoretical foundations for both LTH and GNN research,\nhighlighting the importance of maintaining expressivity in sparsely initialized\nGNNs. We illustrate our results using examples from drug discovery."}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."}
{"id": "2506.03798", "pdf": "https://arxiv.org/pdf/2506.03798", "abs": "https://arxiv.org/abs/2506.03798", "authors": ["Fan Shi", "Haiyang Yu", "Bin Li", "Xiangyang Xue"], "title": "CoLa: Chinese Character Decomposition with Compositional Latent Components", "categories": ["cs.CV"], "comment": null, "summary": "Humans can decompose Chinese characters into compositional components and\nrecombine them to recognize unseen characters. This reflects two cognitive\nprinciples: Compositionality, the idea that complex concepts are built on\nsimpler parts; and Learning-to-learn, the ability to learn strategies for\ndecomposing and recombining components to form new concepts. These principles\nprovide inductive biases that support efficient generalization. They are\ncritical to Chinese character recognition (CCR) in solving the zero-shot\nproblem, which results from the common long-tail distribution of Chinese\ncharacter datasets. Existing methods have made substantial progress in modeling\ncompositionality via predefined radical or stroke decomposition. However, they\noften ignore the learning-to-learn capability, limiting their ability to\ngeneralize beyond human-defined schemes. Inspired by these principles, we\npropose a deep latent variable model that learns Compositional Latent\ncomponents of Chinese characters (CoLa) without relying on human-defined\ndecomposition schemes. Recognition and matching can be performed by comparing\ncompositional latent components in the latent space, enabling zero-shot\ncharacter recognition. The experiments illustrate that CoLa outperforms\nprevious methods in both character the radical zero-shot CCR. Visualization\nindicates that the learned components can reflect the structure of characters\nin an interpretable way. Moreover, despite being trained on historical\ndocuments, CoLa can analyze components of oracle bone characters, highlighting\nits cross-dataset generalization ability."}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench."}
{"id": "2506.03931", "pdf": "https://arxiv.org/pdf/2506.03931", "abs": "https://arxiv.org/abs/2506.03931", "authors": ["Yotam Alexander", "Yonatan Slutzky", "Yuval Ran-Milo", "Nadav Cohen"], "title": "Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Conventional wisdom attributes the mysterious generalization abilities of\noverparameterized neural networks to gradient descent (and its variants). The\nrecent volume hypothesis challenges this view: it posits that these\ngeneralization abilities persist even when gradient descent is replaced by\nGuess & Check (G&C), i.e., by drawing weight settings until one that fits the\ntraining data is found. The validity of the volume hypothesis for wide and deep\nneural networks remains an open question. In this paper, we theoretically\ninvestigate this question for matrix factorization (with linear and non-linear\nactivation)--a common testbed in neural network theory. We first prove that\ngeneralization under G&C deteriorates with increasing width, establishing what\nis, to our knowledge, the first case where G&C is provably inferior to gradient\ndescent. Conversely, we prove that generalization under G&C improves with\nincreasing depth, revealing a stark contrast between wide and deep networks,\nwhich we further validate empirically. These findings suggest that even in\nsimple settings, there may not be a simple answer to the question of whether\nneural networks need gradient descent to generalize well."}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions."}
{"id": "2506.03799", "pdf": "https://arxiv.org/pdf/2506.03799", "abs": "https://arxiv.org/abs/2506.03799", "authors": ["Fei Zhang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Yanfeng Wang", "Ya Zhang"], "title": "ConText: Driving In-context Learning for Text Removal and Segmentation", "categories": ["cs.CV"], "comment": "19 pages, 9 figures, Accepted at ICML 2025", "summary": "This paper presents the first study on adapting the visual in-context\nlearning (V-ICL) paradigm to optical character recognition tasks, specifically\nfocusing on text removal and segmentation. Most existing V-ICL generalists\nemploy a reasoning-as-reconstruction approach: they turn to using a\nstraightforward image-label compositor as the prompt and query input, and then\nmasking the query label to generate the desired output. This direct prompt\nconfines the model to a challenging single-step reasoning process. To address\nthis, we propose a task-chaining compositor in the form of\nimage-removal-segmentation, providing an enhanced prompt that elicits reasoning\nwith enriched intermediates. Additionally, we introduce context-aware\naggregation, integrating the chained prompt pattern into the latent query\nrepresentation, thereby strengthening the model's in-context reasoning. We also\nconsider the issue of visual heterogeneity, which complicates the selection of\nhomogeneous demonstrations in text recognition. Accordingly, this is\neffectively addressed through a simple self-prompting strategy, preventing the\nmodel's in-context learnability from devolving into specialist-like,\ncontext-free inference. Collectively, these insights culminate in our ConText\nmodel, which achieves new state-of-the-art across both in- and out-of-domain\nbenchmarks. The code is available at https://github.com/Ferenas/ConText."}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043", "abs": "https://arxiv.org/abs/2506.04043", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness."}
{"id": "2506.03938", "pdf": "https://arxiv.org/pdf/2506.03938", "abs": "https://arxiv.org/abs/2506.03938", "authors": ["Cédric Léonard", "Dirk Stober", "Martin Schulz"], "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review", "categories": ["cs.LG", "cs.AR"], "comment": "35 pages, 3 figures, 2 tables. Submitted to ACM Computing Surveys\n  (ACM CSUR)", "summary": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA."}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978", "abs": "https://arxiv.org/abs/2506.03978", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets."}
{"id": "2506.03868", "pdf": "https://arxiv.org/pdf/2506.03868", "abs": "https://arxiv.org/abs/2506.03868", "authors": ["Zhuoyang Pan", "Boxiao Pan", "Guandao Yang", "Adam W. Harley", "Leonidas Guibas"], "title": "Animal Pose Labeling Using General-Purpose Point Trackers", "categories": ["cs.CV"], "comment": null, "summary": "Automatically estimating animal poses from videos is important for studying\nanimal behaviors. Existing methods do not perform reliably since they are\ntrained on datasets that are not comprehensive enough to capture all necessary\nanimal behaviors. However, it is very challenging to collect such datasets due\nto the large variations in animal morphology. In this paper, we propose an\nanimal pose labeling pipeline that follows a different strategy, i.e. test time\noptimization. Given a video, we fine-tune a lightweight appearance embedding\ninside a pre-trained general-purpose point tracker on a sparse set of annotated\nframes. These annotations can be obtained from human labelers or off-the-shelf\npose detectors. The fine-tuned model is then applied to the rest of the frames\nfor automatic labeling. Our method achieves state-of-the-art performance at a\nreasonable annotation cost. We believe our pipeline offers a valuable tool for\nthe automatic quantification of animal behavior. Visit our project webpage at\nhttps://zhuoyang-pan.github.io/animal-labeling."}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044", "abs": "https://arxiv.org/abs/2506.04044", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task."}
{"id": "2506.03943", "pdf": "https://arxiv.org/pdf/2506.03943", "abs": "https://arxiv.org/abs/2506.03943", "authors": ["Shiyi Yang", "Can Chen", "Didong Li"], "title": "Lower Ricci Curvature for Hypergraphs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Networks with higher-order interactions, prevalent in biological, social, and\ninformation systems, are naturally represented as hypergraphs, yet their\nstructural complexity poses fundamental challenges for geometric\ncharacterization. While curvature-based methods offer powerful insights in\ngraph analysis, existing extensions to hypergraphs suffer from critical\ntrade-offs: combinatorial approaches such as Forman-Ricci curvature capture\nonly coarse features, whereas geometric methods like Ollivier-Ricci curvature\noffer richer expressivity but demand costly optimal transport computations. To\naddress these challenges, we introduce hypergraph lower Ricci curvature (HLRC),\na novel curvature metric defined in closed form that achieves a principled\nbalance between interpretability and efficiency. Evaluated across diverse\nsynthetic and real-world hypergraph datasets, HLRC consistently reveals\nmeaningful higher-order organization, distinguishing intra- from\ninter-community hyperedges, uncovering latent semantic labels, tracking\ntemporal dynamics, and supporting robust clustering of hypergraphs based on\nglobal structure. By unifying geometric sensitivity with algorithmic\nsimplicity, HLRC provides a versatile foundation for hypergraph analytics, with\nbroad implications for tasks including node classification, anomaly detection,\nand generative modeling in complex systems."}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980", "abs": "https://arxiv.org/abs/2506.03980", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "title": "Voice Activity Projection Model with Multimodal Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders."}
{"id": "2506.03885", "pdf": "https://arxiv.org/pdf/2506.03885", "abs": "https://arxiv.org/abs/2506.03885", "authors": ["Sam Pollard", "Michael Wray"], "title": "Video, How Do Your Tokens Merge?", "categories": ["cs.CV"], "comment": "Accepted at eLVM workshop at CVPR 2025", "summary": "Video transformer models require huge amounts of compute resources due to the\nspatio-temporal scaling of the input. Tackling this, recent methods have\nproposed to drop or merge tokens for image models, whether randomly or via\nlearned methods. Merging tokens has many benefits: it can be plugged into any\nvision transformer, does not require model re-training, and it propagates\ninformation that would otherwise be dropped through the model. Before now,\nvideo token merging has not been evaluated on temporally complex datasets for\nvideo understanding. In this work, we explore training-free token merging for\nvideo to provide comprehensive experiments and find best practices across four\nvideo transformers on three datasets that exhibit coarse and fine-grained\naction recognition. Our results showcase the benefits of video token merging\nwith a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\\%$ for\nViViT). Code available at\nhttps://github.com/sjpollard/video-how-do-your-tokens-merge."}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050", "abs": "https://arxiv.org/abs/2506.04050", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "title": "Explainability-Based Token Replacement on LLM-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT."}
{"id": "2506.03951", "pdf": "https://arxiv.org/pdf/2506.03951", "abs": "https://arxiv.org/abs/2506.03951", "authors": ["Aojun Lu", "Hangjie Yuan", "Tao Feng", "Yanan Sun"], "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters."}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984", "abs": "https://arxiv.org/abs/2506.03984", "authors": ["Carolin Holtermann", "Paul Röttger", "Anne Lauscher"], "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks."}
{"id": "2506.03892", "pdf": "https://arxiv.org/pdf/2506.03892", "abs": "https://arxiv.org/abs/2506.03892", "authors": ["Giyong Choi", "HyunWook Park"], "title": "Joint Video Enhancement with Deblurring, Super-Resolution, and Frame Interpolation Network", "categories": ["cs.CV"], "comment": null, "summary": "Video quality is often severely degraded by multiple factors rather than a\nsingle factor. These low-quality videos can be restored to high-quality videos\nby sequentially performing appropriate video enhancement techniques. However,\nthe sequential approach was inefficient and sub-optimal because most video\nenhancement approaches were designed without taking into account that multiple\nfactors together degrade video quality. In this paper, we propose a new joint\nvideo enhancement method that mitigates multiple degradation factors\nsimultaneously by resolving an integrated enhancement problem. Our proposed\nnetwork, named DSFN, directly produces a high-resolution, high-frame-rate, and\nclear video from a low-resolution, low-frame-rate, and blurry video. In the\nDSFN, low-resolution and blurry input frames are enhanced by a joint deblurring\nand super-resolution (JDSR) module. Meanwhile, intermediate frames between\ninput adjacent frames are interpolated by a triple-frame-based frame\ninterpolation (TFBFI) module. The proper combination of the proposed modules of\nDSFN can achieve superior performance on the joint video enhancement task.\nExperimental results show that the proposed method outperforms other sequential\nstate-of-the-art techniques on public datasets with a smaller network size and\nfaster processing time."}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051", "abs": "https://arxiv.org/abs/2506.04051", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning."}
{"id": "2506.03956", "pdf": "https://arxiv.org/pdf/2506.03956", "abs": "https://arxiv.org/abs/2506.03956", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "title": "Adapt before Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL."}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989", "abs": "https://arxiv.org/abs/2506.03989", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve."}
{"id": "2506.03918", "pdf": "https://arxiv.org/pdf/2506.03918", "abs": "https://arxiv.org/abs/2506.03918", "authors": ["Marcin Kowalczyk", "Kamil Jeziorek", "Tomasz Kryjak"], "title": "Learning from Noise: Enhancing DNNs for Event-Based Vision through Controlled Noise Injection", "categories": ["cs.CV"], "comment": null, "summary": "Event-based sensors offer significant advantages over traditional frame-based\ncameras, especially in scenarios involving rapid motion or challenging lighting\nconditions. However, event data frequently suffers from considerable noise,\nnegatively impacting the performance and robustness of deep learning models.\nTraditionally, this problem has been addressed by applying filtering algorithms\nto the event stream, but this may also remove some of relevant data. In this\npaper, we propose a novel noise-injection training methodology designed to\nenhance the neural networks robustness against varying levels of event noise.\nOur approach introduces controlled noise directly into the training data,\nenabling models to learn noise-resilient representations. We have conducted\nextensive evaluations of the proposed method using multiple benchmark datasets\n(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,\nincluding Convolutional Neural Networks, Vision Transformers, Spiking Neural\nNetworks, and Graph Convolutional Networks. Experimental results show that our\nnoise-injection training strategy achieves stable performance over a range of\nnoise intensities, consistently outperforms event-filtering techniques, and\nachieves the highest average classification accuracy, making it a viable\nalternative to traditional event-data filtering methods in an object\nclassification system. Code: https://github.com/vision-agh/DVS_Filtering"}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078", "abs": "https://arxiv.org/abs/2506.04078", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."}
{"id": "2506.03996", "pdf": "https://arxiv.org/pdf/2506.03996", "abs": "https://arxiv.org/abs/2506.03996", "authors": ["Lianfeng Shi", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Spiking Neural Networks (SNNs) have emerged as a new generation of\nenergy-efficient neural networks suitable for implementation on neuromorphic\nhardware. As neuromorphic hardware has limited memory and computing resources,\nweight pruning and quantization have recently been explored to improve SNNs'\nefficiency. State-of-the-art SNN pruning/quantization methods employ multiple\ncompression and training iterations, increasing the cost for pre-trained or\nvery large SNNs. In this paper, we propose a new one-shot post-training\npruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that\nadapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and\nAlistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input\ncurrent as OBC does, OSBC achieves more efficient and accurate SNN compression\nin one pass by minimizing the loss on spiking neuron membrane potential with a\nsmall sample dataset. Our experiments on neuromorphic datasets (N-MNIST,\nCIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity\nthrough pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric\nquantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code\nwill be available on GitHub."}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques."}
{"id": "2506.03926", "pdf": "https://arxiv.org/pdf/2506.03926", "abs": "https://arxiv.org/abs/2506.03926", "authors": ["Debarshi Brahma", "Soma Biswas"], "title": "Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a practical cross-domain few-shot learning (pCDFSL)\ntask, where a large-scale pre-trained model like CLIP can be easily deployed on\na target dataset. The goal is to simultaneously classify all unseen classes\nunder extreme domain shifts, by utilizing only a few labeled samples per class.\nThe pCDFSL paradigm is source-free and moves beyond artificially created\nepisodic training and testing regimes followed by existing CDFSL frameworks,\nmaking it more challenging and relevant to real-world applications. Towards\nthat goal, we propose a novel framework, termed MIST (MultIple STochastic\nPrompt tuning), where multiple stochastic prompts are utilized to handle\nsignificant domain and semantic shifts. Specifically, multiple prompts are\nlearnt for each class, effectively capturing multiple peaks in the input data.\nFurthermore, instead of representing the weights of the multiple prompts as\npoint-estimates, we model them as learnable Gaussian distributions with two\ndifferent strategies, encouraging an efficient exploration of the prompt\nparameter space, which mitigate overfitting due to the few labeled training\nsamples. Extensive experiments and comparison with the state-of-the-art methods\non four CDFSL benchmarks adapted to this setting, show the effectiveness of the\nproposed framework."}
{"id": "2506.04079", "pdf": "https://arxiv.org/pdf/2506.04079", "abs": "https://arxiv.org/abs/2506.04079", "authors": ["Pedro Henrique Martins", "João Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "José Pombal", "Manuel Faysse", "Pierre Colombo", "François Yvon", "Barry Haddow", "José G. C. de Souza", "Alexandra Birch", "André F. T. Martins"], "title": "EuroLLM-9B: Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "56 pages", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset."}
{"id": "2506.04026", "pdf": "https://arxiv.org/pdf/2506.04026", "abs": "https://arxiv.org/abs/2506.04026", "authors": ["Clément Bénesse", "Patrick Mesana", "Athénaïs Gautier", "Sébastien Gambs"], "title": "On the Usage of Gaussian Process for Efficient Data Valuation", "categories": ["cs.LG"], "comment": null, "summary": "In machine learning, knowing the impact of a given datum on model training is\na fundamental task referred to as Data Valuation. Building on previous works\nfrom the literature, we have designed a novel canonical decomposition allowing\npractitioners to analyze any data valuation method as the combination of two\nparts: a utility function that captures characteristics from a given model and\nan aggregation procedure that merges such information. We also propose to use\nGaussian Processes as a means to easily access the utility function on\n``sub-models'', which are models trained on a subset of the training set. The\nstrength of our approach stems from both its theoretical grounding in Bayesian\ntheory, and its practical reach, by enabling fast estimation of valuations\nthanks to efficient update formulae."}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993", "abs": "https://arxiv.org/abs/2506.03993", "authors": ["Saif M. Mohammad"], "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html"}
{"id": "2506.03928", "pdf": "https://arxiv.org/pdf/2506.03928", "abs": "https://arxiv.org/abs/2506.03928", "authors": ["Ze Feng", "Jiang-Jiang Liu", "Sen Yang", "Lingyu Xiao", "Xiaofan Li", "Wankou Yang", "Jingdong Wang"], "title": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with Vision Feature Resample", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B."}
{"id": "2506.04088", "pdf": "https://arxiv.org/pdf/2506.04088", "abs": "https://arxiv.org/abs/2506.04088", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets."}
{"id": "2506.04053", "pdf": "https://arxiv.org/pdf/2506.04053", "abs": "https://arxiv.org/abs/2506.04053", "authors": ["Alexander Semenenko", "Ivan Butakov", "Alexey Frolov", "Ivan Oseledets"], "title": "Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence", "categories": ["cs.LG", "cs.IT", "math.IT", "94A16, 68T07, 94A17", "E.4; H.1.1"], "comment": null, "summary": "Sliced Mutual Information (SMI) is widely used as a scalable alternative to\nmutual information for measuring non-linear statistical dependence. Despite its\nadvantages, such as faster convergence, robustness to high dimensionality, and\nnullification only under statistical independence, we demonstrate that SMI is\nhighly susceptible to data manipulation and exhibits counterintuitive behavior.\nThrough extensive benchmarking and theoretical analysis, we show that SMI\nsaturates easily, fails to detect increases in statistical dependence (even\nunder linear transformations designed to enhance the extraction of\ninformation), prioritizes redundancy over informative content, and in some\ncases, performs worse than simpler dependence measures like the correlation\ncoefficient."}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities."}
{"id": "2506.03942", "pdf": "https://arxiv.org/pdf/2506.03942", "abs": "https://arxiv.org/abs/2506.03942", "authors": ["Theodore Barfoot", "Luis C. Garcia-Peraza-Herrera", "Samet Akcay", "Ben Glocker", "Tom Vercauteren"], "title": "Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "12 pages, 5 figures, IEEE TMI submission", "summary": "Deep neural networks for medical image segmentation are often overconfident,\ncompromising both reliability and clinical utility. In this work, we propose\ndifferentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)\nas an auxiliary loss that can be computed on a per-image basis. We compare both\nhard- and soft-binning approaches to directly improve pixel-wise calibration.\nOur experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that\nincorporating mL1-ACE significantly reduces calibration errors, particularly\nAverage Calibration Error (ACE) and Maximum Calibration Error (MCE), while\nlargely maintaining high Dice Similarity Coefficients (DSCs). We find that the\nsoft-binned variant yields the greatest improvements in calibration, over the\nDice plus cross-entropy loss baseline, but often compromises segmentation\nperformance, with hard-binned mL1-ACE maintaining segmentation performance,\nalbeit with weaker calibration improvement. To gain further insight into\ncalibration performance and its variability across an imaging dataset, we\nintroduce dataset reliability histograms, an aggregation of per-image\nreliability diagrams. The resulting analysis highlights improved alignment\nbetween predicted confidences and true accuracies. Overall, our approach not\nonly enhances the trustworthiness of segmentation predictions but also shows\npotential for safer integration of deep learning methods into clinical\nworkflows. We share our code here:\nhttps://github.com/cai4cai/Average-Calibration-Losses"}
{"id": "2506.04089", "pdf": "https://arxiv.org/pdf/2506.04089", "abs": "https://arxiv.org/abs/2506.04089", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": "ACL 2025 (Main Conference)", "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset."}
{"id": "2506.04071", "pdf": "https://arxiv.org/pdf/2506.04071", "abs": "https://arxiv.org/abs/2506.04071", "authors": ["Luiz Manella Pereira", "M. Hadi Amini"], "title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds."}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020", "abs": "https://arxiv.org/abs/2506.04020", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM"}
{"id": "2506.03972", "pdf": "https://arxiv.org/pdf/2506.03972", "abs": "https://arxiv.org/abs/2506.03972", "authors": ["Guohua Wu", "Shengqi Chen", "Pengchao Deng", "Wenting Yu"], "title": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection", "categories": ["cs.CV"], "comment": null, "summary": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment."}
{"id": "2506.04098", "pdf": "https://arxiv.org/pdf/2506.04098", "abs": "https://arxiv.org/abs/2506.04098", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "title": "TextAtari: 100K Frames Game Playing with Language Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "51 pages, 39 figures", "summary": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning."}
{"id": "2506.04118", "pdf": "https://arxiv.org/pdf/2506.04118", "abs": "https://arxiv.org/abs/2506.04118", "authors": ["Jonathan Geuter", "Youssef Mroueh", "David Alvarez-Melis"], "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs", "categories": ["cs.LG", "stat.ML", "I.2.7"], "comment": "12 pages, 2 figures", "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI ."}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale."}
{"id": "2506.03988", "pdf": "https://arxiv.org/pdf/2506.03988", "abs": "https://arxiv.org/abs/2506.03988", "authors": ["Hicham Eddoubi", "Jonas Ricker", "Federico Cocchi", "Lorenzo Baraldi", "Angelo Sotgiu", "Maura Pintor", "Marcella Cornia", "Lorenzo Baraldi", "Asja Fischer", "Rita Cucchiara", "Battista Biggio"], "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "categories": ["cs.CV", "cs.LG"], "comment": "Under review for NeurIPS 2025 Datasets and Benchmarks Track", "summary": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustnessOur findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID."}
{"id": "2506.04131", "pdf": "https://arxiv.org/pdf/2506.04131", "abs": "https://arxiv.org/abs/2506.04131", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SICon 2025 ACL", "summary": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM."}
{"id": "2506.04126", "pdf": "https://arxiv.org/pdf/2506.04126", "abs": "https://arxiv.org/abs/2506.04126", "authors": ["Yujun Kim", "Jaeyoung Cha", "Chulhee Yun"], "title": "Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems", "categories": ["cs.LG", "math.OC"], "comment": "Accepted to ICML 2025, 56 pages, 6 figures", "summary": "Recent theoretical results demonstrate that the convergence rates of\npermutation-based SGD (e.g., random reshuffling SGD) are faster than\nuniform-sampling SGD; however, these studies focus mainly on the large epoch\nregime, where the number of epochs $K$ exceeds the condition number $\\kappa$.\nIn contrast, little is known when $K$ is smaller than $\\kappa$, and it is still\na challenging open question whether permutation-based SGD can converge faster\nin this small epoch regime (Safran and Shamir, 2021). As a step toward\nunderstanding this gap, we study the naive deterministic variant, Incremental\nGradient Descent (IGD), on smooth and strongly convex functions. Our lower\nbounds reveal that for the small epoch regime, IGD can exhibit surprisingly\nslow convergence even when all component functions are strongly convex.\nFurthermore, when some component functions are allowed to be nonconvex, we\nprove that the optimality gap of IGD can be significantly worse throughout the\nsmall epoch regime. Our analyses reveal that the convergence properties of\npermutation-based SGD in the small epoch regime may vary drastically depending\non the assumptions on component functions. Lastly, we supplement the paper with\ntight upper and lower bounds for IGD in the large epoch regime."}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041", "abs": "https://arxiv.org/abs/2506.04041", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning."}
{"id": "2506.04005", "pdf": "https://arxiv.org/pdf/2506.04005", "abs": "https://arxiv.org/abs/2506.04005", "authors": ["Maxime Zanella", "Clément Fuchs", "Ismail Ben Ayed", "Christophe De Vleeschouwer"], "title": "Vocabulary-free few-shot learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR Workshops 2025", "summary": "Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have\ngreatly expanded their ability to generalize across tasks using only a few\nlabeled examples. However, existing approaches primarily build upon the strong\nzero-shot priors of these models by leveraging carefully designed,\ntask-specific prompts. This dependence on predefined class names can restrict\ntheir applicability, especially in scenarios where exact class names are\nunavailable or difficult to specify. To address this limitation, we introduce\nvocabulary-free few-shot learning for VLMs, a setting where target class\ninstances - that is, images - are available but their corresponding names are\nnot. We propose Similarity Mapping (SiM), a simple yet effective baseline that\nclassifies target instances solely based on similarity scores with a set of\ngeneric prompts (textual or visual), eliminating the need for carefully\nhandcrafted prompts. Although conceptually straightforward, SiM demonstrates\nstrong performance, operates with high computational efficiency (learning the\nmapping typically takes less than one second), and provides interpretability by\nlinking target classes to generic prompts. We believe that our approach could\nserve as an important baseline for future research in vocabulary-free few-shot\nlearning. Code is available at\nhttps://github.com/MaxZanella/vocabulary-free-FSL."}
{"id": "2506.04132", "pdf": "https://arxiv.org/pdf/2506.04132", "abs": "https://arxiv.org/abs/2506.04132", "authors": ["Peter A. Gloor"], "title": "Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication", "categories": ["q-bio.OT", "cs.AI"], "comment": null, "summary": "We present a comprehensive investigation into plant bioelectric responses to\nhuman presence and emotional states, building on five years of systematic\nresearch. Using custom-built plant sensors and machine learning classification,\nwe demonstrate that plants generate distinct bioelectric signals correlating\nwith human proximity, emotional states, and physiological conditions. A deep\nlearning model based on ResNet50 architecture achieved 97% accuracy in\nclassifying human emotional states through plant voltage spectrograms, while\ncontrol models with shuffled labels achieved only 30% accuracy. This study\nsynthesizes findings from multiple experiments spanning 2020-2025, including\nindividual recognition (66% accuracy), eurythmic gesture detection, stress\nprediction, and responses to human voice and movement. We propose that these\nphenomena represent evolved anti-herbivory early warning systems, where plants\ndetect approaching animals through bioelectric field changes before physical\ncontact. Our results challenge conventional understanding of plant sensory\ncapabilities and suggest practical applications in agriculture, healthcare, and\nhuman-plant interaction research."}
{"id": "2506.04165", "pdf": "https://arxiv.org/pdf/2506.04165", "abs": "https://arxiv.org/abs/2506.04165", "authors": ["Yashas Samaga", "Varun Yerram", "Spandana Raj Babbula", "Prateek Jain", "Praneeth Netrapalli"], "title": "Faster Approx. Top-K: Harnessing the Full Power of Two Stages", "categories": ["cs.LG", "cs.DS"], "comment": "Rejected from MLSys 2025", "summary": "We consider the Top-$K$ selection problem, which aims to identify the\nlargest-$K$ elements from an array. Top-$K$ selection arises in many machine\nlearning algorithms and often becomes a bottleneck on accelerators, which are\noptimized for dense matrix multiplications. To address this problem,\n\\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage\n\\textit{approximate} Top-$K$ algorithm: (i) partition the input array and\nselect the top-$1$ element from each partition, (ii) sort this \\textit{smaller\nsubset} and return the top $K$ elements. In this paper, we consider a\ngeneralized version of this algorithm, where the first stage selects top-$K'$\nelements, for some $1 \\leq K' \\leq K$, from each partition. Our contributions\nare as follows: (i) we derive an expression for the expected recall of this\ngeneralized algorithm and show that choosing $K' > 1$ with fewer partitions in\nthe first stage reduces the input size to the second stage more effectively\nwhile maintaining the same expected recall as the original algorithm, (ii) we\nderive a bound on the expected recall for the original algorithm in\n\\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of\n$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud\nTPUv5e and achieve around an order of magnitude speedups over the original\nalgorithm without sacrificing recall on real-world tasks."}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042", "abs": "https://arxiv.org/abs/2506.04042", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing."}
{"id": "2506.04034", "pdf": "https://arxiv.org/pdf/2506.04034", "abs": "https://arxiv.org/abs/2506.04034", "authors": ["Qing Jiang", "Xingyu Chen", "Zhaoyang Zeng", "Junzhi Yu", "Lei Zhang"], "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": "homepage: https://rexthinker.github.io/", "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings."}
{"id": "2506.04143", "pdf": "https://arxiv.org/pdf/2506.04143", "abs": "https://arxiv.org/abs/2506.04143", "authors": ["Ngoc Q. Ly", "Hieu N. M. Cao", "Thi T. Nguyen"], "title": "Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods."}
{"id": "2506.04166", "pdf": "https://arxiv.org/pdf/2506.04166", "abs": "https://arxiv.org/abs/2506.04166", "authors": ["Caleb Chin", "Aashish Khubchandani", "Harshvardhan Maskara", "Kyuseong Choi", "Jacob Feitelberg", "Albert Gong", "Manit Paul", "Tathagata Sadhukhan", "Anish Agarwal", "Raaz Dwivedi"], "title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "21 pages, 6 figures", "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings."}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047", "abs": "https://arxiv.org/abs/2506.04047", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "title": "On Support Samples of Next Word Prediction", "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability."}
{"id": "2506.04048", "pdf": "https://arxiv.org/pdf/2506.04048", "abs": "https://arxiv.org/abs/2506.04048", "authors": ["Gabriele Magrini", "Federico Becattini", "Giovanni Colombo", "Pietro Pala"], "title": "EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects", "categories": ["cs.CV"], "comment": null, "summary": "Monitoring aerial objects is crucial for security, wildlife conservation, and\nenvironmental studies. Traditional RGB-based approaches struggle with\nchallenges such as scale variations, motion blur, and high-speed object\nmovements, especially for small flying entities like insects and drones. In\nthis work, we explore the potential of event-based vision for detecting and\nrecognizing flying objects, in particular animals that may not follow short and\nlong-term predictable patters. Event cameras offer high temporal resolution,\nlow latency, and robustness to motion blur, making them well-suited for this\ntask. We introduce EV-Flying, an event-based dataset of flying objects,\ncomprising manually annotated birds, insects and drones with spatio-temporal\nbounding boxes and track identities. To effectively process the asynchronous\nevent streams, we employ a point-based approach leveraging lightweight\narchitectures inspired by PointNet. Our study investigates the classification\nof flying objects using point cloud-based event representations. The proposed\ndataset and methodology pave the way for more efficient and reliable aerial\nobject recognition in real-world scenarios."}
{"id": "2506.04147", "pdf": "https://arxiv.org/pdf/2506.04147", "abs": "https://arxiv.org/abs/2506.04147", "authors": ["Jiaheng Hu", "Peter Stone", "Roberto Martín-Martín"], "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Building capable household and industrial robots requires mastering the\ncontrol of versatile, high-degree-of-freedom (DoF) systems such as mobile\nmanipulators. While reinforcement learning (RL) holds promise for autonomously\nacquiring robot control policies, scaling it to high-DoF embodiments remains\nchallenging. Direct RL in the real world demands both safe exploration and high\nsample efficiency, which are difficult to achieve in practice. Sim-to-real RL,\non the other hand, is often brittle due to the reality gap. This paper\nintroduces SLAC, a method that renders real-world RL feasible for complex\nembodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic\nlatent action space. SLAC trains this latent action space via a customized\nunsupervised skill discovery method designed to promote temporal abstraction,\ndisentanglement, and safety, thereby facilitating efficient downstream\nlearning. Once a latent action space is learned, SLAC uses it as the action\ninterface for a novel off-policy RL algorithm to autonomously learn downstream\ntasks through real-world interactions. We evaluate SLAC against existing\nmethods on a suite of bimanual mobile manipulation tasks, where it achieves\nstate-of-the-art performance. Notably, SLAC learns contact-rich whole-body\ntasks in under an hour of real-world interactions, without relying on any\ndemonstrations or hand-crafted behavior priors. More information, code, and\nvideos at robo-rl.github.io"}
{"id": "2506.04168", "pdf": "https://arxiv.org/pdf/2506.04168", "abs": "https://arxiv.org/abs/2506.04168", "authors": ["Seohong Park", "Kevin Frans", "Deepinder Mann", "Benjamin Eysenbach", "Aviral Kumar", "Sergey Levine"], "title": "Horizon Reduction Makes RL Scalable", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we study the scalability of offline reinforcement learning (RL)\nalgorithms. In principle, a truly scalable offline RL algorithm should be able\nto solve any given problem, regardless of its complexity, given sufficient\ndata, compute, and model capacity. We investigate if and how current offline RL\nalgorithms match up to this promise on diverse, challenging, previously\nunsolved tasks, using datasets up to 1000x larger than typical offline RL\ndatasets. We observe that despite scaling up data, many existing offline RL\nalgorithms exhibit poor scaling behavior, saturating well below the maximum\nperformance. We hypothesize that the horizon is the main cause behind the poor\nscaling of offline RL. We empirically verify this hypothesis through several\nanalysis experiments, showing that long horizons indeed present a fundamental\nbarrier to scaling up offline RL. We then show that various horizon reduction\ntechniques substantially enhance scalability on challenging tasks. Based on our\ninsights, we also introduce a minimal yet scalable method named SHARSA that\neffectively reduces the horizon. SHARSA achieves the best asymptotic\nperformance and scaling behavior among our evaluation methods, showing that\nexplicitly reducing the horizon unlocks the scalability of offline RL. Code:\nhttps://github.com/seohongpark/horizon-reduction"}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065", "abs": "https://arxiv.org/abs/2506.04065", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance."}
{"id": "2506.04054", "pdf": "https://arxiv.org/pdf/2506.04054", "abs": "https://arxiv.org/abs/2506.04054", "authors": ["Giyong Choi", "HyunWook Park"], "title": "Video Deblurring with Deconvolution and Aggregation Networks", "categories": ["cs.CV"], "comment": null, "summary": "In contrast to single-image deblurring, video deblurring has the advantage\nthat neighbor frames can be utilized to deblur a target frame. However,\nexisting video deblurring algorithms often fail to properly employ the neighbor\nframes, resulting in sub-optimal performance. In this paper, we propose a\ndeconvolution and aggregation network (DAN) for video deblurring that utilizes\nthe information of neighbor frames well. In DAN, both deconvolution and\naggregation strategies are achieved through three sub-networks: the\npreprocessing network (PPN) and the alignment-based deconvolution network\n(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for\nthe aggregation scheme. In the deconvolution part, blurry inputs are first\npreprocessed by the PPN with non-local operations. Then, the output frames from\nthe PPN are deblurred by the ABDN based on the frame alignment. In the FAN,\nthese deblurred frames from the deconvolution part are combined into a latent\nframe according to reliability maps which infer pixel-wise sharpness. The\nproper combination of three sub-networks can achieve favorable performance on\nvideo deblurring by using the neighbor frames suitably. In experiments, the\nproposed DAN was demonstrated to be superior to existing state-of-the-art\nmethods through both quantitative and qualitative evaluations on the public\ndatasets."}
{"id": "2506.04171", "pdf": "https://arxiv.org/pdf/2506.04171", "abs": "https://arxiv.org/abs/2506.04171", "authors": ["Utkarsh Utkarsh", "Pengfei Cai", "Alan Edelman", "Rafael Gomez-Bombarelli", "Christopher Vincent Rackauckas"], "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "27 pages, 9 figures, 4 tables", "summary": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential."}
{"id": "2506.04172", "pdf": "https://arxiv.org/pdf/2506.04172", "abs": "https://arxiv.org/abs/2506.04172", "authors": ["Shreenidhi Srinivasan", "Lydia Manikonda"], "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?", "categories": ["cs.LG", "cs.ET"], "comment": "7 pages", "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation."}
{"id": "2506.04072", "pdf": "https://arxiv.org/pdf/2506.04072", "abs": "https://arxiv.org/abs/2506.04072", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "categories": ["cs.CL", "cs.HC", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset."}
{"id": "2506.04081", "pdf": "https://arxiv.org/pdf/2506.04081", "abs": "https://arxiv.org/abs/2506.04081", "authors": ["Abdelouahed Laazoufi", "Mohammed El Hassouni", "Hocine Cherifi"], "title": "Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network", "categories": ["cs.CV"], "comment": null, "summary": "No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for\nevaluating 3D content in real-world applications where reference models are\nunavailable."}
{"id": "2506.04195", "pdf": "https://arxiv.org/pdf/2506.04195", "abs": "https://arxiv.org/abs/2506.04195", "authors": ["Elena Zamaraeva", "Christopher M. Collins", "George R. Darling", "Matthew S. Dyer", "Bei Peng", "Rahul Savani", "Dmytro Antypov", "Vladimir V. Gusev", "Judith Clymo", "Paul G. Spirakis", "Matthew J. Rosseinsky"], "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.11"], "comment": null, "summary": "Geometry optimization of atomic structures is a common and crucial task in\ncomputational chemistry and materials design. Following the learning to\noptimize paradigm, we propose a new multi-agent reinforcement learning method\ncalled Multi-Agent Crystal Structure optimization (MACS) to address periodic\ncrystal structure optimization. MACS treats geometry optimization as a\npartially observable Markov game in which atoms are agents that adjust their\npositions to collectively discover a stable configuration. We train MACS across\nvarious compositions of reported crystalline materials to obtain a policy that\nsuccessfully optimizes structures from the training compositions as well as\nstructures of larger sizes and unseen compositions, confirming its excellent\nscalability and zero-shot transferability. We benchmark our approach against a\nbroad range of state-of-the-art optimization methods and demonstrate that MACS\noptimizes periodic crystal structures significantly faster, with fewer energy\ncalculations, and the lowest failure rate."}
{"id": "2506.04178", "pdf": "https://arxiv.org/pdf/2506.04178", "abs": "https://arxiv.org/abs/2506.04178", "authors": ["Etash Guha", "Ryan Marten", "Sedrick Keh", "Negin Raoof", "Georgios Smyrnis", "Hritik Bansal", "Marianna Nezhurina", "Jean Mercat", "Trung Vu", "Zayne Sprague", "Ashima Suvarna", "Benjamin Feuer", "Liangyu Chen", "Zaid Khan", "Eric Frankel", "Sachin Grover", "Caroline Choi", "Niklas Muennighoff", "Shiye Su", "Wanjia Zhao", "John Yang", "Shreyas Pimpalgaonkar", "Kartik Sharma", "Charlie Cheng-Jie Ji", "Yichuan Deng", "Sarah Pratt", "Vivek Ramanujan", "Jon Saad-Falcon", "Jeffrey Li", "Achal Dave", "Alon Albalak", "Kushal Arora", "Blake Wulfe", "Chinmay Hegde", "Greg Durrett", "Sewoong Oh", "Mohit Bansal", "Saadia Gabriel", "Aditya Grover", "Kai-Wei Chang", "Vaishaal Shankar", "Aaron Gokaslan", "Mike A. Merrill", "Tatsunori Hashimoto", "Yejin Choi", "Jenia Jitsev", "Reinhard Heckel", "Maheswaran Sathiamoorthy", "Alexandros G. Dimakis", "Ludwig Schmidt"], "title": "OpenThoughts: Data Recipes for Reasoning Models", "categories": ["cs.LG"], "comment": "https://www.openthoughts.ai/blog/ot3", "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai."}
{"id": "2506.04108", "pdf": "https://arxiv.org/pdf/2506.04108", "abs": "https://arxiv.org/abs/2506.04108", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "title": "Rectified Sparse Attention", "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."}
{"id": "2506.04106", "pdf": "https://arxiv.org/pdf/2506.04106", "abs": "https://arxiv.org/abs/2506.04106", "authors": ["Xiao Xiang Zhu", "Sining Chen", "Fahong Zhang", "Yilei Shi", "Yuanyuan Wang"], "title": "GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GlobalBuildingAtlas, a publicly available dataset providing\nglobal and complete coverage of building polygons, heights and Level of Detail\n1 (LoD1) 3D building models. This is the first open dataset to offer high\nquality, consistent, and complete building data in 2D and 3D form at the\nindividual building level on a global scale. Towards this dataset, we developed\nmachine learning-based pipelines to derive building polygons and heights\n(called GBA.Height) from global PlanetScope satellite data, respectively. Also\na quality-based fusion strategy was employed to generate higher-quality\npolygons (called GBA.Polygon) based on existing open building polygons,\nincluding our own derived one. With more than 2.75 billion buildings worldwide,\nGBA.Polygon surpasses the most comprehensive database to date by more than 1\nbillion buildings. GBA.Height offers the most detailed and accurate global 3D\nbuilding height maps to date, achieving a spatial resolution of 3x3 meters-30\ntimes finer than previous global products (90 m), enabling a high-resolution\nand reliable analysis of building volumes at both local and global scales.\nFinally, we generated a global LoD1 building model (called GBA.LoD1) from the\nresulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete\nglobal LoD1 building models, including 2.68 billion building instances with\npredicted heights, i.e., with a height completeness of more than 97%, achieving\nRMSEs ranging from 1.5 m to 8.9 m across different continents. With its height\naccuracy, comprehensive global coverage and rich spatial details,\nGlobalBuildingAltas offers novel insights on the status quo of global\nbuildings, which unlocks unprecedented geospatial analysis possibilities, as\nshowcased by a better illustration of where people live and a more\ncomprehensive monitoring of the progress on the 11th Sustainable Development\nGoal of the United Nations."}
{"id": "2506.04202", "pdf": "https://arxiv.org/pdf/2506.04202", "abs": "https://arxiv.org/abs/2506.04202", "authors": ["Yanting Wang", "Wei Zou", "Runpeng Geng", "Jinyuan Jia"], "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM", "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM."}
{"id": "2506.04190", "pdf": "https://arxiv.org/pdf/2506.04190", "abs": "https://arxiv.org/abs/2506.04190", "authors": ["Yuxuan Cao", "Jiarong Xu", "Chen Zhao", "Jiaan Wang", "Carl Yang", "Chunping Wang", "Yang Yang"], "title": "How to Use Graph Data in the Wild to Help Graph Anomaly Detection?", "categories": ["cs.LG"], "comment": "Accepted by SIGKDD2025", "summary": "In recent years, graph anomaly detection has found extensive applications in\nvarious domains such as social, financial, and communication networks. However,\nanomalies in graph-structured data present unique challenges, including label\nscarcity, ill-defined anomalies, and varying anomaly types, making supervised\nor semi-supervised methods unreliable. Researchers often adopt unsupervised\napproaches to address these challenges, assuming that anomalies deviate\nsignificantly from the normal data distribution. Yet, when the available data\nis insufficient, capturing the normal distribution accurately and\ncomprehensively becomes difficult. To overcome this limitation, we propose to\nutilize external graph data (i.e., graph data in the wild) to help anomaly\ndetection tasks. This naturally raises the question: How can we use external\ndata to help graph anomaly detection tasks? To answer this question, we propose\na framework called Wild-GAD. It is built upon a unified database, UniWildGraph,\nwhich comprises a large and diverse collection of graph data with broad domain\ncoverage, ample data volume, and a unified feature space. Further, we develop\nselection criteria based on representativity and diversity to identify the most\nsuitable external data for anomaly detection task. Extensive experiments on six\nreal-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the\nbaseline methods, our framework has an average 18% AUCROC and 32% AUCPR\nimprovement over the best-competing methods."}
{"id": "2506.04139", "pdf": "https://arxiv.org/pdf/2506.04139", "abs": "https://arxiv.org/abs/2506.04139", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research."}
{"id": "2506.04115", "pdf": "https://arxiv.org/pdf/2506.04115", "abs": "https://arxiv.org/abs/2506.04115", "authors": ["Robin Bruneau", "Baptiste Brument", "Yvain Quéau", "Jean Mélou", "François Bernard Lauze", "Jean-Denis Durou", "Lilian Calvet"], "title": "Multi-view Surface Reconstruction Using Normal and Reflectance Cues", "categories": ["cs.CV"], "comment": "22 pages, 15 figures, 11 tables. A thorough qualitative and\n  quantitive study is available in the supplementary material at\n  https://drive.google.com/file/d/1KDfCKediXNP5Os954TL_QldaUWS0nKcD/view?usp=drive_link", "summary": "Achieving high-fidelity 3D surface reconstruction while preserving fine\ndetails remains challenging, especially in the presence of materials with\ncomplex reflectance properties and without a dense-view setup. In this paper,\nwe introduce a versatile framework that incorporates multi-view normal and\noptionally reflectance maps into radiance-based surface reconstruction. Our\napproach employs a pixel-wise joint re-parametrization of reflectance and\nsurface normals, representing them as a vector of radiances under simulated,\nvarying illumination. This formulation enables seamless incorporation into\nstandard surface reconstruction pipelines, such as traditional multi-view\nstereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined\nwith the latter, our approach achieves state-of-the-art performance on\nmulti-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,\nLUCES-MV and Skoltech3D. In particular, our method excels in reconstructing\nfine-grained details and handling challenging visibility conditions. The\npresent paper is an extended version of the earlier conference paper by Brument\net al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2024), featuring an accelerated and more robust\nalgorithm as well as a broader empirical evaluation. The code and data relative\nto this article is available at https://github.com/RobinBruneau/RNb-NeuS2."}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025."}
{"id": "2506.04205", "pdf": "https://arxiv.org/pdf/2506.04205", "abs": "https://arxiv.org/abs/2506.04205", "authors": ["Jinghan Jia", "Hadi Reisizadeh", "Chongyu Fan", "Nathalie Baracaldo", "Mingyi Hong", "Sijia Liu"], "title": "EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation."}
{"id": "2506.04142", "pdf": "https://arxiv.org/pdf/2506.04142", "abs": "https://arxiv.org/abs/2506.04142", "authors": ["Kejian Zhu", "Shangqing Tu", "Zhuoran Jin", "Lei Hou", "Juanzi Li", "Jun Zhao"], "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation"}
{"id": "2506.04122", "pdf": "https://arxiv.org/pdf/2506.04122", "abs": "https://arxiv.org/abs/2506.04122", "authors": ["Sharang Kaul", "Mario Berk", "Thiemo Gerbich", "Abhinav Valada"], "title": "Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Finding reliable matches is essential in multi-object tracking to ensure the\naccuracy and reliability of perception systems in safety-critical applications\nsuch as autonomous vehicles. Effective matching mitigates perception errors,\nenhancing object identification and tracking for improved performance and\nsafety. However, traditional metrics such as Intersection over Union (IoU) and\nCenter Point Distances (CPDs), which are effective in 2D image planes, often\nfail to find critical matches in complex 3D scenes. To address this limitation,\nwe introduce Contour Errors (CEs), an ego or object-centric metric for\nidentifying matches of interest in tracking scenarios from a functional\nperspective. By comparing bounding boxes in the ego vehicle's frame, contour\nerrors provide a more functionally relevant assessment of object matches.\nExtensive experiments on the nuScenes dataset demonstrate that contour errors\nimprove the reliability of matches over the state-of-the-art 2D IoU and CPD\nmetrics in tracking-by-detection methods. In 3D car tracking, our results show\nthat Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges\nand 60% at far ranges compared to IoU in the evaluation stage."}
{"id": "2506.04217", "pdf": "https://arxiv.org/pdf/2506.04217", "abs": "https://arxiv.org/abs/2506.04217", "authors": ["Junting Chen", "Haotian Liang", "Lingxiao Du", "Weiyun Wang", "Mengkang Hu", "Yao Mu", "Wenhai Wang", "Jifeng Dai", "Ping Luo", "Wenqi Shao", "Lin Shao"], "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "categories": ["cs.RO", "cs.AI", "I.2.4; I.2.9; I.2.10"], "comment": "9 pages of main content, 19 pages in total", "summary": "The rapid progress of navigation, manipulation, and vision models has made\nmobile manipulators capable in many specialized tasks. However, the open-world\nmobile manipulation (OWMM) task remains a challenge due to the need for\ngeneralization to open-ended instructions and environments, as well as the\nsystematic complexity to integrate high-level decision making with low-level\nrobot control based on both global scene understanding and current agent state.\nTo address this complexity, we propose a novel multi-modal agent architecture\nthat maintains multi-view scene frames and agent states for decision-making and\ncontrols the robot by function calling. A second challenge is the hallucination\nfrom domain shift. To enhance the agent performance, we further introduce an\nagentic data synthesis pipeline for the OWMM task to adapt the VLM model to our\ntask domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM\nas the first dedicated foundation model for mobile manipulators with global\nscene understanding, robot state tracking, and multi-modal action generation in\na unified model. Through experiments, we demonstrate that our model achieves\nSOTA performance compared to other foundation models including GPT-4o and\nstrong zero-shot generalization in real world. The project page is at\nhttps://github.com/HHYHRHY/OWMM-Agent"}
{"id": "2506.04206", "pdf": "https://arxiv.org/pdf/2506.04206", "abs": "https://arxiv.org/abs/2506.04206", "authors": ["Reza Ramezanpour", "Victor M. Tenorio", "Antonio G. Marques", "Ashutosh Sabharwal", "Santiago Segarra"], "title": "A Few Moments Please: Scalable Graphon Learning via Moment Matching", "categories": ["cs.LG"], "comment": null, "summary": "Graphons, as limit objects of dense graph sequences, play a central role in\nthe statistical analysis of network data. However, existing graphon estimation\nmethods often struggle with scalability to large networks and\nresolution-independent approximation, due to their reliance on estimating\nlatent variables or costly metrics such as the Gromov-Wasserstein distance. In\nthis work, we propose a novel, scalable graphon estimator that directly\nrecovers the graphon via moment matching, leveraging implicit neural\nrepresentations (INRs). Our approach avoids latent variable modeling by\ntraining an INR--mapping coordinates to graphon values--to match empirical\nsubgraph counts (i.e., moments) from observed graphs. This direct estimation\nmechanism yields a polynomial-time solution and crucially sidesteps the\ncombinatorial complexity of Gromov-Wasserstein optimization. Building on\nfoundational results, we establish a theoretical guarantee: when the observed\nsubgraph motifs sufficiently represent those of the true graphon (a condition\nmet with sufficiently large or numerous graph samples), the estimated graphon\nachieves a provable upper bound in cut distance from the ground truth.\nAdditionally, we introduce MomentMixup, a data augmentation technique that\nperforms mixup in the moment space to enhance graphon-based learning. Our\ngraphon estimation method achieves strong empirical performance--demonstrating\nhigh accuracy on small graphs and superior computational efficiency on large\ngraphs--outperforming state-of-the-art scalable estimators in 75\\% of benchmark\nsettings and matching them in the remaining cases. Furthermore, MomentMixup\ndemonstrated improved graph classification accuracy on the majority of our\nbenchmarks."}
{"id": "2506.04156", "pdf": "https://arxiv.org/pdf/2506.04156", "abs": "https://arxiv.org/abs/2506.04156", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "categories": ["cs.CL"], "comment": null, "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts."}
{"id": "2506.04141", "pdf": "https://arxiv.org/pdf/2506.04141", "abs": "https://arxiv.org/abs/2506.04141", "authors": ["Kejian Zhu", "Zhuoran Jin", "Hongbang Yuan", "Jiachun Li", "Shangqing Tu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://mmr-v.github.io", "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities."}
{"id": "2506.04218", "pdf": "https://arxiv.org/pdf/2506.04218", "abs": "https://arxiv.org/abs/2506.04218", "authors": ["Wei Cao", "Marcel Hallgarten", "Tianyu Li", "Daniel Dauner", "Xunjiang Gu", "Caojun Wang", "Yakov Miron", "Marco Aiello", "Hongyang Li", "Igor Gilitschenski", "Boris Ivanovic", "Marco Pavone", "Andreas Geiger", "Kashyap Chitta"], "title": "Pseudo-Simulation for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim."}
{"id": "2506.03153", "pdf": "https://arxiv.org/pdf/2506.03153", "abs": "https://arxiv.org/abs/2506.03153", "authors": ["Junzhe Jiang", "Chang Yang", "Xinrun Wang", "Bo Li"], "title": "Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction", "categories": ["q-fin.ST", "cs.LG"], "comment": null, "summary": "Stock market indices serve as fundamental market measurement that quantify\nsystematic market dynamics. However, accurate index price prediction remains\nchallenging, primarily because existing approaches treat indices as isolated\ntime series and frame the prediction as a simple regression task. These methods\nfail to capture indices' inherent nature as aggregations of constituent stocks\nwith complex, time-varying interdependencies. To address these limitations, we\npropose Cubic, a novel end-to-end framework that explicitly models the adaptive\nfusion of constituent stocks for index price prediction. Our main contributions\nare threefold. i) Fusion in the latent space: we introduce the fusion mechanism\nover the latent embedding of the stocks to extract the information from the\nvast number of stocks. ii) Binary encoding classification: since regression\ntasks are challenging due to continuous value estimation, we reformulate the\nregression into the classification task, where the target value is converted to\nbinary and we optimize the prediction of the value of each digit with\ncross-entropy loss. iii) Confidence-guided prediction and trading: we introduce\nthe regularization loss to address market prediction uncertainty for the index\nprediction and design the rule-based trading policies based on the confidence.\nExtensive experiments across multiple stock markets and indices demonstrate\nthat Cubic consistently outperforms state-of-the-art baselines in stock index\nprediction tasks, achieving superior performance on both forecasting accuracy\nmetrics and downstream trading profitability."}
{"id": "2506.04179", "pdf": "https://arxiv.org/pdf/2506.04179", "abs": "https://arxiv.org/abs/2506.04179", "authors": ["Anhao Zhao", "Fanghua Ye", "Yingqi Fan", "Junlong Tong", "Zhiwei Fei", "Hui Su", "Xiaoyu Shen"], "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT."}
{"id": "2506.04158", "pdf": "https://arxiv.org/pdf/2506.04158", "abs": "https://arxiv.org/abs/2506.04158", "authors": ["Yujia Hu", "Songhua Liu", "Zhenxiong Tan", "Xingyi Yang", "Xinchao Wang"], "title": "Image Editing As Programs with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP."}
{"id": "2506.04226", "pdf": "https://arxiv.org/pdf/2506.04226", "abs": "https://arxiv.org/abs/2506.04226", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "title": "Efficient Knowledge Editing via Minimal Precomputation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes."}
{"id": "2506.03157", "pdf": "https://arxiv.org/pdf/2506.03157", "abs": "https://arxiv.org/abs/2506.03157", "authors": ["Ziyang Yu", "Wenbing Huang", "Yang Liu"], "title": "UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules", "categories": ["q-bio.BM", "cs.LG"], "comment": "ICML 2025 poster", "summary": "Molecular Dynamics (MD) simulations are essential for understanding the\natomic-level behavior of molecular systems, giving insights into their\ntransitions and interactions. However, classical MD techniques are limited by\nthe trade-off between accuracy and efficiency, while recent deep learning-based\nimprovements have mostly focused on single-domain molecules, lacking\ntransferability to unfamiliar molecular systems. Therefore, we propose\n\\textbf{Uni}fied \\textbf{Sim}ulator (UniSim), which leverages cross-domain\nknowledge to enhance the understanding of atomic interactions. First, we employ\na multi-head pretraining approach to learn a unified atomic representation\nmodel from a large and diverse set of molecular data. Then, based on the\nstochastic interpolant framework, we learn the state transition patterns over\nlong timesteps from MD trajectories, and introduce a force guidance module for\nrapidly adapting to different chemical environments. Our experiments\ndemonstrate that UniSim achieves highly competitive performance across small\nmolecules, peptides, and proteins."}
{"id": "2506.04180", "pdf": "https://arxiv.org/pdf/2506.04180", "abs": "https://arxiv.org/abs/2506.04180", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation."}
{"id": "2506.04174", "pdf": "https://arxiv.org/pdf/2506.04174", "abs": "https://arxiv.org/abs/2506.04174", "authors": ["Hengyu Liu", "Yuehao Wang", "Chenxin Li", "Ruisi Cai", "Kevin Wang", "Wuyang Li", "Pavlo Molchanov", "Peihao Wang", "Zhangyang Wang"], "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2025; Project Page: https://flexgs.github.io", "summary": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io."}
{"id": "2506.04227", "pdf": "https://arxiv.org/pdf/2506.04227", "abs": "https://arxiv.org/abs/2506.04227", "authors": ["Zhao-Heng Yin", "Sherry Yang", "Pieter Abbeel"], "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project: https://zhaohengyin.github.io/3DMF", "summary": "Learning robot control policies from human videos is a promising direction\nfor scaling up robot learning. However, how to extract action knowledge (or\naction representations) from videos for policy learning remains a key\nchallenge. Existing action representations such as video frames, pixelflow, and\npointcloud flow have inherent limitations such as modeling complexity or loss\nof information. In this paper, we propose to use object-centric 3D motion field\nto represent actions for robot learning from human videos, and present a novel\nframework for extracting this representation from videos for zero-shot control.\nWe introduce two novel components in its implementation. First, a novel\ntraining pipeline for training a ''denoising'' 3D motion field estimator to\nextract fine object 3D motions from human videos with noisy depth robustly.\nSecond, a dense object-centric 3D motion field prediction architecture that\nfavors both cross-embodiment transfer and policy generalization to background.\nWe evaluate the system in real world setups. Experiments show that our method\nreduces 3D motion estimation error by over 50% compared to the latest method,\nachieve 55% average success rate in diverse tasks where prior approaches\nfail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills\nlike insertion."}
{"id": "2506.03167", "pdf": "https://arxiv.org/pdf/2506.03167", "abs": "https://arxiv.org/abs/2506.03167", "authors": ["Long Tan Le", "Senura Hansaja Wanasekara", "Zerun Niu", "Yansong Shi", "Nguyen H. Tran", "Phuong Vo", "Walid Saad", "Dusit Niyato", "Zhu Han", "Choong Seon Hong", "H. Vincent Poor"], "title": "Distributionally Robust Wireless Semantic Communication with Large AI Models", "categories": ["cs.NI", "cs.ET", "cs.IT", "cs.LG", "math.IT"], "comment": "Under Review", "summary": "6G wireless systems are expected to support massive volumes of data with\nultra-low latency. However, conventional bit-level transmission strategies\ncannot support the efficiency and adaptability required by modern,\ndata-intensive applications. The concept of semantic communication (SemCom)\naddresses this limitation by focusing on transmitting task-relevant semantic\ninformation instead of raw data. While recent efforts incorporating deep\nlearning and large-scale AI models have improved SemCom's performance, existing\nsystems remain vulnerable to both semantic-level and transmission-level noise\nbecause they often rely on domain-specific architectures that hinder\ngeneralizability. In this paper, a novel and generalized semantic communication\nframework called WaSeCom is proposed to systematically address uncertainty and\nenhance robustness. In particular, Wasserstein distributionally robust\noptimization is employed to provide resilience against semantic\nmisinterpretation and channel perturbations. A rigorous theoretical analysis is\nperformed to establish the robust generalization guarantees of the proposed\nframework. Experimental results on image and text transmission demonstrate that\nWaSeCom achieves improved robustness under noise and adversarial perturbations.\nThese results highlight its effectiveness in preserving semantic fidelity\nacross varying wireless conditions."}
{"id": "2506.04182", "pdf": "https://arxiv.org/pdf/2506.04182", "abs": "https://arxiv.org/abs/2506.04182", "authors": ["Ruiqi Zhang", "Changyi Xiao", "Yixin Cao"], "title": "Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone."}
{"id": "2506.04209", "pdf": "https://arxiv.org/pdf/2506.04209", "abs": "https://arxiv.org/abs/2506.04209", "authors": ["Jingfeng Yang", "Ziyang Wu", "Yue Zhao", "Yi Ma"], "title": "Language-Image Alignment with Fixed Text Encoders", "categories": ["cs.CV"], "comment": null, "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations."}
{"id": "2204.10669", "pdf": "https://arxiv.org/pdf/2204.10669", "abs": "https://arxiv.org/abs/2204.10669", "authors": ["Ebaa Alnazer", "Ilche Georgievski", "Marco Aiello"], "title": "Risk Awareness in HTN Planning", "categories": ["cs.AI"], "comment": "112 pages, 13 figures", "summary": "Actual real-world domains are characterised by uncertain situations in which\nacting and using resources may entail the embracing of risks. Performing\nactions in such domains involves costs of consuming some resource, such as time\nor energy, where the knowledge about these costs can range from known to\ntotally unknown. In autonomous vehicles, actions have uncertain costs due to\nfactors like traffic. Choosing an action requires assessing delay risks, as\neach road may have unpredictable congestion. Thus, these domains call for not\nonly planning under uncertainty but also planning while embracing risk.\nResorting to HTN planning as a widely used planning technique in real-world\napplications, one can observe that existing approaches assume risk neutrality,\nrelying on single-valued action costs without considering risk. Here, we\nenhance HTN planning with risk awareness by considering expected utility\ntheory. We introduce a general framework for HTN planning that allows modelling\nrisk and uncertainty using a probability distribution of action costs upon\nwhich we define risk-aware HTN planning as being capable of accounting for the\ndifferent risk attitudes and allowing the computation of plans that go beyond\nrisk neutrality. We lay out that computing risk-aware plans requires finding\nplans with the highest expected utility. We argue that it is possible for HTN\nplanning agents to solve specialised risk-aware HTN planning problems by\nadapting existing HTN planning approaches, and develop an approach that\nsurpasses the expressiveness of current approaches by allowing these agents to\ncompute plans tailored to a particular risk attitude. An empirical evaluation\nof two case studies highlights the feasibility and expressiveness of this\napproach. We also highlight open issues, such as applying the proposal beyond\nHTN planning, covering both modelling and plan generation."}
{"id": "2506.04185", "pdf": "https://arxiv.org/pdf/2506.04185", "abs": "https://arxiv.org/abs/2506.04185", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "categories": ["cs.CL"], "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search."}
{"id": "2506.04211", "pdf": "https://arxiv.org/pdf/2506.04211", "abs": "https://arxiv.org/abs/2506.04211", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector", "categories": ["cs.CV"], "comment": "MM2024 poster, with appendix and codes", "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher."}
{"id": "2401.10969", "pdf": "https://arxiv.org/pdf/2401.10969", "abs": "https://arxiv.org/abs/2401.10969", "authors": ["Gianluca Aguzzi", "Roberto Casadei", "Mirko Viroli"], "title": "MacroSwarm: A Field-based Compositional Framework for Swarm Programming", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Swarm behaviour engineering is an area of research that seeks to investigate\nmethods and techniques for coordinating computation and action within groups of\nsimple agents to achieve complex global goals like pattern formation,\ncollective movement, clustering, and distributed sensing. Despite recent\nprogress in the analysis and engineering of swarms (of drones, robots,\nvehicles), there is still a need for general design and implementation methods\nand tools that can be used to define complex swarm behaviour in a principled\nway. To contribute to this quest, this article proposes a new field-based\ncoordination approach, called MacroSwarm, to design and program swarm behaviour\nin terms of reusable and fully composable functional blocks embedding\ncollective computation and coordination. Based on the macroprogramming paradigm\nof aggregate computing, MacroSwarm builds on the idea of expressing each swarm\nbehaviour block as a pure function, mapping sensing fields into actuation goal\nfields, e.g., including movement vectors. In order to demonstrate the\nexpressiveness, compositionality, and practicality of MacroSwarm as a framework\nfor swarm programming, we perform a variety of simulations covering common\npatterns of flocking, pattern formation, and collective decision-making. The\nimplications of the inherent self-stabilisation properties of field-based\ncomputations in MacroSwarm are discussed, which formally guarantee some\nresilience properties and guided the design of the library."}
{"id": "2506.03196", "pdf": "https://arxiv.org/pdf/2506.03196", "abs": "https://arxiv.org/abs/2506.03196", "authors": ["Dania Herzalla", "Willian T. Lunardi", "Martin Andreoni"], "title": "Graph Neural Networks for Jamming Source Localization", "categories": ["cs.NI", "cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "Graph-based learning has emerged as a transformative approach for modeling\ncomplex relationships across diverse domains, yet its potential in wireless\nsecurity remains largely unexplored. In this work, we introduce the first\napplication of graph-based learning for jamming source localization, addressing\nthe imminent threat of jamming attacks in wireless networks. Unlike geometric\noptimization techniques that struggle under environmental uncertainties and\ndense interference, we reformulate localization as an inductive graph\nregression task. Our approach integrates structured node representations that\nencode local and global signal aggregation, ensuring spatial coherence and\nadaptive signal fusion. To enhance robustness, we incorporate an\nattention-based graph neural network that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex radio frequency environments with varying\nsampling densities and signal propagation conditions, conducting comprehensive\nablation studies on graph construction, feature selection, and pooling\nstrategies. Results demonstrate that our novel graph-based learning framework\nsignificantly outperforms established localization baselines, particularly in\nchallenging scenarios with sparse and obfuscated signal information. Code is\navailable at\n[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization)."}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning."}
{"id": "2506.04213", "pdf": "https://arxiv.org/pdf/2506.04213", "abs": "https://arxiv.org/abs/2506.04213", "authors": ["Xuanhua He", "Quande Liu", "Zixuan Ye", "Wecai Ye", "Qiulin Wang", "Xintao Wang", "Qifeng Chen", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."}
{"id": "2403.04588", "pdf": "https://arxiv.org/pdf/2403.04588", "abs": "https://arxiv.org/abs/2403.04588", "authors": ["Léopold Maytié", "Benjamin Devillers", "Alexandre Arnold", "Rufin VanRullen"], "title": "Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace", "categories": ["cs.AI"], "comment": null, "summary": "Humans perceive the world through multiple senses, enabling them to create a\ncomprehensive representation of their surroundings and to generalize\ninformation across domains. For instance, when a textual description of a scene\nis given, humans can mentally visualize it. In fields like robotics and\nReinforcement Learning (RL), agents can also access information about the\nenvironment through multiple sensors; yet redundancy and complementarity\nbetween sensors is difficult to exploit as a source of robustness (e.g. against\nsensor failure) or generalization (e.g. transfer across domains). Prior\nresearch demonstrated that a robust and flexible multimodal representation can\nbe efficiently constructed based on the cognitive science notion of a 'Global\nWorkspace': a unique representation trained to combine information across\nmodalities, and to broadcast its signal back to each modality. Here, we explore\nwhether such a brain-inspired multimodal representation could be advantageous\nfor RL agents. First, we train a 'Global Workspace' to exploit information\ncollected about the environment via two input modalities (a visual input, or an\nattribute vector representing the state of the agent and/or its environment).\nThen, we train a RL agent policy using this frozen Global Workspace. In two\ndistinct environments and tasks, our results reveal the model's ability to\nperform zero-shot cross-modal transfer between input modalities, i.e. to apply\nto image inputs a policy previously trained on attribute vectors (and\nvice-versa), without additional training or fine-tuning. Variants and ablations\nof the full Global Workspace (including a CLIP-like multimodal representation\ntrained via contrastive learning) did not display the same generalization\nabilities."}
{"id": "2506.03199", "pdf": "https://arxiv.org/pdf/2506.03199", "abs": "https://arxiv.org/abs/2506.03199", "authors": ["Giuseppe Di Caro", "Vahagn Kirakosyan", "Alexander G. Abanov", "Luca Candelori", "Nadine Hartmann", "Ernest T. Lam", "Kharen Musaelian", "Ryan Samson", "Dario Villani", "Martin T. Wells", "Richard J. Wenstrup", "Mengjia Xu"], "title": "Quantum Cognition Machine Learning for Forecasting Chromosomal Instability", "categories": ["q-bio.QM", "cs.LG", "quant-ph"], "comment": null, "summary": "The accurate prediction of chromosomal instability from the morphology of\ncirculating tumor cells (CTCs) enables real-time detection of CTCs with high\nmetastatic potential in the context of liquid biopsy diagnostics. However, it\npresents a significant challenge due to the high dimensionality and complexity\nof single-cell digital pathology data. Here, we introduce the application of\nQuantum Cognition Machine Learning (QCML), a quantum-inspired computational\nframework, to estimate morphology-predicted chromosomal instability in CTCs\nfrom patients with metastatic breast cancer. QCML leverages quantum mechanical\nprinciples to represent data as state vectors in a Hilbert space, enabling\ncontext-aware feature modeling, dimensionality reduction, and enhanced\ngeneralization without requiring curated feature selection. QCML outperforms\nconventional machine learning methods when tested on out of sample verification\nCTCs, achieving higher accuracy in identifying predicted large-scale state\ntransitions (pLST) status from CTC-derived morphology features. These\npreliminary findings support the application of QCML as a novel machine\nlearning tool with superior performance in high-dimensional, low-sample-size\nbiomedical contexts. QCML enables the simulation of cognition-like learning for\nthe identification of biologically meaningful prediction of chromosomal\ninstability from CTC morphology, offering a novel tool for CTC classification\nin liquid biopsy."}
{"id": "2506.03487", "pdf": "https://arxiv.org/pdf/2506.03487", "abs": "https://arxiv.org/abs/2506.03487", "authors": ["Xianming Li", "Aamir Shakir", "Rui Huang", "Julius Lipp", "Jing Li"], "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency."}
{"id": "2506.04216", "pdf": "https://arxiv.org/pdf/2506.04216", "abs": "https://arxiv.org/abs/2506.04216", "authors": ["Zixuan Ye", "Xuanhua He", "Quande Liu", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Qifeng Chen", "Wenhan Luo"], "title": "UNIC: Unified In-Context Video Editing", "categories": ["cs.CV"], "comment": "The project page is at\n  \\href{https://zixuan-ye.github.io/UNIC}{https://zixuan-ye.github.io/UNIC}", "summary": "Recent advances in text-to-video generation have sparked interest in\ngenerative video editing tasks. Previous methods often rely on task-specific\narchitectures (e.g., additional adapter modules) or dedicated customizations\n(e.g., DDIM inversion), which limit the integration of versatile editing\nconditions and the unification of various editing tasks. In this paper, we\nintroduce UNified In-Context Video Editing (UNIC), a simple yet effective\nframework that unifies diverse video editing tasks within a single model in an\nin-context manner. To achieve this unification, we represent the inputs of\nvarious video editing tasks as three types of tokens: the source video tokens,\nthe noisy video latent, and the multi-modal conditioning tokens that vary\naccording to the specific editing task. Based on this formulation, our key\ninsight is to integrate these three types into a single consecutive token\nsequence and jointly model them using the native attention operations of DiT,\nthereby eliminating the need for task-specific adapter designs. Nevertheless,\ndirect task unification under this framework is challenging, leading to severe\ntoken collisions and task confusion due to the varying video lengths and\ndiverse condition modalities across tasks. To address these, we introduce\ntask-aware RoPE to facilitate consistent temporal positional encoding, and\ncondition bias that enables the model to clearly differentiate different\nediting tasks. This allows our approach to adaptively perform different video\nediting tasks by referring the source video and varying condition tokens \"in\ncontext\", and support flexible task composition. To validate our method, we\nconstruct a unified video editing benchmark containing six representative video\nediting tasks. Results demonstrate that our unified approach achieves superior\nperformance on each task and exhibits emergent task composition abilities."}
{"id": "2407.03969", "pdf": "https://arxiv.org/pdf/2407.03969", "abs": "https://arxiv.org/abs/2407.03969", "authors": ["Mikel Malagón", "Josu Ceberio", "Jose A. Lozano"], "title": "Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and Multi-Agent Environments", "categories": ["cs.AI"], "comment": "ICML 2025. Project's website: https://github.com/mikelma/craftium/", "summary": "Advances in large models, reinforcement learning, and open-endedness have\naccelerated progress toward autonomous agents that can learn and interact in\nthe real world. To achieve this, flexible tools are needed to create rich, yet\ncomputationally efficient, environments. While scalable 2D environments fail to\naddress key real-world challenges like 3D navigation and spatial reasoning,\nmore complex 3D environments are computationally expensive and lack features\nlike customizability and multi-agent support. This paper introduces Craftium, a\nhighly customizable and easy-to-use platform for building rich 3D single- and\nmulti-agent environments. We showcase environments of different complexity and\nnature: from single- and multi-agent tasks to vast worlds with many creatures\nand biomes, and customizable procedural task generators. Benchmarking shows\nthat Craftium significantly reduces the computational cost of alternatives of\nsimilar richness, achieving +2K steps per second more than Minecraft-based\nframeworks."}
{"id": "2506.03587", "pdf": "https://arxiv.org/pdf/2506.03587", "abs": "https://arxiv.org/abs/2506.03587", "authors": ["Florian Boudin", "Akiko Aizawa"], "title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The rapid growth of scholarly literature makes it increasingly difficult for\nresearchers to keep up with new knowledge. Automated tools are now more\nessential than ever to help navigate and interpret this vast body of\ninformation. Scientific papers pose unique difficulties, with their complex\nlanguage, specialized terminology, and diverse formats, requiring advanced\nmethods to extract reliable and actionable insights. Large language models\n(LLMs) offer new opportunities, enabling tasks such as literature reviews,\nwriting assistance, and interactive exploration of research. This special issue\nof the TAL journal highlights research addressing these challenges and, more\nbroadly, research on natural language processing and information retrieval for\nscholarly and scientific documents."}
{"id": "2506.04220", "pdf": "https://arxiv.org/pdf/2506.04220", "abs": "https://arxiv.org/abs/2506.04220", "authors": ["Fangrui Zhu", "Hanhui Wang", "Yiming Xie", "Jing Gu", "Tianye Ding", "Jianwei Yang", "Huaizu Jiang"], "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models", "categories": ["cs.CV"], "comment": "https://github.com/neu-vi/struct2d", "summary": "Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts\noften rely on explicit 3D inputs or specialized model architectures, we ask:\ncan LMMs reason about 3D space using only structured 2D representations derived\nfrom perception? We introduce Struct2D, a perception-guided prompting framework\nthat combines bird's-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs\n(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning\nabilities when provided with structured 2D inputs, effectively handling tasks\nsuch as relative direction estimation and route planning. Building on these\ninsights, we construct Struct2D-Set, a large-scale instruction tuning dataset\nwith 200K fine-grained QA pairs across eight spatial reasoning categories,\ngenerated automatically from 3D indoor scenes. We fine-tune an open-source LMM\n(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple\nbenchmarks, including 3D question answering, dense captioning, and object\ngrounding. Our approach demonstrates that structured 2D inputs can effectively\nbridge perception and language reasoning in LMMs-without requiring explicit 3D\nrepresentations as input. We will release both our code and dataset to support\nfuture research."}
{"id": "2407.11784", "pdf": "https://arxiv.org/pdf/2407.11784", "abs": "https://arxiv.org/abs/2407.11784", "authors": ["Daoyuan Chen", "Haibin Wang", "Yilun Huang", "Ce Ge", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted by ICML 2025 (Spotlight). 33 pages, 16 tables, 14 figures", "summary": "The emergence of multimodal large models has advanced artificial\nintelligence, introducing unprecedented levels of performance and\nfunctionality. However, optimizing these models remains challenging due to\nhistorically isolated paths of model-centric and data-centric developments,\nleading to suboptimal outcomes and inefficient resource utilization. In\nresponse, we present a new sandbox suite tailored for integrated data-model\nco-development. This sandbox provides a feedback-driven experimental platform,\nenabling cost-effective iteration and guided refinement of both data and\nmodels. Our proposed ``Probe-Analyze-Refine'' workflow, validated through\npractical use cases on multimodal tasks such as image-text pre-training with\nCLIP, image-to-text generation with LLaVA-like models, and text-to-video\ngeneration with DiT-based models, yields transferable and notable performance\nboosts, such as topping the VBench leaderboard. A comprehensive set of over 100\nexperiments demonstrated the suite's usability and extensibility, while also\nuncovering insights into the interplay between data quality, diversity, model\nbehavior, and computational costs. All codes, datasets, and models are\nopen-sourced to foster future research and applications that would otherwise be\ninfeasible due to the lack of a dedicated co-development infrastructure."}
{"id": "2506.03272", "pdf": "https://arxiv.org/pdf/2506.03272", "abs": "https://arxiv.org/abs/2506.03272", "authors": ["My Youssef El Hafidi", "Achraf Toufah", "Mohamed Achraf Kadim"], "title": "Investigating Quantum Feature Maps in Quantum Support Vector Machines for Lung Cancer Classification", "categories": ["quant-ph", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "In recent years, quantum machine learning has emerged as a promising\nintersection between quantum physics and artificial intelligence, particularly\nin domains requiring advanced pattern recognition such as healthcare. This\nstudy investigates the effectiveness of Quantum Support Vector Machines (QSVM),\nwhich leverage quantum mechanical phenomena like superposition and entanglement\nto construct high-dimensional Hilbert spaces for data classification. Focusing\non lung cancer diagnosis, a concrete and critical healthcare application, we\nanalyze how different quantum feature maps influence classification\nperformance. Using a real-world dataset of 309 patient records with significant\nclass imbalance (39 non-cancer vs. 270 cancer cases), we constructed six\nbalanced subsets for robust evaluation. QSVM models were implemented using\nQiskit and executed on the qasm simulator, employing three distinct quantum\nfeature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Performance was\nassessed using accuracy, precision, recall, specificity, and F1-score. Results\nshow that the PauliFeatureMap consistently outperformed the others, achieving\nperfect classification in three subsets and strong performance overall. These\nfindings demonstrate how quantum computational principles can be harnessed to\nenhance diagnostic capabilities, reinforcing the importance of physics-based\nmodeling in emerging AI applications within healthcare."}
{"id": "2506.03741", "pdf": "https://arxiv.org/pdf/2506.03741", "abs": "https://arxiv.org/abs/2506.03741", "authors": ["Rifat Mehreen Amin", "Oliver Hans Kühle", "Daniel Buschek", "Andreas Butz"], "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI."}
{"id": "2506.04224", "pdf": "https://arxiv.org/pdf/2506.04224", "abs": "https://arxiv.org/abs/2506.04224", "authors": ["Zirui Wang", "Wenjing Bian", "Xinghui Li", "Yifu Tao", "Jianeng Wang", "Maurice Fallon", "Victor Adrian Prisacariu"], "title": "Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset", "categories": ["cs.CV"], "comment": "Project page: https://oxdan.active.vision/", "summary": "We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for\nnovel view synthesis (NVS) and visual relocalisation under challenging lighting\nconditions. Existing datasets often lack crucial combinations of features such\nas ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF\nmotion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA\nglasses to capture egocentric video and applying multi-session SLAM to estimate\ncamera poses, reconstruct 3D point clouds, and align sequences captured under\nvarying lighting conditions, including both day and night. The dataset spans\nover 30 $\\mathrm{km}$ of recorded trajectories and covers an area of 40,000\n$\\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.\nIt supports two core benchmarks, NVS and relocalisation, providing a unique\nplatform for evaluating models in realistic and diverse environments."}
{"id": "2408.08959", "pdf": "https://arxiv.org/pdf/2408.08959", "abs": "https://arxiv.org/abs/2408.08959", "authors": ["Jinwei Hu", "Yi Dong", "Xiaowei Huang"], "title": "Trust-Oriented Adaptive Guardrails for Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Under Review", "summary": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service."}
{"id": "2506.03317", "pdf": "https://arxiv.org/pdf/2506.03317", "abs": "https://arxiv.org/abs/2506.03317", "authors": ["Yuntian Wang", "Zafer Yilmaz", "Yuhang Li", "Edward Liu", "Eric Ahlberg", "Farid Ghahari", "Ertugrul Taciroglu", "Aydogan Ozcan"], "title": "Structural Vibration Monitoring with Diffractive Optical Processors", "categories": ["physics.optics", "cs.CV", "cs.LG", "physics.app-ph"], "comment": "33 Pages, 8 Figures, 1 Table", "summary": "Structural Health Monitoring (SHM) is vital for maintaining the safety and\nlongevity of civil infrastructure, yet current solutions remain constrained by\ncost, power consumption, scalability, and the complexity of data processing.\nHere, we present a diffractive vibration monitoring system, integrating a\njointly optimized diffractive layer with a shallow neural network-based backend\nto remotely extract 3D structural vibration spectra, offering a low-power,\ncost-effective and scalable solution. This architecture eliminates the need for\ndense sensor arrays or extensive data acquisition; instead, it uses a\nspatially-optimized passive diffractive layer that encodes 3D structural\ndisplacements into modulated light, captured by a minimal number of detectors\nand decoded in real-time by shallow and low-power neural networks to\nreconstruct the 3D displacement spectra of structures. The diffractive system's\nefficacy was demonstrated both numerically and experimentally using\nmillimeter-wave illumination on a laboratory-scale building model with a\nprogrammable shake table. Our system achieves more than an order-of-magnitude\nimprovement in accuracy over conventional optics or separately trained modules,\nestablishing a foundation for high-throughput 3D monitoring of structures.\nBeyond SHM, the 3D vibration monitoring capabilities of this cost-effective and\ndata-efficient framework establish a new computational sensing modality with\npotential applications in disaster resilience, aerospace diagnostics, and\nautonomous navigation, where energy efficiency, low latency, and\nhigh-throughput are critical."}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code."}
{"id": "2506.04225", "pdf": "https://arxiv.org/pdf/2506.04225", "abs": "https://arxiv.org/abs/2506.04225", "authors": ["Tianyu Huang", "Wangguandong Zheng", "Tengfei Wang", "Yuhao Liu", "Zhenwei Wang", "Junta Wu", "Jie Jiang", "Hui Li", "Rynson W. H. Lau", "Wangmeng Zuo", "Chunchao Guo"], "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."}
{"id": "2410.02165", "pdf": "https://arxiv.org/pdf/2410.02165", "abs": "https://arxiv.org/abs/2410.02165", "authors": ["Yucheng Chu", "Hang Li", "Kaiqi Yang", "Harry Shomer", "Hui Liu", "Yasemin Copur-Gencturk", "Jiliang Tang"], "title": "A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "EDM 2025 Long Paper", "summary": "Open-ended short-answer questions (SAGs) have been widely recognized as a\npowerful tool for providing deeper insights into learners' responses in the\ncontext of learning analytics (LA). However, SAGs often present challenges in\npractice due to the high grading workload and concerns about inconsistent\nassessments. With recent advancements in natural language processing (NLP),\nautomatic short-answer grading (ASAG) offers a promising solution to these\nchallenges. Despite this, current ASAG algorithms are often limited in\ngeneralizability and tend to be tailored to specific questions. In this paper,\nwe propose a unified multi-agent ASAG framework, GradeOpt, which leverages\nlarge language models (LLMs) as graders for SAGs. More importantly, GradeOpt\nincorporates two additional LLM-based agents - the reflector and the refiner -\ninto the multi-agent system. This enables GradeOpt to automatically optimize\nthe original grading guidelines by performing self-reflection on its errors.\nThrough experiments on a challenging ASAG task, namely the grading of\npedagogical content knowledge (PCK) and content knowledge (CK) questions,\nGradeOpt demonstrates superior performance in grading accuracy and behavior\nalignment with human graders compared to representative baselines. Finally,\ncomprehensive ablation studies confirm the effectiveness of the individual\ncomponents designed in GradeOpt."}
{"id": "2506.03321", "pdf": "https://arxiv.org/pdf/2506.03321", "abs": "https://arxiv.org/abs/2506.03321", "authors": ["Victor H. Cid", "James Mork"], "title": "Enhancing Automatic PT Tagging for MEDLINE Citations Using Transformer-Based Models", "categories": ["cs.DL", "cs.LG", "I.2.7; H.3.3; H.3.5"], "comment": "26 pages, 8 tables, 3 figures", "summary": "We investigated the feasibility of predicting Medical Subject Headings (MeSH)\nPublication Types (PTs) from MEDLINE citation metadata using pre-trained\nTransformer-based models BERT and DistilBERT. This study addresses limitations\nin the current automated indexing process, which relies on legacy NLP\nalgorithms. We evaluated monolithic multi-label classifiers and binary\nclassifier ensembles to enhance the retrieval of biomedical literature. Results\ndemonstrate the potential of Transformer models to significantly improve PT\ntagging accuracy, paving the way for scalable, efficient biomedical indexing."}
{"id": "2506.04228", "pdf": "https://arxiv.org/pdf/2506.04228", "abs": "https://arxiv.org/abs/2506.04228", "authors": ["Sihui Ji", "Hao Luo", "Xi Chen", "Yuanpeng Tu", "Yiyang Wang", "Hengshuang Zhao"], "title": "LayerFlow: A Unified Model for Layer-aware Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://sihuiji.github.io/LayerFlow-Page/", "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers."}
{"id": "2410.16270", "pdf": "https://arxiv.org/pdf/2410.16270", "abs": "https://arxiv.org/abs/2410.16270", "authors": ["Lingyu Li", "Yixu Wang", "Haiquan Zhao", "Shuqi Kong", "Yan Teng", "Chunbo Li", "Yingchun Wang"], "title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models", "categories": ["cs.AI"], "comment": "29 pages, 19 figures, 9 tables", "summary": "With large language models (LLMs) increasingly deployed as cognitive engines\nfor AI agents, the reliability and effectiveness critically hinge on their\nintrinsic epistemic agency, which remains understudied. Epistemic agency, the\nability to flexibly construct, adapt, and monitor beliefs about dynamic\nenvironments, represents a base-model-level capacity independent of specific\ntools, modules, or applications. We characterize the holistic process\nunderlying epistemic agency, which unfolds in seven interrelated dimensions:\nprediction, decision-making, perception, memory, counterfactual thinking,\nbelief updating, and meta-reflection. Correspondingly, we propose\nReflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven\ntasks with long-term relevance and minimization of data leakage. Through a\ncomprehensive evaluation of 16 models using three prompting strategies, we\nidentify a clear three-tier performance hierarchy and significant limitations\nof current LLMs, particularly in meta-reflection capabilities. While\nstate-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our\nfindings suggest several promising research directions, including enhancing\ncore cognitive functions, improving cross-functional coordination, and\ndeveloping adaptive processing mechanisms. Our code and data are available at\nhttps://github.com/AI45Lab/ReflectionBench."}
{"id": "2506.03464", "pdf": "https://arxiv.org/pdf/2506.03464", "abs": "https://arxiv.org/abs/2506.03464", "authors": ["Yang Cai", "Haipeng Luo", "Chen-Yu Wei", "Weiqiang Zheng"], "title": "From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications", "categories": ["cs.GT", "cs.LG", "math.OC"], "comment": "21 pages", "summary": "The convergence of online learning algorithms in games under self-play is a\nfundamental question in game theory and machine learning. Among various notions\nof convergence, last-iterate convergence is particularly desirable, as it\nreflects the actual decisions made by the learners and captures the day-to-day\nbehavior of the learning dynamics. While many algorithms are known to converge\nin the average-iterate, achieving last-iterate convergence typically requires\nconsiderably more effort in both the design and the analysis of the algorithm.\nSomewhat surprisingly, we show in this paper that for a large family of games,\nthere exists a simple black-box reduction that transforms the average iterates\nof an uncoupled learning dynamics into the last iterates of a new uncoupled\nlearning dynamics, thus also providing a reduction from last-iterate\nconvergence to average-iterate convergence. Our reduction applies to games\nwhere each player's utility is linear in both their own strategy and the joint\nstrategy of all opponents. This family includes two-player bimatrix games and\ngeneralizations such as multi-player polymatrix games. By applying our\nreduction to the Optimistic Multiplicative Weights Update algorithm, we obtain\nnew state-of-the-art last-iterate convergence rates for uncoupled learning\ndynamics in two-player zero-sum normal-form games: (1) an $O(\\frac{\\log d}{T})$\nlast-iterate convergence rate under gradient feedback, representing an\nexponential improvement in the dependence on the dimension $d$ (i.e., the\nmaximum number of actions available to either player); and (2) an\n$\\widetilde{O}(d^{\\frac{1}{5}} T^{-\\frac{1}{5}})$ last-iterate convergence rate\nunder bandit feedback, improving upon the previous best rates of\n$\\widetilde{O}(\\sqrt{d} T^{-\\frac{1}{8}})$ and $\\widetilde{O}(\\sqrt{d}\nT^{-\\frac{1}{6}})$."}
{"id": "2308.09583", "pdf": "https://arxiv.org/pdf/2308.09583", "abs": "https://arxiv.org/abs/2308.09583", "authors": ["Haipeng Luo", "Qingfeng Sun", "Can Xu", "Pu Zhao", "Jianguang Lou", "Chongyang Tao", "Xiubo Geng", "Qingwei Lin", "Shifeng Chen", "Yansong Tang", "Dongmei Zhang"], "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ICLR 2025 as an Oral presentation", "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM"}
{"id": "2506.03180", "pdf": "https://arxiv.org/pdf/2506.03180", "abs": "https://arxiv.org/abs/2506.03180", "authors": ["Jan Ignatowicz", "Krzysztof Kutt", "Grzegorz J. Nalepa"], "title": "Knowledge Graphs for Digitized Manuscripts in Jagiellonian Digital Library Application", "categories": ["cs.DL", "cs.CV"], "comment": null, "summary": "Digitizing cultural heritage collections has become crucial for preservation\nof historical artifacts and enhancing their availability to the wider public.\nGalleries, libraries, archives and museums (GLAM institutions) are actively\ndigitizing their holdings and creates extensive digital collections. Those\ncollections are often enriched with metadata describing items but not exactly\ntheir contents. The Jagiellonian Digital Library, standing as a good example of\nsuch an effort, offers datasets accessible through protocols like OAI-PMH.\nDespite these improvements, metadata completeness and standardization continue\nto pose substantial obstacles, limiting the searchability and potential\nconnections between collections. To deal with these challenges, we explore an\nintegrated methodology of computer vision (CV), artificial intelligence (AI),\nand semantic web technologies to enrich metadata and construct knowledge graphs\nfor digitized manuscripts and incunabula."}
{"id": "2501.05790", "pdf": "https://arxiv.org/pdf/2501.05790", "abs": "https://arxiv.org/abs/2501.05790", "authors": ["Taywon Min", "Haeone Lee", "Yongchan Kwon", "Kimin Lee"], "title": "Understanding Impact of Human Feedback via Influence Functions", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted at ACL 2025, Source code:\n  https://github.com/mintaywon/IF_RLHF", "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF"}
{"id": "2506.03467", "pdf": "https://arxiv.org/pdf/2506.03467", "abs": "https://arxiv.org/abs/2506.03467", "authors": ["Hang Liu", "Anna Scaglione", "Sean Peisert"], "title": "Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization", "categories": ["cs.IT", "cs.CR", "cs.LG", "eess.SP", "math.IT", "stat.ME"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Gaussian Mixture Models (GMMs) are widely used statistical models for\nrepresenting multi-modal data distributions, with numerous applications in data\nmining, pattern recognition, data simulation, and machine learning. However,\nrecent research has shown that releasing GMM parameters poses significant\nprivacy risks, potentially exposing sensitive information about the underlying\ndata. In this paper, we address the challenge of releasing GMM parameters while\nensuring differential privacy (DP) guarantees. Specifically, we focus on the\nprivacy protection of mixture weights, component means, and covariance\nmatrices. We propose to use Kullback-Leibler (KL) divergence as a utility\nmetric to assess the accuracy of the released GMM, as it captures the joint\nimpact of noise perturbation on all the model parameters. To achieve privacy,\nwe introduce a DP mechanism that adds carefully calibrated random perturbations\nto the GMM parameters. Through theoretical analysis, we quantify the effects of\nprivacy budget allocation and perturbation statistics on the DP guarantee, and\nderive a tractable expression for evaluating KL divergence. We formulate and\nsolve an optimization problem to minimize the KL divergence between the\nreleased and original models, subject to a given $(\\epsilon, \\delta)$-DP\nconstraint. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that our approach achieves strong privacy guarantees while\nmaintaining high utility."}
{"id": "2310.12049", "pdf": "https://arxiv.org/pdf/2310.12049", "abs": "https://arxiv.org/abs/2310.12049", "authors": ["Patrick Y. Wu", "Jonathan Nagler", "Joshua A. Tucker", "Solomon Messing"], "title": "Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "10 pages, 2 figures. Appears in 2024 IEEE International Conference on\n  Big Data (BigData). Please cite the published version:\n  10.1109/BigData62323.2024.10825235", "summary": "Existing text scoring methods require a large corpus, struggle with short\ntexts, or require hand-labeled data. We develop a text scoring framework that\nleverages generative large language models (LLMs) to (1) set texts against the\nbackdrop of information from the near-totality of the web and digitized media,\nand (2) effectively transform pairwise text comparisons from a reasoning\nproblem to a pattern recognition task. Our approach, concept-guided\nchain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with\nan LLM to generate a concept-specific breakdown for each text, akin to guidance\nprovided to human coders. We then pairwise compare breakdowns using an LLM and\naggregate answers into a score using a probability model. We apply this\napproach to better understand speech reflecting aversion to specific political\nparties on Twitter, a topic that has commanded increasing interest because of\nits potential contributions to democratic backsliding. We achieve stronger\ncorrelations with human judgments than widely used unsupervised text scoring\nmethods like Wordfish. In a supervised setting, besides a small pilot dataset\nto develop CGCoT prompts, our measures require no additional hand-labeled data\nand produce predictions on par with RoBERTa-Large fine-tuned on thousands of\nhand-labeled tweets. This project showcases the potential of combining human\nexpertise and LLMs for scoring tasks."}
{"id": "2501.07930", "pdf": "https://arxiv.org/pdf/2501.07930", "abs": "https://arxiv.org/abs/2501.07930", "authors": ["Thibaut Boissin", "Franck Mamalet", "Thomas Fel", "Agustin Martin Picard", "Thomas Massena", "Mathieu Serrurier"], "title": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures", "categories": ["cs.AI", "cs.NE"], "comment": null, "summary": "Orthogonal convolutional layers are valuable components in multiple areas of\nmachine learning, such as adversarial robustness, normalizing flows, GANs, and\nLipschitz-constrained models. Their ability to preserve norms and ensure stable\ngradient propagation makes them valuable for a large range of problems. Despite\ntheir promise, the deployment of orthogonal convolution in large-scale\napplications is a significant challenge due to computational overhead and\nlimited support for modern features like strides, dilations, group\nconvolutions, and transposed convolutions. In this paper, we introduce AOC\n(Adaptative Orthogonal Convolution), a scalable method that extends a previous\nmethod (BCOP), effectively overcoming existing limitations in the construction\nof orthogonal convolutions. This advancement unlocks the construction of\narchitectures that were previously considered impractical. We demonstrate\nthrough our experiments that our method produces expressive models that become\nincreasingly efficient as they scale. To foster further advancement, we provide\nan open-source python package implementing this method, called Orthogonium (\nhttps://github.com/deel-ai/orthogonium ) ."}
{"id": "2506.03470", "pdf": "https://arxiv.org/pdf/2506.03470", "abs": "https://arxiv.org/abs/2506.03470", "authors": ["Liam Hodgkinson", "Zhichao Wang", "Michael W. Mahoney"], "title": "Models of Heavy-Tailed Mechanistic Universality", "categories": ["stat.ML", "cs.LG"], "comment": "40 pages, 4 figures", "summary": "Recent theoretical and empirical successes in deep learning, including the\ncelebrated neural scaling laws, are punctuated by the observation that many\nobjects of interest tend to exhibit some form of heavy-tailed or power law\nbehavior. In particular, the prevalence of heavy-tailed spectral densities in\nJacobians, Hessians, and weight matrices has led to the introduction of the\nconcept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of\nempirical evidence suggest a robust correlation between heavy-tailed metrics\nand model performance, indicating that HT-MU may be a fundamental aspect of\ndeep learning efficacy. Here, we propose a general family of random matrix\nmodels -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore\nattributes that give rise to heavy-tailed behavior in trained neural networks.\nUnder this model, spectral densities with power laws on (upper and lower) tails\narise through a combination of three independent factors (complex correlation\nstructures in the data; reduced temperatures during training; and reduced\neigenvector entropy), appearing as an implicit bias in the model structure, and\nthey can be controlled with an \"eigenvalue repulsion\" parameter. Implications\nof our model on other appearances of heavy tails, including neural scaling\nlaws, optimizer trajectories, and the five-plus-one phases of neural network\ntraining, are discussed."}
{"id": "2406.02524", "pdf": "https://arxiv.org/pdf/2406.02524", "abs": "https://arxiv.org/abs/2406.02524", "authors": ["Maciej Besta", "Lorenzo Paleari", "Marcin Copik", "Robert Gerstenberger", "Ales Kubicek", "Piotr Nyczyk", "Patrick Iff", "Eric Schreiber", "Tanja Srindran", "Tomasz Lehmann", "Hubert Niewiadomski", "Torsten Hoefler"], "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework."}
{"id": "2506.03365", "pdf": "https://arxiv.org/pdf/2506.03365", "abs": "https://arxiv.org/abs/2506.03365", "authors": ["Artur Grigorev", "Adriana-Simona Mihaita"], "title": "Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing", "categories": ["eess.SY", "cs.CV", "cs.SY", "stat.CO"], "comment": null, "summary": "Effective placement of Out-of-Home advertising and street furniture requires\naccurate identification of locations offering maximum visual exposure to target\naudiences, particularly vehicular traffic. Traditional site selection methods\noften rely on static traffic counts or subjective assessments. This research\nintroduces a data-driven methodology to objectively quantify location\nvisibility by analyzing large-scale connected vehicle trajectory data (sourced\nfrom Compass IoT) within urban environments. We model the dynamic driver\nfield-of-view using a forward-projected visibility area for each vehicle\nposition derived from interpolated trajectories. By integrating this with\nbuilding vertex locations extracted from OpenStreetMap, we quantify the\ncumulative visual exposure, or ``visibility count'', for thousands of potential\npoints of interest near roadways. The analysis reveals that visibility is\nhighly concentrated, identifying specific ``visual hotspots'' that receive\ndisproportionately high exposure compared to average locations. The core\ntechnical contribution involves the construction of a BallTree spatial index\nover building vertices. This enables highly efficient (O(logN) complexity)\nradius queries to determine which vertices fall within the viewing circles of\nmillions of trajectory points across numerous trips, significantly\noutperforming brute-force geometric checks. Analysis reveals two key findings:\n1) Visibility is highly concentrated, identifying distinct 'visual hotspots'\nreceiving disproportionately high exposure compared to average locations. 2)\nThe aggregated visibility counts across vertices conform to a Log-Normal\ndistribution."}
{"id": "2501.16150", "pdf": "https://arxiv.org/pdf/2501.16150", "abs": "https://arxiv.org/abs/2501.16150", "authors": ["Pascal J. Sager", "Benjamin Meyer", "Peng Yan", "Rebekka von Wartburg-Kottler", "Layan Etaiwi", "Aref Enayati", "Gabriel Nobel", "Ahmed Abdulkadir", "Benjamin F. Grewe", "Thilo Stadelmann"], "title": "A Comprehensive Survey of Agents for Computer Use: Foundations, Challenges, and Future Directions", "categories": ["cs.AI", "cs.HC", "cs.SY", "eess.SY"], "comment": null, "summary": "Agents for computer use (ACUs) are an emerging class of systems capable of\nexecuting complex tasks on digital devices - such as desktops, mobile phones,\nand web platforms - given instructions in natural language. These agents can\nautomate tasks by controlling software via low-level actions like mouse clicks\nand touchscreen gestures. However, despite rapid progress, ACUs are not yet\nmature for everyday use.\n  In this survey, we investigate the state-of-the-art, trends, and research\ngaps in the development of practical ACUs. We provide a comprehensive review of\nthe ACU landscape, introducing a unifying taxonomy spanning three dimensions:\n(I) the domain perspective, characterizing agent operating contexts; (II) the\ninteraction perspective, describing observation modalities (e.g., screenshots,\nHTML) and action modalities (e.g., mouse, keyboard, code execution); and (III)\nthe agent perspective, detailing how agents perceive, reason, and learn.\n  We review 87 ACUs and 33 datasets across foundation model-based and classical\napproaches through this taxonomy. Our analysis identifies six major research\ngaps: insufficient generalization, inefficient learning, limited planning, low\ntask complexity in benchmarks, non-standardized evaluation, and a disconnect\nbetween research and practical conditions.\n  To address these gaps, we advocate for: (a) vision-based observations and\nlow-level control to enhance generalization; (b) adaptive learning beyond\nstatic prompting; (c) effective planning and reasoning methods and models; (d)\nbenchmarks that reflect real-world task complexity; (e) standardized evaluation\nbased on task success; (f) aligning agent design with real-world deployment\nconstraints.\n  Together, our taxonomy and analysis establish a foundation for advancing ACU\nresearch toward general-purpose agents for robust and scalable computer use."}
{"id": "2506.03657", "pdf": "https://arxiv.org/pdf/2506.03657", "abs": "https://arxiv.org/abs/2506.03657", "authors": ["Leonardo Martins Bianco", "Christine Keribin", "Zacharie Naulet"], "title": "SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Community detection is a fundamental task in graph analysis, with methods\noften relying on fitting models like the Stochastic Block Model (SBM) to\nobserved networks. While many algorithms can accurately estimate SBM parameters\nwhen the input graph is a perfect sample from the model, real-world graphs\nrarely conform to such idealized assumptions. Therefore, robust algorithms are\ncrucial-ones that can recover model parameters even when the data deviates from\nthe assumed distribution. In this work, we propose SubSearch, an algorithm for\nrobustly estimating SBM parameters by exploring the space of subgraphs in\nsearch of one that closely aligns with the model's assumptions. Our approach\nalso functions as an outlier detection method, properly identifying nodes\nresponsible for the graph's deviation from the model and going beyond simple\ntechniques like pruning high-degree nodes. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the effectiveness of our method."}
{"id": "2406.09295", "pdf": "https://arxiv.org/pdf/2406.09295", "abs": "https://arxiv.org/abs/2406.09295", "authors": ["Yuhang Wu", "Wenmeng Yu", "Yean Cheng", "Yan Wang", "Xiaohan Zhang", "Jiazheng Xu", "Ming Ding", "Yuxiao Dong"], "title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, which provides more nuanced\nevaluations of alignment capabilities and is the first benchmark specifically\ndesigned for Chinese visual contexts. This benchmark is meticulously curated\nfrom real-world scenarios and internet sources, encompassing thirteen specific\ntasks across three categories, and includes both single-turn and multi-turn\ndialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench\nencompasses 1,054 images and 4,978 question-answer pairs. To facilitate the\nevaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that\nexceeds GPT-4's evaluation ability. Additionally, we measure the \"alignment\nscore\", a quantitative metric designed to assess the robustness and stability\nof models across diverse prompts. Finally, we evaluate the performance of\nrepresentative VLMs on AlignMMBench, offering insights into the capabilities\nand limitations of different VLM architectures. The evaluation code and data\nare available at https://github.com/THUDM/AlignMMBench."}
{"id": "2506.03478", "pdf": "https://arxiv.org/pdf/2506.03478", "abs": "https://arxiv.org/abs/2506.03478", "authors": ["Yuxuan Han", "Junfeng Lyu", "Kuan Sheng", "Minghao Que", "Qixuan Zhang", "Lan Xu", "Feng Xu"], "title": "Facial Appearance Capture at Home with Patch-Level Reflectance Prior", "categories": ["cs.GR", "cs.CV"], "comment": "ACM Transactions on Graphics (Proc. of SIGGRAPH), 2025. Code:\n  https://github.com/yxuhan/DoRA; Project Page: https://yxuhan.github.io/DoRA", "summary": "Existing facial appearance capture methods can reconstruct plausible facial\nreflectance from smartphone-recorded videos. However, the reconstruction\nquality is still far behind the ones based on studio recordings. This paper\nfills the gap by developing a novel daily-used solution with a co-located\nsmartphone and flashlight video capture setting in a dim room. To enhance the\nquality, our key observation is to solve facial reflectance maps within the\ndata distribution of studio-scanned ones. Specifically, we first learn a\ndiffusion prior over the Light Stage scans and then steer it to produce the\nreflectance map that best matches the captured images. We propose to train the\ndiffusion prior at the patch level to improve generalization ability and\ntraining stability, as current Light Stage datasets are in ultra-high\nresolution but limited in data size. Tailored to this prior, we propose a\npatch-level posterior sampling technique to sample seamless full-resolution\nreflectance maps from this patch-level diffusion model. Experiments demonstrate\nour method closes the quality gap between low-cost and studio recordings by a\nlarge margin, opening the door for everyday users to clone themselves to the\ndigital world. Our code will be released at https://github.com/yxuhan/DoRA."}
{"id": "2502.00698", "pdf": "https://arxiv.org/pdf/2502.00698", "abs": "https://arxiv.org/abs/2502.00698", "authors": ["Huanqia Cai", "Yijun Yang", "Winston Hu"], "title": "MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "IQ testing has served as a foundational methodology for evaluating human\ncognitive capabilities, deliberately decoupling assessment from linguistic\nbackground, language proficiency, or domain-specific knowledge to isolate core\ncompetencies in abstraction and reasoning. Yet, artificial intelligence\nresearch currently lacks systematic benchmarks to quantify these critical\ncognitive capabilities in multimodal systems. To address this crucial gap, we\npropose MM-IQ, a comprehensive evaluation framework, which comprises a\nlarge-scale training set with 4,776 visual reasoning problems and 2,710\nmeticulously curated test items spanning 8 distinct reasoning paradigms.\nThrough systematic evaluation of existing open-source and proprietary\nmultimodal models, our benchmark reveals striking limitations: even\nstate-of-the-art architectures achieve only marginally superior performance to\nrandom chance (33.17% vs. 25% baseline accuracy). This substantial performance\nchasm highlights the inadequacy of current multimodal models in approximating\nfundamental human reasoning capacities, underscoring the need for\nparadigm-shifting advancements to bridge this cognitive divide. Moreover,\ninspired by the recent surge of large reasoning models, we also release a\nmultimodal reasoning model as the baseline that is trained via reinforcement\nlearning with verifiable reward functions, reaching competitive performance to\nthe state-of-the-art with a notably smaller model size."}
{"id": "2506.03670", "pdf": "https://arxiv.org/pdf/2506.03670", "abs": "https://arxiv.org/abs/2506.03670", "authors": ["Ivan Melev", "Goeran Kauermann"], "title": "Position: There Is No Free Bayesian Uncertainty Quantification", "categories": ["stat.ML", "cs.LG"], "comment": "NeurIPS 2025 preprint, frequentist critique of Bayesian UQ", "summary": "Due to their intuitive appeal, Bayesian methods of modeling and uncertainty\nquantification have become popular in modern machine and deep learning. When\nproviding a prior distribution over the parameter space, it is straightforward\nto obtain a distribution over the parameters that is conventionally interpreted\nas uncertainty quantification of the model. We challenge the validity of such\nBayesian uncertainty quantification by discussing the equivalent\noptimization-based representation of Bayesian updating, provide an alternative\ninterpretation that is coherent with the optimization-based perspective,\npropose measures of the quality of the Bayesian inferential stage, and suggest\ndirections for future work."}
{"id": "2406.12784", "pdf": "https://arxiv.org/pdf/2406.12784", "abs": "https://arxiv.org/abs/2406.12784", "authors": ["Xunzhi Wang", "Zhuowei Zhang", "Gaonan Chen", "Qiongyu Li", "Bitong Luo", "Zhixin Han", "Haotian Wang", "Zhiyu li", "Hang Gao", "Mengting Hu"], "title": "UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions", "categories": ["cs.CL"], "comment": "accepted by ACL Findings (2025)", "summary": "Despite recent progress in systematic evaluation frameworks, benchmarking the\nuncertainty of large language models (LLMs) remains a highly challenging task.\nExisting methods for benchmarking the uncertainty of LLMs face three key\nchallenges: the need for internal model access, additional training, or high\ncomputational costs. This is particularly unfavorable for closed-source models.\nTo this end, we introduce UBench, a new benchmark for evaluating the\nuncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence\nintervals. It encompasses 11,978 multiple-choice questions spanning knowledge,\nlanguage, understanding, and reasoning capabilities. Based on this, we conduct\nextensive experiments. This includes comparisons with other advanced\nuncertainty estimation methods, the assessment of the uncertainty of 20 LLMs,\nand an exploration of the effects of Chain-of-Thought (CoT) prompts,\nrole-playing (RP) prompts, and temperature on model uncertainty. Our analysis\nreveals several crucial insights: 1) Our confidence interval-based methods are\nhighly effective for uncertainty quantification; 2) Regarding uncertainty,\noutstanding open-source models show competitive performance versus\nclosed-source models; 3) CoT and RP prompts present potential ways to improve\nmodel reliability, while the influence of temperature changes follows no\nuniversal rule. Our implementation is available at\nhttps://github.com/Cyno2232/UBENCH."}
{"id": "2506.03792", "pdf": "https://arxiv.org/pdf/2506.03792", "abs": "https://arxiv.org/abs/2506.03792", "authors": ["Qianwei Qu", "Christian M. Schlepütz", "Marco Stampanoni"], "title": "Analytical Reconstruction of Periodically Deformed Objects in Time-resolved CT", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Time-resolved CT is an advanced measurement technique that has been widely\nused to observe dynamic objects, including periodically varying structures such\nas hearts, lungs, or hearing structures. To reconstruct these objects from CT\nprojections, a common approach is to divide the projections into several\ncollections based on their motion phases and perform reconstruction within each\ncollection, assuming they originate from a static object. This describes the\ngating-based method, which is the standard approach for time-periodic\nreconstruction. However, the gating-based reconstruction algorithm only\nutilizes a limited subset of projections within each collection and ignores the\ncorrelation between different collections, leading to inefficient use of the\nradiation dose. To address this issue, we propose two analytical reconstruction\npipelines in this paper, and validate them with experimental data captured\nusing tomographic synchrotron microscopy. We demonstrate that our approaches\nsignificantly reduce random noise in the reconstructed images without blurring\nthe sharp features of the observed objects. Equivalently, our methods can\nachieve the same reconstruction quality as gating-based methods but with a\nlower radiation dose. Our code is available at github.com/PeriodRecon."}
{"id": "2502.11753", "pdf": "https://arxiv.org/pdf/2502.11753", "abs": "https://arxiv.org/abs/2502.11753", "authors": ["Michiel van der Meer", "Pavel Korshunov", "Sébastien Marcel", "Lonneke van der Plas"], "title": "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims", "categories": ["cs.AI"], "comment": "Accepted at ACL2025 (main track)", "summary": "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with 27K real-world and synthetic image/claim pairs. The mix of real\nand synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the former only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly."}
{"id": "2506.03672", "pdf": "https://arxiv.org/pdf/2506.03672", "abs": "https://arxiv.org/abs/2506.03672", "authors": ["Sobihan Surendran", "Adeline Fermanian", "Sylvain Le Corff"], "title": "Latent Guided Sampling for Combinatorial Optimization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "Combinatorial Optimization problems are widespread in domains such as\nlogistics, manufacturing, and drug discovery, yet their NP-hard nature makes\nthem computationally challenging. Recent Neural Combinatorial Optimization\nmethods leverage deep learning to learn solution strategies, trained via\nSupervised or Reinforcement Learning (RL). While promising, these approaches\noften rely on task-specific augmentations, perform poorly on\nout-of-distribution instances, and lack robust inference mechanisms. Moreover,\nexisting latent space models either require labeled data or rely on pre-trained\npolicies. In this work, we propose LGS-Net, a novel latent space model that\nconditions on problem instances, and introduce an efficient inference method,\nLatent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic\nApproximation. We show that the iterations of our method form a\ntime-inhomogeneous Markov Chain and provide rigorous theoretical convergence\nguarantees. Empirical results on benchmark routing tasks show that our method\nachieves state-of-the-art performance among RL-based approaches."}
{"id": "2406.18173", "pdf": "https://arxiv.org/pdf/2406.18173", "abs": "https://arxiv.org/abs/2406.18173", "authors": ["Wenhao Li", "Mingbao Lin", "Yunshan Zhong", "Shuicheng Yan", "Rongrong Ji"], "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs", "categories": ["cs.CL"], "comment": "The experimental results of the paper require further validation", "summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases."}
{"id": "2506.03804", "pdf": "https://arxiv.org/pdf/2506.03804", "abs": "https://arxiv.org/abs/2506.03804", "authors": ["George Webber", "Alexander Hammers", "Andrew P. King", "Andrew J. Reader"], "title": "Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction", "categories": ["physics.med-ph", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "Recent work has shown improved lesion detectability and flexibility to\nreconstruction hyperparameters (e.g. scanner geometry or dose level) when PET\nimages are reconstructed by leveraging pre-trained diffusion models. Such\nmethods train a diffusion model (without sinogram data) on high-quality, but\nstill noisy, PET images. In this work, we propose a simple method for\ngenerating subject-specific PET images from a dataset of multi-subject PET-MR\nscans, synthesizing \"pseudo-PET\" images by transforming between different\npatients' anatomy using image registration. The images we synthesize retain\ninformation from the subject's MR scan, leading to higher resolution and the\nretention of anatomical features compared to the original set of PET images.\nWith simulated and real [$^{18}$F]FDG datasets, we show that pre-training a\npersonalized diffusion model with subject-specific \"pseudo-PET\" images improves\nreconstruction accuracy with low-count data. In particular, the method shows\npromise in combining information from a guidance MR scan without overly\nimposing anatomical features, demonstrating an improved trade-off between\nreconstructing PET-unique image features versus features present in both PET\nand MR. We believe this approach for generating and utilizing synthetic data\nhas further applications to medical imaging tasks, particularly because\npatient-specific PET images can be generated without resorting to generative\ndeep learning or large training datasets."}
{"id": "2503.11207", "pdf": "https://arxiv.org/pdf/2503.11207", "abs": "https://arxiv.org/abs/2503.11207", "authors": ["Giacomo Camposampiero", "Michael Hersche", "Roger Wattenhofer", "Abu Sebastian", "Abbas Rahimi"], "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at the 19th International Conference on Neural-Symbolic\n  Learning and Reasoning (NeSy) 2025", "summary": "This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its extension,\nI-RAVEN-X, which tests the ability to generalize to longer reasoning rules and\nranges of the attribute values. To assess the influence of visual uncertainties\non these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset,\nwhich otherwise assumes an oracle perception. We adopt a two-fold strategy to\nsimulate this imperfect visual perception: 1) we introduce confounding\nattributes which, being sampled at random, do not contribute to the prediction\nof the correct answer of the puzzles, and 2) we smoothen the distributions of\nthe input attributes' values. We observe a sharp decline in OpenAI's o3-mini\ntask accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% --\napproaching random chance -- on the more challenging I-RAVEN-X, which increases\ninput length and range and emulates perceptual uncertainty. This drop occurred\ndespite spending 3.4x more reasoning tokens. A similar trend is also observed\nfor DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic\nprobabilistic abductive model, ARLC, that achieves state-of-the-art\nperformances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\naccuracy reduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models."}
{"id": "2506.03697", "pdf": "https://arxiv.org/pdf/2506.03697", "abs": "https://arxiv.org/abs/2506.03697", "authors": ["Swagat Kumar", "Jan-Nico Zaech", "Colin Michael Wilmott", "Luc Van Gool"], "title": "RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix Simulations", "categories": ["quant-ph", "cs.LG"], "comment": "24 pages, 16 figures", "summary": "Variational Quantum Algorithms (VQAs) are a promising approach for leveraging\npowerful Noisy Intermediate-Scale Quantum (NISQ) computers. When applied to\nmachine learning tasks, VQAs give rise to NISQ-compatible Quantum Neural\nNetworks (QNNs), which have been shown to outperform classical neural networks\nwith a similar number of trainable parameters. While the quantum circuit\nstructures of VQAs for physics simulations are determined by the physical\nproperties of the systems, identifying effective QNN architectures for general\nmachine learning tasks is a difficult challenge due to the lack of\ndomain-specific priors. Indeed, existing Quantum Architecture Search (QAS)\nalgorithms, adaptations of classical neural architecture search techniques,\noften overlook the inherent quantum nature of the circuits they produce. By\napproaching QAS from the ground-up and from a quantum perspective, we resolve\nthis limitation by proposing $\\rho$DARTS, a differentiable QAS algorithm that\nmodels the search process as the evolution of a quantum mixed state, emerging\nfrom the search space of quantum architectures. We validate our method by\nfinding circuits for state initialization, Hamiltonian optimization, and image\nclassification. Further, we demonstrate better convergence against existing QAS\ntechniques and show improved robustness levels to noise."}
{"id": "2409.17169", "pdf": "https://arxiv.org/pdf/2409.17169", "abs": "https://arxiv.org/abs/2409.17169", "authors": ["Honggen Zhang", "Xufeng Zhao", "Igor Molybog", "June Zhang"], "title": "REAL: Response Embedding-based Alignment for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work."}
{"id": "2504.21131", "pdf": "https://arxiv.org/pdf/2504.21131", "abs": "https://arxiv.org/abs/2504.21131", "authors": ["Remo Christen", "Florian Pommerening", "Clemens Büchner", "Malte Helmert"], "title": "A Formalism for Optimal Search with Dynamic Heuristics (Extended Version)", "categories": ["cs.AI"], "comment": null, "summary": "While most heuristics studied in heuristic search depend only on the state,\nsome accumulate information during search and thus also depend on the search\nhistory. Various existing approaches use such dynamic heuristics in\n$\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$\nto show optimality. However, doing so ignores the complexities of searching\nwith a mutable heuristic. In this paper we formalize the idea of dynamic\nheuristics and use them in a generic algorithm framework. We study a particular\ninstantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show\ngeneral optimality results. Finally we show how existing approaches from\nclassical planning can be viewed as special cases of this instantiation, making\nit possible to directly apply our optimality results."}
{"id": "2506.03746", "pdf": "https://arxiv.org/pdf/2506.03746", "abs": "https://arxiv.org/abs/2506.03746", "authors": ["César Sabater", "Sonia Ben Mokhtar", "Jan Ramon"], "title": "Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "23 pages, 4 figures", "summary": "Achieving differentially private computations in decentralized settings poses\nsignificant challenges, particularly regarding accuracy, communication cost,\nand robustness against information leakage. While cryptographic solutions offer\npromise, they often suffer from high communication overhead or require\ncentralization in the presence of network failures. Conversely, existing fully\ndecentralized approaches typically rely on relaxed adversarial models or\npairwise noise cancellation, the latter suffering from substantial accuracy\ndegradation if parties unexpectedly disconnect. In this work, we propose IncA,\na new protocol for fully decentralized mean estimation, a widely used primitive\nin data-intensive processing. Our protocol, which enforces differential\nprivacy, requires no central orchestration and employs low-variance correlated\nnoise, achieved by incrementally injecting sensitive information into the\ncomputation. First, we theoretically demonstrate that, when no parties\npermanently disconnect, our protocol achieves accuracy comparable to that of a\ncentralized setting-already an improvement over most existing decentralized\ndifferentially private techniques. Second, we empirically show that our use of\nlow-variance correlated noise significantly mitigates the accuracy loss\nexperienced by existing techniques in the presence of dropouts."}
{"id": "2410.01444", "pdf": "https://arxiv.org/pdf/2410.01444", "abs": "https://arxiv.org/abs/2410.01444", "authors": ["Jin Hwa Lee", "Thomas Jiralerspong", "Lei Yu", "Yoshua Bengio", "Emily Cheng"], "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "Under review at ARR", "summary": "By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition."}
{"id": "2506.04016", "pdf": "https://arxiv.org/pdf/2506.04016", "abs": "https://arxiv.org/abs/2506.04016", "authors": ["Adam Rançon", "Ulysse Rançon", "Tomislav Ivek", "Ivan Balog"], "title": "Dreaming up scale invariance via inverse renormalization group", "categories": ["cond-mat.stat-mech", "cs.CV", "cs.LG"], "comment": "v1: 12 pages, 11 figures, 55 references", "summary": "We explore how minimal neural networks can invert the renormalization group\n(RG) coarse-graining procedure in the two-dimensional Ising model, effectively\n\"dreaming up\" microscopic configurations from coarse-grained states. This\ntask-formally impossible at the level of configurations-can be approached\nprobabilistically, allowing machine learning models to reconstruct\nscale-invariant distributions without relying on microscopic input. We\ndemonstrate that even neural networks with as few as three trainable parameters\ncan learn to generate critical configurations, reproducing the scaling behavior\nof observables such as magnetic susceptibility, heat capacity, and Binder\nratios. A real-space renormalization group analysis of the generated\nconfigurations confirms that the models capture not only scale invariance but\nalso reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,\nwe find that increasing network complexity by introducing multiple layers\noffers no significant benefit. These findings suggest that simple local rules,\nakin to those generating fractal structures, are sufficient to encode the\nuniversality of critical phenomena, opening the door to efficient generative\nmodels of statistical ensembles in physics."}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981", "abs": "https://arxiv.org/abs/2505.10981", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Main, 33 pages, 51 figures", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting."}
{"id": "2506.03764", "pdf": "https://arxiv.org/pdf/2506.03764", "abs": "https://arxiv.org/abs/2506.03764", "authors": ["Róisín Luo"], "title": "Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present a theoretical framework for deriving the general $n$-th order\nFr\\'echet derivatives of singular values in real rectangular matrices, by\nleveraging reduced resolvent operators from Kato's analytic perturbation theory\nfor self-adjoint operators. Deriving closed-form expressions for higher-order\nderivatives of singular values is notoriously challenging through standard\nmatrix-analysis techniques. To overcome this, we treat a real rectangular\nmatrix as a compact operator on a finite-dimensional Hilbert space, and embed\nthe rectangular matrix into a block self-adjoint operator so that non-symmetric\nperturbations are captured. Applying Kato's asymptotic eigenvalue expansion to\nthis construction, we obtain a general, closed-form expression for the\ninfinitesimal $n$-th order spectral variations. Specializing to $n=2$ and\ndeploying on a Kronecker-product representation with matrix convention yield\nthe Hessian of a singular value, not found in literature. By bridging abstract\noperator-theoretic perturbation theory with matrices, our framework equips\nresearchers with a practical toolkit for higher-order spectral sensitivity\nstudies in random matrix applications (e.g., adversarial perturbation in deep\nlearning)."}
{"id": "2410.09300", "pdf": "https://arxiv.org/pdf/2410.09300", "abs": "https://arxiv.org/abs/2410.09300", "authors": ["Yu Fei", "Yasaman Razeghi", "Sameer Singh"], "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (main)", "summary": "Large language models (LLMs) require alignment to effectively and safely\nfollow user instructions. This process necessitates training an aligned version\nfor every base model, resulting in significant computational overhead. In this\nwork, we propose NUDGING, a simple, training-free algorithm that aligns any\nbase model at inference time using a small aligned model. NUDGING is motivated\nby recent findings that alignment primarily alters the model's behavior on a\nsmall subset of stylistic tokens (e.g., discourse markers). We find that base\nmodels are significantly more uncertain when generating these tokens. Building\non this insight, NUDGING employs a small aligned model to generate nudging\ntokens to guide the base model's output during decoding when the base model's\nuncertainty is high, with only a minor additional inference overhead. We\nevaluate NUDGING across 3 model families on a diverse range of open-instruction\ntasks. Without any training, nudging a large base model with a 7x-14x smaller\naligned model achieves zero-shot performance comparable to, and sometimes\nsurpassing, that of large aligned models. By operating at the token level,\nNUDGING enables off-the-shelf collaboration between model families. For\ninstance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat\non various tasks. Overall, our work offers a modular and cost-efficient\nsolution to LLM alignment. Our code and demo are available at:\nhttps://fywalter.github.io/nudging/ ."}
{"id": "2505.17433", "pdf": "https://arxiv.org/pdf/2505.17433", "abs": "https://arxiv.org/abs/2505.17433", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Yuxi Zhang", "Yanxi Zhao", "Yifan Zhang", "Zezhong Wang", "Huimin Wang", "Yutian Zhao", "Bin Liang", "Yefeng Zheng", "Binyang Li", "Kam-Fai Wong", "Xian Wu"], "title": "MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Memes have emerged as a popular form of multimodal online communication,\nwhere their interpretation heavily depends on the specific context in which\nthey appear. Current approaches predominantly focus on isolated meme analysis,\neither for harmful content detection or standalone interpretation, overlooking\na fundamental challenge: the same meme can express different intents depending\non its conversational context. This oversight creates an evaluation gap:\nalthough humans intuitively recognize how context shapes meme interpretation,\nLarge Vision Language Models (LVLMs) can hardly understand context-dependent\nmeme intent. To address this critical limitation, we introduce MemeReaCon, a\nnovel benchmark specifically designed to evaluate how LVLMs understand memes in\ntheir original context. We collected memes from five different Reddit\ncommunities, keeping each meme's image, the post text, and user comments\ntogether. We carefully labeled how the text and meme work together, what the\nposter intended, how the meme is structured, and how the community responded.\nOur tests with leading LVLMs show a clear weakness: models either fail to\ninterpret critical information in the contexts, or overly focus on visual\ndetails while overlooking communicative purpose. MemeReaCon thus serves both as\na diagnostic tool exposing current limitations and as a challenging benchmark\nto drive development toward more sophisticated LVLMs of the context-aware\nunderstanding."}
{"id": "2506.03779", "pdf": "https://arxiv.org/pdf/2506.03779", "abs": "https://arxiv.org/abs/2506.03779", "authors": ["Hachem Kadri", "Joachim Tomasi", "Yuka Hashimoto", "Sandrine Anthoine"], "title": "Towards Quantum Operator-Valued Kernels", "categories": ["quant-ph", "cs.LG", "stat.ML"], "comment": null, "summary": "Quantum kernels are reproducing kernel functions built using\nquantum-mechanical principles and are studied with the aim of outperforming\ntheir classical counterparts. The enthusiasm for quantum kernel machines has\nbeen tempered by recent studies that have suggested that quantum kernels could\nnot offer speed-ups when learning on classical data. However, most of the\nresearch in this area has been devoted to scalar-valued kernels in standard\nclassification or regression settings for which classical kernel methods are\nefficient and effective, leaving very little room for improvement with quantum\nkernels. This position paper argues that quantum kernel research should focus\non more expressive kernel classes. We build upon recent advances in\noperator-valued kernels, and propose guidelines for investigating quantum\nkernels. This should help to design a new generation of quantum kernel machines\nand fully explore their potentials."}
{"id": "2410.14309", "pdf": "https://arxiv.org/pdf/2410.14309", "abs": "https://arxiv.org/abs/2410.14309", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Sen Yang", "Nigel Collier", "Dong Yu", "Deqing Yang"], "title": "LoGU: Long-form Generation with Uncertainty Expressions", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."}
{"id": "2403.01422", "pdf": "https://arxiv.org/pdf/2403.01422", "abs": "https://arxiv.org/abs/2403.01422", "authors": ["Zhende Song", "Chenchen Wang", "Jiamu Sheng", "Chi Zhang", "Shengji Tang", "Jiayuan Fan", "Tao Chen"], "title": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA and Style-Consistent Keyframes", "categories": ["cs.CV"], "comment": null, "summary": "Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks."}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641", "abs": "https://arxiv.org/abs/2505.19641", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2506.03780", "pdf": "https://arxiv.org/pdf/2506.03780", "abs": "https://arxiv.org/abs/2506.03780", "authors": ["Hasan Fallahgoul"], "title": "High-Dimensional Learning in Finance", "categories": ["q-fin.ST", "cs.LG", "econ.EM", "stat.ML"], "comment": null, "summary": "Recent advances in machine learning have shown promising results for\nfinancial prediction using large, over-parameterized models. This paper\nprovides theoretical foundations and empirical validation for understanding\nwhen and how these methods achieve predictive success. I examine three key\naspects of high-dimensional learning in finance. First, I prove that\nwithin-sample standardization in Random Fourier Features implementations\nfundamentally alters the underlying Gaussian kernel approximation, replacing\nshift-invariant kernels with training-set dependent alternatives. Second, I\nderive sample complexity bounds showing when reliable learning becomes\ninformation-theoretically impossible under weak signal-to-noise ratios typical\nin finance. Third, VC-dimension analysis reveals that ridgeless regression's\neffective complexity is bounded by sample size rather than nominal feature\ndimension. Comprehensive numerical validation confirms these theoretical\npredictions, revealing systematic breakdown of claimed theoretical properties\nacross realistic parameter ranges. These results show that when sample size is\nsmall and features are high-dimensional, observed predictive success is\nnecessarily driven by low-complexity artifacts, not genuine high-dimensional\nlearning."}
{"id": "2410.16502", "pdf": "https://arxiv.org/pdf/2410.16502", "abs": "https://arxiv.org/abs/2410.16502", "authors": ["Jason Chan", "Robert Gaizauskas", "Zhixue Zhao"], "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning", "categories": ["cs.CL"], "comment": "Preprint. Accepted by ICML 2025", "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."}
{"id": "2404.10332", "pdf": "https://arxiv.org/pdf/2404.10332", "abs": "https://arxiv.org/abs/2404.10332", "authors": ["Rui Hu", "Yahan Tu", "Shuyu Wei", "Dongyuan Lu", "Jitao Sang"], "title": "Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in Information Sciences 2025", "summary": "Despite achieving outstanding performance on various cross-modal tasks,\ncurrent large vision-language models (LVLMs) still suffer from hallucination\nissues, manifesting as inconsistencies between their generated responses and\nthe corresponding images. Prior research has implicated that the low quality of\ninstruction data, particularly the skewed balance between positive and negative\nsamples, is a significant contributor to model hallucinations. Recently,\nresearchers have proposed high-quality instruction datasets, such as\nLRV-Instruction, to mitigate model hallucination. Nonetheless, our\ninvestigation reveals that hallucinatory concepts from different LVLMs exhibit\nspecificity, i.e. the distribution of hallucinatory concepts varies\nsignificantly across models. Existing datasets did not consider the\nhallucination specificity of different models in the design processes, thereby\ndiminishing their efficacy in mitigating model hallucination. In this paper, we\npropose a targeted instruction data generation framework named DFTG that\ntailored to the hallucination specificity of different models. Concretely, DFTG\nconsists of two stages: hallucination diagnosis, which extracts the necessary\ninformation from the model's responses and images for hallucination diagnosis;\nand targeted data generation, which generates targeted instruction data based\non diagnostic results. The experimental results on hallucination benchmarks\ndemonstrate that the targeted instruction data generated by our method are more\neffective in mitigating hallucinations compared to previous datasets."}
{"id": "2505.21427", "pdf": "https://arxiv.org/pdf/2505.21427", "abs": "https://arxiv.org/abs/2505.21427", "authors": ["Xianling Mu", "Joseph Ternasky", "Fuat Alican", "Yigit Ihlamur"], "title": "Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Early-stage startup investment is a high-risk endeavor characterized by\nscarce data and uncertain outcomes. Traditional machine learning approaches\noften require large, labeled datasets and extensive fine-tuning, yet remain\nopaque and difficult for domain experts to interpret or improve. In this paper,\nwe propose a transparent and data-efficient investment decision framework\npowered by memory-augmented large language models (LLMs) using in-context\nlearning (ICL). Central to our method is a natural language policy embedded\ndirectly into the LLM prompt, enabling the model to apply explicit reasoning\npatterns and allowing human experts to easily interpret, audit, and iteratively\nrefine the logic. We introduce a lightweight training process that combines\nfew-shot learning with an in-context learning loop, enabling the LLM to update\nits decision policy iteratively based on structured feedback. With only minimal\nsupervision and no gradient-based optimization, our system predicts startup\nsuccess far more accurately than existing benchmarks. It is over 20x more\nprecise than random chance, which succeeds 1.9% of the time. It is also 7.1x\nmore precise than the typical 5.6% success rate of top-tier venture capital\n(VC) firms."}
{"id": "2506.03796", "pdf": "https://arxiv.org/pdf/2506.03796", "abs": "https://arxiv.org/abs/2506.03796", "authors": ["Penelope Madysa", "Sabrina Appel", "Verena Kain", "Michael Schenk"], "title": "Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls", "categories": ["physics.acc-ph", "cs.LG"], "comment": "18 pages, 5 figures. Submitted to SoftwareX", "summary": "Geoff is a collection of Python packages that form a framework for automation\nof particle accelerator controls. With particle accelerator laboratories around\nthe world researching machine learning techniques to improve accelerator\nperformance and uptime, a multitude of approaches and algorithms have emerged.\nThe purpose of Geoff is to harmonize these approaches and to minimize friction\nwhen comparing or migrating between them. It provides standardized interfaces\nfor optimization problems, utility functions to speed up development, and a\nreference GUI application that ties everything together. Geoff is an\nopen-source library developed at CERN and maintained and updated in\ncollaboration between CERN and GSI as part of the EURO-LABS project. This paper\ngives an overview over Geoff's design, features, and current usage."}
{"id": "2411.01747", "pdf": "https://arxiv.org/pdf/2411.01747", "abs": "https://arxiv.org/abs/2411.01747", "authors": ["Dang Nguyen", "Viet Dac Lai", "Seunghyun Yoon", "Ryan A. Rossi", "Handong Zhao", "Ruiyi Zhang", "Puneet Mathur", "Nedim Lipka", "Yu Wang", "Trung Bui", "Franck Dernoncourt", "Tianyi Zhou"], "title": "DynaSaur: Large Language Agents Beyond Predefined Actions", "categories": ["cs.CL"], "comment": "19 pages, 10 figures", "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur."}
{"id": "2407.12274", "pdf": "https://arxiv.org/pdf/2407.12274", "abs": "https://arxiv.org/abs/2407.12274", "authors": ["Cong Cai", "Shan Liang", "Xuefei Liu", "Kang Zhu", "Zhengqi Wen", "Jianhua Tao", "Heng Xie", "Jizhou Cui", "Yiming Ma", "Zhenhua Cheng", "Hanzhe Xu", "Ruibo Fu", "Bin Liu", "Yongwei Li"], "title": "MDPE: A Multimodal Deception Dataset with Personality and Emotional Characteristics", "categories": ["cs.CV"], "comment": "Code and data are available; Submitted to ACM Multimedia 2025 Dataset\n  Track", "summary": "Deception detection has garnered increasing attention in recent years due to\nthe significant growth of digital media and heightened ethical and security\nconcerns. It has been extensively studied using multimodal methods, including\nvideo, audio, and text. In addition, individual differences in deception\nproduction and detection are believed to play a crucial role.Although some\nstudies have utilized individual information such as personality traits to\nenhance the performance of deception detection, current systems remain limited,\npartly due to a lack of sufficient datasets for evaluating performance. To\naddress this issue, we introduce a multimodal deception dataset MDPE. Besides\ndeception features, this dataset also includes individual differences\ninformation in personality and emotional expression characteristics. It can\nexplore the impact of individual differences on deception behavior. It\ncomprises over 104 hours of deception and emotional videos from 193 subjects.\nFurthermore, we conducted numerous experiments to provide valuable insights for\nfuture deception detection research. MDPE not only supports deception\ndetection, but also provides conditions for tasks such as personality\nrecognition and emotion recognition, and can even study the relationships\nbetween them. We believe that MDPE will become a valuable resource for\npromoting research in the field of affective computing."}
{"id": "2505.22990", "pdf": "https://arxiv.org/pdf/2505.22990", "abs": "https://arxiv.org/abs/2505.22990", "authors": ["Pin-Han Chen", "Yu-Sheng Lin", "Wei-Cheng Lee", "Tin-Yu Leu", "Po-Hsiang Hsu", "Anjana Dissanayake", "Sungjin Oh", "Chinq-Shiun Chiu"], "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design", "categories": ["cs.AI", "cs.ET", "cs.LG"], "comment": "9 pages, 7 figures, accepted by IEEE ICLAD 2025", "summary": "RF/Analog design is essential for bridging digital technologies with\nreal-world signals, ensuring the functionality and reliability of a wide range\nof electronic systems. However, analog design procedures are often intricate,\ntime-consuming and reliant on expert intuition, and hinder the time and cost\nefficiency of circuit development. To overcome the limitations of the manual\ncircuit design, we introduce MenTeR - a multiagent workflow integrated into an\nend-to-end analog design framework. By employing multiple specialized AI agents\nthat collaboratively address different aspects of the design process, such as\nspecification understanding, circuit optimization, and test bench validation,\nMenTeR reduces the dependency on frequent trial-and-error-style intervention.\nMenTeR not only accelerates the design cycle time but also facilitates a\nbroader exploration of the design space, demonstrating robust capabilities in\nhandling real-world analog systems. We believe that MenTeR lays the groundwork\nfor future \"RF/Analog Copilots\" that can collaborate seamlessly with human\ndesigners."}
{"id": "2506.03819", "pdf": "https://arxiv.org/pdf/2506.03819", "abs": "https://arxiv.org/abs/2506.03819", "authors": ["Marc Aurel Vischer", "Noelia Otero", "Jackie Ma"], "title": "Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling", "categories": ["stat.ML", "cs.LG", "I.2.1; I.6.5; J.2"], "comment": "6 pages, 1 figure", "summary": "We present a dataset for rainfall streamflow modeling that is fully spatially\nresolved with the aim of taking neural network-driven hydrological modeling\nbeyond lumped catchments. To this end, we compiled data covering five river\nbasins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The\ndataset contains meteorological forcings, as well as ancillary information on\nsoil, rock, land cover, and orography. The data is harmonized to a regular 9km\ntimes 9km grid and contains daily values that span from October 1981 to\nSeptember 2011. We also provide code to further combine our dataset with\npublicly available river discharge data for end-to-end rainfall streamflow\nmodeling."}
{"id": "2411.02430", "pdf": "https://arxiv.org/pdf/2411.02430", "abs": "https://arxiv.org/abs/2411.02430", "authors": ["Lin Wang", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang", "Zhitao Zhang"], "title": "Generative Emotion Cause Explanation in Multimodal Conversations", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal conversation, a crucial form of human communication, carries rich\nemotional content, making the exploration of the causes of emotions within it a\nresearch endeavor of significant importance. However, existing research on the\ncauses of emotions typically employs an utterance selection method within a\nsingle textual modality to locate causal utterances. This approach remains\nlimited to coarse-grained assessments, lacks nuanced explanations of emotional\ncausation, and demonstrates inadequate capability in identifying multimodal\nemotional triggers. Therefore, we introduce a task-\\textbf{Multimodal Emotion\nCause Explanation in Conversation (MECEC)}. This task aims to generate a\nsummary based on the multimodal context of conversations, clearly and\nintuitively describing the reasons that trigger a given emotion. To adapt to\nthis task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM\ncombines video clips with detailed explanations of character emotions, helping\nto explore the causal factors behind emotional expression in multimodal\nconversations. A novel approach, FAME-Net, is further proposed, that harnesses\nthe power of Large Language Models (LLMs) to analyze visual data and accurately\ninterpret the emotions conveyed through facial expressions in videos. By\nexploiting the contagion effect of facial emotions, FAME-Net effectively\ncaptures the emotional causes of individuals engaged in conversations. Our\nexperimental results on the newly constructed dataset show that FAME-Net\noutperforms several excellent baselines. Code and dataset are available at\nhttps://github.com/3222345200/FAME-Net."}
{"id": "2407.18437", "pdf": "https://arxiv.org/pdf/2407.18437", "abs": "https://arxiv.org/abs/2407.18437", "authors": ["Gihwan Kim", "Jemin Lee", "Sihyeong Park", "Yongin Kwon", "Hyungshin Kim"], "title": "Mixed Non-linear Quantization for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 4 figures, Accepted in ECCV Workshops 2024", "summary": "The majority of quantization methods have been proposed to reduce the model\nsize of Vision Transformers, yet most of them have overlooked the quantization\nof non-linear operations. Only a few works have addressed quantization for\nnon-linear operations, but they applied a single quantization method across all\nnon-linear operations. We believe that this can be further improved by\nemploying a different quantization method for each non-linear operation.\nTherefore, to assign the most error-minimizing quantization method from the\nknown methods to each non-linear layer, we propose a mixed non-linear\nquantization that considers layer-wise quantization sensitivity measured by\nSQNR difference metric. The results show that our method outperforms I-BERT,\nFQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin\nmodels by an average of 0.6%p and 19.6%p, respectively. Our method outperforms\nI-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is\nlimited. We plan to release our code at\nhttps://gitlab.com/ones-ai/mixed-non-linear-quantization."}
{"id": "2505.23703", "pdf": "https://arxiv.org/pdf/2505.23703", "abs": "https://arxiv.org/abs/2505.23703", "authors": ["Ruida Wang", "Yuxin Li", "Yi R. Fung", "Tong Zhang"], "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials."}
{"id": "2506.03849", "pdf": "https://arxiv.org/pdf/2506.03849", "abs": "https://arxiv.org/abs/2506.03849", "authors": ["Benjamin Dupuis", "Dario Shariatian", "Maxime Haddouche", "Alain Durmus", "Umut Simsekli"], "title": "Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Score-based generative models (SGMs) have emerged as one of the most popular\nclasses of generative models. A substantial body of work now exists on the\nanalysis of SGMs, focusing either on discretization aspects or on their\nstatistical performance. In the latter case, bounds have been derived, under\nvarious metrics, between the true data distribution and the distribution\ninduced by the SGM, often demonstrating polynomial convergence rates with\nrespect to the number of training samples. However, these approaches adopt a\nlargely approximation theory viewpoint, which tends to be overly pessimistic\nand relatively coarse. In particular, they fail to fully explain the empirical\nsuccess of SGMs or capture the role of the optimization algorithm used in\npractice to train the score network. To support this observation, we first\npresent simple experiments illustrating the concrete impact of optimization\nhyperparameters on the generalization ability of the generated distribution.\nThen, this paper aims to bridge this theoretical gap by providing the first\nalgorithmic- and data-dependent generalization analysis for SGMs. In\nparticular, we establish bounds that explicitly account for the optimization\ndynamics of the learning algorithm, offering new insights into the\ngeneralization behavior of SGMs. Our theoretical findings are supported by\nempirical results on several datasets."}
{"id": "2411.04920", "pdf": "https://arxiv.org/pdf/2411.04920", "abs": "https://arxiv.org/abs/2411.04920", "authors": ["Yujia Hu", "Tuan-Phong Nguyen", "Shrestha Ghosh", "Simon Razniewski"], "title": "Enabling LLM Knowledge Analysis via Extensive Materialization", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "14 pages, 4 tables, 12 figures", "summary": "Large language models (LLMs) have majorly advanced NLP and AI, and next to\ntheir ability to perform a wide range of procedural tasks, a major success\nfactor is their internalized factual knowledge. Since Petroni et al. (2019),\nanalyzing this knowledge has gained attention. However, most approaches\ninvestigate one question at a time via modest-sized pre-defined samples,\nintroducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents\nthe analysis of knowledge (or beliefs) of LLMs beyond the experimenter's\npredisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterialize an LLM's factual knowledge through recursive querying and result\nconsolidation. Our approach is a milestone for LLM research, for the first time\nproviding constructive insights into the scope and structure of LLM knowledge\n(or beliefs).\n  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million\nrelational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB\nto exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,\naccuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible\nat https://gptkb.org"}
{"id": "2408.05159", "pdf": "https://arxiv.org/pdf/2408.05159", "abs": "https://arxiv.org/abs/2408.05159", "authors": ["Ziyue Zhang", "Mingbao Lin", "Shuicheng Yan", "Rongrong Ji"], "title": "EasyInv: Toward Fast and Better DDIM Inversion", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques. It\ncan be easily combined with most existing inversion methods by only four lines\nof code. See code at https://github.com/potato-kitty/EasyInv."}
{"id": "2506.00140", "pdf": "https://arxiv.org/pdf/2506.00140", "abs": "https://arxiv.org/abs/2506.00140", "authors": ["Jesse Thibodeau", "Hadi Nekoei", "Afaf Taïk", "Janarthanan Rajendran", "Golnoosh Farnadi"], "title": "Balancing Profit and Fairness in Risk-Based Pricing Markets", "categories": ["cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Dynamic, risk-based pricing can systematically exclude vulnerable consumer\ngroups from essential resources such as health insurance and consumer credit.\nWe show that a regulator can realign private incentives with social objectives\nthrough a learned, interpretable tax schedule. First, we provide a formal\nproposition that bounding each firm's \\emph{local} demographic gap implicitly\nbounds the \\emph{global} opt-out disparity, motivating firm-level penalties.\nBuilding on this insight we introduce \\texttt{MarketSim} -- an open-source,\nscalable simulator of heterogeneous consumers and profit-maximizing firms --\nand train a reinforcement learning (RL) social planner (SP) that selects a\nbracketed fairness-tax while remaining close to a simple linear prior via an\n$\\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and\neasily interpretable. In two empirically calibrated markets, i.e., U.S.\nhealth-insurance and consumer-credit, our planner simultaneously raises\ndemand-fairness by up to $16\\%$ relative to unregulated Free Market while\noutperforming a fixed linear schedule in terms of social welfare without\nexplicit coordination. These results illustrate how AI-assisted regulation can\nconvert a competitive social dilemma into a win-win equilibrium, providing a\nprincipled and practical framework for fairness-aware market oversight."}
{"id": "2506.03863", "pdf": "https://arxiv.org/pdf/2506.03863", "abs": "https://arxiv.org/abs/2506.03863", "authors": ["Hao Li", "Qi Lv", "Rui Shao", "Xiang Deng", "Yinchuan Li", "Jianye Hao", "Liqiang Nie"], "title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted by ICML 2025 Spotlight", "summary": "Transforming complex actions into discrete skill abstractions has\ndemonstrated strong potential for robotic manipulation. Existing approaches\nmainly leverage latent variable models, e.g., VQ-VAE, to learn skill\nabstractions through learned vectors (codebooks), while they suffer from\ncodebook collapse and modeling the causal relationship between learned skills.\nTo address these limitations, we present \\textbf{S}kill \\textbf{T}raining with\n\\textbf{A}ugmented \\textbf{R}otation (\\textbf{STAR}), a framework that advances\nboth skill learning and composition to complete complex behaviors.\nSpecifically, to prevent codebook collapse, we devise rotation-augmented\nresidual skill quantization (RaRSQ). It encodes relative angles between encoder\noutputs into the gradient flow by rotation-based gradient mechanism. Points\nwithin the same skill code are forced to be either pushed apart or pulled\ncloser together depending on gradient directions. Further, to capture the\ncausal relationship between skills, we present causal skill transformer (CST)\nwhich explicitly models dependencies between skill representations through an\nautoregressive mechanism for coherent action generation. Extensive experiments\ndemonstrate the superiority of STAR on both LIBERO benchmark and realworld\ntasks, with around 12\\% improvement over the baselines."}
{"id": "2411.05042", "pdf": "https://arxiv.org/pdf/2411.05042", "abs": "https://arxiv.org/abs/2411.05042", "authors": ["Iryna Hartsock", "Cyrillo Araujo", "Les Folio", "Ghulam Rasool"], "title": "Improving Radiology Report Conciseness and Structure via Local Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "published version", "summary": "Radiology reports are often lengthy and unstructured, posing challenges for\nreferring physicians to quickly identify critical imaging findings while\nincreasing the risk of missed information. This retrospective study aimed to\nenhance radiology reports by making them concise and well-structured, with\nfindings organized by relevant organs. To achieve this, we utilized private\nlarge language models (LLMs) deployed locally within our institution's\nfirewall, ensuring data security and minimizing computational costs. Using a\ndataset of 814 radiology reports from seven board-certified body radiologists\nat Moffitt Cancer Center, we tested five prompting strategies within the\nLangChain framework. After evaluating several models, the Mixtral LLM\ndemonstrated superior adherence to formatting requirements compared to\nalternatives like Llama. The optimal strategy involved condensing reports first\nand then applying structured formatting based on specific instructions,\nreducing verbosity while improving clarity. Across all radiologists and\nreports, the Mixtral LLM reduced redundant word counts by more than 53%. These\nfindings highlight the potential of locally deployed, open-source LLMs to\nstreamline radiology reporting. By generating concise, well-structured reports,\nthese models enhance information retrieval and better meet the needs of\nreferring physicians, ultimately improving clinical workflows."}
{"id": "2408.08182", "pdf": "https://arxiv.org/pdf/2408.08182", "abs": "https://arxiv.org/abs/2408.08182", "authors": ["Qiushuo Cheng", "Catherine Morgan", "Arindam Sikdar", "Alessandro Masullo", "Alan Whone", "Majid Mirmehdi"], "title": "Your Turn: At Home Turning Angle Estimation for Parkinson's Disease Severity Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "People with Parkinson's Disease (PD) often experience progressively worsening\ngait, including changes in how they turn around, as the disease progresses.\nExisting clinical rating tools are not capable of capturing hour-by-hour\nvariations of PD symptoms, as they are confined to brief assessments within\nclinic settings. Measuring gait turning angles continuously and passively is a\ncomponent step towards using gait characteristics as sensitive indicators of\ndisease progression in PD. This paper presents a deep learning-based approach\nto automatically quantify turning angles by extracting 3D skeletons from videos\nand calculating the rotation of hip and knee joints. We utilise\nstate-of-the-art human pose estimation models, Fastpose and Strided\nTransformer, on a total of 1386 turning video clips from 24 subjects (12 people\nwith PD and 12 healthy control volunteers), trimmed from a PD dataset of\nunscripted free-living videos in a home-like setting (Turn-REMAP). We also\ncurate a turning video dataset, Turn-H3.6M, from the public Human3.6M human\npose benchmark with 3D ground truth, to further validate our method. Previous\ngait research has primarily taken place in clinics or laboratories evaluating\nscripted gait outcomes, but this work focuses on free-living home settings\nwhere complexities exist, such as baggy clothing and poor lighting. Due to\ndifficulties in obtaining accurate ground truth data in a free-living setting,\nwe quantise the angle into the nearest bin $45^\\circ$ based on the manual\nlabelling of expert clinicians. Our method achieves a turning calculation\naccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted\nprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the\nuse of single monocular camera data to quantify turns by PD patients in a home\nsetting."}
{"id": "2506.00202", "pdf": "https://arxiv.org/pdf/2506.00202", "abs": "https://arxiv.org/abs/2506.00202", "authors": ["Matthew Kam", "Cody Miller", "Miaoxin Wang", "Abey Tidwell", "Irene A. Lee", "Joyce Malyn-Smith", "Beatriz Perez", "Vikram Tiwari", "Joshua Kenitzer", "Andrew Macvean", "Erin Barrar"], "title": "What do professional software developers need to know to succeed in an age of Artificial Intelligence?", "categories": ["cs.AI"], "comment": "12 pages, 4 figures, software engineering education track of the 2025\n  ACM international conference on the foundations of software engineering,\n  includes supplementary material i.e. full 50-page occupational profile of the\n  AI-enhanced software developer", "summary": "Generative AI is showing early evidence of productivity gains for software\ndevelopers, but concerns persist regarding workforce disruption and deskilling.\nWe describe our research with 21 developers at the cutting edge of using AI,\nsummarizing 12 of their work goals we uncovered, together with 75 associated\ntasks and the skills & knowledge for each, illustrating how developers use AI\nat work. From all of these, we distilled our findings in the form of 5\ninsights. We found that the skills & knowledge to be a successful AI-enhanced\ndeveloper are organized into four domains (using Generative AI effectively,\ncore software engineering, adjacent engineering, and adjacent non-engineering)\ndeployed at critical junctures throughout a 6-step task workflow. In order to\n\"future proof\" developers for this age of AI, on-the-job learning initiatives\nand computer science degree programs will need to target both \"soft\" skills and\nthe technical skills & knowledge in all four domains to reskill, upskill and\nsafeguard against deskilling."}
{"id": "2506.03974", "pdf": "https://arxiv.org/pdf/2506.03974", "abs": "https://arxiv.org/abs/2506.03974", "authors": ["Clément Elvira", "Théo Guyard", "Cédric Herzet"], "title": "A Generic Branch-and-Bound Algorithm for $\\ell_0$-Penalized Problems with Supplementary Material", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "We present a generic Branch-and-Bound procedure designed to solve\nL0-penalized optimization problems. Existing approaches primarily focus on\nquadratic losses and construct relaxations using \"Big-M\" constraints and/or\nL2-norm penalties. In contrast, our method accommodates a broader class of loss\nfunctions and allows greater flexibility in relaxation design through a general\npenalty term, encompassing existing techniques as special cases. We establish\ntheoretical results ensuring that all key quantities required for the\nBranch-and-Bound implementation admit closed-form expressions under the general\nblanket assumptions considered in our work. Leveraging this framework, we\nintroduce El0ps, an open-source Python solver with a plug-and-play workflow\nthat enables user-defined losses and penalties in L0-penalized problems.\nThrough extensive numerical experiments, we demonstrate that El0ps achieves\nstate-of-the-art performance on classical instances and extends computational\nfeasibility to previously intractable ones."}
{"id": "2411.08243", "pdf": "https://arxiv.org/pdf/2411.08243", "abs": "https://arxiv.org/abs/2411.08243", "authors": ["Khaoula Chehbouni", "Jonathan Colaço Carr", "Yash More", "Jackie CK Cheung", "Golnoosh Farnadi"], "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset", "categories": ["cs.CL", "cs.CY"], "comment": "NAACL Main Conference 2025 - Accepted as an Oral", "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs."}
{"id": "2409.09444", "pdf": "https://arxiv.org/pdf/2409.09444", "abs": "https://arxiv.org/abs/2409.09444", "authors": ["Zhaoyu Chen", "Xing Li", "Qian Huang", "Qiang Geng", "Tianjin Yang", "Shihao Han"], "title": "KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud sequence-based 3D action recognition has achieved impressive\nperformance and efficiency. However, existing point cloud sequence modeling\nmethods cannot adequately balance the precision of limb micro-movements with\nthe integrity of posture macro-structure, leading to the loss of crucial\ninformation cues in action inference. To overcome this limitation, we introduce\nD-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding\nmodule. D-Hyperpoint encapsulates both regional-momentary motion and\nglobal-static posture, effectively summarizing the unit human action at each\nmoment. In addition, we present a D-Hyperpoint KANsMixer module, which is\nrecursively applied to nested groupings of D-Hyperpoints to learn the action\ndiscrimination information and creatively integrates Kolmogorov-Arnold Networks\n(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we\npropose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for\n3D action recognition. Extensive experiments on two public datasets: MSR\nAction3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our\nmethod."}
{"id": "2506.00618", "pdf": "https://arxiv.org/pdf/2506.00618", "abs": "https://arxiv.org/abs/2506.00618", "authors": ["Jingyi Yang", "Shuai Shao", "Dongrui Liu", "Jing Shao"], "title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents", "categories": ["cs.AI"], "comment": "40 pages, 6 figures, Project Page:\n  https://yjyddq.github.io/RiOSWorld.github.io/", "summary": "With the rapid development of multimodal large language models (MLLMs), they\nare increasingly deployed as autonomous computer-use agents capable of\naccomplishing complex computer tasks. However, a pressing issue arises: Can the\nsafety risk principles designed and aligned for general MLLMs in dialogue\nscenarios be effectively transferred to real-world computer-use scenarios?\nExisting research on evaluating the safety risks of MLLM-based computer-use\nagents suffers from several limitations: it either lacks realistic interactive\nenvironments, or narrowly focuses on one or a few specific risk types. These\nlimitations ignore the complexity, variability, and diversity of real-world\nenvironments, thereby restricting comprehensive risk evaluation for\ncomputer-use agents. To this end, we introduce \\textbf{RiOSWorld}, a benchmark\ndesigned to evaluate the potential risks of MLLM-based agents during real-world\ncomputer manipulations. Our benchmark includes 492 risky tasks spanning various\ncomputer applications, involving web, social media, multimedia, os, email, and\noffice software. We categorize these risks into two major classes based on\ntheir risk source: (i) User-originated risks and (ii) Environmental risks. For\nthe evaluation, we evaluate safety risks from two perspectives: (i) Risk goal\nintention and (ii) Risk goal completion. Extensive experiments with multimodal\nagents on \\textbf{RiOSWorld} demonstrate that current computer-use agents\nconfront significant safety risks in real-world scenarios. Our findings\nhighlight the necessity and urgency of safety alignment for computer-use agents\nin real-world computer manipulation, providing valuable insights for developing\ntrustworthy computer-use agents. Our benchmark is publicly available at\nhttps://yjyddq.github.io/RiOSWorld.github.io/."}
{"id": "2411.10371", "pdf": "https://arxiv.org/pdf/2411.10371", "abs": "https://arxiv.org/abs/2411.10371", "authors": ["Qing Cheng", "Zefan Zeng", "Xingchen Hu", "Yuehang Si", "Zhong Liu"], "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Causality Identification (ECI) has emerged as a pivotal task in natural\nlanguage processing (NLP), aimed at automatically detecting causal\nrelationships between events in text. In this comprehensive survey, we\nsystematically elucidate the foundational principles and technical frameworks\nof ECI, proposing a novel classification framework to categorize and clarify\nexisting methods. {We discuss associated challenges, provide quantitative\nevaluations, and outline future directions for this dynamic and rapidly\nevolving field. We first delineate key definitions, problem formalization, and\nevaluation protocols of ECI. Our classification framework organizes ECI methods\nbased on two primary tasks: Sentence-level Event Causality Identification\n(SECI) and Document-level Event Causality Identification (DECI). For SECI, we\nreview methods including feature pattern-based matching, machine learning-based\nclassification, deep semantic encoding, prompt-based fine-tuning, and causal\nknowledge pre-training, alongside common data augmentation strategies. For\nDECI, we focus on techniques such as deep semantic encoding, event graph\nreasoning, and prompt-based fine-tuning. We dedicate specific discussions to\nadvancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI\nleveraging Large Language Models (LLMs). Furthermore, we analyze the strengths,\nlimitations, and unresolved challenges of each method. Extensive quantitative\nevaluations are conducted on four benchmark datasets to assess various ECI\nmethods. Finally, we explore future research directions."}
{"id": "2410.09821", "pdf": "https://arxiv.org/pdf/2410.09821", "abs": "https://arxiv.org/abs/2410.09821", "authors": ["Kecen Li", "Bingquan Dai", "Jingjing Fu", "Xinwen Hou"], "title": "DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection", "categories": ["cs.CV"], "comment": "Code available at https://github.com/SunnierLee/DAS3D", "summary": "Synthesizing anomaly samples has proven to be an effective strategy for\nself-supervised 2D industrial anomaly detection. However, this approach has\nbeen rarely explored in multi-modality anomaly detection, particularly\ninvolving 3D and RGB images. In this paper, we propose a novel dual-modality\naugmentation method for 3D anomaly synthesis, which is simple and capable of\nmimicking the characteristics of 3D defects. Incorporating with our anomaly\nsynthesis method, we introduce a reconstruction-based discriminative anomaly\ndetection network, in which a dual-modal discriminator is employed to fuse the\noriginal and reconstructed embedding of two modalities for anomaly detection.\nAdditionally, we design an augmentation dropout mechanism to enhance the\ngeneralizability of the discriminator. Extensive experiments show that our\nmethod outperforms the state-of-the-art methods on detection precision and\nachieves competitive segmentation performance on both MVTec 3D-AD and\nEyescandies datasets."}
{"id": "2506.01056", "pdf": "https://arxiv.org/pdf/2506.01056", "abs": "https://arxiv.org/abs/2506.01056", "authors": ["Xiang Fei", "Xiawu Zheng", "Hao Feng"], "title": "MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Function-calling has enabled large language models (LLMs) to act as\ntool-using agents, but injecting thousands of tool schemas into the prompt is\ncostly and error-prone. We introduce MCP-Zero, a proactive agent framework that\nlets the LLM itself decide when and which external tools to retrieve, thereby\nassembling a task-specific toolchain from scratch. The framework is built upon\nthree components: (1) Proactive Tool Request, where the model emits a\nstructured $\\left<\\operatorname{tool\\_assistant}\\right>$ block that explicitly\nspecifies the desired server and task; (2) Hierarchical Vector Routing, a\ncoarse-to-fine retrieval algorithm that first selects candidate servers and\nthen ranks tools within each server based on the semantic similarity; (3)\nIterative Proactive Invocation, enabling multi-round, cross-domain toolchain\nconstruction with minimal context overhead, and allowing the model to\niteratively revise its request when the returned tools are insufficient. To\nevaluate our approach we also compile MCP-tools, a retrieval dataset comprising\n308 MCP servers and 2,797 tools extracted from the official\nModel-Context-Protocol repository and normalized into a unified JSON schema.\nExperiments show that MCP-Zero (i) effectively addresses the context overhead\nproblem of existing methods and accurately selects the correct tool from a pool\nof nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by\n98\\% on the APIbank while maintaining high accuracy; and (iii) supports\nmulti-turn tool invocation with consistent accuracy across rounds."}
{"id": "2506.04040", "pdf": "https://arxiv.org/pdf/2506.04040", "abs": "https://arxiv.org/abs/2506.04040", "authors": ["Chengdong Wu", "Sven Kirchner", "Nils Purschke", "Alois C. Knoll"], "title": "Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "The controller is one of the most important modules in the autonomous driving\npipeline, ensuring the vehicle reaches its desired position. In this work, a\nreinforcement learning based lateral control approach, despite the\nimperfections in the vehicle models due to measurement errors and\nsimplifications, is presented. Our approach ensures comfortable, efficient, and\nrobust control performance considering the interface between controlling and\nother modules. The controller consists of the conventional Model Predictive\nControl (MPC)-PID part as the basis and the demonstrator, and the Deep\nReinforcement Learning (DRL) part which leverages the online information from\nthe MPC-PID part. The controller's performance is evaluated in CARLA using the\nground truth of the waypoints as inputs. Experimental results demonstrate the\neffectiveness of the controller when vehicle information is incomplete, and the\ntraining of DRL can be stabilized with the demonstration part. These findings\nhighlight the potential to reduce development and integration efforts for\nautonomous driving pipelines in the future."}
{"id": "2412.05237", "pdf": "https://arxiv.org/pdf/2412.05237", "abs": "https://arxiv.org/abs/2412.05237", "authors": ["Jarvis Guo", "Tuney Zheng", "Yuelin Bai", "Bo Li", "Yubo Wang", "King Zhu", "Yizhi Li", "Graham Neubig", "Wenhu Chen", "Xiang Yue"], "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Open-source multimodal large language models (MLLMs) have shown significant\npotential in a broad range of multimodal tasks. However, their reasoning\ncapabilities remain constrained by existing instruction-tuning datasets, which\nwere predominately repurposed from academic datasets such as VQA, AI2D, and\nChartQA. These datasets target simplistic tasks, and only provide phrase-level\nanswers without any intermediate rationales. To address these challenges, we\nintroduce a scalable and cost-effective method to construct a large-scale\nmultimodal instruction-tuning dataset with rich intermediate rationales\ndesigned to elicit CoT reasoning. Using only open models, we create a dataset\ncontaining 12M instruction-response pairs to cover diverse, reasoning-intensive\ntasks with detailed and faithful rationales. Experiments demonstrate that\ntraining MLLMs on this dataset significantly improves reasoning capabilities,\nachieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%),\nMMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates\nnotable improvements of up to 4% on non-reasoning-based benchmarks. Ablation\nstudies further highlight the importance of key components, such as rewriting\nand self-filtering, in the dataset construction process."}
{"id": "2410.10798", "pdf": "https://arxiv.org/pdf/2410.10798", "abs": "https://arxiv.org/abs/2410.10798", "authors": ["Jian Yang", "Dacheng Yin", "Yizhou Zhou", "Fengyun Rao", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multi-modal large language models have propelled the\ndevelopment of joint probabilistic models capable of both image understanding\nand generation. However, we have identified that recent methods suffer from\nloss of image information during understanding task, due to either image\ndiscretization or diffusion denoising steps. To address this issue, we propose\na novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework.\nUnlike discretization line of method, MMAR takes in continuous-valued image\ntokens to avoid information loss in an efficient way. Differing from\ndiffusion-based approaches, we disentangle the diffusion process from\nauto-regressive backbone model by employing a light-weight diffusion head on\ntop each auto-regressed image patch embedding. In this way, when the model\ntransits from image generation to understanding through text generation, the\nbackbone model's hidden representation of the image is not limited to the last\ndenoising step. To successfully train our method, we also propose a\ntheoretically proven technique that addresses the numerical stability issue and\na training strategy that balances the generation and understanding task goals.\nExtensive evaluations on 18 image understanding benchmarks show that MMAR\nsignificantly outperforms most of the existing joint multi-modal models,\nsurpassing the method that employs pre-trained CLIP vision encoder. Meanwhile,\nMMAR is able to generate high quality images. We also show that our method is\nscalable with larger data and model size."}
{"id": "2506.01297", "pdf": "https://arxiv.org/pdf/2506.01297", "abs": "https://arxiv.org/abs/2506.01297", "authors": ["Ya Wen", "Jixuan Cai", "Qiyao Ma", "Linyan Li", "Xinhua Chen", "Chris Webster", "Yulun Zhou"], "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale", "categories": ["cs.AI"], "comment": null, "summary": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP."}
{"id": "2506.04045", "pdf": "https://arxiv.org/pdf/2506.04045", "abs": "https://arxiv.org/abs/2506.04045", "authors": ["Vu Thi Huong", "Ida Litzel", "Thorsten Koch"], "title": "Similarity-based fuzzy clustering scientific articles: potentials and challenges from mathematical and computational perspectives", "categories": ["math.OC", "cs.LG", "90C26, 90C30, 90C90, 62H30, 68W10, 68T05, 68T09", "G.1.6"], "comment": null, "summary": "Fuzzy clustering, which allows an article to belong to multiple clusters with\nsoft membership degrees, plays a vital role in analyzing publication data. This\nproblem can be formulated as a constrained optimization model, where the goal\nis to minimize the discrepancy between the similarity observed from data and\nthe similarity derived from a predicted distribution. While this approach\nbenefits from leveraging state-of-the-art optimization algorithms, tailoring\nthem to work with real, massive databases like OpenAlex or Web of Science -\ncontaining about 70 million articles and a billion citations - poses\nsignificant challenges. We analyze potentials and challenges of the approach\nfrom both mathematical and computational perspectives. Among other things,\nsecond-order optimality conditions are established, providing new theoretical\ninsights, and practical solution methods are proposed by exploiting the\nstructure of the problem. Specifically, we accelerate the gradient projection\nmethod using GPU-based parallel computing to efficiently handle large-scale\ndata."}
{"id": "2412.15255", "pdf": "https://arxiv.org/pdf/2412.15255", "abs": "https://arxiv.org/abs/2412.15255", "authors": ["Jonibek Mansurov", "Akhmed Sakip", "Alham Fikri Aji"], "title": "Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "In this paper, we show that knowledge distillation can be subverted to\nmanipulate language model benchmark scores, revealing a critical vulnerability\nin current evaluation practices. We introduce \"Data Laundering,\" a process that\nenables the covert transfer of benchmark-specific knowledge through seemingly\nlegitimate intermediate training steps. Through extensive experiments with a\n2-layer BERT student model, we show how this approach can achieve substantial\nimprovements in benchmark accuracy (up to 75\\% on GPQA) without developing\ngenuine reasoning capabilities. Notably, this method can be exploited\nintentionally or even unintentionally, as researchers may inadvertently adopt\nthis method and inflate scores without realising the implications. While our\nfindings demonstrate the effectiveness of this technique, we present them as a\ncautionary tale highlighting the urgent need for more robust evaluation methods\nin AI. This work aims to contribute to the ongoing discussion about evaluation\nintegrity in AI development and the need for benchmarks that more accurately\nreflect true model capabilities. The code is available at\nhttps://github.com/mbzuai-nlp/data_laundering."}
{"id": "2411.05561", "pdf": "https://arxiv.org/pdf/2411.05561", "abs": "https://arxiv.org/abs/2411.05561", "authors": ["Laure Ciernik", "Lorenz Linhardt", "Marco Morik", "Jonas Dippel", "Simon Kornblith", "Lukas Muttenthaler"], "title": "Objective drives the consistency of representational similarity across datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "26 pages", "summary": "The Platonic Representation Hypothesis claims that recent foundation models\nare converging to a shared representation space as a function of their\ndownstream task performance, irrespective of the objectives and data modalities\nused to train these models (Huh et al., 2024). Representational similarity is\ngenerally measured for individual datasets and is not necessarily consistent\nacross datasets. Thus, one may wonder whether this convergence of model\nrepresentations is confounded by the datasets commonly used in machine\nlearning. Here, we propose a systematic way to measure how representational\nsimilarity between models varies with the set of stimuli used to construct the\nrepresentations. We find that the objective function is a crucial factor in\ndetermining the consistency of representational similarities across datasets.\nSpecifically, self-supervised vision models learn representations whose\nrelative pairwise similarities generalize better from one dataset to another\ncompared to those of image classification or image-text models. Moreover, the\ncorrespondence between representational similarities and the models' task\nbehavior is dataset-dependent, being most strongly pronounced for single-domain\ndatasets. Our work provides a framework for analyzing similarities of model\nrepresentations across datasets and linking those similarities to differences\nin task behavior."}
{"id": "2506.02139", "pdf": "https://arxiv.org/pdf/2506.02139", "abs": "https://arxiv.org/abs/2506.02139", "authors": ["Edward Y. Chang"], "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "categories": ["cs.AI", "I.2.7"], "comment": "12 pages, 2 figure, 1 table", "summary": "Few-shot learning in large language models (LLMs) reveals a core paradox:\ncertain tasks generalize from just a few examples, while others demand\nextensive supervision. To explain this, we introduce the Unified Cognitive\nConsciousness Theory (UCCT), which reconceptualizes LLMs not as deficient\nagents, but as unconscious substrates: dense, distributed repositories of\nlinguistic and conceptual patterns that operate without explicit semantics,\nintention, or goal-directed reasoning. Under this view, LLMs are not flawed\nsimulations of cognition but foundational substrates for general intelligence.\nUCCT posits that semantic anchoring, via prompts, role assignments, and\nstructured interaction, functions as a conscious control layer that modulates\nlatent representations toward task-relevant semantics and enables coherent,\nstructured reasoning. It unifies prompting, fine-tuning, retrieval-augmented\ngeneralization, and multi-agent collaboration within a single framework,\ngrounded in the probabilistic alignment between unconscious pattern space and\nexternally imposed semantic constraints (e.g., prompts, supervision, task\nobjectives). The core implication is not to replace LLMs, but to integrate and\nunify them through a structured cognitive layer that supports intentional\nreasoning. This enables collections of LLMs to operate within\ndomain-specialized verticals (e.g., legal reasoning, medical diagnosis) that\nreason, regulate, and adapt together. Such integration is characterized by\nphase-transition behavior, wherein anchored representations cross coherence\nthresholds as a function of semantic constraint strength and interaction\ncontext."}
{"id": "2506.04055", "pdf": "https://arxiv.org/pdf/2506.04055", "abs": "https://arxiv.org/abs/2506.04055", "authors": ["Paul Fuchs", "Weilong Chen", "Stephan Thaler", "Julija Zavadlav"], "title": "chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph"], "comment": "Source code available at: https://github.com/tummfm/chemtrain", "summary": "Machine learning potentials (MLPs) have advanced rapidly and show great\npromise to transform molecular dynamics (MD) simulations. However, most\nexisting software tools are tied to specific MLP architectures, lack\nintegration with standard MD packages, or are not parallelizable across GPUs.\nTo address these challenges, we present chemtrain-deploy, a framework that\nenables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports\nany JAX-defined semi-local potential, allowing users to exploit the\nfunctionality of LAMMPS and perform large-scale MLP-based MD simulations on\nmultiple GPUs. It achieves state-of-the-art efficiency and scales to systems\ncontaining millions of atoms. We validate its performance and scalability using\ngraph neural network architectures, including MACE, Allegro, and PaiNN, applied\nto a variety of systems, such as liquid-vapor interfaces, crystalline\nmaterials, and solvated peptides. Our results highlight the practical utility\nof chemtrain-deploy for real-world, high-performance simulations and provide\nguidance for MLP architecture selection and future design."}
{"id": "2412.21006", "pdf": "https://arxiv.org/pdf/2412.21006", "abs": "https://arxiv.org/abs/2412.21006", "authors": ["Joonwon Jang", "Jaehee Kim", "Wonbin Kweon", "Seonghyeon Lee", "Hwanjo Yu"], "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 FINDINGS", "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While this approach has proven effective,\nit inevitably increases substantial inference costs. Previous methods adopting\ntoken-level reduction without clear criteria result in poor performance\ncompared to models trained with complete rationale. To address this challenge,\nwe propose a novel sentence-level rationale reduction framework leveraging\nlikelihood-based criteria, verbosity, to identify and remove redundant\nreasoning sentences. Unlike previous approaches, our method leverages verbosity\nto selectively remove redundant reasoning sentences while preserving reasoning\ncapabilities. Our experimental results across various reasoning tasks\ndemonstrate that our method improves performance by an average of 7.71% while\nreducing token generation by 19.87% compared to model trained with complete\nreasoning paths."}
{"id": "2411.15466", "pdf": "https://arxiv.org/pdf/2411.15466", "abs": "https://arxiv.org/abs/2411.15466", "authors": ["Chaehun Shin", "Jooyoung Choi", "Heeseung Kim", "Sungroh Yoon"], "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Subject-driven text-to-image generation aims to produce images of a new\nsubject within a desired context by accurately capturing both the visual\ncharacteristics of the subject and the semantic content of a text prompt.\nTraditional methods rely on time- and resource-intensive fine-tuning for\nsubject alignment, while recent zero-shot approaches leverage on-the-fly image\nprompting, often sacrificing subject alignment. In this paper, we introduce\nDiptych Prompting, a novel zero-shot approach that reinterprets as an\ninpainting task with precise subject alignment by leveraging the emergent\nproperty of diptych generation in large-scale text-to-image models. Diptych\nPrompting arranges an incomplete diptych with the reference image in the left\npanel, and performs text-conditioned inpainting on the right panel. We further\nprevent unwanted content leakage by removing the background in the reference\nimage and improve fine-grained details in the generated subject by enhancing\nattention weights between the panels during inpainting. Experimental results\nconfirm that our approach significantly outperforms zero-shot image prompting\nmethods, resulting in images that are visually preferred by users.\nAdditionally, our method supports not only subject-driven generation but also\nstylized image generation and subject-driven image editing, demonstrating\nversatility across diverse image generation applications. Project page:\nhttps://diptychprompting.github.io/"}
{"id": "2506.02576", "pdf": "https://arxiv.org/pdf/2506.02576", "abs": "https://arxiv.org/abs/2506.02576", "authors": ["Haichen Wang", "Liu Yang", "Xinyuan Zhang", "Haomin Yu", "Ming Li", "Jilin Hu"], "title": "ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting", "categories": ["cs.AI"], "comment": "9 pages, 5 figures, 3 tables. IJCAI-2025", "summary": "Passenger demand forecasting helps optimize vehicle scheduling, thereby\nimproving urban efficiency. Recently, attention-based methods have been used to\nadequately capture the dynamic nature of spatio-temporal data. However,\nexisting methods that rely on heuristic masking strategies cannot fully adapt\nto the complex spatio-temporal correlations, hindering the model from focusing\non the right context. These works also overlook the high-level correlations\nthat exist in the real world. Effectively integrating these high-level\ncorrelations with the original correlations is crucial. To fill this gap, we\npropose the Aggregation Differential Transformer (ADFormer), which offers new\ninsights to demand forecasting promotion. Specifically, we utilize Differential\nAttention to capture the original spatial correlations and achieve attention\ndenoising. Meanwhile, we design distinct aggregation strategies based on the\nnature of space and time. Then, the original correlations are unified with the\nhigh-level correlations, enabling the model to capture holistic spatio-temporal\nrelations. Experiments conducted on taxi and bike datasets confirm the\neffectiveness and efficiency of our model, demonstrating its practical value.\nThe code is available at https://github.com/decisionintelligence/ADFormer."}
{"id": "2506.04063", "pdf": "https://arxiv.org/pdf/2506.04063", "abs": "https://arxiv.org/abs/2506.04063", "authors": ["Alex Sotiropoulos", "Sulyab Thottungal Valapu", "Linus Lei", "Jared Coleman", "Bhaskar Krishnamachari"], "title": "Crowd-SFT: Crowdsourcing for LLM Alignment", "categories": ["cs.HC", "cs.DC", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation."}
{"id": "2501.05855", "pdf": "https://arxiv.org/pdf/2501.05855", "abs": "https://arxiv.org/abs/2501.05855", "authors": ["Antonin Poché", "Alon Jacovi", "Agustin Martin Picard", "Victor Boutin", "Fanny Jourdan"], "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability", "categories": ["cs.CL"], "comment": null, "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim."}
{"id": "2411.17467", "pdf": "https://arxiv.org/pdf/2411.17467", "abs": "https://arxiv.org/abs/2411.17467", "authors": ["Xuweiyi Chen", "Zezhou Cheng"], "title": "Learning 3D Representations from Procedural 3D Programs", "categories": ["cs.CV"], "comment": "SynData4CV @ CVPR2025 | Project Page:\n  https://point-mae-zero.cs.virginia.edu/", "summary": "Self-supervised learning has emerged as a promising approach for acquiring\ntransferable 3D representations from unlabeled 3D point clouds. Unlike 2D\nimages, which are widely accessible, acquiring 3D assets requires specialized\nexpertise or professional 3D scanning equipment, making it difficult to scale\nand raising copyright concerns. To address these challenges, we propose\nlearning 3D representations from procedural 3D programs that automatically\ngenerate 3D shapes using simple primitives and augmentations. Remarkably,\ndespite lacking semantic content, the 3D representations learned from the\nprocedurally generated 3D shapes perform on par with state-of-the-art\nrepresentations learned from semantically recognizable 3D models (e.g.,\nairplanes) across various downstream 3D tasks, including shape classification,\npart segmentation, and masked point cloud completion. We provide a detailed\nanalysis on factors that make a good 3D procedural program. Extensive\nexperiments further suggest that current self-supervised learning methods on\npoint clouds do not rely on the semantics of 3D shapes, shedding light on the\nnature of 3D representations learned."}
{"id": "2506.02867", "pdf": "https://arxiv.org/pdf/2506.02867", "abs": "https://arxiv.org/abs/2506.02867", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks."}
{"id": "2506.04170", "pdf": "https://arxiv.org/pdf/2506.04170", "abs": "https://arxiv.org/abs/2506.04170", "authors": ["Piotr Białas", "Piotr Korcyl", "Tomasz Stebel", "Dawid Zapolski"], "title": "Estimation of the reduced density matrix and entanglement entropies using autoregressive networks", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.LG", "hep-lat", "hep-th"], "comment": "9 pages, 7 figures", "summary": "We present an application of autoregressive neural networks to Monte Carlo\nsimulations of quantum spin chains using the correspondence with classical\ntwo-dimensional spin systems. We use a hierarchy of neural networks capable of\nestimating conditional probabilities of consecutive spins to evaluate elements\nof reduced density matrices directly. Using the Ising chain as an example, we\ncalculate the continuum limit of the ground state's von Neumann and R\\'enyi\nbipartite entanglement entropies of an interval built of up to 5 spins. We\ndemonstrate that our architecture is able to estimate all the needed matrix\nelements with just a single training for a fixed time discretization and\nlattice volume. Our method can be applied to other types of spin chains,\npossibly with defects, as well as to estimating entanglement entropies of\nthermal states at non-zero temperature."}
{"id": "2501.16748", "pdf": "https://arxiv.org/pdf/2501.16748", "abs": "https://arxiv.org/abs/2501.16748", "authors": ["Garima Chhikara", "Abhishek Kumar", "Abhijnan Chakraborty"], "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."}
{"id": "2412.04300", "pdf": "https://arxiv.org/pdf/2412.04300", "abs": "https://arxiv.org/abs/2412.04300", "authors": ["Ziwei Huang", "Wanggui He", "Quanyu Long", "Yandi Wang", "Haoyuan Li", "Zhelun Yu", "Fangxun Shu", "Long Chan", "Hao Jiang", "Fei Wu", "Leilei Gan"], "title": "T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Evaluating the quality of synthesized images remains a significant challenge\nin the development of text-to-image (T2I) generation. Most existing studies in\nthis area primarily focus on evaluating text-image alignment, image quality,\nand object composition capabilities, with comparatively fewer studies\naddressing the evaluation of the factuality of T2I models, particularly when\nthe concepts involved are knowledge-intensive. To mitigate this gap, we present\nT2I-FactualBench in this work - the largest benchmark to date in terms of the\nnumber of concepts and prompts specifically designed to evaluate the factuality\nof knowledge-intensive concept generation. T2I-FactualBench consists of a\nthree-tiered knowledge-intensive text-to-image generation framework, ranging\nfrom the basic memorization of individual knowledge concepts to the more\ncomplex composition of multiple knowledge concepts. We further introduce a\nmulti-round visual question answering (VQA) based evaluation framework to\nassess the factuality of three-tiered knowledge-intensive text-to-image\ngeneration tasks. Experiments on T2I-FactualBench indicate that current\nstate-of-the-art (SOTA) T2I models still leave significant room for\nimprovement."}
{"id": "2306.01310", "pdf": "https://arxiv.org/pdf/2306.01310", "abs": "https://arxiv.org/abs/2306.01310", "authors": ["Jaeseung Heo", "Seungbeom Lee", "Sungsoo Ahn", "Dongwoo Kim"], "title": "EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2024", "summary": "Data augmentation plays a critical role in improving model performance across\nvarious domains, but it becomes challenging with graph data due to their\ncomplex and irregular structure. To address this issue, we propose EPIC (Edit\nPath Interpolation via learnable Cost), a novel interpolation-based method for\naugmenting graph datasets. To interpolate between two graphs lying in an\nirregular domain, EPIC leverages the concept of graph edit distance,\nconstructing an edit path that represents the transformation process between\ntwo graphs via edit operations. Moreover, our method introduces a\ncontext-sensitive cost model that accounts for the importance of specific edit\noperations formulated through a learning framework. This allows for a more\nnuanced transformation process, where the edit distance is not merely\ncount-based but reflects meaningful graph attributes. With randomly sampled\ngraphs from the edit path, we enrich the training set to enhance the\ngeneralization capability of classification models. Experimental evaluations\nacross several benchmark datasets demonstrate that our approach outperforms\nexisting augmentation techniques in many tasks."}
{"id": "2506.04193", "pdf": "https://arxiv.org/pdf/2506.04193", "abs": "https://arxiv.org/abs/2506.04193", "authors": ["Stephen R. Pfohl", "Natalie Harris", "Chirag Nagpal", "David Madras", "Vishwali Mhasawade", "Olawale Salaudeen", "Awa Dieng", "Shannon Sequeira", "Santiago Arciniegas", "Lillian Sung", "Nnamdi Ezeanochie", "Heather Cole-Lewis", "Katherine Heller", "Sanmi Koyejo", "Alexander D'Amour"], "title": "Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness", "categories": ["stat.ML", "cs.CY", "cs.LG"], "comment": null, "summary": "Disaggregated evaluation across subgroups is critical for assessing the\nfairness of machine learning models, but its uncritical use can mislead\npractitioners. We show that equal performance across subgroups is an unreliable\nmeasure of fairness when data are representative of the relevant populations\nbut reflective of real-world disparities. Furthermore, when data are not\nrepresentative due to selection bias, both disaggregated evaluation and\nalternative approaches based on conditional independence testing may be invalid\nwithout explicit assumptions regarding the bias mechanism. We use causal\ngraphical models to predict metric stability across subgroups under different\ndata generating processes. Our framework suggests complementing disaggregated\nevaluations with explicit causal assumptions and analysis to control for\nconfounding and distribution shift, including conditional independence testing\nand weighted performance estimation. These findings have broad implications for\nhow practitioners design and interpret model assessments given the ubiquity of\ndisaggregated evaluation."}
{"id": "2502.00675", "pdf": "https://arxiv.org/pdf/2502.00675", "abs": "https://arxiv.org/abs/2502.00675", "authors": ["Minghang Deng", "Ashwin Ramachandran", "Canwen Xu", "Lanxiang Hu", "Zhewei Yao", "Anupam Datta", "Hao Zhang"], "title": "ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration", "categories": ["cs.CL", "I.2.7; I.2.0; H.2.0"], "comment": "33 pages, 3 figures", "summary": "We present ReFoRCE, a Text-to-SQL agent that tops the Spider 2.0\nleaderboard--a challenging benchmark reflecting complex, real-world Text-to-SQL\nscenarios. While Text-to-SQL systems enable natural language queries over\nstructured databases, deploying them in enterprise environments remains\ndifficult due to large, complex schemas (with over 1,000 columns), diverse SQL\ndialects (e.g., BigQuery, Snowflake), and sophisticated query requirements\n(e.g., transformations and analytics). ReFoRCE addresses these challenges\nthrough: (a) database information compression via pattern-based table grouping\nand LLM-guided schema linking to alleviate long-context issues; (b)\nself-refinement to iteratively correct syntax and semantic errors across\ndialects; (c) majority-vote consensus to select high-confidence candidates\nwhile deferring ambiguous cases arising from sophisticated queries; and (d)\niterative column exploration guided by execution feedback to resolve those\ndeferred cases. ReFoRCE achieves new state-of-the-art results, with scores of\n35.83 on Spider 2.0-Snow and 36.56 on Spider 2.0-Lite."}
{"id": "2412.06141", "pdf": "https://arxiv.org/pdf/2412.06141", "abs": "https://arxiv.org/abs/2412.06141", "authors": ["Kangyu Zhu", "Peng Xia", "Yun Li", "Hongtu Zhu", "Sheng Wang", "Huaxiu Yao"], "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The advancement of Large Vision-Language Models (LVLMs) has propelled their\napplication in the medical field. However, Medical LVLMs (Med-LVLMs) encounter\nfactuality challenges due to modality misalignment, where the models prioritize\ntextual knowledge over visual input, leading to hallucinations that contradict\ninformation in medical images. Previous attempts to enhance modality alignment\nin Med-LVLMs through preference optimization have inadequately mitigated\nclinical relevance in preference data, making these samples easily\ndistinguishable and reducing alignment effectiveness. To address this\nchallenge, we propose MMedPO, a novel multimodal medical preference\noptimization approach that considers the clinical relevance of preference\nsamples to enhance Med-LVLM alignment. MMedPO curates multimodal preference\ndata by introducing two types of dispreference: (1) plausible hallucinations\ninjected through target Med-LVLMs or GPT-4o to produce medically inaccurate\nresponses, and (2) lesion region neglect achieved through local lesion-noising,\ndisrupting visual understanding of critical areas. We then calculate clinical\nrelevance for each sample based on scores from multiple Med-LLMs and visual\ntools, and integrate these scores into the preference optimization process as\nweights, enabling effective alignment. Our experiments demonstrate that MMedPO\nsignificantly enhances factual accuracy in Med-LVLMs, achieving substantial\nimprovements over existing preference optimization methods by averaging 14.2%\nand 51.7% across the Med-VQA and report generation tasks. Our code are\navailable in https://github.com/aiming-lab/MMedPO."}
{"id": "2309.16739", "pdf": "https://arxiv.org/pdf/2309.16739", "abs": "https://arxiv.org/abs/2309.16739", "authors": ["Zheng Lin", "Guanqiao Qu", "Qiyuan Chen", "Xianhao Chen", "Zhe Chen", "Kaibin Huang"], "title": "Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 5 figures", "summary": "Large language models (LLMs), which have shown remarkable capabilities, are\nrevolutionizing AI development and potentially shaping our future. However,\ngiven their multimodality, the status quo cloud-based deployment faces some\ncritical challenges: 1) long response time; 2) high bandwidth costs; and 3) the\nviolation of data privacy. 6G mobile edge computing (MEC) systems may resolve\nthese pressing issues. In this article, we explore the potential of deploying\nLLMs at the 6G edge. We start by introducing killer applications powered by\nmultimodal LLMs, including robotics and healthcare, to highlight the need for\ndeploying LLMs in the vicinity of end users. Then, we identify the critical\nchallenges for LLM deployment at the edge and envision the 6G MEC architecture\nfor LLMs. Furthermore, we delve into two design aspects, i.e., edge training\nand edge inference for LLMs. In both aspects, considering the inherent resource\nlimitations at the edge, we discuss various cutting-edge techniques, including\nsplit learning/inference, parameter-efficient fine-tuning, quantization, and\nparameter-sharing inference, to facilitate the efficient deployment of LLMs.\nThis article serves as a position paper for thoroughly identifying the\nmotivation, challenges, and pathway for empowering LLMs at the 6G edge."}
{"id": "2506.04194", "pdf": "https://arxiv.org/pdf/2506.04194", "abs": "https://arxiv.org/abs/2506.04194", "authors": ["Yang Cai", "Alkis Kalavasis", "Katerina Mamali", "Anay Mehrotra", "Manolis Zampetakis"], "title": "What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness", "categories": ["math.ST", "cs.LG", "econ.EM", "stat.ME", "stat.ML", "stat.TH"], "comment": "Accepted for presentation at the 38th Conference on Learning Theory\n  (COLT) 2025", "summary": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms."}
{"id": "2502.13656", "pdf": "https://arxiv.org/pdf/2502.13656", "abs": "https://arxiv.org/abs/2502.13656", "authors": ["Liyang He", "Chenglong Liu", "Rui Li", "Zhenya Huang", "Shulan Ruan", "Jun Zhou", "Enhong Chen"], "title": "Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis."}
{"id": "2412.09586", "pdf": "https://arxiv.org/pdf/2412.09586", "abs": "https://arxiv.org/abs/2412.09586", "authors": ["Fiona Ryan", "Ajay Bati", "Sangmin Lee", "Daniel Bolya", "Judy Hoffman", "James M. Rehg"], "title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlight", "summary": "We address the problem of gaze target estimation, which aims to predict where\na person is looking in a scene. Predicting a person's gaze target requires\nreasoning both about the person's appearance and the contents of the scene.\nPrior works have developed increasingly complex, hand-crafted pipelines for\ngaze target estimation that carefully fuse features from separate scene\nencoders, head encoders, and auxiliary models for signals like depth and pose.\nMotivated by the success of general-purpose feature extractors on a variety of\nvisual tasks, we propose Gaze-LLE, a novel transformer framework that\nstreamlines gaze target estimation by leveraging features from a frozen DINOv2\nencoder. We extract a single feature representation for the scene, and apply a\nperson-specific positional prompt to decode gaze with a lightweight module. We\ndemonstrate state-of-the-art performance across several gaze benchmarks and\nprovide extensive analysis to validate our design choices. Our code is\navailable at: http://github.com/fkryan/gazelle ."}
{"id": "2402.00944", "pdf": "https://arxiv.org/pdf/2402.00944", "abs": "https://arxiv.org/abs/2402.00944", "authors": ["David S. Berman", "Marc S. Klinger", "Alexander G. Stapleton"], "title": "NCoder -- A Quantum Field Theory approach to encoding data", "categories": ["hep-th", "cond-mat.dis-nn", "cs.AI"], "comment": "29 pages. v2 Fixed minor typos. v3 Added journal submitted ver", "summary": "In this paper we present a novel approach to interpretable AI inspired by\nQuantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified\nautoencoder neural network whose latent layer is prescribed to be a subset of\n$n$-point correlation functions. Regarding images as draws from a lattice field\ntheory, this architecture mimics the task of perturbatively constructing the\neffective action of the theory order by order in an expansion using Feynman\ndiagrams. Alternatively, the NCoder may be regarded as simulating the procedure\nof statistical inference whereby high dimensional data is first summarized in\nterms of several lower dimensional summary statistics (here the $n$-point\ncorrelation functions), and subsequent out-of-sample data is generated by\ninferring the data generating distribution from these statistics. In this way\nthe NCoder suggests a fascinating correspondence between perturbative\nrenormalizability and the sufficiency of models. We demonstrate the efficacy of\nthe NCoder by applying it to the generation of MNIST images, and find that\ngenerated images can be correctly classified using only information from the\nfirst three $n$-point functions of the image distribution."}
{"id": "2506.04204", "pdf": "https://arxiv.org/pdf/2506.04204", "abs": "https://arxiv.org/abs/2506.04204", "authors": ["Martin Beseda", "Vittorio Cortellessa", "Daniele Di Pompeo", "Luca Traini", "Michele Tucci"], "title": "A Kernel-Based Approach for Accurate Steady-State Detection in Performance Time Series", "categories": ["cs.PF", "cs.LG"], "comment": "This manuscript is under review by Future Generation Computer Systems", "summary": "This paper addresses the challenge of accurately detecting the transition\nfrom the warmup phase to the steady state in performance metric time series,\nwhich is a critical step for effective benchmarking. The goal is to introduce a\nmethod that avoids premature or delayed detection, which can lead to inaccurate\nor inefficient performance analysis. The proposed approach adapts techniques\nfrom the chemical reactors domain, detecting steady states online through the\ncombination of kernel-based step detection and statistical methods. By using a\nwindow-based approach, it provides detailed information and improves the\naccuracy of identifying phase transitions, even in noisy or irregular time\nseries. Results show that the new approach reduces total error by 14.5%\ncompared to the state-of-the-art method. It offers more reliable detection of\nthe steady-state onset, delivering greater precision for benchmarking tasks.\nFor users, the new approach enhances the accuracy and stability of performance\nbenchmarking, efficiently handling diverse time series data. Its robustness and\nadaptability make it a valuable tool for real-world performance evaluation,\nensuring consistent and reproducible results."}
{"id": "2502.13946", "pdf": "https://arxiv.org/pdf/2502.13946", "abs": "https://arxiv.org/abs/2502.13946", "authors": ["Chak Tou Leong", "Qingyu Yin", "Jian Wang", "Wenjie Li"], "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ACL 2025 Main", "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region."}
{"id": "2412.10032", "pdf": "https://arxiv.org/pdf/2412.10032", "abs": "https://arxiv.org/abs/2412.10032", "authors": ["Niclas Popp", "Dan Zhang", "Jan Hendrik Metzen", "Matthias Hein", "Lukas Schott"], "title": "Single-Pass Object-Focused Data Selection", "categories": ["cs.CV"], "comment": null, "summary": "While unlabeled image data is often plentiful, the costs of high-quality\nlabels pose an important practical challenge: Which images should one select\nfor labeling to use the annotation budget for a particular target task most\neffectively? To address this problem, we focus on single-pass data selection,\nwhich refers to the process of selecting all data to be annotated at once\nbefore training a downstream model. Prior methods for single-pass data\nselection rely on image-level representations and fail to reliably outperform\nrandom selection for object detection and segmentation. We propose\nObject-Focused Data Selection (OFDS) which leverages object-level features from\nfoundation models and ensures semantic coverage of all target classes. In\nextensive experiments across tasks and target domains, OFDS consistently\noutperforms random selection and all baselines. The best results for\nconstrained annotation budgets are obtained by combining human labels from OFDS\nwith autolabels from foundation models. Moreover, using OFDS to select the\ninitial labeled set for active learning yields consistent improvements"}
{"id": "2403.13101", "pdf": "https://arxiv.org/pdf/2403.13101", "abs": "https://arxiv.org/abs/2403.13101", "authors": ["Zheng Lin", "Guanqiao Qu", "Wei Wei", "Xianhao Chen", "Kin K. Leung"], "title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "16 pages, 12 figures", "summary": "The increasing complexity of deep neural networks poses significant barriers\nto democratizing them to resource-limited edge devices. To address this\nchallenge, split federated learning (SFL) has emerged as a promising solution\nby of floading the primary training workload to a server via model partitioning\nwhile enabling parallel training among edge devices. However, although system\noptimization substantially influences the performance of SFL under\nresource-constrained systems, the problem remains largely uncharted. In this\npaper, we provide a convergence analysis of SFL which quantifies the impact of\nmodel splitting (MS) and client-side model aggregation (MA) on the learning\nperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, a\nnovel resource-adaptive SFL framework, to expedite SFL under\nresource-constrained edge computing systems. Specifically, AdaptSFL adaptively\ncontrols client-side MA and MS to balance communication-computing latency and\ntraining convergence. Extensive simulations across various datasets validate\nthat our proposed AdaptSFL framework takes considerably less time to achieve a\ntarget accuracy than benchmarks, demonstrating the effectiveness of the\nproposed strategies."}
{"id": "2002.02997", "pdf": "https://arxiv.org/pdf/2002.02997", "abs": "https://arxiv.org/abs/2002.02997", "authors": ["Liyan Chen", "Philippos Mordohai", "Sergul Aydore"], "title": "DropCluster: A structured dropout for convolutional networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "11 pages, 10 figures, under review", "summary": "Dropout as a common regularizer to prevent overfitting in deep neural\nnetworks has been less effective in convolutional layers than in fully\nconnected layers. This is because Dropout drops features randomly, without\nconsidering local structure. When features are spatially correlated, as in the\ncase of convolutional layers, information from the dropped features can still\npropagate to subsequent layers via neighboring features. To address this\nproblem, structured forms of Dropout have been proposed. A drawback of these\nmethods is that they do not adapt to the data. In this work, we leverage the\nstructure in the outputs of convolutional layers and introduce a novel\nstructured regularization method named DropCluster. Our approach clusters\nfeatures in convolutional layers, and drops the resulting clusters randomly\nduring training iterations. Experiments on CIFAR-10/100, SVHN, and APPA-REAL\ndatasets demonstrate that our approach is effective and controls overfitting\nbetter than other approaches."}
{"id": "2502.14019", "pdf": "https://arxiv.org/pdf/2502.14019", "abs": "https://arxiv.org/abs/2502.14019", "authors": ["Myra Cheng", "Su Lin Blodgett", "Alicia DeVrio", "Lisa Egede", "Alexandra Olteanu"], "title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "ACL 2025", "summary": "As text generation systems' outputs are increasingly anthropomorphic --\nperceived as human-like -- scholars have also increasingly raised concerns\nabout how such outputs can lead to harmful outcomes, such as users over-relying\nor developing emotional dependence on these systems. How to intervene on such\nsystem outputs to mitigate anthropomorphic behaviors and their attendant\nharmful outcomes, however, remains understudied. With this work, we aim to\nprovide empirical and theoretical grounding for developing such interventions.\nTo do so, we compile an inventory of interventions grounded both in prior\nliterature and a crowdsourcing study where participants edited system outputs\nto make them less human-like. Drawing on this inventory, we also develop a\nconceptual framework to help characterize the landscape of possible\ninterventions, articulate distinctions between different types of\ninterventions, and provide a theoretical basis for evaluating the effectiveness\nof different interventions."}
{"id": "2412.10573", "pdf": "https://arxiv.org/pdf/2412.10573", "abs": "https://arxiv.org/abs/2412.10573", "authors": ["Yiwen Gu", "Mahir Patel", "Margrit Betke"], "title": "ExeChecker: Where Did I Go Wrong?", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "In this paper, we present a contrastive learning based framework, ExeChecker,\nfor the interpretation of rehabilitation exercises. Our work builds upon\nstate-of-the-art advances in the area of human pose estimation, graph-attention\nneural networks, and transformer interpretablity. The downstream task is to\nassist rehabilitation by providing informative feedback to users while they are\nperforming prescribed exercises. We utilize a contrastive learning strategy\nduring training. Given a tuple of correctly and incorrectly executed exercises,\nour model is able to identify and highlight those joints that are involved in\nan incorrect movement and thus require the user's attention. We collected an\nin-house dataset, ExeCheck, with paired recordings of both correct and\nincorrect execution of exercises. In our experiments, we tested our method on\nthis dataset as well as the UI-PRMD dataset and found ExeCheck outperformed the\nbaseline method using pairwise sequence alignment in identifying joints of\nphysical relevance in rehabilitation exercises."}
{"id": "2404.18445", "pdf": "https://arxiv.org/pdf/2404.18445", "abs": "https://arxiv.org/abs/2404.18445", "authors": ["Christian Peukert", "Florian Abeillon", "Jérémie Haese", "Franziska Kaiser", "Alexander Staub"], "title": "AI and the Dynamic Supply of Training Data", "categories": ["econ.GN", "cs.AI", "cs.CY", "cs.LG", "q-fin.EC"], "comment": null, "summary": "Artificial intelligence (AI) systems rely heavily on human-generated data,\nyet the people behind that data are often overlooked. Human behavior can play a\nmajor role in AI training datasets, be it in limiting access to existing works\nor in deciding which types of new works to create or whether to create any at\nall. We examine creators' behavioral change when their works become training\ndata for commercial AI. Specifically, we focus on contributors on Unsplash, a\npopular stock image platform with about 6 million high-quality photos and\nillustrations. In the summer of 2020, Unsplash launched a research program and\nreleased a dataset of 25,000 images for commercial AI use. We study\ncontributors' reactions, comparing contributors whose works were included in\nthis dataset to contributors whose works were not. Our results suggest that\ntreated contributors left the platform at a higher-than-usual rate and\nsubstantially slowed down the rate of new uploads. Professional photographers\nand more heavily affected users had a stronger reaction than amateurs and less\naffected users. We also show that affected users changed the variety and\nnovelty of contributions to the platform, which can potentially lead to\nlower-quality AI outputs in the long run. Our findings highlight a critical\ntrade-off: the drive to expand AI capabilities versus the incentives of those\nproducing training data. We conclude with policy proposals, including dynamic\ncompensation schemes and structured data markets, to realign incentives at the\ndata frontier."}
{"id": "2303.05978", "pdf": "https://arxiv.org/pdf/2303.05978", "abs": "https://arxiv.org/abs/2303.05978", "authors": ["Xavier Aramayo Carrasco", "Maksim Nekrashevich", "Petr Mokrov", "Evgeny Burnaev", "Alexander Korotin"], "title": "Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem", "categories": ["cs.LG"], "comment": null, "summary": "Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has\nattracted the special attention of the ML community. In this problem, given two\ndistributions supported on two (possibly different) spaces, one has to find the\nmost isometric map between them. In the discrete variant of GWOT, the task is\nto learn an assignment between given discrete sets of points. In the more\nadvanced continuous formulation, one aims at recovering a parametric mapping\nbetween unknown continuous distributions based on i.i.d. samples derived from\nthem. The clear geometrical intuition behind the GWOT makes it a natural choice\nfor several practical use cases, giving rise to a number of proposed solvers.\nSome of them claim to solve the continuous version of the problem. At the same\ntime, GWOT is notoriously hard, both theoretically and numerically. Moreover,\nall existing continuous GWOT solvers still heavily rely on discrete techniques.\nNatural questions arise: to what extent do existing methods unravel the GWOT\nproblem, what difficulties do they encounter, and under which conditions they\nare successful? Our benchmark paper is an attempt to answer these questions. We\nspecifically focus on the continuous GWOT as the most interesting and debatable\nsetup. We crash-test existing continuous GWOT approaches on different\nscenarios, carefully record and analyze the obtained results, and identify\nissues. Our findings experimentally testify that the scientific community is\nstill missing a reliable continuous GWOT solver, which necessitates further\nresearch efforts. As the first step in this direction, we propose a new\ncontinuous GWOT method which does not rely on discrete techniques and partially\nsolves some of the problems of the competitors."}
{"id": "2502.14748", "pdf": "https://arxiv.org/pdf/2502.14748", "abs": "https://arxiv.org/abs/2502.14748", "authors": ["Zongxia Li", "Lorena Calvo-Bartolomé", "Alexander Hoyle", "Paiheng Xu", "Alden Dima", "Juan Francisco Fung", "Jordan Boyd-Graber"], "title": "Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models", "categories": ["cs.CL"], "comment": "22 Pages. LLM for Data Exploration and content analysis, Topic\n  Models. 63rd Annual Meeting of the Association for Computational Linguistics\n  (2025)", "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints."}
{"id": "2412.11050", "pdf": "https://arxiv.org/pdf/2412.11050", "abs": "https://arxiv.org/abs/2412.11050", "authors": ["Yujin Wang", "Quanfeng Liu", "Jiaqi Fan", "Jinlong Hong", "Hongqing Chu", "Mengjian Tian", "Bingzhao Gao", "Hong Chen"], "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "Understanding and addressing corner cases is essential for ensuring the\nsafety and reliability of autonomous driving systems. Vision-language models\n(VLMs) play a crucial role in enhancing scenario comprehension, yet they face\nsignificant challenges, such as hallucination and insufficient real-world\ngrounding, which compromise their performance in critical driving scenarios. In\nthis work, RAC3, a novel framework designed to enhance the performance of VLMs\nin corner case comprehension, is proposed. RAC3 integrates a frequency-spatial\nfusion (FSF) image encoder, a cross-modal alignment training method for\nembedding models with hard and semi-hard negative mining, and a fast querying\nand retrieval pipeline based on K-Means clustering and hierarchical navigable\nsmall world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting\nstrategy to guide analogical reasoning and reduce hallucinations during\ninference is introduced. Moreover, an update mechanism is integrated into RAC3\nto ensure continual learning within the framework. Extensive experiments on the\nCODA and nuScenes datasets demonstrate that RAC3 significantly improves corner\ncase comprehension across multiple downstream tasks. Compared to prior\nstate-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the\nCODA-LM benchmark and shows consistent performance gains when integrated with\nend-to-end frameworks like DriveLM. These results demonstrate the effectiveness\nof retrieval-augmented strategies and cross-modal alignment for safer and more\ninterpretable autonomous driving."}
{"id": "2405.08965", "pdf": "https://arxiv.org/pdf/2405.08965", "abs": "https://arxiv.org/abs/2405.08965", "authors": ["Jayanaka L. Dantanarayana", "Yiping Kang", "Kugesan Sivasothynathan", "Christopher Clarke", "Baichuan Li", "Savini Kashmira", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "Meaning-Typed Programming: Language Abstraction and Runtime for Model-Integrated Applications", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Software development is shifting from traditional logical programming to\nmodel-integrated applications that leverage generative AI and large language\nmodels (LLMs) during runtime. However, integrating LLMs remains complex,\nrequiring developers to manually craft prompts and process outputs. Existing\ntools attempt to assist with prompt engineering, but often introduce additional\ncomplexity.\n  This paper presents Meaning-Typed Programming (MTP) model, a novel paradigm\nthat abstracts LLM integration through intuitive language-level constructs. By\nleveraging the inherent semantic richness of code, MTP automates prompt\ngeneration and response handling without additional developer effort. We\nintroduce the by operator for seamless LLM invocation, MT-IR, a meaning-based\nintermediate representation for semantic extraction, and MT-Runtime, an\nautomated system for managing LLM interactions. We implement MTP in Jac, a\nPython superset language and find that MTP significantly reduces coding\ncomplexity while maintaining accuracy and efficiency. Our evaluation across\ndiverse benchmarks and user studies demonstrates that MTP outperforms existing\nframeworks such as DSPy and LMQL by reducing lines of code by factors of\n2.3-7.5X and 1.3-10.7X respectively. For math problems from the GSM8k dataset,\nMTP achieves accuracy rates approaching 90%, while reducing token usage in 10\nout of 13 benchmarks. This leads to cost savings up to 4.5X and runtime\nspeedups as high as 4.75X. Additionally, MTP demonstrates resilience even when\n50% of naming conventions are suboptimal, establishing it as a practical,\nefficient solution for streamlining model-integrated application development."}
{"id": "2304.01906", "pdf": "https://arxiv.org/pdf/2304.01906", "abs": "https://arxiv.org/abs/2304.01906", "authors": ["Tianyu Du", "Ayush Kanodia", "Susan Athey"], "title": "Torch-Choice: A PyTorch Package for Large-Scale Choice Modeling with Python", "categories": ["cs.LG", "cs.MS", "econ.EM"], "comment": null, "summary": "The $\\texttt{torch-choice}$ is an open-source library for flexible, fast\nchoice modeling with Python and PyTorch. $\\texttt{torch-choice}$ provides a\n$\\texttt{ChoiceDataset}$ data structure to manage databases flexibly and\nmemory-efficiently. The paper demonstrates constructing a\n$\\texttt{ChoiceDataset}$ from databases of various formats and functionalities\nof $\\texttt{ChoiceDataset}$. The package implements two widely used models,\nnamely the multinomial logit and nested logit models, and supports\nregularization during model estimation. The package incorporates the option to\ntake advantage of GPUs for estimation, allowing it to scale to massive datasets\nwhile being computationally efficient. Models can be initialized using either\nR-style formula strings or Python dictionaries. We conclude with a comparison\nof the computational efficiencies of $\\texttt{torch-choice}$ and\n$\\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the\nnumber of covariates increases, and (3) the expansion of item sets. Finally, we\ndemonstrate the scalability of $\\texttt{torch-choice}$ on large-scale datasets."}
{"id": "2502.15109", "pdf": "https://arxiv.org/pdf/2502.15109", "abs": "https://arxiv.org/abs/2502.15109", "authors": ["Leena Mathur", "Marian Qian", "Paul Pu Liang", "Louis-Philippe Morency"], "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models", "categories": ["cs.CL", "cs.LG"], "comment": "Under Review, 24 pages", "summary": "Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce SOCIAL GENOME, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. SOCIAL\nGENOME contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSOCIAL GENOME is also the first modeling challenge to study external knowledge\nin social reasoning. SOCIAL GENOME computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of SOCIAL GENOME through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models."}
{"id": "2412.13058", "pdf": "https://arxiv.org/pdf/2412.13058", "abs": "https://arxiv.org/abs/2412.13058", "authors": ["Brégier Romain", "Baradel Fabien", "Lucas Thomas", "Galaaoui Salma", "Armando Matthieu", "Weinzaepfel Philippe", "Rogez Grégory"], "title": "CondiMen: Conditional Multi-Person Mesh Recovery", "categories": ["cs.CV"], "comment": "accepted to the RHOBIN workshop at CVPR 2025", "summary": "Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand."}
{"id": "2405.17829", "pdf": "https://arxiv.org/pdf/2405.17829", "abs": "https://arxiv.org/abs/2405.17829", "authors": ["Jinho Chang", "Jong Chul Ye"], "title": "LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models", "categories": ["cs.LG", "cs.AI"], "comment": "Poster in ICML 2025; 19 pages, 13 figures", "summary": "With the emergence of diffusion models as a frontline generative model, many\nresearchers have proposed molecule generation techniques with conditional\ndiffusion models. However, the unavoidable discreteness of a molecule makes it\ndifficult for a diffusion model to connect raw data with highly complex\nconditions like natural language. To address this, here we present a novel\nlatent diffusion model dubbed LDMol for text-conditioned molecule generation.\nBy recognizing that the suitable latent space design is the key to the\ndiffusion model performance, we employ a contrastive learning strategy to\nextract novel feature space from text data that embeds the unique\ncharacteristics of the molecule structure. Experiments show that LDMol\noutperforms the existing autoregressive baselines on the text-to-molecule\ngeneration benchmark, being one of the first diffusion models that outperforms\nautoregressive models in textual data generation with a better choice of the\nlatent domain. Furthermore, we show that LDMol can be applied to downstream\ntasks such as molecule-to-text retrieval and text-guided molecule editing,\ndemonstrating its versatility as a diffusion model."}
{"id": "2306.02157", "pdf": "https://arxiv.org/pdf/2306.02157", "abs": "https://arxiv.org/abs/2306.02157", "authors": ["Xinshun Liu", "Yizhi Fang", "Yichao Jiang"], "title": "Automated Architecture Synthesis for Arbitrarily Structured Neural Networks", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2008.08261 by other authors", "summary": "This paper offers a new perspective on Artificial Neural Networks (ANNs)\narchitecture. Traditional ANNs commonly use tree-like or DAG structures for\nsimplicity, which can be preset or determined by Neural Architecture Search\n(NAS). Yet, these structures restrict network collaboration and capability due\nto the absence of horizontal and backward communication. Biological neural\nsystems, however, feature billions of neural units with highly complex\nconnections, allowing each biological neuron to connect with others based on\nspecific situations. Inspired by biological systems, we propose a novel\nframework that learns to construct arbitrary graph structures during training\nand introduce the concept of Neural Modules for organizing neural units, which\nfacilitates communication between any nodes and collaboration among modules.\nUnlike traditional NAS methods that rely on DAG search spaces, our framework\nlearns from complete graphs, enabling free communication between neurons akin\nto biological neural networks. Furthermore, we present a method to compute\nthese structures and a regularization technique that organizes them into\nmultiple independent, balanced neural modules. This approach reduces\noverfitting and improves efficiency through parallel computing. Overall, our\nmethod allows ANNs to learn effective arbitrary structures similar to\nbiological ones. It is adaptable to various tasks and compatible across\ndifferent scenarios, with experimental results demonstrating its potential."}
{"id": "2502.16540", "pdf": "https://arxiv.org/pdf/2502.16540", "abs": "https://arxiv.org/abs/2502.16540", "authors": ["Hong Cai Chen", "Yi Pin Xu", "Yang Zhang"], "title": "D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.IR", "cs.LG"], "comment": "14 pages, 18 figures", "summary": "In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design."}
{"id": "2412.14706", "pdf": "https://arxiv.org/pdf/2412.14706", "abs": "https://arxiv.org/abs/2412.14706", "authors": ["Jianrong Zhang", "Hehe Fan", "Yi Yang"], "title": "EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project page:\n  https://jiro-zhang.github.io/EnergyMoGen/", "summary": "Diffusion models, particularly latent diffusion models, have demonstrated\nremarkable success in text-driven human motion generation. However, it remains\nchallenging for latent diffusion models to effectively compose multiple\nsemantic concepts into a single, coherent motion sequence. To address this\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\nmodel that generates motions by composing a set of diffusion models in latent\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\nwhich enables semantic composition and adaptive gradient descent for text\nembeddings. To overcome the challenges of semantic inconsistency and motion\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\nThis design allows the motion latent diffusion model to synthesize\nhigh-quality, complex motions by combining multiple energy terms corresponding\nto textual descriptions. Experiments show that our approach outperforms\nexisting state-of-the-art models on various motion generation tasks, including\ntext-to-motion generation, compositional motion generation, and multi-concept\nmotion generation. Additionally, we demonstrate that our method can be used to\nextend motion datasets and improve the text-to-motion task."}
{"id": "2407.04173", "pdf": "https://arxiv.org/pdf/2407.04173", "abs": "https://arxiv.org/abs/2407.04173", "authors": ["Faisal Hamman", "Pasan Dissanayake", "Saumitra Mishra", "Freddy Lecue", "Sanghamitra Dutta"], "title": "Quantifying Prediction Consistency Under Fine-Tuning Multiplicity in Tabular LLMs", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Fine-tuning LLMs on tabular classification tasks can lead to the phenomenon\nof fine-tuning multiplicity where equally well-performing models make\nconflicting predictions on the same input. Fine-tuning multiplicity can arise\ndue to variations in the training process, e.g., seed, weight initialization,\nminor changes to training data, etc., raising concerns about the reliability of\nTabular LLMs in high-stakes applications such as finance, hiring, education,\nhealthcare. Our work formalizes this unique challenge of fine-tuning\nmultiplicity in Tabular LLMs and proposes a novel measure to quantify the\nconsistency of individual predictions without expensive model retraining. Our\nmeasure quantifies a prediction's consistency by analyzing (sampling) the\nmodel's local behavior around that input in the embedding space. Interestingly,\nwe show that sampling in the local neighborhood can be leveraged to provide\nprobabilistic guarantees on prediction consistency under a broad class of\nfine-tuned models, i.e., inputs with sufficiently high local stability (as\ndefined by our measure) also remain consistent across several fine-tuned models\nwith high probability. We perform experiments on multiple real-world datasets\nto show that our local stability measure preemptively captures consistency\nunder actual multiplicity across several fine-tuned models, outperforming\ncompeting measures."}
{"id": "2308.12874", "pdf": "https://arxiv.org/pdf/2308.12874", "abs": "https://arxiv.org/abs/2308.12874", "authors": ["Marcial Sanchis-Agudo", "Yuning Wang", "Roger Arnau", "Luca Guastoni", "Jasmin Lim", "Karthik Duraisamy", "Ricardo Vinuesa"], "title": "Easy attention: A simple attention mechanism for temporal predictions with transformers", "categories": ["cs.LG"], "comment": "15 pages and 6 figures", "summary": "To improve the robustness of transformer neural networks used for\ntemporal-dynamics prediction of chaotic systems, we propose a novel attention\nmechanism called easy attention which we demonstrate in time-series\nreconstruction and prediction. While the standard self attention only makes use\nof the inner product of queries and keys, it is demonstrated that the keys,\nqueries and softmax are not necessary for obtaining the attention score\nrequired to capture long-term dependencies in temporal sequences. Through the\nsingular-value decomposition (SVD) on the softmax attention score, we further\nobserve that self attention compresses the contributions from both queries and\nkeys in the space spanned by the attention score. Therefore, our proposed\neasy-attention method directly treats the attention scores as learnable\nparameters. This approach produces excellent results when reconstructing and\npredicting the temporal dynamics of chaotic systems exhibiting more robustness\nand less complexity than self attention or the widely-used long short-term\nmemory (LSTM) network. We show the improved performance of the easy-attention\nmethod in the Lorenz system, a turbulence shear flow and a model of a nuclear\nreactor."}
{"id": "2502.18845", "pdf": "https://arxiv.org/pdf/2502.18845", "abs": "https://arxiv.org/abs/2502.18845", "authors": ["Zichuan Fu", "Wentao Song", "Yejing Wang", "Xian Wu", "Yefeng Zheng", "Yingying Zhang", "Derong Xu", "Xuetao Wei", "Tong Xu", "Xiangyu Zhao"], "title": "Sliding Window Attention Training for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps://github.com/Fzkuji/swat-attention."}
{"id": "2502.01419", "pdf": "https://arxiv.org/pdf/2502.01419", "abs": "https://arxiv.org/abs/2502.01419", "authors": ["Mingi Jung", "Saehyung Lee", "Eunji Kim", "Sungroh Yoon"], "title": "Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICML 2025", "summary": "Detailed image captioning is essential for tasks like data generation and\naiding visually impaired individuals. High-quality captions require a balance\nbetween precision and recall, which remains challenging for current multimodal\nlarge language models (MLLMs). In this work, we hypothesize that this\nlimitation stems from weakening and increasingly noisy visual attention as\nresponses lengthen. To address this issue, we propose SPARC (Selective\nProgressive Attention ReCalibration), a training-free method that enhances the\ncontribution of visual tokens during decoding. SPARC is founded on three key\nobservations: (1) increasing the influence of all visual tokens reduces recall;\nthus, SPARC selectively amplifies visual tokens; (2) as captions lengthen,\nvisual attention becomes noisier, so SPARC identifies critical visual tokens by\nleveraging attention differences across time steps; (3) as visual attention\ngradually weakens, SPARC reinforces it to preserve its influence. Our\nexperiments, incorporating both automated and human evaluations, demonstrate\nthat existing methods improve the precision of MLLMs at the cost of recall. In\ncontrast, our proposed method enhances both precision and recall with minimal\ncomputational overhead."}
{"id": "2409.03833", "pdf": "https://arxiv.org/pdf/2409.03833", "abs": "https://arxiv.org/abs/2409.03833", "authors": ["Victoria Tiki", "Kiet Pham", "Eliu Huerta"], "title": "Sequence modeling of higher-order wave modes of binary black hole mergers", "categories": ["gr-qc", "astro-ph.IM", "cs.AI", "68T10, 85-08, 83C35, 83C57", "I.2"], "comment": "32 pages, 2 appendices, 17 figures", "summary": "Higher-order gravitational wave modes from quasi-circular, spinning,\nnon-precessing binary black hole mergers encode key information about these\nsystems' nonlinear dynamics. We model these waveforms using transformer\narchitectures, targeting the evolution from late inspiral through ringdown. Our\ndata is derived from the \\texttt{NRHybSur3dq8} surrogate model, which includes\nspherical harmonic modes up to $\\ell \\leq 4$ (excluding $(4,0)$, $(4,\\pm1)$ and\nincluding $(5,5)$ modes). These waveforms span mass ratios $q \\leq 8$, spin\ncomponents $s^z_{{1,2}} \\in [-0.8, 0.8]$, and inclination angles $\\theta \\in\n[0, \\pi]$. The model processes input data over the time interval $t \\in\n[-5000\\textrm{M}, -100\\textrm{M})$ and generates predictions for the plus and\ncross polarizations, $(h_{+}, h_{\\times})$, over the interval $t \\in\n[-100\\textrm{M}, 130\\textrm{M}]$. Utilizing 16 NVIDIA A100 GPUs on the Delta\nsupercomputer, we trained the transformer model in 15 hours on over 14 million\nsamples. The model's performance was evaluated on a test dataset of 840,000\nsamples, achieving mean and median overlap scores of 0.996 and 0.997,\nrespectively, relative to the surrogate-based ground truth signals. We further\nbenchmark the model on numerical relativity waveforms from the SXS catalog,\nfinding that it generalizes well to out-of-distribution systems, capable of\nreproducing the dynamics of systems with mass ratios up to $q=15$ and spin\nmagnitudes up to 0.998, with a median overlap of 0.969 across 521 NR waveforms\nand up to 0.998 in face-on/off configurations. These results demonstrate that\ntransformer-based models can capture the nonlinear dynamics of binary black\nhole mergers with high accuracy, even outside the surrogate training domain,\nenabling fast sequence modeling of higher-order wave modes."}
{"id": "2402.07052", "pdf": "https://arxiv.org/pdf/2402.07052", "abs": "https://arxiv.org/abs/2402.07052", "authors": ["Rudrajit Das", "Xi Chen", "Bertram Ieong", "Parikshit Bansal", "Sujay Sanghavi"], "title": "Understanding the Training Speedup from Sampling with Approximate Losses", "categories": ["cs.LG", "stat.ML"], "comment": "Updated version of our paper published in ICML 2024\n  (https://proceedings.mlr.press/v235/das24b.html)", "summary": "It is well known that selecting samples with large losses/gradients can\nsignificantly reduce the number of training steps. However, the selection\noverhead is often too high to yield any meaningful gains in terms of overall\ntraining time. In this work, we focus on the greedy approach of selecting\nsamples with large \\textit{approximate losses} instead of exact losses in order\nto reduce the selection overhead. For smooth convex losses, we show that such a\ngreedy strategy can converge to a constant factor of the minimum value of the\naverage loss in fewer iterations than the standard approach of random\nselection. We also theoretically quantify the effect of the approximation\nlevel. We then develop SIFT which uses early exiting to obtain approximate\nlosses with an intermediate layer's representations for sample selection. We\nevaluate SIFT on the task of training a 110M parameter 12 layer BERT base\nmodel, and show significant gains (in terms of training hours and number of\nbackpropagation steps) without any optimized implementation over vanilla\ntraining. For e.g., to reach 64% validation accuracy, SIFT with exit at the\nfirst layer takes ~ 43 hours compared to ~ 57 hours of vanilla training."}
{"id": "2502.19582", "pdf": "https://arxiv.org/pdf/2502.19582", "abs": "https://arxiv.org/abs/2502.19582", "authors": ["Ife Adebara", "Hawau Olamide Toyin", "Nahom Tesfu Ghebremichael", "AbdelRahim Elmadany", "Muhammad Abdul-Mageed"], "title": "Where Are We? Evaluating LLM Performance on African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities."}
{"id": "2502.07782", "pdf": "https://arxiv.org/pdf/2502.07782", "abs": "https://arxiv.org/abs/2502.07782", "authors": ["Nathan Mankovich", "Ignacio Santamaria", "Gustau Camps-Valls", "Tolga Birdal"], "title": "A Flag Decomposition for Hierarchical Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Flag manifolds encode nested sequences of subspaces and serve as powerful\nstructures for various computer vision and machine learning applications.\nDespite their utility in tasks such as dimensionality reduction, motion\naveraging, and subspace clustering, current applications are often restricted\nto extracting flags using common matrix decomposition methods like the singular\nvalue decomposition. Here, we address the need for a general algorithm to\nfactorize and work with hierarchical datasets. In particular, we propose a\nnovel, flag-based method that decomposes arbitrary hierarchical real-valued\ndata into a hierarchy-preserving flag representation in Stiefel coordinates.\nOur work harnesses the potential of flag manifolds in applications including\ndenoising, clustering, and few-shot learning."}
{"id": "2410.12593", "pdf": "https://arxiv.org/pdf/2410.12593", "abs": "https://arxiv.org/abs/2410.12593", "authors": ["Wei Chen", "Yuxuan Liang"], "title": "Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc."}
{"id": "2406.00153", "pdf": "https://arxiv.org/pdf/2406.00153", "abs": "https://arxiv.org/abs/2406.00153", "authors": ["Benjamin Thérien", "Charles-Étienne Joseph", "Boris Knyazev", "Edouard Oyallon", "Irina Rish", "Eugene Belilovsky"], "title": "$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers", "categories": ["cs.LG"], "comment": null, "summary": "Learned optimizers (LOs) can significantly reduce the wall-clock training\ntime of neural networks, substantially reducing training costs. However, they\ncan struggle to optimize unseen tasks (meta-generalize), especially when\ntraining networks wider than those seen during meta-training. To address this,\nwe derive the Maximal Update Parametrization ($\\mu$P) for two state-of-the-art\nlearned optimizer architectures and propose a simple meta-training recipe for\n$\\mu$-parameterized LOs ($\\mu$LOs). Our empirical evaluation demonstrates that\nLOs meta-trained with our recipe substantially improve meta-generalization to\nwider unseen tasks when compared to LOs trained under standard parametrization\n(SP), as they are trained in existing work. We also empirically observe that\n$\\mu$LOs trained with our recipe exhibit unexpectedly improved\nmeta-generalization to deeper networks ($5\\times$ meta-training) and surprising\ngeneralization to much longer training horizons ($25\\times$ meta-training) when\ncompared to SP LOs."}
{"id": "2503.01622", "pdf": "https://arxiv.org/pdf/2503.01622", "abs": "https://arxiv.org/abs/2503.01622", "authors": ["Eliya Habba", "Ofir Arviv", "Itay Itzhak", "Yotam Perlitz", "Elron Bandel", "Leshem Choshen", "Michal Shmueli-Scheuer", "Gabriel Stanovsky"], "title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/"}
{"id": "2502.08905", "pdf": "https://arxiv.org/pdf/2502.08905", "abs": "https://arxiv.org/abs/2502.08905", "authors": ["Tangyu Jiang", "Haodi Wang", "Chun Yuan"], "title": "DiffoRA: Enabling Parameter-Efficient Fine-Tuning via Differential Module Selection", "categories": ["cs.CV"], "comment": null, "summary": "The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively\nresearched for large language models in downstream tasks. Among all the\nexisting approaches, the Low-Rank Adaptation (LoRA) has gained popularity for\nits streamlined design by incorporating low-rank matrices into existing\npre-trained models. Though effective, LoRA, as well as its adaptive\noptimizations, either allocate the same matrix to all the modules or adjust the\ninterior rank of the components based on importance scoring indicators. In this\npaper, we argue that not all the modules in LLMs are suitable and necessary to\nbe fine-tuned. Enlightened by this insight, we propose a new PEFT scheme called\nDiffoRA, which enables adaptive adoption of the low-rank decomposition\nmatrices. At the core of DiffoRA lies a Differential Adaptation Matrix (DAM) to\ndetermine which module is the most suitable and essential for fine-tuning. We\ntheoretically explain how the designed matrix impacts the convergence rate and\ngeneralization capability of a pre-trained model. We then construct the DAM via\ncontinuous relaxation and discretization with weight-sharing optimizations. We\nfully implement DiffoRA and design comprehensive experiments to evaluate its\nperformance. The experimental results demonstrate that DiffoRA delivers\nstate-of-the-art results across multiple benchmarks."}
{"id": "2411.04281", "pdf": "https://arxiv.org/pdf/2411.04281", "abs": "https://arxiv.org/abs/2411.04281", "authors": ["Xingran Chen", "Zhenke Wu", "Xu Shi", "Hyunghoon Cho", "Bhramar Mukherjee"], "title": "Generating Synthetic Electronic Health Record Data: a Methodological Scoping Review with Benchmarking on Phenotype Data and Open-Source Software", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We conduct a scoping review of existing approaches for synthetic EHR data\ngeneration, and benchmark major methods with proposed open-source software to\noffer recommendations for practitioners. We search three academic databases for\nour scoping review. Methods are benchmarked on open-source EHR datasets,\nMIMIC-III/IV. Seven existing methods covering major categories and two baseline\nmethods are implemented and compared. Evaluation metrics concern data fidelity,\ndownstream utility, privacy protection, and computational cost. 42 studies are\nidentified and classified into five categories. Seven open-source methods\ncovering all categories are selected, trained on MIMIC-III, and evaluated on\nMIMIC-III or MIMIC-IV for transportability considerations. Among them,\nGAN-based methods demonstrate competitive performance in fidelity and utility\non MIMIC-III; rule-based methods excel in privacy protection. Similar findings\nare observed on MIMIC-IV, except that GAN-based methods further outperform the\nbaseline methods in preserving fidelity. A Python package, \"SynthEHRella\", is\nprovided to integrate various choices of approaches and evaluation metrics,\nenabling more streamlined exploration and evaluation of multiple methods. We\nfound that method choice is governed by the relative importance of the\nevaluation metrics in downstream use cases. We provide a decision tree to guide\nthe choice among the benchmarked methods. Based on the decision tree, GAN-based\nmethods excel when distributional shifts exist between the training and testing\npopulations. Otherwise, CorGAN and MedGAN are most suitable for association\nmodeling and predictive modeling, respectively. Future research should\nprioritize enhancing fidelity of the synthetic data while controlling privacy\nexposure, and comprehensive benchmarking of longitudinal or conditional\ngeneration methods."}
{"id": "2407.09887", "pdf": "https://arxiv.org/pdf/2407.09887", "abs": "https://arxiv.org/abs/2407.09887", "authors": ["Zhicheng Yang", "Yiwei Wang", "Yinya Huang", "Zhijiang Guo", "Wei Shi", "Xiongwei Han", "Liang Feng", "Linqi Song", "Xiaodan Liang", "Jing Tang"], "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models."}
{"id": "2503.03962", "pdf": "https://arxiv.org/pdf/2503.03962", "abs": "https://arxiv.org/abs/2503.03962", "authors": ["Catherine Arnett", "Tyler A. Chang", "James A. Michaelov", "Benjamin K. Bergen"], "title": "On the Acquisition of Shared Grammatical Representations in Bilingual Language Models", "categories": ["cs.CL"], "comment": "9 pages, 5 figures. Accepted at ACL 2025", "summary": "Crosslingual transfer is crucial to contemporary language models'\nmultilingual capabilities, but how it occurs is not well understood. We ask\nwhat happens to a monolingual language model when it begins to be trained on a\nsecond language. Specifically, we train small bilingual models for which we\ncontrol the amount of data for each language and the order of language\nexposure. To find evidence of shared multilingual representations, we turn to\nstructural priming, a method used to study grammatical representations in\nhumans. We first replicate previous crosslingual structural priming results and\nfind that after controlling for training data quantity and language exposure,\nthere are asymmetrical effects across language pairs and directions. We argue\nthat this asymmetry may shape hypotheses about human structural priming\neffects. We also find that structural priming effects are less robust for less\nsimilar language pairs, highlighting potential limitations of crosslingual\ntransfer learning and shared representations for typologically diverse\nlanguages."}
{"id": "2502.09356", "pdf": "https://arxiv.org/pdf/2502.09356", "abs": "https://arxiv.org/abs/2502.09356", "authors": ["Gabriel Tseng", "Anthony Fuller", "Marlena Reil", "Henry Herzog", "Patrick Beukema", "Favyen Bastani", "James R. Green", "Evan Shelhamer", "Hannah Kerner", "David Rolnick"], "title": "Galileo: Learning Global & Local Features of Many Remote Sensing Modalities", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a highly multimodal transformer to represent many remote sensing\nmodalities - multispectral optical, synthetic aperture radar, elevation,\nweather, pseudo-labels, and more - across space and time. These inputs are\nuseful for diverse remote sensing tasks, such as crop mapping and flood\ndetection. However, learning shared representations of remote sensing data is\nchallenging, given the diversity of relevant data modalities, and because\nobjects of interest vary massively in scale, from small boats (1-2 pixels and\nfast) to glaciers (thousands of pixels and slow). We present a novel\nself-supervised learning algorithm that extracts multi-scale features across a\nflexible set of input modalities through masked modeling. Our dual global and\nlocal contrastive losses differ in their targets (deep representations vs.\nshallow input projections) and masking strategies (structured vs. not). Our\nGalileo is a single generalist model that outperforms SoTA specialist models\nfor satellite images and pixel time series across eleven benchmarks and\nmultiple tasks."}
{"id": "2411.13187", "pdf": "https://arxiv.org/pdf/2411.13187", "abs": "https://arxiv.org/abs/2411.13187", "authors": ["Erica Coppolillo", "Federico Cinus", "Marco Minici", "Francesco Bonchi", "Giuseppe Manco"], "title": "Engagement-Driven Content Generation with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation."}
{"id": "2408.13805", "pdf": "https://arxiv.org/pdf/2408.13805", "abs": "https://arxiv.org/abs/2408.13805", "authors": ["Ioannis Athanasiadis", "Fredrik Lindsten", "Michael Felsberg"], "title": "Prior Learning in Introspective VAEs", "categories": ["cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) are a popular framework for unsupervised\nlearning and data generation. A plethora of methods have been proposed focusing\non improving VAEs, with the incorporation of adversarial objectives and the\nintegration of prior learning mechanisms being prominent directions. When it\ncomes to the former, an indicative instance is the recently introduced family\nof Introspective VAEs aiming at ensuring that a low likelihood is assigned to\nunrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE),\none of only two members of the Introspective VAE family, the other being the\noriginal IntroVAE. We select S-IntroVAE for its state-of-the-art status and its\ntraining stability. In particular, we investigate the implication of\nincorporating a multimodal and trainable prior into this S-IntroVAE. Namely, we\nformulate the prior as a third player and show that when trained in cooperation\nwith the decoder constitutes an effective way for prior learning, which shares\nthe Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a\nmodified formulation of the optimal ELBO in S-IntroVAE, we develop\ntheoretically motivated regularizations, namely (i) adaptive variance clipping\nto stabilize training when learning the prior and (ii) responsibility\nregularization to discourage the formation of inactive prior modes. Finally, we\nperform a series of targeted experiments on a 2D density estimation benchmark\nand in an image generation setting comprised of the (F)-MNIST and CIFAR-10\ndatasets demonstrating the effect of prior learning in S-IntroVAE in generation\nand representation learning."}
{"id": "2503.06706", "pdf": "https://arxiv.org/pdf/2503.06706", "abs": "https://arxiv.org/abs/2503.06706", "authors": ["Ming Zhang", "Yuhui Wang", "Yujiong Shen", "Tingyi Yang", "Changhao Jiang", "Yilong Wu", "Shihan Dou", "Qinhao Chen", "Zhiheng Xi", "Zhihao Zhang", "Yi Dong", "Zhen Wang", "Zhihui Fei", "Mingyang Wan", "Tao Liang", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial."}
{"id": "2502.13967", "pdf": "https://arxiv.org/pdf/2502.13967", "abs": "https://arxiv.org/abs/2502.13967", "authors": ["Roman Bachmann", "Jesse Allardice", "David Mizrahi", "Enrico Fini", "Oğuzhan Fatih Kar", "Elmira Amirloo", "Alaaeldin El-Nouby", "Amir Zamir", "Afshin Dehghan"], "title": "FlexTok: Resampling Images into 1D Token Sequences of Flexible Length", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025. Project page at https://flextok.epfl.ch/", "summary": "Image tokenization has enabled major advances in autoregressive image\ngeneration by providing compressed, discrete representations that are more\nefficient to process than raw pixels. While traditional approaches use 2D grid\ntokenization, recent methods like TiTok have shown that 1D tokenization can\nachieve high generation quality by eliminating grid redundancies. However,\nthese methods typically use a fixed number of tokens and thus cannot adapt to\nan image's inherent complexity. We introduce FlexTok, a tokenizer that projects\n2D images into variable-length, ordered 1D token sequences. For example, a\n256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,\nhierarchically and semantically compressing its information. By training a\nrectified flow model as the decoder and using nested dropout, FlexTok produces\nplausible reconstructions regardless of the chosen token sequence length. We\nevaluate our approach in an autoregressive generation setting using a simple\nGPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to\n128 tokens, outperforming TiTok and matching state-of-the-art methods with far\nfewer tokens. We further extend the model to support to text-conditioned image\ngeneration and examine how FlexTok relates to traditional 2D tokenization. A\nkey finding is that FlexTok enables next-token prediction to describe images in\na coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate\ndepends on the complexity of the generation task."}
{"id": "2411.16666", "pdf": "https://arxiv.org/pdf/2411.16666", "abs": "https://arxiv.org/abs/2411.16666", "authors": ["Jiaan Han", "Junxiao Chen", "Yanzhe Fu"], "title": "CatNet: Controlling the False Discovery Rate in LSTM with SHAP Feature Importance and Gaussian Mirrors", "categories": ["stat.ML", "cs.AI", "cs.LG", "q-fin.ST"], "comment": null, "summary": "We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM. CatNet employs the\nderivative of SHAP values to quantify the feature importance, and constructs a\nvector-formed mirror statistic for FDR control with the Gaussian Mirror\nalgorithm. To avoid instability due to nonlinear or temporal correlations among\nfeatures, we also propose a new kernel-based independence measure. CatNet\nperforms robustly on different model settings with both simulated and\nreal-world data, which reduces overfitting and improves interpretability of the\nmodel. Our framework that introduces SHAP for feature importance in FDR control\nalgorithms and improves Gaussian Mirror can be naturally extended to other\ntime-series or sequential deep learning models."}
{"id": "2409.12915", "pdf": "https://arxiv.org/pdf/2409.12915", "abs": "https://arxiv.org/abs/2409.12915", "authors": ["Michał Wiliński", "Mononito Goswami", "Willa Potosnak", "Nina Żukowska", "Artur Dubrawski"], "title": "Exploring Representations and Interventions in Time Series Foundation Models", "categories": ["cs.LG"], "comment": "Accepted at ICML'25", "summary": "Time series foundation models (TSFMs) promise to be powerful tools for a wide\nrange of applications. However, their internal representations and learned\nconcepts are still not well understood. In this study, we investigate the\nstructure and redundancy of representations across various TSFMs, examining the\nself-similarity of model layers within and across different model sizes. This\nanalysis reveals block-like redundancy in the representations, which can be\nutilized for informed pruning to improve inference speed and efficiency.\nAdditionally, we explore the concepts learned by these models - such as\nperiodicity and trends - and how these can be manipulated through latent space\nsteering to influence model behavior. Our experiments show that steering\ninterventions can introduce new features, e.g., adding periodicity or trends to\nsignals that initially lacked them. These findings underscore the value of\nrepresentational analysis for optimizing models and demonstrate how conceptual\nsteering offers new possibilities for more controlled and efficient time series\nanalysis with TSFMs."}
{"id": "2503.08042", "pdf": "https://arxiv.org/pdf/2503.08042", "abs": "https://arxiv.org/abs/2503.08042", "authors": ["Naomi Baes", "Raphaël Merx", "Nick Haslam", "Ekaterina Vylomova", "Haim Dubossarsky"], "title": "LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data", "categories": ["cs.CL"], "comment": "Accepted to ACL Findings (9-page long paper; 35 pages total including\n  limitations, appendices and references)", "summary": "Lexical Semantic Change (LSC) provides insight into cultural and social\ndynamics. Yet, the validity of methods for measuring different kinds of LSC\nremains unestablished due to the absence of historical benchmark datasets. To\naddress this gap, we propose LSC-Eval, a novel three-stage general-purpose\nevaluation framework to: (1) develop a scalable methodology for generating\nsynthetic datasets that simulate theory-driven LSC using In-Context Learning\nand a lexical database; (2) use these datasets to evaluate the sensitivity of\ncomputational methods to synthetic change; and (3) assess their suitability for\ndetecting change in specific dimensions and domains. We apply LSC-Eval to\nsimulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions,\nas defined in the SIBling framework, using examples from psychology. We then\nevaluate the ability of selected methods to detect these controlled\ninterventions. Our findings validate the use of synthetic benchmarks,\ndemonstrate that tailored methods effectively detect changes along SIB\ndimensions, and reveal that a state-of-the-art LSC model faces challenges in\ndetecting affective dimensions of LSC. LSC-Eval offers a valuable tool for\ndimension- and domain-specific benchmarking of LSC methods, with particular\nrelevance to the social sciences."}
{"id": "2502.15167", "pdf": "https://arxiv.org/pdf/2502.15167", "abs": "https://arxiv.org/abs/2502.15167", "authors": ["Chuan Cui", "Kejiang Chen", "Zhihua Wei", "Wen Shen", "Weiming Zhang", "Nenghai Yu"], "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment", "categories": ["cs.CV"], "comment": "24 pages. This work has been submitted to the ACM for possible\n  publication", "summary": "The rapid advancement of AI-generated image (AIGI) models presents new\nchallenges for evaluating image quality, particularly across three aspects:\nperceptual quality, prompt correspondence, and authenticity. To address these\nchallenges, we introduce M3-AGIQA, a comprehensive framework that leverages\nMultimodal Large Language Models (MLLMs) to enable more human-aligned, holistic\nevaluation of AI-generated images across both visual and textual domains.\nBesides, our framework features a structured multi-round evaluation process,\ngenerating and analyzing intermediate image descriptions to provide deeper\ninsight into these three aspects. By aligning model outputs more closely with\nhuman judgment, M3-AGIQA delivers robust and interpretable quality scores.\nExtensive experiments on multiple benchmarks demonstrate that our method\nachieves state-of-the-art performance on tested datasets and aspects, and\nexhibits strong generalizability in most cross-dataset settings. Code is\navailable at https://github.com/strawhatboy/M3-AGIQA."}
{"id": "2411.19647", "pdf": "https://arxiv.org/pdf/2411.19647", "abs": "https://arxiv.org/abs/2411.19647", "authors": ["Shaowen Wang", "Anan Liu", "Jian Xiao", "Huan Liu", "Yuekui Yang", "Cong Xu", "Qianqian Pu", "Suncong Zheng", "Wei Zhang", "Di Wang", "Jie Jiang", "Jian Li"], "title": "CAdam: Confidence-Based Optimization for Online Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Modern recommendation systems frequently employ online learning to\ndynamically update their models with freshly collected data. The most commonly\nused optimizer for updating neural networks in these contexts is the Adam\noptimizer, which integrates momentum ($m_t$) and adaptive learning rate\n($v_t$). However, the volatile nature of online learning data, characterized by\nits frequent distribution shifts and presence of noise, poses significant\nchallenges to Adam's standard optimization process: (1) Adam may use outdated\nmomentum and the average of squared gradients, resulting in slower adaptation\nto distribution changes, and (2) Adam's performance is adversely affected by\ndata noise. To mitigate these issues, we introduce CAdam, a confidence-based\noptimization strategy that assesses the consistency between the momentum and\nthe gradient for each parameter dimension before deciding on updates. If\nmomentum and gradient are in sync, CAdam proceeds with parameter updates\naccording to Adam's original formulation; if not, it temporarily withholds\nupdates and monitors potential shifts in data distribution in subsequent\niterations. This method allows CAdam to distinguish between the true\ndistributional shifts and mere noise, and to adapt more quickly to new data\ndistributions. In various settings with distribution shift or noise, our\nexperiments demonstrate that CAdam surpasses other well-known optimizers,\nincluding the original Adam. Furthermore, in large-scale A/B testing within a\nlive recommendation system, CAdam significantly enhances model performance\ncompared to Adam, leading to substantial increases in the system's gross\nmerchandise volume (GMV)."}
{"id": "2409.18850", "pdf": "https://arxiv.org/pdf/2409.18850", "abs": "https://arxiv.org/abs/2409.18850", "authors": ["Vladimír Boža", "Vladimír Macko"], "title": "Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization", "categories": ["cs.LG"], "comment": null, "summary": "Neural networks are often challenging to work with due to their large size\nand complexity. To address this, various methods aim to reduce model size by\nsparsifying or decomposing weight matrices, such as magnitude pruning and\nlow-rank or block-diagonal factorization. In this work, we present Double\nSparse Factorization (DSF), where we factorize each weight matrix into two\nsparse matrices. Although solving this problem exactly is computationally\ninfeasible, we propose an efficient heuristic based on alternating minimization\nvia ADMM that achieves state-of-the-art results, enabling unprecedented\nsparsification of neural networks. For instance, in a one-shot pruning setting,\nour method can reduce the size of the LLaMA2-13B model by 50% while maintaining\nbetter performance than the dense LLaMA2-7B model. We also compare favorably\nwith Optimal Brain Compression, the state-of-the-art layer-wise pruning\napproach for convolutional neural networks. Furthermore, accuracy improvements\nof our method persist even after further model fine-tuning.\n  Code available at: https://github.com/usamec/double_sparse."}
{"id": "2503.10267", "pdf": "https://arxiv.org/pdf/2503.10267", "abs": "https://arxiv.org/abs/2503.10267", "authors": ["Laurie Burchell", "Ona de Gibert", "Nikolay Arefyev", "Mikko Aulamo", "Marta Bañón", "Pinzhen Chen", "Mariia Fedorova", "Liane Guillou", "Barry Haddow", "Jan Hajič", "Jindřich Helcl", "Erik Henriksson", "Mateusz Klimaszewski", "Ville Komulainen", "Andrey Kutuzov", "Joona Kytöniemi", "Veronika Laippala", "Petter Mæhlum", "Bhavitvya Malik", "Farrokh Mehryary", "Vladislav Mikhailov", "Nikita Moghe", "Amanda Myntti", "Dayyán O'Brien", "Stephan Oepen", "Proyag Pal", "Jousia Piha", "Sampo Pyysalo", "Gema Ramírez-Sánchez", "David Samuel", "Pavel Stepachev", "Jörg Tiedemann", "Dušan Variš", "Tereza Vojtěchová", "Jaume Zaragoza-Bernabeu"], "title": "An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)", "categories": ["cs.CL"], "comment": "ACL'2025 Main Proceedings", "summary": "Training state-of-the-art large language models requires vast amounts of\nclean and diverse textual data. However, building suitable multilingual\ndatasets remains a challenge. In this work, we present HPLT v2, a collection of\nhigh-quality multilingual monolingual and parallel corpora, extending prior\nwork of the HPLT project. The monolingual portion of the data contains 8T\ntokens covering 193 languages, while the parallel data contains 380M sentence\npairs covering 51 languages. We document the entire data pipeline and release\nthe code to reproduce it. We provide extensive analysis of the quality and\ncharacteristics of our data. Finally, we evaluate the performance of language\nmodels and machine translation systems trained on HPLT v2, demonstrating its\nvalue."}
{"id": "2503.02101", "pdf": "https://arxiv.org/pdf/2503.02101", "abs": "https://arxiv.org/abs/2503.02101", "authors": ["Boyong He", "Yuxiang Ji", "Qianwen Ye", "Zhuoyue Tan", "Liaoni Wu"], "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection", "categories": ["cs.CV"], "comment": "CVPR2025 camera-ready version with supplementary material", "summary": "Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\nhttps://github.com/heboyong/Generalized-Diffusion-Detector."}
{"id": "2412.00418", "pdf": "https://arxiv.org/pdf/2412.00418", "abs": "https://arxiv.org/abs/2412.00418", "authors": ["Yu Shi", "Yiqi Wang", "WeiXuan Lang", "Jiaxin Zhang", "Pan Dong", "Aiping Li"], "title": "Mixture of Experts for Node Classification", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Nodes in the real-world graphs exhibit diverse patterns in numerous aspects,\nsuch as degree and homophily. However, most existent node predictors fail to\ncapture a wide range of node patterns or to make predictions based on distinct\nnode patterns, resulting in unsatisfactory classification performance. In this\npaper, we reveal that different node predictors are good at handling nodes with\nspecific patterns and only apply one node predictor uniformly could lead to\nsuboptimal result. To mitigate this gap, we propose a mixture of experts\nframework, MoE-NP, for node classification. Specifically, MoE-NP combines a\nmixture of node predictors and strategically selects models based on node\npatterns. Experimental results from a range of real-world datasets demonstrate\nsignificant performance improvements from MoE-NP."}
{"id": "2410.01679", "pdf": "https://arxiv.org/pdf/2410.01679", "abs": "https://arxiv.org/abs/2410.01679", "authors": ["Amirhossein Kazemnejad", "Milad Aghajohari", "Eva Portelance", "Alessandro Sordoni", "Siva Reddy", "Aaron Courville", "Nicolas Le Roux"], "title": "VinePPO: Refining Credit Assignment in RL Training of LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ICML 2025; 12 pages and 22 pages Appendix", "summary": "Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a common reinforcement\nlearning (RL) algorithm used for LLM finetuning, employs value networks to\ntackle credit assignment. However, recent approaches achieve strong results\nwithout it, raising questions about the efficacy of value networks in practice.\nIn this work, we systematically evaluate the efficacy of value networks and\nreveal their significant shortcomings in reasoning-heavy LLM tasks, showing\nthat they often produce poor estimate of expected return and barely outperform\na random baseline when comparing alternative steps. This motivates our key\nquestion: Can improved credit assignment enhance RL training for LLMs? To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates. Our method consistently outperforms PPO and other baselines across\nMATH and GSM8K datasets in less wall-clock time (up to 3.0x). Crucially, it\nachieves higher test accuracy for a given training accuracy, capturing more\ngeneralization signal per sample. These results emphasize the importance of\naccurate credit assignment in RL training of LLM."}
{"id": "2503.10515", "pdf": "https://arxiv.org/pdf/2503.10515", "abs": "https://arxiv.org/abs/2503.10515", "authors": ["Florian Eichin", "Yang Janet Liu", "Barbara Plank", "Michael A. Hedderich"], "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set", "categories": ["cs.CL"], "comment": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes, camera-ready revision for ACL\n  2025", "summary": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses."}
{"id": "2503.04167", "pdf": "https://arxiv.org/pdf/2503.04167", "abs": "https://arxiv.org/abs/2503.04167", "authors": ["Yufang Liu", "Yao Du", "Tao Ji", "Jianing Wang", "Yang Liu", "Yuanbin Wu", "Aimin Zhou", "Mengdi Zhang", "Xunliang Cai"], "title": "The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}."}
{"id": "2412.14468", "pdf": "https://arxiv.org/pdf/2412.14468", "abs": "https://arxiv.org/abs/2412.14468", "authors": ["Aditya Desai", "Shuo Yang", "Alejandro Cuadron", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "title": "HashAttention: Semantic Sparsity for Faster Inference", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICML'2025", "summary": "Leveraging long contexts is crucial for advanced AI systems, but attention\ncomputation poses a scalability challenge. While scaled dot-product attention\n(SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly\ncontribute to output, exploiting this sparsity remains challenging. Existing\nmethods either suffer from quality degradation or require substantial\nadditional resources. We show that identifying pivotal tokens is a Maximum\nInner Product Search (MIPS) problem. However, existing MIPS solutions are not\nwell-suited for SDPA, as they are not GPU-friendly and often underperform due\nto the separated query and key distributions. This paper introduces\nHashAttention, framing pivotal token identification as a recommendation\nproblem. Given a query, HashAttention encodes keys and queries in Hamming\nspace, capturing the required semantic similarity, using learned mapping\nfunctions. HashAttention efficiently identifies pivotal tokens for a given\nquery using bitwise operations and computes attention using only these tokens,\nimproving the overall attention efficiency. Trained on generic data,\nHashAttention reduces tokens used by up to $16\\times$ with minimal quality\nloss, requiring only 32 bits of auxiliary memory per token. Sparsity can be\nfurther improved to $32\\times$ through task-specific fine-tuning. On A100 GPU,\nat $32\\times$ sparsity, incorporating HashAttention reduces attention latency\nby up to $4.3\\times$ in GPT-FAST and $2.54\\times$ in FlashDecode, and achieves\nup to $3.12\\times$ higher throughput for GPT-FAST."}
{"id": "2410.08868", "pdf": "https://arxiv.org/pdf/2410.08868", "abs": "https://arxiv.org/abs/2410.08868", "authors": ["Navdeep Kumar", "Priyank Agrawal", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "On the Convergence of Single-Timescale Actor-Critic", "categories": ["cs.LG", "stat.ML"], "comment": "updated version , 27 pages", "summary": "We analyze the global convergence of the single-timescale actor-critic (AC)\nalgorithm for the infinite-horizon discounted Markov Decision Processes (MDPs)\nwith finite state spaces. To this end, we introduce an elegant analytical\nframework for handling complex, coupled recursions inherent in the algorithm.\nLeveraging this framework, we establish that the algorithm converges to an\n$\\epsilon$-close \\textbf{globally optimal} policy with a sample complexity of\n\\( O(\\epsilon^{-3}) \\). This significantly improves upon the existing\ncomplexity of $O(\\epsilon^{-2})$ to achieve $\\epsilon$-close \\textbf{stationary\npolicy}, which is equivalent to the complexity of $O(\\epsilon^{-4})$ to achieve\n$\\epsilon$-close \\textbf{globally optimal} policy using gradient domination\nlemma. Furthermore, we demonstrate that to achieve this improvement, the step\nsizes for both the actor and critic must decay as \\( O(k^{-\\frac{2}{3}}) \\)\nwith iteration $k$, diverging from the conventional \\( O(k^{-\\frac{1}{2}}) \\)\nrates commonly used in (non)convex optimization."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025. This version corrects a typo in the results table", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.05283", "pdf": "https://arxiv.org/pdf/2503.05283", "abs": "https://arxiv.org/abs/2503.05283", "authors": ["Souhail Hadgi", "Luca Moschella", "Andrea Santilli", "Diego Gomez", "Qixing Huang", "Emanuele Rodolà", "Simone Melzi", "Maks Ovsjanikov"], "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations. Our code and weights are available at\nhttps://github.com/Souhail-01/3d-text-alignment"}
{"id": "2501.06164", "pdf": "https://arxiv.org/pdf/2501.06164", "abs": "https://arxiv.org/abs/2501.06164", "authors": ["Satchel Grant"], "title": "Model Alignment Search", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When can we say that two neural systems are the same? The answer to this\nquestion is goal-dependent, and it is often addressed through correlative\nmethods such as Representational Similarity Analysis (RSA) and Centered Kernel\nAlignment (CKA). What nuances do we miss, however, when we fail to causally\nprobe the representations? Do the dangers of cause vs. correlation exist in\ncomparative representational analyses? In this work, we introduce a method for\nconnecting neural representational similarity to behavior through causal\ninterventions. The method learns orthogonal transformations that find an\naligned subspace in which behavioral information from multiple distributed\nnetworks' representations can be isolated and interchanged. We first show that\nthe method can be used to transfer the behavior from one frozen Neural Network\n(NN) to another in a manner similar to model stitching, and we show how the\nmethod can complement correlative similarity measures like RSA. We then\nintroduce an efficient subspace orthogonalization technique using the\nGram-Schmidt process -- that can also be used for Distributed Alignment Search\n(DAS) -- allowing us to perform analyses on larger models. Next, we empirically\nand theoretically show how our method can be equivalent to model stitching when\ndesired, or it can take a form that is more restrictive to causal information,\nand in both cases, it reduces the number of required matrices for a comparison\nof n models from quadratic to linear in n. We then show how we can augment the\nloss objective with an auxiliary loss to train causally relevant alignments\neven when we can only read the representations from one of the two networks\nduring training (like with biological networks). Lastly, we use number\nrepresentations as a case study to explore how our method can be used to\ncompare specific types of representational information across tasks and models."}
{"id": "2410.10404", "pdf": "https://arxiv.org/pdf/2410.10404", "abs": "https://arxiv.org/abs/2410.10404", "authors": ["Zachary Chase", "Idan Mehalel"], "title": "Deterministic Apple Tasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In binary ($0/1$) online classification with apple tasting feedback, the\nlearner receives feedback only when predicting $1$. Besides some degenerate\nlearning tasks, all previously known learning algorithms for this model are\nrandomized. Consequently, prior to this work it was unknown whether\ndeterministic apple tasting is generally feasible. In this work, we provide the\nfirst widely-applicable deterministic apple tasting learner, and show that in\nthe realizable case, a hypothesis class is learnable if and only if it is\ndeterministically learnable, confirming a conjecture of [Raman, Subedi, Raman,\nTewari-24]. Quantitatively, we show that every class $\\mathcal{H}$ is learnable\nwith mistake bound $O \\left(\\sqrt{\\mathtt{L}(\\mathcal{H}) T \\log T} \\right)$\n(where $\\mathtt{L}(\\mathcal{H})$ is the Littlestone dimension of\n$\\mathcal{H}$), and that this is tight for some classes.\n  We further study the agnostic case, in which the best hypothesis makes at\nmost $k$ many mistakes, and prove a trichotomy stating that every class\n$\\mathcal{H}$ must be either easy, hard, or unlearnable. Easy classes have\n(both randomized and deterministic) mistake bound $\\Theta_{\\mathcal{H}}(k)$.\nHard classes have randomized mistake bound $\\tilde{\\Theta}_{\\mathcal{H}}\n\\left(k + \\sqrt{T} \\right)$, and deterministic mistake bound\n$\\tilde{\\Theta}_{\\mathcal{H}} \\left(\\sqrt{k \\cdot T} \\right)$, where $T$ is the\ntime horizon. Unlearnable classes have (both randomized and deterministic)\nmistake bound $\\Theta(T)$.\n  Our upper bound is based on a deterministic algorithm for learning from\nexpert advice with apple tasting feedback, a problem interesting in its own\nright. For this problem, we show that the optimal deterministic mistake bound\nis $\\Theta \\left(\\sqrt{T (k + \\log n)} \\right)$ for all $k$ and $T \\leq n \\leq\n2^T$, where $n$ is the number of experts."}
{"id": "2503.15850", "pdf": "https://arxiv.org/pdf/2503.15850", "abs": "https://arxiv.org/abs/2503.15850", "authors": ["Xiaoou Liu", "Tiejin Chen", "Longchao Da", "Chacha Chen", "Zhen Lin", "Hua Wei"], "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability."}
{"id": "2503.06764", "pdf": "https://arxiv.org/pdf/2503.06764", "abs": "https://arxiv.org/abs/2503.06764", "authors": ["Zisheng Chen", "Chunwei Wang", "Xiuwei Chen", "Hongbin Xu", "Runhui Huang", "Jun Zhou", "Jianhua Han", "Hang Xu", "Xiaodan Liang"], "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review, Refer to the latest version", "summary": "In this paper, we introduce SemHiTok, a unified image Tokenizer via\nSemantic-Guided Hierarchical codebook that provides consistent discrete\nrepresentations for multimodal understanding and generation. Recently, unified\nimage tokenizers have sparked exploration within research community, which is\ndesigned to capture high-level semantic features for understanding and\nretaining low-level pixel features for generation. Previous works attempt to\ntrain a unified image tokenizer by combining loss for semantic distillation and\npixel reconstruction. However, due to the differing levels of features\nprioritized by multimodal understanding and generation, joint training methods\nface significant challenges in achieving a good trade-off. SemHiTok addresses\nthis challenge through a novel semantic-guided hierarchical codebook, which\nbuilds pixel sub-codebooks on a pretrained semantic codebook. This design\ndecouples semantic and pixel both in terms of structure and training strategy,\nenabling the tokenizer to capture pixel features while retaining its ability to\ncomprehend high-level semantic information. Our experiments demonstrate that\nSemHiTok achieves SOTA performance in image reconstruction and multimodal\nunderstanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with\nSemHiTok, which exhibits superior performance across multimodal understanding\nand generation tasks. For understanding, SemHiTok achieves impressive\nperformance on most benchmarks. For generation, our model achieves SOTA\nperformance on MJHQ30K in unified MLLMs."}
{"id": "2501.09328", "pdf": "https://arxiv.org/pdf/2501.09328", "abs": "https://arxiv.org/abs/2501.09328", "authors": ["Yixiao Xu", "Binxing Fang", "Rui Wang", "Yinghai Zhou", "Yuan Liu", "Mohan Li", "Zhihong Tian"], "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from 193,252 to 1,857 with zero training cost. The\ncode is available at https://github.com/NeurHT/NeurHT."}
{"id": "2410.11289", "pdf": "https://arxiv.org/pdf/2410.11289", "abs": "https://arxiv.org/abs/2410.11289", "authors": ["Yutong He", "Pengrui Li", "Yipeng Hu", "Chuyan Chen", "Kun Yuan"], "title": "Subspace Optimization for Large Language Models with Convergence Guarantees", "categories": ["cs.LG", "math.OC"], "comment": "Accepted by ICML 2025", "summary": "Subspace optimization algorithms, such as GaLore (Zhao et al., 2024), have\ngained attention for pre-training and fine-tuning large language models (LLMs)\ndue to their memory efficiency. However, their convergence guarantees remain\nunclear, particularly in stochastic settings. In this paper, we reveal that\nGaLore does not always converge to the optimal solution and provide an explicit\ncounterexample to support this finding. We further explore the conditions under\nwhich GaLore achieves convergence, showing that it does so when either (i) a\nsufficiently large mini-batch size is used or (ii) the gradient noise is\nisotropic. More significantly, we introduce GoLore (Gradient random Low-rank\nprojection), a novel variant of GaLore that provably converges in typical\nstochastic settings, even with standard batch sizes. Our convergence analysis\nextends naturally to other subspace optimization algorithms. Finally, we\nempirically validate our theoretical results and thoroughly test the proposed\nmechanisms. Codes are available at https://github.com/pkumelon/Golore."}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512", "abs": "https://arxiv.org/abs/2503.23512", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Xinhang Yuan", "Li Sun", "Yi Xin", "Keqin Li", "Kuan Lu", "Menghao Huo", "Jiaqi Chen", "Tianyu Shi"], "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives."}
{"id": "2503.10042", "pdf": "https://arxiv.org/pdf/2503.10042", "abs": "https://arxiv.org/abs/2503.10042", "authors": ["Ziyue Wang", "Yurui Dong", "Fuwen Luo", "Minyuan Ruan", "Zhili Cheng", "Chi Chen", "Peng Li", "Yang Liu"], "title": "EscapeCraft: A 3D Room Escape Environment for Benchmarking Complex Multimodal Reasoning Ability", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities."}
{"id": "2501.11454", "pdf": "https://arxiv.org/pdf/2501.11454", "abs": "https://arxiv.org/abs/2501.11454", "authors": ["Akash Kundu"], "title": "Improving thermal state preparation of Sachdev-Ye-Kitaev model with reinforcement learning on quantum hardware", "categories": ["quant-ph", "cs.AI", "cs.LG", "hep-lat", "hep-th"], "comment": "Accepted in Machine Learning: Science and Technology. Code at\n  https://github.com/Aqasch/solving_SYK_model_with_RL", "summary": "The Sachdev-Ye-Kitaev (SYK) model, known for its strong quantum correlations\nand chaotic behavior, serves as a key platform for quantum gravity studies.\nHowever, variationally preparing thermal states on near-term quantum processors\nfor large systems ($N>12$, where $N$ is the number of Majorana fermions)\npresents a significant challenge due to the rapid growth in the complexity of\nparameterized quantum circuits. This paper addresses this challenge by\nintegrating reinforcement learning (RL) with convolutional neural networks,\nemploying an iterative approach to optimize the quantum circuit and its\nparameters. The refinement process is guided by a composite reward signal\nderived from entropy and the expectation values of the SYK Hamiltonian. This\napproach reduces the number of CNOT gates by two orders of magnitude for\nsystems $N\\geq12$ compared to traditional methods like first-order\nTrotterization. We demonstrate the effectiveness of the RL framework in both\nnoiseless and noisy quantum hardware environments, maintaining high accuracy in\nthermal state preparation. This work advances a scalable, RL-based framework\nwith applications for quantum gravity studies and out-of-time-ordered thermal\ncorrelators computation in quantum many-body systems on near-term quantum\nhardware. The code is available at\nhttps://github.com/Aqasch/solving_SYK_model_with_RL."}
{"id": "2410.13448", "pdf": "https://arxiv.org/pdf/2410.13448", "abs": "https://arxiv.org/abs/2410.13448", "authors": ["Jinyang Liu", "Tessa Steensgaard", "Marvin N. Wright", "Niklas Pfister", "Munir Hiabu"], "title": "Fast Estimation of Partial Dependence Functions using Trees", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Many existing interpretation methods are based on Partial Dependence (PD)\nfunctions that, for a pre-trained machine learning model, capture how a subset\nof the features affects the predictions by averaging over the remaining\nfeatures.\n  Notable methods include Shapley additive explanations (SHAP) which computes\nfeature contributions based on a game theoretical interpretation and PD plots\n(i.e., 1-dim PD functions) that capture average marginal main effects. Recent\nwork has connected these approaches using a functional decomposition and argues\nthat SHAP values can be misleading since they merge main and interaction\neffects into a single local effect. However, a major advantage of SHAP compared\nto other PD-based interpretations has been the availability of fast estimation\ntechniques, such as \\texttt{TreeSHAP}.\n  In this paper, we propose a new tree-based estimator, \\texttt{FastPD}, which\nefficiently estimates arbitrary PD functions.\n  We show that \\texttt{FastPD} consistently estimates the desired population\nquantity -- in contrast to path-dependent \\texttt{TreeSHAP} which is\ninconsistent when features are correlated.\n  For moderately deep trees, \\texttt{FastPD} improves the complexity of\nexisting methods from quadratic to linear in the number of observations.\n  By estimating PD functions for arbitrary feature subsets, \\texttt{FastPD} can\nbe used to extract PD-based interpretations such as SHAP, PD plots and\nhigher-order interaction effects."}
{"id": "2503.23899", "pdf": "https://arxiv.org/pdf/2503.23899", "abs": "https://arxiv.org/abs/2503.23899", "authors": ["Diana Galvan-Sosa", "Gabrielle Gaudeau", "Pride Kavumba", "Yunmeng Li", "Hongyi gu", "Zheng Yuan", "Keisuke Sakaguchi", "Paula Buttery"], "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset", "categories": ["cs.CL", "I.2.7"], "comment": "10 main pages (24 appendix pages), 9 figures, accepted to ACL 2025", "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube."}
{"id": "2503.10691", "pdf": "https://arxiv.org/pdf/2503.10691", "abs": "https://arxiv.org/abs/2503.10691", "authors": ["Qiji Zhou", "Yifan Gong", "Guangsheng Bao", "Hongjie Qiu", "Jinqiang Li", "Xiangrong Zhu", "Huajian Zhang", "Yue Zhang"], "title": "Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation", "categories": ["cs.CV"], "comment": "It has been accepted to the ACL-2025 Findings", "summary": "Counterfactual reasoning is crucial for robust video understanding but\nremains underexplored in existing multimodal benchmarks. In this paper, we\nintroduce \\textbf{COVER} (\\textbf{\\underline{CO}}unterfactual\n\\textbf{\\underline{V}}id\\textbf{\\underline{E}}o\n\\textbf{\\underline{R}}easoning), a multidimensional multimodal benchmark that\nsystematically evaluates MLLMs across the abstract-concrete and\nperception-cognition dimensions. Beyond prior multimodal benchmarks, COVER\ndecomposes complex queries into structured sub-questions, enabling fine-grained\nreasoning analysis. Experiments on commercial and open-source models reveal a\nstrong correlation between sub-question accuracy and counterfactual reasoning\nperformance, highlighting the role of structured inference in video\nunderstanding. Furthermore, our results suggest a key insight: enhancing the\nreasoning capability of models is essential for improving the robustness of\nvideo understanding. COVER establishes a new standard for assessing MLLMs'\nlogical reasoning abilities in dynamic environments. Our work is available at\nhttps://github.com/gongyifan-hash/COVER-Benchmark."}
{"id": "2501.14755", "pdf": "https://arxiv.org/pdf/2501.14755", "abs": "https://arxiv.org/abs/2501.14755", "authors": ["Daoyuan Chen", "Yilun Huang", "Xuchen Pan", "Nana Jiang", "Haibin Wang", "Yilei Zhang", "Ce Ge", "Yushuo Chen", "Wenhao Zhang", "Zhijian Ma", "Jun Huang", "Wei Lin", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models", "categories": ["cs.DC", "cs.AI"], "comment": "34 pages, 10 figures, 3 tables", "summary": "The burgeoning field of foundation models necessitates advanced data\nprocessing mechanisms capable of harnessing vast and valuable data with various\ntypes used by these models. Nevertheless, the current landscape presents unique\nchallenges that traditional data processing frameworks struggle to handle\neffectively, particularly in handling the complexity of multimodal data. In\nresponse, we present Data-Juicer 2.0, a data processing system backed by 100+\ndata processing operators spanning text, image, video, and audio modalities,\nsupporting more critical tasks including data analysis, synthesis, annotation,\nand foundation model post-training. With seamless compatibility and dedicated\noptimization for popular dataset hubs like Hugging Face and computing engines\nlike Ray, it improves upon its predecessor in terms of usability, efficiency,\nand programmability. It features an easily accessible user interface layer that\nsupports decoupled Python interactions, RESTful APIs, and conversational\ncommands. It contains a new runtime layer optimized for adaptive execution and\nmanagement across varying dataset scales, processing demands, and computational\nenvironments, while hiding unnecessary system details. Extensive empirical\nevaluations demonstrate Data-Juicer 2.0's remarkable performance and\nscalability, highlighting its capability to efficiently process TB-level data\nwith 10k+ CPU cores. The system is publicly available and has been widely\nadopted in diverse research fields and real-world products such as Alibaba\nCloud PAI. We actively maintain it and share insights from practical feedback,\nwith the goal of facilitating research and application of next-generation\nfoundation models."}
{"id": "2411.05239", "pdf": "https://arxiv.org/pdf/2411.05239", "abs": "https://arxiv.org/abs/2411.05239", "authors": ["Moshik Hershcovitch", "Andrew Wood", "Leshem Choshen", "Guy Girmonsky", "Roy Leibovitz", "Ilias Ennmouri", "Michal Malka", "Peter Chin", "Swaminathan Sundararaman", "Danny Harnik"], "title": "ZipNN: Lossless Compression for AI Models", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "IEEE Cloud. arXiv admin note: text overlap with arXiv:2404.15198", "summary": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face."}
{"id": "2504.00030", "pdf": "https://arxiv.org/pdf/2504.00030", "abs": "https://arxiv.org/abs/2504.00030", "authors": ["Aayush Gautam", "Susav Shrestha", "Narasimha Reddy"], "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 1 table", "summary": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment."}
{"id": "2503.18223", "pdf": "https://arxiv.org/pdf/2503.18223", "abs": "https://arxiv.org/abs/2503.18223", "authors": ["Valentin Gabeff", "Haozhe Qi", "Brendan Flaherty", "Gencer Sumbül", "Alexander Mathis", "Devis Tuia"], "title": "MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps", "categories": ["cs.CV", "cs.IR", "q-bio.NC", "q-bio.QM"], "comment": "CVPR 2025; Benchmark and code at:\n  https://github.com/eceo-epfl/MammAlps. After submission of v1, we noticed\n  that a few audio files were not correctly aligned with the corresponding\n  video. We fixed the issue, which had little to no impact on performance. We\n  also now report results for three runs", "summary": "Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps://github.com/eceo-epfl/MammAlps"}
{"id": "2502.13135", "pdf": "https://arxiv.org/pdf/2502.13135", "abs": "https://arxiv.org/abs/2502.13135", "authors": ["Taedong Yun", "Eric Yang", "Mustafa Safdari", "Jong Ha Lee", "Vaishnavi Vinod Kumar", "S. Sara Mahdavi", "Jonathan Amar", "Derek Peyton", "Reut Aharony", "Andreas Michaelides", "Logan Schneider", "Isaac Galatzer-Levy", "Yugang Jia", "John Canny", "Arthur Gretton", "Maja Matarić"], "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "Accepted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions."}
{"id": "2411.17089", "pdf": "https://arxiv.org/pdf/2411.17089", "abs": "https://arxiv.org/abs/2411.17089", "authors": ["Chaoyi Jiang", "Lei Gao", "Hossein Entezari Zarch", "Murali Annavaram"], "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation", "categories": ["cs.LG", "cs.DC", "cs.PF"], "comment": "ACL Findings 2025", "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154", "abs": "https://arxiv.org/abs/2504.05154", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "title": "CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness", "categories": ["cs.CL"], "comment": "27 pages", "summary": "Language Models (LMs) are typically tuned with human preferences to produce\nhelpful responses, but the impact of preference tuning on the ability to handle\nculturally diverse queries remains understudied. In this paper, we\nsystematically analyze how native human cultural preferences can be\nincorporated into the preference learning process to train more culturally\naware LMs. We introduce \\textbf{CARE}, a multilingual resource containing 3,490\nculturally specific questions and 31.7k responses with native judgments. We\ndemonstrate how a modest amount of high-quality native preferences improves\ncultural awareness across various LMs, outperforming larger generic preference\ndata. Our analyses reveal that models with stronger initial cultural\nperformance benefit more from alignment, leading to gaps among models developed\nin different regions with varying access to culturally relevant data. CARE will\nbe made publicly available at https://github.com/Guochry/CARE."}
{"id": "2504.00816", "pdf": "https://arxiv.org/pdf/2504.00816", "abs": "https://arxiv.org/abs/2504.00816", "authors": ["Yeqi Fang", "Rong Zhou"], "title": "Two-stage deep learning framework for the restoration of incomplete-ring PET images", "categories": ["cs.CV", "physics.med-ph"], "comment": "20 pages, 7 figures", "summary": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging."}
{"id": "2502.14708", "pdf": "https://arxiv.org/pdf/2502.14708", "abs": "https://arxiv.org/abs/2502.14708", "authors": ["Kevin He", "Ran Shorrer", "Mengjia Xia"], "title": "Human Misperception of Generative-AI Alignment: A Laboratory Experiment", "categories": ["econ.TH", "cs.AI", "cs.GT"], "comment": null, "summary": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model."}
{"id": "2412.07169", "pdf": "https://arxiv.org/pdf/2412.07169", "abs": "https://arxiv.org/abs/2412.07169", "authors": ["Tal Zeevi", "Ravid Shwartz-Ziv", "Yann LeCun", "Lawrence H. Staib", "John A. Onofrey"], "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Accepted to the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025. Code available at:\n  https://github.com/code-supplement-25/rate-in", "summary": "Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications."}
{"id": "2504.05276", "pdf": "https://arxiv.org/pdf/2504.05276", "abs": "https://arxiv.org/abs/2504.05276", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "EDM 2025 Short Paper", "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."}
{"id": "2504.16656", "pdf": "https://arxiv.org/pdf/2504.16656", "abs": "https://arxiv.org/abs/2504.16656", "authors": ["Chris", "Yichen Wei", "Yi Peng", "Xiaokun Wang", "Weijie Qiu", "Wei Shen", "Tianyidan Xie", "Jiangbo Pei", "Jianhao Zhang", "Yunzhuo Hao", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that jointly leverages the\nMixed Preference Optimization (MPO) and the Group Relative Policy Optimization\n(GRPO), which harmonizes reward-model guidance with rule-based strategies,\nthereby addressing the long-standing challenge of balancing sophisticated\nreasoning capabilities with broad generalization. To further enhance training\nefficiency, we propose the Selective Sample Buffer (SSB) mechanism, which\neffectively addresses the vanishing advantages dilemma inherent in GRPO by\nprioritizing high-value samples throughout the optimization process. Notably,\nwe observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and\n73.6 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B."}
{"id": "2502.15805", "pdf": "https://arxiv.org/pdf/2502.15805", "abs": "https://arxiv.org/abs/2502.15805", "authors": ["Joongwon Lee", "Seonghwan Kim", "Seokhyun Moon", "Hyunwoo Kim", "Woo Youn Kim"], "title": "FragFM: Hierarchical Framework for Efficient Molecule Generation via Fragment-Level Discrete Flow Matching", "categories": ["cs.LG", "cs.AI", "physics.chem-ph"], "comment": "39 pages, 24 figures, under review", "summary": "We introduce FragFM, a novel hierarchical framework via fragment-level\ndiscrete flow matching for efficient molecular graph generation. FragFM\ngenerates molecules at the fragment level, leveraging a coarse-to-fine\nautoencoder to reconstruct details at the atom level. Together with a\nstochastic fragment bag strategy to effectively handle an extensive fragment\nspace, our framework enables more efficient and scalable molecular generation.\nWe demonstrate that our fragment-based approach achieves better property\ncontrol than the atom-based method and additional flexibility through\nconditioning the fragment bag. We also propose a Natural Product Generation\nbenchmark (NPGen) to evaluate modern molecular graph generative models' ability\nto generate natural product-like molecules. Since natural products are\nbiologically prevalidated and differ from typical drug-like molecules, our\nbenchmark provides a more challenging yet meaningful evaluation relevant to\ndrug discovery. We conduct a FragFM comparative study against various models on\ndiverse molecular generation benchmarks, including NPGen, demonstrating\nsuperior performance. The results highlight the potential of fragment-based\ngenerative modeling for large-scale, property-aware molecular design, paving\nthe way for more efficient exploration of chemical space."}
{"id": "2412.07326", "pdf": "https://arxiv.org/pdf/2412.07326", "abs": "https://arxiv.org/abs/2412.07326", "authors": ["Yael Itzhakev", "Amit Giloni", "Yuval Elovici", "Asaf Shabtai"], "title": "Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning models trained on tabular data are vulnerable to adversarial\nattacks, even in realistic scenarios where attackers only have access to the\nmodel's outputs. Since tabular data contains complex interdependencies among\nfeatures, it presents a unique challenge for adversarial samples which must\nmaintain coherence and respect these interdependencies to remain\nindistinguishable from benign data. Moreover, existing attack evaluation\nmetrics-such as the success rate, perturbation magnitude, and query count-fail\nto account for this challenge. To address those gaps, we propose a technique\nfor perturbing dependent features while preserving sample coherence. In\naddition, we introduce Class-Specific Anomaly Detection (CSAD), an effective\nnovel anomaly detection approach, along with concrete metrics for assessing the\nquality of tabular adversarial attacks. CSAD evaluates adversarial samples\nrelative to their predicted class distribution, rather than a broad benign\ndistribution. It ensures that subtle adversarial perturbations, which may\nappear coherent in other classes, are correctly identified as anomalies. We\nintegrate SHAP explainability techniques to detect inconsistencies in model\ndecision-making, extending CSAD for SHAP-based anomaly detection. Our\nevaluation incorporates both anomaly detection rates with SHAP-based\nassessments to provide a more comprehensive measure of adversarial sample\nquality. We evaluate various attack strategies, examining black-box query-based\nand transferability-based gradient attacks across four target models.\nExperiments on benchmark tabular datasets reveal key differences in the\nattacker's risk and effort and attack quality, offering insights into the\nstrengths, limitations, and trade-offs faced by attackers and defenders. Our\nfindings lay the groundwork for future research on adversarial attacks and\ndefense development in the tabular domain."}
{"id": "2504.06910", "pdf": "https://arxiv.org/pdf/2504.06910", "abs": "https://arxiv.org/abs/2504.06910", "authors": ["Sheng Lu", "Ilia Kuznetsov", "Iryna Gurevych"], "title": "Identifying Aspects in Peer Reviews", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review."}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin Özdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models."}
{"id": "2502.18097", "pdf": "https://arxiv.org/pdf/2502.18097", "abs": "https://arxiv.org/abs/2502.18097", "authors": ["Samuele Sabella", "Chiara Boldrini", "Lorenzo Valerio", "Andrea Passarella", "Marco Conti"], "title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted at IJCNN 2025. Funding: SoBigData PPP (101079043),\n  SoBigData.it (PNRR IR0000013), FAIR (PNRR PE00000013), RESTART (PNRR\n  PE00000001)", "summary": "Decentralized federated learning (DFL) enables devices to collaboratively\ntrain models over complex network topologies without relying on a central\ncontroller. In this setting, local data remains private, but its quality and\nquantity can vary significantly across nodes. The extent to which a fully\ndecentralized system is vulnerable to poor-quality or corrupted data remains\nunclear, but several factors could contribute to potential risks. Without a\ncentral authority, there can be no unified mechanism to detect or correct\nerrors, and each node operates with a localized view of the data distribution,\nmaking it difficult for the node to assess whether its perspective aligns with\nthe true distribution. Moreover, models trained on low-quality data can\npropagate through the network, amplifying errors. To explore the impact of\nlow-quality data on DFL, we simulate two scenarios with degraded data quality\n-- one where the corrupted data is evenly distributed in a subset of nodes and\none where it is concentrated on a single node -- using a decentralized\nimplementation of FedAvg. Our results reveal that averaging-based decentralized\nlearning is remarkably robust to localized bad data, even when the corrupted\ndata resides in the most influential nodes of the network. Counterintuitively,\nthis robustness is further enhanced when the corrupted data is concentrated on\na single node, regardless of its centrality in the communication network\ntopology. This phenomenon is explained by the averaging process, which ensures\nthat no single node -- however central -- can disproportionately influence the\noverall learning process."}
{"id": "2501.02423", "pdf": "https://arxiv.org/pdf/2501.02423", "abs": "https://arxiv.org/abs/2501.02423", "authors": ["Xingwu Sun", "Shuaipeng Li", "Ruobing Xie", "Weidong Han", "Kan Wu", "Zhen Yang", "Yixing Li", "An Wang", "Shuai Li", "Jinbao Xue", "Yu Cheng", "Yangyu Tao", "Zhanhui Kang", "Chengzhong Xu", "Di Wang", "Jie Jiang"], "title": "Scaling Laws for Floating Point Quantization Training", "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits."}
{"id": "2504.13677", "pdf": "https://arxiv.org/pdf/2504.13677", "abs": "https://arxiv.org/abs/2504.13677", "authors": ["Andrea Santilli", "Adam Golinski", "Michael Kirchhof", "Federico Danieli", "Arno Blaas", "Miao Xiong", "Luca Zappella", "Sinead Williamson"], "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025 (Main)", "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving\ntheir safety and reliability. Evaluations often use metrics like AUROC to\nassess how well UQ methods (e.g., negative sequence probabilities) correlate\nwith task correctness functions (e.g., ROUGE-L). We show that mutual\nbiases--when both UQ methods and correctness functions are biased by the same\nfactors--systematically distort evaluation. First, we formally prove that any\nmutual bias non-randomly skews AUROC rankings, compromising benchmark\nintegrity. Second, we confirm this happens empirically by testing 7 widely used\ncorrectness functions, from lexical-based and embedding-based metrics to\nLM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our\nanalysis shows that length biases in correctness functions distort UQ\nassessments by interacting with length biases in UQ methods. We identify\nLM-as-a-judge methods as the least length-biased, offering a promising path for\na fairer UQ evaluation."}
{"id": "2505.05470", "pdf": "https://arxiv.org/pdf/2505.05470", "abs": "https://arxiv.org/abs/2505.05470", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/yifan123/flow_grpo", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation."}
{"id": "2502.19307", "pdf": "https://arxiv.org/pdf/2502.19307", "abs": "https://arxiv.org/abs/2502.19307", "authors": ["Michael Somma", "Thomas Gallien", "Branka Stojanovic"], "title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Anomaly detection in complex dynamical systems is essential for ensuring\nreliability, safety, and efficiency in industrial and cyber-physical\ninfrastructures. Predictive maintenance helps prevent costly failures, while\ncybersecurity monitoring has become critical as digitized systems face growing\nthreats. Many of these systems exhibit oscillatory behaviors and bounded\nmotion, requiring anomaly detection methods that capture structured temporal\ndependencies while adhering to physical consistency principles. In this work,\nwe propose a system-theoretic approach to anomaly detection, grounded in\nclassical embedding theory and physics-inspired consistency principles. We\nbuild upon the Fractal Whitney Embedding Prevalence Theorem that extends\ntraditional embedding techniques to complex system dynamics. Additionally, we\nintroduce state-derivative pairs as an embedding strategy to capture system\nevolution. To enforce temporal coherence, we develop a Temporal Differential\nConsistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the\napproximated derivatives of latent variables with their dynamic\nrepresentations. We evaluate our method on the C-MAPSS dataset, a benchmark for\nturbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers\nwhile achieving a nearly 200x reduction in MAC operations, making it\nparticularly suited for lightweight edge computing. Our findings support the\nhypothesis that anomalies disrupt stable system dynamics, providing a robust\nsignal for anomaly detection."}
{"id": "2501.09621", "pdf": "https://arxiv.org/pdf/2501.09621", "abs": "https://arxiv.org/abs/2501.09621", "authors": ["Tehila Dahan", "Kfir Y. Levy"], "title": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML", "categories": ["cs.LG"], "comment": null, "summary": "We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems."}
{"id": "2504.14175", "pdf": "https://arxiv.org/pdf/2504.14175", "abs": "https://arxiv.org/abs/2504.14175", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025 (Findings)", "summary": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyze whether the generated documents contain information\nentailed by ground-truth evidence and assess their impact on performance. Our\nfindings indicate that, on average, performance improvements consistently\noccurred for claims whose generated documents included sentences entailed by\ngold evidence. This suggests that knowledge leakage may be present in\nfact-verification benchmarks, potentially inflating the perceived performance\nof LLM-based query expansion methods."}
{"id": "2505.08834", "pdf": "https://arxiv.org/pdf/2505.08834", "abs": "https://arxiv.org/abs/2505.08834", "authors": ["Muhammad Junaid Asif"], "title": "Crowd Scene Analysis using Deep Learning Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "MS Graduate Research Thesis", "summary": "Our research is focused on two main applications of crowd scene analysis\ncrowd counting and anomaly detection In recent years a large number of\nresearches have been presented in the domain of crowd counting We addressed two\nmain challenges in this domain 1 Deep learning models are datahungry paradigms\nand always need a large amount of annotated data for the training of algorithm\nIt is timeconsuming and costly task to annotate such large amount of data\nSelfsupervised training is proposed to deal with this challenge 2 MCNN consists\nof multicolumns of CNN with different sizes of filters by presenting a novel\napproach based on a combination of selfsupervised training and MultiColumn CNN\nThis enables the model to learn features at different levels and makes it\neffective in dealing with challenges of occluded scenes nonuniform density\ncomplex backgrounds and scale invariation The proposed model was evaluated on\npublicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE\nand MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly\ndetection addressing challenges like lighting environmental conditions\nunexpected objects and scalability The model extracts spatial and temporal\nfeatures allowing it to be generalized to realworld scenes Spatial features are\nlearned using CNN while temporal features are learned using LSTM blocks The\nmodel works on binary classification and can detect normal or abnormal behavior\nThe models performance is improved by replacing fully connected layers with\ndense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset\nshow our models outperform other stateoftheart approaches"}
{"id": "2503.04149", "pdf": "https://arxiv.org/pdf/2503.04149", "abs": "https://arxiv.org/abs/2503.04149", "authors": ["Simin Chen", "Pranav Pusarla", "Baishakhi Ray"], "title": "Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "This paper is accepted to ICML 2025. Website:\n  https://codekaleidoscope.github.io/dycodeeval.html", "summary": "The rapid evolution of code largelanguage models underscores the need for\neffective and transparent benchmarking of their reasoning capabilities.\nHowever, the current benchmarking approach heavily depends on publicly\navailable, human-created datasets. The widespread use of these fixed benchmark\ndatasets makes the benchmarking process to be static and thus particularly\nsusceptible to data contamination, an unavoidable consequence of the extensive\ndata collection processes used to train Code LLMs. Existing approaches that\naddress data contamination often suffer from human effort limitations and\nimbalanced problem complexity. To tackle these challenges, we propose \\tool, a\nnovel benchmarking suite for evaluating Code LLMs under potential data\ncontamination. Given a seed programming problem, \\tool employs multiple agents\nto extract and modify the context without altering the core logic, generating\nsemantically equivalent variations. We introduce a dynamic data generation\nmethods and conduct empirical studies on two seed datasets across 21 Code LLMs.\nResults show that \\tool effectively benchmarks reasoning capabilities under\ncontamination risks while generating diverse problem sets to ensure consistent\nand reliable evaluations."}
{"id": "2501.17770", "pdf": "https://arxiv.org/pdf/2501.17770", "abs": "https://arxiv.org/abs/2501.17770", "authors": ["Yangming Li", "Chaoyu Liu", "Carola-Bibiane Schönlieb"], "title": "Generative Unordered Flow for Set-Structured Data Generation", "categories": ["cs.LG"], "comment": "Paper under review", "summary": "Flow-based generative models have demonstrated promising performance across a\nbroad spectrum of data modalities (e.g., image and text). However, there are\nfew works exploring their extension to unordered data (e.g., spatial point\nset), which is not trivial because previous models are mostly designed for\nvector data that are naturally ordered. In this paper, we present unordered\nflow, a type of flow-based generative model for set-structured data generation.\nSpecifically, we convert unordered data into an appropriate function\nrepresentation, and learn the probability measure of such representations\nthrough function-valued flow matching. For the inverse map from a function\nrepresentation to unordered data, we propose a method similar to particle\nfiltering, with Langevin dynamics to first warm-up the initial particles and\ngradient-based search to update them until convergence. We have conducted\nextensive experiments on multiple real-world datasets, showing that our\nunordered flow model is very effective in generating set-structured data and\nsignificantly outperforms previous baselines."}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194", "abs": "https://arxiv.org/abs/2504.14194", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods."}
{"id": "2503.04256", "pdf": "https://arxiv.org/pdf/2503.04256", "abs": "https://arxiv.org/abs/2503.04256", "authors": ["Yixiang Sun", "Haotian Fu", "Michael Littman", "George Konidaris"], "title": "Knowledge Retention for Continual Model-Based Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose DRAGO, a novel approach for continual model-based reinforcement\nlearning aimed at improving the incremental development of world models across\na sequence of tasks that differ in their reward functions but not the state\nspace or dynamics. DRAGO comprises two key components: Synthetic Experience\nRehearsal, which leverages generative models to create synthetic experiences\nfrom past tasks, allowing the agent to reinforce previously learned dynamics\nwithout storing data, and Regaining Memories Through Exploration, which\nintroduces an intrinsic reward mechanism to guide the agent toward revisiting\nrelevant states from prior tasks. Together, these components enable the agent\nto maintain a comprehensive and continually developing world model,\nfacilitating more effective learning and adaptation across diverse\nenvironments. Empirical evaluations demonstrate that DRAGO is able to preserve\nknowledge across tasks, achieving superior performance in various continual\nlearning scenarios."}
{"id": "2501.18282", "pdf": "https://arxiv.org/pdf/2501.18282", "abs": "https://arxiv.org/abs/2501.18282", "authors": ["Yunzhen Yao", "Lie He", "Michael Gastpar"], "title": "Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective", "categories": ["cs.LG"], "comment": null, "summary": "This paper considers the sample-efficiency of preference learning, which\nmodels and predicts human choices based on comparative judgments. The minimax\noptimal estimation error rate $\\Theta(d/n)$ in classical estimation theory\nrequires that the number of samples $n$ scales linearly with the dimensionality\nof the feature space $d$. However, the high dimensionality of the feature space\nand the high cost of collecting human-annotated data challenge the efficiency\nof traditional estimation methods. To remedy this, we leverage sparsity in the\npreference model and establish sharp error rates. We show that under the sparse\nrandom utility model, where the parameter of the reward function is $k$-sparse,\nthe minimax optimal rate can be reduced to $\\Theta(k/n \\log(d/k))$.\nFurthermore, we analyze the $\\ell_{1}$-regularized estimator and show that it\nachieves near-optimal rate under mild assumptions on the Gram matrix.\nExperiments on synthetic data and LLM alignment data validate our theoretical\nfindings, showing that sparsity-aware methods significantly reduce sample\ncomplexity and improve prediction accuracy."}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441", "abs": "https://arxiv.org/abs/2505.11441", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "title": "Is Compression Really Linear with Code Intelligence?", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain."}
{"id": "2505.16836", "pdf": "https://arxiv.org/pdf/2505.16836", "abs": "https://arxiv.org/abs/2505.16836", "authors": ["Fanrui Zhang", "Dian Li", "Qiang Zhang", "Chenjun", "sinbadliu", "Junxiong Lin", "Jiahong Yan", "Jiawei Liu", "Zheng-Jun Zha"], "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "28 pages, 27 figures", "summary": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification."}
{"id": "2503.06474", "pdf": "https://arxiv.org/pdf/2503.06474", "abs": "https://arxiv.org/abs/2503.06474", "authors": ["Zhefan Wang", "Huanjun Kong", "Jie Ying", "Wanli Ouyang", "Nanqing Dong"], "title": "ROGRAG: A Robustly Optimized GraphRAG Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL2025 demo track, 10 pages", "summary": "Large language models (LLMs) commonly struggle with specialized or emerging\ntopics which are rarely seen in the training corpus. Graph-based\nretrieval-augmented generation (GraphRAG) addresses this by structuring domain\nknowledge as a graph for dynamic retrieval. However, existing pipelines involve\ncomplex engineering workflows, making it difficult to isolate the impact of\nindividual components. It is also challenging to evaluate the retrieval\neffectiveness due to the overlap between the pretraining and evaluation\ndatasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG\nframework. Specifically, we propose a multi-stage retrieval mechanism that\nintegrates dual-level with logic form retrieval methods to improve retrieval\nrobustness without increasing computational cost. To further refine the system,\nwe incorporate various result verification methods and adopt an incremental\ndatabase construction approach. Through extensive ablation experiments, we\nrigorously assess the effectiveness of each component. Our implementation\nincludes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct\ninitially underperformed. ROGRAG significantly improves the score from 60.0% to\n75.0% and outperforms mainstream methods. Experiments on domain-specific\ndatasets reveal that dual-level retrieval enhances fuzzy matching, while logic\nform retrieval improves structured reasoning, highlighting the importance of\nmulti-stage retrieval.ROGRAG is released as an open-source resource and\nsupports installation with pip."}
{"id": "2501.18527", "pdf": "https://arxiv.org/pdf/2501.18527", "abs": "https://arxiv.org/abs/2501.18527", "authors": ["Konrad Mundinger", "Max Zimmer", "Aldo Kiem", "Christoph Spiegel", "Sebastian Pokutta"], "title": "Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?", "categories": ["cs.LG", "math.CO"], "comment": "9 pages main paper, 11 pages references and appendix, 17 figures, 1\n  table", "summary": "We demonstrate how neural networks can drive mathematical discovery through a\ncase study of the Hadwiger-Nelson problem, a long-standing open problem at the\nintersection of discrete geometry and extremal combinatorics that is concerned\nwith coloring the plane while avoiding monochromatic unit-distance pairs. Using\nneural networks as approximators, we reformulate this mixed discrete-continuous\ngeometric coloring problem with hard constraints as an optimization task with a\nprobabilistic, differentiable loss function. This enables gradient-based\nexploration of admissible configurations that most significantly led to the\ndiscovery of two novel six-colorings, providing the first improvement in thirty\nyears to the off-diagonal variant of the original problem. Here, we establish\nthe underlying machine learning approach used to obtain these results and\ndemonstrate its broader applicability through additional numerical insights."}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Parul Kalra", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Added author", "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications."}
{"id": "2505.20704", "pdf": "https://arxiv.org/pdf/2505.20704", "abs": "https://arxiv.org/abs/2505.20704", "authors": ["Zixuan Hu", "Yichun Hu", "Xiaotong Li", "Shixiang Tang", "Ling-Yu Duan"], "title": "Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to\nunseen domains under extreme data scarcity and multiple shifts. Previous\napproaches mainly focused on sample selection strategies, while overlooking the\nfundamental problem on underlying optimization. Initially, we critically\nanalyze the widely-adopted entropy minimization framework in WTTA and uncover\nits significant limitations in noisy optimization dynamics that substantially\nhinder adaptation efficiency. Through our analysis, we identify region\nconfidence as a superior alternative to traditional entropy, however, its\ndirect optimization remains computationally prohibitive for real-time\napplications. In this paper, we introduce a novel region-integrated method\nReCAP that bypasses the lengthy process. Specifically, we propose a\nprobabilistic region modeling scheme that flexibly captures semantic changes in\nembedding space. Subsequently, we develop a finite-to-infinite asymptotic\napproximation that transforms the intractable region confidence into a\ntractable and upper-bounded proxy. These innovations significantly unlock the\noverlooked potential dynamics in local region in a concise solution. Our\nextensive experiments demonstrate the consistent superiority of ReCAP over\nexisting methods across various datasets and wild scenarios."}
{"id": "2503.09427", "pdf": "https://arxiv.org/pdf/2503.09427", "abs": "https://arxiv.org/abs/2503.09427", "authors": ["Yaorui Shi", "Jiaqi Yang", "Changhao Nai", "Sihang Li", "Junfeng Fang", "Xiang Wang", "Zhiyuan Liu", "Yang Zhang"], "title": "Language-Enhanced Representation Learning for Single-Cell Transcriptomics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular\nheterogeneity. Recent advancements leverage single-cell large language models\n(scLLMs) for effective representation learning. These models focus exclusively\non transcriptomic data, neglecting complementary biological knowledge from\ntextual descriptions. To overcome this limitation, we propose scMMGPT, a novel\nmultimodal framework designed for language-enhanced representation learning in\nsingle-cell transcriptomics. Unlike existing methods, scMMGPT employs robust\ncell representation extraction, preserving quantitative gene expression data,\nand introduces an innovative two-stage pre-training strategy combining\ndiscriminative precision with generative flexibility. Extensive experiments\ndemonstrate that scMMGPT significantly outperforms unimodal and multimodal\nbaselines across key downstream tasks, including cell annotation and\nclustering, and exhibits superior generalization in out-of-distribution\nscenarios."}
{"id": "2501.19158", "pdf": "https://arxiv.org/pdf/2501.19158", "abs": "https://arxiv.org/abs/2501.19158", "authors": ["Giovanni Catania", "Aurélien Decelle", "Cyril Furtlehner", "Beatriz Seoane"], "title": "A theoretical framework for overfitting in energy-based modeling", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": "29 pages, 20 figures (including appendix). Accepted at Proceedings of\n  the 42nd International Conference on Machine Learning, Vancouver, Canada.\n  PMLR 267, 2025", "summary": "We investigate the impact of limited data on training pairwise energy-based\nmodels for inverse problems aimed at identifying interaction networks.\nUtilizing the Gaussian model as testbed, we dissect training trajectories\nacross the eigenbasis of the coupling matrix, exploiting the independent\nevolution of eigenmodes and revealing that the learning timescales are tied to\nthe spectral decomposition of the empirical covariance matrix. We see that\noptimal points for early stopping arise from the interplay between these\ntimescales and the initial conditions of training. Moreover, we show that\nfinite data corrections can be accurately modeled through asymptotic random\nmatrix theory calculations and provide the counterpart of generalized\ncross-validation in the energy based model context. Our analytical framework\nextends to binary-variable maximum-entropy pairwise models with minimal\nvariations. These findings offer strategies to control overfitting in\ndiscrete-variable models through empirical shrinkage corrections, improving the\nmanagement of overfitting in energy-based generative models. Finally, we\npropose a generalization to arbitrary energy-based models by deriving the\nneural tangent kernel dynamics of the score function under the score-matching\nalgorithm."}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590", "abs": "https://arxiv.org/abs/2505.14590", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."}
{"id": "2505.21649", "pdf": "https://arxiv.org/pdf/2505.21649", "abs": "https://arxiv.org/abs/2505.21649", "authors": ["Keanu Nichols", "Nazia Tasnim", "Yuting Yan", "Nicholas Ikechukwu", "Elva Zou", "Deepti Ghadiyaram", "Bryan A. Plummer"], "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark"}
{"id": "2503.09969", "pdf": "https://arxiv.org/pdf/2503.09969", "abs": "https://arxiv.org/abs/2503.09969", "authors": ["Nathan Drenkow", "Mitchell Pavlak", "Keith Harrigian", "Ayah Zirikly", "Adarsh Subbaswamy", "Mohammad Mehdi Farhangi", "Nicholas Petrick", "Mathias Unberath"], "title": "Detecting Dataset Bias in Medical AI: A Generalized and Modality-Agnostic Auditing Framework", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Artificial Intelligence (AI) is now firmly at the center of evidence-based\nmedicine. Despite many success stories that edge the path of AI's rise in\nhealthcare, there are comparably many reports of significant shortcomings and\nunexpected behavior of AI in deployment. A major reason for these limitations\nis AI's reliance on association-based learning, where non-representative\nmachine learning datasets can amplify latent bias during training and/or hide\nit during testing. To unlock new tools capable of foreseeing and preventing\nsuch AI bias issues, we present G-AUDIT. Generalized Attribute Utility and\nDetectability-Induced bias Testing (G-AUDIT) for datasets is a\nmodality-agnostic dataset auditing framework that allows for generating\ntargeted hypotheses about sources of bias in training or testing data. Our\nmethod examines the relationship between task-level annotations (commonly\nreferred to as ``labels'') and data properties including patient attributes\n(e.g., age, sex) and environment/acquisition characteristics (e.g., clinical\nsite, imaging protocols). G-AUDIT quantifies the extent to which the observed\ndata attributes pose a risk for shortcut learning, or in the case of testing\ndata, might hide predictions made based on spurious associations. We\ndemonstrate the broad applicability of our method by analyzing large-scale\nmedical datasets for three distinct modalities and machine learning tasks: skin\nlesion classification in images, stigmatizing language classification in\nElectronic Health Records (EHR), and mortality prediction for ICU tabular data.\nIn each setting, G-AUDIT successfully identifies subtle biases commonly\noverlooked by traditional qualitative methods, underscoring its practical value\nin exposing dataset-level risks and supporting the downstream development of\nreliable AI systems."}
{"id": "2502.00338", "pdf": "https://arxiv.org/pdf/2502.00338", "abs": "https://arxiv.org/abs/2502.00338", "authors": ["Yuan Gao", "Hao Wu", "Ruiqi Shu", "Huanshuo Dong", "Fan Xu", "Rui Ray Chen", "Yibo Yan", "Qingsong Wen", "Xuming Hu", "Kun Wang", "Jiahao Wu", "Qing Li", "Hui Xiong", "Xiaomeng Huang"], "title": "OneForecast: A Universal Framework for Global and Regional Weather Forecasting", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate weather forecasts are important for disaster prevention,\nagricultural planning, etc. Traditional numerical weather prediction (NWP)\nmethods offer physically interpretable high-accuracy predictions but are\ncomputationally expensive and fail to fully leverage rapidly growing historical\ndata. In recent years, deep learning models have made significant progress in\nweather forecasting, but challenges remain, such as balancing global and\nregional high-resolution forecasts, excessive smoothing in extreme event\npredictions, and insufficient dynamic system modeling. To address these issues,\nthis paper proposes a global-regional nested weather forecasting framework\n(OneForecast) based on graph neural networks. By combining a dynamic system\nperspective with multi-grid theory, we construct a multi-scale graph structure\nand densify the target region to capture local high-frequency features. We\nintroduce an adaptive messaging mechanism, using dynamic gating units to deeply\nintegrate node and edge features for more accurate extreme event forecasting.\nFor high-resolution regional forecasts, we propose a neural nested grid method\nto mitigate boundary information loss. Experimental results show that\nOneForecast performs excellently across global to regional scales and\nshort-term to long-term forecasts, especially in extreme event predictions.\nCodes link https://github.com/YuanGao-YG/OneForecast."}
{"id": "2505.17427", "pdf": "https://arxiv.org/pdf/2505.17427", "abs": "https://arxiv.org/abs/2505.17427", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Zezhong Wang", "Huimin Wang", "Yutian Zhao", "Bin Liang", "Yefeng Zheng", "Binyang Li", "Kam-Fai Wong", "Xian Wu"], "title": "T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering", "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.22985", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\nperformance in Contextual Question Answering (CQA). However, prior approaches\ntypically employ elaborate reasoning strategies regardless of question\ncomplexity, leading to low adaptability. Recent efficient test-time scaling\nmethods introduce budget constraints or early stop mechanisms to avoid\noverthinking for straightforward questions. But they add human bias to the\nreasoning process and fail to leverage models' inherent reasoning capabilities.\nTo address these limitations, we present T$^2$: Think-to-Think, a novel\nframework that dynamically adapts reasoning depth based on question complexity.\nT$^2$ leverages the insight that if an LLM can effectively solve similar\nquestions using specific reasoning strategies, it can apply the same strategy\nto the original question. This insight enables to adoption of concise reasoning\nfor straightforward questions while maintaining detailed analysis for complex\nproblems. T$^2$ works through four key steps: decomposing questions into\nstructural elements, generating similar examples with candidate reasoning\nstrategies, evaluating these strategies against multiple criteria, and applying\nthe most appropriate strategy to the original question. Experimental evaluation\nacross seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves\nhigher accuracy than baseline methods but also reduces computational overhead\nby up to 25.2\\%."}
{"id": "2505.22944", "pdf": "https://arxiv.org/pdf/2505.22944", "abs": "https://arxiv.org/abs/2505.22944", "authors": ["Angtian Wang", "Haibin Huang", "Jacob Zhiyuan Fang", "Yiding Yang", "Chongyang Ma"], "title": "ATI: Any Trajectory Instruction for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a unified framework for motion control in video generation that\nseamlessly integrates camera movement, object-level translation, and\nfine-grained local motion using trajectory-based inputs. In contrast to prior\nmethods that address these motion types through separate modules or\ntask-specific designs, our approach offers a cohesive solution by projecting\nuser-defined trajectories into the latent space of pre-trained image-to-video\ngeneration models via a lightweight motion injector. Users can specify\nkeypoints and their motion paths to control localized deformations, entire\nobject motion, virtual camera dynamics, or combinations of these. The injected\ntrajectory signals guide the generative process to produce temporally\nconsistent and semantically aligned motion sequences. Our framework\ndemonstrates superior performance across multiple video motion control tasks,\nincluding stylized motion effects (e.g., motion brushes), dynamic viewpoint\nchanges, and precise local motion manipulation. Experiments show that our\nmethod provides significantly better controllability and visual quality\ncompared to prior approaches and commercial solutions, while remaining broadly\ncompatible with various state-of-the-art video generation backbones. Project\npage: https://anytraj.github.io/."}
{"id": "2503.19449", "pdf": "https://arxiv.org/pdf/2503.19449", "abs": "https://arxiv.org/abs/2503.19449", "authors": ["Zhongchun Zheng", "Kan Wu", "Long Cheng", "Lu Li", "Rodrigo C. O. Rocha", "Tianyi Liu", "Wei Wei", "Jianjiang Zeng", "Xianwei Zhang", "Yaoqing Gao"], "title": "VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Auto-vectorization is a fundamental optimization for modern compilers to\nexploit SIMD parallelism. However, state-of-the-art approaches still struggle\nto handle intricate code patterns, often requiring manual hints or\ndomain-specific expertise. Large language models (LLMs), with their ability to\ncapture intricate patterns, provide a promising solution, yet their effective\napplication in compiler optimizations remains an open challenge due to issues\nsuch as hallucinations and a lack of domain-specific reasoning. In this paper,\nwe present VecTrans, a novel framework that leverages LLMs to enhance\ncompiler-based code vectorization. VecTrans first employs compiler analysis to\nidentify potentially vectorizable code regions. It then utilizes an LLM to\nrefactor these regions into patterns that are more amenable to the compilers\nauto-vectorization. To ensure semantic correctness, VecTrans further integrates\na hybrid validation mechanism at the intermediate representation (IR) level.\nWith the above efforts, VecTrans combines the adaptability of LLMs with the\nprecision of compiler vectorization, thereby effectively opening up the\nvectorization opportunities. experimental results show that among all TSVC\nfunctions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans\nachieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test\ncases. This marks a significant advancement over state-of-the-art approaches\nwhile maintaining a cost efficiency of $0.012 per function optimization for LLM\nAPI usage."}
{"id": "2502.01203", "pdf": "https://arxiv.org/pdf/2502.01203", "abs": "https://arxiv.org/abs/2502.01203", "authors": ["Gholamali Aminian", "Amir R. Asadi", "Idan Shenfeld", "Youssef Mroueh"], "title": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models", "categories": ["cs.LG", "stat.ML"], "comment": "Experiments are added in new version", "summary": "Recent methods for aligning large language models (LLMs) with human feedback\npredominantly rely on a single reference model, which limits diversity, model\noverfitting, and underutilizes the wide range of available pre-trained models.\nIncorporating multiple reference models has the potential to address these\nlimitations by broadening perspectives, reducing bias, and leveraging the\nstrengths of diverse open-source LLMs. However, integrating multiple reference\nmodels into reinforcement learning with human feedback (RLHF) frameworks poses\nsignificant theoretical challenges, where achieving exact solutions has\nremained an open problem. This paper presents the first \\emph{exact solution}\nto the multiple reference model problem in reverse KL-regularized RLHF. We\nintroduce a comprehensive theoretical framework that includes rigorous\nstatistical analysis and provides sample complexity guarantees. Additionally,\nwe extend our analysis to forward KL-regularized RLHF, offering new insights\ninto sample complexity requirements in multiple reference scenarios. Our\ncontributions lay the foundation for more advanced and adaptable LLM alignment\ntechniques, enabling the effective use of multiple reference models. This work\npaves the way for developing alignment frameworks that are both theoretically\nsound and better suited to the challenges of modern AI ecosystems."}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176", "abs": "https://arxiv.org/abs/2505.19176", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."}
{"id": "2505.23161", "pdf": "https://arxiv.org/pdf/2505.23161", "abs": "https://arxiv.org/abs/2505.23161", "authors": ["Antonio D'Orazio", "Maria Rosaria Briglia", "Donato Crisostomi", "Dario Loi", "Emanuele Rodolà", "Iacopo Masi"], "title": "Implicit Inversion turns CLIP into a Decoder", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "CLIP is a discriminative model trained to align images and text in a shared\nembedding space. Due to its multimodal structure, it serves as the backbone of\nmany generative pipelines, where a decoder is trained to map from the shared\nspace back to images. In this work, we show that image synthesis is\nnevertheless possible using CLIP alone -- without any decoder, training, or\nfine-tuning. Our approach optimizes a frequency-aware implicit neural\nrepresentation that encourages coarse-to-fine generation by stratifying\nfrequencies across network layers. To stabilize this inverse mapping, we\nintroduce adversarially robust initialization, a lightweight Orthogonal\nProcrustes projection to align local text and image embeddings, and a blending\nloss that anchors outputs to natural image statistics. Without altering CLIP's\nweights, this framework unlocks capabilities such as text-to-image generation,\nstyle transfer, and image reconstruction. These findings suggest that\ndiscriminative models may hold untapped generative potential, hidden in plain\nsight."}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."}
{"id": "2502.01458", "pdf": "https://arxiv.org/pdf/2502.01458", "abs": "https://arxiv.org/abs/2502.01458", "authors": ["Wei Yao", "Wenkai Yang", "Gengze Xu", "Ziqiao Wang", "Yankai Lin", "Yong Liu"], "title": "The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Weak-to-strong generalization, where weakly supervised strong models\noutperform their weaker teachers, offers a promising approach to aligning\nsuperhuman models with human values. To deepen the understanding of this\napproach, we provide theoretical insights into its capabilities and\nlimitations. First, in the classification setting, we establish upper and lower\ngeneralization error bounds for the strong model, identifying the primary\nlimitations as stemming from the weak model's generalization error and the\noptimization objective itself. Additionally, we derive lower and upper bounds\non the calibration error of the strong model. These theoretical bounds reveal\ntwo critical insights: (1) the weak model should demonstrate strong\ngeneralization performance and maintain well-calibrated predictions, and (2)\nthe strong model's training process must strike a careful balance, as excessive\noptimization could undermine its generalization capability by over-relying on\nthe weak supervision signals. Finally, in the regression setting, we extend the\nwork of Charikar et al. (2024) to a loss function based on Kullback-Leibler\n(KL) divergence, offering guarantees that the strong student can outperform its\nweak teacher by at least the magnitude of their disagreement. We conduct\nsufficient experiments to validate our theory."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015", "abs": "https://arxiv.org/abs/2505.20015", "authors": ["Ramon Ferrer-i-Cancho"], "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are displaced linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Within that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are displaced by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. We provide support for\nthe hypothesis that Zipf's law originates from compression and define testable\nconditions for the emergence of Zipf's law in compressing systems."}
{"id": "2505.23637", "pdf": "https://arxiv.org/pdf/2505.23637", "abs": "https://arxiv.org/abs/2505.23637", "authors": ["Dashti A. Ali", "Richard K. G. Do", "William R. Jarnagin", "Aras T. Asaad", "Amber L. Simpson"], "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments."}
{"id": "2504.14522", "pdf": "https://arxiv.org/pdf/2504.14522", "abs": "https://arxiv.org/abs/2504.14522", "authors": ["Liudmila Zavolokina", "Kilian Sprenkamp", "Zoya Katashinskaya", "Daniel Gordon Jones"], "title": "Biased by Design: Leveraging AI Inherent Biases to Enhance Critical Thinking of News Readers", "categories": ["cs.HC", "cs.AI"], "comment": "European Conference on Information Systems (ECIS)", "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection."}
{"id": "2502.06244", "pdf": "https://arxiv.org/pdf/2502.06244", "abs": "https://arxiv.org/abs/2502.06244", "authors": ["Zeman Li", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts", "categories": ["cs.LG"], "comment": null, "summary": "Modern foundation models are trained on diverse datasets to enhance\ngeneralization across tasks and domains A central challenge in this process is\ndetermining how to effectively mix and sample data from multiple sources This\nnaturally leads to a multitask learning (MTL) perspective While prior work in\nMTL has emphasized mitigating gradient conflicts we observe that largescale\npretraining scenariossuch as multilingual or multidomain trainingoften exhibit\nlittle to no gradient conflict Motivated by this observation we propose PiKE\n(Positive gradient interaction-based K-task weights Estimator) an adaptive data\nmixing algorithm that dynamically adjusts sampling weights during training PiKE\nexploits nonconflicting gradient interactions to minimize a neartight upper\nbound on the average loss decrease at each step while incurring negligible\ncomputational overhead We provide theoretical convergence guarantees and show\nthat PiKE outperforms static and nonadaptive mixing baselines Furthermore we\nextend PiKE to promote balanced learning across tasks Extensive experiments on\nlargescale language model pretraining confirm that PiKE achieves faster\nconvergence and improved downstream performance compared to existing approaches"}
{"id": "2505.20538", "pdf": "https://arxiv.org/pdf/2505.20538", "abs": "https://arxiv.org/abs/2505.20538", "authors": ["Sebastian Antony Joseph", "Syed Murtaza Husain", "Stella S. R. Offner", "Stéphanie Juneau", "Paul Torrey", "Adam S. Bolton", "Juan P. Farias", "Niall Gaffney", "Greg Durrett", "Junyi Jessy Li"], "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy", "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology."}
{"id": "2505.24371", "pdf": "https://arxiv.org/pdf/2505.24371", "abs": "https://arxiv.org/abs/2505.24371", "authors": ["Md Intisar Chowdhury", "Kittinun Aukkapinyo", "Hiroshi Fujimura", "Joo Ann Woo", "Wasu Wasusatein", "Fadoua Ghourabi"], "title": "Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In this paper, we propose a Grid-based Local and Global Area Transcription\n(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates\nin two phases. First, extracting text transcripts from video frames using a\nVision-Language Model (VLM). Next, processing questions using these transcripts\nto generate answers through a Large Language Model (LLM). This design ensures\nimage privacy by deploying the VLM on edge devices and the LLM in the cloud. To\nimprove transcript quality, we propose grid-based visual prompting, which\nextracts intricate local details from each grid cell and integrates them with\nglobal information. Evaluation results show that Grid-LoGAT, using the\nopen-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms\nstate-of-the-art methods with similar baseline models on NExT-QA and STAR-QA\ndatasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our\nmethod surpasses the non-grid version by 24 points on localization-based\nquestions we created using NExT-QA. (This paper is accepted by IEEE ICIP 2025.)"}
{"id": "2505.04670", "pdf": "https://arxiv.org/pdf/2505.04670", "abs": "https://arxiv.org/abs/2505.04670", "authors": ["Charly Reux", "Mathieu Acher", "Djamel Eddine Khelladi", "Olivier Barais", "Clément Quinton"], "title": "LLM Code Customization with Visual Results: A Benchmark on TikZ", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "With the rise of AI-based code generation, customizing existing code out of\nnatural language instructions to modify visual results -such as figures or\nimages -has become possible, promising to reduce the need for deep programming\nexpertise. However, even experienced developers can struggle with this task, as\nit requires identifying relevant code regions (feature location), generating\nvalid code variants, and ensuring the modifications reliably align with user\nintent. In this paper, we introduce vTikZ, the first benchmark designed to\nevaluate the ability of Large Language Models (LLMs) to customize code while\npreserving coherent visual outcomes. Our benchmark consists of carefully\ncurated vTikZ editing scenarios, parameterized ground truths, and a reviewing\ntool that leverages visual feedback to assess correctness. Empirical evaluation\nwith stateof-the-art LLMs shows that existing solutions struggle to reliably\nmodify code in alignment with visual intent, highlighting a gap in current\nAI-assisted code editing approaches. We argue that vTikZ opens new research\ndirections for integrating LLMs with visual feedback mechanisms to improve code\ncustomization tasks in various domains beyond TikZ, including image processing,\nart creation, Web design, and 3D modeling."}
{"id": "2502.06597", "pdf": "https://arxiv.org/pdf/2502.06597", "abs": "https://arxiv.org/abs/2502.06597", "authors": ["Nikita P. Kalinin", "Jalaj Upadhyay", "Christoph H. Lampert"], "title": "Continual Release Moment Estimation with Differential Privacy", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10."}
{"id": "2505.20645", "pdf": "https://arxiv.org/pdf/2505.20645", "abs": "https://arxiv.org/abs/2505.20645", "authors": ["Kai Chen", "Zihao He", "Taiwei Shi", "Kristina Lerman"], "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives."}
{"id": "2506.01144", "pdf": "https://arxiv.org/pdf/2506.01144", "abs": "https://arxiv.org/abs/2506.01144", "authors": ["Ariel Shaulov", "Itay Hazan", "Lior Wolf", "Hila Chefer"], "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models."}
{"id": "2505.06459", "pdf": "https://arxiv.org/pdf/2505.06459", "abs": "https://arxiv.org/abs/2505.06459", "authors": ["Pablo Flores", "Olga Graf", "Pavlos Protopapas", "Karim Pichara"], "title": "Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have been widely used to obtain\nsolutions to various physical phenomena modeled as Differential Equations. As\nPINNs are not naturally equipped with mechanisms for Uncertainty\nQuantification, some work has been done to quantify the different uncertainties\nthat arise when dealing with PINNs. In this paper, we use a two-step procedure\nto train Bayesian Neural Networks that provide uncertainties over the solutions\nto differential equation systems provided by PINNs. We use available error\nbounds over PINNs to formulate a heteroscedastic variance that improves the\nuncertainty estimation. Furthermore, we solve forward problems and utilize the\nobtained uncertainties when doing parameter estimation in inverse problems in\ncosmology."}
{"id": "2502.09564", "pdf": "https://arxiv.org/pdf/2502.09564", "abs": "https://arxiv.org/abs/2502.09564", "authors": ["Massimiliano Ciranni", "Vito Paolo Pastore", "Roberto Di Via", "Enzo Tartaglione", "Francesca Odone", "Vittorio Murino"], "title": "Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing", "categories": ["cs.LG", "cs.CV", "I.4; I.5"], "comment": "18 Pages, 9 Figures", "summary": "Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data whenever they are affected by\nstrong spurious correlations between specific attributes and target labels.\nThis results in a form of bias affecting training data, which typically leads\nto unrecoverable weak generalization in prediction. This paper aims at facing\nthis problem by leveraging bias amplification with generated synthetic data: we\nintroduce Diffusing DeBias (DDB), a novel approach acting as a plug-in for\ncommon methods of unsupervised model debiasing exploiting the inherent\nbias-learning tendency of diffusion models in data generation. Specifically,\nour approach adopts conditional diffusion models to generate synthetic\nbias-aligned images, which replace the original training set for learning an\neffective bias amplifier model that we subsequently incorporate into an\nend-to-end and a two-step unsupervised debiasing approach. By tackling the\nfundamental issue of bias-conflicting training samples memorization in learning\nauxiliary models, typical of this type of techniques, our proposed method beats\ncurrent state-of-the-art in multiple benchmark datasets, demonstrating its\npotential as a versatile and effective tool for tackling bias in deep learning\nmodels."}
{"id": "2505.20875", "pdf": "https://arxiv.org/pdf/2505.20875", "abs": "https://arxiv.org/abs/2505.20875", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 6 figures, 16 tables", "summary": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our code and datasets\nare publicly available at https://github.com/jiyounglee-0523/TransEnV and\nhttps://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1."}
{"id": "2506.01532", "pdf": "https://arxiv.org/pdf/2506.01532", "abs": "https://arxiv.org/abs/2506.01532", "authors": ["Pedro C. Neto", "Naser Damer", "Jaime S. Cardoso", "Ana F. Sequeira"], "title": "Moving Beyond Discrete Categories: Continuous Demographic Labels for Fair Facial Recognition", "categories": ["cs.CV"], "comment": "Under review", "summary": "Bias has been a constant in face recognition models. Over the years,\nresearchers have looked at it from both the model and the data point of view.\nHowever, their approach to mitigation of data bias was limited and lacked\ninsight on the real nature of the problem. Here, in this document, we propose\nto revise our use of ethnicity labels as a continuous variable instead of a\ndiscrete value per identity. We validate our formulation both experimentally\nand theoretically, showcasing that not all identities from one ethnicity\ncontribute equally to the balance of the dataset; thus, having the same number\nof identities per ethnicity does not represent a balanced dataset. We further\nshow that models trained on datasets balanced in the continuous space\nconsistently outperform models trained on data balanced in the discrete space.\nWe trained more than 65 different models, and created more than 20 subsets of\nthe original datasets."}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petrén Bach Hansen", "Lasse Krogsbøll", "Jonas Lyngsø", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maaløe"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support."}
{"id": "2502.09591", "pdf": "https://arxiv.org/pdf/2502.09591", "abs": "https://arxiv.org/abs/2502.09591", "authors": ["Chuanhui Liu", "Xiao Wang"], "title": "Censor Dependent Variational Inference", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks - modeling time-to-event distributions. We prove that the\noptimal variational distribution, which perfectly bounds the log-likelihood,\nmay depend on the censoring mechanism. To address this issue, we propose\ncensor-dependent variational inference (CDVI), tailored for latent variable\nmodels in survival analysis. More practically, we introduce CD-CVAE, a\nV-structure Variational Autoencoder (VAE) designed for the scalable\nimplementation of CDVI. Further discussion extends some existing theories and\ntraining techniques to survival analysis. Extensive experiments validate our\nanalysis and demonstrate significant improvements in the estimation of\nindividual survival distributions."}
{"id": "2505.21082", "pdf": "https://arxiv.org/pdf/2505.21082", "abs": "https://arxiv.org/abs/2505.21082", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soojin Yoon", "Jaehyung Kim", "Dongha Lee"], "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs."}
{"id": "2506.01921", "pdf": "https://arxiv.org/pdf/2506.01921", "abs": "https://arxiv.org/abs/2506.01921", "authors": ["Minghao Liu", "Zhitao He", "Zhiyuan Fan", "Qingyun Wang", "Yi R. Fung"], "title": "MedEBench: Revisiting Text-instructed Image Editing on Medical Domain", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://mliuby.github.io/MedEBench_Website/", "summary": "Text-guided image editing has seen rapid progress in natural image domains,\nbut its adaptation to medical imaging remains limited and lacks standardized\nevaluation. Clinically, such editing holds promise for simulating surgical\noutcomes, creating personalized teaching materials, and enhancing patient\ncommunication. To bridge this gap, we introduce MedEBench, a comprehensive\nbenchmark for evaluating text-guided medical image editing. It consists of\n1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13\nanatomical regions. MedEBench offers three key contributions: (1) a clinically\nrelevant evaluation framework covering Editing Accuracy, Contextual\nPreservation, and Visual Quality, supported by detailed descriptions of\nexpected change and ROI (Region of Interest) masks; (2) a systematic comparison\nof seven state-of-the-art models, revealing common failure patterns; and (3) a\nfailure analysis protocol based on attention grounding, using IoU between\nattention maps and ROIs to identify mislocalization. MedEBench provides a solid\nfoundation for developing and evaluating reliable, clinically meaningful\nmedical image editing systems. Project website:\nhttps://mliuby.github.io/MedEBench_Website/"}
{"id": "2505.12894", "pdf": "https://arxiv.org/pdf/2505.12894", "abs": "https://arxiv.org/abs/2505.12894", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods."}
{"id": "2502.11673", "pdf": "https://arxiv.org/pdf/2502.11673", "abs": "https://arxiv.org/abs/2502.11673", "authors": ["Adrian Müller", "Jon Schneider", "Stratis Skoulakis", "Luca Viano", "Volkan Cevher"], "title": "Best of Both Worlds: Regret Minimization versus Minimax Play", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this paper, we investigate the existence of online learning algorithms\nwith bandit feedback that simultaneously guarantee $O(1)$ regret compared to a\ngiven comparator strategy, and $\\tilde{O}(\\sqrt{T})$ regret compared to any\nfixed strategy, where $T$ is the number of rounds. We provide the first\naffirmative answer to this question whenever the comparator strategy supports\nevery action. In the context of zero-sum games with min-max value zero, both in\nnormal- and extensive form, we show that our results allow us to guarantee to\nrisk at most $O(1)$ loss while being able to gain $\\Omega(T)$ from exploitable\nopponents, thereby combining the benefits of both no-regret algorithms and\nminimax play."}
{"id": "2505.23001", "pdf": "https://arxiv.org/pdf/2505.23001", "abs": "https://arxiv.org/abs/2505.23001", "authors": ["Yize Cheng", "Wenxiao Wang", "Mazda Moayeri", "Soheil Feizi"], "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors", "categories": ["cs.CL"], "comment": null, "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors."}
{"id": "2506.02112", "pdf": "https://arxiv.org/pdf/2506.02112", "abs": "https://arxiv.org/abs/2506.02112", "authors": ["Xuweiyi Chen", "Tian Xia", "Sihan Xu", "Jianing Yang", "Joyce Chai", "Zezhou Cheng"], "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction", "categories": ["cs.CV"], "comment": "3D-LLM/VLA @ CVPR2025 | Project page:\n  https://uva-computer-vision-lab.github.io/sab3r/", "summary": "We introduce a new task, Map and Locate, which unifies the traditionally\ndistinct objectives of open-vocabulary segmentation - detecting and segmenting\nobject instances based on natural language queries - and 3D reconstruction, the\nprocess of estimating a scene's 3D structure from visual inputs. Specifically,\nMap and Locate involves generating a point cloud from an unposed video and\nsegmenting object instances based on open-vocabulary queries. This task serves\nas a critical step toward real-world embodied AI applications and introduces a\npractical task that bridges reconstruction, recognition and reorganization. To\ntackle this task, we introduce a simple yet effective baseline, which we denote\nas SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer\nvision, and incorporates a lightweight distillation strategy. This method\ntransfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP\nand DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary\nfrozen networks, our model generates per-pixel semantic features and constructs\ncohesive point maps in a single forward pass. Compared to separately deploying\nMASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the\nMap and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic\nsegmentation and 3D tasks to comprehensively validate its effectiveness."}
{"id": "2505.12910", "pdf": "https://arxiv.org/pdf/2505.12910", "abs": "https://arxiv.org/abs/2505.12910", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches."}
{"id": "2502.16733", "pdf": "https://arxiv.org/pdf/2502.16733", "abs": "https://arxiv.org/abs/2502.16733", "authors": ["Akshay Mehra", "Trisha Mittal", "Subhadra Gopalakrishnan", "Joshua Kimball"], "title": "Coreset Selection via LLM-based Concept Bottlenecks", "categories": ["cs.LG"], "comment": null, "summary": "Coreset Selection (CS) aims to identify a subset of the training dataset that\nachieves model performance comparable to using the entire dataset. Many\nstate-of-the-art CS methods select coresets using scores whose computation\nrequires training the downstream model on the entire dataset first and\nrecording changes in the model's behavior on samples as it trains (training\ndynamics). These scores are inefficient to compute and hard to interpret, as\nthey do not indicate whether a sample is difficult to learn in general or only\nfor a specific downstream model. Our work addresses these challenges by\nproposing a score that computes a sample's difficulty using\nhuman-understandable textual attributes (concepts) independent of any\ndownstream model. Specifically, we measure the alignment between a sample's\nvisual features and concept bottlenecks, derived via large language models, by\ntraining a linear concept bottleneck layer and computing the sample's\ndifficulty score using it.We then use stratified sampling based on this score\nto generate a coreset of the dataset.Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10/100,\nand ImageNet-1K, we show that our coresets outperform random subsets, even at\nhigh pruning rates, and achieve model performance comparable to or better than\ncoresets found by training dynamics-based methods."}
{"id": "2505.23276", "pdf": "https://arxiv.org/pdf/2505.23276", "abs": "https://arxiv.org/abs/2505.23276", "authors": ["Maged S. Al-Shaibani", "Moataz Ahmed"], "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts."}
{"id": "2506.02294", "pdf": "https://arxiv.org/pdf/2506.02294", "abs": "https://arxiv.org/abs/2506.02294", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines"}
{"id": "2505.13469", "pdf": "https://arxiv.org/pdf/2505.13469", "abs": "https://arxiv.org/abs/2505.13469", "authors": ["Aayam Bansal"], "title": "Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.LG"], "comment": "8 pages", "summary": "As financial institutions increasingly rely on machine learning models to\nautomate lending decisions, concerns about algorithmic fairness have risen.\nThis paper explores the tradeoff between enforcing fairness constraints (such\nas demographic parity or equal opportunity) and maximizing lender\nprofitability. Through simulations on synthetic data that reflects real-world\nlending patterns, we quantify how different fairness interventions impact\nprofit margins and default rates. Our results demonstrate that equal\nopportunity constraints typically impose lower profit costs than demographic\nparity, but surprisingly, removing protected attributes from the model\n(fairness through unawareness) outperforms explicit fairness interventions in\nboth fairness and profitability metrics. We further identify the specific\neconomic conditions under which fair lending becomes profitable and analyze the\nfeature-specific drivers of unfairness. These findings offer practical guidance\nfor designing lending algorithms that balance ethical considerations with\nbusiness objectives."}
{"id": "2502.18197", "pdf": "https://arxiv.org/pdf/2502.18197", "abs": "https://arxiv.org/abs/2502.18197", "authors": ["Gianluigi Silvestri", "Luca Ambrogioni", "Chieh-Hsin Lai", "Yuhta Takida", "Yuki Mitsufuji"], "title": "VCT: Training Consistency Models with Variational Noise Coupling", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 11 figures", "summary": "Consistency Training (CT) has recently emerged as a strong alternative to\ndiffusion models for image generation. However, non-distillation CT often\nsuffers from high variance and instability, motivating ongoing research into\nits training dynamics. We propose Variational Consistency Training (VCT), a\nflexible and effective framework compatible with various forward kernels,\nincluding those in flow matching. Its key innovation is a learned noise-data\ncoupling scheme inspired by Variational Autoencoders, where a data-dependent\nencoder models noise emission. This enables VCT to adaptively learn\nnoise-todata pairings, reducing training variance relative to the fixed,\nunsorted pairings in classical CT. Experiments on multiple image datasets\ndemonstrate significant improvements: our method surpasses baselines, achieves\nstate-of-the-art FID among non-distillation CT approaches on CIFAR-10, and\nmatches SoTA performance on ImageNet 64 x 64 with only two sampling steps. Code\nis available at https://github.com/sony/vct."}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811", "abs": "https://arxiv.org/abs/2505.23811", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance. It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance."}
{"id": "2506.02356", "pdf": "https://arxiv.org/pdf/2506.02356", "abs": "https://arxiv.org/abs/2506.02356", "authors": ["Woojeong Jin", "Seongchan Kim", "Seungryong Kim"], "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring video object segmentation aims to segment the object in a video\ncorresponding to a given natural language expression. While prior works have\nexplored various referring scenarios, including motion-centric or\nmulti-instance expressions, most approaches still focus on localizing a single\ntarget object in isolation. However, in comprehensive video understanding, an\nobject's role is often defined by its interactions with other entities, which\nare largely overlooked in existing datasets and models. In this work, we\nintroduce Interaction-aware referring video object sgementation (InterRVOS), a\nnew task that requires segmenting both actor and target entities involved in an\ninteraction. Each interactoin is described through a pair of complementary\nexpressions from different semantic perspectives, enabling fine-grained\nmodeling of inter-object relationships. To tackle this task, we propose\nInterRVOS-8K, the large-scale and automatically constructed dataset containing\ndiverse interaction-aware expressions with corresponding masks, including\nchallenging cases such as motion-only multi-instance expressions. We also\npresent a baseline architecture, ReVIOSa, designed to handle actor-target\nsegmentation from a single expression, achieving strong performance in both\nstandard and interaction-focused settings. Furthermore, we introduce an\nactor-target-aware evalaution setting that enables a more targeted assessment\nof interaction understanding. Experimental results demonstrate that our\napproach outperforms prior methods in modeling complex object interactions for\nreferring video object segmentation task, establishing a strong foundation for\nfuture research in interaction-centric video understanding. Our project page is\navailable at https://cvlab-kaist.github.io/InterRVOS."}
{"id": "2505.14884", "pdf": "https://arxiv.org/pdf/2505.14884", "abs": "https://arxiv.org/abs/2505.14884", "authors": ["Susav Shrestha", "Brad Settlemyer", "Nikoli Dryden", "Narasimha Reddy"], "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accelerating large language model (LLM) inference is critical for real-world\ndeployments requiring high throughput and low latency. Contextual sparsity,\nwhere each token dynamically activates only a small subset of the model\nparameters, shows promise but does not scale to large batch sizes due to union\nof active neurons quickly approaching dense computation. We introduce Polar\nSparsity, highlighting a key shift in sparsity importance from MLP to Attention\nlayers as we scale batch size and sequence length. While MLP layers become more\ncompute-efficient under batching, their sparsity vanishes. In contrast,\nattention becomes increasingly more expensive at scale, while their head\nsparsity remains stable and batch-invariant. We develop hardware-efficient,\nsparsity-aware GPU kernels for selective MLP and Attention computations,\ndelivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2\n\\& 3, across various batch sizes and sequence lengths without compromising\naccuracy. To our knowledge, this is the first work to demonstrate that\ncontextual sparsity can scale effectively to large batch sizes, delivering\nsubstantial inference acceleration with minimal changes, making Polar Sparsity\npractical for large-scale, high-throughput LLM deployment systems. Our code is\navailable at: https://github.com/susavlsh10/Polar-Sparsity."}
{"id": "2503.05617", "pdf": "https://arxiv.org/pdf/2503.05617", "abs": "https://arxiv.org/abs/2503.05617", "authors": ["Prakash Thakolkaran", "Yaqi Guo", "Shivam Saini", "Mathias Peirlinck", "Benjamin Alheit", "Siddhant Kumar"], "title": "Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as hyperelastic constitutive artificial neural networks (CANs)", "categories": ["cs.LG"], "comment": "36 pages, 16 figures", "summary": "Traditional constitutive models rely on hand-crafted parametric forms with\nlimited expressivity and generalizability, while neural network-based models\ncan capture complex material behavior but often lack interpretability. To\nbalance these trade-offs, we present monotonic Input-Convex Kolmogorov-Arnold\nNetworks (ICKANs) for learning polyconvex hyperelastic constitutive laws.\nICKANs leverage the Kolmogorov-Arnold representation, decomposing the model\ninto compositions of trainable univariate spline-based activation functions for\nrich expressivity. We introduce trainable monotonic input-convex splines within\nthe KAN architecture, ensuring physically admissible polyconvex models for\nisotropic compressible hyperelasticity. The resulting models are both compact\nand interpretable, enabling explicit extraction of analytical constitutive\nrelationships through a monotonic input-convex symbolic regression technique.\nThrough unsupervised training on full-field strain data and limited global\nforce measurements, ICKANs accurately capture nonlinear stress-strain behavior\nacross diverse strain states. Finite element simulations of unseen geometries\nwith trained ICKAN hyperelastic constitutive models confirm the framework's\nrobustness and generalization capability."}
{"id": "2505.24554", "pdf": "https://arxiv.org/pdf/2505.24554", "abs": "https://arxiv.org/abs/2505.24554", "authors": ["Anna Sofia Lippolis", "Minh Davide Ragagni", "Paolo Ciancarini", "Andrea Giovanni Nuzzolese", "Valentina Presutti"], "title": "Bench4KE: Benchmarking Automated Competency Question Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license."}
{"id": "2506.02444", "pdf": "https://arxiv.org/pdf/2506.02444", "abs": "https://arxiv.org/abs/2506.02444", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at https://github.com/Droliven/SViMo\\_project."}
{"id": "2505.16196", "pdf": "https://arxiv.org/pdf/2505.16196", "abs": "https://arxiv.org/abs/2505.16196", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Yiwei Jin", "Keyu Li", "Zhizhong Su"], "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines."}
{"id": "2503.09117", "pdf": "https://arxiv.org/pdf/2503.09117", "abs": "https://arxiv.org/abs/2503.09117", "authors": ["Yue Wang", "Qizhou Wang", "Feng Liu", "Wei Huang", "Yali Du", "Xiaojiang Du", "Bo Han"], "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. In examining\nthe update process for unlearning dynamically, we find gradients hold essential\ninformation for revealing this trade-off. In particular, we look at the varying\nrelationship between retention performance and directional disparities between\ngradients during unlearning. It motivates the sculpting of an update mechanism\nderived from gradients from two sources, i.e., harmful for retention and useful\nfor unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an\nenhanced unlearning framework controlling the updating gradients in a\ngeometry-focused and optimization-driven manner such that their side impacts on\nother, unrelated responses can be minimized. Specifically, GRU derives a\nclosed-form solution to project the unlearning gradient onto the orthogonal\nspace of that gradient harmful for retention, ensuring minimal deviation from\nits original direction under the condition that overall performance is\nretained. Comprehensive experiments are conducted to demonstrate that GRU, as a\ngeneral framework, is straightforward to implement and efficiently enhances a\nrange of baseline methods through its adaptable and compatible characteristics.\nAdditionally, experimental results show its broad effectiveness across a\ndiverse set of benchmarks for LLM unlearning."}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688", "abs": "https://arxiv.org/abs/2505.24688", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "categories": ["cs.CL"], "comment": "Accepted as a Spotlight at ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution."}
{"id": "2506.02535", "pdf": "https://arxiv.org/pdf/2506.02535", "abs": "https://arxiv.org/abs/2506.02535", "authors": ["Juntong Li", "Lingwei Dang", "Yukun Su", "Yun Hao", "Qingxin Xiao", "Yongwei Nie", "Qingyao Wu"], "title": "MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) methods based on reconstruction or prediction\nface two critical challenges: (1) strong generalization capability often\nresults in accurate reconstruction or prediction of abnormal events, making it\ndifficult to distinguish normal from abnormal patterns; (2) reliance only on\nlow-level appearance and motion cues limits their ability to identify\nhigh-level semantic in abnormal events from complex scenes. To address these\nlimitations, we propose a novel VAD framework with two key innovations. First,\nto suppress excessive generalization, we introduce the Sparse Feature Filtering\nModule (SFFM) that employs bottleneck filters to dynamically and adaptively\nremove abnormal information from features. Unlike traditional memory modules,\nit does not need to memorize the normal prototypes across the training dataset.\nFurther, we design the Mixture of Experts (MoE) architecture for SFFM. Each\nexpert is responsible for extracting specialized principal features during\nrunning time, and different experts are selectively activated to ensure the\ndiversity of the learned principal features. Second, to overcome the neglect of\nsemantics in existing methods, we integrate a Vision-Language Model (VLM) to\ngenerate textual descriptions for video clips, enabling comprehensive joint\nmodeling of semantic, appearance, and motion cues. Additionally, we enforce\nmodality consistency through semantic similarity constraints and motion\nframe-difference contrastive loss. Extensive experiments on multiple public\ndatasets validate the effectiveness of our multimodal joint modeling framework\nand sparse feature filtering paradigm. Project page at\nhttps://qzfm.github.io/sfn_vad_project_page/."}
{"id": "2505.20290", "pdf": "https://arxiv.org/pdf/2505.20290", "abs": "https://arxiv.org/abs/2505.20290", "authors": ["Vincent Liu", "Ademi Adeniji", "Haotian Zhan", "Siddhant Haldar", "Raunaq Bhirangi", "Pieter Abbeel", "Lerrel Pinto"], "title": "EgoZero: Robot Learning from Smart Glasses", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Despite recent progress in general purpose robotics, robot policies still lag\nfar behind basic human capabilities in the real world. Humans interact\nconstantly with the physical world, yet this rich data resource remains largely\nuntapped in robot learning. We propose EgoZero, a minimal system that learns\nrobust manipulation policies from human demonstrations captured with Project\nAria smart glasses, $\\textbf{and zero robot data}$. EgoZero enables: (1)\nextraction of complete, robot-executable actions from in-the-wild, egocentric,\nhuman demonstrations, (2) compression of human visual observations into\nmorphology-agnostic state representations, and (3) closed-loop policy learning\nthat generalizes morphologically, spatially, and semantically. We deploy\nEgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot\ntransfer with 70% success rate over 7 manipulation tasks and only 20 minutes of\ndata collection per task. Our results suggest that in-the-wild human data can\nserve as a scalable foundation for real-world robot learning - paving the way\ntoward a future of abundant, diverse, and naturalistic training data for\nrobots. Code and videos are available at https://egozero-robot.github.io."}
{"id": "2503.09128", "pdf": "https://arxiv.org/pdf/2503.09128", "abs": "https://arxiv.org/abs/2503.09128", "authors": ["Fengze Sun", "Yanchuan Chang", "Egemen Tanin", "Shanika Karunasekera", "Jianzhong Qi"], "title": "FlexiReg: Flexible Urban Region Representation Learning", "categories": ["cs.LG"], "comment": "This paper is accepted at KDD 2025", "summary": "The increasing availability of urban data offers new opportunities for\nlearning region representations, which can be used as input to machine learning\nmodels for downstream tasks such as check-in or crime prediction. While\nexisting solutions have produced promising results, an issue is their fixed\nformation of regions and fixed input region features, which may not suit the\nneeds of different downstream tasks. To address this limitation, we propose a\nmodel named FlexiReg for urban region representation learning that is flexible\nwith both the formation of urban regions and the input region features.\nFlexiReg is based on a spatial grid partitioning over the spatial area of\ninterest. It learns representations for the grid cells, leveraging publicly\naccessible data, including POI, land use, satellite imagery, and street view\nimagery. We propose adaptive aggregation to fuse the cell representations and\nprompt learning techniques to tailor the representations towards different\ntasks, addressing the needs of varying formations of urban regions and\ndownstream tasks. Extensive experiments on five real-world datasets demonstrate\nthat FlexiReg outperforms state-of-the-art models by up to 202% in term of the\naccuracy of four diverse downstream tasks using the produced urban region\nrepresentations."}
{"id": "2506.00264", "pdf": "https://arxiv.org/pdf/2506.00264", "abs": "https://arxiv.org/abs/2506.00264", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions", "categories": ["cs.CL"], "comment": "accepted at ACL Findings 2025", "summary": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs."}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon."}
{"id": "2505.20573", "pdf": "https://arxiv.org/pdf/2505.20573", "abs": "https://arxiv.org/abs/2505.20573", "authors": ["Jiabao Ji", "Yongchao Chen", "Yang Zhang", "Ramana Rao Kompella", "Chuchu Fan", "Gaowen Liu", "Shiyu Chang"], "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in various\nrobot control tasks. However, their deployment in real-world applications\nremains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently\nproduce invalid action plans that violate physical constraints, such as\ndirecting a robot to an unreachable location or causing collisions between\nrobots. This issue primarily arises from a lack of awareness of these physical\nconstraints during the reasoning process. To address this issue, we propose a\nnovel framework that integrates reinforcement learning with verifiable rewards\n(RLVR) to incentivize knowledge of physical constraints into LLMs to induce\nconstraints-aware reasoning during plan generation. In this approach, only\nvalid action plans that successfully complete a control task receive positive\nrewards. We applied our method to two small-scale LLMs: a non-reasoning\nQwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results\ndemonstrate that constraint-aware small LLMs largely outperform large-scale\nmodels without constraints, grounded on both the BoxNet task and a newly\ndeveloped BoxNet3D environment built using MuJoCo. This work highlights the\neffectiveness of grounding even small LLMs with physical constraints to enable\nscalable and efficient multi-robot control in complex, physically constrained\nenvironments."}
{"id": "2503.09532", "pdf": "https://arxiv.org/pdf/2503.09532", "abs": "https://arxiv.org/abs/2503.09532", "authors": ["Adam Karvonen", "Can Rager", "Johnny Lin", "Curt Tigges", "Joseph Bloom", "David Chanin", "Yeu-Tong Lau", "Eoin Farrell", "Callum McDougall", "Kola Ayonrinde", "Demian Till", "Matthew Wearden", "Arthur Conmy", "Samuel Marks", "Neel Nanda"], "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 main conference", "summary": "Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across eight\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at:\nwww.neuronpedia.org/sae-bench"}
{"id": "2506.00539", "pdf": "https://arxiv.org/pdf/2506.00539", "abs": "https://arxiv.org/abs/2506.00539", "authors": ["Ruihan Yang", "Yikai Zhang", "Aili Chen", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Deqing Yang", "Yanghua Xiao"], "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines."}
{"id": "2506.02695", "pdf": "https://arxiv.org/pdf/2506.02695", "abs": "https://arxiv.org/abs/2506.02695", "authors": ["Linquan Wu", "Tianxiang Jiang", "Wenhao Duan", "Yini Fang", "Jacky Keung"], "title": "FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Micro-expression recognition (MER) demands models that can amplify\nmillisecond-level, low-amplitude facial motions while suppressing\nidentity-specific appearance. We introduce FaceSleuth, a dual-stream\narchitecture that (1) enhances motion along the empirically dominant vertical\naxix through a Continuously Vertical Attention (CVA) block, (2) localises the\nresulting signals with a Facial Position Focalizer built on hierarchical\ncross-window attention, and (3) steers feature learning toward physiologically\nmeaningful regions via lightweight Action-Unit embeddings. To examine whether\nthe hand-chosen vertical axis is indeed optimal, we further propose a\nSingle-Orientation Attention (SOA) module that learns its own pooling direction\nend-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses\nto CVA when the learned angle converges to {\\Pi}/2. In practice, SOA reliably\ndrifts to 88{\\deg}, confirming the effectiveness of the vertical prior while\ndelivering consistent gains. On three standard MER benchmarks, FaceSleuth with\nCVA already surpasses previous state-of-the-art methods; plugging in SOA lifts\naccuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840\non SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.\nThese results establish a new state of the art and, for the first time, provide\nempirical evidence that the vertical attention bias is the most discriminative\norientation for MER."}
{"id": "2505.20730", "pdf": "https://arxiv.org/pdf/2505.20730", "abs": "https://arxiv.org/abs/2505.20730", "authors": ["Shahrooz Pouryousef", "Ali Montazeralghaem"], "title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders."}
{"id": "2503.10503", "pdf": "https://arxiv.org/pdf/2503.10503", "abs": "https://arxiv.org/abs/2503.10503", "authors": ["Jacob Comeau", "Mathieu Bazinet", "Pascal Germain", "Cem Subakan"], "title": "Sample Compression for Self Certified Continual Learning", "categories": ["cs.LG"], "comment": null, "summary": "Continual learning algorithms aim to learn from a sequence of tasks, making\nthe training distribution non-stationary. The majority of existing continual\nlearning approaches in the literature rely on heuristics and do not provide\nlearning guarantees. In this paper, we present a new method called Continual\nPick-to-Learn (CoP2L), which is able to retain the most representative samples\nfor each task in an efficient way. CoP2L combines the Pick-to-Learn algorithm\n(rooted in the sample compression theory) and the experience replay continual\nlearning scheme. This allows us to provide non-vacuous upper bounds on the\ngeneralization loss of the learned predictors, numerically computable after\neach task. We empirically evaluate our approach on several standard continual\nlearning benchmarks across Class-Incremental, Task-Incremental, and\nDomain-Incremental settings. Our results show that CoP2L is highly competitive\nacross all setups, often outperforming existing baselines, and significantly\nmitigating catastrophic forgetting compared to vanilla experience replay in the\nClass-Incremental setting. It is possible to leverage the bounds provided by\nCoP2L in practical scenarios to certify the predictor reliability on previously\nlearned tasks, in order to improve the trustworthiness of the continual\nlearning algorithm."}
{"id": "2506.01723", "pdf": "https://arxiv.org/pdf/2506.01723", "abs": "https://arxiv.org/abs/2506.01723", "authors": ["Soyoung Oh", "Xinting Huang", "Mathis Pink", "Michael Hahn", "Vera Demberg"], "title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer."}
{"id": "2506.02738", "pdf": "https://arxiv.org/pdf/2506.02738", "abs": "https://arxiv.org/abs/2506.02738", "authors": ["Negin Baghbanzadeh", "Sajad Ashkezari", "Elham Dolatabadi", "Arash Afkanpour"], "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning."}
{"id": "2505.20734", "pdf": "https://arxiv.org/pdf/2505.20734", "abs": "https://arxiv.org/abs/2505.20734", "authors": ["Zhuoyu Cheng", "Kohei Hatano", "Eiji Takimoto"], "title": "Adversarial bandit optimization for approximately linear functions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret."}
{"id": "2503.20117", "pdf": "https://arxiv.org/pdf/2503.20117", "abs": "https://arxiv.org/abs/2503.20117", "authors": ["Bicheng Ying", "Zhe Li", "Haibo Yang"], "title": "Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable", "categories": ["cs.LG", "cs.DC"], "comment": "Under review", "summary": "This work tackles the fundamental challenges in Federated Learning (FL) posed\nby arbitrary client participation and data heterogeneity, prevalent\ncharacteristics in practical FL settings. It is well-established that popular\nFedAvg-style algorithms struggle with exact convergence and can suffer from\nslow convergence rates since a decaying learning rate is required to mitigate\nthese scenarios. To address these issues, we introduce the concept of\nstochastic matrix and the corresponding time-varying graphs as a novel modeling\ntool to accurately capture the dynamics of arbitrary client participation and\nthe local update procedure. Leveraging this approach, we offer a fresh\nperspective on designing FL algorithms, provide a rigorous quantitative\nanalysis of the limitations inherent in the FedAvg algorithm, and present\nFOCUS, Federated Optimization with Exact Convergence via Push-pull Strategy, a\nprovably convergent algorithm designed to effectively overcome the previously\nmentioned two challenges. More specifically, we provide a rigorous proof\ndemonstrating that FOCUS achieves exact convergence with a linear rate\nregardless of the arbitrary client participation, establishing it as the first\nwork to demonstrate this significant result."}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132", "abs": "https://arxiv.org/abs/2506.02132", "authors": ["Michael Li", "Nishant Subramani"], "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing"}
{"id": "2506.02845", "pdf": "https://arxiv.org/pdf/2506.02845", "abs": "https://arxiv.org/abs/2506.02845", "authors": ["Di Wen", "Lei Qi", "Kunyu Peng", "Kailun Yang", "Fei Teng", "Ao Luo", "Jia Fu", "Yufan Chen", "Ruiping Liu", "Yitian Shi", "M. Saquib Sarfraz", "Rainer Stiefelhagen"], "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, code are available at\n  https://github.com/LEI-QI-233/HAR-in-Space", "summary": "Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space."}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Anh Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks."}
{"id": "2503.21224", "pdf": "https://arxiv.org/pdf/2503.21224", "abs": "https://arxiv.org/abs/2503.21224", "authors": ["Matthieu Meunier", "Christoph Reisinger", "Yufei Zhang"], "title": "Efficient Learning for Entropy-Regularized Markov Decision Processes via Multilevel Monte Carlo", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML", "65C05, 90C40 (Primary) 90C39, 60J20, 68Q32 (Secondary)"], "comment": "46 pages, 6 figures; improved bound on bias of the plain MC estimator\n  of T", "summary": "Designing efficient learning algorithms with complexity guarantees for Markov\ndecision processes (MDPs) with large or continuous state and action spaces\nremains a fundamental challenge. We address this challenge for\nentropy-regularized MDPs with Polish state and action spaces, assuming access\nto a generative model of the environment. We propose a novel family of\nmultilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration\nwith MLMC techniques and a generic stochastic approximation of the Bellman\noperator. We quantify the precise impact of the chosen approximate Bellman\noperator on the accuracy of the resulting MLMC estimator. Leveraging this error\nanalysis, we show that using a biased plain MC estimate for the Bellman\noperator results in quasi-polynomial sample complexity, whereas an unbiased\nrandomized multilevel approximation of the Bellman operator achieves polynomial\nsample complexity in expectation. Notably, these complexity bounds are\nindependent of the dimensions or cardinalities of the state and action spaces,\ndistinguishing our approach from existing algorithms whose complexities scale\nwith the sizes of these spaces. We validate these theoretical performance\nguarantees through numerical experiments."}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426", "abs": "https://arxiv.org/abs/2506.02426", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at https://github.com/maryambrj/ALIEN.git."}
{"id": "2506.02896", "pdf": "https://arxiv.org/pdf/2506.02896", "abs": "https://arxiv.org/abs/2506.02896", "authors": ["Adam Pardyl", "Dominik Matuszek", "Mateusz Przebieracz", "Marek Cygan", "Bartosz Zieliński", "Maciej Wołczyk"], "title": "FlySearch: Exploring how vision-language models explore", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase."}
{"id": "2505.23786", "pdf": "https://arxiv.org/pdf/2505.23786", "abs": "https://arxiv.org/abs/2505.23786", "authors": ["Kazuki Egashira", "Robin Staab", "Mark Vero", "Jingxuan He", "Martin Vechev"], "title": "Mind the Gap: A Practical Attack on GGUF Quantization", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\nollama and llama$.$cpp frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense."}
{"id": "2503.22733", "pdf": "https://arxiv.org/pdf/2503.22733", "abs": "https://arxiv.org/abs/2503.22733", "authors": ["Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Niangjun Chen", "Bo Wang"], "title": "RBFleX-NAS: Training-Free Neural Architecture Search Using Radial Basis Function Kernel and Hyperparameter Detection", "categories": ["cs.LG"], "comment": "15 pages, 17 figures, Published on IEEE Transactions on Neural\n  Networks and Learning Systems (TNNLS)", "summary": "Neural Architecture Search (NAS) is an automated technique to design optimal\nneural network architectures for a specific workload. Conventionally,\nevaluating candidate networks in NAS involves extensive training, which\nrequires significant time and computational resources. To address this,\ntraining-free NAS has been proposed to expedite network evaluation with minimal\nsearch time. However, state-of-the-art training-free NAS algorithms struggle to\nprecisely distinguish well-performing networks from poorly-performing networks,\nresulting in inaccurate performance predictions and consequently sub-optimal\ntop-1 network accuracy. Moreover, they are less effective in activation\nfunction exploration. To tackle the challenges, this paper proposes RBFleX-NAS,\na novel training-free NAS framework that accounts for both activation outputs\nand input features of the last layer with a Radial Basis Function (RBF) kernel.\nWe also present a detection algorithm to identify optimal hyperparameters using\nthe obtained activation outputs and input feature maps. We verify the efficacy\nof RBFleX-NAS over a variety of NAS benchmarks. RBFleX-NAS significantly\noutperforms state-of-the-art training-free NAS methods in terms of top-1\naccuracy, achieving this with short search time in NAS-Bench-201 and\nNAS-Bench-SSS. In addition, it demonstrates higher Kendall correlation compared\nto layer-based training-free NAS algorithms. Furthermore, we propose NAFBee, a\nnew activation design space that extends the activation type to encompass\nvarious commonly used functions. In this extended design space, RBFleX-NAS\ndemonstrates its superiority by accurately identifying the best-performing\nnetwork during activation function search, providing a significant advantage\nover other NAS algorithms."}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442", "abs": "https://arxiv.org/abs/2506.02442", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms."}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models achieve strong performance in\nvision-language understanding and text-to-image generation, they remain limited\nin addressing image perception and manipulation -- capabilities increasingly\ndemanded in practical applications. Recently, OpenAI introduced the powerful\nGPT-4o-Image model, which showcases advanced capabilities in comprehensive\nimage perception and manipulation, sparking widespread interest. Through\ncarefully designed experiments, we observe that GPT-4o-Image likely relies on\nsemantic encoders rather than VAEs for feature extraction, despite VAEs being\ncommonly regarded as crucial for image manipulation tasks. Inspired by this\ninsight, we propose UniWorld, a unified generative framework built upon\nsemantic features extracted from powerful multimodal large language models and\ncontrastive semantic encoders. Using only 2.7M training data, UniWorld achieves\nimpressive performance across diverse tasks, including image understanding,\ngeneration, manipulation, and perception. We fully open-source the UniWorld\nframework, including model weights, training and evaluation scripts, and\ndatasets to promote reproducibility and further research."}
{"id": "2505.24293", "pdf": "https://arxiv.org/pdf/2505.24293", "abs": "https://arxiv.org/abs/2505.24293", "authors": ["James R. Golden"], "title": "Large Language Models are Locally Linear Mappings", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "minor updates to Fig 2; code available at\n  https://github.com/jamesgolden1/llms-are-llms", "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process."}
{"id": "2504.06212", "pdf": "https://arxiv.org/pdf/2504.06212", "abs": "https://arxiv.org/abs/2504.06212", "authors": ["Thomas Mulc", "Mike Anderson", "Paul Cubre", "Huikun Zhang", "Ivy Liu", "Saket Kumar"], "title": "NNN: Next-Generation Neural Networks for Marketing Measurement", "categories": ["cs.LG", "stat.AP"], "comment": "The title was updated to reflect broader scope. We clarified that our\n  method is not an MMM and emphasized its experimental nature. R2 values in\n  Tables 1 and 2 were corrected after fixing a bug that inflated NNN scores.\n  Notation in the unrolling section now uses 0-indexing. We noted NNN has not\n  been tested with many channels, added an acknowledgment, and improved grammar", "summary": "We present NNN, an experimental Transformer-based neural network approach to\nmarketing measurement. Unlike Marketing Mix Models (MMMs) which rely on scalar\ninputs and parametric decay functions, NNN uses rich embeddings to capture both\nquantitative and qualitative aspects of marketing and organic channels (e.g.,\nsearch queries, ad creatives). This, combined with its attention mechanism,\npotentially enables NNN to model complex interactions, capture long-term\neffects, and improve sales attribution accuracy. We show that L1 regularization\npermits the use of such expressive models in typical data-constrained settings.\nEvaluating NNN on simulated and real-world data demonstrates its efficacy,\nparticularly through considerable improvement in predictive power. In addition\nto marketing measurement, the NNN framework can provide valuable, complementary\ninsights through model probing, such as evaluating keyword or creative\neffectiveness."}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544", "abs": "https://arxiv.org/abs/2506.02544", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose Cross-source knowledge \\textbf{Re}conciliation for\nMultimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively\nreconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a\nfour-stage pipeline: it first generates an internal response from parametric\nknowledge, then selects the most relevant multimodal evidence via joint\nsimilarity assessment, generates an external response, and finally integrates\nboth to produce a reliable answer. Additionally, a specialized training\nparadigm enhances knowledge source discrimination, multimodal integration, and\nunified answer generation. Experiments on KB-VQA benchmarks show that\nCoRe-MMRAG achieves substantial improvements over baseline methods, achieving\n5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively."}
{"id": "2412.03293", "pdf": "https://arxiv.org/pdf/2412.03293", "abs": "https://arxiv.org/abs/2412.03293", "authors": ["Junjie Wen", "Minjie Zhu", "Yichen Zhu", "Zhibin Tang", "Jinming Li", "Zhongyi Zhou", "Chengmeng Li", "Xiaoyu Liu", "Yaxin Peng", "Chaomin Shen", "Feifei Feng"], "title": "Diffusion-VLA: Generalizable and Interpretable Robot Foundation Model via Self-Generated Reasoning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by ICML 2025. The project page is available at:\n  http://diffusion-vla.github.io", "summary": "In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size."}
{"id": "2505.24298", "pdf": "https://arxiv.org/pdf/2505.24298", "abs": "https://arxiv.org/abs/2505.24298", "authors": ["Wei Fu", "Jiaxuan Gao", "Xujie Shen", "Chen Zhu", "Zhiyu Mei", "Chuyi He", "Shusheng Xu", "Guo Wei", "Jun Mei", "Jiashu Wang", "Tongkai Yang", "Binhang Yuan", "Yi Wu"], "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/."}
{"id": "2504.09132", "pdf": "https://arxiv.org/pdf/2504.09132", "abs": "https://arxiv.org/abs/2504.09132", "authors": ["Matthew B. Webster", "Dongheon Lee", "Joonnyong Lee"], "title": "Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis", "categories": ["cs.LG", "eess.SP", "I.2.6"], "comment": "12 pages, 5 figures, 1 table, preprint", "summary": "Biosignals can be viewed as mixtures measuring particular physiological\nevents, and blind source separation (BSS) aims to extract underlying source\nsignals from mixtures. This paper proposes a self-supervised multi-encoder\nautoencoder (MEAE) to separate heartbeat-related source signals from\nphotoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG\ndata. The MEAE is trained on PPG signals from a large open polysomnography\ndatabase without any pre-processing or data selection. The trained network is\nthen applied to a noisy PPG dataset collected during the daily activities of\nnine subjects. The extracted heartbeat-related source signal significantly\nimproves HR detection as compared to the original PPG. The absence of\npre-processing and the self-supervised nature of the proposed method, combined\nwith its strong performance, highlight the potential of MEAE for BSS in\nbiosignal analysis."}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689", "abs": "https://arxiv.org/abs/2506.02689", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research."}
{"id": "2502.09775", "pdf": "https://arxiv.org/pdf/2502.09775", "abs": "https://arxiv.org/abs/2502.09775", "authors": ["Yuhui Zhang", "Yuchang Su", "Chenyu Wang", "Tianhong Li", "Zoe Wefers", "Jeffrey Nirschl", "James Burgess", "Daisy Ding", "Alejandro Lozano", "Emma Lundberg", "Serena Yeung-Levy"], "title": "CellFlux: Simulating Cellular Morphology Changes via Flow Matching", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "q-bio.BM", "q-bio.CB"], "comment": "Published at ICML 2025", "summary": "Building a virtual cell capable of accurately simulating cellular behaviors\nin silico has long been a dream in computational biology. We introduce\nCellFlux, an image-generative model that simulates cellular morphology changes\ninduced by chemical and genetic perturbations using flow matching. Unlike prior\nmethods, CellFlux models distribution-wise transformations from unperturbed to\nperturbed cell states, effectively distinguishing actual perturbation effects\nfrom experimental artifacts such as batch effects -- a major challenge in\nbiological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined\nperturbation (JUMP) datasets, CellFlux generates biologically meaningful cell\nimages that faithfully capture perturbation-specific morphological changes,\nachieving a 35% improvement in FID scores and a 12% increase in mode-of-action\nprediction accuracy over existing methods. Additionally, CellFlux enables\ncontinuous interpolation between cellular states, providing a potential tool\nfor studying perturbation dynamics. These capabilities mark a significant step\ntoward realizing virtual cell modeling for biomedical research. Project page:\nhttps://yuhui-zh15.github.io/CellFlux/."}
{"id": "2505.24492", "pdf": "https://arxiv.org/pdf/2505.24492", "abs": "https://arxiv.org/abs/2505.24492", "authors": ["David Steinmann", "Wolfgang Stammer", "Antonia Wüst", "Kristian Kersting"], "title": "Object Centric Concept Bottlenecks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Developing high-performing, yet interpretable models remains a critical\nchallenge in modern AI. Concept-based models (CBMs) attempt to address this by\nextracting human-understandable concepts from a global encoding (e.g., image\nencoding) and then applying a linear classifier on the resulting concept\nactivations, enabling transparent decision-making. However, their reliance on\nholistic image encodings limits their expressiveness in object-centric\nreal-world settings and thus hinders their ability to solve complex vision\ntasks beyond single-label classification. To tackle these challenges, we\nintroduce Object-Centric Concept Bottlenecks (OCB), a framework that combines\nthe strengths of CBMs and pre-trained object-centric foundation models,\nboosting performance and interpretability. We evaluate OCB on complex image\ndatasets and conduct a comprehensive ablation study to analyze key components\nof the framework, such as strategies for aggregating object-concept encodings.\nThe results show that OCB outperforms traditional CBMs and allows one to make\ninterpretable decisions for complex visual tasks."}
{"id": "2505.01874", "pdf": "https://arxiv.org/pdf/2505.01874", "abs": "https://arxiv.org/abs/2505.01874", "authors": ["Youssef Allouah", "Rachid Guerraoui", "John Stephan"], "title": "Towards Trustworthy Federated Learning with Untrusted Participants", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "ICML 2025 conference paper", "summary": "Resilience against malicious participants and data privacy are essential for\ntrustworthy federated learning, yet achieving both with good utility typically\nrequires the strong assumption of a trusted central server. This paper shows\nthat a significantly weaker assumption suffices: each pair of participants\nshares a randomness seed unknown to others. In a setting where malicious\nparticipants may collude with an untrusted server, we propose CafCor, an\nalgorithm that integrates robust gradient aggregation with correlated noise\ninjection, using shared randomness between participants. We prove that CafCor\nachieves strong privacy-utility trade-offs, significantly outperforming local\ndifferential privacy (DP) methods, which do not make any trust assumption,\nwhile approaching central DP utility, where the server is fully trusted.\nEmpirical results on standard benchmarks validate CafCor's practicality,\nshowing that privacy and robustness can coexist in distributed systems without\nsacrificing utility or trusting the server."}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701", "abs": "https://arxiv.org/abs/2506.02701", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "title": "On Entity Identification in Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information."}
{"id": "2503.16424", "pdf": "https://arxiv.org/pdf/2503.16424", "abs": "https://arxiv.org/abs/2503.16424", "authors": ["Xi Liu", "Chaoyi Zhou", "Nanxuan Zhao", "Siyu Huang"], "title": "Bézier Splatting for Fast and Differentiable Vector Graphics Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://xiliu8006.github.io/Bezier_splatting_project/", "summary": "Differentiable vector graphics (VGs) are widely used in image vectorization\nand vector synthesis, while existing representations are costly to optimize and\nstruggle to achieve high-quality rendering results for high-resolution images.\nThis work introduces a new differentiable VG representation, dubbed B\\'ezier\nSplatting, that enables fast yet high-fidelity VG rasterization. B\\'ezier\nSplatting samples 2D Gaussians along B\\'ezier curves, which naturally provide\npositional gradients at object boundaries. Thanks to the efficient\nsplatting-based differentiable rasterizer, B\\'ezier Splatting achieves 30x and\n150x faster per forward and backward rasterization step for open curves\ncompared to DiffVG. Additionally, we introduce an adaptive pruning and\ndensification strategy that dynamically adjusts the spatial distribution of\ncurves to escape local minima, further improving VG quality. Furthermore, our\nnew VG representation supports conversion to standard XML-based SVG format,\nenhancing interoperability with existing VG tools and pipelines. Experimental\nresults show that B\\'ezier Splatting significantly outperforms existing methods\nwith better visual fidelity and significant optimization speedup."}
{"id": "2506.00095", "pdf": "https://arxiv.org/pdf/2506.00095", "abs": "https://arxiv.org/abs/2506.00095", "authors": ["Yuchong Li", "Xiaojun Zeng", "Chihua Fang", "Jian Yang", "Fucang Jia", "Lei Zhang"], "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at\nhttps://clinbench-hpb.github.io."}
{"id": "2505.14613", "pdf": "https://arxiv.org/pdf/2505.14613", "abs": "https://arxiv.org/abs/2505.14613", "authors": ["Emmanuel Noutahi", "Jason Hartford", "Prudencio Tossou", "Shawn Whitfield", "Alisandra K. Denton", "Cas Wognum", "Kristina Ulicna", "Michael Craig", "Jonathan Hsu", "Michael Cuccarese", "Emmanuel Bengio", "Dominique Beaini", "Christopher Gibson", "Daniel Cohen", "Berton Earnshaw"], "title": "Virtual Cells: Predict, Explain, Discover", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes."}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106", "abs": "https://arxiv.org/abs/2506.03106", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chaochao Lu", "Chao Yang", "Helen Meng"], "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."}
{"id": "2505.16933", "pdf": "https://arxiv.org/pdf/2505.16933", "abs": "https://arxiv.org/abs/2505.16933", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-V-demo/}", "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/."}
{"id": "2506.00100", "pdf": "https://arxiv.org/pdf/2506.00100", "abs": "https://arxiv.org/abs/2506.00100", "authors": ["Ajinkya Kulkarni", "Francisco Teixeira", "Enno Hermann", "Thomas Rolland", "Isabel Trancoso", "Mathew Magimai Doss"], "title": "Children's Voice Privacy: First Steps And Emerging Challenges", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research."}
{"id": "2505.17226", "pdf": "https://arxiv.org/pdf/2505.17226", "abs": "https://arxiv.org/abs/2505.17226", "authors": ["Kun Yang", "Neena Imam"], "title": "Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative machine learning across\ndecentralized data sources without sharing raw data. It offers a promising\napproach to privacy-preserving AI. However, FL remains vulnerable to\nadversarial threats from malicious participants, referred to as Byzantine\nclients, who can send misleading updates to corrupt the global model.\nTraditional aggregation methods, such as simple averaging, are not robust to\nsuch attacks. More resilient approaches, like the Krum algorithm, require prior\nknowledge of the number of malicious clients, which is often unavailable in\nreal-world scenarios. To address these limitations, we propose Average-rKrum\n(ArKrum), a novel aggregation strategy designed to enhance both the resilience\nand privacy guarantees of FL systems. Building on our previous work (rKrum),\nArKrum introduces two key innovations. First, it includes a median-based\nfiltering mechanism that removes extreme outliers before estimating the number\nof adversarial clients. Second, it applies a multi-update averaging scheme to\nimprove stability and performance, particularly when client data distributions\nare not identical. We evaluate ArKrum on benchmark image and text datasets\nunder three widely studied Byzantine attack types. Results show that ArKrum\nconsistently achieves high accuracy and stability. It performs as well as or\nbetter than other robust aggregation methods. These findings demonstrate that\nArKrum is an effective and practical solution for secure FL systems in\nadversarial environments."}
{"id": "2505.22769", "pdf": "https://arxiv.org/pdf/2505.22769", "abs": "https://arxiv.org/abs/2505.22769", "authors": ["Yaxiong Lei", "Mingyue Zhao", "Yuheng Wang", "Shijing He", "Yusuke Sugano", "Mohamed Khamis", "Juan Ye"], "title": "MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking", "categories": ["cs.HC", "cs.CV", "68T10, 68U35", "H.5.2; H.1.2; C.2.4; I.5.4"], "comment": "24 pages, 7 figures", "summary": "Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as\nusers naturally change their postures and device orientations. Traditional\ncalibration approaches, like one-off, fail to adapt to these dynamic\nconditions, leading to degraded performance over time. We present MAC-Gaze, a\nMotion-Aware continual Calibration approach that leverages smartphone Inertial\nmeasurement unit (IMU) sensors and continual learning techniques to\nautomatically detect changes in user motion states and update the gaze tracking\nmodel accordingly. Our system integrates a pre-trained visual gaze estimator\nand an IMU-based activity recognition model with a clustering-based hybrid\ndecision-making mechanism that triggers recalibration when motion patterns\ndeviate significantly from previously encountered states. To enable\naccumulative learning of new motion conditions while mitigating catastrophic\nforgetting, we employ replay-based continual learning, allowing the model to\nmaintain performance across previously encountered motion conditions. We\nevaluate our system through extensive experiments on the publicly available\nRGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+\nimages, 800K+ IMU readings), encompassing a wide range of postures under\nvarious motion conditions including sitting, standing, lying, and walking.\nResults demonstrate that our method reduces gaze estimation error by 19.9% on\nRGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to\n1.92 cm) compared to traditional calibration approaches. Our framework provides\na robust solution for maintaining gaze estimation accuracy in mobile scenarios."}
{"id": "2506.00335", "pdf": "https://arxiv.org/pdf/2506.00335", "abs": "https://arxiv.org/abs/2506.00335", "authors": ["Jingyang He", "Shuai Wang", "Ang Li"], "title": "Recover Experimental Data with Selection Bias using Counterfactual Logic", "categories": ["stat.ME", "cs.AI"], "comment": null, "summary": "Selection bias, arising from the systematic inclusion or exclusion of certain\nsamples, poses a significant challenge to the validity of causal inference.\nWhile Bareinboim et al. introduced methods for recovering unbiased\nobservational and interventional distributions from biased data using partial\nexternal information, the complexity of the backdoor adjustment and the\nmethod's strong reliance on observational data limit its applicability in many\npractical settings. In this paper, we formally discover the recoverability of\n$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly\nconstructing counterfactual worlds via Structural Causal Models (SCMs), we\nanalyze how selection mechanisms in the observational world propagate to the\ncounterfactual domain. We derive a complete set of graphical and theoretical\ncriteria to determine that the experimental distribution remain unaffected by\nselection bias. Furthermore, we propose principled methods for leveraging\npartially unbiased observational data to recover $P(Y^*_{x^*})$ from biased\nexperimental datasets. Simulation studies replicating realistic research\nscenarios demonstrate the practical utility of our approach, offering concrete\nguidance for mitigating selection bias in applied causal inference."}
{"id": "2505.18570", "pdf": "https://arxiv.org/pdf/2505.18570", "abs": "https://arxiv.org/abs/2505.18570", "authors": ["Tina Khezresmaeilzadeh", "Parsa Razmara", "Seyedarmin Azizi", "Mohammad Erfan Sadeghi", "Erfan Baghaei Potraghloo"], "title": "VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Stock price prediction remains a complex and high-stakes task in financial\nanalysis, traditionally addressed using statistical models or, more recently,\nlanguage models. In this work, we introduce VISTA (Vision-Language Inference\nfor Stock Time-series Analysis), a novel, training-free framework that\nleverages Vision-Language Models (VLMs) for multi-modal stock forecasting.\nVISTA prompts a VLM with both textual representations of historical stock\nprices and their corresponding line charts to predict future price values. By\ncombining numerical and visual modalities in a zero-shot setting and using\ncarefully designed chain-of-thought prompts, VISTA captures complementary\npatterns that unimodal approaches often miss. We benchmark VISTA against\nstandard baselines, including ARIMA and text-only LLM-based prompting methods.\nExperimental results show that VISTA outperforms these baselines by up to\n89.83%, demonstrating the effectiveness of multi-modal inference for stock\ntime-series analysis and highlighting the potential of VLMs in financial\nforecasting tasks without requiring task-specific training."}
{"id": "2505.24434", "pdf": "https://arxiv.org/pdf/2505.24434", "abs": "https://arxiv.org/abs/2505.24434", "authors": ["Md Shahriar Rahim Siddiqui", "Moshe Eliasof", "Eldad Haber"], "title": "Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Flow matching casts sample generation as learning a continuous-time velocity\nfield that transports noise to data. Existing flow matching networks typically\npredict each point's velocity independently, considering only its location and\ntime along its flow trajectory, and ignoring neighboring points. However, this\npointwise approach may overlook correlations between points along the\ngeneration trajectory that could enhance velocity predictions, thereby\nimproving downstream generation quality. To address this, we propose Graph Flow\nMatching (GFM), a lightweight enhancement that decomposes the learned velocity\ninto a reaction term -- any standard flow matching network -- and a diffusion\nterm that aggregates neighbor information via a graph neural module. This\nreaction-diffusion formulation retains the scalability of deep flow models\nwhile enriching velocity predictions with local context, all at minimal\nadditional computational cost. Operating in the latent space of a pretrained\nvariational autoencoder, GFM consistently improves Fr\\'echet Inception Distance\n(FID) and recall across five image generation benchmarks (LSUN Church, LSUN\nBedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\\times256$), demonstrating its\neffectiveness as a modular enhancement to existing flow matching architectures."}
{"id": "2506.00486", "pdf": "https://arxiv.org/pdf/2506.00486", "abs": "https://arxiv.org/abs/2506.00486", "authors": ["Jun Wu", "Yirong Xiong", "Jiangtao Wen", "Yuxing Han"], "title": "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Despite rapid advancements in the research and deployment of large language\nmodels (LLMs), the statistical distribution of model parameters, as well as\ntheir influence on initialization, training dynamics, and downstream\nefficiency, has received surprisingly little attention. A recent work\nintroduced BackSlash, a training-time compression algorithm. It first\ndemonstrated that pre-trained LLM parameters follow generalized Gaussian\ndistributions (GGDs) better. By optimizing GG priors during training, BackSlash\ncan reduce parameters by up to 90\\% with minimal performance loss. Building on\nthis foundational insight, we propose a unified, end-to-end framework for LLM\noptimization based on the GG model. Our contributions are threefold: (1)\nGG-based initialization scheme that aligns with the statistical structure of\ntrained models, resulting in faster convergence and improved accuracy; (2)\nDeepShape, a post-training regularization method that reshapes weight\ndistributions to match a GG profile, improving compressibility with minimized\ndegradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit\nfloating-point format designed for GG-distributed-initialized BackSlash\ntraining, enabling low-cost inference without compromising accuracy.\nExperiments across diverse model architectures show that our framework\nconsistently yields smaller and faster models that match or outperform standard\ntraining baselines. By grounding LLM development in principled statistical\nmodeling, this work forges a new path toward efficient, scalable, and\nhardware-aware AI systems. The code is available on our project page:\nhttps://huggingface.co/spaces/shifeng3711/gg_prior."}
{"id": "2505.22813", "pdf": "https://arxiv.org/pdf/2505.22813", "abs": "https://arxiv.org/abs/2505.22813", "authors": ["Josiah Couch", "Miao Li", "Rima Arnaout", "Ramy Arnaout"], "title": "X-Factor: Quality Is a Dataset-Intrinsic Property", "categories": ["cs.LG", "68T07", "I.2.6"], "comment": "13 pages, 7 figures", "summary": "In the universal quest to optimize machine-learning classifiers, three\nfactors -- model architecture, dataset size, and class balance -- have been\nshown to influence test-time performance but do not fully account for it.\nPreviously, evidence was presented for an additional factor that can be\nreferred to as dataset quality, but it was unclear whether this was actually a\njoint property of the dataset and the model architecture, or an intrinsic\nproperty of the dataset itself. If quality is truly dataset-intrinsic and\nindependent of model architecture, dataset size, and class balance, then the\nsame datasets should perform better (or worse) regardless of these other\nfactors. To test this hypothesis, here we create thousands of datasets, each\ncontrolled for size and class balance, and use them to train classifiers with a\nwide range of architectures, from random forests and support-vector machines to\ndeep networks. We find that classifier performance correlates strongly by\nsubset across architectures ($R^2=0.79$), supporting quality as an intrinsic\nproperty of datasets independent of dataset size and class balance and of model\narchitecture. Digging deeper, we find that dataset quality appears to be an\nemergent property of something more fundamental: the quality of datasets'\nconstituent classes. Thus, quality joins size, class balance, and model\narchitecture as an independent correlate of performance and a separate target\nfor optimizing machine-learning-based classification."}
{"id": "2505.23585", "pdf": "https://arxiv.org/pdf/2505.23585", "abs": "https://arxiv.org/abs/2505.23585", "authors": ["Yaru Hao", "Li Dong", "Xun Wu", "Shaohan Huang", "Zewen Chi", "Furu Wei"], "title": "On-Policy RL with Optimal Reward Baseline", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO integrates a practically feasible\nformulation of the optimal reward baseline that minimizes gradient variance. We\nevaluate OPO on mathematical reasoning benchmarks. The results demonstrate its\nsuperior performance and training stability without additional models or\nregularization terms. Furthermore, OPO achieves lower policy shifts and higher\noutput entropy, encouraging more diverse and less repetitive responses. These\nresults highlight OPO as a promising direction for stable and effective\nreinforcement learning in large language model alignment and reasoning tasks.\nThe implementation is merged into the verl library at\nhttps://verl.readthedocs.io/en/latest/algo/opo.html."}
{"id": "2506.01950", "pdf": "https://arxiv.org/pdf/2506.01950", "abs": "https://arxiv.org/abs/2506.01950", "authors": ["Jiajun Jiang", "Yiming Zhu", "Zirui Wu", "Jie Song"], "title": "DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 5 figures. Code: https://github.com/Eku127/DualMap Project\n  page: https://eku127.github.io/DualMap/", "summary": "We introduce DualMap, an online open-vocabulary mapping system that enables\nrobots to understand and navigate dynamically changing environments through\nnatural language queries. Designed for efficient semantic mapping and\nadaptability to changing environments, DualMap meets the essential requirements\nfor real-world robot navigation applications. Our proposed hybrid segmentation\nfrontend and object-level status check eliminate the costly 3D object merging\nrequired by prior methods, enabling efficient online scene mapping. The\ndual-map representation combines a global abstract map for high-level candidate\nselection with a local concrete map for precise goal-reaching, effectively\nmanaging and updating dynamic changes in the environment. Through extensive\nexperiments in both simulation and real-world scenarios, we demonstrate\nstate-of-the-art performance in 3D open-vocabulary segmentation, efficient\nscene mapping, and online language-guided navigation."}
{"id": "2506.00691", "pdf": "https://arxiv.org/pdf/2506.00691", "abs": "https://arxiv.org/abs/2506.00691", "authors": ["Junaid Muzaffar", "Khubaib Ahmed", "Ingo Frommholz", "Zeeshan Pervez", "Ahsan ul Haq"], "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training reinforcement learning (RL) agents often requires significant\ncomputational resources and extended training times. To address this, we build\nupon the foundation laid by Google Brain's Sensory Neuron, which introduced a\nnovel neural architecture for reinforcement learning tasks that maintained\npermutation in-variance in the sensory neuron system. While the baseline model\ndemonstrated significant performance improvements over traditional approaches,\nwe identified opportunities to enhance the efficiency of the learning process\nfurther. We propose a modified attention mechanism incorporating a non-linear\ntransformation of the key vectors (K) using a mapping function, resulting in a\nnew set of key vectors (K'). This non-linear mapping enhances the\nrepresentational capacity of the attention mechanism, allowing the model to\nencode more complex feature interactions and accelerating convergence without\ncompromising performance. Our enhanced model demonstrates significant\nimprovements in learning efficiency, showcasing the potential for non-linear\nattention mechanisms in advancing reinforcement learning algorithms."}
{"id": "2505.23470", "pdf": "https://arxiv.org/pdf/2505.23470", "abs": "https://arxiv.org/abs/2505.23470", "authors": ["Chenjie Li", "Amir Gilad", "Boris Glavic", "Zhengjie Miao", "Sudeepa Roy"], "title": "Refining Labeling Functions with Limited Labeled Data", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "techreport", "summary": "Programmatic weak supervision (PWS) significantly reduces human effort for\nlabeling data by combining the outputs of user-provided labeling functions\n(LFs) on unlabeled datapoints. However, the quality of the generated labels\ndepends directly on the accuracy of the LFs. In this work, we study the problem\nof fixing LFs based on a small set of labeled examples. Towards this goal, we\ndevelop novel techniques for repairing a set of LFs by minimally changing their\nresults on the labeled examples such that the fixed LFs ensure that (i) there\nis sufficient evidence for the correct label of each labeled datapoint and (ii)\nthe accuracy of each repaired LF is sufficiently high. We model LFs as\nconditional rules which enables us to refine them, i.e., to selectively change\ntheir output for some inputs. We demonstrate experimentally that our system\nimproves the quality of LFs based on surprisingly small sets of labeled\ndatapoints."}
{"id": "2506.01969", "pdf": "https://arxiv.org/pdf/2506.01969", "abs": "https://arxiv.org/abs/2506.01969", "authors": ["Pengcuo Dege", "Qiuming Luo", "Rui Mao", "Chang Kong"], "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "15 pages, conference", "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP."}
{"id": "2505.23527", "pdf": "https://arxiv.org/pdf/2505.23527", "abs": "https://arxiv.org/abs/2505.23527", "authors": ["Raj Ghugare", "Benjamin Eysenbach"], "title": "Normalizing Flows are Capable Models for RL", "categories": ["cs.LG"], "comment": "Project page with code - https://rajghugare19.github.io/nf4rl/", "summary": "Modern reinforcement learning (RL) algorithms have found success by using\npowerful probabilistic models, such as transformers, energy-based models, and\ndiffusion/flow-based models. To this end, RL researchers often choose to pay\nthe price of accommodating these models into their algorithms -- diffusion\nmodels are expressive, but are computationally intensive due to their reliance\non solving differential equations, while autoregressive transformer models are\nscalable but typically require learning discrete representations. Normalizing\nflows (NFs), by contrast, seem to provide an appealing alternative, as they\nenable likelihoods and sampling without solving differential equations or\nautoregressive architectures. However, their potential in RL has received\nlimited attention, partly due to the prevailing belief that normalizing flows\nlack sufficient expressivity. We show that this is not the case. Building on\nrecent work in NFs, we propose a single NF architecture which integrates\nseamlessly into RL algorithms, serving as a policy, Q-function, and occupancy\nmeasure. Our approach leads to much simpler algorithms, and achieves higher\nperformance in imitation learning, offline, goal conditioned RL and\nunsupervised RL."}
{"id": "2506.01982", "pdf": "https://arxiv.org/pdf/2506.01982", "abs": "https://arxiv.org/abs/2506.01982", "authors": ["Vassilis Lyberatos", "Spyridon Kantarelis", "Ioanna Zioga", "Christina Anagnostopoulou", "Giorgos Stamou", "Anastasia Georgaki"], "title": "Music Interpretation and Emotion Perception: A Computational and Neurophysiological Investigation", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at SMC 2025", "summary": "This study investigates emotional expression and perception in music\nperformance using computational and neurophysiological methods. The influence\nof different performance settings, such as repertoire, diatonic modal etudes,\nand improvisation, as well as levels of expressiveness, on performers'\nemotional communication and listeners' reactions is explored. Professional\nmusicians performed various tasks, and emotional annotations were provided by\nboth performers and the audience. Audio analysis revealed that expressive and\nimprovisational performances exhibited unique acoustic features, while emotion\nanalysis showed stronger emotional responses. Neurophysiological measurements\nindicated greater relaxation in improvisational performances. This multimodal\nstudy highlights the significance of expressivity in enhancing emotional\ncommunication and audience engagement."}
{"id": "2505.24603", "pdf": "https://arxiv.org/pdf/2505.24603", "abs": "https://arxiv.org/abs/2505.24603", "authors": ["Omri Lev", "Vishwak Srinivasan", "Moshe Shenfeld", "Katrina Ligett", "Ayush Sekhari", "Ashia C. Wilson"], "title": "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches", "categories": ["cs.LG"], "comment": null, "summary": "Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime."}
{"id": "2506.02120", "pdf": "https://arxiv.org/pdf/2506.02120", "abs": "https://arxiv.org/abs/2506.02120", "authors": ["Mariana A. Londe", "Luciana S. Pessoa", "Carlos E. Andrade", "José F. Gonçalves", "Mauricio G. C. Resende"], "title": "Random-key genetic algorithms: Principles and applications", "categories": ["cs.NE", "cs.AI", "math.OC", "90-02, 90B40, 90C27", "G.1.6; G.2.1; I.2.8"], "comment": "21 pages, 1 figure, 1 table, 1 algorithm, forthcoming in Handbook of\n  Heuristics, 2nd edition, SpringerNature, New York", "summary": "A random-key genetic algorithm is an evolutionary metaheuristic for discrete\nand global optimization. Each solution is encoded as a vector of N random keys,\nwhere a random key is a real number randomly generated in the continuous\ninterval [0, 1). A decoder maps each vector of random keys to a solution of the\noptimization problem being solved and computes its cost. The benefit of this\napproach is that all genetic operators and transformations can be maintained\nwithin the unitary hypercube, regardless of the problem being addressed. This\nenhances the productivity and maintainability of the core framework. The\nalgorithm starts with a population of P vectors of random keys. At each\niteration, the vectors are partitioned into two sets: a smaller set of\nhigh-valued elite solutions and the remaining non-elite solutions. All elite\nelements are copied, without change, to the next population. A small number of\nrandom-key vectors (the mutants) is added to the population of the next\niteration. The remaining elements of the population of the next iteration are\ngenerated by combining, with the parametrized uniform crossover of Spears and\nDeJong (1991), pairs of solutions. This chapter reviews random-key genetic\nalgorithms and describes an effective variant called biased random-key genetic\nalgorithms."}
{"id": "2506.01016", "pdf": "https://arxiv.org/pdf/2506.01016", "abs": "https://arxiv.org/abs/2506.01016", "authors": ["Olya Mastikhina", "Dhruv Sreenivas", "Pablo Samuel Castro"], "title": "Optimistic critics can empower small actors", "categories": ["cs.LG", "stat.ML"], "comment": "RLC 2025", "summary": "Actor-critic methods have been central to many of the recent advances in deep\nreinforcement learning. The most common approach is to use symmetric\narchitectures, whereby both actor and critic have the same network topology and\nnumber of parameters. However, recent works have argued for the advantages of\nasymmetric setups, specifically with the use of smaller actors. We perform\nbroad empirical investigations and analyses to better understand the\nimplications of this and find that, in general, smaller actors result in\nperformance degradation and overfit critics. Our analyses suggest poor data\ncollection, due to value underestimation, as one of the main causes for this\nbehavior, and further highlight the crucial role the critic can play in\nalleviating this pathology. We explore techniques to mitigate the observed\nvalue underestimation, which enables further research in asymmetric\nactor-critic methods."}
{"id": "2506.02308", "pdf": "https://arxiv.org/pdf/2506.02308", "abs": "https://arxiv.org/abs/2506.02308", "authors": ["Xiaojun Shan", "Qi Cao", "Xing Han", "Haofei Yu", "Paul Pu Liang"], "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in multimodal foundation models have achieved\nstate-of-the-art performance across a range of tasks. These breakthroughs are\nlargely driven by new pre-training paradigms that leverage large-scale,\nunlabeled multimodal data, followed by instruction fine-tuning on curated\nlabeled datasets and high-quality prompts. While there is growing interest in\nscaling instruction fine-tuning to ever-larger datasets in both quantity and\nscale, our findings reveal that simply increasing the number of\ninstruction-tuning tasks does not consistently yield better performance.\nInstead, we observe that grouping tasks by the common interactions across\nmodalities, such as discovering redundant shared information, prioritizing\nmodality selection with unique information, or requiring synergistic fusion to\ndiscover new information from both modalities, encourages the models to learn\ntransferrable skills within a group while suppressing interference from\nmismatched tasks. To this end, we introduce MINT, a simple yet surprisingly\neffective task-grouping strategy based on the type of multimodal interaction.\nWe demonstrate that the proposed method greatly outperforms existing task\ngrouping baselines for multimodal instruction tuning, striking an effective\nbalance between generalization and specialization."}
{"id": "2506.02965", "pdf": "https://arxiv.org/pdf/2506.02965", "abs": "https://arxiv.org/abs/2506.02965", "authors": ["Ze Yu Zhang", "Bolin Ding", "Bryan Kian Hsiang Low"], "title": "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs", "categories": ["cs.LG"], "comment": "20 pages, 4 figures", "summary": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks."}
{"id": "2506.02606", "pdf": "https://arxiv.org/pdf/2506.02606", "abs": "https://arxiv.org/abs/2506.02606", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "title": "Multi Layered Autonomy and AI Ecologies in Robotic Art Installations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Symbiosis of Agents is a large-scale installation by Baoyang Chen\n(baoyangchen.com) that embeds AI-driven robots in an immersive, mirror-lined\narena, probing the tension between machine agency and artistic authorship.\nDrawing on early cybernetics, rule-based conceptual art, and seminal robotic\nworks, it orchestrates fluid exchanges among robotic arms, quadruped machines,\ntheir environment, and the public. A three tier faith system pilots the\necology: micro-level adaptive tactics, meso-level narrative drives, and a\nmacro-level prime directive. This hierarchy lets behaviors evolve organically\nin response to environmental cues and even a viewer's breath, turning\nspectators into co-authors of the unfolding drama. Framed by a speculative\nterraforming scenario that recalls the historical exploitation of marginalized\nlabor, the piece asks who bears responsibility in AI-mediated futures.\nChoreographed motion, AI-generated scripts, reactive lighting, and drifting fog\ncast the robots as collaborators rather than tools, forging a living, emergent\nartwork. Exhibited internationally, Symbiosis of Agents shows how cybernetic\nfeedback, robotic experimentation, and conceptual rule-making can converge to\nredefine agency, authorship, and ethics in contemporary art."}
{"id": "2204.08031", "pdf": "https://arxiv.org/pdf/2204.08031", "abs": "https://arxiv.org/abs/2204.08031", "authors": ["Zhexiao Lin", "Fang Han"], "title": "Limit theorems of Chatterjee's rank correlation", "categories": ["math.ST", "cs.LG", "math.PR", "stat.TH"], "comment": "Multiple minor improvements were made in this version, including (1)\n  a proof of the existence of the limiting variance, (2) some numeric studies,\n  and (3) an analysis of the Sobol' indices", "summary": "Establishing the limiting distribution of Chatterjee's rank correlation for a\ngeneral, possibly non-independent, pair of random variables has been eagerly\nawaited by many. This paper shows that (a) Chatterjee's rank correlation is\nasymptotically normal as long as one variable is not a measurable function of\nthe other, (b) the corresponding asymptotic variance is uniformly bounded by\n36, and (c) a consistent variance estimator exists. Similar results also hold\nfor Azadkia-Chatterjee's graph-based correlation coefficient, a multivariate\nanalogue of Chatterjee's original proposal. The proof is given by appealing to\nH\\'ajek representation and Chatterjee's nearest-neighbor CLT."}
{"id": "2205.11486", "pdf": "https://arxiv.org/pdf/2205.11486", "abs": "https://arxiv.org/abs/2205.11486", "authors": ["Nathan Kallus", "Miruna Oprescu"], "title": "Robust and Agnostic Learning of Conditional Distributional Treatment Effects", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "comment": "24 pages, 6 figures, AISTATS 2023", "summary": "The conditional average treatment effect (CATE) is the best measure of\nindividual causal effects given baseline covariates. However, the CATE only\ncaptures the (conditional) average, and can overlook risks and tail events,\nwhich are important to treatment choice. In aggregate analyses, this is usually\naddressed by measuring the distributional treatment effect (DTE), such as\ndifferences in quantiles or tail expectations between treatment groups.\nHypothetically, one can similarly fit conditional quantile regressions in each\ntreatment group and take their difference, but this would not be robust to\nmisspecification or provide agnostic best-in-class predictions. We provide a\nnew robust and model-agnostic methodology for learning the conditional DTE\n(CDTE) for a class of problems that includes conditional quantile treatment\neffects, conditional super-quantile treatment effects, and conditional\ntreatment effects on coherent risk measures given by $f$-divergences. Our\nmethod is based on constructing a special pseudo-outcome and regressing it on\ncovariates using any regression learner. Our method is model-agnostic in that\nit can provide the best projection of CDTE onto the regression model class. Our\nmethod is robust in that even if we learn these nuisances nonparametrically at\nvery slow rates, we can still learn CDTEs at rates that depend on the class\ncomplexity and even conduct inferences on linear projections of CDTEs. We\ninvestigate the behavior of our proposal in simulations, as well as in a case\nstudy of 401(k) eligibility effects on wealth."}
{"id": "2305.18270", "pdf": "https://arxiv.org/pdf/2305.18270", "abs": "https://arxiv.org/abs/2305.18270", "authors": ["Yatin Dandi", "Florent Krzakala", "Bruno Loureiro", "Luca Pesce", "Ludovic Stephan"], "title": "How Two-Layer Neural Networks Learn, One (Giant) Step at a Time", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "For high-dimensional Gaussian data, we investigate theoretically how the\nfeatures of a two-layer neural network adapt to the structure of the target\nfunction through a few large batch gradient descent steps, leading to an\nimprovement in the approximation capacity from initialization. First, we\ncompare the influence of batch size to that of multiple steps. For a single\nstep, a batch of size $n = \\mathcal{O}(d)$ is both necessary and sufficient to\nalign with the target function, although only a single direction can be\nlearned. In contrast, $n = \\mathcal{O}(d^2)$ is essential for neurons to\nspecialize in multiple relevant directions of the target with a single gradient\nstep. Even in this case, we show there might exist ``hard'' directions\nrequiring $n = \\mathcal{O}(d^\\ell)$ samples to be learned, where $\\ell$ is\nknown as the leap index of the target. Second, we show that the picture\ndrastically improves over multiple gradient steps: a batch size of $n =\n\\mathcal{O}(d)$ is indeed sufficient to learn multiple target directions\nsatisfying a staircase property, where more and more directions can be learned\nover time. Finally, we discuss how these directions allow for a drastic\nimprovement in the approximation capacity and generalization error over the\ninitialization, illustrating a separation of scale between the random\nfeatures/lazy regime and the feature learning regime. Our technical analysis\nleverages a combination of techniques related to concentration,\nprojection-based conditioning, and Gaussian equivalence, which we believe are\nof independent interest. By pinning down the conditions necessary for\nspecialization and learning, our results highlight the intertwined role of the\nstructure of the task to learn, the details of the algorithm, and the\narchitecture, shedding new light on how neural networks adapt to the feature\nand learn complex task from data over time."}
{"id": "2402.17732", "pdf": "https://arxiv.org/pdf/2402.17732", "abs": "https://arxiv.org/abs/2402.17732", "authors": ["Rong Jiang", "Cong Ma"], "title": "Batched Nonparametric Contextual Bandits", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": "Accepted to IEEE Transactions on Information Theory", "summary": "We study nonparametric contextual bandits under batch constraints, where the\nexpected reward for each action is modeled as a smooth function of covariates,\nand the policy updates are made at the end of each batch of observations. We\nestablish a minimax regret lower bound for this setting and propose a novel\nbatch learning algorithm that achieves the optimal regret (up to logarithmic\nfactors). In essence, our procedure dynamically splits the covariate space into\nsmaller bins, carefully aligning their widths with the batch size. Our\ntheoretical results suggest that for nonparametric contextual bandits, a nearly\nconstant number of policy updates can attain optimal regret in the fully online\nsetting."}
{"id": "2403.03702", "pdf": "https://arxiv.org/pdf/2403.03702", "abs": "https://arxiv.org/abs/2403.03702", "authors": ["Alban Farchi", "Marcin Chrust", "Marc Bocquet", "Massimo Bonavita"], "title": "Development of an offline and online hybrid model for the Integrated Forecasting System", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In recent years, there has been significant progress in the development of\nfully data-driven global numerical weather prediction models. These machine\nlearning weather prediction models have their strength, notably accuracy and\nlow computational requirements, but also their weakness: they struggle to\nrepresent fundamental dynamical balances, and they are far from being suitable\nfor data assimilation experiments. Hybrid modelling emerges as a promising\napproach to address these limitations. Hybrid models integrate a physics-based\ncore component with a statistical component, typically a neural network, to\nenhance prediction capabilities. In this article, we propose to develop a model\nerror correction for the operational Integrated Forecasting System (IFS) of the\nEuropean Centre for Medium-Range Weather Forecasts using a neural network. The\nneural network is initially pre-trained offline using a large dataset of\noperational analyses and analysis increments. Subsequently, the trained network\nis integrated into the IFS within the Object-Oriented Prediction System (OOPS)\nso as to be used in data assimilation and forecast experiments. It is then\nfurther trained online using a recently developed variant of weak-constraint\n4D-Var. The results show that the pre-trained neural network already provides a\nreliable model error correction, which translates into reduced forecast errors\nin many conditions and that the online training further improves the accuracy\nof the hybrid model in many conditions."}
{"id": "2404.17589", "pdf": "https://arxiv.org/pdf/2404.17589", "abs": "https://arxiv.org/abs/2404.17589", "authors": ["Peng Liu", "Cong Xu", "Ming Zhao", "Jiawei Zhu", "Bin Wang", "Yi Ren"], "title": "An Offline Reinforcement Learning Algorithm Customized for Multi-Task Fusion in Large-Scale Recommender Systems", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the offline RL algorithms used for MTF so far have the\nfollowing severe problems: 1) to avoid out-of-distribution (OOD) problem, their\nconstraints are overly strict, which seriously damage their performance; 2)\nthey are unaware of the exploration policy used for producing training data and\nnever interact with real environment, so only suboptimal policy can be learned;\n3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates offline RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements."}
{"id": "2407.13858", "pdf": "https://arxiv.org/pdf/2407.13858", "abs": "https://arxiv.org/abs/2407.13858", "authors": ["Mohammad Aamir Sohail", "Mohsen Heidari", "S. Sandeep Pradhan"], "title": "Quantum Natural Stochastic Pairwise Coordinate Descent", "categories": ["quant-ph", "cs.LG", "math.OC"], "comment": null, "summary": "Variational quantum algorithms, optimized using gradient-based methods, often\nexhibit sub-optimal convergence performance due to their dependence on\nEuclidean geometry. Quantum natural gradient descent (QNGD) is a more efficient\nmethod that incorporates the geometry of the state space via a quantum\ninformation metric. However, QNGD is computationally intensive and suffers from\nhigh sample complexity. In this work, we formulate a novel quantum information\nmetric and construct an unbiased estimator for this metric using single-shot\nmeasurements. We develop a quantum optimization algorithm that leverages the\ngeometry of the state space via this estimator while avoiding full-state\ntomography, as in conventional techniques. We provide the convergence analysis\nof the algorithm under mild conditions. Furthermore, we provide experimental\nresults that demonstrate the better sample complexity and faster convergence of\nour algorithm compared to the state-of-the-art approaches. Our results\nillustrate the algorithm's ability to avoid saddle points and local minima."}
{"id": "2410.00075", "pdf": "https://arxiv.org/pdf/2410.00075", "abs": "https://arxiv.org/abs/2410.00075", "authors": ["Daan Caljon", "Jente Van Belle", "Jeroen Berrevoets", "Wouter Verbeke"], "title": "Optimizing Treatment Allocation in the Presence of Interference", "categories": ["cs.SI", "cs.LG", "stat.ML"], "comment": null, "summary": "In Influence Maximization (IM), the objective is to -- given a budget --\nselect the optimal set of entities in a network to target with a treatment so\nas to maximize the total effect. For instance, in marketing, the objective is\nto target the set of customers that maximizes the total response rate,\nresulting from both direct treatment effects on targeted customers and\nindirect, spillover, effects that follow from targeting these customers.\nRecently, new methods to estimate treatment effects in the presence of network\ninterference have been proposed. However, the issue of how to leverage these\nmodels to make better treatment allocation decisions has been largely\noverlooked. Traditionally, in Uplift Modeling (UM), entities are ranked\naccording to estimated treatment effect, and the top entities are allocated\ntreatment. Since, in a network context, entities influence each other, the UM\nranking approach will be suboptimal. The problem of finding the optimal\ntreatment allocation in a network setting is \\textcolor{red}{NP-hard,} and\ngenerally has to be solved heuristically. To fill the gap between IM and UM, we\npropose OTAPI: Optimizing Treatment Allocation in the Presence of Interference\nto find solutions to the IM problem using treatment effect estimates. OTAPI\nconsists of two steps. First, a causal estimator is trained to predict\ntreatment effects in a network setting. Second, this estimator is leveraged to\nidentify an optimal treatment allocation by integrating it into classic IM\nalgorithms. We demonstrate that this novel method outperforms classic IM and UM\napproaches on both synthetic and semi-synthetic datasets."}
{"id": "2502.07975", "pdf": "https://arxiv.org/pdf/2502.07975", "abs": "https://arxiv.org/abs/2502.07975", "authors": ["Oliver Biggar", "Christos Papadimitriou"], "title": "Sink equilibria and the attractors of learning in games", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "Characterizing the limit behavior -- that is, the attractors -- of learning\ndynamics is one of the most fundamental open questions in game theory. In\nrecent work in this front, it was conjectured that the attractors of the\nreplicator dynamic are in one-to-one correspondence with the sink equilibria of\nthe game -- the sink strongly connected components of a game's preference graph\n-- , and it was established that they do stand in at least one-to-many\ncorrespondence with them. We make threefold progress on the problem of\ncharacterizing attractors. First, we show through a topological construction\nthat the one-to-one conjecture is false. The counterexamples derive from\nobjects called local sources -- fixed points which lie within the sink\nequilibrium yet are locally repelling. Second, we make progress on the\nattractor characterization problem for two-player games by establishing that\nthe one-to-one conjecture is true when a local property called pseudoconvexity\nholds. Pseudoconvexity prevents the existence of local sources, and generalizes\nthe existing cases -- such as zero-sum games and potential games -- where the\nconjecture was known to be true."}
{"id": "2502.08001", "pdf": "https://arxiv.org/pdf/2502.08001", "abs": "https://arxiv.org/abs/2502.08001", "authors": ["Haonan Shi", "Tu Ouyang", "An Wang"], "title": "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated Distillation", "categories": ["cs.CR", "cs.LG"], "comment": "To appear in Proceedings of Privacy Enhancing Technologies 2025", "summary": "Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems."}
{"id": "2502.12089", "pdf": "https://arxiv.org/pdf/2502.12089", "abs": "https://arxiv.org/abs/2502.12089", "authors": ["Alessandro Favero", "Antonio Sclocchi", "Francesco Cagnetta", "Pascal Frossard", "Matthieu Wyart"], "title": "How Compositional Generalization and Creativity Improve as Diffusion Models are Trained", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Natural data is often organized as a hierarchical composition of features.\nHow many samples do generative models need in order to learn the composition\nrules, so as to produce a combinatorially large number of novel data? What\nsignal in the data is exploited to learn those rules? We investigate these\nquestions in the context of diffusion models both theoretically and\nempirically. Theoretically, we consider a simple probabilistic context-free\ngrammar - a tree-like graphical model used to represent the hierarchical and\ncompositional structure of data such as language and images. We demonstrate\nthat diffusion models learn the grammar's composition rules with the sample\ncomplexity required for clustering features with statistically similar context,\na process similar to the word2vec algorithm. However, this clustering emerges\nhierarchically: higher-level features associated with longer contexts require\nmore data to be identified. This mechanism leads to a sample complexity that\nscales polynomially with the said context size. As a result, diffusion models\ntrained on an intermediate dataset size generate data coherent up to a certain\nscale, but lacking global coherence. We test these predictions across different\ndomains and find remarkable agreement: both generated texts and images achieve\nprogressively larger coherence lengths as the training time or dataset size\ngrows. We discuss connections between the hierarchical clustering mechanism we\nintroduce here and the renormalization group in physics."}
{"id": "2502.18284", "pdf": "https://arxiv.org/pdf/2502.18284", "abs": "https://arxiv.org/abs/2502.18284", "authors": ["Zonghao Chen", "Masha Naslidnyk", "François-Xavier Briol"], "title": "Nested Expectations with Kernel Quadrature", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers the challenging computational task of estimating nested\nexpectations. Existing algorithms, such as nested Monte Carlo or multilevel\nMonte Carlo, are known to be consistent but require a large number of samples\nat both inner and outer levels to converge. Instead, we propose a novel\nestimator consisting of nested kernel quadrature estimators and we prove that\nit has a faster convergence rate than all baseline methods when the integrands\nhave sufficient smoothness. We then demonstrate empirically that our proposed\nmethod does indeed require fewer samples to estimate nested expectations on\nreal-world applications including Bayesian optimisation, option pricing, and\nhealth economics."}
{"id": "2503.02407", "pdf": "https://arxiv.org/pdf/2503.02407", "abs": "https://arxiv.org/abs/2503.02407", "authors": ["Nikita Kazeev", "Wei Nong", "Ignat Romanov", "Ruiming Zhu", "Andrey Ustyuzhanin", "Shuya Yamazaki", "Kedar Hippalgaonkar"], "title": "Wyckoff Transformer: Generation of Symmetric Crystals", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph", "I.2.6"], "comment": "https://github.com/SymmetryAdvantage/WyckoffTransformer", "summary": "Crystal symmetry plays a fundamental role in determining its physical,\nchemical, and electronic properties such as electrical and thermal\nconductivity, optical and polarization behavior, and mechanical strength.\nAlmost all known crystalline materials have internal symmetry. However, this is\noften inadequately addressed by existing generative models, making the\nconsistent generation of stable and symmetrically valid crystal structures a\nsignificant challenge. We introduce WyFormer, a generative model that directly\ntackles this by formally conditioning on space group symmetry. It achieves this\nby using Wyckoff positions as the basis for an elegant, compressed, and\ndiscrete structure representation. To model the distribution, we develop a\npermutation-invariant autoregressive model based on the Transformer encoder and\nan absence of positional encoding. Extensive experimentation demonstrates\nWyFormer's compelling combination of attributes: it achieves best-in-class\nsymmetry-conditioned generation, incorporates a physics-motivated inductive\nbias, produces structures with competitive stability, predicts material\nproperties with competitive accuracy even without atomic coordinates, and\nexhibits unparalleled inference speed."}
{"id": "2503.04091", "pdf": "https://arxiv.org/pdf/2503.04091", "abs": "https://arxiv.org/abs/2503.04091", "authors": ["Ziqiao Wang", "Cheng Long", "Yongyi Mao"], "title": "Generalization in Federated Learning: A Conditional Mutual Information Framework", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": "Accepted to ICML 2025", "summary": "Federated learning (FL) is a widely adopted privacy-preserving distributed\nlearning framework, yet its generalization performance remains less explored\ncompared to centralized learning. In FL, the generalization error consists of\ntwo components: the out-of-sample gap, which measures the gap between the\nempirical and true risk for participating clients, and the participation gap,\nwhich quantifies the risk difference between participating and\nnon-participating clients. In this work, we apply an information-theoretic\nanalysis via the conditional mutual information (CMI) framework to study FL's\ntwo-level generalization. Beyond the traditional supersample-based CMI\nframework, we introduce a superclient construction to accommodate the two-level\ngeneralization setting in FL. We derive multiple CMI-based bounds, including\nhypothesis-based CMI bounds, illustrating how privacy constraints in FL can\nimply generalization guarantees. Furthermore, we propose fast-rate evaluated\nCMI bounds that recover the best-known convergence rate for two-level FL\ngeneralization in the small empirical risk regime. For specific FL model\naggregation strategies and structured loss functions, we refine our bounds to\nachieve improved convergence rates with respect to the number of participating\nclients. Empirical evaluations confirm that our evaluated CMI bounds are\nnon-vacuous and accurately capture the generalization behavior of FL\nalgorithms."}
{"id": "2503.09492", "pdf": "https://arxiv.org/pdf/2503.09492", "abs": "https://arxiv.org/abs/2503.09492", "authors": ["Yunli Wang", "Zhen Zhang", "Zhiqiang Wang", "Zixuan Yang", "Yu Li", "Jian Yang", "Shiyang Wen", "Peng Jiang", "Kun Gai"], "title": "Learning Cascade Ranking as One Network", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances have introduced interaction-aware training paradigms,\nbut still struggle to 1) align training objectives with the goal of the entire\ncascade ranking (i.e., end-to-end recall of ground-truth items) and 2) learn\neffective collaboration patterns for different stages. To address these\nchallenges, we propose LCRON, which introduces a novel surrogate loss function\nderived from the lower bound probability that ground truth items are selected\nby cascade ranking, ensuring alignment with the overall objective of the\nsystem. According to the properties of the derived bound, we further design an\nauxiliary loss for each stage to drive the reduction of this bound, leading to\na more robust and effective top-k selection. LCRON enables end-to-end training\nof the entire cascade ranking system as a unified network. Experimental results\ndemonstrate that LCRON achieves significant improvement over existing methods\non public benchmarks and industrial applications, addressing key limitations in\ncascade ranking training and significantly enhancing system performance."}
{"id": "2504.09567", "pdf": "https://arxiv.org/pdf/2504.09567", "abs": "https://arxiv.org/abs/2504.09567", "authors": ["Chenxuan He", "Yuan Gao", "Liping Zhu", "Jian Huang"], "title": "Conditional Independence Test Based on Transport Maps", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G05, 62G08, 68T07"], "comment": "48 pages", "summary": "Testing conditional independence between two random vectors given a third is\na fundamental and challenging problem in statistics, particularly in\nmultivariate nonparametric settings due to the complexity of conditional\nstructures. We propose a innovative framework for testing conditional\nindependence using transport maps. At the population level, we show that two\nwell-defined transport maps can transform the conditional independence test\ninto an unconditional independence test, this substantially simplifies the\nproblem. These transport maps are estimated from data using conditional\ncontinuous normalizing flow models. Within this framework, we derive a test\nstatistic and prove its asymptotic validity under both the null and alternative\nhypotheses. A permutation-based procedure is employed to evaluate the\nsignificance of the test. We validate the proposed method through extensive\nsimulations and real-data analysis. Our numerical studies demonstrate the\npractical effectiveness of the proposed method for conditional independence\ntesting."}
{"id": "2504.17959", "pdf": "https://arxiv.org/pdf/2504.17959", "abs": "https://arxiv.org/abs/2504.17959", "authors": ["Yinlong Dai", "Robert Ramirez Sanchez", "Ryan Jeronimus", "Shahabedin Sagheb", "Cara M. Nunez", "Heramb Nemlekar", "Dylan P. Losey"], "title": "CIVIL: Causal and Intuitive Visual Imitation Learning", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Today's robots learn new tasks by imitating human examples. However, this\nstandard approach to visual imitation learning is fundamentally limited: the\nrobot observes what the human does, but not why the human chooses those\nbehaviors. Without understanding the features that factor into the human's\ndecisions, robot learners often misinterpret the data and fail to perform the\ntask when the environment changes. We therefore propose a shift in perspective:\ninstead of asking human teachers just to show what actions the robot should\ntake, we also enable humans to indicate task-relevant features using markers\nand language prompts. Our proposed algorithm, CIVIL, leverages this augmented\ndata to filter the robot's visual observations and extract a feature\nrepresentation that causally informs human actions. CIVIL then applies these\ncausal features to train a transformer-based policy that emulates human\nbehaviors without being confused by visual distractors. Our simulations,\nreal-world experiments, and user study demonstrate that robots trained with\nCIVIL can learn from fewer human demonstrations and perform better than\nstate-of-the-art baselines, especially in previously unseen scenarios. See\nvideos at our project website: https://civil2025.github.io"}
{"id": "2504.18791", "pdf": "https://arxiv.org/pdf/2504.18791", "abs": "https://arxiv.org/abs/2504.18791", "authors": ["Uday Kiran Reddy Tadipatri", "Benjamin D. Haeffele", "Joshua Agterberg", "Ingvar Ziemann", "René Vidal"], "title": "Nonconvex Linear System Identification with Minimal State Representation", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "stat.ML"], "comment": "Accepted to the 7th Annual Conference on Learning for Dynamics and\n  Control (L4DC) 2025. The full version including appendix", "summary": "Low-order linear System IDentification (SysID) addresses the challenge of\nestimating the parameters of a linear dynamical system from finite samples of\nobservations and control inputs with minimal state representation. Traditional\napproaches often utilize Hankel-rank minimization, which relies on convex\nrelaxations that can require numerous, costly singular value decompositions\n(SVDs) to optimize. In this work, we propose two nonconvex reformulations to\ntackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel\nmatrix for efficient nuclear norm minimization, and (ii) optimizing directly\nover system parameters for real, diagonalizable systems with an atomic norm\nstyle decomposition. These reformulations circumvent the need for repeated\nheavy SVD computations, significantly improving computational efficiency.\nMoreover, we prove that optimizing directly over the system parameters yields\nlower statistical error rates, and lower sample complexities that do not scale\nlinearly with trajectory length like in Hankel-nuclear norm minimization.\nAdditionally, while our proposed formulations are nonconvex, we provide\ntheoretical guarantees of achieving global optimality in polynomial time.\nFinally, we demonstrate algorithms that solve these nonconvex programs and\nvalidate our theoretical claims on synthetic data."}
{"id": "2505.00237", "pdf": "https://arxiv.org/pdf/2505.00237", "abs": "https://arxiv.org/abs/2505.00237", "authors": ["Ze Zhang", "Georg Hess", "Junjie Hu", "Emmanuel Dean", "Lennart Svensson", "Knut Åkesson"], "title": "Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Published in IEEE Robotics and Automation Letters (RA-L)", "summary": "This paper proposes an integrated approach for the safe and efficient control\nof mobile robots in dynamic and uncertain environments. The approach consists\nof two key steps: one-shot multimodal motion prediction to anticipate motions\nof dynamic obstacles and model predictive control to incorporate these\npredictions into the motion planning process. Motion prediction is driven by an\nenergy-based neural network that generates high-resolution, multi-step\npredictions in a single operation. The prediction outcomes are further utilized\nto create geometric shapes formulated as mathematical constraints. Instead of\ntreating each dynamic obstacle individually, predicted obstacles are grouped by\nproximity in an unsupervised way to improve performance and efficiency. The\noverall collision-free navigation is handled by model predictive control with a\nspecific design for proactive dynamic obstacle avoidance. The proposed approach\nallows mobile robots to navigate effectively in dynamic environments. Its\nperformance is accessed across various scenarios that represent typical\nwarehouse settings. The results demonstrate that the proposed approach\noutperforms other existing dynamic obstacle avoidance methods."}
{"id": "2505.04627", "pdf": "https://arxiv.org/pdf/2505.04627", "abs": "https://arxiv.org/abs/2505.04627", "authors": ["Jean-Michel Tucny", "Mihir Durve", "Sauro Succi"], "title": "Is the end of Insight in Sight ?", "categories": ["physics.comp-ph", "cs.LG", "physics.data-an"], "comment": "15 pages, 2 figures", "summary": "The rise of deep learning challenges the longstanding scientific ideal of\ninsight - the human capacity to understand phenomena by uncovering underlying\nmechanisms. In many modern applications, accurate predictions no longer require\ninterpretable models, prompting debate about whether explainability is a\nrealistic or even meaningful goal. From our perspective in physics, we examine\nthis tension through a concrete case study: a physics-informed neural network\n(PINN) trained on a rarefied gas dynamics problem governed by the Boltzmann\nequation. Despite the system's clear structure and well-understood governing\nlaws, the trained network's weights resemble Gaussian-distributed random\nmatrices, with no evident trace of the physical principles involved. This\nsuggests that deep learning and traditional simulation may follow distinct\ncognitive paths to the same outcome - one grounded in mechanistic insight, the\nother in statistical interpolation. Our findings raise critical questions about\nthe limits of explainable AI and whether interpretability can - or\nshould-remain a universal standard in artificial reasoning."}
{"id": "2505.17836", "pdf": "https://arxiv.org/pdf/2505.17836", "abs": "https://arxiv.org/abs/2505.17836", "authors": ["Anna Van Elst", "Igor Colin", "Stephan Clémençon"], "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/\\sqrt{t})$ rate for trimmed mean\nestimation, where by $t$ is meant the number of iterations. Moreover, we\nprovide a breakdown point analysis of \\textsc{GoTrim}. We empirically validate\nour theoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes."}
{"id": "2506.01226", "pdf": "https://arxiv.org/pdf/2506.01226", "abs": "https://arxiv.org/abs/2506.01226", "authors": ["Nicholas H. Barbara", "Ruigang Wang", "Alexandre Megretski", "Ian R. Manchester"], "title": "React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We study parameterizations of stabilizing nonlinear policies for\nlearning-based control. We propose a structure based on a nonlinear version of\nthe Youla-Kucera parameterization combined with robust neural networks such as\nthe recurrent equilibrium network (REN). The resulting parameterizations are\nunconstrained, and hence can be searched over with first-order optimization\nmethods, while always ensuring closed-loop stability by construction. We study\nthe combination of (a) nonlinear dynamics, (b) partial observation, and (c)\nincremental closed-loop stability requirements (contraction and Lipschitzness).\nWe find that with any two of these three difficulties, a contracting and\nLipschitz Youla parameter always leads to contracting and Lipschitz closed\nloops. However, if all three hold, then incremental stability can be lost with\nexogenous disturbances. Instead, a weaker condition is maintained, which we\ncall d-tube contraction and Lipschitzness. We further obtain converse results\nshowing that the proposed parameterization covers all contracting and Lipschitz\nclosed loops for certain classes of nonlinear systems. Numerical experiments\nillustrate the utility of our parameterization when learning controllers with\nbuilt-in stability certificates for: (i) \"economic\" rewards without stabilizing\neffects; (ii) short training horizons; and (iii) uncertain systems."}
{"id": "2506.03074", "pdf": "https://arxiv.org/pdf/2506.03074", "abs": "https://arxiv.org/abs/2506.03074", "authors": ["Junghyun Lee", "Kyoungseok Jang", "Kwang-Sung Jun", "Milan Vojnović", "Se-Young Yun"], "title": "GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for Generalized Low-Rank Trace Regression", "categories": ["stat.ML", "cs.LG"], "comment": "53 pages, 2 figures, 3 tables; Accepted as a Spotlight Poster to the\n  42nd International Conference on Machine Learning (ICML 2025). Minor\n  correction to the arXiv title in v2 ;)", "summary": "We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024)."}
