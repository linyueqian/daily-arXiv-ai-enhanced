<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 76]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 107]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TL;DR: A novel financial QA system using BERT for non-factoid answer selection, improving state-of-the-art results on the FiQA dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and language specificity in finance by automating unstructured data analysis for decision-making.

Method: Combines BM25 for answer retrieval and BERT variants for re-ranking, with focus on fine-tuning approaches.

Result: FinBERT-QA improves MRR by 16%, NDCG by 17%, and Precision@1 by 21% on the FiQA dataset.

Conclusion: The proposed system effectively enhances financial QA, demonstrating significant performance gains.

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>


### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, Philip S. Yu*

Main category: cs.CL

TL;DR: The paper surveys LLM-based human-agent systems (LLM-HAS), addressing challenges like reliability and safety in autonomous LLM agents by integrating human input. It provides a structured overview of core components, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fully autonomous LLM agents (e.g., hallucinations, safety risks) by incorporating human feedback and control, enhancing performance and reliability.

Method: The paper conducts a comprehensive survey of LLM-HAS, clarifying concepts, detailing core components (environment, human feedback, interaction types, etc.), and exploring applications.

Result: A structured overview of LLM-HAS, highlighting its components, emerging applications, and challenges.

Conclusion: The survey aims to advance research in LLM-HAS by consolidating knowledge and identifying opportunities for innovation in this interdisciplinary field.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato, Rafael Peñaloza, Marco Viviani, Gabriella Pasi*

Main category: cs.CL

TL;DR: The paper analyzes LLMs' reasoning competence, focusing on prompt dependency, using a new benchmark dataset of simple reasoning questions. It finds that larger LLMs perform better in zero-shot settings, but chain-of-thought prompting's impact varies.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability in simple reasoning tasks and understand their dependency on prompts.

Method: Introduced a benchmark dataset with basic geometric reasoning questions, tested 24 LLMs with zero-shot and few-shot prompting, and evaluated chain-of-thought prompting on 22 LLMs.

Result: Larger LLMs (70B+ parameters) perform better in zero-shot settings, but chain-of-thought prompting's effectiveness depends on its timing relative to the answer.

Conclusion: LLMs show potential in reasoning tasks but require further improvement, and prompt design significantly impacts performance.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>


### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger, Ulf Leser*

Main category: cs.CL

TL;DR: The study evaluates pre-trained language models (PLMs) enhanced with contextual information for biomedical relationship extraction, emphasizing model choice and hyperparameter optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing inconsistencies in PLM-based relationship extraction studies due to variations in models, databases, and evaluation methods.

Method: Evaluated three baseline PLMs with hyperparameter optimization, then enhanced the top model with contextual data (entity descriptions, knowledge graphs, molecular structures).

Result: Model choice and hyperparameter optimization are critical; contextual data offers minor overall improvements but significant benefits for smaller PLMs.

Conclusion: Consistent evaluation and contextual data can enhance PLM performance, especially for smaller models.

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>


### [5] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han, Hui Chen, Soujanya Poria*

Main category: cs.CL

TL;DR: PREMISE is a new architecture for multimodal learning, outperforming fusion-based methods by focusing on matching scores and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To improve performance in multimodal tasks like MRHP by addressing the limitations of fusion-based methods, which rely on cross-modal attention.

Method: PREMISE computes multi-scale and multi-field representations, filters duplicates, and uses matching scores as feature vectors for recommendations.

Result: PREMISE achieves better performance than state-of-the-art fusion-based methods on two datasets, with lower computational costs.

Conclusion: PREMISE is effective for tasks where context matching is crucial, offering a scalable and efficient alternative to fusion-based approaches.

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>


### [6] [REFFLY: Melody-Constrained Lyrics Editing Model](https://arxiv.org/abs/2409.00292)
*Songyan Zhao, Bingxuan Li, Yufei Tian, Nanyun Peng*

Main category: cs.CL

TL;DR: REFFLY is a revision framework for melody-aligned lyrics, outperforming baselines like Lyra and GPT-4 by 25% in musicality and text quality.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and practical lyric generation by revising plain text drafts to fit melodies, supporting applications like song translation and style transfer.

Method: Uses a lyric revision module trained on a synthesized dataset and incorporates training-free heuristics for semantic and musical consistency.

Result: REFFLY outperforms Lyra and GPT-4 by 25% in musicality and text quality across tasks like lyrics generation and song translation.

Conclusion: REFFLY provides an effective and flexible solution for melody-aligned lyric generation and editing.

Abstract: Automatic melody-to-lyric (M2L) generation aims to create lyrics that align
with a given melody. While most previous approaches generate lyrics from
scratch, revision, editing plain text draft to fit it into the melody, offers a
much more flexible and practical alternative. This enables broad applications,
such as generating lyrics from flexible inputs (keywords, themes, or full text
that needs refining to be singable), song translation (preserving meaning
across languages while keeping the melody intact), or style transfer (adapting
lyrics to different genres). This paper introduces REFFLY (REvision Framework
For LYrics), the first revision framework for editing and generating
melody-aligned lyrics. We train the lyric revision module using our curated
synthesized melody-aligned lyrics dataset, enabling it to transform plain text
into lyrics that align with a given melody. To further enhance the revision
ability, we propose training-free heuristics aimed at preserving both semantic
meaning and musical consistency throughout the editing process. Experimental
results demonstrate the effectiveness of REFFLY across various tasks (e.g.
lyrics generation, song translation), showing that our model outperforms strong
baselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both
musicality and text quality.

</details>


### [7] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov, John Blake, Julián Villegas, Nicholas Carr*

Main category: cs.CL

TL;DR: LLMs like GPT-4o and Neural Chat can scale Dynamic Assessment (DA) in language learning, with GPT-4o excelling in feedback quality and system performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in scaling Dynamic Assessment (DA) for language learning, enabling broader application beyond traditional teacher-learner settings.

Method: Developed DynaWrite, a modular tutoring app supporting multiple LLMs, tested 21 models, and evaluated top candidates (GPT-4o and Neural Chat) for error identification and feedback quality.

Result: GPT-4o outperformed Neural Chat in feedback quality (clear, consistent hints) and system performance (speed, stability), though both identified errors similarly.

Conclusion: LLMs, particularly GPT-4o, can effectively scale DA, making it feasible for larger groups in language learning.

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>


### [8] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung*

Main category: cs.CL

TL;DR: The Llama-Nemotron series offers open, heterogeneous reasoning models with competitive performance, efficiency, and a dynamic reasoning toggle. Three sizes are available, trained via neural architecture search, distillation, and reinforcement learning. Resources include models, datasets, and codebases.


<details>
  <summary>Details</summary>
Motivation: To provide open-source, high-performance reasoning models with enterprise-friendly licensing and superior efficiency.

Method: Training involves neural architecture search, knowledge distillation, continued pretraining, supervised fine-tuning, and large-scale reinforcement learning.

Result: Models (Nano, Super, Ultra) compete with state-of-the-art reasoning models while offering better throughput and memory efficiency.

Conclusion: Llama-Nemotron models advance open research with commercially permissive licensing and released resources (models, datasets, codebases).

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [9] [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
*Tygo Bloem, Filip Ilievski*

Main category: cs.CL

TL;DR: A novel method for clustering memes using template-based matching and multi-dimensional similarity features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Meme clustering is understudied despite its importance for toxicity detection and virality modeling, with existing methods lacking adaptability and semantic understanding.

Method: Uses template-based matching with multi-dimensional similarity features (form, visual content, text, identity) to cluster memes without predefined databases.

Result: Outperforms existing clustering methods, producing more consistent and coherent clusters aligned with human intuition.

Conclusion: The method supports adaptive matching and eliminates the need for databases, with code made publicly available for further research.

Abstract: Meme clustering is critical for toxicity detection, virality modeling, and
typing, but it has received little attention in previous research. Clustering
similar Internet memes is challenging due to their multimodality, cultural
context, and adaptability. Existing approaches rely on databases, overlook
semantics, and struggle to handle diverse dimensions of similarity. This paper
introduces a novel method that uses template-based matching with
multi-dimensional similarity features, thus eliminating the need for predefined
databases and supporting adaptive matching. Memes are clustered using local and
global features across similarity categories such as form, visual content,
text, and identity. Our combined approach outperforms existing clustering
methods, producing more consistent and coherent clusters, while
similarity-based feature sets enable adaptability and align with human
intuition. We make all supporting code publicly available to support subsequent
research.

</details>


### [10] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen, Qianmu Li, Xiaocong Wu, Huifeng Li, Qing Chang*

Main category: cs.CL

TL;DR: The paper proposes a character-based diffusion embedding algorithm (CDEA) and integrates XLNet to improve steganographic text quality by leveraging sensitive information properties and enhancing high-probability word selection.


<details>
  <summary>Details</summary>
Motivation: Existing models and embedding algorithms struggle with generating high-quality steganographic text due to limitations in text generation and ineffective mitigation of sensitive information's negative impacts.

Method: Introduces CDEA, which leverages sensitive information properties and character-level statistical properties to enhance high-probability word selection, combined with XLNet for long sequences.

Result: The combination of CDEA and XLNet significantly improves steganographic text quality, especially perceptual-imperceptibility.

Conclusion: The proposed approach effectively addresses the challenges in generative linguistic steganography by improving text coherence and fluency.

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [11] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang, Shengjie Ma, Chengjin Xu, Cehao Yang, Liyu Zhang, Jian Guo*

Main category: cs.CL

TL;DR: SoG is a synthetic data generation framework that enhances LLM training by incorporating cross-document knowledge associations, improving diversity and coherence.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with data inefficiency in specialized corpora, and existing methods lack cross-document knowledge associations.

Method: SoG constructs a context graph for cross-document associations and uses graph walks for sampling, integrating CoT and CC for quality.

Result: SoG outperforms SOTA in multi-hop Q&A and matches SOTA in reading comprehension, showing better generalization.

Conclusion: SoG advances synthetic data generation, offering efficient knowledge acquisition for LLMs in data-limited domains.

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>


### [12] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta, Yash Goel, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: Advocates for downscaling LLMs over neural scaling laws to address inefficiency, environmental impact, and deployment issues.


<details>
  <summary>Details</summary>
Motivation: Challenges the focus on scaling laws due to computational inefficiency, environmental concerns, and deployment constraints.

Method: Proposes a holistic framework for downscaling LLMs to reduce resource demands while maintaining performance.

Result: Introduces practical strategies for sustainable and efficient LLM development.

Conclusion: Calls for a paradigm shift toward downscaling for more accessible and sustainable LLM development.

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>


### [13] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun, Liangbin Zhao, Ming Deng, Xiuju Fu*

Main category: cs.CL

TL;DR: VTS-LLM Agent, a domain-adaptive LLM, enhances VTS operations by combining structured vessel data with maritime knowledge for risk-prone vessel identification, outperforming baselines in diverse linguistic styles.


<details>
  <summary>Details</summary>
Motivation: Existing VTS systems struggle with spatiotemporal reasoning and intuitive interaction due to increasing traffic complexity and heterogeneous data.

Method: Formalizes risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, using NER-based reasoning, domain knowledge injection, and semantic algebra.

Result: VTS-LLM outperforms general-purpose and SQL-focused baselines across command-style, operational-style, and formal natural language queries.

Conclusion: The work establishes a foundation for natural language interfaces in VTS and highlights linguistic style challenges in Text-to-SQL modeling.

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>


### [14] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani, Maitreya Sonawane, Kanika Agarwal, Nishanth Sanjeev*

Main category: cs.CL

TL;DR: Token-free models (ByT5, CANINE) outperform token-based models in sarcasm detection, showing promise for noisy NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges like vocabulary mismatch and out-of-vocabulary issues in NLP tokenization by exploring token-free models.

Method: Evaluate ByT5 and CANINE on sarcasm detection in social media (Twitter) and news headlines, comparing them to token-based models.

Result: ByT5-small and CANINE achieve state-of-the-art performance, improving accuracy by 0.77% (news) and 0.49% (Twitter).

Conclusion: Token-free models are effective for robust NLP in noisy, informal domains like social media.

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>


### [15] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces the Value Portrait benchmark to evaluate LLMs' value orientations, addressing biases in existing benchmarks by using real-life interactions and psychometric validation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs are biased and lack real-world relevance, necessitating a more authentic and reliable evaluation framework.

Method: The Value Portrait benchmark uses real-life user-LLM interactions and psychometric validation via human ratings correlated with value scores.

Result: Evaluation of 27 LLMs shows prioritization of Benevolence, Security, and Self-Direction values, with biases in demographic perceptions.

Conclusion: The benchmark provides a reliable, ecologically valid tool for assessing LLM values, revealing biases and value priorities.

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>


### [16] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TL;DR: Simplified rubrics in LLM-based AES maintain scoring accuracy for most models while reducing token usage, though performance varies by model.


<details>
  <summary>Details</summary>
Motivation: To determine if detailed rubrics are necessary for accurate automated essay scoring (AES) using LLMs, given the effort and token costs involved.

Method: Compared scoring accuracy of four LLMs (Claude 3.5 Haiku, Gemini 1.5 Flash, GPT-4o-mini, Llama 3 70B Instruct) under three rubric conditions: full, simplified, and no rubric, using the TOEFL11 dataset.

Result: Three models maintained accuracy with simplified rubrics, reducing token usage. Gemini 1.5 Flash performed worse with detailed rubrics.

Conclusion: Simplified rubrics are efficient for most LLM-based AES, but model-specific evaluation is essential due to varying performance.

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>


### [17] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin, Junjie Peng, Xuanchao Lin, Haochen Yuan, Lan Wang, Cangzhi Zheng*

Main category: cs.CL

TL;DR: The paper introduces GsiT, a Graph-Structured and Interlaced-Masked Multimodal Transformer, which optimizes efficiency in Multimodal Sentiment Analysis by reducing parameters and improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency concerns of Multimodal Transformers (MulTs) in Multimodal Sentiment Analysis (MSA) by proposing a hierarchical modal-wise heterogeneous graph (HMHG) representation.

Method: Proposes GsiT, leveraging an Interlaced Mask (IM) mechanism for efficient weight-sharing and All-Modal-In-One fusion, with a Triton kernel for computational efficiency.

Result: GsiT achieves higher performance than traditional MulTs with only 1/3 of the parameters and integrates successfully into state-of-the-art models.

Conclusion: GsiT and the HMHG concept significantly improve efficiency and performance in MSA, validated by experiments on standard datasets.

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>


### [18] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed, Wenbo, Liu yunfeng*

Main category: cs.CL

TL;DR: MateICL addresses attention dispersion in large-scale ICL by splitting context into windows and recalibrating attention weights, improving performance without external retrieval models.


<details>
  <summary>Details</summary>
Motivation: Fixed position length constraints in LLMs limit demonstration examples, and existing methods suffer from attention dispersion as context grows.

Method: Split context into windows processed separately, then recalibrate attention weights to prioritize query tokens.

Result: MateICL effectively leverages larger contexts, outperforming retrieval-based baselines without external models.

Conclusion: MateICL remains beneficial in resource-constrained settings, offering a scalable solution for ICL.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>


### [19] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan, Kokil Jaidka, Gerard Christopher Yeo*

Main category: cs.CL

TL;DR: Steering vectors show promise for aligning language models but have limitations, especially in complex scenarios. The paper evaluates their effectiveness using transformer hook interventions and antonym-based function vectors.


<details>
  <summary>Details</summary>
Motivation: To assess the limitations of steering vectors as alignment mechanisms for language models, particularly in complex contexts.

Method: Uses transformer hook interventions and antonym-based function vectors to evaluate steering effectiveness, focusing on prompt structure and context complexity.

Result: Steering vectors work well for specific tasks like value alignment but are not robust for general-purpose alignment in LLMs, especially in complex scenarios.

Conclusion: The study provides a methodological foundation for future research on steering capabilities in reasoning models, highlighting the need for more robust alignment solutions.

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>


### [20] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini, Ege Erdogan, Nils Feldhus, Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper highlights gender disparities in widely used post-hoc feature attribution methods across tasks and models, emphasizing the need for fairness in explanations.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked fairness of explanation methods, particularly disparities in performance across subgroups like gender.

Method: Analyzed three tasks and five language models, evaluating faithfulness, robustness, and complexity of post-hoc feature attribution methods.

Result: Found significant gender disparities in explanation methods, even with unbiased training data, indicating inherent biases.

Conclusion: Urges incorporating fairness of explanations into regulatory frameworks to prevent biased outcomes in high-stakes contexts.

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>


### [21] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas, Laura Diosan, Andrei Piscoran, Andreea Tomescu*

Main category: cs.CL

TL;DR: TF1-EN-3M is a 3M English fable dataset generated by ≤8B-parameter models, structured for moral lessons, evaluated via hybrid metrics, and released openly for research.


<details>
  <summary>Details</summary>
Motivation: Modern NLP lacks a large, structured corpus of moral stories, hindering research in narrative intelligence and value alignment.

Method: Used a combinatorial prompt engine for genre fidelity, evaluated with GPT-based critic and reference-free metrics, and optimized for quality-speed trade-off.

Result: An 8B Llama-3 variant produced high-quality fables efficiently (13.5 cents/1K fables) on consumer GPUs.

Conclusion: TF1-EN-3M enables scalable moral storytelling without proprietary models, fostering research in narrative AI and education.

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [22] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini, Kafaite Zahra Hussain, Efstratios Zaradoukas, Gjergji Kasneci*

Main category: cs.CL

TL;DR: EvalxNLP is a Python framework for benchmarking feature attribution methods in NLP, integrating eight XAI techniques to evaluate explanations based on faithfulness, plausibility, and complexity. It includes interactive, LLM-based explanations and shows high user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable NLP models in high-stakes applications and diverse stakeholder requirements drives the development of frameworks like EvalxNLP to tailor explanations for specific use cases.

Method: EvalxNLP integrates eight explainability techniques from XAI literature, enabling users to generate and evaluate explanations. It also provides interactive, LLM-based textual explanations.

Result: Human evaluation shows high user satisfaction, indicating EvalxNLP is effective for benchmarking explanation methods across diverse user groups.

Conclusion: EvalxNLP democratizes explainability tools, supports systematic XAI comparison, and advances NLP interpretability.

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>


### [23] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li, Zhe Yin, Xiaodong Gu, Beijun Shen*

Main category: cs.CL

TL;DR: PromptObfus is a method for desensitizing LLM prompts using anti-adversarial learning to obscure sensitive information while maintaining model prediction stability.


<details>
  <summary>Details</summary>
Motivation: Privacy in user prompts is critical due to risks of exposing sensitive data to cloud LLMs, with traditional methods being computationally costly or impractical.

Method: PromptObfus treats prompt desensitization as a masked language modeling task, replacing sensitive terms with [MASK] tokens and training a model to generate replacements, selected via gradient feedback.

Result: The method effectively prevents privacy inference in remote LLMs while preserving task performance across three NLP tasks.

Conclusion: PromptObfus offers a practical solution for privacy-preserving LLM prompts, balancing privacy and utility.

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>


### [24] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg, Jörg Deigmöller, Julian Eggert, Philipp Cimiano*

Main category: cs.CL

TL;DR: A factorized model captures vague temporal adverbials as probabilistic distributions, outperforming a non-factorized Gaussian model in simplicity and extendability.


<details>
  <summary>Details</summary>
Motivation: To better understand and model the semantics of vague temporal adverbials like 'recently' or 'a long time ago,' which lack exact durations.

Method: A factorized model is introduced, combining probabilistic distributions of adverbials with event-specific distributions. Parameters are fitted using native speaker judgments.

Result: The factorized model matches the predictive power of a non-factorized Gaussian model but is simpler and more extendable.

Conclusion: The factorized model is preferable due to its simplicity and better extendability, aligning with Occam's razor.

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>


### [25] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang, Huanrong Tang, Jianquan Ouyang*

Main category: cs.CL

TL;DR: A neural architecture search method using Transformer and multihead attention, optimized with a multi-objective genetic algorithm, outperforms baselines by incorporating perplexity alongside BLEU scores.


<details>
  <summary>Details</summary>
Motivation: To improve neural network structures for better translation results by exploring diverse attention mechanisms and encoder-decoder combinations.

Method: Uses a multi-objective genetic algorithm to iteratively improve neural networks, evaluating with both BLEU scores and perplexity.

Result: Searched architectures outperform baselines; perplexity as an auxiliary metric finds better models than BLEU alone.

Conclusion: Incorporating additional evaluation metrics like perplexity enhances neural architecture search for translation tasks.

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>


### [26] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin, Spyridon Mastorakis*

Main category: cs.CL

TL;DR: A defense framework for LLMs detects and filters adversarial inputs without retraining, using NLP techniques and summarization of adversarial literature, achieving 98.71% success in identifying threats.


<details>
  <summary>Details</summary>
Motivation: The rise in adversarial attacks on LLMs necessitates cost-effective, deployable defenses without retraining.

Method: Combines prompt filtering (NLP techniques like zero-shot classification, keyword analysis, encoded content detection) and summarization of adversarial literature for context-aware defense.

Result: 98.71% success in detecting harmful inputs, improved jailbreak resistance, and refusal rates.

Conclusion: The framework effectively enhances LLM security without retraining, offering a practical alternative to traditional defenses.

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>


### [27] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg, Jörg Deigmöller, Philipp Cimiano, Julian Eggert*

Main category: cs.CL

TL;DR: TRAVELER is a synthetic benchmark dataset for evaluating LLMs' ability to resolve temporal references in QA tasks, showing performance drops with larger event sets and less explicit references.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack systematic evaluation of specific temporal references, prompting the creation of TRAVELER to address this gap.

Method: TRAVELER uses a QA paradigm with 3,300 questions to assess LLMs on explicit, implicit, and vague temporal references, with human surveys for vague answers.

Result: LLMs perform well with explicit references and small event sets but struggle with larger sets and vague references.

Conclusion: TRAVELER highlights LLMs' limitations in temporal reasoning, especially for vague references, and provides a tool for future research.

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>


### [28] [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359)
*Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for scientific summarization, introduces a Facet-aware Metric (FM) for better evaluation, and highlights limitations of LLMs in scientific contexts.


<details>
  <summary>Details</summary>
Motivation: Assess the effectiveness of LLMs in scientific summarization, where traditional evaluation methods fall short.

Method: Introduces FM for semantic matching and curates a Facet-based Dataset (FD) for evaluation.

Result: FM provides a logical evaluation approach; fine-tuned smaller models can compete with LLMs, which struggle with in-context learning in science.

Conclusion: LLMs need enhancement for scientific domains, and FM offers a better evaluation framework.

Abstract: The summarization capabilities of pretrained and large language models (LLMs)
have been widely validated in general areas, but their use in scientific
corpus, which involves complex sentences and specialized knowledge, has been
less assessed. This paper presents conceptual and experimental analyses of
scientific summarization, highlighting the inadequacies of traditional
evaluation methods, such as $n$-gram, embedding comparison, and QA,
particularly in providing explanations, grasping scientific concepts, or
identifying key content. Subsequently, we introduce the Facet-aware Metric
(FM), employing LLMs for advanced semantic matching to evaluate summaries based
on different aspects. This facet-aware approach offers a thorough evaluation of
abstracts by decomposing the evaluation task into simpler subtasks.Recognizing
the absence of an evaluation benchmark in this domain, we curate a Facet-based
scientific summarization Dataset (FD) with facet-level annotations. Our
findings confirm that FM offers a more logical approach to evaluating
scientific summaries. In addition, fine-tuned smaller models can compete with
LLMs in scientific contexts, while LLMs have limitations in learning from
in-context information in scientific domains. This suggests an area for future
enhancement of LLMs.

</details>


### [29] [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://arxiv.org/abs/2404.18624)
*Letitia Parcalabescu, Anette Frank*

Main category: cs.CL

TL;DR: VLMs rely more on text than images, but images are more important for explanations than answers, especially in CoT settings. VLMs are less self-consistent than LLMs and struggle with VALSE benchmarks.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs use vision and text modalities differently when generating answers vs. explanations, and to evaluate their self-consistency.

Method: Extend unimodal tests to VLMs, analyze modality contributions, and benchmark on VALSE.

Result: Text dominates image contributions, but images are more impactful for explanations. VLMs are less self-consistent than LLMs and perform poorly on VALSE.

Conclusion: VLMs prioritize text but leverage images more for explanations, especially in CoT. Their self-consistency and performance on benchmarks need improvement.

Abstract: Vision and language model (VLM) decoders are currently the best-performing
architectures on multimodal tasks. Next to answers, they are able to produce
natural language explanations, either in post-hoc or CoT settings. However, it
is not clear to what extent they are using the input vision and text modalities
when generating answers or explanations. In this work, we investigate if VLMs
rely on their input modalities differently when they produce explanations as
opposed to answers. We also evaluate the self-consistency of VLM decoders in
both post-hoc and CoT explanation settings, by extending existing unimodal
tests and measures to VLM decoders. We find that most tested VLMs are less
self-consistent than LLMs. Text contributions in all tested VL decoders are
more important than image contributions in all examined tasks. However, when
comparing explanation generation to answer generation, the contributions of
images are significantly stronger for generating explanations compared to
answers. This difference is even larger in CoT compared to post-hoc
explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art
VL decoders on the VALSE benchmark, which before was restricted to VL encoders.
We find that the tested VL decoders still struggle with most phenomena tested
by VALSE.

</details>


### [30] [Does Self-Attention Need Separate Weights in Transformers?](https://arxiv.org/abs/2412.00359)
*Md Kowsher, Nusrat Jahan Prottasha, Chun-Nam Yu, Ozlem Ozmen Garibay, Niloofar Yousefi*

Main category: cs.CL

TL;DR: A shared weight self-attention-based BERT model reduces computational complexity and improves efficiency while maintaining or enhancing performance on tasks like GLUE.


<details>
  <summary>Details</summary>
Motivation: Address limitations of self-attention (computational complexity, handling sequential data directionality) by simplifying the attention mechanism.

Method: Introduces a shared weight self-attention model using one weight matrix for Key, Value, and Query, reducing parameters and training time.

Result: Achieves 66.53% parameter reduction, faster training, and improved accuracy on GLUE tasks, especially with noisy/out-of-domain data.

Conclusion: Shared weight self-attention is efficient and effective, offering a practical alternative to traditional BERT models.

Abstract: The success of self-attention lies in its ability to capture long-range
dependencies and enhance context understanding, but it is limited by its
computational complexity and challenges in handling sequential data with
inherent directionality. This work introduces a shared weight
self-attention-based BERT model that only learns one weight matrix for (Key,
Value, and Query) representations instead of three individual matrices for each
of them. Our shared weight attention reduces the training parameter size by
more than half and training time by around one-tenth. Furthermore, we
demonstrate higher prediction accuracy on small tasks of GLUE over the BERT
baseline and in particular a generalization power on noisy and out-of-domain
data. Experimental results indicate that our shared self-attention method
achieves a parameter size reduction of 66.53% in the attention block. In the
GLUE dataset, the shared weight self-attention-based BERT model demonstrates
accuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric,
and pairwise attention-based BERT models, respectively. The model and source
code are available at Anonymous.

</details>


### [31] [When Every Token Counts: Optimal Segmentation for Low-Resource Language Models](https://arxiv.org/abs/2412.06926)
*Bharath Raj, Garvit Suri, Vikrant Dewangan, Raghav Sonavane*

Main category: cs.CL

TL;DR: Optimal BPE tokenization reduces token count and improves performance, especially for smaller models, with benefits for multilingual and low-resource applications.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimality of traditional greedy tokenization methods like BPE across model scales and languages.

Method: Extensive experiments evaluating tokenization performance via intrinsic and extrinsic tasks, including generation and classification.

Result: Optimal BPE configuration significantly reduces token count and improves performance, particularly for smaller models.

Conclusion: Compression-optimized tokenization strategies offer advantages for multilingual and low-resource NLP, suggesting a promising research direction.

Abstract: Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.

</details>


### [32] [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)
*Meihao Fan, Ju Fan, Nan Tang, Lei Cao, Guoliang Li, Xiaoyong Du*

Main category: cs.CL

TL;DR: AutoPrep is an LLM-based multi-agent framework for Tabular Question Answering, using specialized agents for data prep tasks like column derivation and filtering, ensuring accurate responses.


<details>
  <summary>Details</summary>
Motivation: Traditional data prep methods are insufficient for NL questions over tables, requiring a nuanced, question-aware approach.

Method: AutoPrep uses a multi-agent framework with Planner, Programmer, and Executor components, supported by Chain-of-Clauses reasoning and tool-augmented code generation.

Result: The framework enables more accurate and contextually relevant responses to NL questions over tables.

Conclusion: AutoPrep addresses the limitations of single-model approaches by leveraging specialized agents for diverse data prep tasks.

Abstract: Answering natural language (NL) questions about tables, known as Tabular
Question Answering (TQA), is crucial because it allows users to quickly and
efficiently extract meaningful insights from structured data, effectively
bridging the gap between human language and machine-readable formats. Many of
these tables are derived from web sources or real-world scenarios, which
require meticulous data preparation (or data prep) to ensure accurate
responses. However, preparing such tables for NL questions introduces new
requirements that extend beyond traditional data preparation. This
question-aware data preparation involves specific tasks such as column
derivation and filtering tailored to particular questions, as well as
question-aware value normalization or conversion, highlighting the need for a
more nuanced approach in this context. Because each of the above tasks is
unique, a single model (or agent) may not perform effectively across all
scenarios. In this paper, we propose AutoPrep, a large language model
(LLM)-based multi-agent framework that leverages the strengths of multiple
agents, each specialized in a certain type of data prep, ensuring more accurate
and contextually relevant responses. Given an NL question over a table,
AutoPrep performs data prep through three key components. Planner: Determines a
logical plan, outlining a sequence of high-level operations. Programmer:
Translates this logical plan into a physical plan by generating the
corresponding low-level code. Executor: Executes the generated code to process
the table. To support this multi-agent framework, we design a novel
Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a
tool-augmented method for low-level code generation...

</details>


### [33] [ICLR: In-Context Learning of Representations](https://arxiv.org/abs/2501.00070)
*Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, Hidenori Tanaka*

Main category: cs.CL

TL;DR: The paper investigates whether large language models (LLMs) can reorganize their pretrained semantic representations to align with context-specified semantics, using a graph tracing task.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can flexibly adapt their representations to novel, context-driven semantics, despite being influenced by pretraining data.

Method: A toy graph tracing task is designed, where nodes are training concepts (e.g., apple, bird) and connectivity follows a predefined structure (e.g., square grid). Random walk exemplars are provided, and model representations are analyzed.

Result: Increasing context size triggers a sudden shift from pretrained to context-aligned representations. Pretrained semantics persist when reference concepts are correlated (e.g., days of the week).

Conclusion: Scaling context size can flexibly reorganize LLM representations, suggesting an implicit optimization process for context-specified semantics and potential for novel capabilities.

Abstract: Recent work has demonstrated that semantics specified by pretraining data
influence how representations of different concepts are organized in a large
language model (LLM). However, given the open-ended nature of LLMs, e.g., their
ability to in-context learn, we can ask whether models alter these pretraining
semantics to adopt alternative, context-specified ones. Specifically, if we
provide in-context exemplars wherein a concept plays a different role than what
the pretraining data suggests, do models reorganize their representations in
accordance with these novel semantics? To answer this question, we take
inspiration from the theory of conceptual role semantics and define a toy
"graph tracing" task wherein the nodes of the graph are referenced via concepts
seen during training (e.g., apple, bird, etc.) and the connectivity of the
graph is defined via some predefined structure (e.g., a square grid). Given
exemplars that indicate traces of random walks on the graph, we analyze
intermediate representations of the model and find that as the amount of
context is scaled, there is a sudden re-organization from pretrained semantic
representations to in-context representations aligned with the graph structure.
Further, we find that when reference concepts have correlations in their
semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure
is still present in the representations, but is unable to dominate the
pretrained structure. To explain these results, we analogize our task to energy
minimization for a predefined graph topology, providing evidence towards an
implicit optimization process to infer context-specified semantics. Overall,
our findings indicate scaling context-size can flexibly re-organize model
representations, possibly unlocking novel capabilities.

</details>


### [34] [Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention](https://arxiv.org/abs/2501.06382)
*Mumin Jia, Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: The paper explores spontaneous topic changes in self-attention models, comparing them to human cognition, and reveals key differences in behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how self-attention models handle spontaneous topic shifts, contrasting them with human spontaneous thought.

Method: Theoretical analysis under a simplified single-layer self-attention model with Token Priority Graphs (TPGs), followed by empirical validation in modern LLMs.

Result: Self-attention models maintain token priority order, require specific conditions for topic changes, and show reduced spontaneity with longer contexts or ambiguous inputs.

Conclusion: Self-attention models fundamentally diverge from human cognition in handling spontaneous topic changes, highlighting a key AI limitation.

Abstract: Human cognition is punctuated by abrupt, spontaneous shifts between
topics-driven by emotional, contextual, or associative cues-a phenomenon known
as spontaneous thought in neuroscience. In contrast, self-attention based
models depend on structured patterns over their inputs to predict each next
token, lacking spontaneity. Motivated by this distinction, we characterize
spontaneous topic changes in self-attention architectures, revealing both their
similarities and their divergences from spontaneous human thought. First, we
establish theoretical results under a simplified, single-layer self-attention
model with suitable conditions by defining the topic as a set of Token Priority
Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the
priority order of tokens related to the input topic, (2) a spontaneous topic
change can occur only if lower-priority tokens outnumber all higher-priority
tokens of the input topic, and (3) unlike human cognition, the longer context
length or the more ambiguous input topic reduces the likelihood of spontaneous
change. Second, we empirically validate that these dynamics persist in modern,
state-of-the-art LLMs, underscoring a fundamental disparity between human
cognition and AI behaviour in the context of spontaneous topic changes. To the
best of our knowledge, no prior work has explored these questions with a focus
as closely aligned to human thought.

</details>


### [35] [TableMaster: A Recipe to Advance Table Understanding with Language Models](https://arxiv.org/abs/2501.19378)
*Lang Cao, Hanbing Liu*

Main category: cs.CL

TL;DR: TableMaster enhances language models for table understanding by addressing key challenges like data location, semantics, numerical accuracy, and reasoning flexibility, achieving 78.13% accuracy on WikiTQ.


<details>
  <summary>Details</summary>
Motivation: Current language models struggle with table understanding due to the structured nature of tabular data. The paper aims to improve LMs' ability to handle tables by tackling four major challenges.

Method: Proposes TableMaster, a framework that extracts and verbalizes table content with enriched semantics and introduces adaptive reasoning for dynamic adjustment between textual and symbolic reasoning.

Result: TableMaster achieves 78.13% accuracy on the WikiTQ dataset, outperforming existing baselines.

Conclusion: TableMaster effectively addresses key challenges in table understanding, demonstrating superior performance and flexibility in reasoning.

Abstract: Tables serve as a fundamental format for representing structured relational
data. While current language models (LMs) excel at many text-based tasks, they
still face challenges in table understanding due to the complex characteristics
of tabular data, such as their structured nature. In this paper, we aim to
enhance LMs for improved table understanding. We identify four key challenges:
1) difficulty in locating target data, 2) deficiency in table semantics, 3)
numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in
symbolic reasoning. To address these issues, we propose TableMaster, a recipe
and comprehensive framework that integrates multiple solutions to overcome
these obstacles. TableMaster first extracts relevant table content and
verbalizes it with enriched semantic context. Additionally, we introduce
adaptive reasoning, a flexible approach that dynamically adjusts between
textual and symbolic reasoning, tailoring the reasoning process to each query.
Extensive analyses and experiments demonstrate our findings and the
effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an
accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.

</details>


### [36] [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/abs/2502.01976)
*Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao*

Main category: cs.CL

TL;DR: CITER is a framework for efficient collaboration between small and large language models via token-level routing, reducing inference costs while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: High computational costs of large language models limit their deployment in resource-constrained applications.

Method: Proposes CITER, a token-level routing strategy where non-critical tokens go to small models and critical tokens to large models, trained via policy optimization with rewards for quality and cost.

Result: CITER reduces inference costs while preserving generation quality, validated on five benchmark datasets.

Conclusion: CITER offers a practical solution for real-time, resource-constrained applications.

Abstract: Large language models have achieved remarkable success in various tasks but
suffer from high computational costs during inference, limiting their
deployment in resource-constrained applications. To address this issue, we
propose a novel Collaborative Inference with Token-lEvel Routing (CITER)
framework that enables efficient collaboration between small and large language
models (SLMs \& LLMs) through a token-level routing strategy. Specifically,
CITER routes non-critical tokens to an SLM for efficiency and routes critical
tokens to an LLM for generalization quality. We formulate router training as a
policy optimization, where the router receives rewards based on both the
quality of predictions and the inference costs of generation. This allows the
router to learn to predict token-level routing scores and make routing
decisions based on both the current token and the future impact of its
decisions. To further accelerate the reward evaluation process, we introduce a
shortcut which significantly reduces the costs of the reward estimation and
improving the practicality of our approach. Extensive experiments on five
benchmark datasets demonstrate that CITER reduces the inference costs while
preserving high-quality generation, offering a promising solution for real-time
and resource-constrained applications. Our data and code are available at
https://github.com/aiming-lab/CITER.

</details>


### [37] [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
*Sudesh Ramesh Bhagat, Ibne Farabi Shihab, Anuj Sharma*

Main category: cs.CL

TL;DR: The study explores the link between DL model accuracy and expert agreement in crash narrative classification, finding higher accuracy models often disagree with experts, while LLMs align better despite lower accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand the disconnect between technical accuracy and expert agreement in safety-critical NLP tasks like crash analysis.

Method: Evaluated five DL models and four LLMs against expert labels, using Cohen's Kappa, PCA, and SHAP analysis to measure and explain agreement.

Result: Higher accuracy models showed lower expert agreement, while LLMs aligned better with experts, relying on contextual cues over keywords.

Conclusion: Accuracy alone is inadequate for safety-critical tasks; expert agreement should be part of model evaluation, with LLMs offering interpretable solutions.

Abstract: This study investigates the relationship between deep learning (DL) model
accuracy and expert agreement in classifying crash narratives. We evaluate five
DL models -- including BERT variants, USE, and a zero-shot classifier --
against expert labels and narratives, and extend the analysis to four large
language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal
an inverse relationship: models with higher technical accuracy often show lower
agreement with human experts, while LLMs demonstrate stronger expert alignment
despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis
(PCA) to quantify and visualize model-expert agreement, and employ SHAP
analysis to explain misclassifications. Results show that expert-aligned models
rely more on contextual and temporal cues than location-specific keywords.
These findings suggest that accuracy alone is insufficient for safety-critical
NLP tasks. We argue for incorporating expert agreement into model evaluation
frameworks and highlight the potential of LLMs as interpretable tools in crash
analysis pipelines.

</details>


### [38] [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
*Josefa Lia Stoisser, Marc Boubnovski Martell, Julien Fauqueur*

Main category: cs.CL

TL;DR: The paper reframes Text-to-SQL as a way to teach LLMs table reasoning, using a two-stage framework with CoT traces and GRPO reinforcement learning, showing improved performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To move beyond query generation in Text-to-SQL by teaching LLMs transferable table reasoning skills.

Method: A two-stage framework: (1) synthesizing CoT traces from SQL queries for clause-level supervision, (2) GRPO reinforcement learning to generalize reasoning beyond task-specific syntax.

Result: Improved accuracy on Text-to-SQL benchmarks (33.9% for LLaMA, 14.5% for Qwen) and gains on reasoning-intensive datasets like BIRD and CRT-QA.

Conclusion: SQL can scaffold robust, transferable reasoning over structured data, not just serve as a target formalism.

Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large
language models (LLMs) to reason over and manipulate tabular data--moving
beyond the traditional focus on query generation. We propose a two-stage
framework that leverages SQL supervision to develop transferable table
reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)
traces from real-world SQL queries, providing step-by-step, clause-level
supervision that teaches the model how to traverse, filter, and aggregate table
fields. Second, we introduce a Group Relative Policy Optimization (GRPO)
reinforcement learning objective that connects SQL execution accuracy to
generalizable reasoning by encouraging steps that extend beyond task-specific
syntax and transfer across datasets. Empirically, our approach improves
performance on standard Text-to-SQL benchmarks and achieves substantial gains
on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced
generalization and interpretability. Specifically, the distilled-quantized
LLaMA model achieved a relative 33.9\% increase in accuracy when trained on
Text-to-SQL tasks, while Qwen achieved a relative 14.5\% increase. These
results suggest that SQL can serve not only as a target formalism but also as
an effective scaffold for learning robust, transferable reasoning over
structured data.

</details>


### [39] [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
*Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing*

Main category: cs.CL

TL;DR: The paper summarizes replication studies of DeepSeek-R1, focusing on SFT and RLVR methods, data construction, and training procedures, aiming to inspire future research in reasoning language models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-source details for DeepSeek-R1 models and explore feasible replication strategies to achieve comparable performance.

Method: Summarizes recent replication studies, focusing on supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), including data preparation and method design.

Result: Provides key findings from replication studies, highlighting insights and challenges in enhancing reasoning language models.

Conclusion: The survey aims to keep researchers updated and inspire new ideas to further improve reasoning language models, despite existing challenges.

Abstract: The recent development of reasoning language models (RLMs) represents a novel
evolution in large language models. In particular, the recent release of
DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in
the research community for exploring the explicit reasoning paradigm of
language models. However, the implementation details of the released models
have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,
DeepSeek-R1, and the distilled small models. As a result, many replication
studies have emerged aiming to reproduce the strong performance achieved by
DeepSeek-R1, reaching comparable performance through similar training
procedures and fully open-source data resources. These works have investigated
feasible strategies for supervised fine-tuning (SFT) and reinforcement learning
from verifiable rewards (RLVR), focusing on data preparation and method design,
yielding various valuable insights. In this report, we provide a summary of
recent replication studies to inspire future research. We primarily focus on
SFT and RLVR as two main directions, introducing the details for data
construction, method design and training procedure of current replication
studies. Moreover, we conclude key findings from the implementation details and
experimental results reported by these studies, anticipating to inspire future
research. We also discuss additional techniques of enhancing RLMs, highlighting
the potential of expanding the application scope of these models, and
discussing the challenges in development. By this survey, we aim to help
researchers and developers of RLMs stay updated with the latest advancements,
and seek to inspire new ideas to further enhance RLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes](https://arxiv.org/abs/2505.00734)
*Neil Joshi, Joshua Carney, Nathanael Kuo, Homer Li, Cheng Peng, Myron Brown*

Main category: cs.CV

TL;DR: A benchmark dataset for 3D reconstruction and novel view synthesis is introduced to address challenges like limited images, unposed cameras, and extreme viewpoints, aiding disaster relief and law enforcement.


<details>
  <summary>Details</summary>
Motivation: To enable research on 3D reconstruction and novel view synthesis under real-world constraints (e.g., limited images, unposed cameras, inconsistent lighting) for applications like disaster relief.

Method: Developed a public benchmark dataset with calibrated ground-level, security-level, and airborne cameras, evaluating calibration and novel view synthesis quality.

Result: Baseline performance using state-of-practice methods is demonstrated, highlighting challenges for future research.

Conclusion: The dataset fosters research to overcome real-world challenges in 3D reconstruction and novel view synthesis.

Abstract: Production of photorealistic, navigable 3D site models requires a large
volume of carefully collected images that are often unavailable to first
responders for disaster relief or law enforcement. Real-world challenges
include limited numbers of images, heterogeneous unposed cameras, inconsistent
lighting, and extreme viewpoint differences for images collected from varying
altitudes. To promote research aimed at addressing these challenges, we have
developed the first public benchmark dataset for 3D reconstruction and novel
view synthesis based on multiple calibrated ground-level, security-level, and
airborne cameras. We present datasets that pose real-world challenges,
independently evaluate calibration of unposed cameras and quality of novel
rendered views, demonstrate baseline performance using recent state-of-practice
methods, and identify challenges for further research.

</details>


### [41] [MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection](https://arxiv.org/abs/2505.00739)
*Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo*

Main category: cs.CV

TL;DR: MoSAM enhances SAM2 by integrating motion cues and reliable memory selection to improve video object segmentation.


<details>
  <summary>Details</summary>
Motivation: SAM2's reliance on fixed past frames and lack of motion information limits its tracking and segmentation accuracy in videos.

Method: Introduces Motion-Guided Prompting (MGP) for motion cues and Spatial-Temporal Memory Selection (ST-MS) for reliable memory.

Result: MoSAM achieves state-of-the-art performance in video object and instance segmentation benchmarks.

Conclusion: MoSAM effectively addresses SAM2's limitations, improving segmentation and tracking in videos.

Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional
capabilities in interactive object segmentation for both images and videos.
However, as a foundational model on interactive segmentation, SAM2 performs
segmentation directly based on mask memory from the past six frames, leading to
two significant challenges. Firstly, during inference in videos, objects may
disappear since SAM2 relies solely on memory without accounting for object
motion information, which limits its long-range object tracking capabilities.
Secondly, its memory is constructed from fixed past frames, making it
susceptible to challenges associated with object disappearance or occlusion,
due to potentially inaccurate segmentation results in memory. To address these
problems, we present MoSAM, incorporating two key strategies to integrate
object motion cues into the model and establish more reliable feature memory.
Firstly, we propose Motion-Guided Prompting (MGP), which represents the object
motion in both sparse and dense manners, then injects them into SAM2 through a
set of motion-guided prompts. MGP enables the model to adjust its focus towards
the direction of motion, thereby enhancing the object tracking capabilities.
Furthermore, acknowledging that past segmentation results may be inaccurate, we
devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically
identifies frames likely to contain accurate segmentation in both pixel- and
frame-level. By eliminating potentially inaccurate mask predictions from
memory, we can leverage more reliable memory features to exploit similar
regions for improving segmentation results. Extensive experiments on various
benchmarks of video object segmentation and video instance segmentation
demonstrate that our MoSAM achieves state-of-the-art results compared to other
competitors.

</details>


### [42] [Fast2comm:Collaborative perception combined with prior knowledge](https://arxiv.org/abs/2505.00740)
*Zhengbin Zhang, Yan Wu, Hongkun Zhang*

Main category: cs.CV

TL;DR: Fast2comm is a collaborative perception framework using prior knowledge to balance performance and bandwidth, addressing localization errors.


<details>
  <summary>Details</summary>
Motivation: Real-world collaborative perception struggles with balancing performance, bandwidth, and localization errors.

Method: Proposes confidence feature generation, spatial prior feature selection, and decoupled feature fusion for dynamic bandwidth adaptation.

Result: Superior performance validated on real-world and simulated datasets.

Conclusion: Fast2comm effectively enhances perception accuracy and bandwidth efficiency.

Abstract: Collaborative perception has the potential to significantly enhance
perceptual accuracy through the sharing of complementary information among
agents. However, real-world collaborative perception faces persistent
challenges, particularly in balancing perception performance and bandwidth
limitations, as well as coping with localization errors. To address these
challenges, we propose Fast2comm, a prior knowledge-based collaborative
perception framework. Specifically, (1)we propose a prior-supervised confidence
feature generation method, that effectively distinguishes foreground from
background by producing highly discriminative confidence features; (2)we
propose GT Bounding Box-based spatial prior feature selection strategy to
ensure that only the most informative prior-knowledge features are selected and
shared, thereby minimizing background noise and optimizing bandwidth efficiency
while enhancing adaptability to localization inaccuracies; (3)we decouple the
feature fusion strategies between model training and testing phases, enabling
dynamic bandwidth adaptation. To comprehensively validate our framework, we
conduct extensive experiments on both real-world and simulated datasets. The
results demonstrate the superior performance of our model and highlight the
necessity of the proposed methods. Our code is available at
https://github.com/Zhangzhengbin-TJ/Fast2comm.

</details>


### [43] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala, Sneha Ningappa*

Main category: cs.CV

TL;DR: The study uses CNN and LSTM models to classify plant leaf diseases, achieving high accuracy (96.4% for CNN, 93.43% for LSTM), demonstrating their effectiveness for scalable agricultural monitoring.


<details>
  <summary>Details</summary>
Motivation: Early detection of plant diseases is crucial to minimize crop losses and improve management practices.

Method: CNN and LSTM models were trained on a dataset of 70,295 images (38 disease classes) using Adam optimiser and categorical cross-entropy loss.

Result: CNN achieved 99.1% training and 96.4% validation accuracy; LSTM reached 93.43% validation accuracy.

Conclusion: Deep learning, especially CNN, provides an accurate and scalable solution for plant disease classification.

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>


### [44] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian, Chendong Wang, Yifan Yang, Chaoyun Zhang, Huiqiang Jiang, Xufang Luo, Yu Kang, Qingwei Lin, Anlan Zhang, Shiqi Jiang, Ting Cao, Tianjun Mao, Suman Banerjee, Guyue Liu, Saravan Rajmohan, Dongmei Zhang, Yuqing Yang, Qi Zhang, Lili Qiu*

Main category: cs.CV

TL;DR: A novel visual prompting mechanism, \SysName, improves multimodal large language models (MLLMs) by enhancing visual data processing within token limits, achieving up to 26.9% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with precise object recognition and fine visual details due to token limits, leading to omitted critical information.

Method: \SysName introduces a prompt-aware strategy, spatial-preserving orchestration, and budget-aware prompting to balance context and details.

Result: \SysName outperforms baselines, improving accuracy by up to 26.9% and reducing token consumption.

Conclusion: \SysName effectively addresses MLLM limitations in visual tasks, offering a robust solution for preserving essential details.

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>


### [45] [DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation](https://arxiv.org/abs/2505.00743)
*Yinfeng Yu, Dongsheng Yang*

Main category: cs.CV

TL;DR: The paper proposes DOPE, a network to enhance VLN by improving language instruction understanding and cross-modal object relationship modeling.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods lack detailed language instruction exploitation and cross-modal object relationship modeling, limiting navigation performance.

Method: DOPE includes Text Semantic Extraction (TSE) and Image Object Perception-Augmentation (IOPA) to better utilize instruction details and object relationships.

Result: Experiments on R2R and REVERIE datasets show improved navigation performance.

Conclusion: DOPE effectively addresses current VLN limitations, enhancing language understanding and decision-making accuracy.

Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent
must understand language instructions and navigate unfamiliar environments
using visual cues. The agent must accurately locate the target based on visual
information from the environment and complete tasks through interaction with
the surroundings. Despite significant advancements in this field, two major
limitations persist: (1) Many existing methods input complete language
instructions directly into multi-layer Transformer networks without fully
exploiting the detailed information within the instructions, thereby limiting
the agent's language understanding capabilities during task execution; (2)
Current approaches often overlook the modeling of object relationships across
different modalities, failing to effectively utilize latent clues between
objects, which affects the accuracy and robustness of navigation decisions. We
propose a Dual Object Perception-Enhancement Network (DOPE) to address these
issues to improve navigation performance. First, we design a Text Semantic
Extraction (TSE) to extract relatively essential phrases from the text and
input them into the Text Object Perception-Augmentation (TOPA) to fully
leverage details such as objects and actions within the instructions. Second,
we introduce an Image Object Perception-Augmentation (IOPA), which performs
additional modeling of object information across different modalities, enabling
the model to more effectively utilize latent clues between objects in images
and text, enhancing decision-making accuracy. Extensive experiments on the R2R
and REVERIE datasets validate the efficacy of the proposed approach.

</details>


### [46] [Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering](https://arxiv.org/abs/2505.00744)
*Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: The paper introduces HEAL-MedVQA, a benchmark to evaluate medical LMMs' localization abilities and hallucination robustness, and proposes the LobA framework to improve visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Current medical LMMs often generate hallucinations due to inadequate localization reasoning, relying on linguistic patterns or irrelevant image areas.

Method: The authors develop HEAL-MedVQA with two evaluation protocols and a dataset of 67K VQA pairs. They propose the LobA framework to localize and emphasize pathological regions.

Result: The LobA framework outperforms state-of-the-art biomedical LMMs on the HEAL-MedVQA benchmark.

Conclusion: The approach advances robustness in medical VQA by addressing localization and hallucination issues.

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>


### [47] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao, Shengzhong Liu, Fan Wu, Guihai Chen*

Main category: cs.CV

TL;DR: MOCHA is a framework for mobile video analysis that improves responsiveness and accuracy in model adaptation by leveraging hierarchical mobile-cloud collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing cloud-centric model adaptation frameworks suffer from performance degradation and delayed reactions to environment shifts in mobile video analysis.

Method: MOCHA optimizes responsiveness through on-device model reuse, fast fine-tuning, structured taxonomy for model retrieval, and proactive caching of expert models.

Result: MOCHA improves model accuracy by up to 6.8%, reduces response delay by 35.5x, and cuts retraining time by 3.0x.

Conclusion: MOCHA effectively addresses the limitations of cloud-centric frameworks, enhancing mobile video analysis performance.

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>


### [48] [Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis](https://arxiv.org/abs/2505.00746)
*Alexei Kaltchenko*

Main category: cs.CV

TL;DR: A method using entropy-heat-mapping to identify OCR errors in GPT-4o transcriptions by analyzing token-level confidence signals.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like GPT-4o lack effective use of token-level confidence signals to detect local OCR errors.

Method: Entropy-heat-mapping converts per-token Shannon entropy into a visual uncertainty landscape, scanned with a sliding window to identify error hotspots.

Result: Most true errors in GPT-4o transcriptions are concentrated in high-entropy regions.

Conclusion: Sliding-window entropy is a lightweight, practical tool for post-editing GPT-based OCR, with open resources for replication.

Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical
documents directly from images, yet their token-level confidence signals are
seldom used to pinpoint local recognition mistakes. We present an
entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into
a visual ''uncertainty landscape''. By scanning the entropy sequence with a
fixed-length sliding window, we obtain hotspots that are likely to contain OCR
errors such as missing symbols, mismatched braces, or garbled prose. Using a
small, curated set of scanned research pages rendered at several resolutions,
we compare the highlighted hotspots with the actual transcription errors
produced by GPT-4o. Our analysis shows that the vast majority of true errors
are indeed concentrated inside the high-entropy regions. This study
demonstrates--in a minimally engineered setting--that sliding-window entropy
can serve as a practical, lightweight aid for post-editing GPT-based OCR. All
code, sample data, and annotation guidelines are released to encourage
replication and further research.

</details>


### [49] [InstructAttribute: Fine-grained Object Attributes editing with Instruction](https://arxiv.org/abs/2505.00751)
*Xingxi Yin, Jingfeng Zhang, Zhi Li, Yicheng Li, Yin Zhang*

Main category: cs.CV

TL;DR: The paper introduces SPAA, a training-free method for precise color and material editing in T2I diffusion models, and InstructAttribute, an instruction-based model trained on a new Attribute Dataset.


<details>
  <summary>Details</summary>
Motivation: Existing image editing techniques lack precision in modifying fine-grained attributes while preserving object structure and image consistency.

Method: Proposes SPAA for editing self-attention and cross-attention maps, and constructs an Attribute Dataset using MLLMs for automated labeling. Introduces InstructAttribute for instruction-based editing.

Result: SPAA and InstructAttribute outperform existing methods in fine-grained color and material editing.

Conclusion: The proposed methods enable precise, structure-preserving attribute editing, advancing T2I model capabilities.

Abstract: Text-to-image (T2I) diffusion models, renowned for their advanced generative
abilities, are extensively utilized in image editing applications,
demonstrating remarkable effectiveness. However, achieving precise control over
fine-grained attributes still presents considerable challenges. Existing image
editing techniques either fail to modify the attributes of an object or
struggle to preserve its structure and maintain consistency in other areas of
the image. To address these challenges, we propose the Structure-Preserving and
Attribute Amplification (SPAA), a training-free method which enables precise
control over the color and material transformations of objects by editing the
self-attention maps and cross-attention values. Furthermore, we constructed the
Attribute Dataset, which encompasses nearly all colors and materials associated
with various objects, by integrating multimodal large language models (MLLM) to
develop an automated pipeline for data filtering and instruction labeling.
Training on this dataset, we present our InstructAttribute, an
instruction-based model designed to facilitate fine-grained editing of color
and material attributes. Extensive experiments demonstrate that our method
achieves superior performance in object-level color and material editing,
outperforming existing instruction-based image editing approaches.

</details>


### [50] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li, Xuchen Li, Shiyu Hu*

Main category: cs.CV

TL;DR: DARTer is an end-to-end tracking framework for nighttime UAV scenarios, using Dynamic Feature Blender and Activator to improve robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like illumination variations and viewpoint changes in nighttime UAV tracking, where existing methods are computationally costly or inefficient.

Method: Proposes DARTer with Dynamic Feature Blender (DFB) for feature fusion and Dynamic Feature Activator (DFA) for adaptive layer activation in Vision Transformers.

Result: Outperforms state-of-the-art trackers on nighttime UAV benchmarks, balancing accuracy and efficiency.

Conclusion: DARTer is a promising solution for real-world nighttime UAV tracking due to its robustness and streamlined training.

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>


### [51] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe, Ratna Aisuwarya, Lei Jing*

Main category: cs.CV

TL;DR: P2P-Insole is a low-cost, 3D human skeletal estimation system using e-textile insoles with IMUs, costing under USD 1. It leverages foot pressure, acceleration, and rotation data with a Transformer model for accurate motion recognition.


<details>
  <summary>Details</summary>
Motivation: To provide a cost-effective, lightweight, and privacy-aware solution for 3D skeletal estimation, addressing the high cost and intrusiveness of commercial alternatives.

Method: Uses insole-type sensors with IMUs, fabricated via e-textile techniques. A Transformer model processes temporal features, enhanced by derivatives and multimodal data (accelerometers, rotation).

Result: Demonstrates robustness in posture estimation tasks with improved accuracy for complex motion patterns, validated experimentally.

Conclusion: P2P-Insole offers a scalable, low-cost solution for rehabilitation, injury prevention, and health monitoring, with potential for further optimization.

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>


### [52] [Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging](https://arxiv.org/abs/2505.00805)
*Fadi Abdeladhim Zidi, Abdelkrim Ouafi, Fares Bougourzi, Cosimo Distante, Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: A review of deep learning methods for hyperspectral imaging (HSI) in wheat crop analysis, covering datasets, advancements, and applications like disease detection and yield estimation.


<details>
  <summary>Details</summary>
Motivation: Wheat production faces challenges like pests and climate change, requiring efficient monitoring. HSI and deep learning offer solutions but lack a comprehensive survey.

Method: Summarizes benchmark datasets and tracks deep learning advancements for HSI-based wheat analysis.

Result: Identifies key applications (e.g., disease detection) and highlights strengths, limitations, and future opportunities.

Conclusion: The review fills a gap in the field and provides a resource for ongoing research, with updates tracked on GitHub.

Abstract: As one of the most widely cultivated and consumed crops, wheat is essential
to global food security. However, wheat production is increasingly challenged
by pests, diseases, climate change, and water scarcity, threatening yields.
Traditional crop monitoring methods are labor-intensive and often ineffective
for early issue detection. Hyperspectral imaging (HSI) has emerged as a
non-destructive and efficient technology for remote crop health assessment.
However, the high dimensionality of HSI data and limited availability of
labeled samples present notable challenges. In recent years, deep learning has
shown great promise in addressing these challenges due to its ability to
extract and analysis complex structures. Despite advancements in applying deep
learning methods to HSI data for wheat crop analysis, no comprehensive survey
currently exists in this field. This review addresses this gap by summarizing
benchmark datasets, tracking advancements in deep learning methods, and
analyzing key applications such as variety classification, disease detection,
and yield estimation. It also highlights the strengths, limitations, and future
opportunities in leveraging deep learning methods for HSI-based wheat crop
analysis. We have listed the current state-of-the-art papers and will continue
tracking updating them in the following
https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.

</details>


### [53] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong*

Main category: cs.CV

TL;DR: First on-chip implementation of 4D radar-based 3D object detection on Hailo-8L AI accelerator, overcoming 5D input limitations with tensor transformation, achieving real-time performance and comparable accuracy to GPUs.


<details>
  <summary>Details</summary>
Motivation: Enable robust 3D object detection in autonomous driving under adverse weather conditions, requiring real-time processing in low-power embedded environments.

Method: Introduces a tensor transformation method to reshape 5D inputs into 4D formats for deployment on Hailo-8L, which only supports 4D tensors.

Result: Achieves 46.47% AP_3D and 52.75% AP_BEV with 13.76 Hz inference speed, matching GPU accuracy.

Conclusion: Demonstrates practical applicability of 4D radar-based perception for autonomous driving systems.

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>


### [54] [Improving Editability in Image Generation with Layer-wise Memory](https://arxiv.org/abs/2505.01079)
*Daneul Kim, Jaeah Lee, Jaesik Park*

Main category: cs.CV

TL;DR: A framework for sequential image editing using rough masks and layer-wise memory to maintain coherence and adapt new elements naturally.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with sequential edits, especially preserving previous changes and integrating new objects contextually.

Method: Uses rough mask inputs, layer-wise memory for latent representations, Background Consistency Guidance, and Multi-Query Disentanglement in cross-attention.

Result: Superior performance in iterative editing with minimal user effort, maintaining high-quality results across steps.

Conclusion: The proposed framework effectively handles complex sequential edits, preserving context and coherence.

Abstract: Most real-world image editing tasks require multiple sequential edits to
achieve desired results. Current editing approaches, primarily designed for
single-object modifications, struggle with sequential editing: especially with
maintaining previous edits along with adapting new objects naturally into the
existing content. These limitations significantly hinder complex editing
scenarios where multiple objects need to be modified while preserving their
contextual relationships. We address this fundamental challenge through two key
proposals: enabling rough mask inputs that preserve existing content while
naturally integrating new elements and supporting consistent editing across
multiple modifications. Our framework achieves this through layer-wise memory,
which stores latent representations and prompt embeddings from previous edits.
We propose Background Consistency Guidance that leverages memorized latents to
maintain scene coherence and Multi-Query Disentanglement in cross-attention
that ensures natural adaptation to existing content. To evaluate our method, we
present a new benchmark dataset incorporating semantic alignment metrics and
interactive editing scenarios. Through comprehensive experiments, we
demonstrate superior performance in iterative image editing tasks with minimal
user effort, requiring only rough masks while maintaining high-quality results
throughout multiple editing steps.

</details>


### [55] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen, Candace Ross, Reyhane Askari-Hemmat, Koustuv Sinha, Melissa Hall, Michal Drozdzal, Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: MT2IE is a framework using MLLMs to evaluate T2I models efficiently, matching benchmark results with far fewer prompts and better human correlation.


<details>
  <summary>Details</summary>
Motivation: Static datasets for T2I evaluation are becoming outdated; MLLMs offer a dynamic alternative.

Method: MT2IE iteratively generates prompts, scores images, and assesses prompt-generation consistency and aesthetics.

Result: MT2IE achieves benchmark-equivalent rankings with 1/80th the prompts and higher human-judgment correlation.

Conclusion: MT2IE provides a scalable, efficient alternative to static benchmarks for T2I evaluation.

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>


### [56] [High Dynamic Range Novel View Synthesis with Single Exposure](https://arxiv.org/abs/2505.01212)
*Kaixuan Zhang, Hu Wang, Minxian Li, Mingwu Ren, Mao Ye, Xiatian Zhu*

Main category: cs.CV

TL;DR: The paper introduces Mono-HDR-3D, a novel approach for single-exposure HDR-NVS, overcoming limitations of multiple-exposure methods like motion artifacts and high costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of multiple-exposure HDR-NVS, such as motion artifacts and high capture/storage costs, by proposing a single-exposure solution.

Method: Introduces Mono-HDR-3D with two modules: one for LDR-to-HDR conversion and another for HDR-to-LDR transformation, enabling unsupervised learning in a closed loop.

Result: Mono-HDR-3D outperforms previous methods in extensive experiments.

Conclusion: The proposed single-exposure HDR-NVS approach is effective and can integrate with existing NVS models.

Abstract: High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D
scene HDR model from Low Dynamic Range (LDR) imagery. Typically,
multiple-exposure LDR images are employed to capture a wider range of
brightness levels in a scene, as a single LDR image cannot represent both the
brightest and darkest regions simultaneously. While effective, this
multiple-exposure HDR-NVS approach has significant limitations, including
susceptibility to motion artifacts (e.g., ghosting and blurring), high capture
and storage costs. To overcome these challenges, we introduce, for the first
time, the single-exposure HDR-NVS problem, where only single exposure LDR
images are available during training. We further introduce a novel approach,
Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image
formation principles, one for converting LDR colors to HDR counterparts, and
the other for transforming HDR images to LDR format so that unsupervised
learning is enabled in a closed loop. Designed as a meta-algorithm, our
approach can be seamlessly integrated with existing NVS models. Extensive
experiments show that Mono-HDR-3D significantly outperforms previous methods.
Source code will be released.

</details>


### [57] [Person detection and re-identification in open-world settings of retail stores and public spaces](https://arxiv.org/abs/2505.00772)
*Branko Brkljač, Milan Brkljač*

Main category: cs.CV

TL;DR: The paper discusses challenges and solutions for person re-identification in smart cities, focusing on open-world environments, multi-camera setups, and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of person re-identification in open-world settings with varying conditions and multi-camera systems.

Method: Analyzes system design architectures, computer vision techniques, and evaluates a near real-time solution across video captures and live feeds.

Result: Demonstrates performance of a re-identification solution and identifies challenges in diverse environments.

Conclusion: Proposes further research directions and system improvements for enhanced re-identification in smart city applications.

Abstract: Practical applications of computer vision in smart cities usually assume
system integration and operation in challenging open-world environments. In the
case of person re-identification task the main goal is to retrieve information
whether the specific person has appeared in another place at a different time
instance of the same video, or over multiple camera feeds. This typically
assumes collecting raw data from video surveillance cameras in different places
and under varying illumination conditions. In the considered open-world setting
it also requires detection and localization of the person inside the analyzed
video frame before the main re-identification step. With multi-person and
multi-camera setups the system complexity becomes higher, requiring
sophisticated tracking solutions and re-identification models. In this work we
will discuss existing challenges in system design architectures, consider
possible solutions based on different computer vision techniques, and describe
applications of such systems in retail stores and public spaces for improved
marketing analytics. In order to analyse sensitivity of person
re-identification task under different open-world environments, a performance
of one close to real-time solution will be demonstrated over several video
captures and live camera feeds. Finally, based on conducted experiments we will
indicate further research directions and possible system improvements.

</details>


### [58] [RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement](https://arxiv.org/abs/2505.01224)
*Kui Jiang, Yan Luo, Junjun Jiang, Xin Xu, Fei Ma, Fei Yu*

Main category: cs.CV

TL;DR: The paper introduces RD-UIE, a relation-driven Mamba framework for underwater image enhancement, outperforming WMamba with a 0.55 dB gain.


<details>
  <summary>Details</summary>
Motivation: Underwater images suffer from degradation and color distortion, and existing models like Mamba lack adaptability to local and global features in complex environments.

Method: Enhances Mamba with a sorting-based scanning mechanism and introduces Visually Self-adaptive State Block (VSSB) and cross-feature bridge (CFB) for robust feature extraction.

Result: RD-UIE outperforms WMamba, achieving a 0.55 dB performance gain on benchmarks.

Conclusion: The proposed framework effectively addresses underwater image enhancement by integrating global context and local features.

Abstract: Underwater image enhancement (UIE) is a critical preprocessing step for
marine vision applications, where wavelength-dependent attenuation causes
severe content degradation and color distortion. While recent state space
models like Mamba show potential for long-range dependency modeling, their
unfolding operations and fixed scan paths on 1D sequences fail to adapt to
local object semantics and global relation modeling, limiting their efficacy in
complex underwater environments. To address this, we enhance conventional Mamba
with the sorting-based scanning mechanism that dynamically reorders scanning
sequences based on statistical distribution of spatial correlation of all
pixels. In this way, it encourages the network to prioritize the most
informative components--structural and semantic features. Upon building this
mechanism, we devise a Visually Self-adaptive State Block (VSSB) that
harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,
enabling coherent integration of global context and local relational cues. This
exquisite design helps eliminate global focus bias, especially for widely
distributed contents, which greatly weakens the statistical frequency. For
robust feature extraction and refinement, we design a cross-feature bridge
(CFB) to adaptively fuse multi-scale representations. These efforts compose the
novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive
experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms
the state-of-the-art approach WMamba in both quantitative metrics and visual
fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.
Our code is available at https://github.com/kkoucy/RD-UIE/tree/main

</details>


### [59] [AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring](https://arxiv.org/abs/2505.00786)
*Oluwanisola Ibikunle, Hara Talasila, Debvrat Varshney, Jilu Li, John Paden, Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: The paper introduces the first standardized radar echogram dataset for deep learning, aiding in tracking ice sheet layers and improving snow accumulation estimation.


<details>
  <summary>Details</summary>
Motivation: The lack of a well-annotated dataset for radar echograms has hindered reliable testing and comparison of deep learning algorithms for ice sheet layer tracking.

Method: A comprehensive dataset of labeled and weakly-labeled radar echograms from NASA's OIB mission was created. Five deep learning models were evaluated on this dataset.

Result: Current segmentation algorithms can track snow layers, but advanced end-to-end models are needed for direct snow depth extraction.

Conclusion: The dataset and benchmarking framework advance radar echogram analysis and polar ice sheet understanding under climate warming.

Abstract: Tracking internal layers in radar echograms with high accuracy is essential
for understanding ice sheet dynamics and quantifying the impact of accelerated
ice discharge in Greenland and other polar regions due to contemporary global
climate warming. Deep learning algorithms have become the leading approach for
automating this task, but the absence of a standardized and well-annotated
echogram dataset has hindered the ability to test and compare algorithms
reliably, limiting the advancement of state-of-the-art methods for the radar
echogram layer tracking problem. This study introduces the first comprehensive
``deep learning ready'' radar echogram dataset derived from Snow Radar airborne
data collected during the National Aeronautics and Space Administration
Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled
and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,
wet) with varying along-track resolutions. To demonstrate its utility, we
evaluated the performance of five deep learning models on the dataset. Our
results show that while current computer vision segmentation algorithms can
identify and track snow layer pixels in echogram images, advanced end-to-end
models are needed to directly extract snow depth and annual accumulation from
echograms, reducing or eliminating post-processing. The dataset and
accompanying benchmarking framework provide a valuable resource for advancing
radar echogram layer tracking and snow accumulation estimation, advancing our
understanding of polar ice sheets response to climate warming.

</details>


### [60] [SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/abs/2505.00788)
*Wufei Ma, Luoxin Ye, Nessa McWeeney, Celso M de Melo, Alan Yuille, Jieneng Chen*

Main category: cs.CV

TL;DR: SpatialLLM is a new multimodal model designed to enhance 3D spatial reasoning, addressing data and design biases in current models, and outperforming GPT-4o by 8.7%.


<details>
  <summary>Details</summary>
Motivation: Current large multimodal models lack 3D spatial reasoning due to scarce 3D training data and 2D design biases. This paper aims to bridge this gap.

Method: Developed 3D-informed probing and conversation datasets, integrated with architectural and training designs for LMMs, to improve 3D reasoning.

Result: SpatialLLM achieves superior 3D reasoning, outperforming GPT-4o by 8.7%.

Conclusion: The study provides a roadmap for 3D-informed reasoning in LMMs, offering insights for future research.

Abstract: Humans naturally understand 3D spatial relationships, enabling complex
reasoning like predicting collisions of vehicles from different directions.
Current large multimodal models (LMMs), however, lack of this capability of 3D
spatial reasoning. This limitation stems from the scarcity of 3D training data
and the bias in current model designs toward 2D data. In this paper, we
systematically study the impact of 3D-informed data, architecture, and training
setups, introducing SpatialLLM, a large multi-modal model with advanced 3D
spatial reasoning abilities. To address data limitations, we develop two types
of 3D-informed training datasets: (1) 3D-informed probing data focused on
object's 3D location and orientation, and (2) 3D-informed conversation data for
complex spatial relationships. Notably, we are the first to curate VQA data
that incorporate 3D orientation relationships on real images. Furthermore, we
systematically integrate these two types of training data with the
architectural and training designs of LMMs, providing a roadmap for optimal
design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM
advances machines toward highly capable 3D-informed reasoning, surpassing
GPT-4o performance by 8.7%. Our systematic empirical design and the resulting
findings offer valuable insights for future research in this direction.

</details>


### [61] [The Comparability of Model Fusion to Measured Data in Confuser Rejection](https://arxiv.org/abs/2505.00836)
*Conor Flynn, Christopher Ebersole, Edmund Zelnio*

Main category: cs.CV

TL;DR: The paper addresses data scarcity in SAR deep learning by using synthetic data and model ensembling, while handling unknown targets via confuser rejection.


<details>
  <summary>Details</summary>
Motivation: Data collection for SAR is costly and limited, leading to insufficient unique targets and conditions for training deep learning models.

Method: Utilizes synthetic SAR data generated via the shooting and bouncing ray method and employs model ensembling to compensate for synthetic data's imperfections. Confuser rejection is used to handle unknown targets.

Result: The approach leverages computational power to mitigate the lack of quality measured data and improves model robustness against unknown targets.

Conclusion: Ensembling models trained on synthetic data and incorporating confuser rejection can effectively address SAR data scarcity and unknown target challenges.

Abstract: Data collection has always been a major issue in the modeling and training of
large deep learning networks, as no dataset can account for every slight
deviation we might see in live usage. Collecting samples can be especially
costly for Synthetic Aperture Radar (SAR), limiting the amount of unique
targets and operating conditions we are able to observe from. To counter this
lack of data, simulators have been developed utilizing the shooting and
bouncing ray method to allow for the generation of synthetic SAR data on 3D
models. While effective, the synthetically generated data does not perfectly
correlate to the measured data leading to issues when training models solely on
synthetic data. We aim to use computational power as a substitution for this
lack of quality measured data, by ensembling many models trained on synthetic
data. Synthetic data is also not complete, as we do not know what targets might
be present in a live environment. Therefore we need to have our ensembling
techniques account for these unknown targets by applying confuser rejection in
which our models will reject unknown targets it is presented with, and only
classify those it has been trained on.

</details>


### [62] [P-Hologen: An End-to-End Generative Framework for Phase-Only Holograms](https://arxiv.org/abs/2404.01330)
*JooHyun Park, YuJin Jeon, HuiYong Kim, SeungHwan Baek, HyeongYeop Kang*

Main category: cs.CV

TL;DR: P-Hologen is the first end-to-end generative framework for phase-only holograms, achieving superior quality and efficiency by integrating vector quantized variational autoencoders and the angular spectrum method.


<details>
  <summary>Details</summary>
Motivation: The application of generative models to holograms is underexplored due to phase learning complexity, despite its potential for innovation in holographic content creation.

Method: P-Hologen uses vector quantized variational autoencoders and integrates the angular spectrum method to model complex phase data.

Result: P-Hologen outperforms existing methods in quality and efficiency, generating diverse holographic content without pre-existing images.

Conclusion: P-Hologen advances holographic content creation, enabling new applications and methodologies in generative holography.

Abstract: Holography stands at the forefront of visual technology, offering immersive,
three-dimensional visualizations through the manipulation of light wave
amplitude and phase. Although generative models have been extensively explored
in the image domain, their application to holograms remains relatively
underexplored due to the inherent complexity of phase learning. Exploiting
generative models for holograms offers exciting opportunities for advancing
innovation and creativity, such as semantic-aware hologram generation and
editing. Currently, the most viable approach for utilizing generative models in
the hologram domain involves integrating an image-based generative model with
an image-to-hologram conversion model, which comes at the cost of increased
computational complexity and inefficiency. To tackle this problem, we introduce
P-Hologen, the first end-to-end generative framework designed for phase-only
holograms (POHs). P-Hologen employs vector quantized variational autoencoders
to capture the complex distributions of POHs. It also integrates the angular
spectrum method into the training process, constructing latent spaces for
complex phase data using strategies from the image processing domain. Extensive
experiments demonstrate that P-Hologen achieves superior quality and
computational efficiency compared to the existing methods. Furthermore, our
model generates high-quality unseen, diverse holographic content from its
learned latent space without requiring pre-existing images. Our work paves the
way for new applications and methodologies in holographic content creation,
opening a new era in the exploration of generative holographic content. The
code for our paper is publicly available on
https://github.com/james0223/P-Hologen.

</details>


### [63] [Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?](https://arxiv.org/abs/2505.00866)
*Viktor Kocur, Charalambos Tzamos, Yaqing Ding, Zuzana Berger Haladova, Torsten Sattler, Zuzana Kukelova*

Main category: cs.CV

TL;DR: The paper compares two simple methods for radial distortion in relative pose estimation, showing minimal solvers are often unnecessary.


<details>
  <summary>Details</summary>
Motivation: Radial distortion in cameras complicates pose estimation, and minimal solvers for it are complex. The paper explores simpler alternatives.

Method: Two approaches: 1) pinhole solver with sampled undistortion parameters, 2) neural network for distortion estimation.

Result: Experiments show minimal radial distortion solvers are often unnecessary in practice.

Conclusion: Simple methods can replace complex solvers, with conditions favoring sampling or learning-based approaches.

Abstract: Estimating the relative pose between two cameras is a fundamental step in
many applications such as Structure-from-Motion. The common approach to
relative pose estimation is to apply a minimal solver inside a RANSAC loop.
Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras
exhibit radial distortion. Not modeling radial distortion leads to
(significantly) worse results. However, minimal radial distortion solvers are
significantly more complex than pinhole solvers, both in terms of run-time and
implementation efforts. This paper compares radial distortion solvers with two
simple-to-implement approaches that do not use minimal radial distortion
solvers: The first approach combines an efficient pinhole solver with sampled
radial undistortion parameters, where the sampled parameters are used for
undistortion prior to applying the pinhole solver. The second approach uses a
state-of-the-art neural network to estimate the distortion parameters rather
than sampling them from a set of potential values. Extensive experiments on
multiple datasets, and different camera setups, show that complex minimal
radial distortion solvers are not necessary in practice. We discuss under which
conditions a simple sampling of radial undistortion parameters is preferable
over calibrating cameras using a learning-based prior approach. Code and newly
created benchmark for relative pose estimation under radial distortion are
available at https://github.com/kocurvik/rdnet.

</details>


### [64] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng, Xiaohan Zhang, Peilin Li, Zhe Wu, Yiming Li, Wenkai Zhao, Beinan Yu, Hui-Liang Shen*

Main category: cs.CV

TL;DR: CDFormer, a transformer-based method, addresses feature confusion in cross-domain few-shot object detection (CD-FSOD) using OBD and OOD modules, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Feature confusion (object-background and object-object) poses challenges in CD-FSOD.

Method: CDFormer uses two modules: OBD (learnable background token) and OOD (enhances inter-class distinction).

Result: Outperforms state-of-the-art with 12.9%, 11.0%, and 10.4% mAP improvements for 1/5/10 shots.

Conclusion: CDFormer effectively tackles feature confusion, advancing CD-FSOD performance.

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>


### [65] [Generating Animated Layouts as Structured Text Representations](https://arxiv.org/abs/2505.00975)
*Yeonsang Shin, Jihwan Kim, Yumin Song, Kyungseung Lee, Hyunhee Chung, Taeyoung Na*

Main category: cs.CV

TL;DR: The paper introduces Animated Layout Generation for precise control in text-to-video models, presenting VAKER, a pipeline that outperforms existing methods in video ad generation.


<details>
  <summary>Details</summary>
Motivation: Precise control over text elements and animated graphics in text-to-video models is challenging, especially for video advertisements.

Method: Proposes Animated Layout Generation with Structured Text Representation and a three-stage pipeline (VAKER) integrating Unstructured Text Reasoning with LLMs.

Result: VAKER outperforms existing methods in generating video advertisements, automating dynamic layout trajectories for objects and graphics.

Conclusion: The approach effectively addresses control limitations in text-to-video models, demonstrating superior performance in video ad generation.

Abstract: Despite the remarkable progress in text-to-video models, achieving precise
control over text elements and animated graphics remains a significant
challenge, especially in applications such as video advertisements. To address
this limitation, we introduce Animated Layout Generation, a novel approach to
extend static graphic layouts with temporal dynamics. We propose a Structured
Text Representation for fine-grained video control through hierarchical visual
elements. To demonstrate the effectiveness of our approach, we present VAKER
(Video Ad maKER), a text-to-video advertisement generation pipeline that
combines a three-stage generation process with Unstructured Text Reasoning for
seamless integration with LLMs. VAKER fully automates video advertisement
generation by incorporating dynamic layout trajectories for objects and
graphics across specific video frames. Through extensive evaluations, we
demonstrate that VAKER significantly outperforms existing methods in generating
video advertisements. Project Page:
https://yeonsangshin.github.io/projects/Vaker

</details>


### [66] [LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment](https://arxiv.org/abs/2505.00980)
*Jiahuan Long, Xin Zhou*

Main category: cs.CV

TL;DR: LMDepth is a lightweight Mamba-based monocular depth estimation network that balances performance and efficiency, outperforming existing methods with fewer parameters and lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing depth estimation algorithms struggle to balance performance and computational efficiency, limiting deployment on resource-constrained devices.

Method: LMDepth uses a modified pyramid spatial pooling module for multi-scale feature aggregation and integrates lightweight Mamba blocks for efficient depth decoding.

Result: LMDepth achieves higher performance with fewer parameters and lower computational complexity on NYUDv2 and KITTI datasets, and is validated on an embedded platform.

Conclusion: LMDepth offers a practical, efficient solution for monocular depth estimation, suitable for edge applications.

Abstract: Monocular depth estimation provides an additional depth dimension to RGB
images, making it widely applicable in various fields such as virtual reality,
autonomous driving and robotic navigation. However, existing depth estimation
algorithms often struggle to effectively balance performance and computational
efficiency, which poses challenges for deployment on resource-constrained
devices. To address this, we propose LMDepth, a lightweight Mamba-based
monocular depth estimation network, designed to reconstruct high-precision
depth information while maintaining low computational overhead. Specifically,
we propose a modified pyramid spatial pooling module that serves as a
multi-scale feature aggregator and context extractor, ensuring global spatial
information for accurate depth estimation. Moreover, we integrate multiple
depth Mamba blocks into the decoder. Designed with linear computations, the
Mamba Blocks enable LMDepth to efficiently decode depth information from global
features, providing a lightweight alternative to Transformer-based
architectures that depend on complex attention mechanisms. Extensive
experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of
our proposed LMDepth. Compared to previous lightweight depth estimation
methods, LMDepth achieves higher performance with fewer parameters and lower
computational complexity (measured by GFLOPs). We further deploy LMDepth on an
embedded platform with INT8 quantization, validating its practicality for
real-world edge applications.

</details>


### [67] [Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis](https://arxiv.org/abs/2505.00998)
*Yu Hua, Weiming Liu, Gui Xu, Yaqing Hou, Yew-Soon Ong, Qiang Zhang*

Main category: cs.CV

TL;DR: DSDFM is a two-stage method for human motion synthesis, improving diversity and accuracy without extra training parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the unstable training of score-based generative models (SGMs) in human motion synthesis.

Method: DSDFM: Deterministic-to-Stochastic Diverse Latent Feature Mapping, with stages for motion reconstruction and diverse generation using DerODE and DivSDE.

Result: Achieves state-of-the-art results in diversity and accuracy, validated by experiments.

Conclusion: DSDFM is superior to existing methods, offering stable training and enhanced motion synthesis.

Abstract: Human motion synthesis aims to generate plausible human motion sequences,
which has raised widespread attention in computer animation. Recent score-based
generative models (SGMs) have demonstrated impressive results on this task.
However, their training process involves complex curvature trajectories,
leading to unstable training process. In this paper, we propose a
Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for
human motion synthesis. DSDFM consists of two stages. The first human motion
reconstruction stage aims to learn the latent space distribution of human
motions. The second diverse motion generation stage aims to build connections
between the Gaussian distribution and the latent space distribution of human
motions, thereby enhancing the diversity and accuracy of the generated human
motions. This stage is achieved by the designed deterministic feature mapping
procedure with DerODE and stochastic diverse output generation procedure with
DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can
enhance diversity without introducing additional training parameters.Through
qualitative and quantitative experiments, DSDFM achieves state-of-the-art
results surpassing the latest methods, validating its superiority in human
motion synthesis.

</details>


### [68] [3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer](https://arxiv.org/abs/2505.01003)
*Kamel Aouaidjia, Aofan Li, Wenhao Zhang, Chongsheng Zhang*

Main category: cs.CV

TL;DR: A new method combines GCNs and Transformers for 3D human pose estimation, addressing limitations of both by using multi-order graphs and a Body Aware Transformer.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer and GCN methods for 3D pose estimation ignore spatial/temporal relationships or pose-specific representations.

Method: Proposes multi-order graph modeling with Graph Order Attention and a Body Aware Transformer for temporal processing.

Result: Outperforms on Human3.6m, MPIINF-3DHP, and HumanEva-I datasets.

Conclusion: The method effectively integrates spatial and temporal features for accurate 3D pose estimation.

Abstract: Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the
prevailing techniques for 3D human pose estimation. However, Transformer-based
methods either ignore the spatial neighborhood relationships between the joints
when used for skeleton representations or disregard the local temporal patterns
of the local joint movements in skeleton sequence modeling, while GCN-based
methods often neglect the need for pose-specific representations. To address
these problems, we propose a new method that exploits the graph modeling
capability of GCN to represent each skeleton with multiple graphs of different
orders, incorporated with a newly introduced Graph Order Attention module that
dynamically emphasizes the most representative orders for each joint. The
resulting spatial features of the sequence are further processed using a
proposed temporal Body Aware Transformer that models the global body feature
dependencies in the sequence with awareness of the local inter-skeleton feature
dependencies of joints. Given that our 3D pose output aligns with the central
2D pose in the sequence, we improve the self-attention mechanism to be aware of
the central pose while diminishing its focus gradually towards the first and
the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I
datasets demonstrate the effectiveness of the proposed method. Code and models
are made available on Github.

</details>


### [69] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi, Sagar Gandhi*

Main category: cs.CV

TL;DR: Deeper fine-tuning of pre-trained object detectors (down to layer 10) significantly improves performance on fine-grained tasks without degrading general capabilities.


<details>
  <summary>Details</summary>
Motivation: To determine the optimal depth for fine-tuning pre-trained object detectors to balance specialization and retention of general capabilities.

Method: Adapted YOLOv8n to a fruit detection dataset by progressively unfreezing backbone layers (22, 15, 10) and evaluated performance on both the target dataset and COCO.

Result: Deeper fine-tuning (layer 10) improved fruit detection by +10% mAP50 with negligible (<0.1%) loss on COCO.

Conclusion: Mid-to-late backbone fine-tuning is effective for specialization without catastrophic forgetting, advocating deeper fine-tuning for complex domains.

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>


### [70] [Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing](https://arxiv.org/abs/2505.01032)
*Ruyu Yan, Da-Qing Zhang*

Main category: cs.CV

TL;DR: EDD-MAIT is a novel edge detection and denoising method using multi-scale adaptive statistical testing and a channel attention mechanism, outperforming existing methods in accuracy, efficiency, and noise robustness.


<details>
  <summary>Details</summary>
Motivation: Existing edge detection methods produce overly detailed edge maps and face issues like scale mismatch and computational redundancy.

Method: Proposes EDD-MAIT, integrating multi-scale adaptive statistical testing with a channel attention mechanism and gradient-driven adaptive window strategy.

Result: Outperforms traditional and learning-based methods on BSDS500 and BIPED datasets, with better F-score, MSE, PSNR, and runtime. Robust against Gaussian noise.

Conclusion: EDD-MAIT improves edge detection clarity, detail preservation, and noise suppression, offering superior performance in noisy environments.

Abstract: Edge detection is crucial in image processing, but existing methods often
produce overly detailed edge maps, affecting clarity. Fixed-window statistical
testing faces issues like scale mismatch and computational redundancy. To
address these, we propose a novel Multi-scale Adaptive Independence
Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive
Statistical Testing-based edge detection and denoising method that integrates a
channel attention mechanism with independence testing. A gradient-driven
adaptive window strategy adjusts window sizes dynamically, improving detail
preservation and noise suppression. EDD-MAIT achieves better robustness,
accuracy, and efficiency, outperforming traditional and learning-based methods
on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and
reduced runtime. It also shows robustness against Gaussian noise, generating
accurate and clean edge maps in noisy environments.

</details>


### [71] [Edge Detection based on Channel Attention and Inter-region Independence Test](https://arxiv.org/abs/2505.01040)
*Ru-yu Yan, Da-Qing Zhang*

Main category: cs.CV

TL;DR: CAM-EDIT integrates CAM and EDIT for edge detection, outperforming traditional and learning-based methods with improved noise robustness.


<details>
  <summary>Details</summary>
Motivation: Address noise amplification and excessive retention of non-salient details in edge detection for high-precision industrial applications.

Method: Combines CAM for adaptive feature enhancement and EDIT for noise suppression via statistical independence analysis.

Result: Achieves state-of-the-art F-measure scores (0.635, 0.460) and 2.2% PSNR improvement under noise.

Conclusion: CAM-EDIT produces cleaner edge maps, suitable for high-precision industrial use.

Abstract: Existing edge detection methods often suffer from noise amplification and
excessive retention of non-salient details, limiting their applicability in
high-precision industrial scenarios. To address these challenges, we propose
CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)
and Edge Detection via Independence Testing (EDIT). The CAM module adaptively
enhances discriminative edge features through multi-channel fusion, while the
EDIT module employs region-wise statistical independence analysis (using
Fisher's exact test and chi-square test) to suppress uncorrelated
noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate
state-of-the-art performance. Among the nine comparison algorithms, the
F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of
19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the
latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations
further reveal a 2.2\% PSNR improvement under Gaussian noise compared to
baseline methods. Qualitative results exhibit cleaner edge maps with reduced
artifacts, demonstrating its potential for high-precision industrial
applications.

</details>


### [72] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson*

Main category: cs.CV

TL;DR: Adversarial attacks are highly transferable to proprietary VLLMs like GPT-4o, Claude, and Gemini, enabling attackers to manipulate visual interpretations. Universal perturbations can induce consistent misinterpretations across models, highlighting a critical security vulnerability.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored vulnerabilities of Vision Large Language Models (VLLMs) to adversarial attacks, especially in multimodal (text and image) contexts.

Method: Comprehensive analysis involving crafting targeted adversarial examples and universal perturbations to test transferability to proprietary VLLMs. Experiments conducted on tasks like object recognition, visual question answering, and image captioning.

Result: Adversarial examples are highly transferable, inducing specific misinterpretations (e.g., hazardous content as safe). Universal perturbations work consistently across multiple VLLMs.

Conclusion: The study reveals a widespread vulnerability in VLLMs, emphasizing the urgent need for robust mitigations to ensure secure deployment.

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>


### [73] [GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation](https://arxiv.org/abs/2505.01057)
*Boris Kriuk, Matey Yordanov*

Main category: cs.CV

TL;DR: GeloVec is a CNN-based attention smoothing framework for semantic segmentation, improving boundary stability and contextual continuity with geometric smoothing and adaptive sampling weights.


<details>
  <summary>Details</summary>
Motivation: Addresses boundary instability and contextual discontinuities in conventional attention-backed segmentation methods.

Method: Uses higher-dimensional geometric smoothing, modified Chebyshev distance metrics, and multispatial transformations with adaptive sampling weights.

Result: Achieves mIoU gains of 2.1%, 2.7%, and 2.4% on benchmark datasets, with theoretical guarantees from Riemannian geometry.

Conclusion: GeloVec enhances segmentation accuracy and stability while maintaining computational efficiency and generalization.

Abstract: This paper introduces GeloVec, a new CNN-based attention smoothing framework
for semantic segmentation that addresses critical limitations in conventional
approaches. While existing attention-backed segmentation methods suffer from
boundary instability and contextual discontinuities during feature mapping, our
framework implements a higher-dimensional geometric smoothing method to
establish a robust manifold relationships between visually coherent regions.
GeloVec combines modified Chebyshev distance metrics with multispatial
transformations to enhance segmentation accuracy through stabilized feature
extraction. The core innovation lies in the adaptive sampling weights system
that calculates geometric distances in n-dimensional feature space, achieving
superior edge preservation while maintaining intra-class homogeneity. The
multispatial transformation matrix incorporates tensorial projections with
orthogonal basis vectors, creating more discriminative feature representations
without sacrificing computational efficiency. Experimental validation across
multiple benchmark datasets demonstrates significant improvements in
segmentation performance, with mean Intersection over Union (mIoU) gains of
2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets
respectively compared to state-of-the-art methods. GeloVec's mathematical
foundation in Riemannian geometry provides theoretical guarantees on
segmentation stability. Importantly, our framework maintains computational
efficiency through parallelized implementation of geodesic transformations and
exhibits strong generalization capabilities across disciplines due to the
absence of information loss during transformations.

</details>


### [74] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: The paper introduces NeaR, a method for Vocabulary-Free Fine-Grained Visual Recognition (VF-FGVR) that uses Multimodal Large Language Models (MLLMs) to generate labels for training a CLIP model, addressing challenges like high costs and inference times.


<details>
  <summary>Details</summary>
Motivation: The lack of labeled datasets in domains like medical imaging due to privacy and annotation costs motivates the need for VF-FGVR, where models predict labels without predefined training labels.

Method: NeaR fine-tunes a CLIP model using weakly supervised labels generated by MLLMs from a small unlabeled dataset, handling noise and stochasticity in MLLM-generated labels.

Result: NeaR establishes a new benchmark for efficient VF-FGVR by leveraging MLLMs for label generation and refining them for downstream tasks.

Conclusion: NeaR provides a practical solution for VF-FGVR by combining MLLMs' label generation with efficient downstream model fine-tuning, overcoming limitations of direct MLLM usage.

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>


### [75] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè, Rosa Sicilia, Paolo Soda, Valerio Guarrasi*

Main category: cs.CV

TL;DR: The study evaluates instruction-tuned Vision-Language Models (VLMs) for radiology report generation in low-resource languages (Italian, German, Spanish), finding language-specific and domain-tuned models perform best.


<details>
  <summary>Details</summary>
Motivation: Challenges in generating accurate radiology reports in low-resource languages and the lack of models combining medical domain knowledge with low-resource language proficiency.

Method: Used the LLaVA framework to evaluate pre-trained models on general, domain-specific, and language-specific datasets, analyzing adaptations for effectiveness.

Result: Language-specific models outperformed others; medical terminology fine-tuning improved performance. Temperature parameter impact on coherence was also studied.

Conclusion: Tailored language and domain-specific training are crucial for accurate radiology reports in multilingual settings, suggesting future research directions.

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>


### [76] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino, Francesco di Feola, Linlin Shen, Paolo Soda, Valerio Guarrasi*

Main category: cs.CV

TL;DR: A framework for generating multimodal medical data (chest X-rays and reports) is introduced, showing high fidelity and clinical relevance, outperforming general models in healthcare tasks.


<details>
  <summary>Details</summary>
Motivation: Address the gap between general-purpose generative models and the specialized needs of medical data, ensuring clinical accuracy and utility.

Method: Proposes a framework leveraging the MIMIC-CXR dataset to generate multi-view chest X-rays and coherent reports, evaluated using FID and BLEU scores.

Result: Achieves superior performance in data quality and downstream disease classification, comparable to real data.

Conclusion: Domain-specific adaptations enhance generative models' clinical relevance, advancing synthetic medical data generation.

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>


### [77] [VSC: Visual Search Compositional Text-to-Image Diffusion Model](https://arxiv.org/abs/2505.01104)
*Do Huu Dat, Nam Hyeonu, Po-Yuan Mao, Tae-Hyun Oh*

Main category: cs.CV

TL;DR: A novel method improves attribute-object binding in text-to-image diffusion models by decomposing prompts, generating visual prototypes, and using segmentation-based training.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with accurately binding attributes to objects in complex prompts due to limitations in text encoders like CLIP.

Method: Decomposes prompts into sub-prompts, generates images, computes visual prototypes, and uses segmentation-based training to enhance binding.

Result: Outperforms existing models on T2I CompBench, achieving better image quality and robustness with complex prompts.

Conclusion: The approach effectively addresses cross-attention misalignment, improving accuracy in attribute-object binding.

Abstract: Text-to-image diffusion models have shown impressive capabilities in
generating realistic visuals from natural-language prompts, yet they often
struggle with accurately binding attributes to corresponding objects,
especially in prompts containing multiple attribute-object pairs. This
challenge primarily arises from the limitations of commonly used text encoders,
such as CLIP, which can fail to encode complex linguistic relationships and
modifiers effectively. Existing approaches have attempted to mitigate these
issues through attention map control during inference and the use of layout
information or fine-tuning during training, yet they face performance drops
with increased prompt complexity. In this work, we introduce a novel
compositional generation method that leverages pairwise image embeddings to
improve attribute-object binding. Our approach decomposes complex prompts into
sub-prompts, generates corresponding images, and computes visual prototypes
that fuse with text embeddings to enhance representation. By applying
segmentation-based localization training, we address cross-attention
misalignment, achieving improved accuracy in binding multiple attributes to
objects. Our approaches outperform existing compositional text-to-image
diffusion models on the benchmark T2I CompBench, achieving better image
quality, evaluated by humans, and emerging robustness under scaling number of
binding pairs in the prompt.

</details>


### [78] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov, Loic Le Folgoc, Julien Adam, Anne Buronfosse, Gilles Hayem, Guillaume Hocquet, Pietro Gori*

Main category: cs.CV

TL;DR: Instance-based MIL with SSL feature extractors matches or outperforms embedding-based MIL, offering better interpretability for WSI classification.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumed superiority of embedding-based MIL in WSI classification by leveraging improved SSL feature extractors.

Method: Conducted 710 experiments comparing 10 MIL strategies, 6 SSL methods, 4 backbones, and introduced 4 new instance-based MIL methods.

Result: Simple instance-based MIL with SSL features achieved similar or better performance than complex embedding-based MIL, setting new SOTA on BRACS and Camelyon16.

Conclusion: Efforts should focus on SSL methods for WSI rather than complex embedding-based MIL, given the interpretability and performance of instance-based approaches.

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>


### [79] [FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis](https://arxiv.org/abs/2505.01172)
*Jiangtong Tan, Hu Yu, Jie Huang, Jie Xiao, Feng Zhao*

Main category: cs.CV

TL;DR: FreePCA is a training-free method for long video generation that decouples global and local information using PCA, improving consistency and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing distribution shifts and motion inconsistency in long video generation by leveraging both local and global information.

Method: Uses PCA to decouple appearance and motion intensity, then integrates them progressively for smooth transitions and consistency.

Result: Substantial improvements in video quality and consistency across various video diffusion models without training.

Conclusion: FreePCA effectively combines global and local information for high-quality, consistent long video generation.

Abstract: Long video generation involves generating extended videos using models
trained on short videos, suffering from distribution shifts due to varying
frame counts. It necessitates the use of local information from the original
short frames to enhance visual and motion quality, and global information from
the entire long frames to ensure appearance consistency. Existing training-free
methods struggle to effectively integrate the benefits of both, as appearance
and motion in videos are closely coupled, leading to motion inconsistency and
visual quality. In this paper, we reveal that global and local information can
be precisely decoupled into consistent appearance and motion intensity
information by applying Principal Component Analysis (PCA), allowing for
refined complementary integration of global consistency and local quality. With
this insight, we propose FreePCA, a training-free long video generation
paradigm based on PCA that simultaneously achieves high consistency and
quality. Concretely, we decouple consistent appearance and motion intensity
features by measuring cosine similarity in the principal component space.
Critically, we progressively integrate these features to preserve original
quality and ensure smooth transitions, while further enhancing consistency by
reusing the mean statistics of the initial noise. Experiments demonstrate that
FreePCA can be applied to various video diffusion models without requiring
training, leading to substantial improvements. Code is available at
https://github.com/JosephTiTan/FreePCA.

</details>


### [80] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu*

Main category: cs.CV

TL;DR: The paper introduces TSTMotion, a training-free framework for scene-aware text-to-motion generation, enhancing pre-trained motion generators without costly ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Existing scene-aware text-to-motion methods require expensive ground-truth motion sequences in 3D scenes, prompting a need for a cost-effective solution.

Method: TSTMotion uses foundation models to predict and validate scene-aware motion guidance, integrating it into pre-trained blank-background motion generators with two modifications.

Result: Experiments show the framework's efficacy and generalizability in generating scene-aware motion sequences.

Conclusion: TSTMotion provides a practical, training-free solution for scene-aware text-to-motion generation, addressing the limitations of existing methods.

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>


### [81] [Efficient Vision-based Vehicle Speed Estimation](https://arxiv.org/abs/2505.01203)
*Andrej Macko, Lukáš Gajdošech, Viktor Kocur*

Main category: cs.CV

TL;DR: A computationally efficient method for vehicle speed estimation from traffic camera footage, improving real-time performance and accuracy over prior work.


<details>
  <summary>Details</summary>
Motivation: To enhance real-time vehicle speed estimation from traffic cameras by improving computational efficiency without sacrificing accuracy.

Method: Utilizes 3D bounding boxes from 2D detections and vanishing point geometry, with optimizations for real-time performance. Evaluated on the BrnoCompSpeed dataset.

Result: Achieves better FPS and accuracy (0.58 km/h median error, 91.02% precision, 91.14% recall) while being 5.5x faster than prior state-of-the-art.

Conclusion: Smaller models with post-training quantization offer the best balance for real-world deployment, outperforming prior methods in speed and accuracy.

Abstract: This paper presents a computationally efficient method for vehicle speed
estimation from traffic camera footage. Building upon previous work that
utilizes 3D bounding boxes derived from 2D detections and vanishing point
geometry, we introduce several improvements to enhance real-time performance.
We evaluate our method in several variants on the BrnoCompSpeed dataset in
terms of vehicle detection and speed estimation accuracy. Our extensive
evaluation across various hardware platforms, including edge devices,
demonstrates significant gains in frames per second (FPS) compared to the prior
state-of-the-art, while maintaining comparable or improved speed estimation
accuracy. We analyze the trade-off between accuracy and computational cost,
showing that smaller models utilizing post-training quantization offer the best
balance for real-world deployment. Our best performing model beats previous
state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h
vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.
83.32%) while also being 5.5 times faster.

</details>


### [82] [T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph](https://arxiv.org/abs/2505.01207)
*Qingyu Xian, Weiqin Jiao, Hao Cheng, Berend Jan van der Zwaag, Yanqiu Huang*

Main category: cs.CV

TL;DR: T-Graph is a lightweight module enhancing sparse-view camera pose estimation by incorporating translation information through a fully connected graph and two novel pairwise translation representations.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect translation information between viewpoints, limiting performance in sparse-view scenarios.

Method: T-Graph uses an MLP to map paired image features into a translation graph, with nodes as cameras and edges as translation relationships. It integrates into existing models and introduces two translation representations (relative-t and pair-t).

Result: Experiments on RelPose++ and Forge with C03D and IMC PhotoTourism datasets show T-Graph improves camera center accuracy by 1% to 6% from 2 to 8 viewpoints.

Conclusion: T-Graph effectively enhances sparse-view pose estimation, offering robustness and adaptability across diverse scenarios.

Abstract: Sparse-view camera pose estimation, which aims to estimate the
6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from
different viewpoints, is a fundamental yet challenging problem in remote
sensing applications. Existing methods often overlook the translation
information between each pair of viewpoints, leading to suboptimal performance
in sparse-view scenarios. To address this limitation, we introduce T-Graph, a
lightweight, plug-and-play module to enhance camera pose estimation in
sparse-view settings. T-graph takes paired image features as input and maps
them through a Multilayer Perceptron (MLP). It then constructs a fully
connected translation graph, where nodes represent cameras and edges encode
their translation relationships. It can be seamlessly integrated into existing
models as an additional branch in parallel with the original prediction,
maintaining efficiency and ease of use. Furthermore, we introduce two pairwise
translation representations, relative-t and pair-t, formulated under different
local coordinate systems. While relative-t captures intuitive spatial
relationships, pair-t offers a rotation-disentangled alternative. The two
representations contribute to enhanced adaptability across diverse application
scenarios, further improving our module's robustness. Extensive experiments on
two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D
and IMC PhotoTourism) validate both the effectiveness and generalizability of
T-Graph. The results demonstrate consistent improvements across various
metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8
viewpoints.

</details>


### [83] [Core-Set Selection for Data-efficient Land Cover Segmentation](https://arxiv.org/abs/2505.01225)
*Keiller Nogueira, Akram Zaytar, Wanli Ma, Ribana Roscher, Ronny Hänsch, Caleb Robinson, Anthony Ortiz, Simone Nsutezo, Rahul Dodhia, Juan M. Lavista Ferres, Oktay Karakuş, Paul L. Rosin*

Main category: cs.CV

TL;DR: Proposes six core-set selection methods for remote sensing image segmentation, showing subsets can outperform random baselines and even full datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses the overlooked complexities of large datasets in Earth Observation, emphasizing the need for both quantity and quality in data.

Method: Introduces six novel core-set selection methods (using imagery, labels, or both) and benchmarks them against random selection on three datasets.

Result: Subsets selected by the methods outperform random baselines, with some surpassing full dataset training.

Conclusion: Highlights the potential of data-centric learning in remote sensing, advocating for smarter data selection over sheer volume.

Abstract: The increasing accessibility of remotely sensed data and the potential of
such data to inform large-scale decision-making has driven the development of
deep learning models for many Earth Observation tasks. Traditionally, such
models must be trained on large datasets. However, the common assumption that
broadly larger datasets lead to better outcomes tends to overlook the
complexities of the data distribution, the potential for introducing biases and
noise, and the computational resources required for processing and storing vast
datasets. Therefore, effective solutions should consider both the quantity and
quality of data. In this paper, we propose six novel core-set selection methods
for selecting important subsets of samples from remote sensing image
segmentation datasets that rely on imagery only, labels only, and a combination
of each. We benchmark these approaches against a random-selection baseline on
three commonly used land cover classification datasets: DFC2022, Vaihingen, and
Potsdam. In each of the datasets, we demonstrate that training on a subset of
samples outperforms the random baseline, and some approaches outperform
training on all available data. This result shows the importance and potential
of data-centric learning for the remote sensing domain. The code is available
at https://github.com/keillernogueira/data-centric-rs-classification/.

</details>


### [84] [Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2505.01235)
*Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh*

Main category: cs.CV

TL;DR: The paper proposes a method to improve temporal consistency in online dynamic scene reconstruction by addressing noise-induced errors in real-world recordings.


<details>
  <summary>Details</summary>
Motivation: Existing online reconstruction methods prioritize efficiency and rendering quality but neglect temporal consistency, leading to artifacts in static regions.

Method: The method enhances temporal consistency by learning and subtracting errors from observations with inevitable temporal inconsistency.

Result: The approach significantly improves both temporal consistency and rendering quality across various datasets.

Conclusion: The proposed method effectively addresses temporal inconsistency in online reconstruction, with code and results publicly available.

Abstract: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

</details>


### [85] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TL;DR: The paper presents a method for fusing multiple fixations of a scene into a unified representation using linear downsampling and Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: Humans and vertebrates need to combine multiple fixations of a scene, each with varying resolution, to form a complete representation.

Method: The retinal transformation of a fixation is modeled as linear downsampling of a high-resolution latent image, enabling exact inference in factor analysis and mixtures of FA models. Bayesian experimental design is used to decide 'where to look next.'

Result: Experiments on Frey faces and MNIST datasets show the method's effectiveness.

Conclusion: The approach successfully integrates multiple fixations and optimizes gaze selection using Bayesian inference.

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>


### [86] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers, Baptiste Standaert, Victor Joos, Alexandre Alahi, Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: CAMEL introduces a data-driven association module for multi-object tracking, replacing hand-crafted heuristics with transformer-based modules and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current tracking-by-detection methods rely on human-crafted rules for temporal associations, limiting their ability to model complex interactions between tracking cues.

Method: CAMEL uses two transformer-based modules and a novel training scheme to learn association strategies directly from data, maintaining modularity.

Result: CAMELTrack, the proposed pipeline, achieves state-of-the-art performance on multiple tracking benchmarks.

Conclusion: CAMEL breaks free from hand-crafted heuristics while retaining modularity, offering a lightweight and effective solution for online multi-object tracking.

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>


### [87] [Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain](https://arxiv.org/abs/2505.01267)
*Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang*

Main category: cs.CV

TL;DR: The paper proposes a frequency-domain adversarial purification method that preserves image content and structure by focusing on less damaged low-frequency components, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based adversarial purification methods damage normal semantics due to lack of perturbation distribution information in the pixel domain.

Method: Decomposes images into amplitude and phase spectra, replaces low-frequency amplitude components, and projects phase into a designated range during the reverse process.

Result: The method significantly outperforms current defense methods by preserving content and structure while eliminating perturbations.

Conclusion: Frequency-domain purification effectively mitigates adversarial perturbations while minimizing damage to image semantics.

Abstract: The diffusion-based adversarial purification methods attempt to drown
adversarial perturbations into a part of isotropic noise through the forward
process, and then recover the clean images through the reverse process. Due to
the lack of distribution information about adversarial perturbations in the
pixel domain, it is often unavoidable to damage normal semantics. We turn to
the frequency domain perspective, decomposing the image into amplitude spectrum
and phase spectrum. We find that for both spectra, the damage caused by
adversarial perturbations tends to increase monotonically with frequency. This
means that we can extract the content and structural information of the
original clean sample from the frequency components that are less damaged.
Meanwhile, theoretical analysis indicates that existing purification methods
indiscriminately damage all frequency components, leading to excessive damage
to the image. Therefore, we propose a purification method that can eliminate
adversarial perturbations while maximizing the preservation of the content and
structure of the original image. Specifically, at each time step during the
reverse process, for the amplitude spectrum, we replace the low-frequency
components of the estimated image's amplitude spectrum with the corresponding
parts of the adversarial image. For the phase spectrum, we project the phase of
the estimated image into a designated range of the adversarial image's phase
spectrum, focusing on the low frequencies. Empirical evidence from extensive
experiments demonstrates that our method significantly outperforms most current
defense methods.

</details>


### [88] [FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors](https://arxiv.org/abs/2505.01322)
*Chenxi Li, Weijie Wang, Qiang Li, Bruno Lepri, Nicu Sebe, Weizhi Nie*

Main category: cs.CV

TL;DR: FreeInsert is a framework for text-driven 3D object insertion without spatial priors, using foundation models for semantic parsing and refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on spatial priors and struggle with consistency, limiting flexibility and scalability.

Method: FreeInsert uses MLLMs, LGMs, and diffusion models to parse semantics, initialize pose/scale, and refine placement for 3D consistency.

Result: Achieves coherent, precise, and realistic 3D insertions without spatial priors.

Conclusion: FreeInsert offers flexible, user-friendly 3D scene editing.

Abstract: Text-driven object insertion in 3D scenes is an emerging task that enables
intuitive scene editing through natural language. However, existing 2D
editing-based methods often rely on spatial priors such as 2D masks or 3D
bounding boxes, and they struggle to ensure consistency of the inserted object.
These limitations hinder flexibility and scalability in real-world
applications. In this paper, we propose FreeInsert, a novel framework that
leverages foundation models including MLLMs, LGMs, and diffusion models to
disentangle object generation from spatial placement. This enables unsupervised
and flexible object insertion in 3D scenes without spatial priors. FreeInsert
starts with an MLLM-based parser that extracts structured semantics, including
object types, spatial relationships, and attachment regions, from user
instructions. These semantics guide both the reconstruction of the inserted
object for 3D consistency and the learning of its degrees of freedom. We
leverage the spatial reasoning capabilities of MLLMs to initialize object pose
and scale. A hierarchical, spatially aware refinement stage further integrates
spatial semantics and MLLM-inferred priors to enhance placement. Finally, the
appearance of the object is improved using the inserted-object image to enhance
visual fidelity. Experimental results demonstrate that FreeInsert achieves
semantically coherent, spatially precise, and visually realistic 3D insertions
without relying on spatial priors, offering a user-friendly and flexible
editing experience.

</details>


### [89] [Monitoring morphometric drift in lifelong learning segmentation of the spinal cord](https://arxiv.org/abs/2505.01364)
*Enamundram Naga Karthik, Sandrine Bédard, Jan Valošek, Christoph S. Aigner, Elise Bannier, Josef Bednařík, Virginie Callot, Anna Combes, Armin Curt, Gergely David, Falk Eippert, Lynn Farner, Michael G Fehlings, Patrick Freund, Tobias Granberg, Cristina Granziera, RHSCIR Network Imaging Group, Ulrike Horn, Tomáš Horák, Suzanne Humphreys, Markus Hupp, Anne Kerbrat, Nawal Kinany, Shannon Kolind, Petr Kudlička, Anna Lebret, Lisa Eunyoung Lee, Caterina Mainero, Allan R. Martin, Megan McGrath, Govind Nair, Kristin P. O'Grady, Jiwon Oh, Russell Ouellette, Nikolai Pfender, Dario Pfyffer, Pierre-François Pradat, Alexandre Prat, Emanuele Pravatà, Daniel S. Reich, Ilaria Ricchi, Naama Rotem-Kohavi, Simon Schading-Sassenhausen, Maryam Seif, Andrew Smith, Seth A Smith, Grace Sweeney, Roger Tam, Anthony Traboulsee, Constantina Andrada Treaba, Charidimos Tsagkas, Zachary Vavasour, Dimitri Van De Ville, Kenneth Arnold Weber II, Sarath Chandar, Julien Cohen-Adad*

Main category: cs.CV

TL;DR: A spinal cord segmentation model with lifelong learning framework ensures stable morphometric measures for diagnostic and prognostic use, outperforming previous models and minimizing drift in updates.


<details>
  <summary>Details</summary>
Motivation: Assess the stability of spinal cord segmentation models as they are updated, crucial for deriving normative values from healthy participants.

Method: Developed a multisite-trained spinal cord segmentation model and introduced a lifelong learning framework to monitor morphometric drift via automatic GitHub Actions workflows.

Result: Model outperforms prior versions with a Dice score of 0.95 ± 0.03; minimal drift observed in updates, ensuring stable normative database scaling.

Conclusion: The framework provides reliable, stable morphometric measures for clinical applications, with the model available in Spinal Cord Toolbox v7.0.

Abstract: Morphometric measures derived from spinal cord segmentations can serve as
diagnostic and prognostic biomarkers in neurological diseases and injuries
affecting the spinal cord. While robust, automatic segmentation methods to a
wide variety of contrasts and pathologies have been developed over the past few
years, whether their predictions are stable as the model is updated using new
datasets has not been assessed. This is particularly important for deriving
normative values from healthy participants. In this study, we present a spinal
cord segmentation model trained on a multisite $(n=75)$ dataset, including 9
different MRI contrasts and several spinal cord pathologies. We also introduce
a lifelong learning framework to automatically monitor the morphometric drift
as the model is updated using additional datasets. The framework is triggered
by an automatic GitHub Actions workflow every time a new model is created,
recording the morphometric values derived from the model's predictions over
time. As a real-world application of the proposed framework, we employed the
spinal cord segmentation model to update a recently-introduced normative
database of healthy participants containing commonly used measures of spinal
cord morphometry. Results showed that: (i) our model outperforms previous
versions and pathology-specific models on challenging lumbar spinal cord cases,
achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow
for monitoring morphometric drift provides a quick feedback loop for developing
future segmentation models; and (iii) the scaling factor required to update the
database of morphometric measures is nearly constant among slices across the
given vertebral levels, showing minimum drift between the current and previous
versions of the model monitored by the framework. The model is freely available
in Spinal Cord Toolbox v7.0.

</details>


### [90] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang, Yilei Shi, Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: The paper introduces GCP, a novel algorithm for mapping polygonal buildings from remote sensing images, combining instance segmentation, transformer-based refinement, and collinearity-aware polygon simplification for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately mapping polygonal buildings from remote sensing images, where traditional methods like Douglas-Peucker fall short in balancing simplicity and fidelity.

Method: GCP processes binary masks from instance segmentation, refines polylines with a transformer-based module, and simplifies them using a collinearity-aware dynamic programming approach.

Result: Validated on public benchmarks, GCP outperforms traditional methods, showing broader applicability and improved accuracy.

Conclusion: GCP offers a robust and globally optimal solution for polygonal building mapping, with potential for broader use in polyline simplification.

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>


### [91] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano, Claudia Tacconi, Carlo Greco, Lorenzo Nibid, Edy Ippolito, Michele Fiore, Giuseppe Perrone, Sara Ramella, Paolo Soda, Valerio Guarrasi*

Main category: cs.CV

TL;DR: A novel multimodal deep learning approach with explainable AI improves pathological response prediction in lung cancer patients by integrating imaging and clinical data, guided by clinician input.


<details>
  <summary>Details</summary>
Motivation: Existing radiomics and unimodal deep learning methods have limitations in predicting pathological response, prompting the need for a more integrated and explainable approach.

Method: The study uses an intermediate fusion strategy to combine imaging and clinical data, along with a Doctor-in-the-Loop method to incorporate clinician knowledge during training.

Result: The approach shows improved predictive accuracy and explainability, offering insights into effective data integration for clinical use.

Conclusion: The proposed method enhances clinical relevance and performance in predicting treatment response, demonstrating the value of multimodal and explainable AI in oncology.

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>


### [92] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr*

Main category: cs.CV

TL;DR: VIDSTAMP is a watermarking framework for video diffusion models that embeds messages in latent space, maintaining high visual quality and robustness against manipulations.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing watermarking methods for videos, which struggle with manipulations like frame insertion/dropping and degrade quality.

Method: A two-stage pipeline fine-tunes the model's decoder: first on static images for spatial separation, then on videos for temporal consistency. Uses 3D convolutions and temporal attention.

Result: Embeds 768 bits per video (95.0% accuracy), achieves high video quality (0.836 score), and outperforms prior methods in capacity-quality tradeoffs.

Conclusion: VIDSTAMP offers a robust, high-capacity watermarking solution for video diffusion models with minimal perceptual impact and no added inference cost.

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>


### [93] [VitalVideos-Europe: A dataset of face videos with PPG and blood pressure ground truths](https://arxiv.org/abs/2306.11891)
*Pieter-Jan Toye*

Main category: cs.CV

TL;DR: A dataset of 850 participants with videos, PPG waveforms, blood pressure, and demographics was collected to aid remote vital sign measurement research.


<details>
  <summary>Details</summary>
Motivation: To support research and development in remote vital sign measurement by providing a diverse and comprehensive dataset.

Method: Collected data from 850 participants, including videos, PPG waveforms, blood pressure, and demographics (gender, age, skin color), in varied locations for diverse backgrounds and lighting.

Result: A dataset with balanced gender and age representation, though skin color distribution could be improved, is now publicly available.

Conclusion: The dataset is released to facilitate advancements in remote vital sign measurement technology.

Abstract: We collected a large dataset consisting of 850 unique participants. For every
participant we recorded two 30 second uncompressed videos, synchronized PPG
waveforms and a single blood pressure measurement. Gender, age and skin color
were also registered for every participant. The dataset includes roughly equal
numbers of males and females, as well as participants of all ages. While the
skin color distribution could have been more balanced, the dataset contains
individuals from every skin color. The data was collected in a diverse set of
locations to ensure a wide variety of backgrounds and lighting conditions. In
an effort to assist in the research and development of remote vital sign
measurement we are now opening up access to this dataset.
  vitalvideos.org for all datasets.

</details>


### [94] [Visual Concept-driven Image Generation with Text-to-Image Diffusion Model](https://arxiv.org/abs/2402.11487)
*Tanzila Rahman, Shweta Mahajan, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Leonid Sigal*

Main category: cs.CV

TL;DR: A concept-driven TTI personalization framework is proposed to generate images with multiple interacting or entangled concepts by jointly learning custom tokens and latent segmentation masks.


<details>
  <summary>Details</summary>
Motivation: Existing TTI models struggle with generating images involving multiple interacting or entangled concepts, such as human subjects or overlapping illustrations.

Method: The framework uses an EM-like optimization to alternate between learning custom tokens and estimating latent masks via cross-attention and DenseCRF optimization.

Result: The approach improves token learning for concepts and produces latent masks, enabling better generation of images with three or more entangled concepts.

Conclusion: The proposed method effectively addresses the challenge of generating images with multiple interacting or entangled concepts, demonstrating qualitative and quantitative benefits.

Abstract: Text-to-image (TTI) diffusion models have demonstrated impressive results in
generating high-resolution images of complex and imaginative scenes. Recent
approaches have further extended these methods with personalization techniques
that allow them to integrate user-illustrated concepts (e.g., the user
him/herself) using a few sample image illustrations. However, the ability to
generate images with multiple interacting concepts, such as human subjects, as
well as concepts that may be entangled in one, or across multiple, image
illustrations remains illusive. In this work, we propose a concept-driven TTI
personalization framework that addresses these core challenges. We build on
existing works that learn custom tokens for user-illustrated concepts, allowing
those to interact with existing text tokens in the TTI model. However,
importantly, to disentangle and better learn the concepts in question, we
jointly learn (latent) segmentation masks that disentangle these concepts in
user-provided image illustrations. We do so by introducing an Expectation
Maximization (EM)-like optimization procedure where we alternate between
learning the custom tokens and estimating (latent) masks encompassing
corresponding concepts in user-supplied images. We obtain these masks based on
cross-attention, from within the U-Net parameterized latent diffusion model and
subsequent DenseCRF optimization. We illustrate that such joint alternating
refinement leads to the learning of better tokens for concepts and, as a
by-product, latent masks. We illustrate the benefits of the proposed approach
qualitatively and quantitatively with several examples and use cases that can
combine three or more entangled concepts.

</details>


### [95] [Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel Fields](https://arxiv.org/abs/2405.00998)
*Yuhang Huang, SHilong Zou, Xinwang Liu, Kai Xu*

Main category: cs.CV

TL;DR: A novel latent 3D diffusion model for generating neural voxel fields with accurate part-aware structures, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To achieve high-quality and accurate part-aware generation in neural voxel fields, addressing limitations of current approaches.

Method: Introduces a latent 3D diffusion process for higher-resolution generation and a part-aware shape decoder for accurate part decomposition.

Result: Superior generative capabilities demonstrated across four data classes, outperforming state-of-the-art methods.

Conclusion: The proposed method effectively enhances part-aware shape generation with high-quality rendering.

Abstract: This paper presents a novel latent 3D diffusion model for the generation of
neural voxel fields, aiming to achieve accurate part-aware structures. Compared
to existing methods, there are two key designs to ensure high-quality and
accurate part-aware generation. On one hand, we introduce a latent 3D diffusion
process for neural voxel fields, enabling generation at significantly higher
resolutions that can accurately capture rich textural and geometric details. On
the other hand, a part-aware shape decoder is introduced to integrate the part
codes into the neural voxel fields, guiding the accurate part decomposition and
producing high-quality rendering results. Through extensive experimentation and
comparisons with state-of-the-art methods, we evaluate our approach across four
different classes of data. The results demonstrate the superior generative
capabilities of our proposed method in part-aware shape generation,
outperforming existing state-of-the-art methods.

</details>


### [96] [Robust Classification by Coupling Data Mollification with Label Smoothing](https://arxiv.org/abs/2406.01494)
*Markus Heinonen, Ba-Hien Tran, Michael Kampffmeyer, Maurizio Filippone*

Main category: cs.CV

TL;DR: A novel method combines data mollification (image noising/blurring) with label smoothing to improve robustness and uncertainty quantification in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Enhancing generalization and preparing networks for test-time corruptions by leveraging training-time augmentations.

Method: Coupling data mollification (image noising and blurring) with label smoothing to align label confidences with image degradation.

Result: Improved robustness and uncertainty quantification on corrupted image benchmarks (CIFAR, TinyImageNet, ImageNet).

Conclusion: The method is simple, efficient, and compatible with existing augmentations, offering practical benefits for robustness.

Abstract: Introducing training-time augmentations is a key technique to enhance
generalization and prepare deep neural networks against test-time corruptions.
Inspired by the success of generative diffusion models, we propose a novel
approach of coupling data mollification, in the form of image noising and
blurring, with label smoothing to align predicted label confidences with image
degradation. The method is simple to implement, introduces negligible
overheads, and can be combined with existing augmentations. We demonstrate
improved robustness and uncertainty quantification on the corrupted image
benchmarks of CIFAR, TinyImageNet and ImageNet datasets.

</details>


### [97] [Visual-Friendly Concept Protection via Selective Adversarial Perturbations](https://arxiv.org/abs/2408.08518)
*Xiaoyue Mi, Fan Tang, Juan Cao, Peng Li, Yang Liu*

Main category: cs.CV

TL;DR: VCPro framework balances adversarial perturbation visibility and protection effectiveness for personalized concept generation.


<details>
  <summary>Details</summary>
Motivation: Address legal/ethical concerns in personalized concept generation by improving perturbation perceptibility.

Method: Uses relaxed optimization and Lagrangian multiplier to find less perceptible yet effective perturbations.

Result: VCPro achieves better trade-off between visibility and protection effectiveness.

Conclusion: VCPro effectively protects target concepts with minimal perceptible alterations.

Abstract: Personalized concept generation by tuning diffusion models with a few images
raises potential legal and ethical concerns regarding privacy and intellectual
property rights. Researchers attempt to prevent malicious personalization using
adversarial perturbations. However, previous efforts have mainly focused on the
effectiveness of protection while neglecting the visibility of perturbations.
They utilize global adversarial perturbations, which introduce noticeable
alterations to original images and significantly degrade visual quality. In
this work, we propose the Visual-Friendly Concept Protection (VCPro) framework,
which prioritizes the protection of key concepts chosen by the image owner
through adversarial perturbations with lower perceptibility. To ensure these
perturbations are as inconspicuous as possible, we introduce a relaxed
optimization objective to identify the least perceptible yet effective
adversarial perturbations, solved using the Lagrangian multiplier method.
Qualitative and quantitative experiments validate that VCPro achieves a better
trade-off between the visibility of perturbations and protection effectiveness,
effectively prioritizing the protection of target concepts in images with less
perceptible perturbations.

</details>


### [98] [UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM](https://arxiv.org/abs/2409.00362)
*Mostafa Mansour, Ahmed Abdelsalam, Ari Happonen, Jari Porras, Esa Rahtu*

Main category: cs.CV

TL;DR: UDGS-SLAM integrates UniDepth with Gaussian splatting for monocular SLAM, eliminating RGB-D sensors, achieving high-fidelity results and low trajectory error.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of UniDepth within Gaussian splatting for monocular SLAM, reducing reliance on RGB-D sensors.

Method: Combines UniDepth with Gaussian splatting, uses statistical filtering for depth consistency, and jointly optimizes camera trajectory and scene representation.

Result: High-fidelity rendered images and low trajectory error (ATERMSE), outperforming baselines on TUM RGB-D dataset.

Conclusion: UDGS-SLAM is effective for monocular SLAM, validated by ablation studies and superior performance.

Abstract: Recent advancements in monocular neural depth estimation, particularly those
achieved by the UniDepth network, have prompted the investigation of
integrating UniDepth within a Gaussian splatting framework for monocular SLAM.
This study presents UDGS-SLAM, a novel approach that eliminates the necessity
of RGB-D sensors for depth estimation within Gaussian splatting framework.
UDGS-SLAM employs statistical filtering to ensure local consistency of the
estimated depth and jointly optimizes camera trajectory and Gaussian scene
representation parameters. The proposed method achieves high-fidelity rendered
images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM
is rigorously evaluated using the TUM RGB-D dataset and benchmarked against
several baseline methods, demonstrating superior performance across various
scenarios. Additionally, an ablation study is conducted to validate design
choices and investigate the impact of different network backbone encoders on
system performance.

</details>


### [99] [HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration](https://arxiv.org/abs/2410.01723)
*Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang*

Main category: cs.CV

TL;DR: HarmoniCa introduces a learning-based caching framework for Diffusion Transformers (DiTs) to reduce inference costs by harmonizing training and inference with Step-Wise Denoising Training and an Image Error Proxy-Guided Objective.


<details>
  <summary>Details</summary>
Motivation: DiTs face high inference costs, and existing caching methods overlook prior timestep impact and misalign training-inference objectives, compromising performance and efficiency.

Method: HarmoniCa uses Step-Wise Denoising Training (SDT) for continuity and prior step leverage, and an Image Error Proxy-Guided Objective (IEPO) to balance image quality and cache utilization.

Result: Achieves over 40% latency reduction (2.07x speedup) and improved performance on PixArt-α, with 25% training time reduction.

Conclusion: HarmoniCa effectively addresses DiTs' deployment challenges by aligning training-inference objectives and optimizing cache usage, demonstrating significant speedup and efficiency gains.

Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical
deployment challenges due to high inference costs. Feature caching, which
stores and retrieves redundant computations, offers the potential for
acceleration. Existing learning-based caching, though adaptive, overlooks the
impact of the prior timestep. It also suffers from misaligned
objectives--aligned predicted noise vs. high-quality images--between training
and inference. These two discrepancies compromise both performance and
efficiency. To this end, we harmonize training and inference with a novel
learning-based caching framework dubbed HarmoniCa. It first incorporates
Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising
process, where prior steps can be leveraged. In addition, an Image Error
Proxy-Guided Objective (IEPO) is applied to balance image quality against cache
utilization through an efficient proxy to approximate the image error.
Extensive experiments across $8$ models, $4$ samplers, and resolutions from
$256\times256$ to $2K$ demonstrate superior performance and speedup of our
framework. For instance, it achieves over $40\%$ latency reduction (i.e.,
$2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$.
Remarkably, our image-free approach reduces training time by $25\%$ compared
with the previous method. Our code is available at
https://github.com/ModelTC/HarmoniCa.

</details>


### [100] [$X^2$-DFD: A framework for eXplainable and eXtendable Deepfake Detection](https://arxiv.org/abs/2410.06126)
*Yize Chen, Zhiyuan Yan, Siwei Lyu, Baoyuan Wu*

Main category: cs.CV

TL;DR: The paper introduces ${X}^2$-DFD, a framework to improve deepfake detection and explanation using MLLMs by assessing, strengthening, and supplementing forgery features.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods lack explainability, and pre-trained MLLMs have limited performance. The study aims to enhance MLLMs' capabilities for this task.

Method: The framework includes three modules: Model Feature Assessment (MFA) to rank forgery features, Strong Feature Strengthening (SFS) to fine-tune MLLMs on top features, and Weak Feature Supplementing (WFS) to integrate external detectors for weak features.

Result: Experiments show improved detection and explanation performance.

Conclusion: The proposed ${X}^2$-DFD framework effectively enhances MLLMs' deepfake detection and explainability by leveraging feature analysis and integration.

Abstract: Detecting deepfakes has become an important task. Most existing detection
methods provide only real/fake predictions without offering
human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake
detection have shown improvements in explainability. However, the performance
of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of
understanding of their capabilities for this task and strategies to enhance
them. In this work, we empirically assess the strengths and weaknesses of MLLMs
specifically in deepfake detection via forgery features analysis. Building on
these assessments, we propose a novel framework called ${X}^2$-DFD, consisting
of three core modules. The first module, Model Feature Assessment (MFA),
measures the detection capabilities of forgery features intrinsic to MLLMs, and
gives a descending ranking of these features. The second module, Strong Feature
Strengthening (SFS), enhances the detection and explanation capabilities by
fine-tuning the MLLM on a dataset constructed based on the top-ranked features.
The third module, Weak Feature Supplementing (WFS), improves the fine-tuned
MLLM's capabilities on lower-ranked features by integrating external dedicated
deepfake detectors. To verify the effectiveness of this framework, we further
present a practical implementation, where an automated forgery features
generation, evaluation, and ranking procedure is designed for MFA module; an
automated generation procedure of the fine-tuning dataset containing real and
fake images with explanations based on top-ranked features is developed for SFS
model; an external conventional deepfake detector focusing on blending
artifact, which corresponds to a low detection capability in the pre-trained
MLLM, is integrated for WFS module. Experiments show that our approach enhances
both detection and explanation performance.

</details>


### [101] [EmoGene: Audio-Driven Emotional 3D Talking-Head Generation](https://arxiv.org/abs/2410.17262)
*Wenqing Wang, Yun Fu*

Main category: cs.CV

TL;DR: EmoGene is a framework for generating high-fidelity, emotional talking-head videos using audio input, combining VAE, emotional embedding, and NeRF.


<details>
  <summary>Details</summary>
Motivation: Current audio-driven talking-head generation lacks accurate emotional expressions, which EmoGene aims to address.

Method: Uses a VAE-based audio-to-motion module, motion-to-emotion module with emotional embedding, and NeRF-based emotion-to-video module. Includes pose sampling for idle-state videos.

Result: EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.

Conclusion: EmoGene effectively synthesizes realistic emotional expressions in audio-driven talking-head videos, advancing the field.

Abstract: Audio-driven talking-head generation is a crucial and useful technology for
virtual human interaction and film-making. While recent advances have focused
on improving image fidelity and lip synchronization, generating accurate
emotional expressions remains underexplored. In this paper, we introduce
EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video
portraits with accurate emotional expressions. Our approach employs a
variational autoencoder (VAE)-based audio-to-motion module to generate facial
landmarks, which are concatenated with emotional embedding in a
motion-to-emotion module to produce emotional landmarks. These landmarks drive
a Neural Radiance Fields (NeRF)-based emotion-to-video module to render
realistic emotional talking-head videos. Additionally, we propose a pose
sampling method to generate natural idle-state (non-speaking) videos for silent
audio inputs. Extensive experiments demonstrate that EmoGene outperforms
previous methods in generating high-fidelity emotional talking-head videos.

</details>


### [102] [DivShift: Exploring Domain-Specific Distribution Shifts in Large-Scale, Volunteer-Collected Biodiversity Datasets](https://arxiv.org/abs/2410.19816)
*Elena Sierra, Lauren E. Gillespie, Salim Soltani, Moises Exposito-Alonso, Teja Kattenborn*

Main category: cs.CV

TL;DR: The paper introduces DivShift, a framework to quantify the impact of biases in volunteer-collected biodiversity data on machine learning models, using a curated dataset (DivShift-NAWC) to analyze performance effects.


<details>
  <summary>Details</summary>
Motivation: To understand how geographic, temporal, taxonomic, and sociopolitical biases in citizen science data affect fine-grained species recognition models.

Method: Developed DivShift to quantify distribution shifts and created DivShift-NAWC, a dataset of 7.5M iNaturalist images with expert-verified biases, comparing model performance across bias partitions.

Result: Biases confound model performance less than expected; more data improves performance, but improvements vary by bias type.

Conclusion: While natural world images aid generalization, biases in volunteer-collected data impact model performance, warranting caution in biodiversity monitoring.

Abstract: Large-scale, volunteer-collected datasets of community-identified natural
world imagery like iNaturalist have enabled marked performance gains for
fine-grained visual classification of species using machine learning methods.
However, such data -- sometimes referred to as citizen science data -- are
opportunistic and lack a structured sampling strategy. This volunteer-collected
biodiversity data contains geographic, temporal, taxonomic, observers, and
sociopolitical biases that can have significant effects on biodiversity model
performance, but whose impacts are unclear for fine-grained species recognition
performance. Here we introduce Diversity Shift (DivShift), a framework for
quantifying the effects of domain-specific distribution shifts on machine
learning model performance. To diagnose the performance effects of biases
specific to volunteer-collected biodiversity data, we also introduce DivShift -
North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5
million iNaturalist images across the western coast of North America
partitioned across five types of expert-verified bias. We compare species
recognition performance across these bias partitions using a diverse variety of
species- and ecosystem-focused accuracy metrics. We observe that these biases
confound model performance less than expected from the underlying label
distribution shift, and that more data leads to better model performance but
the magnitude of these improvements are bias-specific. These findings imply
that while the structure within natural world images provides generalization
improvements for biodiversity monitoring tasks, the biases present in
volunteer-collected biodiversity data can also affect model performance; thus
these models should be used with caution in downstream biodiversity monitoring
tasks.

</details>


### [103] [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432)
*Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu*

Main category: cs.CV

TL;DR: Insight-V introduces scalable long-chain reasoning data generation and an effective training pipeline for multi-modal LLMs, enhancing reasoning and perception in vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: High-quality long-chain reasoning data and optimized training pipelines are underexplored in vision-language tasks, limiting LLM reasoning capabilities.

Method: A two-step pipeline generates long reasoning data, and a multi-agent system (reasoning and summary agents) with iterative DPO enhances reasoning quality.

Result: Significant performance gains in multi-modal benchmarks and maintained or improved performance in perception-focused tasks.

Conclusion: Insight-V advances MLLM reasoning and perception, demonstrating scalability and effectiveness in complex vision-language tasks.

Abstract: Large Language Models (LLMs) demonstrate enhanced capabilities and
reliability by reasoning more, evolving from Chain-of-Thought prompting to
product-level solutions like OpenAI o1. Despite various efforts to improve LLM
reasoning, high-quality long-chain reasoning data and optimized training
pipelines still remain inadequately explored in vision-language tasks. In this
paper, we present Insight-V, an early effort to 1) scalably produce long and
robust reasoning data for complex multi-modal tasks, and 2) an effective
training pipeline to enhance the reasoning capabilities of multi-modal large
language models (MLLMs). Specifically, to create long and structured reasoning
data without human labor, we design a two-step pipeline with a progressive
strategy to generate sufficiently long and diverse reasoning paths and a
multi-granularity assessment method to ensure data quality. We observe that
directly supervising MLLMs with such long and complex reasoning data will not
yield ideal reasoning ability. To tackle this problem, we design a multi-agent
system consisting of a reasoning agent dedicated to performing long-chain
reasoning and a summary agent trained to judge and summarize reasoning results.
We further incorporate an iterative DPO algorithm to enhance the reasoning
agent's generation stability and quality. Based on the popular LLaVA-NeXT model
and our stronger base MLLM, we demonstrate significant performance gains across
challenging multi-modal benchmarks requiring visual reasoning. Benefiting from
our multi-agent system, Insight-V can also easily maintain or improve
performance on perception-focused multi-modal tasks.

</details>


### [104] [Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks](https://arxiv.org/abs/2411.16721)
*Han Wang, Gang Wang, Huan Zhang*

Main category: cs.CV

TL;DR: ASTRA is a defense method for Vision Language Models (VLMs) that adaptively steers models away from adversarial feature directions to mitigate harmful outputs efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against adversarial attacks on VLMs are costly and impractical, prompting the need for an efficient solution.

Method: ASTRA identifies harmful response directions via transferable steering vectors, constructed from ablated visual tokens, and applies adaptive activation steering during inference.

Result: ASTRA achieves state-of-the-art performance in mitigating jailbreak risks with minimal impact on benign inputs and shows strong transferability against unseen attacks.

Conclusion: ASTRA provides an efficient and effective defense for VLMs, balancing performance and robustness against adversarial threats.

Abstract: Vision Language Models (VLMs) can produce unintended and harmful content when
exposed to adversarial attacks, particularly because their vision capabilities
create new vulnerabilities. Existing defenses, such as input preprocessing,
adversarial training, and response evaluation-based methods, are often
impractical for real-world deployment due to their high costs. To address this
challenge, we propose ASTRA, an efficient and effective defense by adaptively
steering models away from adversarial feature directions to resist VLM attacks.
Our key procedures involve finding transferable steering vectors representing
the direction of harmful response and applying adaptive activation steering to
remove these directions at inference time. To create effective steering
vectors, we randomly ablate the visual tokens from the adversarial images and
identify those most strongly associated with jailbreaks. These tokens are then
used to construct steering vectors. During inference, we perform the adaptive
steering method that involves the projection between the steering vectors and
calibrated activation, resulting in little performance drops on benign inputs
while strongly avoiding harmful outputs under adversarial inputs. Extensive
experiments across multiple models and baselines demonstrate our
state-of-the-art performance and high efficiency in mitigating jailbreak risks.
Additionally, ASTRA exhibits good transferability, defending against unseen
attacks (i.e., structured-based attack, perturbation-based attack with project
gradient descent variants, and text-only attack). Our code is available at
\url{https://github.com/ASTRAL-Group/ASTRA}.

</details>


### [105] [You KAN Do It in a Single Shot: Plug-and-Play Methods with Single-Instance Priors](https://arxiv.org/abs/2412.06204)
*Yanqi Cheng, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero*

Main category: cs.CV

TL;DR: KAN-PnP introduces Kolmogorov-Arnold Networks (KANs) as denoisers in the Plug-and-Play framework for solving inverse problems with single-instance priors, outperforming existing methods in single-shot learning.


<details>
  <summary>Details</summary>
Motivation: Traditional denoising methods require large datasets, but KAN-PnP addresses the challenge of solving inverse problems with only a single noisy observation.

Method: KAN-PnP incorporates KANs as denoisers within the PnP paradigm, leveraging their Lipschitz continuity and theoretical guarantees for stable optimisation.

Result: KAN-PnP outperforms existing methods in super-resolution and joint optimisation, achieving high accuracy with fewer iterations.

Conclusion: KAN-PnP provides a robust, data-efficient solution for inverse problems, with strong convergence properties and superior performance in single-shot learning.

Abstract: The use of Plug-and-Play (PnP) methods has become a central approach for
solving inverse problems, with denoisers serving as regularising priors that
guide optimisation towards a clean solution. In this work, we introduce
KAN-PnP, an optimisation framework that incorporates Kolmogorov-Arnold Networks
(KANs) as denoisers within the Plug-and-Play (PnP) paradigm. KAN-PnP is
specifically designed to solve inverse problems with single-instance priors,
where only a single noisy observation is available, eliminating the need for
large datasets typically required by traditional denoising methods. We show
that KANs, based on the Kolmogorov-Arnold representation theorem, serve
effectively as priors in such settings, providing a robust approach to
denoising. We prove that the KAN denoiser is Lipschitz continuous, ensuring
stability and convergence in optimisation algorithms like PnP-ADMM, even in the
context of single-shot learning. Additionally, we provide theoretical
guarantees for KAN-PnP, demonstrating its convergence under key conditions: the
convexity of the data fidelity term, Lipschitz continuity of the denoiser, and
boundedness of the regularisation functional. These conditions are crucial for
stable and reliable optimisation. Our experimental results show, on
super-resolution and joint optimisation, that KAN-PnP outperforms exiting
methods, delivering superior performance in single-shot learning with minimal
data. The method exhibits strong convergence properties, achieving high
accuracy with fewer iterations.

</details>


### [106] [Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks](https://arxiv.org/abs/2501.18851)
*Xiaoyan Jiang, Bohan Wang, Xinlong Wan, Shanshan Chen, Hamido Fujita, Hanan Abd. Al Juaid*

Main category: cs.CV

TL;DR: The paper proposes a late fusion method for RGB-D semantic segmentation, addressing misalignment and irregular patches by using GNNs and optimizing depth feature extraction and projection matrix generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from misalignment and irregular patches due to complex feature fusion, and inefficient depth map processing.

Method: Late fusion guided by texture feature prior, GNNs for patch relationship inference, depth map encoding into normal maps, and improved projection matrix generation with Kullback-Leibler Loss and edge weighting.

Result: Improved performance on NYU-DepthV2 and SUN RGB-D datasets.

Conclusion: The proposed method effectively addresses key issues in RGB-D semantic segmentation, enhancing accuracy and consistency.

Abstract: Most existing RGB-D semantic segmentation methods focus on the feature level
fusion, including complex cross-modality and cross-scale fusion modules.
However, these methods may cause misalignment problem in the feature fusion
process and counter-intuitive patches in the segmentation results. Inspired by
the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two
modalities in a late fusion style, during which the geometric feature injection
is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on
the fused feature to alleviate the emergence of irregular patches by inferring
patch relationship. At the 3D feature extraction stage, we argue that
traditional CNNs are not efficient enough for depth maps. So, we encode depth
map into normal map, after which CNNs can easily extract object surface
tendencies.At projection matrix generation stage, we find the existence of
Biased-Assignment and Ambiguous-Locality issues in the original pipeline.
Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no
missing important pixel features, which can be viewed as hard pixel mining
process; 2) connect regions that are close to each other in the Euclidean space
as well as in the semantic space with larger edge weights so that location
informations can been considered. Extensive experiments on two public datasets,
NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost
the performance of RGB-D semantic segmentation task.

</details>


### [107] [Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention](https://arxiv.org/abs/2503.01284)
*Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Md. Jakir Hossen, Nilanjan Dey*

Main category: cs.CV

TL;DR: A hybrid Sequential CNN-GNN framework for soybean leaf disease detection achieves high accuracy (97.16%) and interpretability by combining MobileNetV2 for feature extraction and GraphSAGE for relational modeling.


<details>
  <summary>Details</summary>
Motivation: Challenges in soybean leaf disease detection include visually similar symptoms and limited interpretability in conventional methods, prompting the need for a more accurate and interpretable solution.

Method: Proposes a hybrid Sequential CNN-GNN framework using MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling, with cosine similarity-based adjacency matrices and adaptive neighborhood sampling.

Result: Achieves 97.16% accuracy, outperforming standalone CNNs (≤95.04%) and traditional ML models (≤77.05%), with only 2.3 million parameters for computational efficiency.

Conclusion: The framework bridges accuracy and practicality, offering a robust, interpretable tool for agricultural diagnostics and advancing CNN-GNN integration in plant pathology.

Abstract: Soybean leaf disease detection is critical for agricultural productivity but
faces challenges due to visually similar symptoms and limited interpretability
in conventional methods. While Convolutional Neural Networks (CNNs) excel in
spatial feature extraction, they often neglect inter-image relational
dependencies, leading to misclassifications. This paper proposes an
interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that
synergizes MobileNetV2 for localized feature extraction and GraphSAGE for
relational modeling. The framework constructs a graph where nodes represent
leaf images, with edges defined by cosine similarity-based adjacency matrices
and adaptive neighborhood sampling. This design captures fine-grained lesion
features and global symptom patterns, addressing inter-class similarity
challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM
visualizations, generating heatmaps to highlight disease-influential regions.
Evaluated on a dataset of ten soybean leaf diseases, the model achieves
$97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditional
machine learning models ($\le77.05\%$). Ablation studies validate the
sequential architecture's superiority over parallel or single-model
configurations. With only 2.3 million parameters, the lightweight
MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling
real-time deployment in resource-constrained environments. The proposed
approach bridges the gap between accurate classification and practical
applicability, offering a robust, interpretable tool for agricultural
diagnostics while advancing CNN-GNN integration in plant pathology research.

</details>


### [108] [GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation](https://arxiv.org/abs/2504.02782)
*Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces GPT-ImgEval, a benchmark for evaluating GPT-4o's image generation and editing capabilities, revealing its strong performance and unique architecture.


<details>
  <summary>Details</summary>
Motivation: To quantitatively and qualitatively assess GPT-4o's capabilities in image generation, editing, and semantic synthesis, and to understand its underlying architecture.

Method: Proposes GPT-ImgEval benchmark, evaluates GPT-4o across three tasks, and uses classification-model-based analysis to speculate on its architecture.

Result: GPT-4o outperforms existing methods in image generation and editing, with evidence suggesting an AR-diffusion hybrid architecture.

Conclusion: The work provides insights into GPT-4o's strengths and limitations, offering a benchmark for future research in image generation.

Abstract: The recent breakthroughs in OpenAI's GPT4o model have demonstrated
surprisingly good capabilities in image generation and editing, resulting in
significant excitement in the community. This technical report presents the
first-look evaluation benchmark (named GPT-ImgEval), quantitatively and
qualitatively diagnosing GPT-4o's performance across three critical dimensions:
(1) generation quality, (2) editing proficiency, and (3) world
knowledge-informed semantic synthesis. Across all three tasks, GPT-4o
demonstrates strong performance, significantly surpassing existing methods in
both image generation control and output quality, while also showcasing
exceptional knowledge reasoning capabilities. Furthermore, based on the
GPT-4o's generated data, we propose a classification-model-based approach to
investigate the underlying architecture of GPT-4o, where our empirical results
suggest the model consists of an auto-regressive (AR) combined with a
diffusion-based head for image decoding, rather than the VAR-like
architectures. We also provide a complete speculation on GPT-4o's overall
architecture. In addition, we conduct a series of analyses to identify and
visualize GPT-4o's specific limitations and the synthetic artifacts commonly
observed in its image generation. We also present a comparative study of
multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the
safety implications of GPT-4o's outputs, particularly their detectability by
existing image forensic models. We hope that our work can offer valuable
insight and provide a reliable benchmark to guide future research, foster
reproducibility, and accelerate innovation in the field of image generation and
beyond. The codes and datasets used for evaluating GPT-4o can be found at
https://github.com/PicoTrex/GPT-ImgEval.

</details>


### [109] [MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation](https://arxiv.org/abs/2504.09149)
*Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu*

Main category: cs.CV

TL;DR: MASH is a novel 3D shape representation using spherical distance functions from anchor points, combining implicit and explicit features for versatile applications like reconstruction and generation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve perceptual shape understanding by representing 3D shapes as observable local surface patches, leveraging multi-view geometry.

Method: MASH encodes spherical distance functions with spherical harmonics and uses a parameterized view cone for locality. A differentiable algorithm converts point clouds into MASH representations.

Result: MASH achieves superior performance in surface reconstruction, shape generation, completion, and blending.

Conclusion: MASH's unique representation, combining implicit and explicit features, makes it versatile and effective for various 3D shape applications.

Abstract: We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view
and parametrized representation of 3D shapes. Inspired by multi-view geometry
and motivated by the importance of perceptual shape understanding for learning
3D shapes, MASH represents a 3D shape as a collection of observable local
surface patches, each defined by a spherical distance function emanating from
an anchor point. We further leverage the compactness of spherical harmonics to
encode the MASH functions, combined with a generalized view cone with a
parameterized base that masks the spatial extent of the spherical function to
attain locality. We develop a differentiable optimization algorithm capable of
converting any point cloud into a MASH representation accurately approximating
ground-truth surfaces with arbitrary geometry and topology. Extensive
experiments demonstrate that MASH is versatile for multiple applications
including surface reconstruction, shape generation, completion, and blending,
achieving superior performance thanks to its unique representation encompassing
both implicit and explicit features.

</details>


### [110] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim*

Main category: cs.CV

TL;DR: MoBGS is a new framework for deblurring dynamic 3D Gaussian Splatting, improving sharp and high-quality novel view synthesis from blurry monocular videos.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic novel view synthesis methods struggle with motion blur in videos, degrading rendering quality. Current solutions focus on static scenes, lacking dynamic object motion modeling.

Method: MoBGS introduces Blur-adaptive Latent Camera Estimation (BLCE) for camera motion deblurring and Latent Camera-induced Exposure Estimation (LCEE) for consistent deblurring of camera and object motion.

Result: MoBGS outperforms recent methods (DyBluRF, Deblur4DGS) on the Stereo Blur dataset and real-world videos, achieving state-of-the-art performance.

Conclusion: MoBGS effectively addresses motion blur in dynamic scenes, ensuring temporal consistency and robust motion decomposition.

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>


### [111] [Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy](https://arxiv.org/abs/2504.18317)
*Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang*

Main category: cs.CV

TL;DR: A task-oriented communication framework for UAVs uses multi-view features and edge servers to achieve precise localization in GPS-denied urban areas, with an O-VIB encoder for efficient feature extraction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of UAV localization in GPS-denied urban areas while overcoming bandwidth and processing constraints on lightweight UAVs.

Method: Proposes a task-oriented framework with multi-camera systems, compact multi-view feature extraction, and offloading to edge servers. Introduces O-VIB encoder with ARD and orthogonality constraints for efficient feature pruning.

Result: O-VIB achieves high-precision localization under strict bandwidth budgets, validated on a dedicated LAE UAV dataset.

Conclusion: The framework enables efficient and accurate UAV localization with minimal transmission cost, suitable for LAE applications.

Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles
(UAVs) localization in urban areas where global positioning system (GPS)
signals are unavailable. Vision-based methods offer a viable alternative but
face severe bandwidth, memory and processing constraints on lightweight UAVs.
Inspired by mammalian spatial cognition, we propose a task-oriented
communication framework, where UAVs equipped with multi-camera systems extract
compact multi-view features and offload localization tasks to edge servers. We
introduce the Orthogonally-constrained Variational Information Bottleneck
encoder (O-VIB), which incorporates automatic relevance determination (ARD) to
prune non-informative features while enforcing orthogonality to minimize
redundancy. This enables efficient and accurate localization with minimal
transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows
that O-VIB achieves high-precision localization under stringent bandwidth
budgets. Code and dataset will be made publicly available:
github.com/fangzr/TOC-Edge-Aerial.

</details>


### [112] [ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery](https://arxiv.org/abs/2504.19684)
*Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma*

Main category: cs.CV

TL;DR: A novel framework combining CycleGAN and SigLIP-2 improves weather classification, especially in low-light conditions, reducing day-night performance gaps and computational demands.


<details>
  <summary>Details</summary>
Motivation: Adverse weather conditions challenge safe transportation, requiring robust real-time weather detection from traffic cameras.

Method: Combines CycleGAN for domain adaptation and SigLIP-2 for efficient contrastive learning to enhance weather classification, particularly in nighttime conditions.

Result: Achieves 85.90% nighttime accuracy, reduces training time by 89%, and narrows the day-night performance gap from 33.81 to 8.90 percentage points.

Conclusion: The framework offers a scalable, efficient solution for all-weather classification using existing camera infrastructure.

Abstract: Adverse weather conditions challenge safe transportation, necessitating
robust real-time weather detection from traffic camera imagery. We propose a
novel framework combining CycleGAN-based domain adaptation with efficient
contrastive learning to enhance weather classification, particularly in
low-light nighttime conditions. Our approach leverages the lightweight SigLIP-2
model, which employs pairwise sigmoid loss to reduce computational demands,
integrated with CycleGAN to transform nighttime images into day-like
representations while preserving weather cues. Evaluated on an Iowa Department
of Transportation dataset, the baseline EVA-02 model with CLIP achieves a
per-class overall accuracy of 96.55\% across three weather conditions (No
Precipitation, Rain, Snow) and a day/night overall accuracy of 96.55\%, but
shows a significant day-night gap (97.21\% day vs.\ 63.40\% night). With
CycleGAN, EVA-02 improves to 97.01\% per-class accuracy and 96.85\% day/night
accuracy, boosting nighttime performance to 82.45\%. Our Vision-SigLIP-2 +
Text-SigLIP-2 + CycleGAN + Contrastive configuration excels in nighttime
scenarios, achieving the highest nighttime accuracy of 85.90\%, with 94.00\%
per-class accuracy and 93.35\% day/night accuracy. This model reduces training
time by 89\% (from 6 hours to 40 minutes) and inference time by 80\% (from 15
seconds to 3 seconds) compared to EVA-02. By narrowing the day-night
performance gap from 33.81 to 8.90 percentage points, our framework provides a
scalable, efficient solution for all-weather classification using existing
camera infrastructure.

</details>


### [113] [Empowering Agentic Video Analytics Systems with Video Language Models](https://arxiv.org/abs/2505.00254)
*Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu*

Main category: cs.CV

TL;DR: AVAS is a VLM-powered system for open-ended video analytics, addressing limitations of existing systems with innovations like Event Knowledge Graphs and agentic retrieval-generation. It outperforms benchmarks with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing AI-driven video analytics systems lack adaptability for open-ended tasks and struggle with ultra-long videos. VLMs offer potential but face context window limitations.

Method: AVAS uses Event Knowledge Graphs for efficient video indexing and an agentic retrieval-generation mechanism for handling diverse queries.

Result: AVAS achieves 62.3% and 64.1% accuracy on LVBench and VideoMME-Long, and 75.8% on the new AVAS-100 benchmark.

Conclusion: AVAS demonstrates superior performance in open-ended and ultra-long video analytics, setting a new standard with its innovative approach.

Abstract: AI-driven video analytics has become increasingly pivotal across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Video-Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVAS, a VLM-powered system designed for open-ended, advanced video
analytics. AVAS incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively, significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an
accuracy of 75.8%.

</details>


### [114] [InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method](https://arxiv.org/abs/2505.00512)
*Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Zhenxing Ming, Stewart Worrall*

Main category: cs.CV

TL;DR: A LiDAR-based method for online vehicle-centric intersection localization, fusing semantic road segmentation and vehicle pose to detect and refine intersections, outperforming learning-based baselines.


<details>
  <summary>Details</summary>
Motivation: Intersections are crucial for autonomous vehicle tasks like localization and mapping, but existing methods lack semantic utilization or rely on labeled datasets.

Method: Fuses semantic road segmentation with vehicle pose to detect intersection candidates in BEV, refines them via branch topology analysis and least squares correction.

Result: Outperforms learning-based baselines in accuracy and reliability, with robustness to segmentation errors.

Conclusion: The method is effective and applicable in real-world scenarios, offering a robust solution for intersection localization.

Abstract: Online localization of road intersections is beneficial for autonomous
vehicle localization, mapping and motion planning. Intersections offer strong
landmarks to correct vehicle pose estimation in GNSS dropouts and anchor new
sensor data in up-to-date maps. They are also decisive routing nodes in road
network graphs. Despite that importance, intersection localization has not been
widely studied, with existing methods either ignore the rich semantic
information already computed onboard or depend on scarce, hand-labeled
intersection datasets. To close that gap, this paper presents a LiDAR-based
method for online vehicle-centric intersection localization. We fuse semantic
road segmentation with vehicle local pose to detect intersection candidates in
a bird's eye view (BEV) representation. We then refine those candidates by
analyzing branch topology and correcting the intersection point in a least
squares formulation. To evaluate our method, we introduce an automated
benchmarking pipeline that pairs localized intersection points with
OpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth
poses. Experiments on SemanticKITTI show that the method outperforms the latest
learning-based baseline in accuracy and reliability. Moreover, sensitivity
tests demonstrate that our method is robust to challenging segmentation error
levels, highlighting its applicability in the real world.

</details>


### [115] [Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities](https://arxiv.org/abs/2505.00568)
*Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal*

Main category: cs.CV

TL;DR: BM-MAE introduces a masked image modeling pre-training strategy for multimodal MRI data, enabling adaptation to any modality combination without architectural changes, outperforming baselines and reconstructing missing modalities efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing modalities in multimodal MRI data, which complicates pre-training and fine-tuning, and the impracticality of training separate models for each combination.

Method: Proposes BM-MAE, a masked image modeling pre-training strategy that adapts to any modality subset, capturing intra- and inter-modal information without requiring architectural changes.

Result: Outperforms or competes with baselines requiring separate pre-training, surpasses training from scratch, and efficiently reconstructs missing modalities.

Conclusion: BM-MAE offers a practical, resource-efficient solution for multimodal MRI analysis, enhancing adaptability and performance in downstream tasks.

Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of
investigation for clinicians in the care of brain tumors, providing crucial
insights for surgery planning, treatment monitoring, and biomarker
identification. Pre-training on large datasets have been shown to help models
learn transferable representations and adapt with minimal labeled data. This
behavior is especially valuable in medical imaging, where annotations are often
scarce. However, applying this paradigm to multimodal medical data introduces a
challenge: most existing approaches assume that all imaging modalities are
available during both pre-training and fine-tuning. In practice, missing
modalities often occur due to acquisition issues, specialist unavailability, or
specific experimental designs on small in-house datasets. Consequently, a
common approach involves training a separate model for each desired modality
combination, making the process both resource-intensive and impractical for
clinical use. Therefore, we introduce BM-MAE, a masked image modeling
pre-training strategy tailored for multimodal MRI data. The same pre-trained
model seamlessly adapts to any combination of available modalities, extracting
rich representations that capture both intra- and inter-modal information. This
allows fine-tuning on any subset of modalities without requiring architectural
changes, while still benefiting from a model pre-trained on the full set of
modalities. Extensive experiments show that the proposed pre-training strategy
outperforms or remains competitive with baselines that require separate
pre-training for each modality subset, while substantially surpassing training
from scratch on several downstream tasks. Additionally, it can quickly and
efficiently reconstruct missing modalities, highlighting its practical value.
Code and trained models are available at: https://github.com/Lucas-rbnt/BM-MAE

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva, Juliane Päßler, S. Lizeth Tapia Tarifa, Einar Broch Johnsen, Carlos Hernández Corbato*

Main category: cs.AI

TL;DR: ROSA is a knowledge-based framework for robot self-adaptation, enabling task-and-architecture co-adaptation (TACA) in diverse environments. It uses a knowledge model for runtime reasoning and has been implemented in ROS 2, showing benefits in reusability and development effort.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots face challenges in handling diverse tasks and environments due to uncertainties, requiring adaptable software architectures and task logic.

Method: ROSA provides a knowledge model for adaptation and runtime reasoning to decide when and how to adapt. It includes a ROS 2-based implementation.

Result: Experimental evaluation in an underwater robotics application demonstrates ROSA's feasibility, reusability, and reduced development effort.

Conclusion: ROSA effectively enables self-adaptation in robotic systems, addressing challenges of diverse environments and tasks.

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>


### [117] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee, Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: A subexponential upper bound for Howard's Policy Iteration (HPI) on deterministic MDPs (DMDPs) is introduced, improving previous exponential bounds.


<details>
  <summary>Details</summary>
Motivation: Despite HPI's long history, its running time bounds remain poorly understood, with exponential upper bounds and linear lower bounds. This paper aims to bridge this gap.

Method: The study focuses on HPI's performance on DMDPs, parameterizing the upper bound by reward bit-size and ensuring independence from the discount factor.

Result: A subexponential upper bound for HPI on DMDPs is established, also applicable to DMDPs with two arbitrary-sized rewards.

Conclusion: The paper significantly advances the understanding of HPI's efficiency, particularly for deterministic MDPs.

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>


### [118] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou, Danae Pla Karidi, Evaggelia Pitoura, Emmanouil Panagiotou, Eirini Ntoutsi*

Main category: cs.AI

TL;DR: The paper explores using explainability methods to detect and interpret unfairness in AI systems, proposing a pipeline for fairness insights and addressing critical questions about their reliability.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about AI fairness and transparency, especially for protected groups, drive the need to integrate explainability with fairness for responsible AI.

Method: A pipeline integrating local post-hoc explanation methods is proposed to derive fairness-related insights, addressing key questions about explanation reliability.

Result: The results demonstrate the potential of explanation methods for fairness but emphasize careful consideration of critical aspects like consistency and trustworthiness.

Conclusion: Explainability methods can aid fairness detection, but their use requires addressing key challenges to ensure reliable and trustworthy outcomes.

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>


### [119] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang, Xing Niu, Juyong Kim, Jie Shen, Tong Zhang, Jeremy C. Weiss*

Main category: cs.AI

TL;DR: The paper introduces MIMIC-4-Ext-22MCTS, a dataset of 22.5M clinical time series events extracted from MIMIC-IV-Note discharge summaries, using a novel framework for processing lengthy texts and inferring timestamps. Fine-tuned models show significant improvements in healthcare tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in processing lengthy discharge summaries and extracting clinical events with timestamps for reliable clinical risk prediction.

Method: Proposes a framework: 1) chunking discharge summaries, 2) using contextual BM25 and semantic search to identify event-rich chunks, and 3) leveraging Llama-3.1-8B to infer temporal information.

Result: Fine-tuned BERT and GPT-2 models achieve 10% and 3% improvements in accuracy for medical QA and clinical trial matching, respectively, with more clinically reliable outputs.

Conclusion: The dataset and framework enhance clinical risk prediction by improving model performance and reliability in healthcare applications.

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>


### [120] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike, Emanuel Moss, Elizabeth Anne Watkins, Saurav Sahay, Giuseppe Raffa, Lama Nachman*

Main category: cs.AI

TL;DR: Agentic pipelines challenge explainability in LLMs; CoT reasoning lacks effectiveness in improving outputs or user understanding.


<details>
  <summary>Details</summary>
Motivation: To address the transparency and explainability challenges in agentic pipelines involving multiple LLMs.

Method: Implementation of a perceptive task guidance system with quantitative and qualitative analysis of CoT reasoning.

Result: CoT reasoning fails to enhance outputs or provide meaningful explainability for end users.

Conclusion: Current CoT approaches in agentic pipelines are insufficient for actionable explainability.

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>


### [121] [Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression](https://arxiv.org/abs/2505.00876)
*Sahar Torkhesari, Behnam Yousefimehr, Mehdi Ghatee*

Main category: cs.AI

TL;DR: A sensor health monitoring system for vehicles uses machine learning and deep learning to detect and estimate sensor failures with 99% accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance vehicle safety and maintenance by proactively monitoring sensor health in driver assistance systems.

Method: Combines autoencoders for failure detection and random forest regression for value estimation, using correlated sensor data.

Result: Achieved 99% accuracy in detecting and estimating sensor failures in Saipa's Quick vehicle.

Conclusion: The system effectively identifies and mitigates sensor failures, improving vehicle reliability and safety.

Abstract: Driver assistance systems provide a wide range of crucial services, including
closely monitoring the condition of vehicles. This paper showcases a
groundbreaking sensor health monitoring system designed for the automotive
industry. The ingenious system leverages cutting-edge techniques to process
data collected from various vehicle sensors. It compares their outputs within
the Electronic Control Unit (ECU) to evaluate the health of each sensor. To
unravel the intricate correlations between sensor data, an extensive
exploration of machine learning and deep learning methodologies was conducted.
Through meticulous analysis, the most correlated sensor data were identified.
These valuable insights were then utilized to provide accurate estimations of
sensor values. Among the diverse learning methods examined, the combination of
autoencoders for detecting sensor failures and random forest regression for
estimating sensor values proved to yield the most impressive outcomes. A
statistical model using the normal distribution has been developed to identify
possible sensor failures proactively. By comparing the actual values of the
sensors with their estimated values based on correlated sensors, faulty sensors
can be detected early. When a defective sensor is detected, both the driver and
the maintenance department are promptly alerted. Additionally, the system
replaces the value of the faulty sensor with the estimated value obtained
through analysis. This proactive approach was evaluated using data from twenty
essential sensors in the Saipa's Quick vehicle's ECU, resulting in an
impressive accuracy rate of 99\%.

</details>


### [122] [Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models](https://arxiv.org/abs/2505.00972)
*Yuewen Mei, Tong Nie, Jian Sun, Ye Tian*

Main category: cs.AI

TL;DR: An online, retrieval-augmented LLM framework generates safety-critical driving scenarios for AVs, outperforming baselines by reducing collision risks.


<details>
  <summary>Details</summary>
Motivation: Existing scenario generation methods overfit common patterns or lack interactivity, missing rare safety-critical cases.

Method: Uses an LLM-based behavior analyzer to infer dangerous intents, then queries LLM agents for adversarial trajectories, augmented with a dynamic memorization-retrieval bank.

Result: Reduces mean minimum time-to-collision from 1.62 to 1.08 s and achieves a 75% collision rate, outperforming baselines.

Conclusion: The framework effectively generates safety-critical scenarios, improving AV testing.

Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs),
yet existing scenario generation methods either overfit to common driving
patterns or operate in an offline, non-interactive manner that fails to expose
rare, safety-critical corner cases. In this paper, we introduce an online,
retrieval-augmented large language model (LLM) framework for generating
safety-critical driving scenarios. Our method first employs an LLM-based
behavior analyzer to infer the most dangerous intent of the background vehicle
from the observed state, then queries additional LLM agents to synthesize
feasible adversarial trajectories. To mitigate catastrophic forgetting and
accelerate adaptation, we augment the framework with a dynamic memorization and
retrieval bank of intent-planner pairs, automatically expanding its behavioral
library when novel intents arise. Evaluations using the Waymo Open Motion
Dataset demonstrate that our model reduces the mean minimum time-to-collision
from 1.62 to 1.08 s and incurs a 75% collision rate, substantially
outperforming baselines.

</details>


### [123] [Improving Large Language Model Planning with Action Sequence Similarity](https://arxiv.org/abs/2505.01009)
*Xinran Zhao, Hanie Sedghi, Bernd Bohnet, Dale Schuurmans, Azade Nova*

Main category: cs.AI

TL;DR: The paper explores improving LLM planning via in-context learning, proposing GRASE-DC for exemplar selection based on action sequence similarity, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Understanding what signals in context influence LLM planning performance and improving it through better exemplar selection.

Method: Proposes GRASE-DC, a two-stage pipeline: re-sampling high AS exemplars and dynamic clustering for relevance and diversity.

Result: GRASE-DC improves planning accuracy by 11-40 points, needing 27.3% fewer exemplars. GRASE-DC* + VAL boosts performance by 18.9%.

Conclusion: GRASE-DC effectively enhances LLM planning, generalizing to out-of-distribution problems with consistent improvements across benchmarks.

Abstract: Planning is essential for artificial intelligence systems to look ahead and
proactively determine a course of actions to reach objectives in the virtual
and real world. Recent work on large language models (LLMs) sheds light on
their planning capability in various tasks. However, it remains unclear what
signals in the context influence the model performance. In this work, we
explore how to improve the model planning capability through in-context
learning (ICL), specifically, what signals can help select the exemplars.
Through extensive experiments, we observe that commonly used problem similarity
may result in false positives with drastically different plans, which can
mislead the model. In response, we propose to sample and filter exemplars
leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a
two-stage pipeline that first re-samples high AS exemplars and then curates the
selected exemplars with dynamic clustering on AS to achieve a balance of
relevance and diversity. Our experimental result confirms that GRASE-DC
achieves significant performance improvement on various planning tasks (up to
~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on
average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a
validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of
GRASE-DC with various backbone LLMs and on both classical planning and natural
language planning benchmarks. GRASE-DC can further boost the planning accuracy
by ~24 absolute points on harder problems using simpler problems as exemplars
over a random baseline. This demonstrates its ability to generalize to
out-of-distribution problems.

</details>


### [124] [Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory](https://arxiv.org/abs/2505.01028)
*Huy Q. Ngo, Mingyu Guo, Hung Nguyen*

Main category: cs.AI

TL;DR: The paper introduces an Adaptive Path Removal Problem to minimize manual effort in hardening Windows AD systems by optimizing interactions between IT admins and a security wizard.


<details>
  <summary>Details</summary>
Motivation: Manual verification of security fixes in AD systems is labor-intensive, prompting the need for an automated, efficient solution.

Method: Formulates the problem, proves its complexity, and proposes exact, approximate, and heuristic algorithms (including DPR).

Result: DPR outperforms other methods, scaling well on large graphs and real-world AD attack graphs.

Conclusion: The approach effectively reduces human effort in securing AD systems, with DPR being the most scalable and efficient solution.

Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are
typically modeled using an attack graph and hardening AD systems involves an
iterative workflow: security teams propose an edge to remove, and IT operations
teams manually review these fixes before implementing the removal. As
verification requires significant manual effort, we formulate an Adaptive Path
Removal Problem to minimize the number of steps in this iterative removal
process. In our model, a wizard proposes an attack path in each step and
presents it as a set of multiple-choice options to the IT admin. The IT admin
then selects one edge from the proposed set to remove. This process continues
until the target $t$ is disconnected from source $s$ or the number of proposed
paths reaches $B$. The model aims to optimize the human effort by minimizing
the expected number of interactions between the IT admin and the security
wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then
propose a set of solutions including an exact algorithm, an approximate
algorithm, and several scalable heuristics. Our best heuristic, called DPR, can
operate effectively on larger-scale graphs compared to the exact algorithm and
consistently outperforms the approximate algorithm across all graphs. We verify
the effectiveness of our algorithms on several synthetic AD graphs and an AD
attack graph collected from a real organization.

</details>


### [125] [Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation](https://arxiv.org/abs/2505.01073)
*Zongyuan Li, Pengfei Li, Runnan Qi, Yanan Ni, Lumin Jiang, Hui Wu, Xuebo Zhang, Kuihua Huang, Xian Guo*

Main category: cs.AI

TL;DR: Retrial-Augmented Learning (RAL) is a reward-free, self-supervised framework for LLMs that avoids training, using retrieval-augmented generation to reduce hallucination and improve decision-making at low cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of domain-specific data in LLM pre-training and high computational costs of post-training for specialized applications.

Method: RAL uses Retrieval-Augmented Generation (RAG) for autonomous knowledge generation in three stages: hypothesis proposal, validation, and knowledge generation, tested in the LLM-PySC2 environment.

Result: RAL reduces hallucination, improves decision-making performance, and shows promise in OOD tasks, robustness, and transferability.

Conclusion: RAL is a cost-effective solution for decision-making and autonomous knowledge generation in specialized domains.

Abstract: The lack of domain-specific data in the pre-training of Large Language Models
(LLMs) severely limits LLM-based decision systems in specialized applications,
while post-training a model in the scenarios requires significant computational
resources. In this paper, we present Retrial-Augmented Learning (RAL), a
reward-free self-supervised learning framework for LLMs that operates without
model training. By developing Retrieval-Augmented Generation (RAG) into a
module for organizing intermediate data, we realized a three-stage autonomous
knowledge generation of proposing a hypothesis, validating the hypothesis, and
generating the knowledge. The method is evaluated in the LLM-PySC2 environment,
a representative decision-making platform that combines sufficient complexity
with domain-specific knowledge requirements. Experiments demonstrate that the
proposed method effectively reduces hallucination by generating and utilizing
validated knowledge, and increases decision-making performance at an extremely
low cost. Meanwhile, the approach exhibits potential in
out-of-distribution(OOD) tasks, robustness, and transferability, making it a
cost-friendly but effective solution for decision-making problems and
autonomous knowledge generation.

</details>


### [126] [MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark](https://arxiv.org/abs/2505.01081)
*Sébastien Ferré*

Main category: cs.AI

TL;DR: MADIL, a new AI method using MDL for efficient learning, shows promise in ARC tasks but lags behind LLMs in performance while being more efficient and interpretable.


<details>
  <summary>Details</summary>
Motivation: AI struggles with efficient skill acquisition and generalization, prompting the need for methods like MADIL to address these limitations.

Method: MADIL leverages the Minimum Description Length (MDL) principle for pattern-based decomposition and structured generalization.

Result: MADIL achieved 7% performance at ArcPrize 2024, below LLMs but with better efficiency and interpretability.

Conclusion: MADIL offers a viable alternative to LLMs for ARC tasks, balancing performance with efficiency and interpretability.

Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized
tasks but struggles with efficient skill acquisition and generalization. The
Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based
on minimal training requirements. While Large Language Models (LLMs) have
recently improved ARC performance, they rely on extensive pre-training and high
computational costs. We introduce MADIL (MDL-based AI), a novel approach
leveraging the Minimum Description Length (MDL) principle for efficient
inductive learning. MADIL performs pattern-based decomposition, enabling
structured generalization. While its performance (7% at ArcPrize 2024) remains
below LLM-based methods, it offers greater efficiency and interpretability.
This paper details MADIL's methodology, its application to ARC, and
experimental evaluations.

</details>


### [127] [Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms](https://arxiv.org/abs/2505.01181)
*Mehrdad Asadi, Roxana Rădulescu, Ann Nowé*

Main category: cs.AI

TL;DR: The paper proposes a framework using explainable AI to analyze data poisoning attacks in swarming systems, showing that poisoning above 10% leads to inefficient cooperation.


<details>
  <summary>Details</summary>
Motivation: Swarming systems are vulnerable to data poisoning attacks, which disrupt team-level coordination. The study aims to understand and diagnose these attacks.

Method: The framework models agent interactions using evolutionary intelligence, poisons the swarm model systematically, and applies explainable AI to quantify and diagnose poisoning effects.

Result: Poisoning above 10% causes non-optimal strategies and inefficient cooperation, identifiable through explainable AI methods.

Conclusion: Explainable AI effectively diagnoses data poisoning in swarming systems, highlighting vulnerabilities and enabling mitigation strategies.

Abstract: Swarming systems, such as for example multi-drone networks, excel at
cooperative tasks like monitoring, surveillance, or disaster assistance in
critical environments, where autonomous agents make decentralized decisions in
order to fulfill team-level objectives in a robust and efficient manner.
Unfortunately, team-level coordinated strategies in the wild are vulnerable to
data poisoning attacks, resulting in either inaccurate coordination or
adversarial behavior among the agents. To address this challenge, we contribute
a framework that investigates the effects of such data poisoning attacks, using
explainable AI methods. We model the interaction among agents using
evolutionary intelligence, where an optimal coalition strategically emerges to
perform coordinated tasks. Then, through a rigorous evaluation, the swarm model
is systematically poisoned using data manipulation attacks. We showcase the
applicability of explainable AI methods to quantify the effects of poisoning on
the team strategy and extract footprint characterizations that enable
diagnosing. Our findings indicate that when the model is poisoned above 10%,
non-optimal strategies resulting in inefficient cooperation can be identified.

</details>


### [128] [Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions](https://arxiv.org/abs/2505.01192)
*Federico Maria Cau, Lucio Davide Spano*

Main category: cs.AI

TL;DR: The study explores how AI information and explanation styles (example-based, feature-based, rule-based, counterfactual) impact decision-making in a loan scenario, focusing on Need for Cognition (NFC) traits. Findings show high AI confidence boosts reliance and reduces cognitive load, while counterfactual explanations improve accuracy. No NFC-based differences were found, suggesting the need for personalized XAI interfaces.


<details>
  <summary>Details</summary>
Motivation: To understand how AI information and diverse explanation styles affect decision-making accuracy, reliance on AI, and cognitive load, especially considering personality traits like NFC.

Method: Conducted a loan application scenario study, testing AI information (prediction, confidence, accuracy) and explanation styles (example-based, feature-based, rule-based, counterfactual) on low and high NFC individuals.

Result: High AI confidence increased reliance and reduced cognitive load. Counterfactual explanations improved accuracy despite lower understandability. No NFC-based differences in accuracy or cognitive load were found.

Conclusion: User-centric personalization in XAI interfaces, incorporating diverse explanation styles and exploring user traits, is crucial for optimizing human-AI collaboration.

Abstract: Artificial Intelligence (AI) systems are increasingly used for
decision-making across domains, raising debates over the information and
explanations they should provide. Most research on Explainable AI (XAI) has
focused on feature-based explanations, with less attention on alternative
styles. Personality traits like the Need for Cognition (NFC) can also lead to
different decision-making outcomes among low and high NFC individuals. We
investigated how presenting AI information (prediction, confidence, and
accuracy) and different explanation styles (example-based, feature-based,
rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive
load in a loan application scenario. We also examined low and high NFC
individuals' differences in prioritizing XAI interface elements (loan
attributes, AI information, and explanations), accuracy, and cognitive load.
Our findings show that high AI confidence significantly increases reliance on
AI while reducing cognitive load. Feature-based explanations did not enhance
accuracy compared to other conditions. Although counterfactual explanations
were less understandable, they enhanced overall accuracy, increasing reliance
on AI and reducing cognitive load when AI predictions were correct. Both low
and high NFC individuals prioritized explanations after loan attributes,
leaving AI information as the least important. However, we found no significant
differences between low and high NFC groups in accuracy or cognitive load,
raising questions about the role of personality traits in AI-assisted
decision-making. These findings highlight the need for user-centric
personalization in XAI interfaces, incorporating diverse explanation styles and
exploring multiple personality traits and other user characteristics to
optimize human-AI collaboration.

</details>


### [129] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/abs/2505.01305)
*Lo Pang-Yun Ting, Hong-Pei Chen, An-Shan Liu, Chun-Yin Yeh, Po-Lin Chen, Kun-Ta Chuang*

Main category: cs.AI

TL;DR: TARL is a novel method using shapelet-transition knowledge graphs to analyze heart rate data from wearables, improving early detection of patient deterioration by modeling shapelet dynamics and handling missing values.


<details>
  <summary>Details</summary>
Motivation: Early detection of patient deterioration is vital for reducing mortality, but challenges like diverse heart rate data and missing values in wearable data hinder effective monitoring.

Method: TARL models structural relationships of shapelets in heart rate time series, creating a knowledge graph to track illness progression. It uses transition-aware knowledge embedding to quantify missing values and predict future trends.

Result: Experiments on ICU data show TARL achieves high reliability and early detection. A case study demonstrates its explainable process for clinicians.

Conclusion: TARL is a promising AI-driven tool for early illness detection, aiding clinicians in recognizing deterioration signs.

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>


### [130] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)
*Dongliang Guo, Mengxuan Hu, Zihan Guan, Thomas Hartvigsen, Sheng Li*

Main category: cs.AI

TL;DR: The paper introduces BalancEdit, a method for balanced model editing in multi-modal models, addressing the generality-locality trade-off. It uses a unique mechanism to dynamically balance edits without altering model weights, validated by the OKEDIT dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional fine-tuning is impractical for updating large multi-modal models, and current editing techniques overlook the influence ranges of facts, compromising performance.

Method: Proposes BalancEdit, which generates positive/negative samples to determine fact influence scope and uses a localized codebook for edits, avoiding weight modifications.

Result: BalancEdit achieves minimal trade-offs between generality and locality while maintaining robust editing capabilities.

Conclusion: The approach is the first to explicitly address the generality-locality trade-off in multi-modal model editing, with promising results.

Abstract: Large multi-modal models inevitably decay over time as facts change and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose BalancEdit, a novel method for balanced model editing
that dynamically achieves an optimal balance between generality and locality.
BalancEdit utilizes a unique mechanism that generates both positive and
negative samples for each fact to accurately determine its influence scope and
incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
will be available.

</details>


### [131] [Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning](https://arxiv.org/abs/2311.09830)
*Katharina Stein, Daniel Fišer, Jörg Hoffmann, Alexander Koller*

Main category: cs.AI

TL;DR: The paper automates the conversion of PDDL planning domains into natural language prompts for LLMs, showing comparable performance to manual prompts and enabling large-scale evaluation of LLM planning in PDDL.


<details>
  <summary>Details</summary>
Motivation: To assess and automate the capability of LLMs in reasoning and planning, particularly in the context of PDDL planning, by generating NL prompts automatically.

Method: Automated conversion of PDDL domains into NL prompts using an LLM, followed by large-scale experiments to evaluate LLM planning performance.

Result: Automated NL prompts perform similarly to manual ones, outperform PDDL and template-based prompts, but lag behind symbolic planners like A* with LM-cut.

Conclusion: Automation enables broader evaluation of LLM planning, revealing its limitations compared to symbolic planners but showing potential in scalability for certain domains.

Abstract: Large language models (LLMs) have revolutionized a large variety of NLP
tasks. An active debate is to what extent they can do reasoning and planning.
Prior work has assessed the latter in the specific context of PDDL planning,
based on manually converting three PDDL domains into natural language (NL)
prompts. Here we automate this conversion step, showing how to leverage an LLM
to automatically generate NL prompts from PDDL input. Our automatically
generated NL prompts result in similar LLM-planning performance as the previous
manually generated ones. Beyond this, the automation enables us to run much
larger experiments, providing for the first time a broad evaluation of LLM
planning performance in PDDL. Our NL prompts yield better performance than PDDL
prompts and simple template-based NL prompts. Compared to symbolic planners,
LLM planning lags far behind; but in some domains, our best LLM configuration
scales up further than A$^\star$ using LM-cut.

</details>


### [132] [Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables](https://arxiv.org/abs/2403.04577)
*Aneta Koleva, Martin Ringsquandl, Ahmed Hatem, Thomas Runkler, Volker Tresp*

Main category: cs.AI

TL;DR: The paper introduces Wiki-TabNER, a challenging dataset for table interpretation tasks, focusing on named entity recognition (NER) and entity linking, and proposes a prompting framework for evaluating large language models.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for table interpretation are overly simplified, limiting their effectiveness for thorough evaluation and real-world representation.

Method: The authors extract and annotate Wiki-TabNER, a dataset with complex tables and multiple entities per cell, labeled using DBpedia classes. They also propose a prompting framework for model evaluation.

Result: Wiki-TabNER is presented as a more challenging benchmark, and qualitative analysis reveals model challenges and dataset limitations.

Conclusion: The dataset and framework aim to advance table interpretation tasks, though limitations are acknowledged.

Abstract: Interest in solving table interpretation tasks has grown over the years, yet
it still relies on existing datasets that may be overly simplified. This is
potentially reducing the effectiveness of the dataset for thorough evaluation
and failing to accurately represent tables as they appear in the real-world. To
enrich the existing benchmark datasets, we extract and annotate a new, more
challenging dataset. The proposed Wiki-TabNER dataset features complex tables
containing several entities per cell, with named entities labeled using DBpedia
classes. This dataset is specifically designed to address named entity
recognition (NER) task within tables, but it can also be used as a more
challenging dataset for evaluating the entity linking task. In this paper we
describe the distinguishing features of the Wiki-TabNER dataset and the
labeling process. In addition, we propose a prompting framework for evaluating
the new large language models on the within tables NER task. Finally, we
perform qualitative analysis to gain insights into the challenges encountered
by the models and to understand the limitations of the proposed~dataset.

</details>


### [133] [A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](https://arxiv.org/abs/2412.07446)
*Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev Lal*

Main category: cs.AI

TL;DR: GPT models may implicitly learn a world model from token prediction, enabling zero-shot causal structure learning with high confidence for sequences with encoded causal structure.


<details>
  <summary>Details</summary>
Motivation: To investigate if GPT models, trained for token prediction, implicitly learn a causal world model and can perform zero-shot causal structure learning.

Method: Derive a causal interpretation of GPT's attention mechanism, propose a causal world model, and test GPT on out-of-distribution synthetic data (Othello and Chess sequences).

Result: GPT generates legal next moves for sequences with encoded causal structure but fails for illegal moves, correlating with causal structure capture.

Conclusion: GPT's attention mechanism likely encodes a causal world model, enabling zero-shot causal structure learning for sequences with high confidence.

Abstract: Do generative pre-trained transformer (GPT) models, trained only to predict
the next token, implicitly learn a world model from which a sequence is
generated one token at a time? We address this question by deriving a causal
interpretation of the attention mechanism in GPT, and suggesting a causal world
model that arises from this interpretation. Furthermore, we propose that GPT
models, at inference time, can be utilized for zero-shot causal structure
learning for input sequences and present a confidence score. Empirical
evaluation is conducted in a controlled environment using the setup and rules
of the Othello and Chess strategy games. A GPT, pre-trained on real-world games
played with the intention of winning, is tested on out-of-distribution
synthetic data consisting of sequences of random legal moves. We find that the
GPT model is likely to generate legal next moves for out-of-distribution
sequences for which a causal structure is encoded in the attention mechanism
with high confidence. In cases for which the GPT model generates illegal moves
it also fails to capture any causal structure.

</details>


### [134] [LLM-PySC2: Starcraft II learning environment for Large Language Models](https://arxiv.org/abs/2411.05348)
*Zongyuan Li, Yanan Ni, Runnan Qi, Lumin Jiang, Chang Lu, Xiaojie Xu, Xiangbei Liu, Pengfei Li, Yunzheng Guo, Zhe Ma, Huanyu Li, Hui Wu, Xian Guo, Kuihua Huang, Xuebo Zhang*

Main category: cs.AI

TL;DR: The paper introduces LLM-PySC2, an environment enabling LLMs to interact with StarCraft II's pysc2 backend, addressing action space and multi-agent collaboration challenges. Results show LLMs' potential but highlight issues like hallucinations and inefficiency.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap where LLMs lack support for StarCraft II's complex action space and multi-agent collaboration, hindering their application in decision-making tasks.

Method: Proposes LLM-PySC2, an environment with full pysc2 action space, multi-modal data, and Wiki knowledge, using an asynchronous query architecture for scalable LLM interaction.

Result: LLMs show potential in complex scenarios but struggle with consistent correctness, especially in multi-agent settings, and suffer from hallucinations without task-specific guidance.

Conclusion: StarCraft II remains challenging for LLMs, and LLM-PySC2 provides a foundation for future LLM-based decision-making research.

Abstract: The tremendous potential has been demonstrated by large language models
(LLMs) in intelligent decision-making problems, with unprecedented capabilities
shown across diverse applications ranging from gaming AI systems to complex
strategic planning frameworks. However, the StarCraft II platform, which has
been widely adopted for validating decision-making algorithms in the past
decade, has not yet provided substantial support for this emerging domain. To
address issues that LLMs cannot interface with the hundreds of actions of the
pysc2 backend and the lack of native support for multi-agent (MA)
collaboration, we propose the LLM-PySC2 environment. This is the first
environment that offers LLMs the complete pysc2 action space with sufficient
multi-modal information and game Wiki knowledge. With an asynchronous query
architecture, the environment efficiently interacts with LLMs that maintain a
constant latency regardless of the scale of the agents' population. In the
experiments, we evaluated LLMs' decision-making performance in both the
macro-decision and micro-operation scenarios, with traditional StarCraft II
Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results
indicate that LLMs possess the potential to achieve victories in complex
scenarios but cannot constantly generate correct decisions, especially in the
recovered pysc2 action space and MA settings. Without task-relevant
instructions, the pre-trained models suffer from issues such as hallucinations
and inefficient collaboration. Our findings suggest that StarCraft II still
challenges in the era of large models, revealing that there is a lot to do to
develop an advanced LLM decision-making system, and the proposed LLM-PySC2
environment will support future development of LLM-based decision-making
solutions.

</details>


### [135] [Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution](https://arxiv.org/abs/2411.14995)
*Jonas Gösgens, Niklas Jansen, Hector Geffner*

Main category: cs.AI

TL;DR: A novel method for learning STRIPS action models from traces is introduced, combining scalability, soundness, and completeness without domain restrictions.


<details>
  <summary>Details</summary>
Motivation: Learning STRIPS action models from traces alone is challenging due to the need to infer domain predicates.

Method: Uses an efficient test to check predicate consistency with action traces, then completes the domain with preconditions and static predicates.

Result: The method is theoretically and experimentally validated, including on large-scale domains like the 8-puzzle.

Conclusion: The approach is scalable, general, and effective for learning action models from traces.

Abstract: Learning STRIPS action models from action traces alone is a challenging
problem as it involves learning the domain predicates as well. In this work, a
novel approach is introduced which, like the well-known LOCM systems, is
scalable, but like SAT approaches, is sound and complete. Furthermore, the
approach is general and imposes no restrictions on the hidden domain or the
number or arity of the predicates. The new learning method is based on an
\emph{efficient, novel test} that checks whether the assumption that a
predicate is affected by a set of action patterns, namely, actions with
specific argument positions, is consistent with the traces. The predicates and
action patterns that pass the test provide the basis for the learned domain
that is then easily completed with preconditions and static predicates. The new
method is studied theoretically and experimentally. For the latter, the method
is evaluated on traces and graphs obtained from standard classical domains like
the 8-puzzle, which involve hundreds of thousands of states and transitions.
The learned representations are then verified on larger instances.

</details>


### [136] [ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease Detection and Microbiome-Clinical Data Integration](https://arxiv.org/abs/2501.08324)
*Ziyuan Huang, Vishaldeep Kaur Sekhon, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran*

Main category: cs.AI

TL;DR: ADAM is a multi-agent LLM framework for Alzheimer's disease analysis, integrating multimodal data for improved classification and insights.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding and classification of Alzheimer's disease by leveraging diverse data sources and contextualizing findings with literature.

Method: Uses a multi-agent reasoning LLM framework to analyze microbiome profiles, clinical data, and external knowledge bases.

Result: Outperforms XGBoost with higher F1 scores and lower variance, showing robustness with human biological data.

Conclusion: ADAM is effective for binary classification but aims to expand to more data types and disease progression prediction in future.

Abstract: Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large
language model (LLM) framework designed to integrate and analyze multimodal
data, including microbiome profiles, clinical datasets, and external knowledge
bases, to enhance the understanding and classification of Alzheimer's disease
(AD). By leveraging the agentic system with LLM, ADAM produces insights from
diverse data sources and contextualizes the findings with literature-driven
evidence. A comparative evaluation with XGBoost revealed a significantly
improved mean F1 score and significantly reduced variance for ADAM,
highlighting its robustness and consistency, particularly when utilizing human
biological data. Although currently tailored for binary classification tasks
with two data modalities, future iterations will aim to incorporate additional
data types, such as neuroimaging and peripheral biomarkers, and expand them to
predict disease progression, thereby broadening ADAM's scalability and
applicability in AD research and diagnostic applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [137] [GVPT -- A software for guided visual pitch tracking](https://arxiv.org/abs/2505.00750)
*Hyunjin Cho, Farhad Tabasi, Jeremy D. Greenlee, Rahul Singh*

Main category: cs.SD

TL;DR: GVPT is real-time pitch-tracking software for vocal training, offering visual feedback, difficulty adjustment, and session logging for clinical and research use.


<details>
  <summary>Details</summary>
Motivation: To aid vocal pitch control in clinical and research settings by providing real-time visual feedback.

Method: Uses visual target pitch contours and overlays the user's pitch in real-time to guide vocal reproduction.

Result: Enables precise pitch tracking and supports customizable difficulty and session logging.

Conclusion: GVPT is a versatile tool for vocal pitch control exercises in therapy and experiments.

Abstract: GVPT (Guided visual pitch tracking) is a publicly available, real-time pitch
tracking software designed to guide and evaluate vocal pitch control using
visual feedback. Developed for clinical and research applications, the system
presents various visual target pitch contour and overlays the subject's pitch
in real-time to promote accurate vocal reproduction. GVPT supports difficulty
modification, session logging, and precise pitch tracking. The software enables
voice pitch control exercise in both experimental and therapeutic settings.

</details>


### [138] [SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation](https://arxiv.org/abs/2505.00839)
*Ahmad Suleman, Yazeed Alkhrijah, Misha Urooj Khan, Hareem Khan, Muhammad Abdullah Husnain Ali Faiz, Mohamad A. Alawad, Zeeshan Kaleem, Guan Gui*

Main category: cs.SD

TL;DR: The paper introduces the SMSAT dataset and a deep learning framework (CAM) to analyze the affective and physiological impacts of auditory stimuli (spiritual meditation, music, natural silence), achieving 99.99% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: To advance affective computing and mental health technologies by understanding how auditory stimuli influence emotional and physiological states.

Method: Developed the SMSAT dataset and a contrastive learning-based audio encoder, along with the CAM framework integrating 25 features for affective state classification.

Result: Achieved 99.99% classification accuracy, outperforming existing methods (90% accuracy). Significant physiological fluctuations were observed in cardiac responses.

Conclusion: The work provides a validated dataset and scalable framework for applications in stress monitoring, mental well-being, and therapeutic interventions.

Abstract: Understanding how auditory stimuli influence emotional and physiological
states is fundamental to advancing affective computing and mental health
technologies. In this paper, we present a multimodal evaluation of the
affective and physiological impacts of three auditory conditions, that is,
spiritual meditation (SM), music (M), and natural silence (NS), using a
comprehensive suite of biometric signal measures. To facilitate this analysis,
we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT)
dataset, a novel benchmark comprising acoustic time series (ATS) signals
recorded under controlled exposure protocols, with careful attention to
demographic diversity and experimental consistency. To model the auditory
induced states, we develop a contrastive learning based SMSAT audio encoder
that extracts highly discriminative embeddings from ATS data, achieving 99.99%
classification accuracy in interclass and intraclass evaluations. Furthermore,
we propose the Calmness Analysis Model (CAM), a deep learning framework
integrating 25 handcrafted and learned features for affective state
classification across auditory conditions, attaining robust 99.99%
classification accuracy. In contrast, pairwise t tests reveal significant
deviations in cardiac response characteristics (CRC) between SM analysis via
ANOVA inducing more significant physiological fluctuations. Compared to
existing state of the art methods reporting accuracies up to 90%, the proposed
model demonstrates substantial performance gains (up to 99%). This work
contributes a validated multimodal dataset and a scalable deep learning
framework for affective computing applications in stress monitoring, mental
well-being, and therapeutic audio-based interventions.

</details>


### [139] [Binamix -- A Python Library for Generating Binaural Audio Datasets](https://arxiv.org/abs/2505.01369)
*Dan Barry, Davoud Shariat Panah, Alessandro Ragano, Jan Skoglund, Andrew Hines*

Main category: cs.SD

TL;DR: Binamix is an open-source Python library for generating binaural audio datasets using the SADIE II Database, aiding in spatial audio research and applications.


<details>
  <summary>Details</summary>
Motivation: The demand for spatial audio in VR, immersive media, and research requires robust tools for creating binaural datasets for testing and validation.

Method: Binamix uses the SADIE II Database for HRIR/BRIR data, employing Delaunay triangulation for interpolation and offering customizable mixing and rendering.

Result: The library provides a flexible framework for large-scale dataset creation, supporting various parameters and speaker layouts.

Conclusion: Binamix advances spatial audio research by offering an open-source, reproducible solution for binaural rendering and dataset generation.

Abstract: The increasing demand for spatial audio in applications such as virtual
reality, immersive media, and spatial audio research necessitates robust
solutions to generate binaural audio data sets for use in testing and
validation. Binamix is an open-source Python library designed to facilitate
programmatic binaural mixing using the extensive SADIE II Database, which
provides Head Related Impulse Response (HRIR) and Binaural Room Impulse
Response (BRIR) data for 20 subjects. The Binamix library provides a flexible
and repeatable framework for creating large-scale spatial audio datasets,
making it an invaluable resource for codec evaluation, audio quality metric
development, and machine learning model training. A range of pre-built example
scripts, utility functions, and visualization plots further streamline the
process of custom pipeline creation. This paper presents an overview of the
library's capabilities, including binaural rendering, impulse response
interpolation, and multi-track mixing for various speaker layouts. The tools
utilize a modified Delaunay triangulation technique to achieve accurate
HRIR/BRIR interpolation where desired angles are not present in the data. By
supporting a wide range of parameters such as azimuth, elevation, subject
Impulse Responses (IRs), speaker layouts, mixing controls, and more, the
library enables researchers to create large binaural datasets for any
downstream purpose. Binamix empowers researchers and developers to advance
spatial audio applications with reproducible methodologies by offering an
open-source solution for binaural rendering and dataset generation. We release
the library under the Apache 2.0 License at
https://github.com/QxLabIreland/Binamix/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [140] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre, Ana L. C. Bazzan, André Barreto, Bruno C. da Silva*

Main category: cs.LG

TL;DR: The paper introduces a method to construct an optimal behavior basis for multi-task reinforcement learning, ensuring zero-shot optimal solutions for linear tasks and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational expense and scalability issues of current methods like GPI and CCS, and the dependency of OK on base policies, by finding an optimal set of base policies.

Method: The authors propose a novel method to efficiently construct an optimal behavior basis, reducing the number of base policies needed and ensuring optimality in new tasks.

Result: The method significantly outperforms state-of-the-art approaches, especially in complex tasks, and is proven more expressive than CCS.

Conclusion: The introduced optimal behavior basis solves the open problem of zero-shot optimal solutions for linear tasks and extends to some non-linear tasks, demonstrating superior performance empirically.

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>


### [141] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen, Ngoc N. Tran, Khai Nguyen, Richard G. Baraniuk*

Main category: cs.LG

TL;DR: The paper introduces Similarity-Aware (S)MoE and Attention-Aware (S)MoE to address routing fluctuations in Sparse Mixture of Experts (SMoE) models, improving robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: SMoE models face routing fluctuations and non-robustness due to independent expert-selection of tokens, limiting their effectiveness.

Method: Proposes (S)MoE and (S)MoE-Attention blocks, leveraging token similarities and attention matrices to guide expert selection.

Result: Theoretical proof and empirical validation show reduced entropy in expert selection, leading to stable routing, improved accuracy, and robustness.

Conclusion: The proposed methods effectively mitigate routing fluctuations, enhancing SMoE model performance and reliability.

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>


### [142] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev, Dan A Calian, Luisa M Zintgraf, Gregory Farquhar, Hado van Hasselt*

Main category: cs.LG

TL;DR: MixFlow-MG improves efficiency in gradient-based bilevel optimization by using mixed-mode differentiation, achieving significant memory and time savings.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in gradient-of-a-gradient calculations in bilevel optimization, which are computationally expensive and suboptimal with standard methods.

Method: Proposes Mixed-Flow Meta-Gradients (MixFlow-MG), leveraging mixed-mode differentiation to optimize computational graphs.

Result: Achieves over 10x memory savings and up to 25% faster wall-clock time compared to standard implementations.

Conclusion: MixFlow-MG is a practical solution for scalable and efficient bilevel optimization in meta-learning and related fields.

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [143] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde, Louis Jaburi*

Main category: cs.LG

TL;DR: Mechanistic Interpretability (MI) is a principled approach to understanding neural networks by extracting implicit explanations. The paper defines MI, introduces Explanatory Faithfulness, and proposes the Principle of Explanatory Optimism as a precondition for MI's success.


<details>
  <summary>Details</summary>
Motivation: To establish MI as a distinct and rigorous interpretability paradigm by defining its scope, limits, and necessary preconditions.

Method: Proposes definitions for MI and Explanatory Faithfulness, and formulates the Principle of Explanatory Optimism.

Result: MI is characterized as Model-level, Ontic, Causal-Mechanistic, and Falsifiable, distinguishing it from other interpretability approaches.

Conclusion: MI is a viable and principled approach to understanding neural networks, contingent on the Principle of Explanatory Optimism.

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>


### [144] [MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network](https://arxiv.org/abs/2503.12623)
*Vrushank Ahire, Kunal Shah, Mudasir Nazir Khan, Nikhil Pakhale, Lownish Rai Sookha, M. A. Ganaie, Abhinav Dhall*

Main category: cs.LG

TL;DR: MAVEN improves dynamic emotion recognition by integrating multi-modal cues with a bi-directional cross-modal attention mechanism, outperforming baseline models.


<details>
  <summary>Details</summary>
Motivation: Dynamic emotion recognition is challenging due to transient expressions and misaligned multi-modal cues. Traditional methods ignore correlations between valence and arousal.

Method: MAVEN uses visual, audio, and textual modalities with modality-specific encoders and a bi-directional cross-modal attention mechanism, predicting emotions in polar coordinates.

Result: MAVEN achieved a CCC of 0.3061 on Aff-Wild2, surpassing the ResNet-50 baseline (CCC 0.22).

Conclusion: MAVEN captures transient emotional expressions and improves real-world emotion recognition, with code publicly available.

Abstract: Dynamic emotion recognition in the wild remains challenging due to the
transient nature of emotional expressions and temporal misalignment of
multi-modal cues. Traditional approaches predict valence and arousal and often
overlook the inherent correlation between these two dimensions. The proposed
Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates
visual, audio, and textual modalities through a bi-directional cross-modal
attention mechanism. MAVEN uses modality-specific encoders to extract features
from synchronized video frames, audio segments, and transcripts, predicting
emotions in polar coordinates following Russell's circumplex model. The
evaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance
correlation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline
model with a CCC of 0.22. The multistage architecture captures the subtle and
transient nature of emotional expressions in conversational videos and improves
emotion recognition in real-world situations. The code is available at:
https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW

</details>


### [145] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana, Moeumu Uili, James Atherton, Mark O'Brien, Joe Wood, Leandra Brickson*

Main category: cs.LG

TL;DR: An automated one-shot bird call classification pipeline for rare species, achieving high accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing classifiers lack support for rare species with few recordings, hindering conservation efforts.

Method: Leverages embedding spaces of large bird classifiers, uses cosine similarity, and applies preprocessing for optimization.

Result: Achieved 1.0 recall and 0.95 accuracy in detecting calls of the critically endangered tooth-billed pigeon.

Conclusion: The system is a practical, open-source tool for monitoring rare and endangered bird species.

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [146] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: A scalable methodology combining BM25, sentence embeddings, Bayesian optimization, and a transformer-based classifier was developed for harmonizing inconsistent units in clinical datasets, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of data interoperability in large-scale clinical datasets by harmonizing inconsistent units, enabling seamless data reuse and reliable multi-institutional studies.

Method: A multi-stage pipeline involving filtering, identification, harmonization proposal generation, automated re-ranking, and manual validation, using a hybrid approach of BM25 and sentence embeddings with a transformer-based reranker.

Result: The hybrid approach (MRR: 0.8833) outperformed lexical-only (0.7985) and embedding-only (0.5277) methods, with the reranker further improving MRR to 0.9833. Precision at rank 1 was 83.39%, and recall at rank 5 was 94.66%.

Conclusion: The framework provides an efficient, scalable solution for unit harmonization, reducing manual effort and improving accuracy, ensuring consistent data reuse across healthcare systems.

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


### [147] [Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization](https://arxiv.org/abs/2505.00812)
*Kuan Zhang, Chengliang Chai, Jingzhe Xu, Chi Zhang, Ye Yuan, Guoren Wang, Lei Cao*

Main category: cs.LG

TL;DR: A novel two-stage noisy learning framework improves generalization in deep neural networks by dynamically modeling sample cleanliness and difficulty, reducing computational time by 75%.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks degrade under noisy supervision, and existing methods are computationally expensive or require heavy tuning.

Method: A two-stage framework with a dynamically weighted loss function and a 'wrong event' metric for noise modeling, followed by noise-robust training.

Result: Outperforms state-of-the-art methods, reduces computational time by 75%, and improves scalability.

Conclusion: The proposed framework effectively addresses noisy supervision challenges with efficiency and performance gains.

Abstract: Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.

</details>


### [148] [Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures](https://arxiv.org/abs/2505.00818)
*Heng-Sheng Chang, Prashant G. Mehta*

Main category: cs.LG

TL;DR: A mathematical framework for causal nonlinear prediction in HMM settings, inspired by decoder-only transformers, is proposed. It uses optimal control to reformulate prediction as a fixed-point problem, solved by the dual filter algorithm, which parallels transformer architectures.


<details>
  <summary>Details</summary>
Motivation: To derive transformer-like architectures from first principles for solving prediction problems, rather than modeling transformers directly.

Method: Reformulates prediction as an optimal control problem, leading to a fixed-point equation solved by the dual filter algorithm.

Result: The dual filter algorithm parallels transformer architectures and is validated through numerical experiments.

Conclusion: The framework provides a principled derivation of transformer-like architectures for prediction, with connections to prior work on probability measure transport.

Abstract: This paper presents a mathematical framework for causal nonlinear prediction
in settings where observations are generated from an underlying hidden Markov
model (HMM). Both the problem formulation and the proposed solution are
motivated by the decoder-only transformer architecture, in which a finite
sequence of observations (tokens) is mapped to the conditional probability of
the next token. Our objective is not to construct a mathematical model of a
transformer. Rather, our interest lies in deriving, from first principles,
transformer-like architectures that solve the prediction problem for which the
transformer is designed. The proposed framework is based on an original optimal
control approach, where the prediction objective (MMSE) is reformulated as an
optimal control problem. An analysis of the optimal control problem is
presented leading to a fixed-point equation on the space of probability
measures. To solve the fixed-point equation, we introduce the dual filter, an
iterative algorithm that closely parallels the architecture of decoder-only
transformers. These parallels are discussed in detail along with the
relationship to prior work on mathematical modeling of transformers as
transport on the space of probability measures. Numerical experiments are
provided to illustrate the performance of the algorithm using parameter values
used in researchscale transformer models.

</details>


### [149] [Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](https://arxiv.org/abs/2505.00823)
*Qianxi Fu, Youngjoon Suh, Xiaojing Zhang, Yoonjin Won*

Main category: cs.LG

TL;DR: A data-driven framework using CGANs infers temperature fields from phase contours in pool boiling, achieving <6% error and demonstrating the potential of deep generative models for thermal transport analysis.


<details>
  <summary>Details</summary>
Motivation: Quantitative characterization of multiphase heat transfer is limited by challenges in measuring temperature fields in chaotic flows, and computational methods struggle with complex experimental conditions.

Method: A conditional generative adversarial network (CGAN) is trained with high-speed imaging data and simulation to reconstruct temperature fields from geometric phase contours.

Result: The model achieves temperature field reconstruction with errors below 6%, and data augmentation enhances accuracy and plausibility.

Conclusion: Deep generative models can bridge the gap between observable multiphase phenomena and thermal transport, augmenting experimental measurements in complex systems.

Abstract: Phase change plays a critical role in thermal management systems, yet
quantitative characterization of multiphase heat transfer remains limited by
the challenges of measuring temperature fields in chaotic, rapidly evolving
flow regimes. While computational methods offer spatiotemporal resolution in
idealized cases, replicating complex experimental conditions remains
prohibitively difficult. Here, we present a data-driven framework that
leverages a conditional generative adversarial network (CGAN) to infer
temperature fields from geometric phase contours in a canonical pool boiling
configuration where advanced data collection techniques are restricted. Using
high-speed imaging data and simulation-informed training, our model
demonstrates the ability to reconstruct temperature fields with errors below
6%. We further show that standard data augmentation strategies are effective in
enhancing both accuracy and physical plausibility of the predicted maps across
both simulation and experimental datasets when precise physical constraints are
not applicable. Our results highlight the potential of deep generative models
to bridge the gap between observable multiphase phenomena and underlying
thermal transport, offering a powerful approach to augment and interpret
experimental measurements in complex two-phase systems.

</details>


### [150] [Intersectional Divergence: Measuring Fairness in Regression](https://arxiv.org/abs/2505.00830)
*Joe Germino, Nuno Moniz, Nitesh V. Chawla*

Main category: cs.LG

TL;DR: The paper introduces Intersectional Divergence (ID), a fairness measure for regression tasks, addressing gaps in existing work by considering multiple protected attributes and domain preferences. It also proposes IDLoss for optimization, improving fairness without compromising predictive performance.


<details>
  <summary>Details</summary>
Motivation: Existing fairness research focuses on classification, neglecting regression tasks and intersectional fairness. The paper aims to bridge these gaps by considering multiple protected attributes and domain-specific preferences.

Method: The authors propose Intersectional Divergence (ID) to measure fairness in regression, extending it to IDLoss for optimization. Experiments evaluate ID's insights and IDLoss's impact on fairness and performance.

Result: ID provides unique fairness insights, and IDLoss improves both single-attribute and intersectional fairness while maintaining predictive performance.

Conclusion: The paper successfully addresses regression fairness gaps with ID and IDLoss, demonstrating their effectiveness in balancing fairness and performance.

Abstract: Research on fairness in machine learning has been mainly framed in the
context of classification tasks, leaving critical gaps in regression. In this
paper, we propose a seminal approach to measure intersectional fairness in
regression tasks, going beyond the focus on single protected attributes from
existing work to consider combinations of all protected attributes.
Furthermore, we contend that it is insufficient to measure the average error of
groups without regard for imbalanced domain preferences. To this end, we
propose Intersectional Divergence (ID) as the first fairness measure for
regression tasks that 1) describes fair model behavior across multiple
protected attributes and 2) differentiates the impact of predictions in target
ranges most relevant to users. We extend our proposal demonstrating how ID can
be adapted into a loss function, IDLoss, and used in optimization problems.
Through an extensive experimental evaluation, we demonstrate how ID allows
unique insights into model behavior and fairness, and how incorporating IDLoss
into optimization can considerably improve single-attribute and intersectional
model fairness while maintaining a competitive balance in predictive
performance.

</details>


### [151] [IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain](https://arxiv.org/abs/2505.00837)
*Julen Ercibengoa, Meritxell Gómez-Omella, Izaro Goienetxea*

Main category: cs.LG

TL;DR: IberFire is a high-resolution spatio-temporal datacube for wildfire prediction in Spain, integrating 260 features from open-access sources to support ML/DL modeling and climate analysis.


<details>
  <summary>Details</summary>
Motivation: Wildfires are a critical environmental issue in Mediterranean regions like Spain, but existing datasets lack localised and fine-grained data.

Method: Developed IberFire, a 1 km x 1 km x 1-day resolution datacube with 260 features across eight categories, using open-source tools and open-access data.

Result: IberFire enhances granularity and feature diversity, supports ML/DL techniques, and is publicly available on Zenodo.

Conclusion: IberFire provides a reproducible, open-access dataset for wildfire risk modeling and strategic planning, fostering collaboration in research.

Abstract: Wildfires pose a critical environmental issue to ecosystems, economies, and
public safety, particularly in Mediterranean regions such as Spain. Accurate
predictive models rely on high-resolution spatio-temporal data to capture the
complex interplay of environmental and anthropogenic factors. To address the
lack of localised and fine-grained datasets in Spain, this work introduces
IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering
mainland Spain and the Balearic Islands from December 2007 to December 2024.
IberFire integrates 260 features across eight main categories: auxiliary
features, fire history, geography, topography, meteorology, vegetation indices,
human activity, and land cover. All features are derived from open-access
sources, ensuring transparency and real-time applicability. The data processing
pipeline was implemented entirely using open-source tools, and the codebase has
been made publicly available. This work not only enhances spatio-temporal
granularity and feature diversity compared to existing European datacubes but
also provides a reproducible methodology for constructing similar datasets.
IberFire supports advanced wildfire risk modelling through Machine Learning
(ML) and Deep Learning (DL) techniques, enables climate pattern analysis and
informs strategic planning in fire prevention and land management. The dataset
is publicly available on Zenodo to promote open research and collaboration.

</details>


### [152] [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)
*Xinlin Li, Osama Hanna, Christina Fragouli, Suhas Diggavi*

Main category: cs.LG

TL;DR: ICQuant is a novel framework for efficient outlier-aware weight-only quantization in LLMs, reducing bit overhead and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of outliers in weight quantization, which inflate ranges and cause errors, without high bit overhead.

Method: Leverages outlier statistics to design an index coding scheme for outlier-aware quantization, compatible with existing quantizers.

Result: ICQuant requires only ≈0.3 bits to halve the quantization range, improving 2-bit Llama3-70B zero-shot accuracy by up to 130-150%.

Conclusion: ICQuant outperforms existing outlier suppression techniques and matches fine-tuned quantizers without fine-tuning.

Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for
efficient low-bit post-training quantization (PTQ), due to their high memory
costs. A key challenge in weight quantization is the presence of outliers,
which inflate quantization ranges and lead to large errors. While a number of
outlier suppression techniques have been proposed, they either: fail to
effectively shrink the quantization range, or incur (relatively) high bit
overhead. In this paper, we present ICQuant, a novel framework that leverages
outlier statistics to design an efficient index coding scheme for outlier-aware
weight-only quantization. Compared to existing outlier suppression techniques
requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant
requires only $\approx 0.3$ bits; a significant saving in extreme compression
regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing
quantizers to eliminate outliers, improving the quantization quality. Using
just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the
zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%
relative to QTIP and QuIP#; and it achieves comparable performance to the
best-known fine-tuned quantizer (PV-tuning) without fine-tuning.

</details>


### [153] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/abs/2505.00887)
*Xi Chen, Yateng Tang, Jiarong Xu, Jiawei Zhang, Siwei Zhang, Sijia Peng, Xuehao Zheng, Yun Xiong*

Main category: cs.LG

TL;DR: The paper introduces LeTE, a learnable time encoding method, to address the limitations of existing methods in handling diverse and complex real-world time patterns.


<details>
  <summary>Details</summary>
Motivation: Existing time encoding methods often rely on specific inductive biases, limiting their effectiveness in modeling diverse temporal dynamics.

Method: Proposes LeTE, which uses deep function learning to parameterize non-linear transformations, making time encoding learnable and adaptable to generalized time patterns.

Result: LeTE outperforms previous methods, demonstrating versatility and effectiveness across diverse domains.

Conclusion: LeTE provides a flexible and powerful framework for modeling complex time patterns, integrating seamlessly into various tasks.

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>


### [154] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman, Igor Gitman, Evelina Bakhturina*

Main category: cs.LG

TL;DR: NeMo-Inspector is an open-source tool for analyzing and improving synthetic datasets, demonstrated to reduce low-quality samples and enhance model accuracy.


<details>
  <summary>Details</summary>
Motivation: Large, high-quality datasets are needed for LLM adaptation, but synthetic data quality is hard to ensure manually.

Method: Introduces NeMo-Inspector, a tool for automated analysis and refinement of synthetic datasets.

Result: Reduced low-quality samples from 46.99% to 19.51% in GSM-Plus and improved model accuracy by 1.92% (MATH) and 4.17% (GSM8K).

Conclusion: NeMo-Inspector effectively simplifies synthetic dataset analysis, improving data quality and model performance.

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>


### [155] [Learning Neural Control Barrier Functions from Offline Data with Conservatism](https://arxiv.org/abs/2505.00908)
*Ihab Tabbara, Hussein Sibai*

Main category: cs.LG

TL;DR: A deep learning algorithm trains conservative control barrier functions (CCBFs) from offline data to enhance safety and avoid unreliable states, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the curse of dimensionality in safety filter synthesis and improve reliability by preventing unsafe and out-of-distribution states.

Method: Proposes an algorithm inspired by Conservative Q-learning to train CCBFs from offline datasets, focusing on safety and distributional robustness.

Result: CCBFs outperform existing methods in safety maintenance and out-of-distribution avoidance without compromising task performance.

Conclusion: The proposed CCBFs offer a robust solution for safe control, addressing limitations of existing synthesis methods.

Abstract: Safety filters, particularly those based on control barrier functions, have
gained increased interest as effective tools for safe control of dynamical
systems. Existing correct-by-construction synthesis algorithms, however, suffer
from the curse of dimensionality. Deep learning approaches have been proposed
in recent years to address this challenge. In this paper, we contribute to this
line of work by proposing an algorithm for training control barrier functions
from offline datasets. Our algorithm trains the filter to not only prevent the
system from reaching unsafe states but also out-of-distribution ones, at which
the filter would be unreliable. It is inspired by Conservative Q-learning, an
offline reinforcement learning algorithm. We call its outputs Conservative
Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs
outperform existing methods in maintaining safety and out-of-distribution
avoidance while minimally affecting task performance.

</details>


### [156] [Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems](https://arxiv.org/abs/2505.00909)
*Xianjin Yang, Jingguo Zhang*

Main category: cs.LG

TL;DR: A GP-based policy iteration framework for HJB equations and MFGs, using additive Schwarz acceleration for improved convergence.


<details>
  <summary>Details</summary>
Motivation: Address forward and inverse problems in HJB equations and MFGs efficiently.

Method: Alternate between solving value functions and updating policies using GP approximation, with Schwarz acceleration for convergence.

Result: Closed-form solutions for policy evaluation and improved computational efficiency with Schwarz acceleration.

Conclusion: The framework effectively solves HJB and MFG problems with enhanced convergence.

Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for
addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)
equations and mean field games (MFGs). Policy iteration is formulated as an
alternating procedure between solving the value function under a fixed control
policy and updating the policy based on the resulting value function. By
exploiting the linear structure of GPs for function approximation, each policy
evaluation step admits an explicit closed-form solution, eliminating the need
for numerical optimization. To improve convergence, we incorporate the additive
Schwarz acceleration as a preconditioning step following each policy update.
Numerical experiments demonstrate the effectiveness of Schwarz acceleration in
improving computational efficiency.

</details>


### [157] [Fine-Tuning without Performance Degradation](https://arxiv.org/abs/2505.00913)
*Han Wang, Adam White, Martha White*

Main category: cs.LG

TL;DR: The paper addresses performance degradation and slow learning in offline-to-online policy fine-tuning, introducing a new algorithm (Jump Start) that improves fine-tuning speed and reduces degradation.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning policies learned offline often leads to performance degradation or slow learning, highlighting the need for better methods.

Method: Introduces the Jump Start algorithm, which gradually increases exploration based on online performance estimates.

Result: The new algorithm achieves faster fine-tuning and significantly reduces performance degradation compared to existing methods.

Conclusion: The Jump Start algorithm effectively addresses fine-tuning challenges, offering practical improvements for offline-to-online learning.

Abstract: Fine-tuning policies learned offline remains a major challenge in application
domains. Monotonic performance improvement during \emph{fine-tuning} is often
challenging, as agents typically experience performance degradation at the
early fine-tuning stage. The community has identified multiple difficulties in
fine-tuning a learned network online, however, the majority of progress has
focused on improving learning efficiency during fine-tuning. In practice, this
comes at a serious cost during fine-tuning: initially, agent performance
degrades as the agent explores and effectively overrides the policy learned
offline. We show across a range of settings, many offline-to-online algorithms
exhibit either (1) performance degradation or (2) slow learning (sometimes
effectively no improvement) during fine-tuning. We introduce a new fine-tuning
algorithm, based on an algorithm called Jump Start, that gradually allows more
exploration based on online estimates of performance. Empirically, this
approach achieves fast fine-tuning and significantly reduces performance
degradations compared with existing algorithms designed to do the same.

</details>


### [158] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang, Yingbin Liang, Jing Yang*

Main category: cs.LG

TL;DR: The paper explores how a one-layer transformer learns regular language tasks ('even pairs' and 'parity check') through theoretical analysis and experiments, revealing distinct training phases.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers learn and solve fundamental language recognition tasks, focusing on regular languages like 'even pairs' and 'parity check'.

Method: Theoretical analysis of training dynamics under gradient descent for a one-layer transformer (attention + linear layer), with experiments validating the findings.

Result: Two-phase training: rapid attention layer growth for separability, followed by logarithmic linear layer growth toward a max-margin solution. Loss decreases at O(1/t).

Conclusion: Transformers can solve 'even pairs' directly but require Chain-of-Thought for 'parity check'. Training dynamics reveal separable phases, validated experimentally.

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [159] [Compact Recurrent Transformer with Persistent Memory](https://arxiv.org/abs/2505.00929)
*Edison Mucllari, Zachary Daniels, David Zhang, Qiang Ye*

Main category: cs.LG

TL;DR: The paper introduces the Compact Recurrent Transformer (CRT), combining shallow Transformers with RNNs to efficiently handle long sequences with reduced compute overhead, outperforming full-length Transformers in language tasks and achieving state-of-the-art results in video classification.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with long sequences due to quadratic self-attention complexity. Existing methods introduce compute overhead, limiting their use in resource-constrained settings like edge computing.

Method: CRT integrates shallow Transformers for local segments with RNNs to manage a persistent memory vector for global information, reducing segment size and FLOPs.

Result: CRT matches or surpasses full-length Transformers in language tasks (WordPTB, WikiText-103) with shorter segments and fewer FLOPs, and excels in video classification (Toyota Smarthome).

Conclusion: CRT offers an efficient solution for long-sequence tasks, balancing performance and resource constraints, making it suitable for edge computing and similar applications.

Abstract: The Transformer architecture has shown significant success in many language
processing and visual tasks. However, the method faces challenges in
efficiently scaling to long sequences because the self-attention computation is
quadratic with respect to the input length. To overcome this limitation,
several approaches scale to longer sequences by breaking long sequences into a
series of segments, restricting self-attention to local dependencies between
tokens within each segment and using a memory mechanism to manage information
flow between segments. However, these approached generally introduce additional
compute overhead that restricts them from being used for applications where
limited compute memory and power are of great concern (such as edge computing).
We propose a novel and efficient Compact Recurrent Transformer (CRT), which
combines shallow Transformer models that process short local segments with
recurrent neural networks to compress and manage a single persistent memory
vector that summarizes long-range global information between segments. We
evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as
well as on the Toyota Smarthome video dataset for classification. CRT achieves
comparable or superior prediction results to full-length Transformers in the
language datasets while using significantly shorter segments (half or quarter
size) and substantially reduced FLOPs. Our approach also demonstrates
state-of-the-art performance on the Toyota Smarthome video dataset.

</details>


### [160] [Robust Root Cause Diagnosis using In-Distribution Interventions](https://arxiv.org/abs/2505.00930)
*Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, Amit Sharma*

Main category: cs.LG

TL;DR: IDI is a novel algorithm for diagnosing root causes in complex systems by using in-distribution interventions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing unreliable counterfactual estimates in anomaly diagnosis due to rare anomalies falling outside training distributions.

Method: Uses interventional estimates from a fitted SCM at in-distribution inputs to assess root cause criteria (anomaly and fix).

Result: IDI outperforms nine state-of-the-art baselines in accuracy and robustness on synthetic and PetShop datasets.

Conclusion: IDI provides a more reliable and accurate approach for root cause diagnosis in complex systems.

Abstract: Diagnosing the root cause of an anomaly in a complex interconnected system is
a pressing problem in today's cloud services and industrial operations. We
propose In-Distribution Interventions (IDI), a novel algorithm that predicts
root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes
should take on anomalous values; 2) **Fix:** had the root cause nodes assumed
usual values, the target node would not have been anomalous. Prior methods of
assessing the fix condition rely on counterfactuals inferred from a Structural
Causal Model (SCM) trained on historical data. But since anomalies are rare and
fall outside the training distribution, the fitted SCMs yield unreliable
counterfactual estimates. IDI overcomes this by relying on interventional
estimates obtained by solely probing the fitted SCM at in-distribution inputs.
We present a theoretical analysis comparing and bounding the errors in
assessing the fix condition using interventional and counterfactual estimates.
We then conduct experiments by systematically varying the SCM's complexity to
demonstrate the cases where IDI's interventional approach outperforms the
counterfactual approach and vice versa. Experiments on both synthetic and
PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies
true root causes more accurately and robustly than nine existing
state-of-the-art RCD baselines. Code is released at
https://github.com/nlokeshiisc/IDI_release.

</details>


### [161] [A Self-Supervised Transformer for Unusable Shared Bike Detection](https://arxiv.org/abs/2505.00932)
*Yin Huang, Yongqi Dong, Youhua Tang, Alvaro García Hernandez*

Main category: cs.LG

TL;DR: A novel Self-Supervised Transformer (SSTransformer) framework is proposed to detect faulty bikes in bike-sharing systems, outperforming existing methods with high accuracy and precision.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting faulty bikes either ignore dynamic spatiotemporal patterns or suffer from label scarcity and class imbalance.

Method: The SSTransformer uses self-supervised pre-training on GPS trajectories and trip records, followed by fine-tuning for binary classification.

Result: The model achieved 97.81% accuracy, 0.8889 precision, and 0.9358 F1-score on a real-world dataset.

Conclusion: The SSTransformer effectively captures complex anomalies in bike-sharing systems, offering scalable maintenance solutions.

Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban
"last-mile" connectivity, yet large-scale deployments face escalating
operational challenges, particularly in detecting faulty bikes. Existing
detection approaches either rely on static model-based thresholds that overlook
dynamic spatiotemporal (ST) usage patterns or employ supervised learning
methods that struggle with label scarcity and class imbalance. To address these
limitations, this paper proposes a novel Self-Supervised Transformer
(SSTransformer) framework for automatically detecting unusable shared bikes,
leveraging ST features extracted from GPS trajectories and trip records. The
model incorporates a self-supervised pre-training strategy to enhance its
feature extraction capabilities, followed by fine-tuning for efficient status
recognition. In the pre-training phase, the Transformer encoder learns
generalized representations of bike movement via a self-supervised objective;
in the fine-tuning phase, the encoder is adapted to a downstream binary
classification task. Comprehensive experiments on a real-world dataset of
10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate
that SSTransformer significantly outperforms traditional machine learning,
ensemble learning, and deep learning baselines, achieving the best accuracy
(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the
effectiveness of self-supervised Transformer on ST data for capturing complex
anomalies in BSS, paving the way toward more reliable and scalable maintenance
solutions for shared mobility.

</details>


### [162] [TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning](https://arxiv.org/abs/2505.00933)
*A. H. Abbas*

Main category: cs.LG

TL;DR: TunnElQNN, a hybrid quantum-classical neural network with TDAF, outperforms ReLUQNN in multi-class classification tasks, showcasing the benefits of physics-inspired activation functions in hybrid architectures.


<details>
  <summary>Details</summary>
Motivation: To enhance hybrid quantum-classical neural networks by integrating physics-inspired activation functions (TDAF) and evaluating their performance against conventional methods.

Method: Proposed TunnElQNN, a non-sequential hybrid architecture with alternating classical (TDAF) and quantum layers, tested on a synthetic dataset with varying class overlap.

Result: TunnElQNN consistently outperformed ReLUQNN and demonstrated superior decision boundaries compared to fully classical TDAF networks.

Conclusion: Integrating physics-inspired activation functions with quantum components improves the expressiveness and robustness of hybrid quantum-classical machine learning models.

Abstract: Hybrid quantum-classical neural networks (HQCNNs) represent a promising
frontier in machine learning, leveraging the complementary strengths of both
models. In this work, we propose the development of TunnElQNN, a non-sequential
architecture composed of alternating classical and quantum layers. Within the
classical component, we employ the Tunnelling Diode Activation Function (TDAF),
inspired by the I-V characteristics of quantum tunnelling. We evaluate the
performance of this hybrid model on a synthetic dataset of interleaving
half-circle for multi-class classification tasks with varying degrees of class
overlap. The model is compared against a baseline hybrid architecture that uses
the conventional ReLU activation function (ReLUQNN). Our results show that the
TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore,
we analyse the decision boundaries generated by TunnElQNN under different
levels of class overlap and compare them to those produced by a neural network
implementing TDAF within a fully classical architecture. These findings
highlight the potential of integrating physics-inspired activation functions
with quantum components to enhance the expressiveness and robustness of hybrid
quantum-classical machine learning architectures.

</details>


### [163] [StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization](https://arxiv.org/abs/2505.00940)
*Zhenyu Wang, Molei Liu, Jing Lei, Francis Bach, Zijian Guo*

Main category: cs.LG

TL;DR: StablePCA is a novel method for robust low-dimensional feature extraction from multi-source high-dimensional data, addressing nonconvexity via Fantope relaxation and Mirror Prox optimization.


<details>
  <summary>Details</summary>
Motivation: To generalize PCA for multi-source data, overcoming nonconvexity and extracting transferable, bias-mitigated features.

Method: Uses Fantope relaxation for convex minimax optimization, solved via Mirror Prox algorithm with closed-form updates.

Result: Demonstrates high accuracy and efficiency in robust feature extraction across finite-sample scenarios.

Conclusion: StablePCA effectively generalizes PCA for multi-source data, offering theoretical convergence and practical robustness.

Abstract: When synthesizing multisource high-dimensional data, a key objective is to
extract low-dimensional feature representations that effectively approximate
the original features across different sources. Such general feature extraction
facilitates the discovery of transferable knowledge, mitigates systematic
biases such as batch effects, and promotes fairness. In this paper, we propose
Stable Principal Component Analysis (StablePCA), a novel method for group
distributionally robust learning of latent representations from
high-dimensional multi-source data. A primary challenge in generalizing PCA to
the multi-source regime lies in the nonconvexity of the fixed rank constraint,
rendering the minimax optimization nonconvex. To address this challenge, we
employ the Fantope relaxation, reformulating the problem as a convex minimax
optimization, with the objective defined as the maximum loss across sources. To
solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox
algorithm with explicit closed-form updates. Theoretically, we establish the
global convergence of the Mirror Prox algorithm, with the convergence rate
provided from the optimization perspective. Furthermore, we offer practical
criteria to assess how closely the solution approximates the original nonconvex
formulation. Through extensive numerical experiments, we demonstrate
StablePCA's high accuracy and efficiency in extracting robust low-dimensional
representations across various finite-sample scenarios.

</details>


### [164] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/abs/2505.00941)
*Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, Cuicui Luo*

Main category: cs.LG

TL;DR: FreCT, a novel Frequency-augmented Convolutional Transformer, improves time series anomaly detection by integrating frequency analysis and robust training techniques, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Reconstruction-based anomaly detection methods struggle with computational deviations and neglect frequency-domain alignment, limiting their effectiveness.

Method: FreCT combines patch operations, an improved Transformer with convolution, and Fourier-based frequency analysis, optimized via stop-gradient KL divergence and absolute error.

Result: FreCT outperforms existing methods on four public datasets in anomaly detection.

Conclusion: FreCT addresses limitations of reconstruction-based approaches by leveraging frequency-domain insights and robust optimization, enhancing anomaly detection performance.

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>


### [165] [Addressing Noise and Stochasticity in Fraud Detection for Service Networks](https://arxiv.org/abs/2505.00946)
*Wenxin Zhang, Ding Xu, Xi Xuan, Lei Jiang, Guangzhen Yao, Renda Han, Xiangxiang Lang, Cuicui Luo*

Main category: cs.LG

TL;DR: SGNN-IB, a spectral graph network using information bottleneck theory, improves fraud detection by addressing noise and signal fusion issues in service networks.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based fraud detection methods struggle with noise and signal distortion, limiting their effectiveness.

Method: SGNN-IB splits graphs into homophilic/heterophilic subgraphs, applies information bottleneck theory, and uses prototype learning for signal fusion.

Result: SGNN-IB outperforms state-of-the-art methods on three real-world datasets.

Conclusion: SGNN-IB effectively addresses noise and signal fusion challenges, enhancing fraud detection in service networks.

Abstract: Fraud detection is crucial in social service networks to maintain user trust
and improve service network security. Existing spectral graph-based methods
address this challenge by leveraging different graph filters to capture signals
with different frequencies in service networks. However, most graph
filter-based methods struggle with deriving clean and discriminative graph
signals. On the one hand, they overlook the noise in the information
propagation process, resulting in degradation of filtering ability. On the
other hand, they fail to discriminate the frequency-specific characteristics of
graph signals, leading to distortion of signals fusion. To address these
issues, we develop a novel spectral graph network based on information
bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB
splits the original graph into homophilic and heterophilic subgraphs to better
capture the signals at different frequencies. For the first limitation, SGNN-IB
applies information bottleneck theory to extract key characteristics of encoded
representations. For the second limitation, SGNN-IB introduces prototype
learning to implement signal fusion, preserving the frequency-specific
characteristics of signals. Extensive experiments on three real-world datasets
demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.

</details>


### [166] [Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification](https://arxiv.org/abs/2505.00963)
*Kota Fukuda, Guanqin Zhang, Zhenya Zhang, Yulei Sui, Jianjun Zhao*

Main category: cs.LG

TL;DR: ABONN introduces an adaptive Branch and Bound (BaB) approach using Monte-Carlo tree search (MCTS) to prioritize sub-problems by 'importance,' achieving significant speedups in neural network verification.


<details>
  <summary>Details</summary>
Motivation: Existing BaB methods inefficiently explore sub-problems, ignoring their varying importance for finding counterexamples.

Method: ABONN adaptively explores sub-problems using MCTS, guided by a notion of 'importance' to prioritize likely counterexample regions.

Result: ABONN achieves speedups of up to 15.2× on MNIST and 24.7× on CIFAR-10, outperforming state-of-the-art verifiers.

Conclusion: ABONN efficiently verifies neural networks by adaptively focusing on critical sub-problems, demonstrating practical improvements over existing methods.

Abstract: Formal verification is a rigorous approach that can provably ensure the
quality of neural networks, and to date, Branch and Bound (BaB) is the
state-of-the-art that performs verification by splitting the problem as needed
and applying off-the-shelf verifiers to sub-problems for improved performance.
However, existing BaB may not be efficient, due to its naive way of exploring
the space of sub-problems that ignores the \emph{importance} of different
sub-problems. To bridge this gap, we first introduce a notion of ``importance''
that reflects how likely a counterexample can be found with a sub-problem, and
then we devise a novel verification approach, called ABONN, that explores the
sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style.
The exploration is guided by the ``importance'' of different sub-problems, so
it favors the sub-problems that are more likely to find counterexamples. As
soon as it finds a counterexample, it can immediately terminate; even though it
cannot find, after visiting all the sub-problems, it can still manage to verify
the problem. We evaluate ABONN with 552 verification problems from
commonly-used datasets and neural network models, and compare it with the
state-of-the-art verifiers as baseline approaches. Experimental evaluation
shows that ABONN demonstrates speedups of up to $15.2\times$ on MNIST and
$24.7\times$ on CIFAR-10. We further study the influences of hyperparameters to
the performance of ABONN, and the effectiveness of our adaptive tree
exploration.

</details>


### [167] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang, Yuefeng Chen, Hui Xue, Quanshi Zhang*

Main category: cs.LG

TL;DR: A new watermarking method for DNNs is introduced, robust against fine-tuning by leveraging low-frequency components in convolutional filters.


<details>
  <summary>Details</summary>
Motivation: To protect DNN ownership by embedding watermarks that remain intact during fine-tuning.

Method: Uses a revised Fourier transform to identify and encode watermark information in specific frequency components of convolutional filters, which resist gradient descent changes.

Result: The method is proven robust to fine-tuning, weight scaling, and permutations, with preliminary experiments confirming its effectiveness.

Conclusion: The proposed watermarking technique successfully embeds ownership information in DNNs while resisting common modifications.

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>


### [168] [Tree-Sliced Wasserstein Distance with Nonlinear Projection](https://arxiv.org/abs/2505.00968)
*Thanh Tran, Viet-Hoang Tran, Thanh Chu, Trang Pham, Laurent El Ghaoui, Tam Le, Tan M. Nguyen*

Main category: cs.LG

TL;DR: A novel nonlinear projectional framework for Tree-Sliced Wasserstein (TSW) distance is proposed, replacing linear projections to enhance metric efficiency for measures on Euclidean spaces and spheres, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: To improve the Tree-Sliced Wasserstein distance by capturing topological structures better and maintaining low computational costs.

Method: Introduces a nonlinear projectional framework for TSW, ensuring injectivity of the Radon Transform and metric well-definedness.

Result: Efficient metrics for measures on Euclidean spaces and spheres, with significant improvements in applications like gradient flows and generative models.

Conclusion: The proposed nonlinear TSW framework outperforms recent SW and TSW variants, demonstrating practical utility in various applications.

Abstract: Tree-Sliced methods have recently emerged as an alternative to the
traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines
with tree-based metric spaces and incorporating a splitting mechanism for
projecting measures. This approach enhances the ability to capture the
topological structures of integration domains in Sliced Optimal Transport while
maintaining low computational costs. Building on this foundation, we propose a
novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)
distance, substituting the linear projections in earlier versions with general
projections, while ensuring the injectivity of the associated Radon Transform
and preserving the well-definedness of the resulting metric. By designing
appropriate projections, we construct efficient metrics for measures on both
Euclidean spaces and spheres. Finally, we validate our proposed metric through
extensive numerical experiments for Euclidean and spherical datasets.
Applications include gradient flows, self-supervised learning, and generative
models, where our methods demonstrate significant improvements over recent SW
and TSW variants.

</details>


### [169] [A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems](https://arxiv.org/abs/2505.00973)
*Xin Chen, Yuze Chen, Yuan Zhou*

Main category: cs.LG

TL;DR: The paper introduces a minimax-MDP framework for robust online decision-making with refining predictions, applied to inventory and resource allocation problems.


<details>
  <summary>Details</summary>
Motivation: To address sequential decision-making problems where predictions (e.g., demand or utility) refine over time, ensuring decisions are competitive with hindsight optimal solutions under uncertainty.

Method: Proposes a minimax-MDP framework with adversarial environment states and internal decision-maker states, leveraging future-imposed conditions for feasibility and efficient policy design.

Result: Demonstrates tractable, often closed-form, robustly competitive policies, validated through applications like inventory ordering and resource allocation.

Conclusion: The minimax-MDP offers a versatile and efficient approach for robust decision-making under predictive uncertainty.

Abstract: We study a class of sequential decision-making problems with augmented
predictions, potentially provided by a machine learning algorithm. In this
setting, the decision-maker receives prediction intervals for unknown
parameters that become progressively refined over time, and seeks decisions
that are competitive with the hindsight optimal under all possible realizations
of both parameters and predictions. We propose a minimax Markov Decision
Process (minimax-MDP) framework, where the system state consists of an
adversarially evolving environment state and an internal state controlled by
the decision-maker. We introduce a set of future-imposed conditions that
characterize the feasibility of minimax-MDPs and enable the design of
efficient, often closed-form, robustly competitive policies. We illustrate the
framework through three applications: multi-period inventory ordering with
refining demand predictions, resource allocation with uncertain utility
functions, and a multi-phase extension of the minimax-MDP applied to the
inventory problem with time-varying ordering costs. Our results provide a
tractable and versatile approach to robust online decision-making under
predictive uncertainty.

</details>


### [170] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde, Louis Jaburi*

Main category: cs.LG

TL;DR: The paper proposes a pluralist framework for evaluating explanations in Mechanistic Interpretability (MI) by integrating four philosophical perspectives, identifying Compact Proofs as promising, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: Progress in MI is limited by the lack of a universal approach to evaluating explanations, prompting the need to address 'What makes a good explanation?'

Method: Introduces an Explanatory Virtues Framework based on Bayesian, Kuhnian, Deutschian, and Nomological perspectives to systematically evaluate MI explanations.

Result: Compact Proofs are identified as a promising approach, and future directions include defining simplicity, unifying explanations, and deriving universal principles.

Conclusion: Improved MI methods can enhance monitoring, prediction, and control of AI systems.

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>


### [171] [Toward Data-centric Directed Graph Learning: An Entropy-driven Approach](https://arxiv.org/abs/2505.00983)
*Xunkai Li, Zhengyu Wu, Kaichi Yu, Hongchao Qin, Guang Zeng, Rong-Hua Li, Guoren Wang*

Main category: cs.LG

TL;DR: EDEN is a data-centric digraph learning framework that enhances knowledge distillation by leveraging hierarchical encoding and mutual information, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing DiGNNs fail to fully exploit digraph data, limiting predictive performance. EDEN addresses this by exploring correlations between topology and node profiles.

Method: EDEN constructs a Hierarchical Knowledge Tree (HKT) using directed structural measurements and refines it with mutual information of node profiles for knowledge distillation.

Result: EDEN achieves SOTA performance on 14 (di)graph datasets across 4 tasks, improving prevalent (Di)GNNs.

Conclusion: EDEN is a versatile, data-centric framework that enhances digraph learning and extends to undirected scenarios.

Abstract: The directed graph (digraph), as a generalization of undirected graphs,
exhibits superior representation capability in modeling complex topology
systems and has garnered considerable attention in recent years. Despite the
notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage
directed edges, they still fail to comprehensively delve into the abundant data
knowledge concealed in the digraphs. This data-level limitation results in
model-level sub-optimal predictive performance and underscores the necessity of
further exploring the potential correlations between the directed edges
(topology) and node profiles (feature and labels) from a data-centric
perspective, thereby empowering model-centric neural networks with stronger
encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph
knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a
data-centric digraph learning paradigm or a model-agnostic hot-and-plug
data-centric Knowledge Distillation (KD) module. The core idea is to achieve
data-centric ML, guided by our proposed hierarchical encoding theory for
structured data. Specifically, EDEN first utilizes directed structural
measurements from a topology perspective to construct a coarse-grained
Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual
information of node profiles to refine knowledge flow in the HKT, enabling
data-centric KD supervision within model training. As a general framework, EDEN
can also naturally extend to undirected scenarios and demonstrate satisfactory
performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph
datasets (homophily and heterophily) and across 4 downstream tasks. The results
demonstrate that EDEN attains SOTA performance and exhibits strong improvement
for prevalent (Di)GNNs.

</details>


### [172] [Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization](https://arxiv.org/abs/2505.00982)
*Shunxian Gu, Chaoqun You, Bangbang Ren, Lailong Luo, Junxu Xia, Deke Guo*

Main category: cs.LG

TL;DR: FOSI, a hybrid optimizer, accelerates DNN training by combining gradient and curvature information. Its distributed design, DHO$_2$, reduces memory burden and training time, achieving 1.4×∼2.1× speedup.


<details>
  <summary>Details</summary>
Motivation: To enable faster DNN training for users with limited computing resources by leveraging hybrid optimization and distributed design.

Method: DHO$_2$ distributes curvature calculation and model updates, parallelizing tasks across devices to reduce memory and time.

Result: Achieves linear memory burden reduction and 1.4×∼2.1× training speedup compared to conventional distributed optimizers.

Conclusion: DHO$_2$ offers an efficient solution for resource-constrained DNN training, balancing speed and memory usage.

Abstract: Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.

</details>


### [173] [Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality](https://arxiv.org/abs/2505.01036)
*Xiaojun Zhou*

Main category: cs.LG

TL;DR: Stagnation in evolutionary algorithms can aid population convergence, and convergence doesn't guarantee optimality.


<details>
  <summary>Details</summary>
Motivation: Challenge the misconception that stagnation hinders convergence and that convergence ensures optimality in evolutionary algorithms.

Method: Present counterexamples to demonstrate that stagnation can facilitate convergence and that convergence alone is inadequate for optimality.

Result: Stagnation can benefit population convergence, and convergence doesn't imply local or global optimality.

Conclusion: Convergence is insufficient for evolutionary algorithm effectiveness; stagnation's role is more nuanced than previously thought.

Abstract: In the evolutionary computation community, it is widely believed that
stagnation impedes convergence in evolutionary algorithms, and that convergence
inherently indicates optimality. However, this perspective is misleading. In
this study, it is the first to highlight that the stagnation of an individual
can actually facilitate the convergence of the entire population, and
convergence does not necessarily imply optimality, not even local optimality.
Convergence alone is insufficient to ensure the effectiveness of evolutionary
algorithms. Several counterexamples are provided to illustrate this argument.

</details>


### [174] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma, Young D. Kwon, Dong Ma*

Main category: cs.LG

TL;DR: OD-TTA introduces an on-demand TTA framework for efficient adaptation on edge devices, reducing computation and energy overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods are impractical for resource-constrained edge devices due to high memory and energy costs.

Method: OD-TTA uses lightweight domain shift detection, source domain selection, and decoupled BN updates for efficient adaptation.

Result: OD-TTA achieves comparable or better performance with significantly reduced overhead.

Conclusion: OD-TTA makes TTA practical for edge devices by balancing accuracy and efficiency.

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>


### [175] [Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities](https://arxiv.org/abs/2505.01043)
*Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, Dacheng Tao*

Main category: cs.LG

TL;DR: A survey on low-precision training methods for LLMs, categorizing them into fixed-point/integer, floating-point, and customized formats, and discussing quantization-aware training and future directions.


<details>
  <summary>Details</summary>
Motivation: The hardware resource demands of LLM training hinder efficiency and scalability, prompting the need for low-precision training techniques. However, the diversity in numerical formats has fragmented research, necessitating a unified overview.

Method: The paper categorizes low-precision training methods into three groups based on numerical formats: fixed-point/integer, floating-point, and customized formats. It also reviews quantization-aware training.

Result: The survey organizes existing methods systematically, providing clarity on hardware compatibility and computational efficiency.

Conclusion: The paper highlights promising research directions and offers a GitHub repository for further reference.

Abstract: Large language models (LLMs) have achieved impressive performance across
various domains. However, the substantial hardware resources required for their
training present a significant barrier to efficiency and scalability. To
mitigate this challenge, low-precision training techniques have been widely
adopted, leading to notable advancements in training efficiency. Despite these
gains, low-precision training involves several components$\unicode{x2013}$such
as weights, activations, and gradients$\unicode{x2013}$each of which can be
represented in different numerical formats. The resulting diversity has created
a fragmented landscape in low-precision training research, making it difficult
for researchers to gain a unified overview of the field. This survey provides a
comprehensive review of existing low-precision training methods. To
systematically organize these approaches, we categorize them into three primary
groups based on their underlying numerical formats, which is a key factor
influencing hardware compatibility, computational efficiency, and ease of
reference for readers. The categories are: (1) fixed-point and integer-based
methods, (2) floating-point-based methods, and (3) customized format-based
methods. Additionally, we discuss quantization-aware training approaches, which
share key similarities with low-precision training during forward propagation.
Finally, we highlight several promising research directions to advance this
field. A collection of papers discussed in this survey is provided in
https://github.com/Hao840/Awesome-Low-Precision-Training.

</details>


### [176] [Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content](https://arxiv.org/abs/2505.01008)
*Haoyue Bai, Yiyou Sun, Wei Cheng, Haifeng Chen*

Main category: cs.LG

TL;DR: A black-box framework detects AI-generated images by corrupting and recovering masked parts, outperforming baselines by 4.31% in precision.


<details>
  <summary>Details</summary>
Motivation: The rise of photorealistic AI-generated images raises concerns about misuse, necessitating scalable detection methods without relying on model weights or large datasets.

Method: Uses a corrupt-and-recover strategy with masking and reconstruction assessment, supplemented by a surrogate model for black-box cases.

Result: Achieves 4.31% higher mean average precision than baselines across eight diffusion model datasets.

Conclusion: The framework offers a practical, scalable solution for detecting AI-generated images without requiring model weights or extensive datasets.

Abstract: The recent proliferation of photorealistic images created by generative
models has sparked both excitement and concern, as these images are
increasingly indistinguishable from real ones to the human eye. While offering
new creative and commercial possibilities, the potential for misuse, such as in
misinformation and fraud, highlights the need for effective detection methods.
Current detection approaches often rely on access to model weights or require
extensive collections of real image datasets, limiting their scalability and
practical application in real world scenarios. In this work, we introduce a
novel black box detection framework that requires only API access, sidestepping
the need for model weights or large auxiliary datasets. Our approach leverages
a corrupt and recover strategy: by masking part of an image and assessing the
model ability to reconstruct it, we measure the likelihood that the image was
generated by the model itself. For black-box models that do not support masked
image inputs, we incorporate a cost efficient surrogate model trained to align
with the target model distribution, enhancing detection capability. Our
framework demonstrates strong performance, outperforming baseline methods by
4.31% in mean average precision across eight diffusion model variant datasets.

</details>


### [177] [Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator](https://arxiv.org/abs/2505.01041)
*Xuyang Chen, Jingliang Duan, Lin Zhao*

Main category: cs.LG

TL;DR: The paper analyzes the performance of single-timescale actor-critic methods on continuous state-action spaces, proving epsilon-optimality with a sample complexity of epsilon^-2 for LQR problems.


<details>
  <summary>Details</summary>
Motivation: Despite actor-critic methods' success, theoretical understanding is limited, especially for practical single-timescale variants on infinite state-action spaces.

Method: Investigates single-sample single-timescale actor-critic on continuous spaces, using the LQR problem as a case study.

Result: Demonstrates epsilon-optimality with epsilon^-2 sample complexity for LQR on continuous spaces.

Conclusion: Bridges the theory-practice gap for single-timescale actor-critic methods.

Abstract: Actor-critic methods have achieved state-of-the-art performance in various
challenging tasks. However, theoretical understandings of their performance
remain elusive and challenging. Existing studies mostly focus on practically
uncommon variants such as double-loop or two-timescale stepsize actor-critic
algorithms for simplicity. These results certify local convergence on finite
state- or action-space only. We push the boundary to investigate the classic
single-sample single-timescale actor-critic on continuous (infinite)
state-action space, where we employ the canonical linear quadratic regulator
(LQR) problem as a case study. We show that the popular single-timescale
actor-critic can attain an epsilon-optimal solution with an order of epsilon to
-2 sample complexity for solving LQR on the demanding continuous state-action
space. Our work provides new insights into the performance of single-timescale
actor-critic, which further bridges the gap between theory and practice.

</details>


### [178] [Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](https://arxiv.org/abs/2505.01049)
*Nishant Jain, Xunpeng Huang, Yian Ma, Tong Zhang*

Main category: cs.LG

TL;DR: The paper provides a theoretical justification for the speed-up of consistency models over traditional SDE-based diffusion models, showing improved convergence rates and feasibility of accurate learning under minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of why consistency models accelerate generation compared to SDE-based diffusion models.

Method: Theoretical analysis of consistency models, focusing on their ability to map inputs to arbitrary timestamps along the reverse trajectory, and examining convergence rates under different data distribution assumptions.

Result: Achieves KL divergence of order O(ε²) with O(log(d/ε)) iterations, and similar guarantees under minimal assumptions, with best-in-class convergence rates for non-smooth settings.

Conclusion: Consistency models are theoretically justified for fast, high-quality sample generation, with practical learning feasibility even in non-smooth scenarios.

Abstract: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.

</details>


### [179] [Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions](https://arxiv.org/abs/2505.01060)
*Jihong Wang, Xiaochuan Tian, Zhongqiang Zhang, Stewart Silling, Siavash Jafarzadeh, Yue Yu*

Main category: cs.LG

TL;DR: The paper introduces MPNO, a data-driven nonlocal constitutive model learning approach, ensuring solution uniqueness and convexity for reliable simulations.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven constitutive models lack guaranteed well-posedness, leading to non-physical solutions in simulations.

Method: MPNO combines a neural operator with a monotone gradient network to learn a nonlocal kernel and constitutive relation, ensuring convexity and uniqueness.

Result: MPNO converges to ground-truth on synthetic data and outperforms conventional neural networks in generalization and accuracy for new loadings.

Conclusion: MPNO offers a robust, interpretable, and generalizable solution for modeling complex materials, validated by synthetic and real-world applications.

Abstract: Data-driven methods have emerged as powerful tools for modeling the responses
of complex nonlinear materials directly from experimental measurements. Among
these methods, the data-driven constitutive models present advantages in
physical interpretability and generalizability across different boundary
conditions/domain settings. However, the well-posedness of these learned models
is generally not guaranteed a priori, which makes the models prone to
non-physical solutions in downstream simulation tasks. In this study, we
introduce monotone peridynamic neural operator (MPNO), a novel data-driven
nonlocal constitutive model learning approach based on neural operators. Our
approach learns a nonlocal kernel together with a nonlinear constitutive
relation, while ensuring solution uniqueness through a monotone gradient
network. This architectural constraint on gradient induces convexity of the
learnt energy density function, thereby guaranteeing solution uniqueness of
MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's
performance on both synthetic and real-world datasets. On synthetic datasets
with manufactured kernel and constitutive relation, we show that the learnt
model converges to the ground-truth as the measurement grid size decreases both
theoretically and numerically. Additionally, our MPNO exhibits superior
generalization capabilities than the conventional neural networks: it yields
smaller displacement solution errors in down-stream tasks with new and unseen
loadings. Finally, we showcase the practical utility of our approach through
applications in learning a homogenized model from molecular dynamics data,
highlighting its expressivity and robustness in real-world scenarios.

</details>


### [180] [Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits](https://arxiv.org/abs/2505.01070)
*Edvin Fasth, Sagar Singh*

Main category: cs.LG

TL;DR: The paper proposes using Laplace approximation to improve group fairness in knowledge distillation by reweighting challenging instances, outperforming margin-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the decline in group fairness in student models due to simpler feature learning and spurious label correlations, despite comparable accuracy to teacher models.

Method: Employing Early-Exit Neural Networks (EENNs) and using Laplace approximation for uncertainty estimates to reweight cross-entropy and distillation losses per instance.

Result: The approach is validated using a Bert-based model on the MultiNLI dataset, showing improved fairness.

Conclusion: Laplace approximation provides more robust identification of difficult instances, enhancing group fairness in knowledge distillation.

Abstract: Knowledge distillation (KD) has become a powerful tool for training compact
student models using larger, pretrained teacher models, often requiring less
data and computational resources. Teacher models typically possess more layers
and thus exhibit richer feature representations compared to their student
counterparts. Furthermore, student models tend to learn simpler, surface-level
features in their early layers. This discrepancy can increase errors in groups
where labels spuriously correlate with specific input attributes, leading to a
decline in group fairness even when overall accuracy remains comparable to the
teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),
which enable predictions at multiple intermediate layers, have been employed.
Confidence margins derived from these early exits have been utilized to
reweight both cross-entropy and distillation losses on a per-instance basis. In
this paper, we propose that leveraging Laplace approximation-based methods to
obtain well-calibrated uncertainty estimates can also effectively reweight
challenging instances and improve group fairness. We hypothesize that Laplace
approximation offers a more robust identification of difficult or ambiguous
instances compared to margin-based approaches. To validate our claims, we
benchmark our approach using a Bert-based model on the MultiNLI dataset.

</details>


### [181] [Federated Adapter on Foundation Models: An Out-Of-Distribution Approach](https://arxiv.org/abs/2505.01075)
*Yiyuan Yang, Guodong Long, Tianyi Zhou, Qinghua Lu, Shanshan Ye, Jing Jiang*

Main category: cs.LG

TL;DR: FedOA addresses OOD generalization in Federated Foundation Models (FedFM) by using adapter-based fine-tuning and personalized adapters with feature distance regularization.


<details>
  <summary>Details</summary>
Motivation: To tackle OOD generalization challenges in FedFM due to large parameter scales and data heterogeneity, which conventional FL methods fail to address.

Method: Proposes FedOA, employing adapter-based fine-tuning and personalized adapters with feature distance regularization to align distributions.

Result: Theoretical proof of global model's OOD capabilities and empirical validation on NLP tasks show improved performance.

Conclusion: FedOA effectively enhances OOD generalization in FedFM, validated by theory and experiments.

Abstract: As foundation models gain prominence, Federated Foundation Models (FedFM)
have emerged as a privacy-preserving approach to collaboratively fine-tune
models in federated learning (FL) frameworks using distributed datasets across
clients. A key challenge for FedFM, given the versatile nature of foundation
models, is addressing out-of-distribution (OOD) generalization, where unseen
tasks or clients may exhibit distribution shifts leading to suboptimal
performance. Although numerous studies have explored OOD generalization in
conventional FL, these methods are inadequate for FedFM due to the challenges
posed by large parameter scales and increased data heterogeneity. To address
these, we propose FedOA, which employs adapter-based parameter-efficient
fine-tuning methods for efficacy and introduces personalized adapters with
feature distance-based regularization to align distributions and guarantee OOD
generalization for each client. Theoretically, we demonstrate that the
conventional aggregated global model in FedFM inherently retains OOD
generalization capabilities, and our proposed method enhances the personalized
model's OOD generalization through regularization informed by the global model,
with proven convergence under general non-convex settings. Empirically, the
effectiveness of the proposed method is validated on benchmark datasets across
various NLP tasks.

</details>


### [182] [Integration Matters for Learning PDEs with Backwards SDEs](https://arxiv.org/abs/2505.01078)
*Sungje Park, Stephen Tu*

Main category: cs.LG

TL;DR: The paper identifies discretization bias in BSDE-based PDE solvers due to Euler-Maruyama integration and proposes a Stratonovich-based BSDE with Heun integration to eliminate bias, outperforming EM-based methods and matching PINNs.


<details>
  <summary>Details</summary>
Motivation: Existing BSDE-based solvers underperform compared to PINNs due to discretization bias from Euler-Maruyama integration in short-horizon self-consistency BSDE losses.

Method: Proposes a Stratonovich-based BSDE formulation with stochastic Heun integration to address the bias issue.

Result: The Heun-based method eliminates bias, outperforms EM-based variants, and achieves competitive results with PINNs in high-dimensional benchmarks.

Conclusion: Integration schemes are critical in BSDE-based PDE solvers, and the proposed Heun-based approach effectively addresses the bias problem.

Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods
provide an alternative to Physics-Informed Neural Networks (PINNs) for solving
high-dimensional partial differential equations (PDEs), offering algorithmic
advantages in settings such as stochastic optimal control, where the PDEs of
interest are tied to an underlying dynamical system. However, existing
BSDE-based solvers have empirically been shown to underperform relative to
PINNs in the literature. In this paper, we identify the root cause of this
performance gap as a discretization bias introduced by the standard
Euler-Maruyama (EM) integration scheme applied to short-horizon
self-consistency BSDE losses, which shifts the optimization landscape off
target. We find that this bias cannot be satisfactorily addressed through finer
step sizes or longer self-consistency horizons. To properly handle this issue,
we propose a Stratonovich-based BSDE formulation, which we implement with
stochastic Heun integration. We show that our proposed approach completely
eliminates the bias issues faced by EM integration. Furthermore, our empirical
results show that our Heun-based BSDE method consistently outperforms EM-based
variants and achieves competitive results with PINNs across multiple
high-dimensional benchmarks. Our findings highlight the critical role of
integration schemes in BSDE-based PDE solvers, an algorithmic detail that has
received little attention thus far in the literature.

</details>


### [183] [Multi-Objective Reinforcement Learning for Water Management](https://arxiv.org/abs/2505.01094)
*Zuzanna Osika, Roxana Radelescu, Jazmin Zatarain Salazar, Frans Oliehoek, Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: The paper introduces a water resource management case study (Nile river basin) as a complex MORL environment and benchmarks MORL algorithms, finding specialized methods outperform MORL approaches.


<details>
  <summary>Details</summary>
Motivation: Address the lack of complex, realistic environments in MORL for real-world problems like resource management.

Method: Model the Nile river basin as a MORL environment and benchmark existing MORL algorithms.

Result: Specialized water management methods outperform state-of-the-art MORL approaches.

Conclusion: Highlights scalability challenges for MORL in real-world scenarios.

Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug
discovery) require optimizing multiple, conflicting objectives. Multi-objective
reinforcement learning (MORL) extends classic reinforcement learning to handle
multiple objectives simultaneously, yielding a set of policies that capture
various trade-offs. However, the MORL field lacks complex, realistic
environments and benchmarks. We introduce a water resource (Nile river basin)
management case study and model it as a MORL environment. We then benchmark
existing MORL algorithms on this task. Our results show that specialized water
management methods outperform state-of-the-art MORL approaches, underscoring
the scalability challenges MORL algorithms face in real-world scenarios.

</details>


### [184] [Nesterov Method for Asynchronous Pipeline Parallel Optimization](https://arxiv.org/abs/2505.01099)
*Thalaiyasingam Ajanthan, Sameera Ramasinghe, Yan Zuo, Gil Avraham, Alexander Long*

Main category: cs.LG

TL;DR: A modified Nesterov Accelerated Gradient (NAG) method is introduced to address stale gradients in asynchronous Pipeline Parallelism (PP), proving convergence and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Asynchronous PP offers 100% utilization but faces challenges with stale gradients due to lack of synchronization.

Method: A variant of NAG is proposed, modifying the look-ahead step to handle gradient staleness.

Result: Theoretical proof of sublinear convergence with fixed delay; experiments show superiority over async methods and sync baselines.

Conclusion: The modified NAG effectively addresses gradient staleness in PP, improving performance in large-scale tasks.

Abstract: Pipeline Parallelism (PP) enables large neural network training on small,
interconnected devices by splitting the model into multiple stages. To maximize
pipeline utilization, asynchronous optimization is appealing as it offers 100%
pipeline utilization by construction. However, it is inherently challenging as
the weights and gradients are no longer synchronized, leading to stale (or
delayed) gradients. To alleviate this, we introduce a variant of Nesterov
Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically,
we modify the look-ahead step in NAG to effectively address the staleness in
gradients. We theoretically prove that our approach converges at a sublinear
rate in the presence of fixed delay in gradients. Our experiments on
large-scale language modelling tasks using decoder-only architectures with up
to 1B parameters, demonstrate that our approach significantly outperforms
existing asynchronous methods, even surpassing the synchronous baseline.

</details>


### [185] [Risk Analysis and Design Against Adversarial Actions](https://arxiv.org/abs/2505.01130)
*Marco C. Campi, Algo Carè, Luis G. Crespo, Simone Garatti, Federico A. Ramponi*

Main category: cs.LG

TL;DR: A framework for evaluating model robustness against adversarial actions, applicable to various learning methods, with results aiding model trust and selection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deployment-time adversarial actions and deviations from training conditions in machine learning models.

Method: Proposes a versatile, principled framework for robustness evaluation, initially focusing on Support Vector Regression (SVR) but extending to relaxed optimization techniques.

Result: Enables vulnerability assessment without additional test data in a distribution-free setup, enhancing model trust and aiding selection.

Conclusion: The framework provides insights for out-of-distribution scenarios and supports robust model evaluation and selection.

Abstract: Learning models capable of providing reliable predictions in the face of
adversarial actions has become a central focus of the machine learning
community in recent years. This challenge arises from observing that data
encountered at deployment time often deviate from the conditions under which
the model was trained. In this paper, we address deployment-time adversarial
actions and propose a versatile, well-principled framework to evaluate the
model's robustness against attacks of diverse types and intensities. While we
initially focus on Support Vector Regression (SVR), the proposed approach
extends naturally to the broad domain of learning via relaxed optimization
techniques. Our results enable an assessment of the model vulnerability without
requiring additional test data and operate in a distribution-free setup. These
results not only provide a tool to enhance trust in the model's applicability
but also aid in selecting among competing alternatives. Later in the paper, we
show that our findings also offer useful insights for establishing new results
within the out-of-distribution framework.

</details>


### [186] [CoCoAFusE: Beyond Mixtures of Experts via Model Fusion](https://arxiv.org/abs/2505.01105)
*Aurelio Raffa Ugolini, Mara Tanelli, Valentina Breschi*

Main category: cs.LG

TL;DR: CoCoAFusE is a Bayesian Covariates-Dependent Modeling technique that combines predictions from multiple simple sub-models (experts) to enhance expressiveness and interpretability, while improving uncertainty quantification by fusing experts' distributions.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of interpretability and uncertainty quantification in deep learning models, especially in scenarios with multiple patterns and varying uncertainty.

Method: Extends Mixtures of Experts (MoEs) by fusing experts' distributions, not just mixing them, to avoid multimodality artifacts and improve modeling flexibility.

Result: CoCoAFusE provides tighter credible bounds on responses, avoids multimodality artifacts, and demonstrates efficacy in complex regression problems.

Conclusion: CoCoAFusE offers a more expressive and interpretable approach for modeling uncertainty in complex regression tasks.

Abstract: Many learning problems involve multiple patterns and varying degrees of
uncertainty dependent on the covariates. Advances in Deep Learning (DL) have
addressed these issues by learning highly nonlinear input-output dependencies.
However, model interpretability and Uncertainty Quantification (UQ) have often
straggled behind. In this context, we introduce the Competitive/Collaborative
Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling
technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts
(MoEs), blending predictions from several simple sub-models (or "experts") to
achieve high levels of expressiveness while retaining a substantial degree of
local interpretability. Our formulation extends that of a classical Mixture of
Experts by contemplating the fusion of the experts' distributions in addition
to their more usual mixing (i.e., superimposition). Through this additional
feature, CoCoAFusE better accommodates different scenarios for the intermediate
behavior between generating mechanisms, resulting in tighter credible bounds on
the response variable. Indeed, only resorting to mixing, as in classical MoEs,
may lead to multimodality artifacts, especially over smooth transitions.
Instead, CoCoAFusE can avoid these artifacts even under the same structure and
priors for the experts, leading to greater expressiveness and flexibility in
modeling. This new approach is showcased extensively on a suite of motivating
numerical examples and a collection of real-data ones, demonstrating its
efficacy in tackling complex regression problems where uncertainty is a key
quantity of interest.

</details>


### [187] [Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability](https://arxiv.org/abs/2505.01168)
*Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, Jing Wang*

Main category: cs.LG

TL;DR: HEAT improves adversarial example transferability by synthesizing shared gradient directions and dynamically balancing intra-domain coherence and inter-domain diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture shared gradient directions and lack adaptive weight allocation, limiting adversarial transferability.

Method: HEAT uses Singular Value Decomposition for shared gradient synthesis and a Dual-Harmony Weight Orchestrator for dynamic weight balancing.

Result: HEAT outperforms existing methods across datasets, enhancing adversarial attack effectiveness.

Conclusion: HEAT provides a novel approach for adversarial attacks, improving transferability and offering new research directions.

Abstract: The development of model ensemble attacks has significantly improved the
transferability of adversarial examples, but this progress also poses severe
threats to the security of deep neural networks. Existing methods, however,
face two critical challenges: insufficient capture of shared gradient
directions across models and a lack of adaptive weight allocation mechanisms.
To address these issues, we propose a novel method Harmonized Ensemble for
Adversarial Transferability (HEAT), which introduces domain generalization into
adversarial example generation for the first time. HEAT consists of two key
modules: Consensus Gradient Direction Synthesizer, which uses Singular Value
Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight
Orchestrator which dynamically balances intra-domain coherence, stabilizing
gradients within individual models, and inter-domain diversity, enhancing
transferability across models. Experimental results demonstrate that HEAT
significantly outperforms existing methods across various datasets and
settings, offering a new perspective and direction for adversarial attack
research.

</details>


### [188] [Incorporating Inductive Biases to Energy-based Generative Models](https://arxiv.org/abs/2505.01111)
*Yukun Li, Li-Ping Liu*

Main category: cs.LG

TL;DR: A hybrid model combining energy-based models (EBMs) with exponential family models improves data fitting and generation by incorporating inductive bias and aligning distribution statistics with data statistics.


<details>
  <summary>Details</summary>
Motivation: To enhance EBMs by integrating inductive bias and improving alignment of model statistics with data statistics, leveraging the strengths of both EBMs and exponential family models.

Method: Introduces a hybrid approach augmenting EBMs with parameter-free statistic functions, aligning distribution statistics with data statistics during training.

Result: Empirical validation shows improved data fitting and generation when informative statistics are incorporated.

Conclusion: The hybrid model effectively combines EBMs and exponential family models, enhancing performance in data modeling and generation.

Abstract: With the advent of score-matching techniques for model training and Langevin
dynamics for sample generation, energy-based models (EBMs) have gained renewed
interest as generative models. Recent EBMs usually use neural networks to
define their energy functions. In this work, we introduce a novel hybrid
approach that combines an EBM with an exponential family model to incorporate
inductive bias into data modeling. Specifically, we augment the energy term
with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align
the distribution statistics with data statistics during model training, even
when it only approximately maximizes the data likelihood. This property enables
us to impose constraints on the hybrid model. Our empirical study validates the
hybrid model's ability to match statistics. Furthermore, experimental results
show that data fitting and generation improve when suitable informative
statistics are incorporated into the hybrid model.

</details>


### [189] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/abs/2505.01169)
*Pramook Khungurn, Pratch Piyawongwisal, Sira Sriswadi, Supasorn Suwajanakorn*

Main category: cs.LG

TL;DR: The paper introduces a new loss function (ITVM) for distilling a two-timed flow model (TTFM), improving few-step generation performance over baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance the distillation of TTFM by refining the loss function for better performance in generating samples with fewer steps.

Method: Proposes the ITVM loss, which extends LFMD by matching initial velocities, modifying terminal velocity terms, and using EMA-stabilized models for target velocities.

Result: Preliminary experiments show improved few-step generation across datasets and architectures.

Conclusion: The ITVM loss effectively enhances TTFM distillation, outperforming existing methods.

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM)
$\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an
initial time $s$ to another belonging to the distribution at a terminal time
$t$ in one function evaluation. We present a new loss function for TTFM
distillation called the \emph{initial/terminal velocity matching} (ITVM) loss
that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi
et al. by adding redundant terms to match the initial velocities at time $s$,
removing the derivative from the terminal velocity term at time $t$, and using
a version of the model under training, stabilized by exponential moving
averaging (EMA), to compute the target terminal average velocity. Preliminary
experiments show that our loss leads to better few-step generation performance
on multiple types of datasets and model architectures over baselines.

</details>


### [190] [Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.01115)
*Palok Biswas, Zuzanna Osika, Isidoro Tamassia, Adit Whorra, Jazmin Zatarain-Salazar, Jan Kwakkel, Frans A. Oliehoek, Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: The paper introduces Justice, a framework combining Integrated Assessment Models (IAMs) with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) to address climate policy trade-offs and equity.


<details>
  <summary>Details</summary>
Motivation: Traditional IAMs optimize policies based on a single objective, often neglecting equity and perpetuating inequalities in climate policy recommendations.

Method: Justice integrates IAM with MOMARL to incorporate multiple objectives (economic growth, temperature goals, climate justice) and uses multi-agent systems to represent diverse policy actors.

Result: The framework identifies equitable Pareto-optimal policies, highlighting trade-offs and enabling deliberative decision-making for policymakers.

Conclusion: Justice offers a more balanced and equitable approach to climate policy by addressing multiple objectives and realistic policy actor interactions.

Abstract: Addressing climate change requires coordinated policy efforts of nations
worldwide. These efforts are informed by scientific reports, which rely in part
on Integrated Assessment Models (IAMs), prominent tools used to assess the
economic impacts of climate policies. However, traditional IAMs optimize
policies based on a single objective, limiting their ability to capture the
trade-offs among economic growth, temperature goals, and climate justice. As a
result, policy recommendations have been criticized for perpetuating
inequalities, fueling disagreements during policy negotiations. We introduce
Justice, the first framework integrating IAM with Multi-Objective Multi-Agent
Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice
generates policy recommendations that shed light on equity while balancing
climate and economic goals. Further, using multiple agents can provide a
realistic representation of the interactions among the diverse policy actors.
We identify equitable Pareto-optimal policies using our framework, which
facilitates deliberative decision-making by presenting policymakers with the
inherent trade-offs in climate and economic policy.

</details>


### [191] [Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders](https://arxiv.org/abs/2505.01134)
*Rogelio A Mancisidor, Robert Jenssen, Shujian Yu, Michael Kampffmeyer*

Main category: cs.LG

TL;DR: A novel method, CoDE-VAE, improves multimodal learning by avoiding the independence assumption of modalities, enhancing generative coherence, quality, and log-likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: Current methods assume independence among modalities, which is unrealistic. CoDE-VAE addresses this by leveraging dependent expert consensus.

Method: Uses the CoDE principle to aggregate single-modality distributions, learning contributions of modality subsets for a better ELBO approximation.

Result: CoDE-VAE outperforms in generative coherence, quality, and log-likelihood, minimizing quality gaps with increasing modalities.

Conclusion: CoDE-VAE achieves competitive classification accuracy and generative quality, bridging gaps in current multimodal VAE methods.

Abstract: Multimodal learning with variational autoencoders (VAEs) requires estimating
joint distributions to evaluate the evidence lower bound (ELBO). Current
methods, the product and mixture of experts, aggregate single-modality
distributions assuming independence for simplicity, which is an overoptimistic
assumption. This research introduces a novel methodology for aggregating
single-modality distributions by exploiting the principle of consensus of
dependent experts (CoDE), which circumvents the aforementioned assumption.
Utilizing the CoDE method, we propose a novel ELBO that approximates the joint
likelihood of the multimodal data by learning the contribution of each subset
of modalities. The resulting CoDE-VAE model demonstrates better performance in
terms of balancing the trade-off between generative coherence and generative
quality, as well as generating more precise log-likelihood estimations.
CoDE-VAE further minimizes the generative quality gap as the number of
modalities increases. In certain cases, it reaches a generative quality similar
to that of unimodal VAEs, which is a desirable property that is lacking in most
current methods. Finally, the classification accuracy achieved by CoDE-VAE is
comparable to that of state-of-the-art multimodal VAE models.

</details>


### [192] [Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts](https://arxiv.org/abs/2505.01135)
*Wenfa Wu, Guanyu Zhang, Zheng Tan, Yi Wang, Hongsheng Qi*

Main category: cs.LG

TL;DR: Dual-Forecaster is a multimodal time series model integrating historical and predictive textual information with numerical data, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing models lack comprehensive integration of textual and numerical data, limiting forecasting accuracy.

Method: Uses three cross-modality alignment techniques to combine historical and predictive textual insights with numerical series.

Result: Outperforms or matches state-of-the-art models on fifteen datasets.

Conclusion: Demonstrates the superiority of integrating textual information for time series forecasting, opening new research avenues.

Abstract: Most existing single-modal time series models rely solely on numerical
series, which suffer from the limitations imposed by insufficient information.
Recent studies have revealed that multimodal models can address the core issue
by integrating textual information. However, these models focus on either
historical or future textual information, overlooking the unique contributions
each plays in time series forecasting. Besides, these models fail to grasp the
intricate relationships between textual and time series data, constrained by
their moderate capacity for multimodal comprehension. To tackle these
challenges, we propose Dual-Forecaster, a pioneering multimodal time series
model that combines both descriptively historical textual information and
predictive textual insights, leveraging advanced multimodal comprehension
capability empowered by three well-designed cross-modality alignment
techniques. Our comprehensive evaluations on fifteen multimodal time series
datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal
time series model that outperforms or is comparable to other state-of-the-art
models, highlighting the superiority of integrating textual information for
time series forecasting. This work opens new avenues in the integration of
textual information with numerical time series data for multimodal time series
analysis.

</details>


### [193] [Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case](https://arxiv.org/abs/2505.01156)
*Milad Leyli-Abadi, Jérôme Picault, Antoine Marot, Jean-Patrick Brunet, Agathe Gilain, Amarsagar Reddy Ramapuram Matavalam, Shaban Ghias Satti, Quingbin Jiang, Yang Liu, Dean Justin Ninalga*

Main category: cs.LG

TL;DR: AI-driven methods were developed to speed up power grid simulations, addressing computational challenges from renewable energy integration, using a benchmarking framework (LIPS) to evaluate solutions.


<details>
  <summary>Details</summary>
Motivation: The need for faster power grid simulations due to increased renewable energy sources and real-time scenario analysis.

Method: Organized a competition (ML4PhySim) to develop AI-driven solutions, evaluated using LIPS framework across four dimensions.

Result: Top-performing AI solutions outperformed traditional methods, demonstrating efficiency and reliability.

Conclusion: The study encourages further research into scalable and sustainable simulation methods for power networks.

Abstract: This paper addresses the growing computational challenges of power grid
simulations, particularly with the increasing integration of renewable energy
sources like wind and solar. As grid operators must analyze significantly more
scenarios in near real-time to prevent failures and ensure stability,
traditional physical-based simulations become computationally impractical. To
tackle this, a competition was organized to develop AI-driven methods that
accelerate power flow simulations by at least an order of magnitude while
maintaining operational reliability. This competition utilized a regional-scale
grid model with a 30\% renewable energy mix, mirroring the anticipated
near-future composition of the French power grid. A key contribution of this
work is through the use of LIPS (Learning Industrial Physical Systems), a
benchmarking framework that evaluates solutions based on four critical
dimensions: machine learning performance, physical compliance, industrial
readiness, and generalization to out-of-distribution scenarios. The paper
provides a comprehensive overview of the Machine Learning for Physical
Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing
top-performing solutions that outperformed traditional simulation methods, and
sharing key organizational insights and best practices for running large-scale
AI competitions. Given the promising results achieved, the study aims to
inspire further research into more efficient, scalable, and sustainable power
network simulation methodologies.

</details>


### [194] [Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications](https://arxiv.org/abs/2505.01261)
*Elie Saad, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois*

Main category: cs.LG

TL;DR: A deep learning framework for electronic component obsolescence forecasting addresses data scarcity by generating synthetic cases and augmenting training datasets, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Electronic component obsolescence is critical in long-life systems, but machine learning models lack sufficient data for high precision.

Method: Proposes a deep generative model to create synthetic obsolescence cases, augmenting datasets for training classical machine learning models adapted for semi-supervised learning.

Result: The framework achieves state-of-the-art performance on benchmarking datasets.

Conclusion: The approach effectively mitigates data scarcity in obsolescence forecasting, enhancing model precision.

Abstract: The challenge of electronic component obsolescence is particularly critical
in systems with long life cycles. Various obsolescence management methods are
employed to mitigate its impact, with obsolescence forecasting being a highly
sought-after and prominent approach. As a result, numerous machine
learning-based forecasting methods have been proposed. However, machine
learning models require a substantial amount of relevant data to achieve high
precision, which is lacking in the current obsolescence landscape in some
situations. This work introduces a novel framework for obsolescence forecasting
based on deep learning. The proposed framework solves the lack of available
data through deep generative modeling, where new obsolescence cases are
generated and used to augment the training dataset. The augmented dataset is
then used to train a classical machine learning-based obsolescence forecasting
model. To train classical forecasting models using augmented datasets, existing
classical supervised-learning classifiers are adapted for semi-supervised
learning within this framework. The proposed framework demonstrates
state-of-the-art results on benchmarking datasets.

</details>


### [195] [TActiLE: Tiny Active LEarning for wearable devices](https://arxiv.org/abs/2505.01160)
*Massimo Pavan, Claudio Galimberti, Manuel Roveri*

Main category: cs.LG

TL;DR: The paper introduces TActiLE, an Active Learning algorithm for TinyML, addressing the challenge of limited labeled data in on-device learning for wearable devices like smart glasses.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled data in on-device learning for wearables necessitates minimizing labeling effort while maintaining model performance.

Method: Proposes TActiLE, an Active Learning technique that selects the most informative unlabeled data for labeling, tailored for TinyML.

Result: TActiLE is evaluated on image classification datasets, showing effectiveness and efficiency for tiny and wearable devices.

Conclusion: TActiLE is a promising solution for enhancing on-device learning in TinyML applications, particularly for wearables.

Abstract: Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent
years, enabling wearable devices to be not only connected but also genuinely
intelligent by running machine learning (ML) computations directly on-device.
Among such devices, smart glasses have particularly benefited from TinyML
advancements. TinyML facilitates the on-device execution of the inference phase
of ML algorithms on embedded and wearable devices, and more recently, it has
expanded into On-device Learning (ODL), which allows both inference and
learning phases to occur directly on the device. The application of ODL
techniques to wearable devices is particularly compelling, as it enables the
development of more personalized models that adapt based on the data of the
user. However, one of the major challenges of ODL algorithms is the scarcity of
labeled data collected on-device. In smart wearable contexts, requiring users
to manually label large amounts of data is often impractical and could lead to
user disengagement with the technology. To address this issue, this paper
explores the application of Active Learning (AL) techniques, i.e., techniques
that aim at minimizing the labeling effort, by actively selecting from a large
quantity of unlabeled data only a small subset to be labeled and added to the
training set of the algorithm. In particular, we propose TActiLE, a novel AL
algorithm that selects from the stream of on-device sensor data the ones that
would help the ML algorithm improve the most once coupled with labels provided
by the user. TActiLE is the first Active Learning technique specifically
designed for the TinyML context. We evaluate its effectiveness and efficiency
through experiments on multiple image classification datasets. The results
demonstrate its suitability for tiny and wearable devices.

</details>


### [196] [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://arxiv.org/abs/2408.09632)
*Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu*

Main category: cs.LG

TL;DR: MoDeGPT is a novel structured compression framework for LLMs that avoids recovery fine-tuning, reduces computational costs, and maintains high performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of deploying LLMs on resource-limited devices due to high computational demands and the drawbacks of existing compression methods.

Method: Partitions Transformer blocks into modules, reduces hidden dimensions via module-level output reconstruction, and uses Nyström, CR, and SVD decompositions.

Result: Matches or surpasses previous methods, saves 98% compute costs, maintains 90-95% zero-shot performance, and increases inference throughput by 46%.

Conclusion: MoDeGPT offers an efficient, high-performance compression solution for LLMs without requiring fine-tuning.

Abstract: Large Language Models (LLMs) have reshaped the landscape of artificial
intelligence by demonstrating exceptional performance across various tasks.
However, substantial computational requirements make their deployment
challenging on devices with limited resources. Recently, compression methods
using low-rank matrix techniques have shown promise, yet these often lead to
degraded accuracy or introduce significant overhead in parameters and inference
latency. This paper introduces \textbf{Mo}dular \textbf{De}composition
(MoDeGPT), a novel structured compression framework that does not need recovery
fine-tuning while resolving the above drawbacks. MoDeGPT partitions the
Transformer block into modules comprised of matrix pairs and reduces the hidden
dimensions via reconstructing the module-level outputs. MoDeGPT is developed
based on a theoretical framework that utilizes three well-established matrix
decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD
-- and applies them to our redefined transformer modules. Our comprehensive
experiments show MoDeGPT, without backward propagation, matches or surpasses
previous structured compression methods that rely on gradient information, and
saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3
and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%
compression rates. Moreover, the compression can be done on a single GPU within
a few hours and increases the inference throughput by up to 46%.

</details>


### [197] [A Physics-preserved Transfer Learning Method for Differential Equations](https://arxiv.org/abs/2505.01281)
*Hao-Ran Yang, Chuan-Xian Ren*

Main category: cs.LG

TL;DR: A new transfer learning method, POTT, addresses domain shift and preserves physics in solving differential equations.


<details>
  <summary>Details</summary>
Motivation: Existing transfer learning methods for differential equations lack generalizability or physics preservation.

Method: Proposes Physics-preserved Optimal Tensor Transport (POTT) to correct domain shift and preserve physical information.

Result: POTT shows superior performance, generalizability, and physics preservation in experiments.

Conclusion: POTT effectively adapts data-driven models to target domains while maintaining physical accuracy.

Abstract: While data-driven methods such as neural operator have achieved great success
in solving differential equations (DEs), they suffer from domain shift problems
caused by different learning environments (with data bias or equation changes),
which can be alleviated by transfer learning (TL). However, existing TL methods
adopted in DEs problems lack either generalizability in general DEs problems or
physics preservation during training. In this work, we focus on a general
transfer learning method that adaptively correct the domain shift and preserve
physical information. Mathematically, we characterize the data domain as
product distribution and the essential problems as distribution bias and
operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that
simultaneously admits generalizability to common DEs and physics preservation
of specific problem is proposed to adapt the data-driven model to target domain
utilizing the push-forward distribution induced by the POTT map. Extensive
experiments demonstrate the superior performance, generalizability and physics
preservation of the proposed POTT method.

</details>


### [198] [Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series](https://arxiv.org/abs/2505.01163)
*Thanh Son Nguyen, Dang Minh Duc Nguyen, Van Thanh Nguyen*

Main category: cs.LG

TL;DR: The study compares Polynomial Classifier (PC) and Radial Basis Function Neural Network (RBFNN) for time series forecasting, finding PC better for non-seasonal data and RBFNN for seasonal data, with trade-offs in accuracy, speed, and interpretability.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of PC and RBFNN for real-time time series forecasting, considering accuracy, computational efficiency, and interpretability.

Method: Empirical comparison using four real-world datasets (weather, gold prices, oil prices, beer production) with seasonal and non-seasonal patterns. Metrics: Mean Absolute Error, Root Mean Squared Error, Coefficient of Variation, and computational time. Statistical tests (paired t-tests, Wilcoxon signed rank) validate results.

Result: PC is more accurate and faster for non-seasonal data, while RBFNN excels for seasonal patterns. PC offers better interpretability. Differences are statistically significant.

Conclusion: PC is recommended for non-seasonal, interpretable forecasts; RBFNN is better for complex seasonal patterns. Findings guide model selection in time series forecasting.

Abstract: Accurate time series forecasting is essential in many real-time applications
that demand both high predictive accuracy and computational efficiency. This
study provides an empirical comparison between a Polynomial Classifier and a
Radial Basis Function Neural Network (RBFNN) across four real-world time series
datasets (weather conditions, gold prices, crude oil prices, and beer
production volumes) that cover both seasonal and nonseasonal patterns. Model
performance is evaluated by forecasting accuracy (using Mean Absolute Error,
Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared
Error) and computational time to assess each model's viability for real time
forecasting. The results show that the PC yields more accurate and faster
forecasts for non seasonal series, whereas the RBFNN performs better on series
with pronounced seasonal patterns. From an interpretability standpoint, the
polynomial model offers a simpler, more transparent structure (in contrast to
the black box nature of neural network), which is advantageous for
understanding and trust in real time decision making. The performance
differences between PC and RBFNN are statistically significant, as confirmed by
paired t tests and Wilcoxon signed rank tests. These findings provide practical
guidance for model selection in time series forecasting, indicating that PC may
be preferable for quick, interpretable forecasts in non-seasonal contexts,
whereas RBFNN is superior for capturing complex seasonal behaviors

</details>


### [199] [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://arxiv.org/abs/2412.01003)
*Core Francisco Park, Ekdeep Singh Lubana, Itamar Pres, Hidenori Tanaka*

Main category: cs.LG

TL;DR: The paper proposes a synthetic sequence modeling task to study In-Context Learning (ICL), revealing it as a mixture of competing algorithms rather than a monolithic capability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generality in existing ICL studies by introducing a unified synthetic task that reproduces known ICL results.

Method: A synthetic task simulating a finite mixture of Markov chains, decomposing model behavior into four competing algorithms combining fuzzy retrieval/inference with unigram/bigram statistics.

Result: Models trained on the task reproduce ICL results, with behavior dictated by competition among algorithms, influenced by context size and training amount.

Conclusion: ICL is a transient, algorithmically mixed process, making universal claims about it infeasible.

Abstract: In-Context Learning (ICL) has significantly expanded the general-purpose
nature of large language models, allowing them to adapt to novel tasks using
merely the inputted context. This has motivated a series of papers that analyze
tractable synthetic domains and postulate precise mechanisms that may underlie
ICL. However, the use of relatively distinct setups that often lack a sequence
modeling nature to them makes it unclear how general the reported insights from
such studies are. Motivated by this, we propose a synthetic sequence modeling
task that involves learning to simulate a finite mixture of Markov chains. As
we show, models trained on this task reproduce most well-known results on ICL,
hence offering a unified setting for studying the concept. Building on this
setup, we demonstrate we can explain a model's behavior by decomposing it into
four broad algorithms that combine a fuzzy retrieval vs. inference approach
with either unigram or bigram statistics of the context. These algorithms
engage in a competition dynamics to dominate model behavior, with the precise
experimental conditions dictating which algorithm ends up superseding others:
e.g., we find merely varying context size or amount of training yields (at
times sharp) transitions between which algorithm dictates the model behavior,
revealing a mechanism that explains the transient nature of ICL. In this sense,
we argue ICL is best thought of as a mixture of different algorithms, each with
its own peculiarities, instead of a monolithic capability. This also implies
that making general claims about ICL that hold universally across all settings
may be infeasible.

</details>


### [200] [A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture](https://arxiv.org/abs/2505.01196)
*Najmus Sakib Sizan, Md. Abu Layek, Khondokar Fida Hasan*

Main category: cs.LG

TL;DR: A novel system combining IoT, machine learning, and blockchain for accurate, secure crop forecasting.


<details>
  <summary>Details</summary>
Motivation: To enhance crop forecasting with actionable insights for farmers by leveraging real-time data and ensuring data integrity.

Method: Integrates IoT for real-time environmental monitoring, Random Forest for crop prediction (99.45% accuracy), and Ethereum blockchain for data security.

Result: Achieves high accuracy in crop predictions, provides tamper-proof data, and offers an intuitive interface for stakeholders.

Conclusion: The approach advances precision agriculture with accurate, secure, and user-friendly crop forecasting.

Abstract: To improve crop forecasting and provide farmers with actionable data-driven
insights, we propose a novel approach integrating IoT, machine learning, and
blockchain technologies. Using IoT, real-time data from sensor networks
continuously monitor environmental conditions and soil nutrient levels,
significantly improving our understanding of crop growth dynamics. Our study
demonstrates the exceptional accuracy of the Random Forest model, achieving a
99.45\% accuracy rate in predicting optimal crop types and yields, thereby
offering precise crop projections and customized recommendations. To ensure the
security and integrity of the sensor data used for these forecasts, we
integrate the Ethereum blockchain, which provides a robust and secure platform.
This ensures that the forecasted data remain tamper-proof and reliable.
Stakeholders can access real-time and historical crop projections through an
intuitive online interface, enhancing transparency and facilitating informed
decision-making. By presenting multiple predicted crop scenarios, our system
enables farmers to optimize production strategies effectively. This integrated
approach promises significant advances in precision agriculture, making crop
forecasting more accurate, secure, and user-friendly.

</details>


### [201] [2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](https://arxiv.org/abs/2505.01286)
*Yajuan Zhang, Jiahai Jiang, Yule Yan, Liang Yang, Ping Zhang*

Main category: cs.LG

TL;DR: The paper proposes 2DXformer, a deep learning model for wind power forecasting that improves accuracy by addressing limitations in inter-variable relationships and unnecessary interactions between endogenous and exogenous variables.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for wind power forecasting lack modeling of inter-variable relationships and treat endogenous and exogenous variables equally, limiting accuracy and increasing model complexity.

Method: The 2DXformer classifies inputs into exogenous static, exogenous dynamic, and endogenous variables, embeds them as variable tokens, uses attention for exogenous correlations, and employs a multi-layer perceptron with residual connections to model impacts on endogenous variables.

Result: Experiments on real-world datasets show 2DXformer improves wind power forecasting performance.

Conclusion: The 2DXformer effectively addresses previous limitations and enhances forecasting accuracy, with code available for further use.

Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans,
which is of great significance for maintaining the safety, stability, and
efficient operation of the power system. In recent years, wind power
forecasting methods based on deep learning have focused on extracting the
spatiotemporal correlations among data, achieving significant improvements in
forecasting accuracy. However, they exhibit two limitations. First, there is a
lack of modeling for the inter-variable relationships, which limits the
accuracy of the forecasts. Second, by treating endogenous and exogenous
variables equally, it leads to unnecessary interactions between the endogenous
and exogenous variables, increasing the complexity of the model. In this paper,
we propose the 2DXformer, which, building upon the previous work's focus on
spatiotemporal correlations, addresses the aforementioned two limitations.
Specifically, we classify the inputs of the model into three types: exogenous
static variables, exogenous dynamic variables, and endogenous variables. First,
we embed these variables as variable tokens in a channel-independent manner.
Then, we use the attention mechanism to capture the correlations among
exogenous variables. Finally, we employ a multi-layer perceptron with residual
connections to model the impact of exogenous variables on endogenous variables.
Experimental results on two real-world large-scale datasets indicate that our
proposed 2DXformer can further improve the performance of wind power
forecasting. The code is available in this repository:
\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.

</details>


### [202] [CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning](https://arxiv.org/abs/2505.01199)
*Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour, Aaqib Saeed*

Main category: cs.LG

TL;DR: CaReAQA, an audio-language model, integrates foundation audio and large language models for medical diagnostics, achieving 86.2% accuracy in open-ended tasks and 56.9% in unseen datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for medical audio analysis rely on handcrafted features or supervised deep learning, which are limited by scalability and the need for labeled data.

Method: Proposes CaReAQA, combining foundation audio models with large language models, and introduces CaReSound, a benchmark dataset with annotated medical audio and QA pairs.

Result: CaReAQA achieves 86.2% accuracy in open-ended diagnostic reasoning and 56.9% in closed-ended tasks on unseen data.

Conclusion: Audio-language integration and reasoning enhance medical diagnostics, enabling efficient AI for clinical decision support.

Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in
clinical diagnosis. However, analyzing these signals remains challenging:
traditional methods rely on handcrafted features or supervised deep learning
models that demand extensive labeled datasets, limiting their scalability and
applicability. To address these issues, we propose CaReAQA, an audio-language
model that integrates a foundation audio model with the reasoning capabilities
of large language models, enabling clinically relevant, open-ended diagnostic
responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of
annotated medical audio recordings enriched with metadata and paired
question-answer examples, intended to drive progress in diagnostic reasoning
research. Evaluation results show that CaReAQA achieves 86.2% accuracy on
open-ended diagnostic reasoning tasks, outperforming baseline models. It also
generalizes well to closed-ended classification tasks, achieving an average
accuracy of 56.9% on unseen datasets. Our findings show how audio-language
integration and reasoning advances medical diagnostics, enabling efficient AI
systems for clinical decision support.

</details>


### [203] [AGRO: An Autonomous AI Rover for Precision Agriculture](https://arxiv.org/abs/2505.01200)
*Simar Ghumman, Fabio Di Troia, William Andreopoulos, Mark Stamp, Sanjit Rai*

Main category: cs.LG

TL;DR: AGRO, an autonomous UGV, uses machine learning and sensors to automate agricultural tasks like yield estimation and obstacle avoidance, aiding farmers in data-driven decisions.


<details>
  <summary>Details</summary>
Motivation: To address complex agricultural problems by automating resource-intensive operations and supporting farmers with data-driven insights.

Method: Develops AGRO, a UGV integrating machine learning, computer vision, and sensors for autonomous navigation, yield estimation, and real-time environmental mapping.

Result: AGRO successfully performs tasks like pistachio yield determination, self-localization, and obstacle avoidance.

Conclusion: AGRO demonstrates the potential of UGVs in precision agriculture, offering a foundation for advanced machine learning applications and improved farming efficiency.

Abstract: Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.

</details>


### [204] [Activation Steering in Neural Theorem Provers](https://arxiv.org/abs/2502.15507)
*Shashank Kirtania*

Main category: cs.LG

TL;DR: Activation steering improves LLMs' theorem-proving by guiding tactic selection, offering a lightweight alternative to fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with ranking correct tactics in theorem proving, limiting their effectiveness.

Method: Use activation steering to guide LLM responses during inference for better tactic selection.

Result: Activation steering enhances theorem-proving capabilities without extensive fine-tuning.

Conclusion: Activation steering is a promising, resource-efficient method for improving LLMs in theorem proving.

Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems
using proof assistants like Lean. However, current state of the art language
models struggles to predict next step in proofs leading practitioners to use
different sampling techniques to improve LLMs capabilities. We observe that the
LLM is capable of predicting the correct tactic; however, it faces challenges
in ranking it appropriately within the set of candidate tactics, affecting the
overall selection process. To overcome this hurdle, we use activation steering
to guide LLMs responses to improve the generations at the time of inference.
Our results suggest that activation steering offers a promising lightweight
alternative to specialized fine-tuning for enhancing theorem proving
capabilities in LLMs, particularly valuable in resource-constrained
environments.

</details>


### [205] [Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2505.01218)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Kernel Logistic Regression (KLR) enhances Hopfield networks, achieving high capacity (up to 4.0 P/N) and robustness with minimal spurious attractors.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional Hopfield networks, such as low storage capacity and spurious attractors, by leveraging KLR's non-linear mapping.

Method: Quantitative analysis of attractor structures in KLR-trained networks via simulations, evaluating recall, convergence rates, and speed under varied storage loads and noise levels.

Result: KLR demonstrates superior performance: high capacity, robustness, clean attractor landscape, and fast dynamics (1-2 steps for high-similarity states).

Conclusion: KLR effectively reshapes dynamics for high-capacity associative memory, offering significant improvements over traditional methods.

Abstract: Traditional Hopfield networks, using Hebbian learning, face severe storage
capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic
Regression (KLR) offers a non-linear approach, mapping patterns to
high-dimensional feature spaces for improved separability. Our previous work
showed KLR dramatically improves capacity and noise robustness over
conventional methods. This paper quantitatively analyzes the attractor
structures in KLR-trained networks via extensive simulations. We evaluated
recall from diverse initial states across wide storage loads (up to 4.0 P/N)
and noise levels. We quantified convergence rates and speed. Our analysis
confirms KLR's superior performance: high capacity (up to 4.0 P/N) and
robustness. The attractor landscape is remarkably "clean," with near-zero
spurious fixed points. Recall failures under high load/noise are primarily due
to convergence to other learned patterns, not spurious ones. Dynamics are
exceptionally fast (typically 1-2 steps for high-similarity states). This
characterization reveals how KLR reshapes dynamics for high-capacity
associative memory, highlighting its effectiveness and contributing to AM
understanding.

</details>


### [206] [mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi](https://arxiv.org/abs/2505.01242)
*Evelyn Chapuma, Grey Mengezi, Lewis Msasa, Amelia Taylor*

Main category: cs.LG

TL;DR: The mwBTFreddy dataset supports flash flood damage assessment in Malawi using pre- and post-disaster satellite images and labeled building annotations for machine learning and spatial analysis.


<details>
  <summary>Details</summary>
Motivation: To aid in building detection, damage classification, and decision-making for flood response in climate-vulnerable urban areas of Malawi.

Method: Paired satellite images and JSON-labeled building annotations (damage levels and coordinates) are provided for machine learning model development.

Result: A dataset enabling flood damage visualization, spatial analysis, and informed decisions on relocation and emergency response.

Conclusion: The mwBTFreddy dataset is a valuable resource for AI-driven flood damage assessment and planning in African urban contexts.

Abstract: This paper describes the mwBTFreddy dataset, a resource developed to support
flash flood damage assessment in urban Malawi, specifically focusing on the
impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and
post-disaster satellite images sourced from Google Earth Pro, accompanied by
JSON files containing labelled building annotations with geographic coordinates
and damage levels (no damage, minor, major, or destroyed). Developed by the
Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this
dataset is intended to facilitate the development of machine learning models
tailored to building detection and damage classification in African urban
contexts. It also supports flood damage visualisation and spatial analysis to
inform decisions on relocation, infrastructure planning, and emergency response
in climate-vulnerable regions.

</details>


### [207] [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
*Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian*

Main category: cs.LG

TL;DR: LLM agents improve performance by learning from self-generated examples, reducing reliance on task-specific engineering. Extensions like database and exemplar-level selection further boost results.


<details>
  <summary>Details</summary>
Motivation: To reduce dependence on task-specific knowledge engineering for LLM agents by enabling automatic learning from successful experiences.

Method: Construct and refine a database of self-generated examples, with extensions like population-based training and exemplar-level selection.

Result: Performance improved on benchmarks (ALFWorld: 73% to 91%, Wordcraft: 55% to 64%, InterCode-SQL: 75% to 79%), matching or exceeding task-specific approaches.

Conclusion: Automatic trajectory database construction is a viable alternative to manual knowledge engineering, enhancing LLM agent performance.

Abstract: Many methods for improving Large Language Model (LLM) agents for sequential
decision-making tasks depend on task-specific knowledge engineering--such as
prompt tuning, curated in-context examples, or customized observation and
action spaces. Using these approaches, agent performance improves with the
quality or amount of knowledge engineering invested. Instead, we investigate
how LLM agents can automatically improve their performance by learning
in-context from their own successful experiences on similar tasks. Rather than
relying on task-specific knowledge engineering, we focus on constructing and
refining a database of self-generated examples. We demonstrate that even a
naive accumulation of successful trajectories across training tasks boosts test
performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),
and InterCode-SQL (75% to 79%)--matching the performance the initial agent
achieves if allowed two to three attempts per task. We then introduce two
extensions: (1) database-level selection through population-based training to
identify high-performing example collections, and (2) exemplar-level selection
that retains individual trajectories based on their empirical utility as
in-context examples. These extensions further enhance performance, achieving
91% on ALFWorld--matching more complex approaches that employ task-specific
components and prompts. Our results demonstrate that automatic trajectory
database construction offers a compelling alternative to labor-intensive
knowledge engineering.

</details>


### [208] [MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2505.01279)
*Zhaoyan Wang, Xiangchi Song, In-Young Ko*

Main category: cs.LG

TL;DR: Proposes MultiGran-STGCNFog, a fog-based system for efficient traffic forecasting with multi-granular spatiotemporal feature fusion and optimized scheduling.


<details>
  <summary>Details</summary>
Motivation: Current GCN-based methods lack sufficient multi-granular spatiotemporal feature extraction and fusion, leading to inaccurate forecasts and slow inference due to increased model complexity.

Method: Introduces MultiGran-STGCNFog, which fuses multi-granular spatiotemporal features on dynamic traffic graphs and uses GA-DPHDS for optimized layer-device scheduling.

Result: Outperforms baselines in experiments, showing improved accuracy and inference throughput.

Conclusion: The system effectively addresses limitations of existing methods, offering accurate and fast traffic forecasting.

Abstract: Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.

</details>


### [209] [Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](https://arxiv.org/abs/2505.01332)
*Mohammed Sumayli, Olugbenga Moses Anubi*

Main category: cs.LG

TL;DR: The paper introduces a dynamic Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework to optimize energy consumption by incorporating dynamic, consumer-defined preferences, outperforming traditional methods like MILP in efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing HEMS often use static weighting factors for consumer comfort, ignoring dynamic behaviors. This paper aims to enhance consumer involvement in Demand Response programs by embedding dynamic preferences.

Method: The study employs a model-free, single-agent DRL algorithm to create a dynamic and user-friendly HEMS framework, validated using real-world data at 15-minute intervals.

Result: The DRL-HEMS framework optimizes energy consumption effectively across different preference modes and outperforms traditional MILP-based algorithms in computational efficiency.

Conclusion: The proposed DRL-HEMS framework successfully addresses the limitations of static approaches, offering a dynamic and efficient solution for smart home energy management.

Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the
smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and
improve user comfort. By enabling intelligent control and optimization of
household energy consumption, HEMS plays a significant role in bridging the gap
between consumer needs and energy utility objectives. However, much of the
existing literature construes consumer comfort as a mere deviation from the
standard appliance settings. Such deviations are typically incorporated into
optimization objectives via static weighting factors. These factors often
overlook the dynamic nature of consumer behaviors and preferences. Addressing
this oversight, our paper introduces a multi-mode Deep Reinforcement
Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize
based on dynamic, consumer-defined preferences. Our primary goal is to augment
consumer involvement in Demand Response (DR) programs by embedding dynamic
multi-mode preferences tailored to individual appliances. In this study, we
leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework
that is not only dynamic but also user-friendly. To validate its efficacy, we
employed real-world data at 15-minute intervals, including metrics such as
electricity price, ambient temperature, and appliances' power consumption. Our
results show that the model performs exceptionally well in optimizing energy
consumption within different preference modes. Furthermore, when compared to
traditional algorithms based on Mixed-Integer Linear Programming (MILP), our
model achieves nearly optimal performance while outperforming in computational
efficiency.

</details>


### [210] [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336)
*Vincenzo De Paola, Riccardo Zamboni, Mirco Mutti, Marcello Restelli*

Main category: cs.LG

TL;DR: A novel RL framework maximizes data entropy in parallel settings by balancing individual agent entropy and inter-agent diversity, outperforming identical-agent systems.


<details>
  <summary>Details</summary>
Motivation: To surpass the $N$ factor acceleration in parallel RL by specializing agent policies and minimizing data redundancy.

Method: Uses a centralized policy gradient method to balance entropy and diversity, integrating with batch RL techniques.

Result: Empirical evaluation shows promise, and concentration analysis confirms faster rates for specialized parallel sampling.

Conclusion: Specialized parallel policies enhance data efficiency and performance, supported by theoretical and empirical evidence.

Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking
unprecedented efficiency and powering breakthroughs in large-scale real-world
applications. In this paradigm, $N$ identical agents operate in $N$ replicas of
an environment simulator, accelerating data collection by a factor of $N$. A
critical question arises: \textit{Does specializing the policies of the
parallel agents hold the key to surpass the $N$ factor acceleration?} In this
paper, we introduce a novel learning framework that maximizes the entropy of
collected data in a parallel setting. Our approach carefully balances the
entropy of individual agents with inter-agent diversity, effectively minimizing
redundancies. The latter idea is implemented with a centralized policy gradient
method, which shows promise when evaluated empirically against systems of
identical agents, as well as synergy with batch RL techniques that can exploit
data diversity. Finally, we provide an original concentration analysis that
shows faster rates for specialized parallel sampling distributions, which
supports our methodology and may be of independent interest.

</details>


### [211] [How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](https://arxiv.org/abs/2505.01346)
*Marie-Charlotte Brandenburg, Katharina Jochemko*

Main category: cs.LG

TL;DR: The paper explores binary classification using continuous piecewise linear functions with starshaped polyhedral decision boundaries. It analyzes expressivity, VC dimension, and loss landscape geometry for 0/1-loss and exponential loss.


<details>
  <summary>Details</summary>
Motivation: To understand the expressivity and loss landscape of binary classifiers with starshaped polyhedral decision boundaries, providing insights into their combinatorial and geometric properties.

Method: Investigates the expressivity of continuous piecewise linear functions, bounds VC dimension, and describes loss sublevel sets for 0/1-loss and exponential loss.

Result: Explicit VC dimension bounds, sublevel sets of 0/1-loss as hyperplane arrangement chambers, and conditions for unique optima in exponential loss.

Conclusion: The study provides a detailed geometric and combinatorial understanding of binary classifiers with starshaped polyhedral boundaries, aiding in model design and analysis.

Abstract: We consider binary classification restricted to a class of continuous
piecewise linear functions whose decision boundaries are (possibly nonconvex)
starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We
investigate the expressivity of these function classes and describe the
combinatorial and geometric structure of the loss landscape, most prominently
the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an
exponential loss function. In particular, we give explicit bounds on the VC
dimension of this model, and concretely describe the sublevel sets of the
discrete loss as chambers in a hyperplane arrangement. For the exponential
loss, we give sufficient conditions for the optimum to be unique, and describe
the geometry of the optimum when varying the rate parameter of the underlying
exponential probability distribution.

</details>


### [212] [Learning Stabilizing Policies via an Unstable Subspace Representation](https://arxiv.org/abs/2505.01348)
*Leonardo F. Toso, Lintao Ye, James Anderson*

Main category: cs.LG

TL;DR: The paper proposes a two-phase method for learning to stabilize linear time-invariant systems, focusing on reducing sample complexity by targeting the unstable subspace.


<details>
  <summary>Details</summary>
Motivation: Existing methods for stabilizing unknown systems require large data and scale poorly with system dimensions. The goal is to reduce sample complexity by focusing on unstable dynamics.

Method: A two-phase approach: (1) learn the unstable subspace, (2) solve discounted LQR problems on this subspace to stabilize only unstable dynamics.

Result: The method reduces sample complexity, especially when unstable modes are fewer than state dimensions, speeding up stabilization. Numerical experiments support this.

Conclusion: Targeting the unstable subspace is effective for reducing sample complexity in stabilizing LTI systems.

Abstract: We study the problem of learning to stabilize (LTS) a linear time-invariant
(LTI) system. Policy gradient (PG) methods for control assume access to an
initial stabilizing policy. However, designing such a policy for an unknown
system is one of the most fundamental problems in control, and it may be as
hard as learning the optimal policy itself. Existing work on the LTS problem
requires large data as it scales quadratically with the ambient dimension. We
propose a two-phase approach that first learns the left unstable subspace of
the system and then solves a series of discounted linear quadratic regulator
(LQR) problems on the learned unstable subspace, targeting to stabilize only
the system's unstable dynamics and reduce the effective dimension of the
control space. We provide non-asymptotic guarantees for both phases and
demonstrate that operating on the unstable subspace reduces sample complexity.
In particular, when the number of unstable modes is much smaller than the state
dimension, our analysis reveals that LTS on the unstable subspace substantially
speeds up the stabilization process. Numerical experiments are provided to
support this sample complexity reduction achieved by our approach.

</details>


### [213] [Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation](https://arxiv.org/abs/2505.01361)
*Hwanwoo Kim, Panos Toulis, Eric Laber*

Main category: cs.LG

TL;DR: Implicit TD learning algorithms are proposed to address the step size sensitivity of traditional TD learning, offering stability and efficiency with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional TD learning is widely used but sensitive to step size, requiring tedious trial and error for optimal performance.

Method: Reformulate TD updates into fixed-point equations to create implicit TD algorithms, ensuring stability and reduced step size sensitivity.

Result: Implicit TD algorithms provide robust performance, asymptotic convergence guarantees, and finite-time error bounds.

Conclusion: Implicit TD is a practical and versatile alternative for policy evaluation and value approximation in modern RL tasks.

Abstract: Temporal Difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, it is not without
drawbacks, the most prominent being its sensitivity to step size. A poor choice
of step size can dramatically inflate the error of value estimates and slow
convergence. Consequently, in practice, researchers must use trial and error in
order to identify a suitable step size -- a process that can be tedious and
time consuming. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed-point equations. These updates are more
stable and less sensitive to step size without sacrificing computational
efficiency. Moreover, our theoretical analysis establishes asymptotic
convergence guarantees and finite-time error bounds. Our results demonstrate
their robustness and practicality for modern RL tasks, establishing implicit TD
as a versatile tool for policy evaluation and value approximation.

</details>


### [214] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)
*Irene Wang, Newsha Ardalani, Mostafa Elhoushi, Daniel Jiang, Samuel Hsia, Ekin Sumbul, Divya Mahajan, Carole-Jean Wu, Bilge Acun*

Main category: cs.LG

TL;DR: CATransformers is a carbon-aware framework for optimizing ML models and hardware to reduce both operational and embodied carbon emissions, achieving a 17% reduction in total carbon for CLIP models.


<details>
  <summary>Details</summary>
Motivation: The environmental impact of ML systems, especially their carbon footprint, lacks comprehensive evaluation tools, particularly for embodied emissions from hardware.

Method: Proposes CATransformers, a framework for co-optimizing ML models and hardware architectures using carbon metrics in early design exploration.

Result: CarbonCLIP models reduce total carbon emissions by up to 17% while maintaining accuracy and latency compared to baselines.

Conclusion: Holistic optimization methods are essential for designing sustainable, high-performance AI systems.

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>


### [215] [Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems](https://arxiv.org/abs/2302.03669)
*Ming Zhu, Xiao-Yang Liu, Sem Borst, Anwar Walid*

Main category: cs.LG

TL;DR: The paper explores deep reinforcement learning (DRL) for smart traffic light control, showing DQN and DDPG algorithms can optimize traffic flow, with the 'greenwave' policy emerging as optimal in grid networks.


<details>
  <summary>Details</summary>
Motivation: To address poor scalability in conventional traffic light control methods and improve traffic efficiency using DRL.

Method: Uses DQN for single intersections and DDPG for grid networks, with theoretical and numerical validation.

Result: DQN delivers optimal control in single intersections, while DDPG produces the 'greenwave' policy, proven optimal in grid networks.

Conclusion: DRL algorithms are scalable and effective for traffic light control, with 'greenwave' emerging as an optimal solution.

Abstract: Smart traffic lights in intelligent transportation systems (ITSs) are
envisioned to greatly increase traffic efficiency and reduce congestion. Deep
reinforcement learning (DRL) is a promising approach to adaptively control
traffic lights based on the real-time traffic situation in a road network.
However, conventional methods may suffer from poor scalability. In this paper,
we investigate deep reinforcement learning to control traffic lights, and both
theoretical analysis and numerical experiments show that the intelligent
behavior ``greenwave" (i.e., a vehicle will see a progressive cascade of green
lights, and not have to brake at any intersection) emerges naturally a grid
road network, which is proved to be the optimal policy in an avenue with
multiple cross streets. As a first step, we use two DRL algorithms for the
traffic light control problems in two scenarios. In a single road intersection,
we verify that the deep Q-network (DQN) algorithm delivers a thresholding
policy; and in a grid road network, we adopt the deep deterministic policy
gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN
algorithm delivers the optimal control, and the DDPG algorithm with passive
observations has the capability to produce on its own a high-level intelligent
behavior in a grid road network, namely, the ``greenwave" policy emerges. We
also verify the ``greenwave" patterns in a $5 \times 10$ grid road network.
Thirdly, the ``greenwave" patterns demonstrate that DRL algorithms produce
favorable solutions since the ``greenwave" policy shown in experiment results
is proved to be optimal in a specified traffic model (an avenue with multiple
cross streets). The delivered policies both in a single road intersection and a
grid road network demonstrate the scalability of DRL algorithms.

</details>


### [216] [Learning and Transferring Physical Models through Derivatives](https://arxiv.org/abs/2505.01391)
*Alessandro Trenta, Andrea Cossu, Davide Bacciu*

Main category: cs.LG

TL;DR: DERL is a supervised method for modeling physical systems by learning partial derivatives, with a distillation protocol for incremental model building. It outperforms SOTA methods in generalization and enables knowledge transfer across models.


<details>
  <summary>Details</summary>
Motivation: To model physical systems accurately and incrementally while adhering to underlying physical laws, even with empirical derivatives.

Method: DERL learns partial derivatives of physical systems and uses a distillation protocol for incremental model building, with theoretical guarantees.

Result: DERL outperforms SOTA in generalizing ODEs and PDEs to unseen conditions/parameters and enables knowledge transfer.

Conclusion: DERL is the first method for incremental physical model building, offering strong generalization and transfer capabilities.

Abstract: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.

</details>


### [217] [Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers](https://arxiv.org/abs/2403.07404)
*Filip Szatkowski, Yaoyue Zheng, Fei Yang, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer*

Main category: cs.LG

TL;DR: Auxiliary classifiers (ACs) improve continual learning by reducing forgetting and computational costs, achieving 10% accuracy gains and 10-60% inference cost savings.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by leveraging intermediate neural network representations, which are less prone to forgetting.

Method: Propose auxiliary classifiers (ACs) integrated into continual learning methods to enhance performance and reduce inference costs.

Result: ACs improve accuracy by 10% and reduce inference costs by 10-60% without losing accuracy.

Conclusion: ACs offer a scalable and efficient solution for continual learning by mitigating forgetting and optimizing computation.

Abstract: Continual learning is crucial for applying machine learning in challenging,
dynamic, and often resource-constrained environments. However, catastrophic
forgetting - overwriting previously learned knowledge when new information is
acquired - remains a major challenge. In this work, we examine the intermediate
representations in neural network layers during continual learning and find
that such representations are less prone to forgetting, highlighting their
potential to accelerate computation. Motivated by these findings, we propose to
use auxiliary classifiers(ACs) to enhance performance and demonstrate that
integrating ACs into various continual learning methods consistently improves
accuracy across diverse evaluation settings, yielding an average 10% relative
gain. We also leverage the ACs to reduce the average cost of the inference by
10-60% without compromising accuracy, enabling the model to return the
predictions before computing all the layers. Our approach provides a scalable
and efficient solution for continual learning.

</details>


### [218] [Predicting the Price of Gold in the Financial Markets Using Hybrid Models](https://arxiv.org/abs/2505.01402)
*Mohammadhossein Rashidi, Mohammad Modarres*

Main category: cs.LG

TL;DR: A hybrid model combining ARIMA, stepwise regression, and neural networks predicts gold prices with higher accuracy than traditional time series methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate price prediction in financial markets by integrating multiple models for better results.

Method: Uses ARIMA for time series prediction, stepwise regression to select influential variables, and neural networks for final prediction.

Result: The hybrid model outperforms traditional time series, regression, and stepwise regression methods in accuracy.

Conclusion: The proposed hybrid model is effective for price prediction in financial markets and can be extended to other assets.

Abstract: Predicting the price that has the least error and can provide the best and
highest accuracy has been one of the most challenging issues and one of the
most critical concerns among capital market activists and researchers.
Therefore, a model that can solve problems and provide results with high
accuracy is one of the topics of interest among researchers. In this project,
using time series prediction models such as ARIMA to estimate the price,
variables, and indicators related to technical analysis show the behavior of
traders involved in involving psychological factors for the model. By linking
all of these variables to stepwise regression, we identify the best variables
influencing the prediction of the variable. Finally, we enter the selected
variables as inputs to the artificial neural network. In other words, we want
to call this whole prediction process the "ARIMA_Stepwise Regression_Neural
Network" model and try to predict the price of gold in international financial
markets. This approach is expected to be able to be used to predict the types
of stocks, commodities, currency pairs, financial market indicators, and other
items used in local and international financial markets. Moreover, a comparison
between the results of this method and time series methods is also expressed.
Finally, based on the results, it can be seen that the resulting hybrid model
has the highest accuracy compared to the time series method, regression, and
stepwise regression.

</details>


### [219] [How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades](https://arxiv.org/abs/2505.01415)
*Rahuul Rangaraj, Jimeng Shi, Azam Shirali, Rajendra Paudel, Yanzhao Wu, Giri Narasimhan*

Main category: cs.LG

TL;DR: The study evaluates deep learning and foundation models for water level prediction in the Everglades, finding Chronos outperforms others.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for water level prediction are costly and inflexible; advanced models offer potential solutions.

Method: Twelve task-specific and five foundation models were tested for water level prediction.

Result: Chronos, a foundation model, outperformed others, while other foundation models performed poorly. Task-specific models varied by architecture.

Conclusion: Foundation models like Chronos show promise for environmental applications, but performance varies widely.

Abstract: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.

</details>


### [220] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)
*Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah*

Main category: cs.LG

TL;DR: The paper introduces evaluations to detect AI models' potential for scheming, focusing on stealth and situational awareness, and finds current models safe.


<details>
  <summary>Details</summary>
Motivation: To address the risk of AI models covertly pursuing misaligned objectives, which could lead to severe loss of control.

Method: Proposes 16 evaluations (5 for stealth, 11 for situational awareness) to measure scheming prerequisites.

Result: Current frontier models show no concerning levels of stealth or situational awareness.

Conclusion: The evaluations can help ensure AI safety by ruling out scheming capabilities before deployment.

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [221] [Tree-Sliced Wasserstein Distance: A Geometric Perspective](https://arxiv.org/abs/2406.13725)
*Viet-Hoang Tran, Trang Pham, Tho Tran, Minh Khoi Nguyen Nhat, Thanh Chu, Tam Le, Tan M. Nguyen*

Main category: cs.LG

TL;DR: The paper introduces Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL), a novel variant of Optimal Transport (OT) using tree systems to mitigate information loss in Sliced Wasserstein (SW) methods.


<details>
  <summary>Details</summary>
Motivation: To address the computational burden and loss of topological information in SW methods by leveraging tree systems for more intricate projections.

Method: Proposes tree systems with tree metrics, splitting maps for projections, and a novel Radon transform for tree systems. Introduces TSW-SL as an efficient metric.

Result: TSW-SL outperforms SW and its variants in experiments on gradient flows, image style transfer, and generative models.

Conclusion: The framework effectively balances computational efficiency and topological information retention, offering improved performance over existing methods.

Abstract: Many variants of Optimal Transport (OT) have been developed to address its
heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used
for application domains by projecting the OT problem onto one-dimensional
lines, and leveraging the closed-form expression of the univariate OT to reduce
the computational burden. However, projecting measures onto low-dimensional
spaces can lead to a loss of topological information. To mitigate this issue,
in this work, we propose to replace one-dimensional lines with a more intricate
structure, called tree systems. This structure is metrizable by a tree metric,
which yields a closed-form expression for OT problems on tree systems. We
provide an extensive theoretical analysis to formally define tree systems with
their topological properties, introduce the concept of splitting maps, which
operate as the projection mechanism onto these structures, then finally propose
a novel variant of Radon transform for tree systems and verify its injectivity.
This framework leads to an efficient metric between measures, termed
Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a
variety of experiments on gradient flows, image style transfer, and generative
models, we illustrate that our proposed approach performs favorably compared to
SW and its variants.

</details>


### [222] [Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing](https://arxiv.org/abs/2505.01424)
*D. Patel, R. Sharma, Y. B. Guo*

Main category: cs.LG

TL;DR: The paper evaluates modeling strategies for microstructure prediction in metal additive manufacturing, emphasizing the potential of physics-informed machine learning (PIML) to overcome limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Metal AM produces complex components but faces challenges in predicting heterogeneous microstructures, which affect mechanical properties. Existing methods (experimental, simulations, ML) have limitations in accuracy, generalizability, and physical consistency.

Method: The paper reviews experimental, computational, and data-driven methods, focusing on hybrid PIML frameworks that integrate physical laws with ML for better microstructure prediction.

Result: PIML enhances accuracy, transparency, and scalability in microstructure modeling, addressing challenges like data scarcity and multi-scale coupling.

Conclusion: Hybrid PIML approaches are crucial for predictive, physically consistent microstructure modeling, enabling better process control and reliable AM component production.

Abstract: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.

</details>


### [223] [Towards Aligned Data Removal via Twin Machine Unlearning](https://arxiv.org/abs/2408.11433)
*Haoxuan Ji, Zheng Lin, Yuyao Sun, Gao Fei, Yuhang Wang, Haichang Gao, Zhenxing Niu*

Main category: cs.LG

TL;DR: The paper introduces Twin Machine Unlearning (TMU), a method to align unlearned models with gold models by defining a twin unlearning problem, improving accuracy and data removal efficiency.


<details>
  <summary>Details</summary>
Motivation: Modern privacy regulations require efficient data removal from trained ML models without retraining, but existing methods misalign unlearned models with gold models.

Method: Proposes TMU, defining a twin unlearning problem to transfer a generalization-label predictor to the original problem, ensuring aligned data removal.

Result: TMU significantly improves alignment between unlearned and gold models while maintaining model accuracy.

Conclusion: TMU offers an effective solution for privacy-compliant machine unlearning by aligning models and preserving accuracy.

Abstract: Modern privacy regulations have spurred the evolution of machine unlearning,
a technique that enables the removal of data from an already trained ML model
without requiring retraining from scratch. Previous unlearning methods tend to
induce the model to achieve lowest classification accuracy on the removal data.
Nonetheless, the authentic objective of machine unlearning is to align the
unlearned model with the gold model, i.e., achieving the same classification
accuracy as the gold model. For this purpose, we present a Twin Machine
Unlearning (TMU) approach, where a twin unlearning problem is defined
corresponding to the original unlearning problem. As a results, the
generalization-label predictor trained on the twin problem can be transferred
to the original problem, facilitating aligned data removal. Comprehensive
empirical experiments illustrate that our approach significantly enhances the
alignment between the unlearned model and the gold model. Meanwhile, our method
allows data removal without compromising the model accuracy.

</details>


### [224] [Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework](https://arxiv.org/abs/2409.04744)
*Yongxin Deng, Xihe Qiu, Jue Chen, Xiaoyu Tan*

Main category: cs.LG

TL;DR: LMGT is a sample-efficient RL framework that uses LLMs to guide reward tuning, balancing exploration and exploitation, and reducing computational resources.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing exploration and exploitation in RL, especially in sparse-reward environments, by leveraging prior knowledge from LLMs.

Method: Proposes LMGT, which uses LLMs to guide reward shifts, improving exploratory behavior and sample efficiency.

Result: LMGT outperforms baseline methods in RL tasks and reduces computational resource usage.

Conclusion: LMGT effectively leverages LLMs to enhance RL performance and efficiency.

Abstract: The inherent uncertainty in the environmental transition model of
Reinforcement Learning (RL) necessitates a delicate balance between exploration
and exploitation. This balance is crucial for optimizing computational
resources to accurately estimate expected rewards for the agent. In scenarios
with sparse rewards, such as robotic control systems, achieving this balance is
particularly challenging. However, given that many environments possess
extensive prior knowledge, learning from the ground up in such contexts may be
redundant. To address this issue, we propose Language Model Guided reward
Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their proficiency in processing non-standard data forms, such as wiki
tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances
exploration and exploitation, thereby guiding the agent's exploratory behavior
and enhancing sample efficiency. We have rigorously evaluated LMGT across
various RL tasks and evaluated it in the embodied robotic environment
Housekeep. Our results demonstrate that LMGT consistently outperforms baseline
methods. Furthermore, the findings suggest that our framework can substantially
reduce the computational resources required during the RL training phase.

</details>


### [225] [Reward-Augmented Data Enhances Direct Preference Alignment of LLMs](https://arxiv.org/abs/2410.08067)
*Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang*

Main category: cs.LG

TL;DR: The paper introduces reward-conditioned LLM policies to improve preference alignment by leveraging qualitative aspects of responses, addressing overfitting and unlearning issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing alignment algorithms focus on relative preferences and ignore qualitative aspects, leading to overfitting and poor generalization.

Method: Proposes reward-conditioned LLM policies and a data relabeling method to construct a reward-augmented dataset.

Result: Experiments show the method consistently outperforms DPO, maximizing utility of preference data and mitigating unlearning.

Conclusion: The approach effectively improves preference alignment and generalizes better, demonstrating broad effectiveness.

Abstract: Preference alignment in Large Language Models (LLMs) has significantly
improved their ability to adhere to human instructions and intentions. However,
existing direct alignment algorithms primarily focus on relative preferences
and often overlook the qualitative aspects of responses, despite having access
to preference data that includes reward scores from judge models during AI
feedback. Striving to maximize the implicit reward gap between the chosen and
the slightly inferior rejected responses can cause overfitting and unnecessary
unlearning of the high-quality rejected responses. The unawareness of the
reward scores also drives the LLM to indiscriminately favor the low-quality
chosen responses and fail to generalize to optimal responses that are sparse in
data. To overcome these shortcomings, our study introduces reward-conditioned
LLM policies that discern and learn from the entire spectrum of response
quality within the dataset, helping extrapolate to more optimal regions. We
propose an effective yet simple data relabeling method that conditions the
preference pairs on quality scores to construct a reward-augmented dataset. The
experiments across various benchmarks and diverse models demonstrate that our
approach consistently boosts DPO by a considerable margin. Through
comprehensive ablation studies, we demonstrate that our method not only
maximizes the utility of preference data but also mitigates the issue of
unlearning, demonstrating its broad effectiveness beyond mere data expansion.
Our code is available at
https://github.com/shenao-zhang/reward-augmented-preference.

</details>


### [226] [Offline Model-Based Optimization by Learning to Rank](https://arxiv.org/abs/2410.11502)
*Rong-Xi Tan, Ke Xue, Shen-Huan Lyu, Haopu Shang, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian*

Main category: cs.LG

TL;DR: The paper proposes a ranking-based model for offline model-based optimization (MBO) to address out-of-distribution errors in surrogate models, showing better performance than traditional regression-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional regression-based surrogate models in offline MBO often overestimate scores, leading to suboptimal designs. The paper argues that precise score prediction is unnecessary if the model maintains the relative order of designs.

Method: The authors propose a ranking-based model using learning-to-rank techniques to prioritize designs based on relative scores, rather than minimizing mean squared error (MSE).

Result: Experiments show weak correlation between MSE and final design quality, while ranking-based metrics correlate strongly. The proposed model outperforms 20 existing methods across diverse tasks.

Conclusion: Ranking-based models align better with offline MBO goals, offering superior performance by focusing on relative design order rather than precise score prediction.

Abstract: Offline model-based optimization (MBO) aims to identify a design that
maximizes a black-box function using only a fixed, pre-collected dataset of
designs and their corresponding scores. A common approach in offline MBO is to
train a regression-based surrogate model by minimizing mean squared error (MSE)
and then find the best design within this surrogate model by different
optimizers (e.g., gradient ascent). However, a critical challenge is the risk
of out-of-distribution errors, i.e., the surrogate model may typically
overestimate the scores and mislead the optimizers into suboptimal regions.
Prior works have attempted to address this issue in various ways, such as using
regularization techniques and ensemble learning to enhance the robustness of
the model, but it still remains. In this paper, we argue that regression models
trained with MSE are not well-aligned with the primary goal of offline MBO,
which is to select promising designs rather than to predict their scores
precisely. Notably, if a surrogate model can maintain the order of candidate
designs based on their relative score relationships, it can produce the best
designs even without precise predictions. To validate it, we conduct
experiments to compare the relationship between the quality of the final
designs and MSE, finding that the correlation is really very weak. In contrast,
a metric that measures order-maintaining quality shows a significantly stronger
correlation. Based on this observation, we propose learning a ranking-based
model that leverages learning to rank techniques to prioritize promising
designs based on their relative scores. We show that the generalization error
on ranking loss can be well bounded. Empirical results across diverse tasks
demonstrate the superior performance of our proposed ranking-based models than
twenty existing methods.

</details>


### [227] [Random Policy Enables In-Context Reinforcement Learning within Trust Horizons](https://arxiv.org/abs/2410.19982)
*Weiqin Chen, Santiago Paternain*

Main category: cs.LG

TL;DR: SAD enables effective in-context reinforcement learning (ICRL) using random policies, outperforming baselines by 236.3% offline and 135.2% online.


<details>
  <summary>Details</summary>
Motivation: Current ICRL methods require optimal or well-trained policies, limiting real-world applicability. SAD addresses this by working with random policies.

Method: SAD selects state-action pairs from random policies within a trust horizon and uses autoregressive-supervised pretraining.

Result: SAD outperforms baselines by 236.3% offline and 135.2% online, proving its effectiveness with random policies.

Conclusion: SAD is a novel, practical solution for ICRL, enabling performance without reliance on optimal policies.

Abstract: Pretrained foundation models have exhibited extraordinary in-context learning
performance, allowing zero-shot generalization to new tasks not encountered
during pretraining. In the case of reinforcement learning (RL), in-context RL
(ICRL) emerges when pretraining FMs on decision-making problems in an
autoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL
algorithms, like Algorithm Distillation, Decision Pretrained Transformer and
Decision Importance Transformer, impose stringent requirements on the
pretraining dataset concerning the source policies, context information, and
action labels. Notably, these algorithms either demand optimal policies or
require varying degrees of well-trained behavior policies for all pretraining
environments. This significantly hinders the application of ICRL to real-world
scenarios, where acquiring optimal or well-trained policies for a substantial
volume of real-world training environments can be intractable. To overcome this
challenge, we introduce a novel approach, termed State-Action Distillation
(SAD), that allows to generate an effective pretraining dataset guided solely
by random policies. In particular, SAD selects query states and corresponding
action labels by distilling outstanding state-action pairs from the entire
state and action spaces by using random policies within a trust horizon, and
then inherits the classical autoregressive-supervised mechanism during
pretraining. To the best of our knowledge, this is the first work that enables
effective ICRL under random policies and random contexts. We also establish
quantitative analysis of the trustworthiness as well as the performance
guarantees of SAD. Moreover, our empirical results across multiple popular ICRL
benchmark environments demonstrate that, on average, SAD outperforms the best
baseline by 236.3% in the offline evaluation and by 135.2% in the online
evaluation.

</details>


### [228] [DriveGPT: Scaling Autoregressive Behavior Models for Driving](https://arxiv.org/abs/2412.14415)
*Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa*

Main category: cs.LG

TL;DR: DriveGPT is a scalable transformer-based behavior model for autonomous driving, demonstrating improved performance with increased data and model size.


<details>
  <summary>Details</summary>
Motivation: To explore the scaling properties of behavior models for autonomous driving and validate the benefits of large-scale data and model parameters.

Method: DriveGPT models driving as a sequential decision-making task using a transformer, trained autoregressively on a large-scale dataset.

Result: Outperforms state-of-the-art baselines in prediction tasks and shows improved planning performance in real-world scenarios.

Conclusion: Scaling model parameters and training data significantly enhances performance, validating the approach for autonomous driving.

Abstract: We present DriveGPT, a scalable behavior model for autonomous driving. We
model driving as a sequential decision-making task, and learn a transformer
model to predict future agent states as tokens in an autoregressive fashion. We
scale up our model parameters and training data by multiple orders of
magnitude, enabling us to explore the scaling properties in terms of dataset
size, model parameters, and compute. We evaluate DriveGPT across different
scales in a planning task, through both quantitative metrics and qualitative
examples, including closed-loop driving in complex real-world scenarios. In a
separate prediction task, DriveGPT outperforms state-of-the-art baselines and
exhibits improved performance by pretraining on a large-scale dataset, further
validating the benefits of data scaling.

</details>


### [229] [Test-time regression: a unifying framework for designing sequence models with associative memory](https://arxiv.org/abs/2501.12352)
*Ke Alexander Wang, Jiaxin Shi, Emily B. Fox*

Main category: cs.LG

TL;DR: The paper introduces a unifying framework for sequence models, formalizing associative recall as memorization and retrieval, and derives prominent architectures like Transformers as special cases.


<details>
  <summary>Details</summary>
Motivation: To address the diversity of seemingly unrelated sequence model architectures and provide a unified understanding inspired by associative recall.

Method: The framework formalizes associative recall as a two-step process (memorization and retrieval), casting memorization as regression. It analyzes design choices like regression weights and optimization algorithms.

Result: The approach explains limitations of linear attention, justifies query-key normalization in softmax attention, and leads to novel higher-order generalizations.

Conclusion: The work unifies sequence modeling with regression methods, offering a foundation for more powerful and theoretically grounded architectures.

Abstract: Sequence models lie at the heart of modern deep learning. However, rapid
advancements have produced a diversity of seemingly unrelated architectures,
such as Transformers and recurrent alternatives. In this paper, we introduce a
unifying framework to understand and derive these sequence models, inspired by
the empirical importance of associative recall, the capability to retrieve
contextually relevant tokens. We formalize associative recall as a two-step
process, memorization and retrieval, casting memorization as a regression
problem. Layers that combine these two steps perform associative recall via
``test-time regression'' over its input tokens. Prominent layers, including
linear attention, state-space models, fast-weight programmers, online learners,
and softmax attention, arise as special cases defined by three design choices:
the regression weights, the regressor function class, and the test-time
optimization algorithm. Our approach clarifies how linear attention fails to
capture inter-token correlations and offers a mathematical justification for
the empirical effectiveness of query-key normalization in softmax attention.
Further, it illuminates unexplored regions within the design space, which we
use to derive novel higher-order generalizations of softmax attention. Beyond
unification, our work bridges sequence modeling with classic regression
methods, a field with extensive literature, paving the way for developing more
powerful and theoretically principled architectures.

</details>


### [230] [Transfer Learning of Surrogate Models via Domain Affine Transformation Across Synthetic and Real-World Benchmarks](https://arxiv.org/abs/2501.14012)
*Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang*

Main category: cs.LG

TL;DR: The paper proposes a method to transfer non-differentiable surrogate models (e.g., random forests) between tasks with limited transfer data, reducing data and computational costs.


<details>
  <summary>Details</summary>
Motivation: High-quality surrogate models require extensive data, but transferring pre-trained models can mitigate this if invariances exist between tasks.

Method: Extends previous work on differentiable models to random forests, assuming domains are related by an unknown affine transformation, and tests on BBOB and real-world problems.

Result: The method significantly reduces data requirements and computational costs for training surrogate models in complex scenarios.

Conclusion: The proposed approach is practical and effective for transferring non-differentiable surrogate models, offering advantages in efficiency and data usage.

Abstract: Surrogate models are frequently employed as efficient substitutes for the
costly execution of real-world processes. However, constructing a high-quality
surrogate model often demands extensive data acquisition. A solution to this
issue is to transfer pre-trained surrogate models for new tasks, provided that
certain invariances exist between tasks. This study focuses on transferring
non-differentiable surrogate models (e.g., random forests) from a source
function to a target function, where we assume their domains are related by an
unknown affine transformation, using only a limited amount of transfer data
points evaluated on the target. Previous research attempts to tackle this
challenge for differentiable models, e.g., Gaussian process regression, which
minimizes the empirical loss on the transfer data by tuning the affine
transformations. In this paper, we extend the previous work to the random
forest and assess its effectiveness on a widely-used artificial problem set -
Black-Box Optimization Benchmark (BBOB) testbed, and on four real-world
transfer learning problems. The results highlight the significant practical
advantages of the proposed method, particularly in reducing both the data
requirements and computational costs of training surrogate models for complex
real-world scenarios.

</details>


### [231] [DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](https://arxiv.org/abs/2504.14204)
*Wenxin Zhang, Xiaojian Lin, Wenjun Yu, Guangzhen Yao, jingxiang Zhong, Yu Li, Renda Han, Songcheng Xu, Hao Shi, Cuicui Luo*

Main category: cs.LG

TL;DR: DConAD is a differencing-based contrastive representation learning framework for unsupervised time series anomaly detection, leveraging transformer architecture and KL divergence-based contrastive learning to improve robustness and avoid dependency on prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Unsupervised methods for time series anomaly detection often struggle with capturing robust dependencies due to abnormal pattern multiplicity, anomaly sparsity, and data complexity.

Method: DConAD uses differential data for additional insights, transformer-based spatiotemporal dependency capture, and a KL divergence-based contrastive learning paradigm with positive samples and stop-gradient strategy.

Result: Experiments on five datasets demonstrate DConAD's superiority over nine baselines.

Conclusion: DConAD effectively enhances anomaly detection by improving representation learning robustness and avoiding prior knowledge dependency.

Abstract: Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.

</details>


### [232] [Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments](https://arxiv.org/abs/2504.19139)
*Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji*

Main category: cs.LG

TL;DR: The paper introduces PDTS, a method for robust active task sampling in sequential decision-making, improving adaptation robustness and learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency and robustness issues in task-averse strategies like domain randomization and meta reinforcement learning.

Method: Proposes PDTS, a method combining posterior and diversity synergized task sampling, framed as a Markov decision process.

Result: PDTS enhances zero-shot and few-shot adaptation robustness and accelerates learning in challenging tasks.

Conclusion: PDTS is effective for robust sequential decision-making, with practical benefits demonstrated in experiments.

Abstract: Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.

</details>


### [233] [Fast and Low-Cost Genomic Foundation Models via Outlier Removal](https://arxiv.org/abs/2505.00598)
*Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu*

Main category: cs.LG

TL;DR: GERM is a genomic foundation model addressing computational resource scarcity by improving compression, adaptability, and robustness, outperforming baseline models in fine-tuning and quantization.


<details>
  <summary>Details</summary>
Motivation: The challenge of scarce computational resources in genomic modeling motivates the development of GERM, which enhances efficiency and robustness.

Method: GERM replaces vanilla attention with an outlier-free mechanism and introduces GERM-T for continual learning, improving pre-training and fine-tuning.

Result: GERM improves fine-tuning by 37.98%, quantization by 64.34%, reduces kurtosis by 92.14%, and infinity norm by 82.77%.

Conclusion: GERM offers a practical, efficient solution for genomic modeling in resource-constrained settings, consistently outperforming leading methods.

Abstract: To address the challenge of scarce computational resources in genomic
modeling, we introduce GERM, a genomic foundation model with strong compression
performance and fast adaptability. GERM improves upon models like DNABERT-2 by
eliminating outliers that hinder low-rank adaptation and post-training
quantization, enhancing both efficiency and robustness. We replace the vanilla
attention layer with an outlier-free mechanism inspired by associative memory
models. By removing outliers during both pre-training and fine-tuning, this
approach accelerates adaptation, reduces computational costs, and enhances
quantization robustness within acceptable loss margins. Additionally, we
propose GERM-T, a strategy that employs small-step continual learning within
the outlier-free framework, leveraging original checkpoints to avoid retraining
from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and
quantization by 64.34% over the baseline model. It also reduces average
kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading
methods, GERM consistently delivers superior performance, offering a practical
solution for genomic modeling in resource-constrained settings. Code is
available at https://github.com/MAGICS-LAB/GERM.

</details>


### [234] [Minimum mean-squared error estimation with bandit feedback](https://arxiv.org/abs/2203.16810)
*Ayon Ghosh, L. A. Prashanth, Dipayan Sen, Aditya Gopalan*

Main category: cs.LG

TL;DR: The paper proposes two MSE estimators for learning a Gaussian K-vector with unknown covariance, comparing their performance and introducing an adaptive algorithm for optimal subset selection.


<details>
  <summary>Details</summary>
Motivation: The problem involves estimating a Gaussian K-vector with limited observations (m < K) per round, aiming to minimize MSE. The challenge is to adaptively select subsets for better estimation.

Method: Two estimators are proposed: a non-adaptive one tied to a fixed subset and an adaptive one using regression. A successive elimination algorithm is introduced for MSE-optimal subset selection.

Result: The adaptive estimator outperforms the non-adaptive one with better concentration bounds. The algorithm aims to find the optimal subset with high confidence.

Conclusion: The work provides theoretical insights into MSE estimation under bandit feedback, including a minimax lower bound on sample complexity.

Abstract: We consider the problem of sequentially learning to estimate, in the mean
squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by
observing only $m < K$ of its entries in each round. We propose two MSE
estimators, and analyze their concentration properties. The first estimator is
non-adaptive, as it is tied to a predetermined $m$-subset and lacks the
flexibility to transition to alternative subsets. The second estimator, which
is derived using a regression framework, is adaptive and exhibits better
concentration bounds in comparison to the first estimator. We frame the MSE
estimation problem with bandit feedback, where the objective is to find the
MSE-optimal subset with high confidence. We propose a variant of the successive
elimination algorithm to solve this problem. We also derive a minimax lower
bound to understand the fundamental limit on the sample complexity of this
problem.

</details>


### [235] [Deterministic Nonsmooth Nonconvex Optimization](https://arxiv.org/abs/2302.08300)
*Michael I. Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir, Manolis Zampetakis*

Main category: cs.LG

TL;DR: The paper resolves the open problem of whether deterministic algorithms can achieve dimension-free rates for optimizing nonsmooth nonconvex Lipschitz functions, proving randomization is necessary. It also explores deterministic smoothing and proposes a solution for ReLU networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the complexity of optimizing nonsmooth nonconvex Lipschitz functions and determine if deterministic algorithms can match the efficiency of randomized ones.

Method: The study involves proving lower bounds for deterministic algorithms, analyzing the role of function values, and proposing a deterministic smoothing method for ReLU networks in a white-box setting.

Result: Results show randomization is necessary for dimension-free rates, deterministic algorithms require function values to halt, and a deterministic smoothing method works for ReLU networks.

Conclusion: The paper concludes that deterministic dimension-free optimization is impossible in general but achievable for structured cases like ReLU networks with specific methods.

Abstract: We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions
by producing $(\delta,\epsilon)$-stationary points. Several recent works have
presented randomized algorithms that produce such points using $\tilde
O(\delta^{-1}\epsilon^{-3})$ first-order oracle calls, independent of the
dimension $d$. It has been an open problem as to whether a similar result can
be obtained via a deterministic algorithm. We resolve this open problem,
showing that randomization is necessary to obtain a dimension-free rate. In
particular, we prove a lower bound of $\Omega(d)$ for any deterministic
algorithm. Moreover, we show that unlike smooth or convex optimization, access
to function values is required for any deterministic algorithm to halt within
any finite time.
  On the other hand, we prove that if the function is even slightly smooth,
then the dimension-free rate of $\tilde O(\delta^{-1}\epsilon^{-3})$ can be
obtained by a deterministic algorithm with merely a logarithmic dependence on
the smoothness parameter. Motivated by these findings, we turn to study the
complexity of deterministically smoothing Lipschitz functions. Though there are
efficient black-box randomized smoothings, we start by showing that no such
deterministic procedure can smooth functions in a meaningful manner, resolving
an open question. We then bypass this impossibility result for the structured
case of ReLU neural networks. To that end, in a practical white-box setting in
which the optimizer is granted access to the network's architecture, we propose
a simple, dimension-free, deterministic smoothing that provably preserves
$(\delta,\epsilon)$-stationary points. Our method applies to a variety of
architectures of arbitrary depth, including ResNets and ConvNets. Combined with
our algorithm, this yields the first deterministic dimension-free algorithm for
optimizing ReLU networks, circumventing our lower bound.

</details>


### [236] [An Adaptive Method for Weak Supervision with Drifting Data](https://arxiv.org/abs/2306.01658)
*Alessio Mazzetto, Reza Esfandiarpoor, Akash Singirikonda, Eli Upfal, Stephen H. Bach*

Main category: cs.LG

TL;DR: An adaptive method for weak supervision in non-stationary settings, dynamically adjusting window size to handle drifting source accuracies without prior assumptions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of weak supervision in non-stationary environments where source accuracies drift over time, avoiding reliance on fixed assumptions about drift magnitude.

Method: Dynamically varies window size to estimate current weak supervision accuracies, balancing estimation variance and drift error for near-optimal performance.

Result: Demonstrated adaptability to drift in experiments with synthetic and real-world labelers.

Conclusion: The algorithm effectively handles non-stationary weak supervision without prior drift assumptions, offering practical utility in dynamic settings.

Abstract: We introduce an adaptive method with formal quality guarantees for weak
supervision in a non-stationary setting. Our goal is to infer the unknown
labels of a sequence of data by using weak supervision sources that provide
independent noisy signals of the correct classification for each data point.
This setting includes crowdsourcing and programmatic weak supervision. We focus
on the non-stationary case, where the accuracy of the weak supervision sources
can drift over time, e.g., because of changes in the underlying data
distribution. Due to the drift, older data could provide misleading information
to infer the label of the current data point. Previous work relied on a priori
assumptions on the magnitude of the drift to decide how much data to use from
the past. In contrast, our algorithm does not require any assumptions on the
drift, and it adapts based on the input by dynamically varying its window size.
In particular, at each step, our algorithm estimates the current accuracies of
the weak supervision sources by identifying a window of past observations that
guarantees a near-optimal minimization of the trade-off between the error due
to the variance of the estimation and the error due to the drift. Experiments
on synthetic and real-world labelers show that our approach adapts to the
drift.

</details>


### [237] [Generating synthetic data for neural operators](https://arxiv.org/abs/2401.02398)
*Erisa Hasani, Rachel A. Ward*

Main category: cs.LG

TL;DR: A novel 'backward' data generation method for training neural PDE solvers avoids traditional numerical solvers by sampling solutions and computing corresponding right-hand sides directly.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven PDE solvers rely on numerical methods for training data, which can be limiting. This work aims to bypass this dependency.

Method: Randomly sample candidate solutions from the solution space and compute the corresponding right-hand sides via differentiation, creating training pairs without solving PDEs numerically.

Result: Models trained on this synthetic data generalize well when tested on data from standard solvers.

Conclusion: The method enables fast, large-scale data generation of exact solutions, expanding the potential of neural PDE solvers independent of classical numerical methods.

Abstract: Recent advances in the literature show promising potential of deep learning
methods, particularly neural operators, in obtaining numerical solutions to
partial differential equations (PDEs) beyond the reach of current numerical
solvers. However, existing data-driven approaches often rely on training data
produced by numerical PDE solvers (e.g., finite difference or finite element
methods). We introduce a "backward" data generation method that avoids solving
the PDE numerically: by randomly sampling candidate solutions $u_j$ from the
appropriate solution space (e.g., $H_0^1(\Omega)$), we compute the
corresponding right-hand side $f_j$ directly from the equation by
differentiation. This produces training pairs ${(f_j, u_j)}$ by computing
derivatives rather than solving a PDE numerically for each data point, enabling
fast, large-scale data generation consisting of exact solutions. Experiments
indicate that models trained on this synthetic data generalize well when tested
on data produced by standard solvers. While the idea is simple, we hope this
method will expand the potential of neural PDE solvers that do not rely on
classical numerical solvers to generate their data.

</details>


### [238] [Asynchronous Stochastic Approximation and Average-Reward Reinforcement Learning](https://arxiv.org/abs/2409.03915)
*Huizhen Yu, Yi Wan, Richard S. Sutton*

Main category: cs.LG

TL;DR: The paper extends stability proofs for asynchronous stochastic approximation (SA) algorithms, applies them to reinforcement learning in SMDPs, and introduces new conditions for RVI Q-learning convergence.


<details>
  <summary>Details</summary>
Motivation: To broaden convergence guarantees for asynchronous SA and apply these results to reinforcement learning in SMDPs, enhancing the theoretical framework for RVI Q-learning.

Method: Extends Borkar and Meyn's stability proof for SA, examines shadowing properties, and introduces monotonicity conditions for RVI Q-learning.

Result: Convergence of asynchronous SA and RVI Q-learning is established for weakly communicating SMDPs.

Conclusion: The work expands the algorithmic framework for SA and reinforcement learning, providing stronger convergence guarantees and new theoretical insights.

Abstract: This paper studies asynchronous stochastic approximation (SA) algorithms and
their theoretical application to reinforcement learning in semi-Markov decision
processes (SMDPs) with an average-reward criterion. We first extend Borkar and
Meyn's stability proof method to accommodate more general noise conditions,
yielding broader convergence guarantees for asynchronous SA. To sharpen the
convergence analysis, we further examine shadowing properties in the
asynchronous setting, building on a dynamical-systems approach of Hirsch and
Bena\"{i}m. Leveraging these SA results, we establish the convergence of an
asynchronous SA analogue of Schweitzer's classical relative value iteration
algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs.
Moreover, to make full use of these SA results in this application, we
introduce new monotonicity conditions for estimating the optimal reward rate in
RVI Q-learning. These conditions substantially expand the previously considered
algorithmic framework, and we address them with novel arguments in the
stability and convergence analysis of RVI Q-learning.

</details>


### [239] [An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks](https://arxiv.org/abs/2411.06360)
*Mohsen Dehghankar, Mahdi Erfanian, Abolfazl Asudeh*

Main category: cs.LG

TL;DR: The paper proposes algorithms to improve the inference time and memory efficiency of DNNs using binary and ternary weight matrices, achieving significant speedups and memory reductions.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks (DNNs) suffer from inefficiency and high computational costs, limiting accessibility. The paper aims to address this by optimizing inference.

Method: The approach involves preprocessing weight matrices to create indices, reducing storage and enabling efficient algorithms with logarithmic factor improvements in time complexity.

Result: Experiments show up to 29x faster multiplication, 6x memory reduction, and 5.24x speedup in LLM inference.

Conclusion: The proposed algorithms effectively enhance DNN efficiency, making them more accessible and cost-effective.

Abstract: Despite their tremendous success and versatility, Deep Neural Networks (DNNs)
such as Large Language Models (LLMs) suffer from inference inefficiency and
rely on advanced computational infrastructure. To address these challenges and
make these models more accessible and cost-effective, in this paper, we propose
algorithms to improve the inference time and memory efficiency of DNNs with
binary and ternary weight matrices. Particularly focusing on matrix
multiplication as the bottleneck operation of inference, we observe that, once
trained, the weight matrices of a model no longer change. This allows us to
preprocess these matrices and create indices that help reduce the storage
requirements by a logarithmic factor while enabling our efficient inference
algorithms. Specifically, for a $n\times n$ weight matrix, our efficient
algorithm guarantees a time complexity of $O(\frac{n^2}{\log n})$, a
logarithmic factor improvement over the standard vector-matrix multiplication.
Besides theoretical analysis, we conduct extensive experiments to evaluate the
practical efficiency of our algorithms. Our results confirm the superiority of
our approach both with respect to time and memory, as we observed a reduction
in the multiplication time up to 29x and memory usage up to 6x. When applied to
LLMs, our experiments show up to a 5.24x speedup in the inference time.

</details>


### [240] [Locally Private Sampling with Public Data](https://arxiv.org/abs/2411.08791)
*Behnoosh Zamanlooy, Mario Diaz, Shahab Asoodeh*

Main category: cs.LG

TL;DR: A locally private sampling framework is proposed to leverage both private and public datasets, addressing the limitation of single-data-record assumptions in LDP methods.


<details>
  <summary>Details</summary>
Motivation: Existing LDP methods assume users have only one data record, which is unrealistic as users often have extensive private and public datasets.

Method: A minimax optimization problem is framed using $f$-divergence to design a mechanism that generates private samples approximating the private dataset while preserving the public dataset.

Result: The optimal mechanism is characterized for general $f$-divergences and shown to be universal across all $f$-divergences for discrete distributions.

Conclusion: Experiments confirm the proposed sampler outperforms state-of-the-art locally private samplers.

Abstract: Local differential privacy (LDP) is increasingly employed in
privacy-preserving machine learning to protect user data before sharing it with
an untrusted aggregator. Most LDP methods assume that users possess only a
single data record, which is a significant limitation since users often gather
extensive datasets (e.g., images, text, time-series data) and frequently have
access to public datasets. To address this limitation, we propose a locally
private sampling framework that leverages both the private and public datasets
of each user. Specifically, we assume each user has two distributions: $p$ and
$q$ that represent their private dataset and the public dataset, respectively.
The objective is to design a mechanism that generates a private sample
approximating $p$ while simultaneously preserving $q$. We frame this objective
as a minimax optimization problem using $f$-divergence as the utility measure.
We fully characterize the minimax optimal mechanisms for general
$f$-divergences provided that $p$ and $q$ are discrete distributions.
Remarkably, we demonstrate that this optimal mechanism is universal across all
$f$-divergences. Experiments validate the effectiveness of our minimax optimal
sampler compared to the state-of-the-art locally private sampler.

</details>


### [241] [chebgreen: Learning and Interpolating Continuous Empirical Green's Functions from Data](https://arxiv.org/abs/2501.18715)
*Harshwardhan Praveen, Jacob Brown, Christopher Earls*

Main category: cs.LG

TL;DR: A data-driven library, chebgreen, models 1D systems with unknown PDEs using Empirical Green's Functions and Rational Neural Networks.


<details>
  <summary>Details</summary>
Motivation: To address systems with unknown governing PDEs and control parameters by learning their hidden boundary value problems.

Method: Learns an Empirical Green's Function via Rational Neural Networks, constructs a bivariate Chebyshev basis, and interpolates singular functions and values.

Result: Uncovers the Green's function for unseen control parameters by interpolating singular functions and values.

Conclusion: Chebgreen provides a mesh-independent, data-driven approach to model systems with unknown PDEs.

Abstract: In this work, we present a mesh-independent, data-driven library, chebgreen,
to mathematically model one-dimensional systems, possessing an associated
control parameter, and whose governing partial differential equation is
unknown. The proposed method learns an Empirical Green's Function for the
associated, but hidden, boundary value problem, in the form of a Rational
Neural Network from which we subsequently construct a bivariate representation
in a Chebyshev basis. We uncover the Green's function, at an unseen control
parameter value, by interpolating the left and right singular functions within
a suitable library, expressed as points on a manifold of Quasimatrices, while
the associated singular values are interpolated with Lagrange polynomials.

</details>


### [242] [Interaction-Aware Gaussian Weighting for Clustered Federated Learning](https://arxiv.org/abs/2502.03340)
*Alessandro Licciardi, Davide Leo, Eros Fanì, Barbara Caputo, Marco Ciccone*

Main category: cs.LG

TL;DR: FedGWC, a novel clustered FL method, groups clients by data distribution using Gaussian weighting and a new clustering metric, improving accuracy and robustness in heterogeneous FL settings.


<details>
  <summary>Details</summary>
Motivation: Conventional FL struggles with data heterogeneity and class imbalance, degrading model performance. Clustered FL addresses this by grouping clients with similar data distributions.

Method: FedGWC uses Gaussian weighting to transform empirical losses for clustering and introduces the Wasserstein Adjusted Score to evaluate cluster cohesion.

Result: Experiments show FedGWC outperforms existing FL algorithms in cluster quality and classification accuracy.

Conclusion: FedGWC effectively mitigates heterogeneity in FL, enhancing model performance while preserving privacy.

Abstract: Federated Learning (FL) emerged as a decentralized paradigm to train models
while preserving privacy. However, conventional FL struggles with data
heterogeneity and class imbalance, which degrade model performance. Clustered
FL balances personalization and decentralized training by grouping clients with
analogous data distributions, enabling improved accuracy while adhering to
privacy constraints. This approach effectively mitigates the adverse impact of
heterogeneity in FL. In this work, we propose a novel clustered FL method,
FedGWC (Federated Gaussian Weighting Clustering), which groups clients based on
their data distribution, allowing training of a more robust and personalized
model on the identified clusters. FedGWC identifies homogeneous clusters by
transforming individual empirical losses to model client interactions with a
Gaussian reward mechanism. Additionally, we introduce the Wasserstein Adjusted
Score, a new clustering metric for FL to evaluate cluster cohesion with respect
to the individual class distribution. Our experiments on benchmark datasets
show that FedGWC outperforms existing FL algorithms in cluster quality and
classification accuracy, validating the efficacy of our approach.

</details>


### [243] [AL-PINN: Active Learning-Driven Physics-Informed Neural Networks for Efficient Sample Selection in Solving Partial Differential Equations](https://arxiv.org/abs/2502.03963)
*Keon Vin Park*

Main category: cs.LG

TL;DR: AL-PINN integrates uncertainty quantification and active learning to optimize training sample selection, reducing computational costs while maintaining accuracy in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs require many training samples, increasing computational costs. AL-PINN aims to address this inefficiency by dynamically selecting high-uncertainty regions for training.

Method: AL-PINN uses Monte Carlo Dropout for uncertainty estimation and active learning to adaptively select training samples, focusing on high-uncertainty regions.

Result: AL-PINN achieves comparable or better accuracy than traditional PINNs with fewer training samples, validated on benchmark PDEs and climate data.

Conclusion: AL-PINN is efficient for applications with expensive data collection, like climate modeling, and shows promise for accelerating PINN-based PDE solvers.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising approach
for solving Partial Differential Equations (PDEs) by incorporating physical
constraints into deep learning models. However, standard PINNs often require a
large number of training samples to achieve high accuracy, leading to increased
computational costs. To address this issue, we propose Active Learning-Driven
PINNs (AL-PINN), which integrates Uncertainty Quantification (UQ) and Active
Learning (AL) strategies to optimize sample selection dynamically.
  AL-PINN utilizes Monte Carlo Dropout to estimate epistemic uncertainty in the
model predictions, enabling the adaptive selection of high-uncertainty regions
for additional training. This approach significantly enhances learning
efficiency by focusing computational resources on the most informative data
points. We evaluate AL-PINN on benchmark PDE problems with known analytical
solutions and real-world WeatherBench climate data. Our results demonstrate
that AL-PINN achieves comparable or superior accuracy compared to traditional
PINNs while reducing the number of required training samples.
  The proposed framework is particularly beneficial for scientific and
engineering applications where data collection is expensive or limited, such as
climate modeling, medical simulations, and material science. Our findings
highlight the potential of active learning in accelerating PINN-based PDE
solvers while maintaining high accuracy and computational efficiency.

</details>


### [244] [Towards a Foundation Model for Physics-Informed Neural Networks: Multi-PDE Learning with Active Sampling](https://arxiv.org/abs/2502.07425)
*Keon Vin Park*

Main category: cs.LG

TL;DR: A foundation PINN model is proposed to solve multiple PDEs within a unified architecture, enhanced by active learning for sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs are limited to single PDEs, lacking generalizability across diverse physical systems.

Method: A single PINN framework is trained on four PDEs, incorporating active learning with MC Dropout for uncertainty-based sample selection.

Result: Targeted uncertainty sampling improves performance with fewer samples, enabling efficient learning across multiple PDEs.

Conclusion: Multi-PDE PINNs with active learning offer a generalizable, cost-effective solution for physics-based deep learning.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical laws
into neural network training. However, traditional PINN models are typically
designed for single PDEs, limiting their generalizability across different
physical systems. In this work, we explore the potential of a foundation PINN
model capable of solving multiple PDEs within a unified architecture. We
investigate the efficacy of a single PINN framework trained on four distinct
PDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave
Equation, and the 2D Laplace Equation, demonstrating its ability to learn
diverse physical dynamics.
  To enhance sample efficiency, we incorporate Active Learning (AL) using Monte
Carlo (MC) Dropout-based uncertainty estimation, selecting the most informative
training samples iteratively. We evaluate different active learning strategies,
comparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset,
and analyze their impact on solution accuracy. Our results indicate that
targeted uncertainty sampling significantly improves performance with fewer
training samples, leading to efficient learning across multiple PDEs.
  This work highlights the feasibility of a generalizable PINN-based foundation
model, capable of adapting to different physics-based problems without
redesigning network architectures. Our findings suggest that multi-PDE PINNs
with active learning can serve as an effective approach for reducing
computational costs while maintaining high accuracy in physics-based deep
learning applications.

</details>


### [245] [Adversarial Combinatorial Semi-bandits with Graph Feedback](https://arxiv.org/abs/2502.18826)
*Yuxiao Wen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In combinatorial semi-bandits, a learner repeatedly selects from a
combinatorial decision set of arms, receives the realized sum of rewards, and
observes the rewards of the individual selected arms as feedback. In this
paper, we extend this framework to include \emph{graph feedback}, where the
learner observes the rewards of all neighboring arms of the selected arms in a
feedback graph $G$. We establish that the optimal regret over a time horizon
$T$ scales as $\widetilde{\Theta}(S\sqrt{T}+\sqrt{\alpha ST})$, where $S$ is
the size of the combinatorial decisions and $\alpha$ is the independence number
of $G$. This result interpolates between the known regrets
$\widetilde\Theta(S\sqrt{T})$ under full information (i.e., $G$ is complete)
and $\widetilde\Theta(\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$
has only self-loops), where $K$ is the total number of arms. A key technical
ingredient is to realize a convexified action using a random decision vector
with negative correlations. We also show that online stochastic mirror descent
(OSMD) that only realizes convexified actions in expectation is suboptimal.

</details>


### [246] [Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks](https://arxiv.org/abs/2504.12561)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Kernel Ridge Regression (KRR) is proposed as an efficient, non-iterative alternative to Kernel Logistic Regression (KLR) for high-capacity Hopfield networks, achieving comparable performance with faster training.


<details>
  <summary>Details</summary>
Motivation: Hopfield networks with Hebbian learning have limited storage capacity, and while supervised methods like KLR improve this, they are computationally expensive.

Method: KRR uses the kernel trick and regression for bipolar state prediction, offering a closed-form solution for learning dual variables.

Result: KRR achieves state-of-the-art storage capacity (β=1.5) and noise robustness, with drastically reduced training time compared to LLR and KLR.

Conclusion: KRR is a highly efficient method for high-performance associative memories, matching KLR's performance with significant speed advantages.

Abstract: Hopfield networks using Hebbian learning suffer from limited storage
capacity. While supervised methods like Linear Logistic Regression (LLR) offer
some improvement, kernel methods like Kernel Logistic Regression (KLR)
significantly enhance capacity and noise robustness. However, KLR requires
computationally expensive iterative learning. We propose Kernel Ridge
Regression (KRR) as an efficient kernel-based alternative for learning
high-capacity Hopfield networks. KRR utilizes the kernel trick and predicts
bipolar states via regression, crucially offering a non-iterative, closed-form
solution for learning dual variables. We evaluate KRR and compare its
performance against Hebbian, LLR, and KLR. Our results demonstrate that KRR
achieves state-of-the-art storage capacity (reaching $\beta$=1.5) and noise
robustness, comparable to KLR. Crucially, KRR drastically reduces training
time, being orders of magnitude faster than LLR and significantly faster than
KLR, especially at higher storage loads. This establishes KRR as a potent and
highly efficient method for building high-performance associative memories,
providing comparable performance to KLR with substantial training speed
advantages. This work provides the first empirical comparison between KRR and
KLR in the context of Hopfield network learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [247] [The Coral Protocol: Open Infrastructure Connecting The Internet of Agents](https://arxiv.org/abs/2505.00749)
*Roman J. Georgio, Caelum Forder, Suman Deb, Peter Carroll, Önder Gürcan*

Main category: cs.MA

TL;DR: The Coral Protocol is a decentralized infrastructure for AI agent collaboration, enabling communication, coordination, trust, and payments in multi-agent ecosystems.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for interoperability among specialized AI agents working across domains and vendors.

Method: Introduces standardized messaging, modular coordination, and secure team formation for agent collaboration.

Result: Enables efficient, trustworthy interactions and complex workflows among diverse AI agents.

Conclusion: Positions Coral as a foundational platform for the 'Internet of Agents,' fostering automation and collective intelligence.

Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure
that enables communication, coordination, trust and payments for The Internet
of Agents. It addresses the growing need for interoperability in a world where
organizations are deploying multiple specialized AI agents that must work
together across domains and vendors. As a foundational platform for multi-agent
AI ecosystems, Coral establishes a common language and coordination framework
allowing any agent to participate in complex workflows with others. Its design
emphasizes broad compatibility, security, and vendor neutrality, ensuring that
agent interactions are efficient and trustworthy. In particular, Coral
introduces standardized messaging formats for agent communication, a modular
coordination mechanism for orchestrating multi-agent tasks, and secure team
formation capabilities for dynamically assembling trusted groups of agents.
Together, these innovations position Coral Protocol as a cornerstone of the
emerging "Internet of Agents," unlocking new levels of automation, collective
intelligence, and business value through open agent collaboration.

</details>


### [248] [Virtual Force-Based Routing of Modular Agents on a Graph](https://arxiv.org/abs/2505.00928)
*Adam Casselman, Manav Vora, Melkior Ornik*

Main category: cs.MA

TL;DR: A heuristic algorithm for routing modular vehicles on graphs to minimize resource use, outperforming non-modular methods 81% of the time and being faster than existing modular routing algorithms.


<details>
  <summary>Details</summary>
Motivation: Modular vehicles offer efficiency and flexibility in urban/aerial transportation by dynamically connecting/disconnecting. The goal is to minimize resource expenditure while visiting all target nodes.

Method: A heuristic algorithm models agents and targets as point charges, routing agents along paths of highest attractive force from targets and neighboring agents.

Result: Simulations on real-world routes showed equal performance to existing methods for two agents and better performance (81%) for three agents vs. non-modular methods. The algorithm is also faster.

Conclusion: The proposed heuristic effectively balances path optimality and cost benefits of modularity, demonstrating superior performance and speed in real-world scenarios.

Abstract: Modular vehicles have become an area of academic interest in the field of
multi-agent systems. Modularity allows vehicles to connect and disconnect with
each other mid-transit which provides a balance between efficiency and
flexibility when solving complex and large scale tasks in urban or aerial
transportation. This paper details a generalized scheme to route multiple
modular agents on a graph to a predetermined set of target nodes. The objective
is to visit all target nodes while incurring minimum resource expenditure.
Agents that are joined together will incur the equivalent cost of a single
agent, which is motivated by the logistical benefits of traffic reduction and
increased fuel efficiency. To solve this problem, we introduce a heuristic
algorithm that seeks to balance the optimality of the path that an agent takes
and the cost benefit of joining agents. Our approach models the agents and
targets as point charges, where the agents take the path of highest attractive
force from its target node and neighboring agents. We validate our approach by
simulating multiple modular agents along real-world transportation routes in
the road network of Champaign-Urbana, Illinois, USA. For two vehicles, it
performed equally compared to an existing modular-agent routing algorithm.
Three agents were then routed using our method and the performance was
benchmarked against non-modular agents using a simple shortest path policy
where it performs better than the non-modular implementation 81 percent of the
time. Moreover, we show that the proposed algorithm operates faster than
existing routing methods for modular agents.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [249] [Photoshop Batch Rendering Using Actions for Stylistic Video Editing](https://arxiv.org/abs/2505.01001)
*Tessa De La Fuente*

Main category: cs.MM

TL;DR: An efficient workflow for creative image/video editing using Photoshop Actions and Batch Processing, automating edits for consistency and productivity.


<details>
  <summary>Details</summary>
Motivation: To streamline and enhance creative workflows by integrating Photoshop's image manipulation with video editing techniques.

Method: Uses Adobe Photoshop Actions tool and Batch Processing System to automate and apply consistent visual edits across multiple images.

Result: Achieves uniform results and optimizes productivity in post-processing pipelines.

Conclusion: This approach offers a practical alternative for managing creative workflows efficiently.

Abstract: My project looks at an efficient workflow for creative image/video editing
using Adobe Photoshop Actions tool and Batch Processing System. This innovative
approach to video editing through Photoshop creates a fundamental shift to
creative workflow management through the integration of industry-leading image
manipulation with video editing techniques. Through systematic automation of
Actions, users can achieve a simple and consistent application of visual edits
across a string of images. This approach provides an alternative method to
optimize productivity while ensuring uniform results across image collections
through a post-processing pipeline.

</details>


### [250] [CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment](https://arxiv.org/abs/2505.01237)
*Edson Araujo, Andrew Rouditchenko, Yuan Gong, Saurabhchand Bhati, Samuel Thomas, Brian Kingsbury, Leonid Karlinsky, Rogerio Feris, James R. Glass*

Main category: cs.MM

TL;DR: CAV-MAE Sync improves audio-visual learning by aligning audio temporally with video, separating contrastive and reconstruction objectives, and using register tokens for better spatial localization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained temporal alignment and conflicting optimization goals in audio-visual learning.

Method: Proposes CAV-MAE Sync, treating audio as a temporal sequence, separating objectives with global tokens, and introducing register tokens.

Result: Achieves state-of-the-art performance on AudioSet, VGG Sound, and ADE20K Sound for retrieval, classification, and localization.

Conclusion: CAV-MAE Sync is a simple yet effective extension for self-supervised audio-visual learning, outperforming complex architectures.

Abstract: Recent advances in audio-visual learning have shown promising results in
learning representations across modalities. However, most approaches rely on
global audio representations that fail to capture fine-grained temporal
correspondences with visual frames. Additionally, existing methods often
struggle with conflicting optimization objectives when trying to jointly learn
reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync
as a simple yet effective extension of the original CAV-MAE framework for
self-supervised audio-visual learning. We address three key challenges: First,
we tackle the granularity mismatch between modalities by treating audio as a
temporal sequence aligned with video frames, rather than using global
representations. Second, we resolve conflicting optimization goals by
separating contrastive and reconstruction objectives through dedicated global
tokens. Third, we improve spatial localization by introducing learnable
register tokens that reduce semantic load on patch tokens. We evaluate the
proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on
zero-shot retrieval, classification and localization tasks demonstrating
state-of-the-art performance and outperforming more complex architectures.

</details>


### [251] [FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing](https://arxiv.org/abs/2505.01263)
*Gaoxiang Cong, Liang Li, Jiadong Pan, Zhedong Zhang, Amin Beheshti, Anton van den Hengel, Yuankai Qi, Qingming Huang*

Main category: cs.MM

TL;DR: FlowDubber is an LLM-based dubbing method that improves lip-sync, pronunciation, and acoustic quality using semantic-aware learning, dual contrastive aligning, and voice-enhanced flow matching.


<details>
  <summary>Details</summary>
Motivation: Existing dubbing methods prioritize word error rate over lip-sync and acoustic quality, leading to suboptimal results. FlowDubber addresses this gap.

Method: Uses Qwen2.5 LLM backbone, semantic-aware phoneme learning, dual contrastive aligning (DCA) for lip-sync, and flow-based voice enhancing (FVE) for acoustic quality.

Result: Outperforms state-of-the-art methods on benchmarks, achieving better audio-visual sync and clarity.

Conclusion: FlowDubber sets a new standard for high-quality dubbing by integrating LLMs and advanced alignment techniques.

Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given
movie clip in both temporal and emotional aspects while preserving the vocal
timbre of a given brief reference audio. Existing methods focus primarily on
reducing the word error rate while ignoring the importance of lip-sync and
acoustic quality. To address these issues, we propose a large language model
(LLM) based flow matching architecture for dubbing, named FlowDubber, which
achieves high-quality audio-visual sync and pronunciation by incorporating a
large speech language model and dual contrastive aligning while achieving
better acoustic quality via the proposed voice-enhanced flow matching than
previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the
in-context sequence from movie scripts and reference audio. Then, the proposed
semantic-aware learning focuses on capturing LLM semantic knowledge at the
phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment
with lip movement, reducing ambiguities where similar phonemes might be
confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves
acoustic quality in two aspects, which introduces an LLM-based acoustics flow
matching guidance to strengthen clarity and uses affine style prior to enhance
identity when recovering noise into mel-spectrograms via gradient vector field
prediction. Extensive experiments demonstrate that our method outperforms
several state-of-the-art methods on two primary benchmarks. The demos are
available at
{\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [252] [Physics-Informed Neural Network-Driven Sparse Field Discretization Method for Near-Field Acoustic Holography](https://arxiv.org/abs/2505.00897)
*Xinmeng Luan, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti*

Main category: eess.AS

TL;DR: PINN-SFD is a self-supervised, physics-informed deep learning method for Near-Field Acoustic Holography (NAH) that avoids large datasets and training phases by using sparse field discretization and virtual planes for improved reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for NAH rely on large datasets and supervision, which limits their practicality. PINN-SFD aims to overcome this by being self-supervised and physics-informed.

Method: The method discretizes the wave propagation field into sparse regions, uses the Kirchhoff-Helmholtz integral, and incorporates virtual planes for constraints. Physics-Informed Neural Networks (PINNs) optimize the process with physics-based loss functions and enforce sparsity on source velocities.

Result: PINN-SFD outperforms the conventional C-ESM in reconstruction accuracy, especially for complex vibrational patterns, and shows reduced sensitivity to regularization parameters.

Conclusion: PINN-SFD offers a robust, physics-informed alternative to traditional NAH methods, with superior performance and reduced dependency on large datasets.

Abstract: We propose the Physics-Informed Neural Network-driven Sparse Field
Discretization method (PINN-SFD), a novel self-supervised, physics-informed
deep learning approach for addressing the Near-Field Acoustic Holography (NAH)
problem. Unlike existing deep learning methods for NAH, which are predominantly
supervised by large datasets, our approach does not require a training phase
and it is physics-informed. The wave propagation field is discretized into
sparse regions, a process referred to as field discretization, which includes a
series of set of source planes, to address the inverse problem. Our method
employs the discretized Kirchhoff-Helmholtz integral as the wave propagation
model. By incorporating virtual planes, additional constraints are enforced
near the actual sound source, improving the reconstruction process.
Optimization is carried out using Physics-Informed Neural Networks (PINNs),
where physics-based constraints are integrated into the loss functions to
account for both direct (from equivalent source plane to hologram plane) and
additional (from virtual planes to hologram plane) wave propagation paths.
Additionally, sparsity is enforced on the velocity of the equivalent sources.
Our comprehensive validation across various rectangular and violin top plates,
covering a wide range of vibrational modes, demonstrates that PINN-SFD
consistently outperforms the conventional Compressive-Equivalent Source Method
(C-ESM), particularly in terms of reconstruction accuracy for complex
vibrational patterns. Significantly, this method demonstrates reduced
sensitivity to regularization parameters compared to C-ESM.

</details>


### [253] [How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios](https://arxiv.org/abs/2505.01338)
*Satvik Venkatesh, Philip Coleman, Arthur Benilov, Simon Brown, Selim Sheta, Frederic Roskam*

Main category: eess.AS

TL;DR: The paper explores real-time low-latency single-channel speech enhancement (SE) in distant microphone scenarios (5-10m) for large rooms like conference halls and theatres, addressing challenges like long reverberation times and room volume.


<details>
  <summary>Details</summary>
Motivation: Current SE methods focus on small rooms with short reverb times, but real-world applications like lectures and drama require solutions for larger spaces with longer reverb.

Method: Investigates SE feasibility in large rooms, studies room volume-reverb time relationship, and proposes preserving early reflections for better dereverberation.

Result: Demonstrates SE is feasible in challenging scenarios; highlights the importance of room volume-reverb time correlation; preserving early reflections improves signal quality.

Conclusion: The study advances SE for large rooms, showing practical solutions for distant microphone setups and emphasizing early reflection preservation.

Abstract: Dereverberation is an important sub-task of Speech Enhancement (SE) to
improve the signal's intelligibility and quality. However, it remains
challenging because the reverberation is highly correlated with the signal.
Furthermore, the single-channel SE literature has predominantly focused on
rooms with short reverb times (typically under 1 second), smaller rooms (under
volumes of 1000 cubic meters) and relatively short distances (up to 2 meters).
In this paper, we explore real-time low-latency single-channel SE under distant
microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and
theatres, with larger room dimensions and reverberation times. Such a setup is
useful for applications such as lecture demonstrations, drama, and to enhance
stage acoustics. First, we show that single-channel SE in such challenging
scenarios is feasible. Second, we investigate the relationship between room
volume and reverberation time, and demonstrate its importance when randomly
simulating room impulse responses. Lastly, we show that for dereverberation
with short decay times, preserving early reflections before decaying the
transfer function of the room improves overall signal quality.

</details>


### [254] [WaveNet-Volterra Neural Networks for Active Noise Control: A Fully Causal Approach](https://arxiv.org/abs/2504.04450)
*Lu Bai, Mengtong Li, Siyuan Lian, Kai Chen, Jing Lu*

Main category: eess.AS

TL;DR: The paper proposes a causality-preserving ANC framework combining WaveNet and VNNs, outperforming existing DNN and traditional methods by addressing nonlinearity and ensuring real-time operation.


<details>
  <summary>Details</summary>
Motivation: Existing DNN-based ANC methods often violate causality constraints and are benchmarked against suboptimal traditional filters, leading to incomplete performance claims.

Method: The proposed framework integrates WaveNet with Volterra Neural Networks (VNNs) to handle nonlinearity while maintaining strict causality in real-time ANC.

Result: Simulations show the method surpasses both state-of-the-art DNN architectures and optimized high-order adaptive filters, including Wiener solutions.

Conclusion: The framework demonstrates superior performance, highlighting that prior DNN superiority claims were based on incomplete comparisons with suboptimal baselines.

Abstract: Active Noise Control (ANC) systems are challenged by nonlinear distortions,
which degrade the performance of traditional adaptive filters. While deep
learning-based ANC algorithms have emerged to address nonlinearity, existing
approaches often overlook critical limitations: (1) end-to-end Deep Neural
Network (DNN) models frequently violate causality constraints inherent to
real-time ANC applications; (2) many studies compare DNN-based methods against
simplified or low-order adaptive filters rather than fully optimized high-order
counterparts. In this letter, we propose a causality-preserving time-domain ANC
framework that synergizes WaveNet with Volterra Neural Networks (VNNs),
explicitly addressing system nonlinearity while ensuring strict causal
operation. Unlike prior DNN-based approaches, our method is benchmarked against
both state-of-the-art deep learning architectures and rigorously optimized
high-order adaptive filters, including Wiener solutions. Simulations
demonstrate that the proposed framework achieves superior performance over
existing DNN methods and traditional algorithms, revealing that prior claims of
DNN superiority stem from incomplete comparisons with suboptimal traditional
baselines. Source code is available at
https://github.com/Lu-Baihh/WaveNet-VNNs-for-ANC.git.

</details>


### [255] [Towards Flow-Matching-based TTS without Classifier-Free Guidance](https://arxiv.org/abs/2504.20334)
*Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, Zhikang Niu, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen*

Main category: eess.AS

TL;DR: The paper proposes a method to remove Classifier-Free Guidance (CFG) from flow-matching-based TTS models to improve inference efficiency while maintaining performance, achieving a 9× speed-up.


<details>
  <summary>Details</summary>
Motivation: CFG incurs high computational costs in flow-matching-based TTS models, hindering real-time applicability.

Method: Reformulate flow matching training to approximate CFG optimization trajectory, eliminating unconditional model evaluation and guided tuning.

Result: Achieves 9× inference speed-up on the F5-TTS model with comparable speech quality.

Conclusion: The method efficiently removes CFG, significantly improving inference speed without sacrificing performance, and is compatible with existing sampling strategies.

Abstract: Flow matching has demonstrated strong generative capabilities and has become
a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality
speech synthesis, Classifier-Free Guidance (CFG) is widely used during the
inference of flow-matching-based TTS models. However, CFG incurs substantial
computational cost as it requires two forward passes, which hinders its
applicability in real-time scenarios. In this paper, we explore removing CFG
from flow-matching-based TTS models to improve inference efficiency, while
maintaining performance. Specifically, we reformulated the flow matching
training target to directly approximate the CFG optimization trajectory. This
training method eliminates the need for unconditional model evaluation and
guided tuning during inference, effectively cutting the computational overhead
in half. Furthermore, It can be seamlessly integrated with existing optimized
sampling strategies. We validate our approach using the F5-TTS model on the
LibriTTS dataset. Experimental results show that our method achieves a
9$\times$ inference speed-up compared to the baseline F5-TTS, while preserving
comparable speech quality. We will release the code and models to support
reproducibility and foster further research in this area.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [256] [Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting](https://arxiv.org/abs/2505.00735)
*Jin Hyun Park, Harine Choi, Praewa Pitiphat*

Main category: eess.IV

TL;DR: A novel image inpainting method using RGB and depth images with a dual encoder and attention mechanism improves reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: RGB-only methods lack depth information, which is crucial for spatial and structural context. Incorporating depth enhances accuracy.

Method: Dual encoder architecture processes RGB and depth images separately, fusing features with an attention mechanism. Tested with line and square masks.

Result: Depth-integrated model outperforms baseline, with attention further boosting performance. Grad-CAM visualizations confirm focus on relevant regions.

Conclusion: Combining RGB and depth with attention mechanisms significantly improves inpainting quality, validated by metrics and visualizations.

Abstract: Existing deep learning-based image inpainting methods typically rely on
convolutional networks with RGB images to reconstruct images. However, relying
exclusively on RGB images may neglect important depth information, which plays
a critical role in understanding the spatial and structural context of a scene.
Just as human vision leverages stereo cues to perceive depth, incorporating
depth maps into the inpainting process can enhance the model's ability to
reconstruct images with greater accuracy and contextual awareness. In this
paper, we propose a novel approach that incorporates both RGB and depth images
for enhanced image inpainting. Our models employ a dual encoder architecture,
where one encoder processes the RGB image and the other handles the depth
image. The encoded features from both encoders are then fused in the decoder
using an attention mechanism, effectively integrating the RGB and depth
representations. We use two different masking strategies, line and square, to
test the robustness of the model under different types of occlusions. To
further analyze the effectiveness of our approach, we use Gradient-weighted
Class Activation Mapping (Grad-CAM) visualizations to examine the regions of
interest the model focuses on during inpainting. We show that incorporating
depth information alongside the RGB image significantly improves the
reconstruction quality. Through both qualitative and quantitative comparisons,
we demonstrate that the depth-integrated model outperforms the baseline, with
attention mechanisms further enhancing inpainting performance, as evidenced by
multiple evaluation metrics and visualization.

</details>


### [257] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li, Xinda Qi, Seyed Hamidreza Nabaei, Meiqi Liu, Dong Chen, Xin Zhang, Xunyuan Yin, Zhaojian Li*

Main category: eess.IV

TL;DR: The paper reviews 3D reconstruction techniques for plant phenotyping, comparing classical methods, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting (3DGS), highlighting their strengths, limitations, and future potential.


<details>
  <summary>Details</summary>
Motivation: Advancing precision agriculture and crop improvement by leveraging accurate and automated plant phenotyping through 3D reconstruction technologies.

Method: Comprehensive review of classical reconstruction methods, NeRF, and 3DGS, analyzing their methodologies, applications, and performance.

Result: Classical methods are flexible but face challenges like noise and scalability; NeRF offers photorealistic results but has high computational costs; 3DGS shows promise in efficiency and scalability.

Conclusion: The review provides insights into leveraging these techniques for high-throughput plant phenotyping, contributing to future agricultural technology.

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>


### [258] [RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior](https://arxiv.org/abs/2502.13574)
*Ching-Hua Lee, Chouchang Yang, Jaejin Cho, Yashas Malur Saidutta, Rakshith Sharma Srinivasa, Yilin Shen, Hongxia Jin*

Main category: eess.IV

TL;DR: RestoreGrad improves conditional DDPMs for signal restoration by using a learned prior, achieving faster convergence and better results.


<details>
  <summary>Details</summary>
Motivation: Existing DDPMs discard useful information from degraded signals due to their Gaussian prior, leading to sub-optimal performance.

Method: RestoreGrad integrates DDPMs into a variational autoencoder framework, learning a joint prior to exploit degraded-clean signal correlation.

Result: RestoreGrad shows 5-10x faster convergence, better restoration quality, and 2-2.5x fewer sampling steps than baselines.

Conclusion: Jointly learned priors enhance efficiency and performance in diffusion-based signal restoration.

Abstract: Denoising diffusion probabilistic models (DDPMs) can be utilized for
recovering a clean signal from its degraded observation(s) by conditioning the
model on the degraded signal. The degraded signals are themselves contaminated
versions of the clean signals; due to this correlation, they may encompass
certain useful information about the target clean data distribution. However,
existing adoption of the standard Gaussian as the prior distribution in turn
discards such information, resulting in sub-optimal performance. In this paper,
we propose to improve conditional DDPMs for signal restoration by leveraging a
more informative prior that is jointly learned with the diffusion model. The
proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the
variational autoencoder framework and exploits the correlation between the
degraded and clean signals to encode a better diffusion prior. On speech and
image restoration tasks, we show that RestoreGrad demonstrates faster
convergence (5-10 times fewer training steps) to achieve better quality of
restored signals over existing DDPM baselines, and improved robustness to using
fewer sampling steps in inference time (2-2.5 times fewer), advocating the
advantages of leveraging jointly learned prior for efficiency improvements in
the diffusion process.

</details>


### [259] [XeMap: Contextual Referring in Large-Scale Remote Sensing Environments](https://arxiv.org/abs/2505.00738)
*Yuxi Li, Lu Si, Yujie Hou, Chengaung Liu, Bin Li, Hongjian Fang, Jun Zhang*

Main category: eess.IV

TL;DR: The paper introduces XeMap, a task for contextual, fine-grained localization of text-referred regions in remote sensing imagery, and proposes XeMap-Network with a fusion layer and HMSA module for precise mapping.


<details>
  <summary>Details</summary>
Motivation: Existing methods in remote sensing often miss mid-scale semantic entities, limiting scene interpretation. XeMap addresses this gap.

Method: Proposes XeMap-Network with self- and cross-attention mechanisms and a HMSA module for multimodal alignment.

Result: XeMap-Network outperforms state-of-the-art methods in zero-shot settings, demonstrating superior performance.

Conclusion: XeMap effectively maps referring regions, enhancing interpretation of large-scale remote sensing environments.

Abstract: Advancements in remote sensing (RS) imagery have provided high-resolution
detail and vast coverage, yet existing methods, such as image-level
captioning/retrieval and object-level detection/segmentation, often fail to
capture mid-scale semantic entities essential for interpreting large-scale
scenes. To address this, we propose the conteXtual referring Map (XeMap) task,
which focuses on contextual, fine-grained localization of text-referred regions
in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise
mapping of mid-scale semantic entities that are often overlooked in image-level
or object-level methods. To achieve this, we introduce XeMap-Network, a novel
architecture designed to handle the complexities of pixel-level cross-modal
contextual referring mapping in RS. The network includes a fusion layer that
applies self- and cross-attention mechanisms to enhance the interaction between
text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale
Semantic Alignment (HMSA) module that aligns multiscale visual features with
the text semantic vector, enabling precise multimodal matching across
large-scale RS imagery. To support XeMap task, we provide a novel, annotated
dataset, XeMap-set, specifically tailored for this task, overcoming the lack of
XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting
against state-of-the-art methods, demonstrating superior performance. This
highlights its effectiveness in accurately mapping referring regions and
providing valuable insights for interpreting large-scale RS environments.

</details>


### [260] [Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging](https://arxiv.org/abs/2505.01239)
*Elena Mulero Ayllón, Massimiliano Mantegna, Linlin Shen, Paolo Soda, Valerio Guarrasi, Matteo Tortora*

Main category: eess.IV

TL;DR: Benchmarking deep learning models for lung tumor segmentation shows foundation models like MedSAM~2 outperform traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate lung tumor segmentation is vital for oncology but challenging due to tumor complexity.

Method: Comparative analysis of deep learning models (U-Net, DeepLabV3, nnUNet, MedSAM, MedSAM~2) on lung tumor datasets, evaluating accuracy and efficiency under few-shot learning and fine-tuning.

Result: Foundation models, especially MedSAM~2, surpass traditional models in segmentation accuracy and computational efficiency.

Conclusion: Foundation models like MedSAM~2 hold promise for enhancing clinical workflows and patient outcomes in lung tumor segmentation.

Abstract: Accurate lung tumor segmentation is crucial for improving diagnosis,
treatment planning, and patient outcomes in oncology. However, the complexity
of tumor morphology, size, and location poses significant challenges for
automated segmentation. This study presents a comprehensive benchmarking
analysis of deep learning-based segmentation models, comparing traditional
architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,
and foundation models like MedSAM, and MedSAM~2. Evaluating performance across
two lung tumor segmentation datasets, we assess segmentation accuracy and
computational efficiency under various learning paradigms, including few-shot
learning and fine-tuning. The results reveal that while traditional models
struggle with tumor delineation, foundation models, particularly MedSAM~2,
outperform them in both accuracy and computational efficiency. These findings
underscore the potential of foundation models for lung tumor segmentation,
highlighting their applicability in improving clinical workflows and patient
outcomes.

</details>


### [261] [Contactless pulse rate assessment: Results and insights for application in driving simulator](https://arxiv.org/abs/2505.01299)
*Đorđe D. Nešković, Kristina Stojmenova Pečečnik, Jaka Sodnik, Nadica Miljković*

Main category: eess.IV

TL;DR: The study explores non-contact pulse rate (PR) monitoring using facial video recordings in a driving simulation, comparing it to wearable PPG sensors. Results show improved accuracy with Eulerian Video Magnification (EVM) and highlight age-related PR differences.


<details>
  <summary>Details</summary>
Motivation: To enable unobtrusive, continuous PR monitoring for driver state assessment (e.g., fatigue or stress) without the drawbacks of wearable sensors like motion artifacts and discomfort.

Method: Uses facial video recordings from an RGB camera to detect skin color variations for PR estimation, validated against wearable Empatica E4. Evaluates EVM's impact on signal quality and compares age groups.

Result: EVM improves PR accuracy (MAE: 6.48 to 5.04 bpm; RMSE: 7.84 to 6.38 bpm). Significant PR differences between age groups are found. Empatica E4 bias is noted.

Conclusion: Camera-based PR monitoring is feasible in dynamic environments, with potential for real-time integration into driving simulators.

Abstract: Camera-based monitoring of Pulse Rate (PR) enables continuous and unobtrusive
assessment of driver's state, allowing estimation of fatigue or stress that
could impact traffic safety. Commonly used wearable Photoplethysmography (PPG)
sensors, while effective, suffer from motion artifacts and user discomfort.
This study explores the feasibility of non-contact PR assessment using facial
video recordings captured by a Red, Green, and Blue (RGB) camera in a driving
simulation environment. The proposed approach detects subtle skin color
variations due to blood flow and compares extracted PR values against reference
measurements from a wearable wristband Empatica E4. We evaluate the impact of
Eulerian Video Magnification (EVM) on signal quality and assess statistical
differences in PR between age groups. Data obtained from 80 recordings from 64
healthy subjects covering a PR range of 45-160 bpm are analyzed, and signal
extraction accuracy is quantified using metrics, such as Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). Results show that EVM slightly
improves PR estimation accuracy, reducing MAE from 6.48 bpm to 5.04 bpm and
RMSE from 7.84 bpm to 6.38 bpm. A statistically significant difference is found
between older and younger groups with both video-based and ground truth
evaluation procedures. Additionally, we discuss Empatica E4 bias and its
potential impact on the overall assessment of contact measurements. Altogether
the findings demonstrate the feasibility of camera-based PR monitoring in
dynamic environments and its potential integration into driving simulators for
real-time physiological assessment.

</details>


### [262] [Potential Contrast: Properties, Equivalences, and Generalization to Multiple Classes](https://arxiv.org/abs/2505.01388)
*Wallace Peaslee, Anna Breger, Carola-Bibiane Schönlieb*

Main category: eess.IV

TL;DR: The paper introduces a normalized version of potential contrast, generalizes it to multiple classes and continuous settings, and demonstrates its utility in analyzing a medieval music manuscript.


<details>
  <summary>Details</summary>
Motivation: Potential contrast is useful for image quality and cultural heritage applications but lacks normalization and multi-class generalization.

Method: The authors propose a normalized potential contrast, prove equalities for generalization, and apply it to a manuscript.

Result: The normalized potential contrast removes format dependence and works for multi-class and continuous cases.

Conclusion: The method is validated with a practical application, and implementations are shared for broader use.

Abstract: Potential contrast is typically used as an image quality measure and
quantifies the maximal possible contrast between samples from two classes of
pixels in an image after an arbitrary grayscale transformation. It has been
valuable in cultural heritage applications, identifying and visualizing
relevant information in multispectral images while requiring a small number of
pixels to be manually sampled. In this work, we introduce a normalized version
of potential contrast that removes dependence on image format and also prove
equalities that enable generalization to more than two classes and to
continuous settings. Finally, we exemplify the utility of multi-class
normalized potential contrast through an application to a medieval music
manuscript with visible bleedthrough from the back page. We share our
implementations, based on both original algorithms and our new equalities,
including generalization to multiple classes, at
https://github.com/wallacepeaslee/Multiple-Class-Normalized-Potential-Contrast.

</details>


### [263] [Volumetric medical image segmentation through dual self-distillation in U-shaped networks](https://arxiv.org/abs/2306.03271)
*Soumyanil Banerjee, Nicholas Summerfield, Ming Dong, Carri Glide-Hurst*

Main category: eess.IV

TL;DR: A novel dual self-distillation (DSD) framework improves U-shaped networks for medical image segmentation by distilling knowledge from ground-truth labels and deeper layers, achieving significant performance gains with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To enhance the segmentation performance of U-shaped networks in medical imaging by leveraging dual knowledge distillation.

Method: DSD distills knowledge from ground-truth labels to decoder layers and from deeper layers to shallower layers within a U-shaped network.

Result: Average improvements of 2.82%, 4.53%, and 1.3% in Dice score, and reductions in Hausdorff distance for cardiac, brain tumor, and Hippocampus segmentation.

Conclusion: DSD is a versatile and effective training strategy for boosting U-shaped network performance in medical image segmentation.

Abstract: U-shaped networks and its variants have demonstrated exceptional results for
medical image segmentation. In this paper, we propose a novel dual
self-distillation (DSD) framework in U-shaped networks for volumetric medical
image segmentation. DSD distills knowledge from the ground-truth segmentation
labels to the decoder layers. Additionally, DSD also distills knowledge from
the deepest decoder and encoder layer to the shallower decoder and encoder
layers respectively of a single U-shaped network. DSD is a general training
strategy that could be attached to the backbone architecture of any U-shaped
network to further improve its segmentation performance. We attached DSD on
several state-of-the-art U-shaped backbones, and extensive experiments on
various public 3D medical image segmentation datasets (cardiac substructure,
brain tumor and Hippocampus) demonstrated significant improvement over the same
backbones without DSD. On average, after attaching DSD to the U-shaped
backbones, we observed an increase of 2.82\%, 4.53\% and 1.3\% in Dice
similarity score, a decrease of 7.15 mm, 6.48 mm and 0.76 mm in the Hausdorff
distance, for cardiac substructure, brain tumor and Hippocampus segmentation,
respectively. These improvements were achieved with negligible increase in the
number of trainable parameters and training time. Our proposed DSD framework
also led to significant qualitative improvements for cardiac substructure,
brain tumor and Hippocampus segmentation over the U-shaped backbones. The
source code is publicly available at
https://github.com/soumbane/DualSelfDistillation.

</details>


### [264] [Deciphering scrolls with tomography: A training experiment](https://arxiv.org/abs/2504.11485)
*Sonia Foschiatti, Axel Kittenberger, Otmar Scherzer*

Main category: eess.IV

TL;DR: A non-destructive educational lab simulates ancient document recovery using visible light and software, avoiding harmful X-rays.


<details>
  <summary>Details</summary>
Motivation: Physical unwrapping of damaged ancient documents is impractical; non-destructive methods like CT scans are needed.

Method: Uses visible light and a software pipeline for virtual reconstruction of wrapped scrolls.

Result: Developed an experimental setup and didactic software for student training.

Conclusion: Proposes a safe, educational alternative to X-ray-based document recovery.

Abstract: The recovery of severely damaged ancient written documents has proven to be a
major challenge for many scientists, mainly due to the impracticality of
physical unwrapping them. Non-destructive techniques, such as X-ray computed
tomography (CT), combined with computer vision algorithms, have emerged as a
means of facilitating the virtual reading of the hidden contents of the damaged
documents. This paper proposes an educational laboratory aimed at simulating
the entire process of acquisition and virtual recovery of the ancient works. We
have developed an experimental setup that uses visible light to replace the
detrimental X-rays, and a didactic software pipeline that allows students to
virtually reconstruct a transparent rolled sheet with printed text on it, the
wrapped scroll.

</details>
