{"id": "2507.01019", "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "MALIBU is a benchmark to assess bias in LLM-based multi-agent systems, revealing biases and the need for nuanced fairness strategies.", "motivation": "Addressing implicit biases in LLM-based multi-agent systems to ensure fairness and equitable representation.", "method": "MALIBU evaluates bias through scenario-based assessments with two-phase LLM judging (scoring and comparing responses).", "result": "Quantified biases in LLM outputs, showing bias mitigation may favor marginalized personas over neutrality.", "conclusion": "Nuanced detection, balanced fairness strategies, and transparent benchmarks are needed for multi-agent systems."}}
{"id": "2507.01160", "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "categories": ["cs.CL"], "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "The paper proposes evaluating abstractive summaries by comparing overlapping events in generated summaries, reference summaries, and original news articles, using a Norwegian dataset.", "motivation": "Current evaluation methods rely on overlapping units or similarity scores, but summaries should ideally report events like the original articles.", "method": "Calculate overlapping events between generated summaries, reference summaries, and original articles using a richly annotated Norwegian dataset.", "result": "The approach offers deeper insight into event information in summaries.", "conclusion": "Event-based evaluation provides a more meaningful measure of summary quality."}}
{"id": "2507.01170", "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "The paper analyzes geographic entry shifts in the Nordisk familjebok encyclopedia between its first and second editions, revealing a trend away from Europe towards other regions, influenced by historical events like WWI.", "motivation": "To study intellectual shifts in Sweden by examining changes in geographic entries across editions of the Nordisk familjebok.", "method": "Digitized texts were resegmented, entries matched using semantic embeddings, and geographic entries classified and linked to Wikidata.", "result": "A significant shift in geographic focus from Europe to North America, Africa, Asia, Australia, and northern Scandinavia was observed.", "conclusion": "The findings reflect broader intellectual and historical changes, such as the impact of WWI and emerging global powers."}}
{"id": "2507.01213", "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "categories": ["cs.CL"], "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "The paper proposes xLSTM with Multihead Exponential Gated Fusion (MEGA) for Aspect-based Sentiment Analysis (ABSA), addressing efficiency and performance gaps in existing methods.", "motivation": "Existing ABSA methods struggle with balancing computational efficiency and high performance, lacking global context or demanding excessive resources.", "method": "The MEGA framework integrates bi-directional mLSTM with forward and partially flipped backward streams (PF-mLSTM) and a multihead cross exponential gated fusion mechanism (MECGAF).", "result": "MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency.", "conclusion": "The proposed MEGA framework effectively addresses ABSA challenges by optimizing short-range dependency capture while maintaining global context and computational efficiency."}}
{"id": "2507.01059", "pdf": "https://arxiv.org/pdf/2507.01059", "abs": "https://arxiv.org/abs/2507.01059", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "title": "Automated Vehicles Should be Connected with Natural Language", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "AI": {"tldr": "The paper proposes using natural language for multi-agent collaborative driving to improve communication of intent and reasoning, addressing bandwidth and interoperability issues.", "motivation": "Existing communication methods in multi-agent driving (sensor data, neural features, perception results) are limited in bandwidth, completeness, and interoperability, and lack decision-level fusion.", "method": "Advocates transitioning from perception-oriented data exchanges to natural language communication for intent and reasoning.", "result": "Natural language balances semantic density and bandwidth, adapts to real-time conditions, and bridges heterogeneous platforms.", "conclusion": "Natural language enables proactive coordination, enhancing safety, efficiency, and transparency in intelligent transportation systems."}}
{"id": "2507.01021", "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time.", "AI": {"tldr": "An open-source framework for Command-style dictation bridges the gap between Online and Batch processing, using VAD and parallel Whisper models for efficient multiplexing.", "motivation": "Address the inefficiency gap between resource-heavy Online systems and high-latency Batch processing in dictation systems.", "method": "Uses Voice Activity Detection (VAD) to segment audio and transcribes segments in parallel with Whisper models, supporting multiplexing across audios.", "result": "Deployed in 15% of India's courtrooms; shows latency reduction with increased user concurrency.", "conclusion": "The framework is efficient, scalable, and open-source, with real-world applicability demonstrated."}}
{"id": "2507.01339", "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/", "AI": {"tldr": "GuideSep is a diffusion-based music source separation model that allows instrument-agnostic separation beyond the traditional four-stem setup, using waveform mimicry and mel-spectrogram masks for flexible guidance.", "motivation": "Existing methods lack flexibility for real-world applications due to their focus on fixed four-stem separation.", "method": "GuideSep uses a diffusion-based approach conditioned on waveform mimicry (e.g., humming) and mel-spectrogram masks, avoiding reliance on fixed labels. A mask-prediction baseline is also designed for comparison.", "result": "GuideSep achieves high-quality separation and versatile instrument extraction, outperforming predictive approaches.", "conclusion": "The model demonstrates the potential of user-guided, diffusion-based generative processes for flexible and high-quality music source separation."}}
{"id": "2507.01055", "pdf": "https://arxiv.org/pdf/2507.01055", "abs": "https://arxiv.org/abs/2507.01055", "authors": ["Hao Yang", "Xinlong Liang", "Zhang Li", "Yue Sun", "Zheyu Hu", "Xinghe Xie", "Behdad Dashtbozorg", "Jincheng Huang", "Shiwei Zhu", "Luyi Han", "Jiong Zhang", "Shanshan Wang", "Ritse Mann", "Qifeng Yu", "Tao Tan"], "title": "Prompt Mechanisms in Medical Imaging: A Comprehensive Survey", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning offers transformative potential in medical imaging, yet its\nclinical adoption is frequently hampered by challenges such as data scarcity,\ndistribution shifts, and the need for robust task generalization. Prompt-based\nmethodologies have emerged as a pivotal strategy to guide deep learning models,\nproviding flexible, domain-specific adaptations that significantly enhance\nmodel performance and adaptability without extensive retraining. This\nsystematic review critically examines the burgeoning landscape of prompt\nengineering in medical imaging. We dissect diverse prompt modalities, including\ntextual instructions, visual prompts, and learnable embeddings, and analyze\ntheir integration for core tasks such as image generation, segmentation, and\nclassification. Our synthesis reveals how these mechanisms improve\ntask-specific outcomes by enhancing accuracy, robustness, and data efficiency\nand reducing reliance on manual feature engineering while fostering greater\nmodel interpretability by making the model's guidance explicit. Despite\nsubstantial advancements, we identify persistent challenges, particularly in\nprompt design optimization, data heterogeneity, and ensuring scalability for\nclinical deployment. Finally, this review outlines promising future\ntrajectories, including advanced multimodal prompting and robust clinical\nintegration, underscoring the critical role of prompt-driven AI in accelerating\nthe revolution of diagnostics and personalized treatment planning in medicine.", "AI": {"tldr": "Prompt-based methodologies enhance deep learning in medical imaging by improving adaptability, accuracy, and efficiency, though challenges like data heterogeneity and scalability remain.", "motivation": "Address challenges in clinical adoption of deep learning (e.g., data scarcity, distribution shifts) using prompt-based strategies.", "method": "Systematic review of prompt engineering, analyzing textual, visual, and learnable embeddings for tasks like image generation, segmentation, and classification.", "result": "Prompts improve accuracy, robustness, and interpretability while reducing manual feature engineering. Challenges include prompt design and scalability.", "conclusion": "Prompt-driven AI holds promise for revolutionizing diagnostics and personalized medicine, with future work needed in multimodal prompting and clinical integration."}}
{"id": "2507.01231", "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "categories": ["cs.AI"], "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "The paper refutes claims that LRMs lack reasoning by refining benchmarks (Towers of Hanoi, River Crossing), showing failures stem from complexity or unsolvable setups, not inherent incapacity.", "motivation": "To clarify the debate on LRMs' reasoning capabilities by addressing flaws in prior studies and providing nuanced evidence.", "method": "Replicated and refined benchmarks using incremental stepwise prompting and agentic collaborative dialogue.", "result": "LRMs solve solvable River Crossing problems (100+ agent pairs) but struggle with moderately complex Towers of Hanoi (8 disks).", "conclusion": "LRMs are stochastic searchers in poorly understood state spaces; progress requires fine-grained analysis."}}
{"id": "2507.01026", "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "categories": ["cs.LG"], "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ is a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis by dynamically re-scoring class attributes and using group-level prototypes.", "motivation": "Conventional ZSL methods require extensive synthetic data and computational resources, relaxing ZSL assumptions. FSIGenZ addresses this by leveraging instance-level attribute variability.", "method": "Introduces Model-Specific Attribute Scoring (MSAS) to dynamically re-score attributes and estimates group-level prototypes. Uses Dual-Purpose Semantic Regularization (DPSR) and a semantic-aware contrastive classifier (SCC).", "result": "Achieves competitive performance on SUN, AwA2, and CUB benchmarks using fewer synthetic features.", "conclusion": "FSIGenZ offers an efficient alternative to traditional ZSL methods by reducing synthetic data dependency while maintaining performance."}}
{"id": "2507.01320", "pdf": "https://arxiv.org/pdf/2507.01320", "abs": "https://arxiv.org/abs/2507.01320", "authors": ["Xiangzuo Liu", "Zhikai Liu", "PengPeng Yu", "Ruishan Huang", "Fan Liang"], "title": "Robust Multi-generation Learned Compression of Point Cloud Attribute", "categories": ["cs.MM"], "comment": null, "summary": "Existing learned point cloud attribute compression methods primarily focus on\nsingle-pass rate-distortion optimization, while overlooking the issue of\ncumulative distortion in multi-generation compression scenarios. This paper,\nfor the first time, investigates the multi-generation issue in learned point\ncloud attribute compression. We identify two primary factors contributing to\nquality degradation in multi-generation compression: quantization-induced\nnon-idempotency and transformation irreversibility. To address the former, we\npropose a Mapping Idempotency Constraint, that enables the network to learn the\ncomplete compression-decompression mapping, enhancing its robustness to\nrepeated processes. To address the latter, we introduce a Transformation\nReversibility Constraint, which preserves reversible information flow via a\nquantization-free training path. Further, we propose a Latent Variable\nConsistency Constraint which enhances the multi-generation compression\nrobustness by incorporating a decompression-compression cross-generation path\nand a latent variable consistency loss term. Extensive experiments conducted on\nthe Owlii and 8iVFB datasets verify that the proposed methods can effectively\nsuppress multi-generation loss while maintaining single-pass rate-distortion\nperformance comparable to baseline models.", "AI": {"tldr": "The paper addresses cumulative distortion in multi-generation learned point cloud attribute compression, proposing constraints to enhance robustness and reversibility.", "motivation": "Existing methods overlook cumulative distortion in multi-generation compression, leading to quality degradation.", "method": "Proposes three constraints: Mapping Idempotency, Transformation Reversibility, and Latent Variable Consistency, to mitigate multi-generation loss.", "result": "Experiments on Owlii and 8iVFB datasets show effective suppression of multi-generation loss while maintaining single-pass performance.", "conclusion": "The proposed constraints successfully address multi-generation compression issues without compromising baseline performance."}}
{"id": "2507.01099", "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "A 4D video generation model is proposed to ensure multi-view 3D consistency in videos, improving robotic planning and interaction by predicting future sequences from novel viewpoints without requiring camera poses.", "motivation": "Enhancing robot planning and interaction in complex environments by understanding and predicting physical world dynamics.", "method": "Supervising the model with cross-view pointmap alignment to learn a shared 3D representation, enabling prediction of future video sequences from novel viewpoints using RGB-D observations.", "result": "Produces more visually stable and spatially aligned predictions across simulated and real-world datasets, and supports robot manipulation by recovering trajectories.", "conclusion": "The method advances video generation for robotics by ensuring geometric consistency and enabling robust manipulation and generalization."}}
{"id": "2507.01234", "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "categories": ["cs.CL"], "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "A debiasing algorithm reduces biases in text embeddings caused by confounders like source or language, improving similarity and clustering metrics without degrading out-of-distribution performance.", "motivation": "Text embeddings can be biased by spurious attributes (e.g., source or language), which affects applications pooling texts from different corpora.", "method": "A debiasing algorithm removes information about observed confounders from encoder representations.", "result": "Substantial bias reduction with minimal computational cost; improved similarity and clustering metrics across tasks.", "conclusion": "Debiasing improves embedding quality without harming out-of-distribution performance."}}
{"id": "2507.01378", "pdf": "https://arxiv.org/pdf/2507.01378", "abs": "https://arxiv.org/abs/2507.01378", "authors": ["Ziyao Wang", "Rongpeng Li", "Sizhao Li", "Yuming Xiang", "Haiping Wang", "Zhifeng Zhao", "Honggang Zhang"], "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.", "AI": {"tldr": "RALLY, a Role-Adaptive LLM-Driven Yoked navigation algorithm, enhances UAV swarm control by combining LLM semantic reasoning with MARL online learning, outperforming traditional methods in task coverage and generalization.", "motivation": "Traditional MARL and LLM-based frameworks for UAV swarm control face issues like poor generalization, limited scalability, and ineffective exploration due to rigid role structures and static priors.", "method": "RALLY integrates an LLM-driven semantic decision framework, dynamic role-heterogeneity mechanism, and RMIX-based assignment strategy to blend offline priors with online MARL policies.", "result": "Experiments show RALLY excels in task coverage, convergence speed, and generalization compared to conventional methods.", "conclusion": "RALLY demonstrates strong potential for collaborative navigation in multi-UAV systems by addressing the limitations of existing approaches."}}
{"id": "2507.01022", "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field.", "AI": {"tldr": "The study evaluates eight open-source Music Generation Systems (MGS) in music production workflows, combining technical and practical criteria. Findings highlight MGS as complementary tools, not replacements, with limitations in coherence and emotional depth.", "motivation": "To explore the practical and creative affordances of MGS in music production workflows and address their limitations and integration challenges.", "method": "Mixed qualitative and quantitative approach with a single-evaluator methodology, focusing on architectural diversity across symbolic and audio-based systems.", "result": "MGS enhance human expertise but lack thematic coherence, emphasizing the need for human creativity in complex tasks.", "conclusion": "The study provides a structured evaluation framework and identifies areas for AI integration, guiding future MGS development."}}
{"id": "2507.01563", "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices.", "AI": {"tldr": "A real-time EV siren detection system using E2PANNs, optimized for urban conditions, deployed on Raspberry Pi 5 with low-latency performance.", "motivation": "To address unreliable AudioSet annotations and enable real-time, distributed acoustic monitoring for smart cities.", "method": "Fine-tuned E2PANNs, curated datasets (AudioSet-EV, Augmented, Unified-EV), multithreaded inference with adaptive frame sizing and probability smoothing.", "result": "Low-latency detection with improved robustness under realistic audio conditions.", "conclusion": "Feasibility of deploying IoS-compatible SED solutions for distributed emergency vehicle tracking in smart cities."}}
{"id": "2507.01074", "pdf": "https://arxiv.org/pdf/2507.01074", "abs": "https://arxiv.org/abs/2507.01074", "authors": ["N. P. Garc\u00eda-de-la-Puente", "Roc\u00edo del Amor", "Fernando Garc\u00eda-Torres", "Niels M\u00f8ller Israelsen", "Coraline Lapre", "Christian Rosenberg Petersen", "Ole Bang", "Dominik Brouczek", "Martin Schwentenwein", "Kevin Neumann", "Niels Benson", "Valery Naranjo"], "title": "MID-INFRARED (MIR) OCT-based inspection in industry", "categories": ["eess.IV", "cs.CV"], "comment": "Paper accepted at i-ESA 2024 12th International Conference on\n  Interoperability for Enterprise Systems and Applications 6 pages, 2 figures,\n  2 tables", "summary": "This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography\n(OCT) systems as a tool to penetrate different materials and detect sub-surface\nirregularities. This is useful for monitoring production processes, allowing\nNon-Destructive Inspection Techniques of great value to the industry. In this\nexploratory study, several acquisitions are made on composite and ceramics to\nknow the capabilities of the system. In addition, it is assessed which\npreprocessing and AI-enhanced vision algorithms can be anomaly-detection\nmethodologies capable of detecting abnormal zones in the analyzed objects.\nLimitations and criteria for the selection of optimal parameters will be\ndiscussed, as well as strengths and weaknesses will be highlighted.", "AI": {"tldr": "The paper evaluates MIR OCT systems for detecting sub-surface irregularities in materials like composites and ceramics, using preprocessing and AI-enhanced vision algorithms for anomaly detection.", "motivation": "To provide non-destructive inspection techniques for industrial production monitoring by assessing MIR OCT systems.", "method": "Exploratory study involving acquisitions on composites and ceramics, with preprocessing and AI-enhanced vision algorithms for anomaly detection.", "result": "Identifies capabilities, limitations, and optimal parameters for MIR OCT systems in detecting sub-surface irregularities.", "conclusion": "MIR OCT systems, combined with AI-enhanced algorithms, show promise for industrial non-destructive inspection, though limitations and parameter selection criteria must be considered."}}
{"id": "2507.01282", "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "The paper discusses the limitations of LLMs in clinical settings, especially for dementia diagnosis, and suggests hybrid AI approaches combining statistical learning with expert knowledge for better interpretability and workflow integration.", "motivation": "Despite high benchmark scores, LLMs fail to improve bedside medical diagnosis, prompting a need to identify practical limitations and solutions for clinical AI.", "method": "The paper reviews limitations of data-driven AI in clinical settings, advocating for hybrid approaches like neuro-symbolic AI that integrate LLMs with human expertise.", "result": "Current AI lacks transparency and causal reasoning, but hybrid methods (e.g., PEIRS, ATHENA-CDS) show promise in improving interpretability and workflow fit.", "conclusion": "Future AI should prioritize explanatory coherence, clinician understanding, and patient outcomes, moving beyond accuracy to practical clinical integration."}}
{"id": "2507.01027", "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "categories": ["cs.LG"], "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "DBellQuant is a post-training quantization framework that compresses LLMs to nearly 1-bit weights and 6-bit activations with minimal performance loss, outperforming existing methods.", "motivation": "Address computational and memory challenges in LLMs by improving quantization effectiveness, which is often limited by quantization errors and activation outliers.", "method": "Uses the Learnable Transformation for Dual-Bell (LTDB) algorithm to transform weight distributions into dual-bell forms, reducing binarization errors and smoothing activations.", "result": "Achieves a perplexity of 14.39 on LLaMA2-13B with 6-bit activation quantization, outperforming BiLLM's 21.35.", "conclusion": "DBellQuant sets a new state-of-the-art for LLM compression, demonstrating potential for real-world deployment."}}
{"id": "2507.01582", "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain.", "AI": {"tldr": "The paper introduces the Expressive Compound Word (ECP) representation and the Expressive Music Variational AutoEncoder (XMVAE) to generate classical piano performances, combining composer and pianist roles for superior musical quality.", "motivation": "To emulate the dual creative roles of composers and pianists in generating expressive classical piano performances from scratch.", "method": "Proposes the XMVAE model with two branches: a VQ-VAE for score-related content (Composer) and a vanilla VAE for expressive details (Pianist), trained jointly with Seq2Seq architectures.", "result": "XMVAE outperforms state-of-the-art models in generating classical performances, with pretraining on extra score datasets enhancing performance.", "conclusion": "The XMVAE framework effectively captures both structural and expressive aspects of classical music, achieving high-quality performance generation."}}
{"id": "2507.01123", "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "The paper presents a deep learning and remote sensing-based approach for landslide detection and prediction, using multi-source satellite data and evaluating various models.", "motivation": "Landslides threaten infrastructure, economies, and lives, requiring accurate detection and prediction methods.", "method": "Integrates Sentinel-2 and ALOS PALSAR data with deep learning models (U-Net, DeepLabV3+, Res-Net) for landslide identification.", "result": "The framework enhances early warning systems and disaster management, showing the potential of deep learning and multi-source remote sensing.", "conclusion": "The study highlights the effectiveness of combining deep learning and remote sensing for scalable and transferable landslide prediction models."}}
{"id": "2507.01259", "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "The paper introduces gAIus, an LLM-based agent for legal tasks in non-English/Chinese contexts, focusing on Polish Civil Code. It proposes a retrieval mechanism outperforming embedding-based methods, improving GPT-3.5-turbo by 419% and GPT-4o-mini from 31% to 86%.", "motivation": "Addressing the challenge of LLMs providing accurate, referenced legal answers for non-English/Chinese jurisdictions, particularly Poland.", "method": "Proposes gAIus, a cognitive LLM-based agent with a human-friendly retrieval mechanism, evaluated using Polish law apprenticeship exam questions.", "result": "Significant performance improvements: GPT-3.5-turbo boosted by 419%, GPT-4o-mini accuracy raised from 31% to 86%.", "conclusion": "Demonstrates the potential of specialized LLM architectures for legal tasks, with future research and applications suggested."}}
{"id": "2507.01701", "pdf": "https://arxiv.org/pdf/2507.01701", "abs": "https://arxiv.org/abs/2507.01701", "authors": ["Bochen Han", "Songmao Zhang"], "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.", "AI": {"tldr": "Incorporating blackboard architecture into LLM multi-agent systems improves information sharing, dynamic agent selection, and consensus-building, achieving competitive performance with fewer tokens.", "motivation": "To enhance problem-solving in multi-agent systems by enabling dynamic information sharing and agent selection without predefined workflows.", "method": "Proposed a blackboard architecture for LLM multi-agent systems, allowing shared information and dynamic agent selection based on blackboard content, iterating until consensus.", "result": "Competitive with SOTA static and dynamic MASs, achieving best average performance with fewer tokens.", "conclusion": "The blackboard architecture enables complex, dynamic problem-solving where structured workflows are absent."}}
{"id": "2507.01024", "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics.", "AI": {"tldr": "The paper introduces the Hello Afrika project, which addresses the lack of speech command models for African languages, starting with Kinyarwanda. A custom corpus was used to build and deploy the model on various devices, with performance evaluated using metrics.", "motivation": "There is a shortage of speech command models for African languages, limiting accessibility for persons with disabilities. The project aims to fill this gap, starting with Kinyarwanda.", "method": "A custom speech command corpus (general directives, numbers, wake word) was developed. The model was built and deployed on PC, mobile, and edge devices.", "result": "The model was successfully deployed and its performance assessed using suitable metrics.", "conclusion": "The Hello Afrika project provides a foundational step toward speech command models for African languages, with Kinyarwanda as the first case study."}}
{"id": "2507.01805", "pdf": "https://arxiv.org/pdf/2507.01805", "abs": "https://arxiv.org/abs/2507.01805", "authors": ["Alejandro Sosa Welford", "Leonardo Pepino"], "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish", "categories": ["cs.SD", "eess.AS"], "comment": "5 pages, 2 figures. Accepted at Interspeech 2025", "summary": "This work addresses the development of a database for the automatic\nassessment of text-to-speech (TTS) systems in Spanish, aiming to improve the\naccuracy of naturalness prediction models. The dataset consists of 4,326 audio\nsamples from 52 different TTS systems and human voices and is, up to our\nknowledge, the first of its kind in Spanish. To label the audios, a subjective\ntest was designed based on the ITU-T Rec. P.807 standard and completed by 92\nparticipants. Furthermore, the utility of the collected dataset was validated\nby training automatic naturalness prediction systems. We explored two\napproaches: fine-tuning an existing model originally trained for English, and\ntraining small downstream networks on top of frozen self-supervised speech\nmodels. Our models achieve a mean absolute error of 0.8 on a five-point MOS\nscale. Further analysis demonstrates the quality and diversity of the developed\ndataset, and its potential to advance TTS research in Spanish.", "AI": {"tldr": "A Spanish database for TTS system assessment was created, featuring 4,326 audio samples from 52 TTS systems and human voices. Subjective tests labeled the data, and automatic naturalness prediction models achieved a 0.8 MAE on a five-point MOS scale.", "motivation": "To improve naturalness prediction models for Spanish TTS systems by creating the first dedicated dataset.", "method": "Developed a dataset with subjective labeling (ITU-T Rec. P.807 standard) and trained models via fine-tuning and downstream networks on frozen self-supervised models.", "result": "Models achieved a 0.8 MAE on a five-point MOS scale, validating dataset quality and diversity.", "conclusion": "The dataset advances Spanish TTS research, demonstrating its utility for training accurate prediction models."}}
{"id": "2507.01204", "pdf": "https://arxiv.org/pdf/2507.01204", "abs": "https://arxiv.org/abs/2507.01204", "authors": ["Haotian Wu", "Gongpu Chen", "Pier Luigi Dragotti", "Deniz G\u00fcnd\u00fcz"], "title": "LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression", "categories": ["eess.IV", "cs.IT", "math.IT", "68P30, 94A08", "I.4.2; E.4"], "comment": null, "summary": "We introduce and validate the lottery codec hypothesis, which states that\nuntrained subnetworks within randomly initialized networks can serve as\nsynthesis networks for overfitted image compression, achieving rate-distortion\n(RD) performance comparable to trained networks. This hypothesis leads to a new\nparadigm for image compression by encoding image statistics into the network\nsubstructure. Building on this hypothesis, we propose LotteryCodec, which\noverfits a binary mask to an individual image, leveraging an over-parameterized\nand randomly initialized network shared by the encoder and the decoder. To\naddress over-parameterization challenges and streamline subnetwork search, we\ndevelop a rewind modulation mechanism that improves the RD performance.\nLotteryCodec outperforms VTM and sets a new state-of-the-art in single-image\ncompression. LotteryCodec also enables adaptive decoding complexity through\nadjustable mask ratios, offering flexible compression solutions for diverse\ndevice constraints and application requirements.", "AI": {"tldr": "Untrained subnetworks in random networks can achieve image compression performance like trained networks, leading to LotteryCodec, a new method that outperforms VTM.", "motivation": "To explore if untrained subnetworks can match trained networks in image compression, enabling efficient and flexible solutions.", "method": "Proposes LotteryCodec, which overfits a binary mask to an image using a shared, over-parameterized random network, with a rewind modulation mechanism for efficiency.", "result": "LotteryCodec outperforms VTM, setting a new state-of-the-art in single-image compression and offering adaptive decoding complexity.", "conclusion": "The lottery codec hypothesis is validated, showing untrained subnetworks can rival trained ones, enabling innovative compression solutions."}}
{"id": "2507.01376", "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "categories": ["cs.AI"], "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "The paper reviews AI agent technologies, focusing on LLM-Agents, MLLM-Agents, and Agentic AI, and explores their applications and challenges in smart manufacturing.", "motivation": "To clarify the definitions, capabilities, and practical uses of emerging AI paradigms in smart manufacturing, which remain unclear.", "method": "Systematically reviews AI and AI agent evolution, examines core concepts and advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their manufacturing applications.", "result": "Identifies potential applications and integration of these AI technologies in manufacturing, along with associated challenges.", "conclusion": "The study provides insights into the role of advanced AI agents in smart manufacturing but highlights unresolved challenges."}}
{"id": "2507.01028", "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "The paper analyzes non-contrastive self-supervised learning methods, focusing on stop gradient and exponential moving average techniques to prevent collapse, and proves their stability theoretically.", "motivation": "To understand why stop gradient and exponential moving average procedures avoid representation collapse in self-supervised learning, despite not optimizing the original objective.", "method": "Theoretical analysis from optimization and dynamical systems perspectives, examining the linear case and stability of limit points.", "result": "Stop gradient and exponential moving average avoid collapse and lead to stable equilibria, unlike minimizing the original objective directly.", "conclusion": "These procedures are effective for preventing collapse in self-supervised learning, supported by theoretical guarantees."}}
{"id": "2507.01652", "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.", "AI": {"tldr": "LASADGen introduces a linear attention mechanism (LASAD) for autoregressive image generation, improving efficiency and quality by preserving 2D spatial relationships.", "motivation": "Existing AR models for image generation use transformers with high computational complexity and memory overhead, while linear attention degrades quality due to poor long-range dependency capture.", "method": "Proposes LASAD, a linear attention mechanism with spatial-aware decay, and LASADGen, an autoregressive generator using it for efficient, high-quality image generation.", "result": "LASADGen achieves state-of-the-art performance and efficiency on ImageNet, balancing computational cost and spatial understanding.", "conclusion": "LASADGen bridges the gap between efficiency and quality in autoregressive image generation by leveraging spatial-aware linear attention."}}
{"id": "2507.01163", "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "The paper introduces cp_measure, a Python library for modular, API-first feature extraction from biological images, enhancing reproducibility and integration with machine learning workflows.", "motivation": "Current tools like CellProfiler hinder automated and reproducible analyses for image-based profiling, limiting machine learning applications in computational biology.", "method": "Develop cp_measure, a Python library extracting CellProfiler's core measurement capabilities for programmatic feature extraction.", "result": "cp_measure retains high fidelity with CellProfiler features and integrates seamlessly with the scientific Python ecosystem.", "conclusion": "cp_measure enables scalable, reproducible, and automated image-based profiling pipelines for machine learning in computational biology."}}
{"id": "2507.01278", "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4 shows moderate performance in simulating clinical decisions for diabetic retinopathy and glaucoma screening but lacks precision for complex tasks. Metadata inclusion doesn't significantly impact results.", "motivation": "To explore the utility of large language models (LLMs) like GPT-4 in ophthalmology, specifically for interpreting retinal fundus images and simulating clinical decisions.", "method": "A retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 was given structured prompts with or without metadata to assign severity scores and recommend referrals. Performance was measured using accuracy, F1 scores, and Cohen's kappa.", "result": "GPT-4 performed moderately for diabetic retinopathy (accuracy 67.5%) and binary referral tasks (accuracy 82.3%) but poorly for glaucoma (accuracy ~78%). Metadata had no significant effect.", "conclusion": "GPT-4 can simulate basic ophthalmic decision-making but isn't suitable for clinical use. It may assist in education, documentation, or image annotation workflows."}}
{"id": "2507.01769", "pdf": "https://arxiv.org/pdf/2507.01769", "abs": "https://arxiv.org/abs/2507.01769", "authors": ["Yuta Takahashi", "Shin-ichiro Sakai"], "title": "Distance-based Relative Orbital Transition for Satellite Swarm Array Deployment Under J2 Perturbation", "categories": ["cs.MA"], "comment": null, "summary": "This paper presents an autonomous guidance and control strategy for a\nsatellite swarm that enables scalable distributed space structures for\ninnovative science and business opportunities. The averaged $J_2$ orbital\nparameters that describe the drift and periodic orbital motion were derived\nalong with their target values to achieve a distributed space structure in a\ndecentralized manner. This enabled the design of a distance-based orbital\nstabilizer to ensure autonomous deployment into a monolithic formation of a\ncoplanar equidistant configuration on a user-defined orbital plane. Continuous\nformation control was assumed to be achieved through fuel-free actuation, such\nas satellite magnetic field interaction and differential aerodynamic forces,\nthereby maintaining long-term formation stability without thruster usage. A\nmajor challenge for such actuation systems is the potential loss of control\ncapability due to increasing inter-satellite distances resulting from unstable\norbital dynamics, particularly for autonomous satellite swarms. To mitigate\nthis risk, our decentralized deployment controller minimized drift distance\nduring unexpected communication outages. As a case study, we consider the\ndeployment of palm-sized satellites into a coplanar equidistant formation in a\n$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.", "AI": {"tldr": "An autonomous guidance and control strategy for satellite swarms enables scalable distributed space structures using decentralized methods, fuel-free actuation, and drift mitigation.", "motivation": "To enable innovative science and business opportunities through scalable, distributed space structures using autonomous satellite swarms.", "method": "Derived averaged $J_2$ orbital parameters for decentralized deployment, designed a distance-based orbital stabilizer, and used fuel-free actuation (magnetic field interaction, differential aerodynamic forces).", "result": "Achieved autonomous deployment into coplanar equidistant formations and maintained long-term stability without thrusters.", "conclusion": "The decentralized approach effectively mitigates drift risks and enables stable, scalable satellite swarm formations."}}
{"id": "2507.01172", "pdf": "https://arxiv.org/pdf/2507.01172", "abs": "https://arxiv.org/abs/2507.01172", "authors": ["Marios Glytsos", "Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings", "categories": ["eess.AS"], "comment": "In Proceedings of the 25th International Society for Music\n  Information Retrieval Conference (ISMIR 2024), San Francisco, USA, November\n  2024. The dataset is available at: https://zenodo.org/records/12802440", "summary": "Recent advancements in music source separation (MSS) have focused in the\nmulti-timbral case, with existing architectures tailored for the separation of\ndistinct instruments, overlooking thus the challenge of separating instruments\nwith similar timbral characteristics. Addressing this gap, our work focuses on\nmonotimbral MSS, specifically within the context of classical guitar duets. To\nthis end, we introduce the GuitarDuets dataset, featuring a combined total of\napproximately three hours of real and synthesized classical guitar duet\nrecordings, as well as note-level annotations of the synthesized duets. We\nperform an extensive cross-dataset evaluation by adapting Demucs, a\nstate-of-the-art MSS architecture, to monotimbral source separation.\nFurthermore, we develop a joint permutation-invariant transcription and\nseparation framework, to exploit note event predictions as auxiliary\ninformation. Our results indicate that utilizing both the real and synthesized\nsubsets of GuitarDuets leads to improved separation performance in an\nindependently recorded test set compared to utilizing solely one subset. We\nalso find that while the availability of ground-truth note labels greatly helps\nthe performance of the separation network, the predicted note estimates result\nonly in marginal improvement. Finally, we discuss the behavior of commonly\nutilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.", "AI": {"tldr": "The paper addresses monotimbral music source separation (MSS) in classical guitar duets, introducing the GuitarDuets dataset and adapting Demucs for improved performance.", "motivation": "Existing MSS architectures focus on multi-timbral cases, neglecting instruments with similar timbres, like classical guitar duets.", "method": "The study introduces the GuitarDuets dataset, adapts Demucs for monotimbral MSS, and develops a joint transcription-separation framework.", "result": "Combining real and synthesized data improves separation. Ground-truth note labels aid performance, but predicted notes offer marginal gains.", "conclusion": "The work highlights the challenges of monotimbral MSS and evaluates metrics like SDR and SI-SDR in this context."}}
{"id": "2507.01143", "pdf": "https://arxiv.org/pdf/2507.01143", "abs": "https://arxiv.org/abs/2507.01143", "authors": ["Reza Jalayer", "Masoud Jalayer", "Amirali Baniasadi"], "title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "comment": "35 pages", "summary": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "AI": {"tldr": "A robotics-focused review of sound source localization (SSL) highlighting classical methods and deep learning advancements, addressing gaps in existing surveys.", "motivation": "Existing SSL surveys lack focus on robotics constraints and recent deep learning progress, necessitating a specialized review.", "method": "Reviews classical SSL methods (TDOA, beamforming, SRP, subspace analysis) and modern ML/DL approaches (NNs, CNNs, CRNNs, attention-based architectures), including data and training strategies.", "result": "Categorizes studies by robot types and applications, identifies challenges (robustness, multiplicity, constraints), and outlines future directions for DL-based SSL.", "conclusion": "Proposes a roadmap for robust, adaptable, efficient, and explainable DL-based SSL in next-gen robots."}}
{"id": "2507.01279", "pdf": "https://arxiv.org/pdf/2507.01279", "abs": "https://arxiv.org/abs/2507.01279", "authors": ["Ahmad Chaddad", "Jihao Peng", "Yihang Wu"], "title": "Classification based deep learning models for lung cancer and disease using medical images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted in IEEE Transactions on Radiation and Plasma Medical\n  Sciences", "summary": "The use of deep learning (DL) in medical image analysis has significantly\nimproved the ability to predict lung cancer. In this study, we introduce a\nnovel deep convolutional neural network (CNN) model, named ResNet+, which is\nbased on the established ResNet framework. This model is specifically designed\nto improve the prediction of lung cancer and diseases using the images. To\naddress the challenge of missing feature information that occurs during the\ndownsampling process in CNNs, we integrate the ResNet-D module, a variant\ndesigned to enhance feature extraction capabilities by modifying the\ndownsampling layers, into the traditional ResNet model. Furthermore, a\nconvolutional attention module was incorporated into the bottleneck layers to\nenhance model generalization by allowing the network to focus on relevant\nregions of the input images. We evaluated the proposed model using five public\ndatasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and\nLCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT\n$n$=425024 images). To address class imbalance, we used data augmentation\ntechniques to artificially increase the representation of underrepresented\nclasses in the training dataset. The experimental results show that ResNet+\nmodel demonstrated remarkable accuracy/F1, reaching 98.14/98.14\\% on the\nLC25000 dataset and 99.25/99.13\\% on the IQ-OTH/NCCD dataset. Furthermore, the\nResNet+ model saved computational cost compared to the original ResNet series\nin predicting lung cancer images. The proposed model outperformed the baseline\nmodels on publicly available datasets, achieving better performance metrics.\nOur codes are publicly available at\nhttps://github.com/AIPMLab/Graduation-2024/tree/main/Peng.", "AI": {"tldr": "A novel CNN model, ResNet+, improves lung cancer prediction by integrating ResNet-D and attention modules, achieving high accuracy and computational efficiency.", "motivation": "To enhance lung cancer and disease prediction in medical images by addressing feature loss during downsampling and improving model generalization.", "method": "ResNet+ integrates ResNet-D for better feature extraction and a convolutional attention module to focus on relevant image regions. Evaluated on five datasets with data augmentation for class imbalance.", "result": "Achieved 98.14%/98.14% (accuracy/F1) on LC2500 and 99.25%/99.13% on IQ-OTH/NCCD, with reduced computational cost.", "conclusion": "ResNet+ outperforms baseline models, offering improved performance and efficiency for lung cancer image analysis."}}
{"id": "2507.01410", "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "categories": ["cs.AI"], "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "A formal method for Ethical Decision Making models using fuzzy rules and Petri nets, demonstrated with a medical case study.", "motivation": "Addressing the challenge of evaluating moral machines due to ontological and epistemic complexities in ethics.", "method": "Develops Ethical Decision Making models via ethical risk assessment, specified as fuzzy rules, and verifies them using fuzzy Petri nets.", "result": "Proposes a verifiable and validatable approach for moral machine performance evaluation.", "conclusion": "The method effectively bridges ethical theory and practical machine decision-making, validated through a medical case study."}}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "PathCoT improves pathology visual reasoning in MLLMs by integrating expert knowledge and self-evaluation into the CoT process, addressing hallucinations and answer divergence.", "motivation": "Existing MLLMs struggle with pathology tasks due to lack of domain knowledge and errors in CoT reasoning steps.", "method": "PathCoT integrates pathology expert knowledge into MLLMs' reasoning and adds a self-evaluation step to ensure answer reliability.", "result": "PathCoT outperforms on the PathMMU dataset, enhancing pathology visual understanding and reasoning.", "conclusion": "PathCoT effectively addresses domain-specific challenges in MLLMs, improving accuracy and reliability in pathology tasks."}}
{"id": "2507.01776", "pdf": "https://arxiv.org/pdf/2507.01776", "abs": "https://arxiv.org/abs/2507.01776", "authors": ["Yuxuan Yang"], "title": "Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "The integration of machine learning (ML) into spatial design holds immense\npotential for optimizing space utilization, enhancing functionality, and\nstreamlining design processes. ML can automate tasks, predict performance\noutcomes, and tailor spaces to user preferences. However, the emotional,\ncultural, and aesthetic dimensions of design remain crucial for creating spaces\nthat truly resonate with users-elements that ML alone cannot address. The key\nchallenge lies in harmonizing data-driven efficiency with the nuanced,\nsubjective aspects of design. This paper proposes a human-machine collaboration\nframework to bridge this gap. An effective framework should recognize that\nwhile ML enhances design efficiency through automation and prediction, it must\nbe paired with human creativity to ensure spaces are emotionally engaging and\nculturally relevant. Human designers contribute intuition, empathy, and\ncultural insight, guiding ML-generated solutions to align with users' emotional\nand cultural needs. Additionally, we explore how various ML models can be\nintegrated with human-centered design principles. These models can automate\ndesign generation and optimization, while human designers refine the outputs to\nensure emotional resonance and aesthetic appeal. Through case studies in office\nand residential design, we illustrate how this framework fosters both\ncreativity and cultural relevance. By merging ML with human creativity, spatial\ndesign can achieve a balance of efficiency and emotional impact, resulting in\nenvironments that are both functional and deeply human.", "AI": {"tldr": "A human-machine collaboration framework is proposed to balance ML-driven efficiency with human creativity in spatial design, ensuring emotional and cultural relevance.", "motivation": "The potential of ML in optimizing spatial design is limited by its inability to address emotional, cultural, and aesthetic aspects, necessitating a collaborative approach.", "method": "The paper introduces a framework integrating ML models (for automation and prediction) with human-centered design principles (for emotional and cultural refinement).", "result": "Case studies in office and residential design demonstrate the framework's success in achieving functional efficiency and emotional resonance.", "conclusion": "Combining ML with human creativity yields spatial designs that are both efficient and deeply human, addressing both functional and subjective needs."}}
{"id": "2507.01182", "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "categories": ["cs.CV"], "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "Proposes efficient networks (SDNet and STDNet) for salient object detection (SOD) on resource-constrained devices, achieving real-time performance with superior accuracy.", "motivation": "Address the challenge of deploying SOD on resource-constrained devices due to the computational expense of existing deep neural network models.", "method": "Combines traditional SOD wisdom with CNNs using Pixel Difference Convolutions (PDCs) and introduces Difference Convolution Reparameterization (DCR) for efficiency. Extends to video SOD with SpatioTemporal Difference Convolution (STDC).", "result": "Models achieve 46 FPS (images) and 150 FPS (videos) on Jetson Orin with <1M parameters, outperforming lightweight models in speed and accuracy.", "conclusion": "The proposed networks offer a significant efficiency-accuracy trade-off, enabling real-time SOD on constrained devices."}}
{"id": "2507.01281", "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "CARE-RAG improves RAG systems by addressing knowledge conflicts through conflict-driven summarization of internal and retrieved evidence, enhancing reliability.", "motivation": "Knowledge conflicts in RAG systems undermine reliability; CARE-RAG aims to rethink and synthesize all evidence for trustworthy generation.", "method": "CARE-RAG derives parameter-aware and context-aware evidence, uses a distilled 3B LLaMA3.2 model for conflict-driven summarization, and includes QA Repair for benchmark integrity.", "result": "CARE-RAG outperforms RAG baselines, especially with noisy or conflicting evidence, on revised QA datasets.", "conclusion": "CARE-RAG enhances RAG reliability by effectively managing knowledge conflicts and refining evidence."}}
{"id": "2507.01298", "pdf": "https://arxiv.org/pdf/2507.01298", "abs": "https://arxiv.org/abs/2507.01298", "authors": ["Debasish Pattanayak", "Ajay D. Kshemkalyani", "Manish Kumar", "Anisur Rahaman Molla", "Gokarna Sharma"], "title": "Optimal Dispersion Under Asynchrony", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "comment": "35 pages, 5 figures, 2 tables, and 6 pseudocodes", "summary": "We study the dispersion problem in anonymous port-labeled graphs: $k \\leq n$\nmobile agents, each with a unique ID and initially located arbitrarily on the\nnodes of an $n$-node graph with maximum degree $\\Delta$, must autonomously\nrelocate so that no node hosts more than one agent. Dispersion serves as a\nfundamental task in distributed computing of mobile agents, and its complexity\nstems from key challenges in local coordination under anonymity and limited\nmemory.\n  The goal is to minimize both the time to achieve dispersion and the memory\nrequired per agent. It is known that any algorithm requires $\\Omega(k)$ time in\nthe worst case, and $\\Omega(\\log k)$ bits of memory per agent. A recent result\n[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and\nan $O(k \\log k)$-time algorithm in the asynchronous setting, both using\n$O(\\log(k+\\Delta))$ bits.\n  In this paper, we close the complexity gap in the asynchronous setting by\npresenting the first dispersion algorithm that runs in optimal $O(k)$ time\nusing $O(\\log(k+\\Delta))$ bits of memory per agent. Our solution is based on a\nnovel technique we develop in this paper that constructs a port-one tree in\nanonymous graphs, which may be of independent interest.", "AI": {"tldr": "The paper presents an optimal asynchronous dispersion algorithm for anonymous graphs, achieving O(k) time and O(log(k+\u0394)) memory per agent, using a novel port-one tree technique.", "motivation": "Dispersion is a fundamental task in mobile agent distributed computing, but existing solutions have time or memory inefficiencies in asynchronous settings.", "method": "The authors introduce a novel technique to construct a port-one tree in anonymous graphs, enabling efficient agent coordination.", "result": "The algorithm achieves optimal O(k) time and O(log(k+\u0394)) memory per agent in asynchronous settings.", "conclusion": "The work closes the complexity gap for asynchronous dispersion, with potential broader applications for the port-one tree technique."}}
{"id": "2507.01348", "pdf": "https://arxiv.org/pdf/2507.01348", "abs": "https://arxiv.org/abs/2507.01348", "authors": ["Cheng Zhuangfei", "Zhang Guangyan", "Tu Zehai", "Song Yangyang", "Mao Shuiyang", "Jiao Xiaoqi", "Li Jingyu", "Guo Yiwen", "Wu Jiasong"], "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech", "categories": ["eess.AS", "cs.SD", "I.2.7"], "comment": "10 pages, includes references, 4 figures, 4 tables", "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments.", "AI": {"tldr": "The paper introduces SpeechAccentLLM, a framework for foreign accent conversion (FAC) using LLM-based techniques, featuring SpeechCodeVAE for tokenization and SpeechRestorer for refining outputs.", "motivation": "FAC is challenging, and the study aims to leverage LLM success in TTS for FAC, addressing data scarcity and improving speech quality.", "method": "The framework integrates CTC into codebook discretization (SpeechCodeVAE), uses multitask learning for FAC and TTS, and introduces SpeechRestorer for postprocessing.", "result": "Experiments show optimal trade-offs in token properties, accelerated convergence, superior speech quality, and improved prosodic continuity.", "conclusion": "The proposed framework effectively addresses FAC challenges, demonstrating improved performance and robustness."}}
{"id": "2507.01349", "pdf": "https://arxiv.org/pdf/2507.01349", "abs": "https://arxiv.org/abs/2507.01349", "authors": ["Hitoshi Suda", "Junya Koguchi", "Shunsuke Yoshida", "Tomohiko Nakamura", "Satoru Fukayama", "Jun Ogata"], "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted at ISMIR 2025", "summary": "Japanese idol groups, comprising performers known as \"idols,\" are an\nindispensable part of Japanese pop culture. They frequently appear in live\nconcerts and television programs, entertaining audiences with their singing and\ndancing. Similar to other J-pop songs, idol group music covers a wide range of\nstyles, with various types of chord progressions and instrumental arrangements.\nThese tracks often feature numerous instruments and employ complex mastering\ntechniques, resulting in high signal loudness. Additionally, most songs include\na song division (utawari) structure, in which members alternate between singing\nsolos and performing together. Hence, these songs are well-suited for\nbenchmarking various music information processing techniques such as singer\ndiarization, music source separation, and automatic chord estimation under\nchallenging conditions. Focusing on these characteristics, we constructed a\nsong corpus titled IdolSongsJp by commissioning professional composers to\ncreate 15 tracks in the style of Japanese idol groups. This corpus includes not\nonly mastered audio tracks but also stems for music source separation, dry\nvocal tracks, and chord annotations. This paper provides a detailed description\nof the corpus, demonstrates its diversity through comparisons with real-world\nidol group songs, and presents its application in evaluating several music\ninformation processing techniques.", "AI": {"tldr": "A corpus of Japanese idol group songs, IdolSongsJp, was created to benchmark music processing techniques like singer diarization and chord estimation.", "motivation": "Japanese idol group songs, with their complex structures and high loudness, present challenges for music information processing, necessitating a dedicated corpus for benchmarking.", "method": "Professional composers created 15 tracks mimicking idol group styles, including mastered audio, stems, dry vocals, and chord annotations.", "result": "The corpus, IdolSongsJp, demonstrates diversity comparable to real-world idol songs and is applied to evaluate music processing techniques.", "conclusion": "IdolSongsJp serves as a valuable resource for testing and advancing music information processing under challenging conditions."}}
{"id": "2507.01291", "pdf": "https://arxiv.org/pdf/2507.01291", "abs": "https://arxiv.org/abs/2507.01291", "authors": ["Wenxuan Li", "Xinze Zhou", "Qi Chen", "Tianyu Lin", "Pedro R. A. S. Bassi", "Szymon Plotka", "Jaroslaw B. Cwikla", "Xiaoxi Chen", "Chen Ye", "Zheren Zhu", "Kai Ding", "Heng Li", "Kang Wang", "Yang Yang", "Yucheng Tang", "Daguang Xu", "Alan L. Yuille", "Zongwei Zhou"], "title": "PanTS: The Pancreatic Tumor Segmentation Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "PanTS is a large-scale, multi-institutional dataset curated to advance\nresearch in pancreatic CT analysis. It contains 36,390 CT scans from 145\nmedical centers, with expert-validated, voxel-wise annotations of over 993,000\nanatomical structures, covering pancreatic tumors, pancreas head, body, and\ntail, and 24 surrounding anatomical structures such as vascular/skeletal\nstructures and abdominal/thoracic organs. Each scan includes metadata such as\npatient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,\netc. AI models trained on PanTS achieve significantly better performance in\npancreatic tumor detection, localization, and segmentation compared to those\ntrained on existing public datasets. Our analysis indicates that these gains\nare directly attributable to the 16x larger-scale tumor annotations and\nindirectly supported by the 24 additional surrounding anatomical structures. As\nthe largest and most comprehensive resource of its kind, PanTS offers a new\nbenchmark for developing and evaluating AI models in pancreatic CT analysis.", "AI": {"tldr": "PanTS is a large-scale dataset for pancreatic CT analysis with 36,390 scans and 993,000 expert-annotated structures, improving AI model performance in tumor detection and segmentation.", "motivation": "To advance research in pancreatic CT analysis by providing a comprehensive, large-scale dataset with detailed annotations and metadata.", "method": "Curated 36,390 CT scans from 145 medical centers, including voxel-wise annotations of tumors and 24 surrounding structures, along with metadata.", "result": "AI models trained on PanTS outperform those on existing datasets, attributed to larger-scale annotations and additional anatomical structures.", "conclusion": "PanTS sets a new benchmark for AI model development and evaluation in pancreatic CT analysis."}}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve is an AI-assisted grading platform using LLMs to transcribe and evaluate handwritten STEM responses, reducing grading time by 65% with high accuracy.", "motivation": "Grading handwritten responses in large STEM courses is time-consuming; Pensieve aims to streamline this process.", "method": "Leverages LLMs for transcription and evaluation, integrating a human-in-the-loop interface for the entire grading pipeline.", "result": "Deployed in 20+ institutions, graded 300,000+ responses, reducing grading time by 65% with 95.4% agreement for high-confidence predictions.", "conclusion": "Pensieve effectively automates grading while maintaining accuracy, significantly improving efficiency in STEM courses."}}
{"id": "2507.01030", "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "categories": ["cs.LG"], "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "The paper develops Flamelet Generated Manifold (FGM) libraries for methane combustion using machine learning, achieving high accuracy with an optimized MLP model.", "motivation": "FGM's precision in combustion modeling is resource-intensive. This research aims to streamline FGM library creation for methane fuel using machine learning.", "method": "Four ML algorithms (MLP, Random Forest, Linear Regression, SVM) were tested. MLP was optimized with hyperparameter tuning, selecting a 4-hidden-layer architecture.", "result": "The best model (MLP with 4 layers) achieved 99.81% accuracy, with an error rate of 2.30% across seven libraries.", "conclusion": "MLP is optimal for FGM library generation, offering high accuracy for methane combustion simulations."}}
{"id": "2507.01800", "pdf": "https://arxiv.org/pdf/2507.01800", "abs": "https://arxiv.org/abs/2507.01800", "authors": ["Shengli Zhou", "Jianuo Zhu", "Qilin Huang", "Fangjing Wang", "Yanfu Zhang", "Feng Zheng"], "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision", "categories": ["cs.CV", "cs.MM"], "comment": "ICANN 2025", "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.", "AI": {"tldr": "The paper introduces HCNQA, a 3D VQA model using hierarchical concentration narrowing supervision to ensure rational reasoning pathways, outperforming answer-centric methods.", "motivation": "Addressing the limitations of answer-centric supervision in 3D VQA, which lacks reasoning pathway oversight and risks superficial shortcuts, and underthinking in slow-thinking methods.", "method": "Proposes hierarchical concentration narrowing supervision, mimicking human focus progression, to guide models through three reasoning phases with checkpoint supervision.", "result": "Demonstrates improved performance and rational reasoning pathway development in 3D VQA tasks.", "conclusion": "HCNQA effectively ensures rational reasoning and outperforms existing methods, with code publicly available."}}
{"id": "2507.01254", "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "A robust single-modality parallel processing framework for brain tumor segmentation, effective even with missing MRI modalities, outperforming existing methods.", "motivation": "Conventional methods fail when MRI modalities are missing due to various issues, necessitating a solution for accurate segmentation with incomplete data.", "method": "Uses Holder divergence and mutual information to maintain modality-specific features and dynamically adjust network parameters based on available inputs.", "result": "Achieves high segmentation accuracy on BraTS 2018 and 2020 datasets, outperforming existing methods with missing modalities.", "conclusion": "The proposed framework is robust and effective for brain tumor segmentation, even with incomplete MRI data."}}
{"id": "2507.01297", "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "categories": ["cs.CL", "cs.IR"], "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "CompactDS, a diverse web-scale datastore, enhances RAG performance on reasoning-intensive benchmarks, achieving significant accuracy improvements with simplicity and efficiency.", "motivation": "Prior RAG work underperformed on reasoning-intensive tasks due to limited datastores. This work addresses the gap by introducing a high-quality, web-scale datastore.", "method": "Developed CompactDS, filtering web content for quality and combining in-memory ANN with on-disk exact search for efficient retrieval.", "result": "Achieved 10-33% relative accuracy gains across benchmarks (MMLU, MMLU Pro, GPQA, MATH) with minimal RAG, outperforming web search and complex systems.", "conclusion": "CompactDS demonstrates the importance of diverse, high-quality data for RAG, offering a simple, reproducible solution for retrieval-based AI systems."}}
{"id": "2507.01350", "pdf": "https://arxiv.org/pdf/2507.01350", "abs": "https://arxiv.org/abs/2507.01350", "authors": ["Abhinav Sinha", "Shashi Ranjan Kumar"], "title": "Cooperative Target Capture in 3D Engagements over Switched Dynamic Graphs", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": null, "summary": "This paper presents a leaderless cooperative guidance strategy for\nsimultaneous time-constrained interception of a stationary target when the\ninterceptors exchange information over switched dynamic graphs. We specifically\nfocus on scenarios when the interceptors lack radial acceleration capabilities,\nrelying solely on their lateral acceleration components. This consideration\naligns with their inherent kinematic turn constraints. The proposed strategy\nexplicitly addresses the complexities of coupled 3D engagements, thereby\nmitigating performance degradation that typically arises when the pitch and yaw\nchannels are decoupled into two separate, mutually orthogonal planar\nengagements. Moreover, our formulation incorporates modeling uncertainties\nassociated with the time-to-go estimation into the derivation of cooperative\nguidance commands to ensure robustness against inaccuracies in dynamic\nengagement scenarios. To optimize control efficiency, we analytically derive\nthe lateral acceleration components in the orthogonal pitch and yaw channels by\nsolving an instantaneous optimization problem, subject to an affine constraint.\nWe show that the proposed cooperative guidance commands guarantee consensus in\ntime-to-go values within a predefined time, which can be prescribed as a design\nparameter, regardless of the interceptors' initial configurations. We provide\nsimulations to attest to the efficacy of the proposed method.", "AI": {"tldr": "A leaderless cooperative guidance strategy for simultaneous interception of a stationary target using interceptors with lateral acceleration only, addressing 3D engagement complexities and time-to-go uncertainties.", "motivation": "To overcome performance degradation in decoupled pitch and yaw channels and handle modeling uncertainties in dynamic engagements.", "method": "Derives lateral acceleration components via instantaneous optimization with an affine constraint, ensuring consensus in time-to-go within a predefined time.", "result": "Guarantees consensus in time-to-go values regardless of initial configurations, validated through simulations.", "conclusion": "The proposed strategy effectively handles 3D engagements and uncertainties, ensuring robust and efficient interception."}}
{"id": "2507.01356", "pdf": "https://arxiv.org/pdf/2507.01356", "abs": "https://arxiv.org/abs/2507.01356", "authors": ["Hitoshi Suda", "Shinnosuke Takamichi", "Satoru Fukayama"], "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Perceived voice likability plays a crucial role in various social\ninteractions, such as partner selection and advertising. A system that provides\nreference likable voice samples tailored to target audiences would enable users\nto adjust their speaking style and voice quality, facilitating smoother\ncommunication. To this end, we propose a voice conversion method that controls\nthe likability of input speech while preserving both speaker identity and\nlinguistic content. To improve training data scalability, we train a likability\npredictor on an existing voice likability dataset and employ it to\nautomatically annotate a large speech synthesis corpus with likability ratings.\nExperimental evaluations reveal a significant correlation between the\npredictor's outputs and human-provided likability ratings. Subjective and\nobjective evaluations further demonstrate that the proposed approach\neffectively controls voice likability while preserving both speaker identity\nand linguistic content.", "AI": {"tldr": "A voice conversion method controls voice likability while preserving speaker identity and content, using a likability predictor trained on existing data.", "motivation": "Voice likability is key in social interactions; a system for tailored voice samples could improve communication.", "method": "Train a likability predictor on existing data, annotate a large corpus, and use it for voice conversion.", "result": "Predictor outputs correlate with human ratings; method effectively controls likability without losing identity/content.", "conclusion": "The approach successfully adjusts voice likability while maintaining speaker identity and linguistic content."}}
{"id": "2507.01611", "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility.", "AI": {"tldr": "A novel neural vocoder framework combines QHM and ARMA models with neural networks for high-quality, flexible speech synthesis with reduced time and complexity.", "motivation": "Existing neural vocoders lack interpretability and flexibility in speech modification, and suffer from high computational costs.", "method": "Proposes a framework integrating QHM and ARMA models with neural networks to encode speech into ARMA functions for accurate harmonic modeling.", "result": "Outperforms other methods in speed, quality, and flexibility for speech synthesis and modification.", "conclusion": "The hybrid approach leverages QHM and ARMA strengths, offering efficient, high-quality speech synthesis and modification."}}
{"id": "2507.01323", "pdf": "https://arxiv.org/pdf/2507.01323", "abs": "https://arxiv.org/abs/2507.01323", "authors": ["Rongchang Zhao", "Huanchi Liu", "Jian Zhang"], "title": "SWinMamba: Serpentine Window State Space Model for Vascular Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Vascular segmentation in medical images is crucial for disease diagnosis and\nsurgical navigation. However, the segmented vascular structure is often\ndiscontinuous due to its slender nature and inadequate prior modeling. In this\npaper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve\naccurate vascular segmentation. The proposed SWinMamba innovatively models the\ncontinuity of slender vascular structures by incorporating serpentine window\nsequences into bidirectional state space models. The serpentine window\nsequences enable efficient feature capturing by adaptively guiding global\nvisual context modeling to the vascular structure. Specifically, the Serpentine\nWindow Tokenizer (SWToken) adaptively splits the input image using overlapping\nserpentine window sequences, enabling flexible receptive fields (RFs) for\nvascular structure modeling. The Bidirectional Aggregation Module (BAM)\nintegrates coherent local features in the RFs for vascular continuity\nrepresentation. In addition, dual-domain learning with Spatial-Frequency Fusion\nUnit (SFFU) is designed to enhance the feature representation of vascular\nstructure. Extensive experiments on three challenging datasets demonstrate that\nthe proposed SWinMamba achieves superior performance with complete and\nconnected vessels.", "AI": {"tldr": "SWinMamba, a novel method for vascular segmentation, uses serpentine window sequences and bidirectional state space models to ensure continuity in slender vascular structures, outperforming existing methods.", "motivation": "Discontinuous vascular segmentation in medical images hinders disease diagnosis and surgical navigation, necessitating improved modeling of slender structures.", "method": "SWinMamba incorporates serpentine window sequences (SWToken) for adaptive feature capture and a Bidirectional Aggregation Module (BAM) for continuity. Dual-domain learning with SFFU enhances feature representation.", "result": "SWinMamba achieves superior performance on three datasets, producing complete and connected vascular structures.", "conclusion": "The proposed SWinMamba effectively addresses vascular discontinuity, offering a robust solution for medical image segmentation."}}
{"id": "2507.01446", "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "categories": ["cs.AI"], "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "A multi-agent system using LLMs and fuzzy logic to improve customer service by reducing hallucination risks in SMS-based interactions.", "motivation": "Enhancing customer service quality and response time is vital for loyalty and market share, but LLMs pose hallucination risks.", "method": "Proposes a multi-agent system combining LLMs and fuzzy logic to handle SMS customer requests.", "result": "The system aims to mitigate hallucination risks while improving service efficiency.", "conclusion": "The integration of LLMs with fuzzy logic offers a promising solution for reliable customer service automation."}}
{"id": "2507.01031", "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "categories": ["cs.LG", "cs.SE"], "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "The paper discusses porting PyTorch-based geometric learning frameworks to Intel's Gaudi-v2 HPUs, addressing challenges and providing utilities, tutorials, and examples to ease adoption.", "motivation": "Geometric learning is vital for non-Euclidean data, but non-CUDA hardware like Intel's Gaudi-v2 HPUs requires significant adaptation.", "method": "The authors developed core utilities for essential operations on Gaudi-v2 HPUs and shared tutorials and real-world examples.", "result": "They created a GitHub repository with resources to facilitate geometric-learning on non-CUDA hardware.", "conclusion": "The work reduces barriers for researchers using non-CUDA hardware and supports further optimization and portability."}}
{"id": "2408.12601", "pdf": "https://arxiv.org/pdf/2408.12601", "abs": "https://arxiv.org/abs/2408.12601", "authors": ["Weiliang Chen", "Fangfu Liu", "Diankun Wu", "Haowen Sun", "Jiwen Lu", "Yueqi Duan"], "title": "DreamCinema: Cinematic Transfer with Free Camera and 3D Character", "categories": ["cs.CV", "cs.GR", "cs.MM"], "comment": "Project page: https://liuff19.github.io/DreamCinema", "summary": "We are living in a flourishing era of digital media, where everyone has the\npotential to become a personal filmmaker. Current research on video generation\nsuggests a promising avenue for controllable film creation in pixel space using\nDiffusion models. However, the reliance on overly verbose prompts and\ninsufficient focus on cinematic elements (e.g., camera movement) results in\nvideos that lack cinematic quality. Furthermore, the absence of 3D modeling\noften leads to failures in video generation, such as inconsistent character\nmodels at different frames, ultimately hindering the immersive experience for\nviewers. In this paper, we propose a new framework for film creation,\nDream-Cinema, which is designed for user-friendly, 3D space-based film creation\nwith generative models. Specifically, we decompose 3D film creation into four\nkey elements: 3D character, driven motion, camera movement, and environment. We\nextract the latter three elements from user-specified film shots and generate\nthe 3D character using a generative model based on a provided image. To\nseamlessly recombine these elements and ensure smooth film creation, we propose\nstructure-guided character animation, shape-aware camera movement optimization,\nand environment-aware generative refinement. Extensive experiments demonstrate\nthe effectiveness of our method in generating high-quality films with free\ncamera and 3D characters.", "AI": {"tldr": "Dream-Cinema is a framework for 3D film creation using generative models, focusing on cinematic elements like camera movement and 3D characters to improve video quality.", "motivation": "Current video generation methods lack cinematic quality and 3D consistency, hindering immersive experiences.", "method": "Decomposes film creation into 3D character, motion, camera movement, and environment, using generative models and optimization techniques.", "result": "Generates high-quality films with free camera movement and consistent 3D characters.", "conclusion": "Dream-Cinema offers a user-friendly, 3D-based solution for cinematic film creation."}}
{"id": "2507.01255", "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "categories": ["cs.CV"], "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "AIGVE-MACS is a new model for evaluating AI-generated videos, providing both numerical scores and detailed comments. It outperforms existing methods and includes a benchmark dataset and refinement framework.", "motivation": "Existing evaluation metrics for AI-generated videos lack interpretability and alignment with human judgment.", "method": "AIGVE-MACS uses Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on AIGVE-BENCH 2, a large annotated dataset.", "result": "The model achieves state-of-the-art performance in scoring and comment quality, with a 53.5% improvement in video generation quality through iterative refinement.", "conclusion": "AIGVE-MACS sets a new standard for human-aligned evaluation of AI-generated videos, with released benchmarks and models."}}
{"id": "2507.01299", "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "categories": ["cs.CL"], "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA introduces a novel method for activation sparsification in LLMs, achieving consistent sparsity and speed-up without additional training or magnitude-based pruning.", "motivation": "To address limitations of existing methods (time-consuming recovery training or unstable sparsity) and improve LLM efficiency.", "method": "Uses layerwise orthogonal rotations to transform activations for sparsification, applying Top-K selection for consistent sparsity.", "result": "Achieves 1.30x speed-up with minimal performance drop (0.17 perplexity gap) and reduces accuracy gap to 0.54%.", "conclusion": "LaRoSA is effective across LLMs, offering robust efficiency improvements without compromising performance."}}
{"id": "2507.01485", "pdf": "https://arxiv.org/pdf/2507.01485", "abs": "https://arxiv.org/abs/2507.01485", "authors": ["Yibo Qiu", "Zan Huang", "Zhiyu Wang", "Handi Liu", "Yiling Qiao", "Yifeng Hu", "Shu'ang Sun", "Hangke Peng", "Ronald X Xu", "Mingzhai Sun"], "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments", "categories": ["cs.RO", "cs.AI", "cs.MA", "q-bio.QM"], "comment": null, "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.", "AI": {"tldr": "BioMARS is an AI-driven robotic system integrating LLMs and VLMs to autonomously design and execute biological experiments, outperforming manual methods in tasks like cell passaging and culture.", "motivation": "Current LLMs and VLMs in biology are limited by rigid protocols, poor adaptability, and high complexity. BioMARS aims to overcome these challenges with autonomous experimentation.", "method": "BioMARS uses a hierarchical architecture with three agents: Biologist (protocol synthesis), Technician (execution planning), and Inspector (error handling). It integrates modular robotics and multimodal perception.", "result": "BioMARS matches or exceeds manual performance in cell passaging and culture, and outperforms conventional methods in optimizing retinal pigment epithelial cell differentiation.", "conclusion": "BioMARS demonstrates the feasibility of generalizable AI-driven lab automation, highlighting the transformative potential of language-based reasoning in biology."}}
{"id": "2507.01750", "pdf": "https://arxiv.org/pdf/2507.01750", "abs": "https://arxiv.org/abs/2507.01750", "authors": ["Jose A. Lopez", "Georg Stemmer", "H\u00e9ctor Cordourier Maruri"], "title": "Generalizable Detection of Audio Deepfakes", "categories": ["eess.AS", "cs.SD"], "comment": "8 pages, 3 figures", "summary": "In this paper, we present our comprehensive study aimed at enhancing the\ngeneralization capabilities of audio deepfake detection models. We investigate\nthe performance of various pre-trained backbones, including Wav2Vec2, WavLM,\nand Whisper, across a diverse set of datasets, including those from the\nASVspoof challenges and additional sources. Our experiments focus on the\neffects of different data augmentation strategies and loss functions on model\nperformance. The results of our research demonstrate substantial enhancements\nin the generalization capabilities of audio deepfake detection models,\nsurpassing the performance of the top-ranked single system in the ASVspoof 5\nChallenge. This study contributes valuable insights into the optimization of\naudio models for more robust deepfake detection and facilitates future research\nin this critical area.", "AI": {"tldr": "Study improves audio deepfake detection models' generalization using pre-trained backbones, data augmentation, and loss functions, outperforming ASVspoof 5's top system.", "motivation": "Enhance generalization of audio deepfake detection models for robust performance across diverse datasets.", "method": "Evaluated pre-trained backbones (Wav2Vec2, WavLM, Whisper) with varied data augmentation and loss functions.", "result": "Achieved significant generalization improvements, surpassing ASVspoof 5 Challenge's top system.", "conclusion": "Provides insights for optimizing audio models and advancing deepfake detection research."}}
{"id": "2507.01821", "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanu\u00ebl A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications.", "AI": {"tldr": "A low-complexity DNN for real-time wind noise suppression on resource-constrained devices, achieving performance close to state-of-the-art with minimal parameters and computational power.", "motivation": "Wind noise degrades outdoor audio quality, and existing solutions struggle with real-time suppression on constrained devices.", "method": "Proposes a single-channel DNN leveraging wind noise's spectral characteristics, designed for low complexity (249K parameters, ~73 MHz).", "result": "Performance comparable to state-of-the-art ULCNet, suitable for embedded/mobile applications.", "conclusion": "The model effectively suppresses wind noise with minimal resource usage, making it practical for real-time applications."}}
{"id": "2507.01326", "pdf": "https://arxiv.org/pdf/2507.01326", "abs": "https://arxiv.org/abs/2507.01326", "authors": ["Dong Liang", "Xingyu Qiu", "Yuzhen Li", "Wei Wang", "Kuanquan Wang", "Suyu Dong", "Gongning Luo"], "title": "Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 3 figures, accepted by MICCAI", "summary": "MR imaging techniques are of great benefit to disease diagnosis. However, due\nto the limitation of MR devices, significant intensity inhomogeneity often\nexists in imaging results, which impedes both qualitative and quantitative\nmedical analysis. Recently, several unsupervised deep learning-based models\nhave been proposed for MR image improvement. However, these models merely\nconcentrate on global appearance learning, and neglect constraints from image\nstructures and smoothness of bias field, leading to distorted corrected\nresults. In this paper, novel structure and smoothness constrained dual\nnetworks, named S2DNets, are proposed aiming to self-supervised bias field\ncorrection. S2DNets introduce piece-wise structural constraints and smoothness\nof bias field for network training to effectively remove non-uniform intensity\nand retain much more structural details. Extensive experiments executed on both\nclinical and simulated MR datasets show that the proposed model outperforms\nother conventional and deep learning-based models. In addition to comparison on\nvisual metrics, downstream MR image segmentation tasks are also used to\nevaluate the impact of the proposed model. The source code is available at:\nhttps://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.", "AI": {"tldr": "S2DNets, a novel dual-network model, addresses MR image intensity inhomogeneity by incorporating structural constraints and bias field smoothness, outperforming existing methods.", "motivation": "Intensity inhomogeneity in MR images hampers diagnosis. Current deep learning models lack structural and smoothness constraints, leading to distorted corrections.", "method": "Proposes S2DNets with piece-wise structural constraints and bias field smoothness for self-supervised correction.", "result": "Outperforms conventional and deep learning models on clinical and simulated datasets, improving downstream segmentation tasks.", "conclusion": "S2DNets effectively corrects inhomogeneity while preserving structural details, enhancing MR image analysis."}}
{"id": "2507.01489", "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "categories": ["cs.AI", "cs.MA"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "The paper introduces Agent-as-tool, a hierarchical framework separating tool calling and reasoning processes in LLM-based agents, improving performance with minimal fine-tuning.", "motivation": "Existing methods struggle with simultaneous tool calling and reasoning, burdening models with redundant data.", "method": "Proposes Agent-as-tool, a hierarchical framework decoupling tool calling (handled by one agent) and reasoning (handled by another).", "result": "Achieved strong performance with slight fine-tuning (63.2% exact match, 75.2% cover exact match in Bamboogle).", "conclusion": "Agent-as-tool effectively improves reasoning by isolating tool calling, outperforming prior methods."}}
{"id": "2507.01032", "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "Proposes an uncertainty-aware, multi-view dynamic decision framework for omics data classification to reduce costs while maintaining accuracy.", "motivation": "High costs and resource waste in multi-omics profiling necessitate a cost-effective, accurate diagnostic method.", "method": "Uses neural networks with refined activation functions for Dirichlet distribution parameters, integrates modalities via Dempster-Shafer theory, and employs dynamic decision-making.", "result": "Achieves accurate classification with fewer omics modalities in 50%+ cases, matching full-omics performance.", "conclusion": "The framework reduces redundant testing while preserving diagnostic accuracy and biological insights."}}
{"id": "2507.01269", "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "This paper reviews weed mapping methods, focusing on data acquisition, processing, and mapping techniques to improve precision management and sustainability.", "motivation": "The lack of comprehensive reviews on weed mapping, especially covering the entire pipeline from data to tools, hinders progress in the field.", "method": "The review systematically examines state-of-the-art methods in data acquisition (sensors, platforms), processing (annotation, modeling), and mapping (spatiotemporal analysis, decision tools) following PRISMA guidelines.", "result": "The synthesis provides a holistic understanding of weed mapping, highlighting advancements in sensors, analytics, and machine learning for better spatial-temporal resolution.", "conclusion": "This review serves as a foundational reference for future research, aiming to develop efficient, scalable, and sustainable weed management systems."}}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "Advanced instruction-tuned reasoning models like Deepseek-R1 excel in solving complex physics problems, achieving state-of-the-art accuracy and unique symbolic reasoning patterns, with few-shot prompting further boosting performance.", "motivation": "Addressing the challenge of physics reasoning for LLMs, which requires deep conceptual understanding and problem-solving skills.", "method": "Application of advanced instruction-tuned reasoning models (e.g., Deepseek-R1) to diverse physics problems from the SciBench benchmark, using few-shot prompting.", "result": "Models achieve top accuracy and exhibit unique symbolic reasoning patterns, with few-shot prompting enhancing performance.", "conclusion": "Reasoning models show promise in physics problem-solving, with strategic prompting offering potential for further improvements."}}
{"id": "2507.01717", "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "Agent Ideate, a framework using LLMs and autonomous agents, generates business ideas from patents, outperforming standalone LLMs in quality, relevance, and novelty.", "motivation": "Patents hold valuable technical knowledge, but accessing and interpreting this information is challenging. The goal is to leverage LLMs and agents to unlock innovation from patent data.", "method": "Designed Agent Ideate, a framework combining LLMs and agent-based architectures, tested in Computer Science, NLP, and Material Chemistry domains.", "result": "Agentic approach outperformed standalone LLMs in idea quality, relevance, and novelty.", "conclusion": "Combining LLMs with agentic workflows enhances innovation by effectively generating business ideas from patents."}}
{"id": "2507.01765", "pdf": "https://arxiv.org/pdf/2507.01765", "abs": "https://arxiv.org/abs/2507.01765", "authors": ["Sarina Meyer", "Ekaterina Kolos", "Ngoc Thang Vu"], "title": "First Steps Towards Voice Anonymization for Code-Switching Speech", "categories": ["eess.AS"], "comment": "accepted at Interspeech 2025", "summary": "The goal of voice anonymization is to modify an audio such that the true\nidentity of its speaker is hidden. Research on this task is typically limited\nto the same English read speech datasets, thus the efficacy of current methods\nfor other types of speech data remains unknown. In this paper, we present the\nfirst investigation of voice anonymization for the multilingual phenomenon of\ncode-switching speech. We prepare two corpora for this task and propose\nadaptations to a multilingual anonymization model to make it applicable for\ncode-switching speech. By testing the anonymization performance of this and two\nlanguage-independent methods on the datasets, we find that only the\nmultilingual system performs well in terms of privacy and utility preservation.\nFurthermore, we observe challenges in performing utility evaluations on this\ndata because of its spontaneous character and the limited code-switching\nsupport by the multilingual speech recognition model.", "AI": {"tldr": "The paper investigates voice anonymization for multilingual code-switching speech, proposing adaptations to a multilingual model and evaluating its performance against language-independent methods.", "motivation": "Current voice anonymization research is limited to English read speech, leaving the efficacy for multilingual code-switching speech unexplored.", "method": "Two corpora are prepared, and a multilingual anonymization model is adapted for code-switching speech. Performance is tested against language-independent methods.", "result": "Only the multilingual system performs well in privacy and utility preservation. Utility evaluations face challenges due to spontaneous speech and limited code-switching support in recognition models.", "conclusion": "The study highlights the effectiveness of multilingual models for code-switching speech anonymization but notes evaluation challenges."}}
{"id": "2507.01931", "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "The study compares OpenAI's Whisper and Facebook's Wav2Vec-BERT for Bangla ASR, finding Wav2Vec-BERT superior in performance and efficiency.", "motivation": "To evaluate state-of-the-art ASR models for low-resource languages like Bangla.", "method": "Experiments with fine-tuning and hyperparameter optimization on Mozilla Common Voice-17 and OpenSLR datasets, measuring WER, CER, training time, and computational efficiency.", "result": "Wav2Vec-BERT outperformed Whisper in all metrics, requiring fewer resources.", "conclusion": "Wav2Vec-BERT is more effective for low-resource ASR tasks, offering insights for robust system development."}}
{"id": "2507.01387", "pdf": "https://arxiv.org/pdf/2507.01387", "abs": "https://arxiv.org/abs/2507.01387", "authors": ["Ahmad Soliman", "Ron Keuth", "Marian Himstedt"], "title": "BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The limited availability of bronchoscopy images makes image synthesis\nparticularly interesting for training deep learning models. Robust image\ntranslation across different domains -- virtual bronchoscopy, phantom as well\nas in-vivo and ex-vivo image data -- is pivotal for clinical applications. This\npaper proposes BronchoGAN introducing anatomical constraints for image-to-image\ntranslation being integrated into a conditional GAN. In particular, we force\nbronchial orifices to match across input and output images. We further propose\nto use foundation model-generated depth images as intermediate representation\nensuring robustness across a variety of input domains establishing models with\nsubstantially less reliance on individual training datasets. Moreover our\nintermediate depth image representation allows to easily construct paired image\ndata for training. Our experiments showed that input images from different\ndomains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to\nimages mimicking realistic human airway appearance. We demonstrated that\nanatomical settings (i.e. bronchial orifices) can be robustly preserved with\nour approach which is shown qualitatively and quantitatively by means of\nimproved FID, SSIM and dice coefficients scores. Our anatomical constraints\nenabled an improvement in the Dice coefficient of up to 0.43 for synthetic\nimages. Through foundation models for intermediate depth representations,\nbronchial orifice segmentation integrated as anatomical constraints into\nconditional GANs we are able to robustly translate images from different\nbronchoscopy input domains. BronchoGAN allows to incorporate public CT scan\ndata (virtual bronchoscopy) in order to generate large-scale bronchoscopy image\ndatasets with realistic appearance. BronchoGAN enables to bridge the gap of\nmissing public bronchoscopy images.", "AI": {"tldr": "BronchoGAN uses anatomical constraints and foundation model-generated depth images for robust image-to-image translation across bronchoscopy domains, improving synthetic image quality and reducing reliance on specific datasets.", "motivation": "The limited availability of bronchoscopy images hinders deep learning model training. Robust translation across diverse domains (virtual, phantom, in-vivo, ex-vivo) is crucial for clinical applications.", "method": "Proposes BronchoGAN, integrating anatomical constraints (bronchial orifice matching) into a conditional GAN. Uses foundation model-generated depth images as intermediate representation for robustness and paired training data creation.", "result": "Successful translation of varied input domains (e.g., virtual bronchoscopy, phantoms) to realistic human airway images. Improved FID, SSIM, and Dice scores (up to 0.43).", "conclusion": "BronchoGAN bridges the gap of missing public bronchoscopy images by leveraging CT scan data, enabling large-scale dataset generation with realistic appearance."}}
{"id": "2507.01597", "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "T3DM improves TKG reasoning by addressing distribution shift and enhancing negative sampling.", "motivation": "Current TKGR methods struggle with event distribution shifts and low-quality negative samples.", "method": "Proposes T3DM for distribution shift modeling and adversarial-based negative sampling.", "result": "T3DM outperforms state-of-the-art baselines in robustness and performance.", "conclusion": "T3DM effectively addresses key challenges in TKGR, offering superior results."}}
{"id": "2507.01034", "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "categories": ["cs.LG", "cs.AI"], "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions.", "AI": {"tldr": "The paper proposes a data-driven approach using LSTM to forecast electricity load, generation, and deficits in Benghazi, Libya, outperforming other models like ARIMA and XGBoost.", "motivation": "Accurate electricity forecasting is vital for grid stability in Benghazi, Libya, where infrastructure and generation deficits persist.", "method": "Multiple time series models (ARIMA, SARIMA, XGBoost, LSTM) were applied to historical data (2019, 2023), enhanced with preprocessing techniques.", "result": "LSTM outperformed other models, effectively handling non-stationary and seasonal patterns, especially when integrated with exogenous factors like temperature.", "conclusion": "The optimized LSTM framework provides actionable insights for policymakers and grid operators in volatile, data-scarce regions."}}
{"id": "2507.01275", "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its\nflexible data requirements during model training. Dominant methods based on\ncontrastive learning not only introduce haze-unrelated content information, but\nalso ignore haze-specific properties in the frequency domain (\\ie,~haze-related\ndegradation is mainly manifested in the amplitude spectrum). To address these\nissues, we propose a novel frequency domain-based diffusion model, named \\ours,\nfor fully exploiting the beneficial knowledge in unpaired clear data. In\nparticular, inspired by the strong generative ability shown by Diffusion Models\n(DMs), we tackle the dehazing task from the perspective of frequency domain\nreconstruction and perform the DMs to yield the amplitude spectrum consistent\nwith the distribution of clear images. To implement it, we propose an Amplitude\nResidual Encoder (ARE) to extract the amplitude residuals, which effectively\ncompensates for the amplitude gap from the hazy to clear domains, as well as\nprovide supervision for the DMs training. In addition, we propose a Phase\nCorrection Module (PCM) to eliminate artifacts by further refining the phase\nspectrum during dehazing with a simple attention mechanism. Experimental\nresults demonstrate that our \\ours outperforms other state-of-the-art methods\non both synthetic and real-world datasets.", "AI": {"tldr": "A novel frequency domain-based diffusion model (\\ours) is proposed for unpaired image dehazing, leveraging amplitude spectrum reconstruction and phase correction to outperform existing methods.", "motivation": "Existing contrastive learning methods for unpaired image dehazing introduce irrelevant content and ignore haze-specific frequency domain properties, particularly amplitude spectrum degradation.", "method": "The proposed model uses a Diffusion Model (DM) for frequency domain reconstruction, an Amplitude Residual Encoder (ARE) to bridge the amplitude gap, and a Phase Correction Module (PCM) to refine the phase spectrum.", "result": "The model achieves superior performance on synthetic and real-world datasets compared to state-of-the-art methods.", "conclusion": "The frequency domain approach with DMs, ARE, and PCM effectively addresses unpaired image dehazing, demonstrating significant improvements over existing techniques."}}
{"id": "2507.01335", "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "LEDOM is the first reverse language model trained autoregressively, showing potential for general tasks and introducing Reverse Reward for improved mathematical reasoning.", "motivation": "To explore the capabilities and applications of a purely reverse language model, which processes sequences in reverse order.", "method": "LEDOM is trained on 435B tokens with 2B and 7B parameter variants, using autoregressive training for reverse token prediction.", "result": "LEDOM demonstrates unique backward reasoning capabilities, and its Reverse Reward application improves mathematical reasoning tasks.", "conclusion": "LEDOM has broad application potential, and the release of models and data aims to foster further research."}}
{"id": "2406.07431", "pdf": "https://arxiv.org/pdf/2406.07431", "abs": "https://arxiv.org/abs/2406.07431", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "title": "Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments", "categories": ["cs.MA", "cs.CV"], "comment": "9 pages, 10 figures, 2 tables, IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2024", "summary": "We study pursuit-evasion games in highly occluded urban environments, e.g.\ntall buildings in a city, where a scout (quadrotor) tracks multiple dynamic\ntargets on the ground. We show that we can build a neural radiance field (NeRF)\nrepresentation of the city -- online -- using RGB and depth images from\ndifferent vantage points. This representation is used to calculate the\ninformation gain to both explore unknown parts of the city and track the\ntargets -- thereby giving a completely first-principles approach to actively\ntracking dynamic targets. We demonstrate, using a custom-built simulator using\nOpen Street Maps data of Philadelphia and New York City, that we can explore\nand locate 20 stationary targets within 300 steps. This is slower than a greedy\nbaseline, which does not use active perception. But for dynamic targets that\nactively hide behind occlusions, we show that our approach maintains, at worst,\na tracking error of 200m; the greedy baseline can have a tracking error as\nlarge as 600m. We observe a number of interesting properties in the scout's\npolicies, e.g., it switches its attention to track a different target\nperiodically, as the quality of the NeRF representation improves over time, the\nscout also becomes better in terms of target tracking. Code is available at\nhttps://github.com/grasp-lyrl/ActiveScout.", "AI": {"tldr": "A method for tracking dynamic targets in occluded urban environments using neural radiance fields (NeRF) for active perception, outperforming greedy baselines in dynamic scenarios.", "motivation": "To address the challenge of tracking dynamic targets in highly occluded urban settings, where traditional methods fail due to lack of active perception.", "method": "Uses NeRF to create an online 3D representation of the city from RGB and depth images, enabling information gain for exploration and tracking. Tested in a custom simulator with Open Street Maps data.", "result": "For dynamic targets, maintains tracking error of 200m (vs. 600m for greedy baseline). Locates 20 stationary targets in 300 steps, slower but more robust than greedy methods.", "conclusion": "The approach demonstrates effectiveness in dynamic target tracking, with improved performance as the NeRF representation evolves, and offers a principled solution for active perception."}}
{"id": "2507.01888", "pdf": "https://arxiv.org/pdf/2507.01888", "abs": "https://arxiv.org/abs/2507.01888", "authors": ["Nina R. Benway", "Saba Tabatabaee", "Dongliang Wang", "Benjamin Munson", "Jonathan L. Preston", "Carol Espy-Wilson"], "title": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders", "categories": ["eess.AS"], "comment": "This manuscript is in submission for publication. It has not yet been\n  peer reviewed", "summary": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/.", "AI": {"tldr": "The study evaluated if articulatory kinematics, inferred via Articulatory Phonology neural networks, matched perceptual ratings of /r/ and /s/ in children with speech sound disorders. Results showed alignment for /r/ and partial alignment for /s/, supporting clinical interpretability.", "motivation": "To assess whether inferred articulatory patterns align with perceptual error categories and gradient ratings, aiding clinical diagnosis and intervention for speech sound disorders.", "method": "Articulatory Phonology vocal tract variables were inferred for 5,961 utterances from 118 children and 3 adults. Perceptual ratings used the PERCEPT Rating Scale. Linear mixed models tested alignment of articulatory patterns with perceptual categories and gradient scores.", "result": "For /r/, 17 of 18 hypotheses were supported, involving tongue tip/body constrictions. For /s/, 7 of 15 hypotheses were supported, mainly tongue tip-related. PERCEPT scores predicted articulatory proximity to correct productions.", "conclusion": "Inferred vocal tract variables aligned with perceptual judgments, especially for /r/, supporting their clinical utility and the PERCEPT Rating Scale's effectiveness in quantifying articulatory errors."}}
{"id": "2408.12633", "pdf": "https://arxiv.org/pdf/2408.12633", "abs": "https://arxiv.org/abs/2408.12633", "authors": ["John M McBride", "Elizabeth Phillips", "Patrick E Savage", "Steven Brown", "Tsvi Tlusty"], "title": "Melody predominates over harmony in the evolution of musical scales across 96 countries", "categories": ["cs.SD", "eess.AS", "physics.soc-ph"], "comment": null, "summary": "The standard theory of musical scales since antiquity has been based on\nharmony, rather than melody. While recent analyses provide mixed support for a\nrole of melody as well as harmony, we lack a comparative analysis based on\ncross-cultural data. We address this longstanding problem through a rigorous\ncomputational comparison of the main theories using 1,314 scales from 96\ncountries. There is near-universal support for melodic theories, which predict\nstep-sizes of 1-3 semitones. Harmony accounts for the prevalence of certain\nsimple-integer-ratio intervals, particularly for music-theoretic scales from\nEurasian societies, which may explain their dominance amongst Western scholars.\nHowever, harmony is a poor predictor of scales measured from ethnographic\nrecordings, particularly outside of Eurasia. Overall, we show that the\nhistorical emphasis on harmony is misguided and that melody is the primary\ndeterminant of the world's musical scales.", "AI": {"tldr": "The paper challenges the historical focus on harmony in musical scales, showing melody is the primary determinant globally, supported by cross-cultural data.", "motivation": "To address the lack of comparative analysis on the role of melody vs. harmony in musical scales using cross-cultural data.", "method": "A computational comparison of 1,314 scales from 96 countries to evaluate melodic and harmonic theories.", "result": "Melodic theories are nearly universally supported, while harmony predicts scales poorly outside Eurasian societies.", "conclusion": "Melody, not harmony, is the primary determinant of musical scales worldwide, challenging traditional Western scholarship."}}
{"id": "2507.01564", "pdf": "https://arxiv.org/pdf/2507.01564", "abs": "https://arxiv.org/abs/2507.01564", "authors": ["Chia-Ming Lee", "Bo-Cheng Qiu", "Ting-Yao Chen", "Ming-Han Sun", "Fang-Ying Lin", "Jung-Tse Tsai", "I-An Tsai", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich classifies chest CT scans from four distinct medical centers. To address\nmulti-source variability, we employ the Spatial-Slice Feature Learning (SSFL)\nframework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing\npipeline combines lung region extraction, quality control, and adaptive slice\nsampling to select eight representative slices per scan. We compare\nEfficientNet and Swin Transformer architectures on the validation set. The\nEfficientNet model achieves an F1-score of 94.68%, compared to the Swin\nTransformer's 93.34%. The results demonstrate the effectiveness of our\nKDS-based pipeline on multi-source data and highlight the importance of dataset\nbalance in multi-institutional medical imaging evaluation.", "AI": {"tldr": "The paper presents a solution for COVID-19 detection in chest CT scans from multiple medical centers using the SSFL framework with KDS. EfficientNet outperforms Swin Transformer, achieving a 94.68% F1-score.", "motivation": "To address multi-source variability in COVID-19 detection from chest CT scans across different medical centers.", "method": "Uses the SSFL framework with KDS for preprocessing, including lung region extraction, quality control, and adaptive slice sampling. Compares EfficientNet and Swin Transformer architectures.", "result": "EfficientNet achieves a higher F1-score (94.68%) than Swin Transformer (93.34%).", "conclusion": "The KDS-based pipeline is effective for multi-source data, and dataset balance is crucial for multi-institutional medical imaging evaluation."}}
{"id": "2507.01749", "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "categories": ["cs.AI"], "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "The paper proposes a crowd-shipping system using in-store customers as couriers, employing an MDP model with NeurADP and DDQN for dynamic order assignment and pricing, achieving significant cost savings.", "motivation": "Addresses the need for efficient last-mile delivery in urban areas by leveraging existing shoppers for deliveries.", "method": "Uses a Markov Decision Process (MDP) model with NeurADP for adaptive order assignment and DDQN for dynamic pricing, integrating multi-drop routing and handling offer acceptance uncertainty.", "result": "Achieves 6.7% cost savings over fixed pricing and 18% over myopic baselines; flexible delays and multi-destination routing further reduce costs by 8% and 17%.", "conclusion": "Dynamic, forward-looking policies enhance crowd-shipping efficiency, offering practical insights for urban logistics."}}
{"id": "2507.01035", "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "AI": {"tldr": "The paper proposes a hybrid GNN-LLM recommender system optimized for speed and efficiency using quantization, LoRA, distillation, FPGA, and DeepSpeed, achieving significant improvements in accuracy and training time.", "motivation": "Online services require fast and efficient recommender systems to handle complex user-item interactions in real-time, addressing computational bottlenecks in hybrid GNN-LLM models.", "method": "A hybrid GNN-LLM architecture was optimized with quantization, LoRA, distillation, FPGA, and DeepSpeed, tested under R 4.4.2.", "result": "The optimized system achieved 13.6% higher accuracy (NDCG@10: 0.75) at 40-60ms latency, with LoRA reducing training time by 66% (3.8 hours).", "conclusion": "Hardware-software co-design and parameter-efficient tuning enable hybrid models to outperform standalone GNN or LLM approaches, recommending FPGA and LoRA for real-time deployment. Future work includes federated learning and advanced fusion architectures."}}
{"id": "2507.01290", "pdf": "https://arxiv.org/pdf/2507.01290", "abs": "https://arxiv.org/abs/2507.01290", "authors": ["Sunyong Seo", "Semin Kim", "Jongha Lee"], "title": "Learning an Ensemble Token from Task-driven Priors in Facial Analysis", "categories": ["cs.CV"], "comment": "11pages, 8figures, 4tables", "summary": "Facial analysis exhibits task-specific feature variations. While\nConvolutional Neural Networks (CNNs) have enabled the fine-grained\nrepresentation of spatial information, Vision Transformers (ViTs) have\nfacilitated the representation of semantic information at the patch level.\nAlthough the generalization of conventional methodologies has advanced visual\ninterpretability, there remains paucity of research that preserves the unified\nfeature representation on single task learning during the training process. In\nthis work, we introduce ET-Fuser, a novel methodology for learning ensemble\ntoken by leveraging attention mechanisms based on task priors derived from\npre-trained models for facial analysis. Specifically, we propose a robust prior\nunification learning method that generates a ensemble token within a\nself-attention mechanism, which shares the mutual information along the\npre-trained encoders. This ensemble token approach offers high efficiency with\nnegligible computational cost. Our results show improvements across a variety\nof facial analysis, with statistically significant enhancements observed in the\nfeature representations.", "AI": {"tldr": "ET-Fuser introduces a novel method for unified feature representation in facial analysis using ensemble tokens and attention mechanisms, improving performance with minimal computational cost.", "motivation": "Existing methods lack unified feature representation in single-task learning for facial analysis, despite advancements in CNNs and ViTs.", "method": "ET-Fuser leverages attention mechanisms and task priors from pre-trained models to generate ensemble tokens, sharing mutual information across encoders.", "result": "The method shows statistically significant improvements in feature representations across various facial analysis tasks.", "conclusion": "ET-Fuser efficiently enhances facial analysis performance by unifying feature representation with negligible computational overhead."}}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "The paper introduces SynPref-40M, a large-scale preference dataset, and Skywork-Reward-V2, a suite of reward models, to address limitations in current reward models by improving data quality and scale through human-AI collaboration.", "motivation": "Current reward models perform poorly due to narrow, synthetic, or low-quality preference datasets, limiting their ability to capture nuanced human preferences.", "method": "A human-AI synergistic pipeline curates a large-scale dataset (SynPref-40M), and Skywork-Reward-V2 models are trained on a subset of this data.", "result": "Skywork-Reward-V2 achieves state-of-the-art performance across seven benchmarks, demonstrating versatility in alignment, correctness, safety, and bias resistance.", "conclusion": "The work highlights the importance of high-quality data curation and human-AI collaboration in advancing reward models, unlocking their potential."}}
{"id": "2503.02189", "pdf": "https://arxiv.org/pdf/2503.02189", "abs": "https://arxiv.org/abs/2503.02189", "authors": ["Dickness Kakitahi Kwesiga", "Angshuman Guin", "Michael Hunter"], "title": "Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor", "categories": ["cs.MA"], "comment": null, "summary": "Previous studies that have formulated multi-agent reinforcement learning (RL)\nalgorithms for adaptive traffic signal control have primarily used value-based\nRL methods. However, recent literature has shown that policy-based methods may\nperform better in partially observable environments. Additionally, RL methods\nremain largely untested for real-world normally signal timing plans because of\nthe simplifying assumptions common in the literature. The current study\nattempts to address these gaps and formulates a multi-agent proximal policy\noptimization (MA-PPO) algorithm to implement adaptive and coordinated traffic\ncontrol along an arterial corridor. The formulated MA-PPO has a\ncentralized-critic architecture under a centralized training and decentralized\nexecution framework. Agents are designed to allow selection and implementation\nof up to eight signal phases, as commonly implemented in field controllers. The\nformulated algorithm is tested on a simulated real-world seven intersection\ncorridor. The speed of convergence for each agent was found to depend on the\nsize of the action space, which depends on the number and sequence of signal\nphases. The performance of the formulated MA-PPO adaptive control algorithm is\ncompared with the field implemented actuated-coordinated signal control (ASC),\nmodeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The\ntrained MA-PPO performed significantly better than the ASC for all movements.\nCompared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the\nprimary and secondary coordination directions, respectively. For cross streets\nmovements MA-PPO also showed significant crossing time reductions. Volume\nsensitivity experiments revealed that the formulated MA-PPO demonstrated good\nstability, robustness, and adaptability to changes in traffic demand.", "AI": {"tldr": "The paper introduces a multi-agent proximal policy optimization (MA-PPO) algorithm for adaptive traffic signal control, showing superior performance over traditional methods in simulated real-world conditions.", "motivation": "Address gaps in multi-agent RL for traffic control, particularly the underuse of policy-based methods and untested assumptions in real-world scenarios.", "method": "Formulates MA-PPO with a centralized-critic architecture, tested on a simulated seven-intersection corridor, comparing it to actuated-coordinated signal control (ASC).", "result": "MA-PPO outperformed ASC, improving travel times by 2% and 24% in primary and secondary directions, respectively, and reducing crossing times.", "conclusion": "MA-PPO is stable, robust, and adaptable to traffic demand changes, proving its potential for real-world traffic signal control."}}
{"id": "2410.18908", "pdf": "https://arxiv.org/pdf/2410.18908", "abs": "https://arxiv.org/abs/2410.18908", "authors": ["Jing Peng", "Yucheng Wang", "Bohan Li", "Yiwei Guo", "Hankun Wang", "Yangui Fang", "Yu Xi", "Haoyu Li", "Xu Li", "Ke Zhang", "Shuai Wang", "Kai Yu"], "title": "A Survey on Speech Large Language Models for Understanding", "categories": ["eess.AS"], "comment": "This paper is submitted as an invited overview to IEEE JSTSP", "summary": "Speech understanding is essential for interpreting the diverse forms of\ninformation embedded in spoken language, including linguistic, paralinguistic,\nand non-linguistic cues that are vital for effective human-computer\ninteraction. The rapid advancement of large language models (LLMs) has\ncatalyzed the emergence of Speech Large Language Models (Speech LLMs), which\nmarks a transformative shift toward general-purpose speech understanding\nsystems. To further clarify and systematically delineate task objectives, in\nthis paper, we formally define the concept of speech understanding and\nintroduce a structured taxonomy encompassing its informational, functional, and\nformat dimensions. Within this scope of definition, we present a comprehensive\nreview of current Speech LLMs, analyzing their architectures through a\nthree-stage abstraction: Modality Feature Extraction, Modality Information\nFusion, and LLM Inference. In addition, we examine training strategies, discuss\nrepresentative datasets, and review evaluation methodologies adopted in the\nfield. Based on empirical analyses and experimental evidence, we identify two\nkey challenges currently facing Speech LLMs: instruction sensitivity and\ndegradation in semantic reasoning and propose concrete directions for\naddressing these issues. Through this systematic and detailed survey, we aim to\noffer a foundational reference for researchers and practitioners working toward\nmore robust, generalizable, and human-aligned Speech LLMs.", "AI": {"tldr": "The paper defines speech understanding, introduces a taxonomy, reviews Speech LLMs, and identifies challenges like instruction sensitivity and semantic reasoning degradation.", "motivation": "To clarify and systematize speech understanding for human-computer interaction, leveraging advancements in LLMs.", "method": "Defines speech understanding, introduces a taxonomy, reviews Speech LLMs' architectures, training, datasets, and evaluation.", "result": "Identifies challenges: instruction sensitivity and semantic reasoning degradation in Speech LLMs.", "conclusion": "Provides a foundational reference for robust, generalizable, and human-aligned Speech LLMs."}}
{"id": "2412.03771", "pdf": "https://arxiv.org/pdf/2412.03771", "abs": "https://arxiv.org/abs/2412.03771", "authors": ["Ysobel Sims", "Alexandre Mendes", "Stephan Chalup"], "title": "Embedding-Space Diffusion for Zero-Shot Environmental Sound Classification", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Zero-shot learning enables models to generalise to unseen classes by\nleveraging semantic information, bridging the gap between training and testing\nsets with non-overlapping classes. While much research has focused on zero-shot\nlearning in computer vision, the application of these methods to environmental\naudio remains underexplored, with poor performance in existing studies.\nGenerative methods, which have demonstrated success in computer vision, are\nnotably absent from zero-shot environmental sound classification studies.\n  To address this gap, this work investigates generative methods for zero-shot\nlearning in environmental audio. Two successful generative models from computer\nvision are adapted: a cross-aligned and distribution-aligned variational\nautoencoder (CADA-VAE) and a leveraging invariant side generative adversarial\nnetwork (LisGAN). Additionally, we introduced a novel diffusion model\nconditioned on class auxiliary data. Synthetic embeddings generated by the\ndiffusion model are combined with seen class embeddings to train a classifier.\n  Experiments are conducted on five environmental audio datasets, ESC-50,\nARCA23K-FSD, FSC22, UrbanSound8k and TAU Urban Acoustics 2019, and one music\nclassification dataset, GTZAN. Results show that the diffusion model\noutperforms all baseline methods on average across six audio datasets.\n  This work establishes the diffusion model as a promising approach for\nzero-shot learning and introduces the first benchmark of generative methods for\nzero-shot environmental sound classification, providing a foundation for future\nresearch.", "AI": {"tldr": "The paper explores generative methods for zero-shot learning in environmental audio, introducing a novel diffusion model that outperforms existing methods.", "motivation": "Address the underexplored application of generative methods in zero-shot environmental sound classification, where existing studies show poor performance.", "method": "Adapts CADA-VAE and LisGAN from computer vision and introduces a diffusion model conditioned on class auxiliary data. Synthetic embeddings from the diffusion model are combined with seen class embeddings for classifier training.", "result": "The diffusion model outperforms baseline methods across six audio datasets, including ESC-50, ARCA23K-FSD, FSC22, UrbanSound8k, TAU Urban Acoustics 2019, and GTZAN.", "conclusion": "The diffusion model is a promising approach for zero-shot learning, establishing the first benchmark for generative methods in environmental sound classification."}}
{"id": "2507.01588", "pdf": "https://arxiv.org/pdf/2507.01588", "abs": "https://arxiv.org/abs/2507.01588", "authors": ["Keuntek Lee", "Jaehyun Park", "Nam Ik Cho"], "title": "Enhancing Multi-Exposure High Dynamic Range Imaging with Overlapped Codebook for Improved Representation Learning", "categories": ["eess.IV"], "comment": "Accepted to International Conference on Pattern Recognition.\n  Springer, Cham, 2025 (ICPR 2024)", "summary": "High dynamic range (HDR) imaging technique aims to create realistic HDR\nimages from low dynamic range (LDR) inputs. Specifically, Multi-exposure HDR\nimaging uses multiple LDR frames taken from the same scene to improve\nreconstruction performance. However, there are often discrepancies in motion\namong the frames, and different exposure settings for each capture can lead to\nsaturated regions. In this work, we first propose an Overlapped codebook (OLC)\nscheme, which can improve the capability of the VQGAN framework for learning\nimplicit HDR representations by modeling the common exposure bracket process in\nthe shared codebook structure. Further, we develop a new HDR network that\nutilizes HDR representations obtained from a pre-trained VQ network and OLC.\nThis allows us to compensate for saturated regions and enhance overall visual\nquality. We have tested our approach extensively on various datasets and have\ndemonstrated that it outperforms previous methods both qualitatively and\nquantitatively", "AI": {"tldr": "Proposes an Overlapped Codebook (OLC) scheme and a new HDR network to improve HDR imaging from LDR inputs, addressing motion discrepancies and saturation issues.", "motivation": "To enhance HDR image reconstruction from LDR inputs by overcoming motion discrepancies and saturation problems in multi-exposure HDR imaging.", "method": "Introduces an Overlapped Codebook (OLC) scheme for VQGAN to learn implicit HDR representations and a new HDR network leveraging pre-trained VQ and OLC.", "result": "Outperforms previous methods in qualitative and quantitative evaluations on various datasets.", "conclusion": "The proposed OLC and HDR network effectively improve HDR imaging quality by addressing key challenges in multi-exposure setups."}}
{"id": "2507.01833", "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "categories": ["cs.AI"], "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "The paper questions mandatory conditions for answer set semantics in non-monotonic logic programming, refines Gelfond's principles, and proposes new semantics based on well-supportedness and minimality.", "motivation": "To evaluate whether existing conditions (minimal model property, constraint monotonicity, foundedness) are too restrictive for answer set semantics and to refine foundational principles.", "method": "Refines Gelfond's answer set (GAS) principles, introduces well-supportedness and minimality, and defines new semantics.", "result": "Proposes refined principles and new semantics, demonstrating their applicability and computational complexity.", "conclusion": "The refined GAS principles offer a flexible alternative to existing semantics, addressing limitations of traditional conditions."}}
{"id": "2507.01037", "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs.", "AI": {"tldr": "The paper introduces FSTA and L2Seg to optimize iterative solvers for VRPs by reducing redundant computations. L2Seg variants improve solver speed by up to 7x.", "motivation": "Redundant computations in iterative solvers for large-scale VRPs due to stable solution segments.", "method": "FSTA decomposes solutions into stable segments (hypernodes) and focuses search on unstable parts. L2Seg, a neural framework, identifies stable/unstable segments with three variants (non-autoregressive, autoregressive, and their synergy).", "result": "L2Seg accelerates solvers by up to 7x, with the NAR and AR synergy performing best.", "conclusion": "L2Seg is a flexible, efficient framework compatible with various solvers and VRPs."}}
{"id": "2507.01305", "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "A technique called DiffusionLight estimates lighting from a single LDR image by reframing it as a chrome ball inpainting problem, using Stable Diffusion XL. It addresses challenges like inconsistent outputs and slow runtime with iterative inpainting and Turbo LoRA for speedup.", "motivation": "Existing methods rely on limited HDR panorama datasets, leading to generalization failures. The goal is to leverage diffusion models for better lighting estimation.", "method": "Uses iterative inpainting to compute a median chrome ball as a stable prior, fine-tunes an Exposure LoRA for HDR light probes, and introduces DiffusionLight-Turbo for faster runtime.", "result": "Produces convincing light estimates across diverse settings with superior generalization. Runtime is reduced from 30 minutes to 30 seconds.", "conclusion": "DiffusionLight and its Turbo variant offer effective, scalable solutions for lighting estimation from LDR images, balancing quality and speed."}}
{"id": "2507.01437", "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "A deep learning method using attention mechanisms is proposed for unified modeling of information extraction and multi-label disease prediction in EHR texts, outperforming existing approaches.", "motivation": "Addressing the unstructured nature and high-dimensional semantic complexity of electronic health record (EHR) texts.", "method": "Transformer-based architecture with multi-layer self-attention for representation learning, and a Sigmoid-based multi-label classifier for disease prediction, enhanced by context-aware semantic alignment.", "result": "The model consistently outperforms existing methods across metrics, showing strong generalization under varying conditions.", "conclusion": "The framework provides an efficient foundation for processing clinical texts and is significant for multi-label medical text modeling."}}
{"id": "2403.14583", "pdf": "https://arxiv.org/pdf/2403.14583", "abs": "https://arxiv.org/abs/2403.14583", "authors": ["Zhan Gao", "Guang Yang", "Amanda Prorok"], "title": "Co-Optimizing Reconfigurable Environments and Policies for Decentralized Multi-Agent Navigation", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": null, "summary": "This work views the multi-agent system and its surrounding environment as a\nco-evolving system, where the behavior of one affects the other. The goal is to\ntake both agent actions and environment configurations as decision variables,\nand optimize these two components in a coordinated manner to improve some\nmeasure of interest. Towards this end, we consider the problem of decentralized\nmulti-agent navigation in a cluttered environment, where we assume that the\nlayout of the environment is reconfigurable. By introducing two sub-objectives\n-- multi-agent navigation and environment optimization -- we propose an\nagent-environment co-optimization problem and develop a coordinated algorithm\nthat alternates between these sub-objectives to search for an optimal synthesis\nof agent actions and environment configurations; ultimately, improving the\nnavigation performance. Due to the challenge of explicitly modeling the\nrelation between the agents, the environment and their performance therein, we\nleverage policy gradient to formulate a model-free learning mechanism within\nthe coordinated framework. A formal convergence analysis shows that our\ncoordinated algorithm tracks the local minimum solution of an associated\ntime-varying non-convex optimization problem. Experiments corroborate\ntheoretical findings and show the benefits of co-optimization. Interestingly,\nthe results also indicate that optimized environments can offer structural\nguidance to de-conflict agents in motion.", "AI": {"tldr": "The paper proposes a co-optimization framework for multi-agent navigation and environment reconfiguration, using policy gradient for model-free learning, and demonstrates improved performance and structural guidance.", "motivation": "To address the interdependence of agent actions and environment configurations in multi-agent systems, aiming to optimize both for better navigation performance.", "method": "Introduces a coordinated algorithm alternating between multi-agent navigation and environment optimization, leveraging policy gradient for model-free learning.", "result": "The algorithm converges to local minima, with experiments showing improved navigation and structural guidance from optimized environments.", "conclusion": "Co-optimizing agents and environments enhances performance and provides structural guidance, validated by theory and experiments."}}
{"id": "2411.06399", "pdf": "https://arxiv.org/pdf/2411.06399", "abs": "https://arxiv.org/abs/2411.06399", "authors": ["Jinbo Hu", "Yin Cao", "Ming Wu", "Fang Kang", "Feiran Yang", "Wenwu Wang", "Mark D. Plumbley", "Jun Yang"], "title": "PSELDNets: Pre-trained Neural Networks on a Large-scale Synthetic Dataset for Sound Event Localization and Detection", "categories": ["eess.AS", "cs.SD"], "comment": "16 pages, 9 figures, accepted by IEEE Transactions on Audio, Speech,\n  and Language Processing. The code is available at\n  https://github.com/Jinbo-Hu/PSELDNets", "summary": "Sound event localization and detection (SELD) has seen substantial\nadvancements through learning-based methods. These systems, typically trained\nfrom scratch on specific datasets, have shown considerable generalization\ncapabilities. Recently, deep neural networks trained on large-scale datasets\nhave achieved remarkable success in the sound event classification (SEC) field,\nprompting an open question of whether these advances can be extended to the\ndevelopment of SELD foundation models. In this paper, leveraging the power of\npre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on a\nlarge-scale synthetic dataset. The synthetic dataset, generated by convolving\nsound events with simulated spatial room impulse responses (SRIRs), contains\n1,167 hours of audio clips with an ontology of 170 sound classes. These\nPSELDNets are applied to various SELD scenarios. When we adapt PSELDNets to\nspecific scenarios, particularly in cases of low-resource data, we introduce a\ndata-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on\nsynthetic-test-set using collected SRIRs from the TAU Spatial Room Impulse\nResponse Database (TAU-SRIR DB) and achieve satisfactory performance. We also\ncarried out experiments to validate the transferability of PSELDNets to three\npublicly available datasets and our own real-world recordings. The results\ndemonstrate that PSELDNets surpass state-of-the-art systems across all publicly\navailable datasets. Given the need for direction-of-arrival estimation, SELD\ngenerally relies on sufficient multi-channel audio clips. However,\nincorporating the AdapterBit, PSELDNets show more efficient adaptability to\nvarious scenarios using minimal multi-channel or even just monophonic audio\nclips, outperforming traditional fine-tuning approaches.", "AI": {"tldr": "The paper proposes pre-trained SELD networks (PSELDNets) using a large-scale synthetic dataset and introduces AdapterBit for efficient fine-tuning in low-resource scenarios, achieving state-of-the-art performance.", "motivation": "To extend advancements in sound event classification (SEC) to sound event localization and detection (SELD) by leveraging pre-trained models and synthetic data.", "method": "Develop PSELDNets using a synthetic dataset (1,167 hours, 170 classes) and fine-tune with AdapterBit for low-resource scenarios.", "result": "PSELDNets outperform state-of-the-art systems on synthetic and real-world datasets, even with minimal multi-channel or monophonic audio.", "conclusion": "PSELDNets with AdapterBit offer efficient adaptability and superior performance, advancing SELD capabilities."}}
{"id": "2504.05684", "pdf": "https://arxiv.org/pdf/2504.05684", "abs": "https://arxiv.org/abs/2504.05684", "authors": ["Tri Ton", "Ji Woo Hong", "Chang D. Yoo"], "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "This paper introduces Timestep-Adaptive Representation Alignment with\nOnset-Aware Conditioning (TARO), a novel framework for high-fidelity and\ntemporally coherent video-to-audio synthesis. Built upon flow-based\ntransformers, which offer stable training and continuous transformations for\nenhanced synchronization and audio quality, TARO introduces two key\ninnovations: (1) Timestep-Adaptive Representation Alignment (TRA), which\ndynamically aligns latent representations by adjusting alignment strength based\non the noise schedule, ensuring smooth evolution and improved fidelity, and (2)\nOnset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp\nevent-driven markers of audio-relevant visual moments to enhance\nsynchronization with dynamic visual events. Extensive experiments on the\nVGGSound and Landscape datasets demonstrate that TARO outperforms prior\nmethods, achieving relatively 53% lower Frechet Distance (FD), 29% lower\nFrechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its\nsuperior audio quality and synchronization precision.", "AI": {"tldr": "TARO is a new framework for video-to-audio synthesis using flow-based transformers, featuring dynamic alignment (TRA) and onset-aware conditioning (OAC) for better fidelity and synchronization. It outperforms prior methods in audio quality and alignment.", "motivation": "To improve high-fidelity and temporally coherent video-to-audio synthesis by addressing synchronization and audio quality challenges.", "method": "Uses flow-based transformers with TRA for dynamic latent representation alignment and OAC for onset-aware synchronization.", "result": "Achieves 53% lower FD, 29% lower FAD, and 97.19% alignment accuracy on VGGSound and Landscape datasets.", "conclusion": "TARO sets a new benchmark for video-to-audio synthesis with superior performance in audio quality and synchronization."}}
{"id": "2507.01794", "pdf": "https://arxiv.org/pdf/2507.01794", "abs": "https://arxiv.org/abs/2507.01794", "authors": ["Carlo Alberto Barbano", "Benoit Dufumier", "Edouard Duchesnay", "Marco Grangetto", "Pietro Gori"], "title": "Robust brain age estimation from structural MRI with contrastive learning", "categories": ["eess.IV", "cs.CV", "68T07", "I.2.6"], "comment": "11 pages", "summary": "Estimating brain age from structural MRI has emerged as a powerful tool for\ncharacterizing normative and pathological aging. In this work, we explore\ncontrastive learning as a scalable and robust alternative to supervised\napproaches for brain age estimation. We introduce a novel contrastive loss\nfunction, $\\mathcal{L}^{exp}$, and evaluate it across multiple public\nneuroimaging datasets comprising over 20,000 scans. Our experiments reveal four\nkey findings. First, scaling pre-training on diverse, multi-site data\nconsistently improves generalization performance, cutting external mean\nabsolute error (MAE) nearly in half. Second, $\\mathcal{L}^{exp}$ is robust to\nsite-related confounds, maintaining low scanner-predictability as training size\nincreases. Third, contrastive models reliably capture accelerated aging in\npatients with cognitive impairment and Alzheimer's disease, as shown through\nbrain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike\nsupervised baselines, $\\mathcal{L}^{exp}$ maintains a strong correlation\nbetween brain age accuracy and downstream diagnostic performance, supporting\nits potential as a foundation model for neuroimaging. These results position\ncontrastive learning as a promising direction for building generalizable and\nclinically meaningful brain representations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.01018", "pdf": "https://arxiv.org/pdf/2507.01018", "abs": "https://arxiv.org/abs/2507.01018", "authors": ["Mohammed K. Alzaylaee"], "title": "A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart homes that integrate Internet of Things (IoT) devices face increasing\ncybersecurity risks, posing significant challenges to these environments. The\nstudy explores security threats in smart homes ecosystems, categorizing them\ninto vulnerabilities at the network layer, device level, and those from\ncloud-based and AI-driven systems. Research findings indicate that post-quantum\nencryption, coupled with AI-driven anomaly detection, is highly effective in\nenhancing security; however, computational resource demands present significant\nchallenges. Blockchain authentication together with zero-trust structures\nbuilds security resilience, although they need changes to existing\ninfrastructure. The specific security strategies show their effectiveness\nthrough ANOVA, Chi-square tests, and Monte Carlo simulations yet lack\nsufficient scalability according to the results. The research demonstrates the\nrequirement for improvement in cryptographic techniques, alongside AI-enhanced\nthreat detection and adaptive security models which must achieve a balance\nbetween performance and efficiency and real-time applicability within smart\nhome ecosystems.", "AI": {"tldr": "The paper examines cybersecurity risks in IoT-enabled smart homes, identifying vulnerabilities and evaluating solutions like post-quantum encryption, AI-driven anomaly detection, and blockchain authentication. While effective, these methods face scalability and resource challenges.", "motivation": "The increasing cybersecurity risks in smart homes due to IoT integration necessitate exploring robust security solutions to protect these environments.", "method": "The study categorizes threats into network, device, cloud, and AI-driven vulnerabilities, evaluating solutions like post-quantum encryption, AI anomaly detection, and blockchain authentication. Effectiveness is tested via ANOVA, Chi-square, and Monte Carlo simulations.", "result": "Post-quantum encryption and AI-driven anomaly detection improve security but face computational demands. Blockchain and zero-trust enhance resilience but require infrastructure changes. Methods lack scalability.", "conclusion": "The research highlights the need for improved cryptographic techniques, AI-enhanced threat detection, and adaptive security models balancing performance and real-time applicability in smart homes."}}
{"id": "2507.01039", "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "AI": {"tldr": "A reinforcement learning approach using PPO trains neuro-fuzzy controllers, outperforming DQN-based methods in stability and convergence.", "motivation": "To improve the training of explainable neuro-fuzzy controllers by replacing off-policy DQN with stable on-policy PPO.", "method": "Proximal Policy Optimization (PPO) is applied to train neuro-fuzzy controllers, evaluated in CartPole-v1 against ANFIS-DQN baselines.", "result": "PPO-trained agents achieved a mean return of 500 +/- 0, showing less variance and faster convergence than DQN methods.", "conclusion": "PPO is a promising method for training explainable neuro-fuzzy controllers in RL tasks."}}
{"id": "2507.01340", "pdf": "https://arxiv.org/pdf/2507.01340", "abs": "https://arxiv.org/abs/2507.01340", "authors": ["Cuong Le", "Huy-Phuong Le", "Duc Le", "Minh-Thien Duong", "Van-Binh Nguyen", "My-Ha Le"], "title": "Physics-informed Ground Reaction Dynamics from Human Motion Capture", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 4 tables, HSI 2025", "summary": "Body dynamics are crucial information for the analysis of human motions in\nimportant research fields, ranging from biomechanics, sports science to\ncomputer vision and graphics. Modern approaches collect the body dynamics,\nexternal reactive force specifically, via force plates, synchronizing with\nhuman motion capture data, and learn to estimate the dynamics from a black-box\ndeep learning model. Being specialized devices, force plates can only be\ninstalled in laboratory setups, imposing a significant limitation on the\nlearning of human dynamics. To this end, we propose a novel method for\nestimating human ground reaction dynamics directly from the more reliable\nmotion capture data with physics laws and computational simulation as\nconstrains. We introduce a highly accurate and robust method for computing\nground reaction forces from motion capture data using Euler's integration\nscheme and PD algorithm. The physics-based reactive forces are used to inform\nthe learning model about the physics-informed motion dynamics thus improving\nthe estimation accuracy. The proposed approach was tested on the GroundLink\ndataset, outperforming the baseline model on: 1) the ground reaction force\nestimation accuracy compared to the force plates measurement; and 2) our\nsimulated root trajectory precision. The implementation code is available at\nhttps://github.com/cuongle1206/Phys-GRD", "AI": {"tldr": "A novel method estimates human ground reaction dynamics from motion capture data using physics laws and computational simulation, outperforming baseline models in accuracy.", "motivation": "Force plates, used for measuring body dynamics, are limited to lab setups. The paper aims to overcome this by estimating dynamics directly from motion capture data.", "method": "The method uses Euler's integration and PD algorithm to compute ground reaction forces from motion capture data, incorporating physics laws for accuracy.", "result": "The approach outperforms baseline models in ground reaction force estimation accuracy and simulated root trajectory precision on the GroundLink dataset.", "conclusion": "The proposed physics-based method provides a reliable alternative to force plates, enhancing human motion analysis."}}
{"id": "2507.01449", "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "LogitSpec improves speculative decoding by using the last token's logit to predict and retrieve draft tokens, achieving faster LLM inference without training.", "motivation": "Retrieval-based speculative decoding often fails to find accurate draft tokens, limiting its effectiveness. LogitSpec addresses this by expanding the retrieval range.", "method": "LogitSpec uses the last token's logit to predict the next next token and retrieves references for both the next and next next tokens.", "result": "Achieves up to 2.61\u00d7 speedup and 3.28 mean accepted tokens per decoding step.", "conclusion": "LogitSpec is a training-free, plug-and-play solution that enhances speculative decoding efficiency."}}
{"id": "2505.08195", "pdf": "https://arxiv.org/pdf/2505.08195", "abs": "https://arxiv.org/abs/2505.08195", "authors": ["Jinming Hu", "Hassan Nawaz", "Yuting Rui", "Lijie Chi", "Arif Ullah", "Pavlo O. Dral"], "title": "Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations", "categories": ["physics.comp-ph", "cs.AI", "cs.LG", "cs.MA", "physics.chem-ph"], "comment": null, "summary": "We have developed Aitomia - a platform powered by AI to assist in performing\nAI-driven atomistic and quantum chemical (QC) simulations. This evolving\nintelligent assistant platform is equipped with chatbots and AI agents to help\nexperts and guide non-experts in setting up and running the atomistic\nsimulations, monitoring their computation status, analyzing the simulation\nresults, and summarizing them for the user in text and graphical forms. We\nachieve these goals by exploiting open-source large language models (LLMs,\noriginal and fine-tuned), rule-based agents, and a retrieval-augmented\ngeneration (RAG) system. Aitomia leverages the versatility of our MLatom\necosystem, supporting AI-enhanced computational chemistry tasks ranging from\nground- to excited-state calculations such as geometry optimizations,\nthermochemistry, and spectra calculations. Aitomia is the first intelligent\nassistant publicly accessible online on a cloud computing platform for\natomistic simulations of broad scope (Aitomistic Hub at\nhttps://aitomistic.xyz), while it may also be deployed locally as described at\nhttp://mlatom.com/aitomia. Aitomia is expected to lower the barrier to\nperforming atomistic simulations, democratizing simulations, and accelerating\nresearch and development in the relevant fields.", "AI": {"tldr": "Aitomia is an AI-powered platform for atomistic and quantum chemical simulations, assisting users in setup, monitoring, analysis, and summarization using LLMs, rule-based agents, and RAG.", "motivation": "To democratize atomistic simulations by lowering barriers for experts and non-experts, accelerating research in computational chemistry.", "method": "Utilizes open-source LLMs (original and fine-tuned), rule-based agents, and RAG, integrated with the MLatom ecosystem for diverse chemistry tasks.", "result": "Aitomia is the first publicly accessible intelligent assistant for broad-scope atomistic simulations, available online and locally.", "conclusion": "Aitomia aims to democratize simulations, making them more accessible and accelerating research in computational chemistry."}}
{"id": "2409.13514", "pdf": "https://arxiv.org/pdf/2409.13514", "abs": "https://arxiv.org/abs/2409.13514", "authors": ["Iuliia Thorbecke", "Esa\u00fa Villatoro-Tello", "Juan Zuluaga-Gomez", "Shashi Kumar", "Sergio Burdisso", "Pradeep Rangappa", "Andr\u00e9s Carofilis", "Srikanth Madikeri", "Petr Motlicek", "Karthik Pandia", "Kadri Hacio\u011flu", "Andreas Stolcke"], "title": "Unifying Global and Near-Context Biasing in a Single Trie Pass", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to TSD2025", "summary": "Despite the success of end-to-end automatic speech recognition (ASR) models,\nchallenges persist in recognizing rare, out-of-vocabulary words - including\nnamed entities (NE) - and in adapting to new domains using only text data. This\nwork presents a practical approach to address these challenges through an\nunexplored combination of an NE bias list and a word-level n-gram language\nmodel (LM). This solution balances simplicity and effectiveness, improving\nentities' recognition while maintaining or even enhancing overall ASR\nperformance. We efficiently integrate this enriched biasing method into a\ntransducer-based ASR system, enabling context adaptation with almost no\ncomputational overhead. We present our results on three datasets spanning four\nlanguages and compare them to state-of-the-art biasing strategies. We\ndemonstrate that the proposed combination of keyword biasing and n-gram LM\nimproves entity recognition by up to 32% relative and reduces overall WER by up\nto a 12% relative.", "AI": {"tldr": "The paper proposes a method combining an NE bias list and a word-level n-gram LM to improve rare word and named entity recognition in ASR, achieving significant performance gains.", "motivation": "Challenges in recognizing rare words and named entities in ASR, and adapting to new domains using only text data.", "method": "Integration of an NE bias list and a word-level n-gram LM into a transducer-based ASR system.", "result": "Improves entity recognition by up to 32% and reduces overall WER by up to 12%.", "conclusion": "The proposed method effectively balances simplicity and performance, enhancing ASR adaptability and accuracy."}}
{"id": "2504.12398", "pdf": "https://arxiv.org/pdf/2504.12398", "abs": "https://arxiv.org/abs/2504.12398", "authors": ["Woongji Kim", "Beomseok Oh", "Junsuk Rho", "Wonkyu Moon"], "title": "An accurate measurement of parametric array using a spurious sound filter topologically equivalent to a half-wavelength resonator", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "comment": "12 pages, 11 figures. Published in Applied Acoustics", "summary": "Parametric arrays (PA) offer exceptional directivity and compactness compared\nto conventional loudspeakers, facilitating various acoustic applications.\nHowever, accurate measurement of audio signals generated by PA remains\nchallenging due to spurious ultrasonic sounds arising from microphone\nnonlinearities. Existing filtering methods, including Helmholtz resonators,\nphononic crystals, polymer films, and grazing incidence techniques, exhibit\npractical constraints such as size limitations, fabrication complexity, or\ninsufficient attenuation. To address these issues, we propose and demonstrate a\nnovel acoustic filter based on the design of a half-wavelength resonator. The\ndeveloped filter exploits the nodal plane in acoustic pressure distribution,\neffectively minimizing microphone exposure to targeted ultrasonic frequencies.\nFabrication via stereolithography (SLA) 3D printing ensures high dimensional\naccuracy, which is crucial for high-frequency acoustic filters. Finite element\nmethod (FEM) simulations guided filter optimization for suppression frequencies\nat 40 kHz and 60 kHz, achieving high transmission loss (TL) around 60 dB.\nExperimental validations confirm the filter's superior performance in\nsignificantly reducing spurious acoustic signals, as reflected in frequency\nresponse, beam pattern, and propagation curve measurements. The proposed filter\nensures stable and precise acoustic characterization, independent of\nmeasurement distances and incidence angles. This new approach not only improves\nmeasurement accuracy but also enhances reliability and reproducibility in\nparametric array research and development.", "AI": {"tldr": "A novel half-wavelength resonator-based acoustic filter is proposed to address challenges in measuring audio signals from parametric arrays, achieving high attenuation of spurious ultrasonic sounds.", "motivation": "Accurate measurement of audio signals from parametric arrays is hindered by spurious ultrasonic sounds caused by microphone nonlinearities, with existing filtering methods having practical limitations.", "method": "The filter exploits the nodal plane in acoustic pressure distribution, fabricated via SLA 3D printing, and optimized using FEM simulations for frequencies at 40 kHz and 60 kHz.", "result": "The filter achieves high transmission loss (~60 dB), significantly reducing spurious signals, as validated by experiments in frequency response, beam pattern, and propagation curve measurements.", "conclusion": "The proposed filter enhances measurement accuracy, reliability, and reproducibility in parametric array research, independent of distance and angle."}}
{"id": "2507.01828", "pdf": "https://arxiv.org/pdf/2507.01828", "abs": "https://arxiv.org/abs/2507.01828", "authors": ["Tyler Ward", "Meredith K. Owen", "O'Kira Coleman", "Brian Noehren", "Abdullah-Al-Zubaer Imran"], "title": "Autoadaptive Medical Segment Anything Model", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures, 3 tables", "summary": "Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose ADA-SAM (automated,\ndomain-specific, and adaptive segment anything model), a novel multitask\nlearning framework for medical image segmentation that leverages class\nactivation maps from an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the Segment Anything\n(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient\nfeedback mechanism to create a learnable connection between the segmentation\nand classification branches by using the segmentation gradients to guide and\nimprove the classification predictions. We validate ADA-SAM on real-world\nclinical data collected during rehabilitation trials, and demonstrate that our\nproposed method outperforms both fully-supervised and semi-supervised baselines\nby double digits in limited label settings. Our code is available at:\nhttps://github.com/tbwa233/ADA-SAM.", "AI": {"tldr": "ADA-SAM is a multitask learning framework for medical image segmentation that combines class activation maps and a gradient feedback mechanism to improve accuracy in limited-label settings.", "motivation": "Manual annotation for medical image segmentation is costly and error-prone, necessitating annotation-efficient methods.", "method": "ADA-SAM uses class activation maps from an auxiliary classifier to guide a semi-supervised segmentation branch and employs a gradient feedback mechanism to link segmentation and classification.", "result": "ADA-SAM outperforms fully-supervised and semi-supervised baselines by double digits in limited-label settings.", "conclusion": "ADA-SAM provides an accurate, automatic, and annotation-efficient solution for medical image segmentation."}}
{"id": "2507.01025", "pdf": "https://arxiv.org/pdf/2507.01025", "abs": "https://arxiv.org/abs/2507.01025", "authors": ["Yutong Lu", "Dan Huang", "Pin Chen"], "title": "HPC-AI Coupling Methodology for Scientific Applications", "categories": ["cs.CE", "cs.AI", "physics.comp-ph"], "comment": "14 pages, 11 figures", "summary": "Artificial intelligence (AI) technologies have fundamentally transformed\nnumerical-based high-performance computing (HPC) applications with data-driven\napproaches and endeavored to address existing challenges, e.g. high\ncomputational intensity, in various scientific domains. In this study, we\nexplore the scenarios of coupling HPC and AI (HPC-AI) in the context of\nemerging scientific applications, presenting a novel methodology that\nincorporates three patterns of coupling: surrogate, directive, and coordinate.\nEach pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,\nand typical HPC-AI ensembles. Through case studies in materials science, we\ndemonstrate the application and effectiveness of these patterns. The study\nhighlights technical challenges, performance improvements, and implementation\ndetails, providing insight into promising perspectives of HPC-AI coupling. The\nproposed coupling patterns are applicable not only to materials science but\nalso to other scientific domains, offering valuable guidance for future HPC-AI\nensembles in scientific discovery.", "AI": {"tldr": "The paper explores coupling HPC and AI (HPC-AI) in scientific applications, introducing three coupling patterns (surrogate, directive, coordinate) and demonstrating their effectiveness in materials science.", "motivation": "To address challenges like high computational intensity in HPC applications by leveraging AI-driven approaches.", "method": "Proposes three coupling patterns (surrogate, directive, coordinate) and validates them through case studies in materials science.", "result": "Demonstrates performance improvements and applicability of HPC-AI coupling, with insights into technical challenges.", "conclusion": "The coupling patterns are broadly applicable beyond materials science, offering guidance for future HPC-AI integration in scientific discovery."}}
{"id": "2507.01040", "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers", "AI": {"tldr": "Clifford Neural Layers enhance PDE modeling using Clifford Algebra, optimizing 2/3D convolutional and multivector activation layers for CPU performance, achieving 30% faster inference than PyTorch.", "motivation": "To improve PDE modeling efficiency by integrating Clifford Algebra into neural networks and optimizing performance for CPU.", "method": "Optimized inference of 2/3D Clifford convolutional layers and multivector activation layers for CPU performance.", "result": "30% faster inference than standard PyTorch in large data and network sizes (>L2 cache).", "conclusion": "The implementation demonstrates significant speed improvements, with open-sourced code available for further use."}}
{"id": "2507.01342", "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "categories": ["cs.CV"], "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of\nseveral modules that transform raw sensor data into visually pleasing images in\na display color space. Among these, the auto white balance (AWB) module is\nessential for compensating for scene illumination. However, commercial AWB\nsystems often strive to compute aesthetic white-balance preferences rather than\naccurate neutral color correction. While learning-based methods have improved\nAWB accuracy, they typically struggle to generalize across different camera\nsensors -- an issue for smartphones with multiple cameras. Recent work has\nexplored cross-camera AWB, but most methods remain focused on achieving neutral\nwhite balance. In contrast, this paper is the first to address aesthetic\nconsistency by learning a post-illuminant-estimation mapping that transforms\nneutral illuminant corrections into aesthetically preferred corrections in a\ncamera-agnostic space. Once trained, our mapping can be applied after any\nneutral AWB module to enable consistent and stylized color rendering across\nunseen cameras. Our proposed model is lightweight -- containing only $\\sim$500\nparameters -- and runs in just 0.024 milliseconds on a typical flagship mobile\nCPU. Evaluated on a dataset of 771 smartphone images from three different\ncameras, our method achieves state-of-the-art performance while remaining fully\ncompatible with existing cross-camera AWB techniques, introducing minimal\ncomputational and memory overhead.", "AI": {"tldr": "The paper introduces a lightweight method for transforming neutral white balance corrections into aesthetically preferred ones, ensuring consistency across different cameras.", "motivation": "Commercial AWB systems prioritize aesthetics over accuracy, and learning-based methods struggle with generalization across camera sensors. This work addresses aesthetic consistency in AWB.", "method": "The authors propose a post-illuminant-estimation mapping that transforms neutral corrections into aesthetic ones in a camera-agnostic space. The model is lightweight (~500 parameters) and efficient.", "result": "The method achieves state-of-the-art performance on a dataset of 771 images from three cameras, with minimal computational overhead (0.024 ms runtime).", "conclusion": "The approach enables consistent and stylized color rendering across unseen cameras while remaining compatible with existing cross-camera AWB techniques."}}
{"id": "2507.01479", "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "The paper proposes using direct preference optimization (DPO) to personalize LLM-based automatic text simplification (ATS) for individuals with intellectual disabilities, incorporating their feedback for better alignment with their needs.", "motivation": "Existing LLM-based ATS systems lack personalization for target groups like individuals with intellectual disabilities, as they don't incorporate preference feedback during training.", "method": "Extends supervised fine-tuning (SFT) with DPO, post-training LLM-based ATS models using human feedback from the target group. Introduces a pipeline for personalized ATS development.", "result": "Demonstrates the importance of involving target group members in designing personalized AI accessibility solutions, improving alignment with human expectations.", "conclusion": "Highlights a step towards personalized inclusive AI systems by integrating feedback from both experts and target group individuals."}}
{"id": "2507.00631", "pdf": "https://arxiv.org/pdf/2507.00631", "abs": "https://arxiv.org/abs/2507.00631", "authors": ["David Shi", "Kevin Joo"], "title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "comment": "9 pages, 1 figure", "summary": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "AI": {"tldr": "A protocol ensures correctness in AI systems by using collateralized claims and recursive verification, where incorrect results are penalized and correct ones rewarded.", "motivation": "In dynamic, low-trust environments, ensuring correctness in autonomous AI agents is challenging without upfront specification or centralized oversight.", "method": "Tasks are published as intents; solvers compete to fulfill them, with correctness verified post hoc. Challengers can stake against results to trigger verification, penalizing incorrect agents and rewarding correct ones.", "result": "Correctness becomes the Nash equilibrium when incentives align across solvers, challengers, and verifiers.", "conclusion": "The protocol enforces correctness through decentralized verification and aligned incentives, making it robust in low-trust settings."}}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808", "abs": "https://arxiv.org/abs/2507.00808", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "title": "Multi-interaction TTS toward professional recording reproduction", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthesized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enables iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available:\nhttps://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/", "AI": {"tldr": "A TTS method with multi-step interaction allows iterative refinement of synthesized speech, emulating voice actor-director feedback.", "motivation": "Current TTS lacks fine-grained style refinement post-synthesis, often deviating from user intent.", "method": "Proposes a TTS model with multi-step interaction, modeling user-TTS feedback like actor-director dynamics.", "result": "Experiments show iterative style refinements align with user directions, proving multi-interaction capability.", "conclusion": "The method enables intuitive, rapid refinement of TTS outputs, addressing a gap in current synthesis systems."}}
{"id": "2507.01881", "pdf": "https://arxiv.org/pdf/2507.01881", "abs": "https://arxiv.org/abs/2507.01881", "authors": ["Niccol\u00f2 McConnell", "Pardeep Vasudev", "Daisuke Yamada", "Daryl Cheng", "Mehran Azimbagirad", "John McCabe", "Shahab Aslani", "Ahmed H. Shahin", "Yukun Zhou", "The SUMMIT Consortium", "Andre Altmann", "Yipeng Hu", "Paul Taylor", "Sam M. Janes", "Daniel C. Alexander", "Joseph Jacob"], "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.", "AI": {"tldr": "TANGERINE is an open-source, computationally efficient vision foundation model for LDCT analysis, enabling rapid fine-tuning for diverse lung disease tasks with minimal resources.", "motivation": "The shortage of radiologists for interpreting LDCT scans in lung cancer screening programs necessitates an accessible, scalable solution for early-stage disease detection.", "method": "TANGERINE uses a 3D masked autoencoder framework, pretrained via self-supervised learning on 98,000 LDCT scans, and fine-tuned for disease-specific tasks with limited data and computational resources.", "result": "It achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer, while generalizing well across clinical centers.", "conclusion": "TANGERINE's lightweight, open-source design supports integration into medical imaging tools, expanding lung cancer screening to comprehensive respiratory disease management."}}
{"id": "2507.01036", "pdf": "https://arxiv.org/pdf/2507.01036", "abs": "https://arxiv.org/abs/2507.01036", "authors": ["Seth Bulin"], "title": "Systemic Constraints of Undecidability", "categories": ["cs.FL", "cs.AI", "math.LO"], "comment": "Submitted version; includes appendices with formal definitions and\n  structural embeddings. Prepared in Nature Computational Science format.\n  Keywords: computability theory, undecidability, causal systems, structural\n  closure, recursion theory, Turing machines, hypercomputation,\n  metaundecidability, epistemic limits, consciousness, modeling limits", "summary": "This paper presents a theory of systemic undecidability, reframing\nincomputability as a structural property of systems rather than a localized\nfeature of specific functions or problems. We define a notion of causal\nembedding and prove a closure principle: any subsystem that participates\nfunctionally in the computation of an undecidable system inherits its\nundecidability. This result positions undecidability as a pervasive constraint\non prediction, modeling, and epistemic access in both natural and artificial\nsystems. Our framework disarms oracle mimicry and challenges the view that\ncomputational limits can be circumvented through architectural innovation. By\ngeneralizing classical results into a dynamic systems context, this work\naugments the logical trajectory of G\\\"odel, Turing, and Chaitin, offering a new\nperspective of the topology of computability and its interrelation to the\nboundaries of scientific knowledge.", "AI": {"tldr": "The paper reframes incomputability as a structural property of systems, proving that subsystems inheriting undecidability from larger systems constrain prediction and modeling.", "motivation": "To generalize incomputability beyond specific functions, positioning it as a pervasive constraint in natural and artificial systems.", "method": "Defines causal embedding and proves a closure principle where subsystems inherit undecidability from larger systems.", "result": "Undecidability is shown as a pervasive constraint, challenging the idea of circumventing computational limits through innovation.", "conclusion": "The work extends classical incomputability results, offering a new perspective on computability's topology and its limits in scientific knowledge."}}
{"id": "2507.01041", "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks.", "AI": {"tldr": "The paper proposes fast DAG-based and block-wise algorithms for optimal model splitting in split learning, reducing computational complexity and training delays.", "motivation": "Complex AI model architectures increase computational complexity for optimal model splitting in split learning, necessitating efficient solutions.", "method": "Represents AI models as DAGs, reformulates splitting as a minimum s-t cut problem, and introduces DAG-based and block-wise algorithms for optimal splitting.", "result": "The algorithms identify optimal splitting in milliseconds and reduce training delay by 24.62%-38.95% compared to benchmarks.", "conclusion": "The proposed algorithms efficiently solve the model splitting problem, enhancing computational efficiency and reducing training delays in dynamic edge networks."}}
{"id": "2507.01347", "pdf": "https://arxiv.org/pdf/2507.01347", "abs": "https://arxiv.org/abs/2507.01347", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Generalized Test-Time Augmentation (GTTA), a highly effective\nmethod for improving the performance of a trained model, which unlike other\nexisting Test-Time Augmentation approaches from the literature is general\nenough to be used off-the-shelf for many vision and non-vision tasks, such as\nclassification, regression, image segmentation and object detection. By\napplying a new general data transformation, that randomly perturbs multiple\ntimes the PCA subspace projection of a test input, GTTA forms robust ensembles\nat test time in which, due to sound statistical properties, the structural and\nsystematic noises in the initial input data is filtered out and final estimator\nerrors are reduced. Different from other existing methods, we also propose a\nfinal self-supervised learning stage in which the ensemble output, acting as an\nunsupervised teacher, is used to train the initial single student model, thus\nreducing significantly the test time computational cost, at no loss in\naccuracy. Our tests and comparisons to strong TTA approaches and SoTA models on\nvarious vision and non-vision well-known datasets and tasks, such as image\nclassification and segmentation, speech recognition and house price prediction,\nvalidate the generality of the proposed GTTA. Furthermore, we also prove its\neffectiveness on the more specific real-world task of salmon segmentation and\ndetection in low-visibility underwater videos, for which we introduce\nDeepSalmon, the largest dataset of its kind in the literature.", "AI": {"tldr": "GTTA is a versatile Test-Time Augmentation method for improving model performance across vision and non-vision tasks by perturbing PCA subspace projections and using self-supervised learning.", "motivation": "To create a general, off-the-shelf method for enhancing model performance across diverse tasks by addressing structural and systematic noise in data.", "method": "GTTA applies random perturbations to PCA subspace projections of test inputs, forms robust ensembles, and includes a self-supervised learning stage to refine the initial model.", "result": "GTTA outperforms existing TTA methods and SoTA models in tasks like image classification, segmentation, speech recognition, and house price prediction.", "conclusion": "GTTA is a highly effective, general-purpose method validated across various tasks, including specialized applications like salmon detection in underwater videos."}}
{"id": "2507.01541", "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "categories": ["cs.CL"], "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "A modular framework combining uncertainty modeling and fine-tuned LLMs improves OOS intent detection in task-oriented dialogue systems, balancing efficiency and performance.", "motivation": "OOS intent detection is crucial for robustness in TODS, especially with unseen or ambiguous queries.", "method": "Uses uncertainty estimation on in-scope intent classifier outputs, then triggers a fine-tuned LLM for high-uncertainty cases.", "result": "Achieves state-of-the-art performance on OOS benchmarks, including real-world TODS data.", "conclusion": "The framework effectively combines traditional methods with LLMs for efficient and accurate OOS detection."}}
{"id": "2507.01428", "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "DiffMark, a diffusion model-based watermarking framework, enhances robustness against Deepfake manipulations by integrating facial and watermark conditions, a cross information fusion module, and adversarial guidance.", "motivation": "Deepfakes threaten security and privacy, but existing watermarking methods lack robustness against such manipulations.", "method": "DiffMark modifies diffusion model training and sampling, using facial and watermark conditions, a CIF module, and adversarial guidance to generate robust watermarked images.", "result": "Experiments show DiffMark effectively resists typical Deepfake manipulations.", "conclusion": "DiffMark offers a robust solution for watermarking against Deepfakes, with potential for broader security applications."}}
{"id": "2507.01042", "pdf": "https://arxiv.org/pdf/2507.01042", "abs": "https://arxiv.org/abs/2507.01042", "authors": ["Harsh Joshi", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Niharika Jain", "Sarthak Jain", "Jiechao Gao", "Usman Naseem"], "title": "Can Argus Judge Them All? Comparing VLMs Across Domains", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their\nperformance consistency across tasks is underexamined. We benchmark CLIP, BLIP,\nand LXMERT across diverse datasets spanning retrieval, captioning, and\nreasoning. Our evaluation includes task accuracy, generation quality,\nefficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows\nstrongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT\nleads in structured reasoning. These results expose trade-offs between\ngeneralization and specialization, informing industrial deployment of VLMs and\nguiding development toward robust, task-flexible architectures.", "AI": {"tldr": "Benchmarking CLIP, BLIP, and LXMERT reveals trade-offs between generalization and specialization in Vision-Language Models (VLMs).", "motivation": "To evaluate the performance consistency of VLMs across diverse tasks and datasets.", "method": "Benchmarked CLIP, BLIP, and LXMERT on retrieval, captioning, and reasoning tasks, measuring accuracy, generation quality, efficiency, and Cross-Dataset Consistency (CDC).", "result": "CLIP generalizes best (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning.", "conclusion": "The study highlights trade-offs in VLM design, aiding industrial deployment and guiding development of robust, flexible architectures."}}
{"id": "2507.01043", "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon \u015awiderski", "Agnieszka Jastrz\u0119bska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability.", "AI": {"tldr": "A method for dynamically adjusting neural network architecture during training using Monte Carlo Tree Search, validated on visual and time series datasets.", "motivation": "To optimize neural network architecture dynamically during training, moving beyond fixed architectures.", "method": "Uses Monte Carlo Tree Search to simulate and compare candidate architecture changes, enabling dynamic growing and shrinking.", "result": "Effective in multivariate time series classification due to dynamic adaptability, with promising performance in visual tasks.", "conclusion": "The method is robust, adaptable, and validated across datasets, with provided Python code for reproducibility."}}
{"id": "2507.01351", "pdf": "https://arxiv.org/pdf/2507.01351", "abs": "https://arxiv.org/abs/2507.01351", "authors": ["Chaoxiang Cai", "Longrong Yang", "Kaibing Chen", "Fan Yang", "Xi Li"], "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "The mixture-of-experts (MoE), which replaces dense models with sparse\narchitectures, has gained attention in large vision-language models (LVLMs) for\nachieving comparable performance with fewer activated parameters. Existing MoE\nframeworks for LVLMs focus on token-to-expert routing (TER), encouraging\ndifferent experts to specialize in processing distinct tokens. However, these\nframeworks often rely on the load balancing mechanism, overlooking the inherent\ndistributional differences between vision and language. To this end, we propose\na Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,\ntackling two challenges: (1) Distribution-aware router for modality-specific\nrouting. We observe that language TER follows a uniform distribution, whereas\nvision TER exhibits a long-tailed distribution. This discrepancy necessitates\ndistinct routing strategies tailored to each modality. (2) Enhancing expert\nactivation for vision tail tokens. Recognizing the importance of vision tail\ntokens, we introduce an oversampling-like strategy by increasing the number of\nactivated experts for these tokens. Experiments on extensive benchmarks\nvalidate the effectiveness of our approach.", "AI": {"tldr": "The paper introduces LTDR, a Long-Tailed Distribution-aware Router for vision-language token-to-expert routing, addressing modality-specific distribution differences and enhancing expert activation for vision tail tokens.", "motivation": "Existing MoE frameworks for LVLMs overlook the distributional differences between vision and language tokens, leading to suboptimal routing.", "method": "Proposes LTDR with a distribution-aware router for modality-specific routing and an oversampling-like strategy for vision tail tokens.", "result": "Experiments on benchmarks show LTDR's effectiveness in handling vision-language TER.", "conclusion": "LTDR improves routing in MoE frameworks by addressing modality-specific distribution challenges and enhancing expert activation for vision tail tokens."}}
{"id": "2507.01543", "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "External information (Wikipedia, web search) degrades stance detection performance in large language models (LLMs), contrary to prior findings with BERT-based systems.", "motivation": "To evaluate the impact of external information on stance detection in LLMs, given its known benefits in BERT-based systems.", "method": "Systematic evaluation across eight LLMs and three datasets with 12 targets, using Wikipedia and web search external information.", "result": "External information degrades performance (up to 27.9% drop in macro F1), as LLMs align predictions with the provided information's stance/sentiment rather than ground truth.", "conclusion": "External information introduces biases in LLM-based stance classifiers, highlighting risks despite fine-tuning and prompting efforts."}}
{"id": "2507.01587", "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ICIP 2025, IEEE International Conference\n  on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive\nperformance; however, many lack the flexibility to adjust the denoising\nstrength based on the noise levels, camera settings, and user preferences. In\nthis paper, we introduce a new controllable denoising framework that adaptively\nremoves noise from images by utilizing information from camera parameters.\nSpecifically, we focus on ISO, shutter speed, and F-number, which are closely\nrelated to noise levels. We convert these selected parameters into a vector to\ncontrol and enhance the performance of the denoising network. Experimental\nresults show that our method seamlessly adds controllability to standard\ndenoising neural networks and improves their performance. Code is available at\nhttps://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "A controllable denoising framework adapts to noise levels using camera parameters (ISO, shutter speed, F-number) to enhance denoising performance.", "motivation": "Existing deep learning-based denoising methods lack flexibility in adjusting denoising strength based on noise levels, camera settings, and user preferences.", "method": "The framework converts camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising network.", "result": "The method adds controllability to standard denoising networks and improves their performance.", "conclusion": "The proposed framework effectively adapts denoising strength using camera parameters, offering improved flexibility and performance."}}
{"id": "2507.01045", "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "A cardiac sensing foundation model (CSFM) using transformers and masked pretraining is introduced, outperforming traditional methods in diverse cardiac signal tasks.", "motivation": "To address limitations of conventional deep learning in cardiac signal analysis, which lacks robustness and generalizability across diverse clinical settings.", "method": "CSFM leverages transformer architectures and generative masked pretraining on multi-modal data (ECG, PPG, and text reports) from 1.7 million individuals.", "result": "CSFM outperforms traditional approaches in diagnostic tasks, demographic recognition, vital sign measurement, and ECG question answering, showing robustness across lead configurations and sensor modalities.", "conclusion": "CSFM is a versatile and scalable solution for comprehensive cardiac monitoring, demonstrating superior performance and adaptability."}}
{"id": "2507.01047", "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems.", "AI": {"tldr": "The paper proposes a variational digital twin (VDT) framework to address gaps in current digital twin literature, offering real-time updates, uncertainty bounds, and efficiency. It demonstrates success in energy-sector applications.", "motivation": "Current digital twin literature lacks clear frameworks for real-time implementation, model uncertainty handling, and efficient information exchange between models and assets.", "method": "The VDT framework enhances standard neural architectures with a Bayesian output layer and a novel updating algorithm for real-time, uncertainty-aware performance.", "result": "VDT achieves high accuracy (R2 > 0.95) in energy-sector applications, reduces experiments and training time, and adapts to degraded instrumentation or dynamic conditions.", "conclusion": "The VDT framework transforms conventional models into dependable, uncertainty-aware digital twins, suitable for industrial and scientific energy systems."}}
{"id": "2507.01367", "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "PGA is a physical attack framework using 3D Gaussian Splatting for robust adversarial camouflage, addressing limitations of prior methods by improving cross-view robustness and adversarial effectiveness.", "motivation": "Prior camouflage-based physical attacks rely on mesh priors and simulators, which are time-consuming and lack real-world accuracy. They also struggle with multi-view robustness and sub-optimal solutions.", "method": "PGA uses 3D Gaussian Splatting (3DGS) for rapid, precise reconstruction with few images. It prevents occlusion among Gaussians and employs min-max optimization to adjust imaging backgrounds, filtering non-robust features.", "result": "Extensive experiments show PGA's effectiveness and superiority in adversarial attacks, outperforming prior methods in robustness and adversarial impact.", "conclusion": "PGA offers a practical, efficient solution for physical adversarial attacks, enhancing robustness across diverse viewpoints and environments."}}
{"id": "2507.01594", "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "LUSTER is an LLM-based unified system for task-oriented dialogue using end-to-end reinforcement learning, improving task success and emotional responsiveness.", "motivation": "Building effective and emotionally intelligent task-oriented dialogue systems remains challenging despite advances in LLMs.", "method": "Proposes LUSTER, combining LLMs with structured reward modeling for short-term (sentiment) and long-term (task success) rewards.", "result": "LUSTER enhances resilience and emotional responsiveness in task-oriented dialogue systems.", "conclusion": "Combining LLMs with structured rewards offers a practical path for next-gen conversational agents."}}
{"id": "2507.01608", "pdf": "https://arxiv.org/pdf/2507.01608", "abs": "https://arxiv.org/abs/2507.01608", "authors": ["Xu Zhang", "Ming Lu", "Yan Chen", "Zhan Ma"], "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference", "categories": ["cs.CV", "eess.IV"], "comment": "International Conference on Multimedia and Expo (ICME), 2025", "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.", "AI": {"tldr": "POLC introduces a perception-oriented latent coding method to enhance semantic richness in compressed images, reducing fine-tuning overhead compared to MSE-based methods.", "motivation": "MSE-optimized latent spaces lack semantic richness, hindering downstream tasks, and require intensive fine-tuning.", "method": "POLC enriches latent features semantically, enabling plug-and-play adapters for fine-tuning with fewer parameters.", "result": "POLC matches state-of-the-art generative coding in rate-perception and improves vision task performance with minimal overhead.", "conclusion": "POLC offers a computationally efficient solution for high-performance compressed domain semantic inference."}}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "A two-stage training framework for detoxifying text on social media achieves strong detoxification, semantic preservation, and generalization with reduced reliance on annotated data.", "motivation": "The urgent need for effective detoxification methods that preserve semantics and generalize well, addressing limitations of existing approaches.", "method": "A two-stage framework: supervised fine-tuning on filtered parallel data, followed by training with unlabeled toxic inputs and a custom reward model using Group Relative Policy Optimization.", "result": "State-of-the-art performance in detoxification, improved generalization, and reduced dependence on annotated data.", "conclusion": "The proposed method effectively mitigates trade-offs in detoxification, offering a robust and data-efficient solution."}}
{"id": "2507.01048", "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "categories": ["cs.LG"], "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "The paper introduces the updated 3W Dataset, a public resource for detecting undesirable events in oil wells using AI/ML, aiming to improve detection methods and outcomes.", "motivation": "Undesirable events in oil wells lead to economic, environmental, and human risks. Public datasets for AI/ML solutions were lacking, prompting the creation and collaborative development of the 3W Dataset.", "method": "The 3W Dataset is a labeled multivariate time series dataset, collaboratively developed and publicly released to support AI/ML research.", "result": "The dataset has become a foundational reference in the field, with structural updates and additional labeled data in its latest version.", "conclusion": "The updated 3W Dataset aims to enhance detection methodologies and enable timely corrective actions for undesirable events in oil wells."}}
{"id": "2507.01368", "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "AI": {"tldr": "The paper introduces Activation Reward Models (Activation RMs), a few-shot reward modeling method using activation steering to align LLMs/LMMs with human preferences, outperforming existing approaches and mitigating reward hacking.", "motivation": "Aligning LLMs/LMMs to human preferences is challenging; traditional reward modeling is inflexible and requires large datasets.", "method": "Activation RMs leverage activation steering for reward signals with minimal supervision, avoiding additional finetuning.", "result": "Activation RMs outperform existing few-shot methods and mitigate reward hacking, achieving state-of-the-art on the new PreferenceHack benchmark.", "conclusion": "Activation RMs offer a flexible, effective solution for aligning models to human preferences, with strong performance and safety benefits."}}
{"id": "2507.01627", "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "categories": ["cs.CL"], "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "A new dataset for chart question answering (CQA) is introduced, featuring real-world, multi-view charts with grounded questions, highlighting a performance gap in state-of-the-art models like GPT-4.1.", "motivation": "To address the lack of ecologically valid reasoning workflows in prior CQA benchmarks by using real-world data from visualization notebooks.", "method": "Construct a dataset from visualization notebooks with multi-view charts and natural language questions, then benchmark it using multimodal large language models like GPT-4.1.", "result": "GPT-4.1 achieves 69.3% accuracy, showing a significant performance gap in this authentic CQA setting.", "conclusion": "The dataset reveals challenges in CQA for state-of-the-art models, emphasizing the need for improved methods in real-world scenarios."}}
{"id": "2507.01712", "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "categories": ["cs.CV", "eess.IV", "stat.AP"], "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "AI": {"tldr": "Proposes a wavelet domain fingerprint for SPN extraction, improving accuracy and speed by avoiding inversion steps.", "motivation": "Enhance camera fingerprint detection for source identification and image forensics by simplifying wavelet-based SPN extraction.", "method": "Modifies wavelet-based SPN extraction by introducing wavelet domain fingerprints, eliminating inversion steps.", "result": "Higher detection accuracy and significantly improved processing speed on real-world datasets.", "conclusion": "The wavelet domain fingerprint method streamlines SPN extraction and comparison, offering better performance."}}
{"id": "2507.01051", "pdf": "https://arxiv.org/pdf/2507.01051", "abs": "https://arxiv.org/abs/2507.01051", "authors": ["Giada Pistilli", "Bruna Trevelin"], "title": "Can AI be Consentful?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The evolution of generative AI systems exposes the challenges of traditional\nlegal and ethical frameworks built around consent. This chapter examines how\nthe conventional notion of consent, while fundamental to data protection and\nprivacy rights, proves insufficient in addressing the implications of\nAI-generated content derived from personal data. Through legal and ethical\nanalysis, we show that while individuals can consent to the initial use of\ntheir data for AI training, they cannot meaningfully consent to the numerous\npotential outputs their data might enable or the extent to which the output is\nused or distributed. We identify three fundamental challenges: the scope\nproblem, the temporality problem, and the autonomy trap, which collectively\ncreate what we term a ''consent gap'' in AI systems and their surrounding\necosystem. We argue that current legal frameworks inadequately address these\nemerging challenges, particularly regarding individual autonomy, identity\nrights, and social responsibility, especially in cases where AI-generated\ncontent creates new forms of personal representation beyond the scope of the\noriginal consent. By examining how these consent limitations intersect with\nbroader principles of responsible AI (including fairness, transparency,\naccountability, and autonomy) we demonstrate the need to evolve ethical and\nlegal approaches to consent.", "AI": {"tldr": "The paper highlights the inadequacy of traditional consent frameworks in addressing AI-generated content, identifying a 'consent gap' due to scope, temporality, and autonomy challenges.", "motivation": "To address the limitations of current consent models in AI systems, especially regarding personal data and AI-generated outputs.", "method": "Legal and ethical analysis of consent frameworks in AI, identifying three key challenges: scope, temporality, and autonomy.", "result": "Reveals a 'consent gap' where traditional frameworks fail to cover AI-generated content, impacting autonomy and identity rights.", "conclusion": "Calls for evolving ethical and legal approaches to consent to better align with responsible AI principles."}}
{"id": "2507.01052", "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond.", "AI": {"tldr": "A novel energy functional for long-sequence memory is introduced, leveraging dense Hopfield networks with exponential storage capacity and temporal dependencies for efficient sequential retrieval.", "motivation": "To address limitations of transformers in long-context tasks by enhancing memory and temporal dependency handling.", "method": "Proposes a temporal kernel $K(m, k)$ within dense Hopfield networks to incorporate temporal dependencies for sequential retrieval.", "result": "Successful storage and retrieval of movie frames, demonstrating high-dimensional pattern handling and sequential efficiency.", "conclusion": "The model improves long-sequence tasks, with applications in NLP, forecasting, and time-series data, offering a promising alternative to transformers."}}
{"id": "2507.01372", "pdf": "https://arxiv.org/pdf/2507.01372", "abs": "https://arxiv.org/abs/2507.01372", "authors": ["Max Hamilton", "Jinlin Lai", "Wenlong Zhao", "Subhransu Maji", "Daniel Sheldon"], "title": "Active Measurement: Efficient Estimation at Scale", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "AI has the potential to transform scientific discovery by analyzing vast\ndatasets with little human effort. However, current workflows often do not\nprovide the accuracy or statistical guarantees that are needed. We introduce\nactive measurement, a human-in-the-loop AI framework for scientific\nmeasurement. An AI model is used to predict measurements for individual units,\nwhich are then sampled for human labeling using importance sampling. With each\nnew set of human labels, the AI model is improved and an unbiased Monte Carlo\nestimate of the total measurement is refined. Active measurement can provide\nprecise estimates even with an imperfect AI model, and requires little human\neffort when the AI model is very accurate. We derive novel estimators,\nweighting schemes, and confidence intervals, and show that active measurement\nreduces estimation error compared to alternatives in several measurement tasks.", "AI": {"tldr": "Active measurement is a human-in-the-loop AI framework for scientific measurement, combining AI predictions with human labeling to improve accuracy and reduce effort.", "motivation": "Current AI workflows lack accuracy and statistical guarantees for scientific discovery.", "method": "Uses AI to predict measurements, samples for human labeling via importance sampling, and refines estimates iteratively.", "result": "Provides precise estimates with minimal human effort, outperforming alternatives in error reduction.", "conclusion": "Active measurement enhances scientific measurement by balancing AI efficiency and human accuracy."}}
{"id": "2507.01633", "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "The paper compares global pointwise scores and pairwise comparisons for evaluating NLP models, finding global scores reliable for overall rankings but weak for rare errors, while pairwise comparisons excel in identifying strong models with lower global scores.", "motivation": "To evaluate the effectiveness of global scores versus pairwise comparisons in NLP model benchmarking, aiding decision-making for model evaluation strategies.", "method": "Computational experiments on synthetic and real-world datasets using global metrics and the Bradley-Terry model for pairwise comparisons.", "result": "Global scores provide reliable overall rankings but underestimate models with rare errors. Pairwise comparisons identify strong models with lower global scores but require more comparisons for convergence.", "conclusion": "Both evaluation strategies have trade-offs; global scores suit overall rankings, while pairwise comparisons are better for nuanced model strengths, especially in text generation."}}
{"id": "2403.08700", "pdf": "https://arxiv.org/pdf/2403.08700", "abs": "https://arxiv.org/abs/2403.08700", "authors": ["Paraskevas Pegios", "Manxi Lin", "Nina Weng", "Morten Bo S\u00f8ndergaard Svendsen", "Zahra Bashir", "Siavash Bigdeli", "Anders Nymark Christensen", "Martin Tolsgaard", "Aasa Feragen"], "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment", "categories": ["eess.IV", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, acquiring high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or fetus dynamics. In this work, we explore diffusion-based\ncounterfactual explainable AI to generate realistic, high-quality standard\nplanes from low-quality non-standard ones. Through quantitative and qualitative\nevaluation, we demonstrate the effectiveness of our approach in generating\nplausible counterfactuals of increased quality. This shows future promise for\nenhancing training of clinicians by providing visual feedback and potentially\nimproving standard plane quality and acquisition for downstream diagnosis and\nmonitoring.", "AI": {"tldr": "The paper proposes a diffusion-based AI method to enhance low-quality obstetric ultrasound images into high-quality standard planes, aiding diagnosis and clinician training.", "motivation": "High-quality obstetric ultrasound images are essential for fetal health diagnosis but are hard to acquire due to variability in sonographer skill and external factors like maternal BMI.", "method": "The authors use diffusion-based counterfactual explainable AI to transform low-quality non-standard ultrasound images into realistic high-quality standard planes.", "result": "The approach effectively generates plausible high-quality counterfactuals, validated through quantitative and qualitative evaluation.", "conclusion": "The method holds promise for improving clinician training and enhancing standard plane acquisition for better diagnosis and monitoring."}}
{"id": "2507.01053", "pdf": "https://arxiv.org/pdf/2507.01053", "abs": "https://arxiv.org/abs/2507.01053", "authors": ["Rafi Al Attrach", "Pedro Moreira", "Rajna Fani", "Renato Umeton", "Leo Anthony Celi"], "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis", "categories": ["cs.IR", "cs.AI", "cs.DB", "68T50, 68P15", "H.2.3; I.2.7; J.3"], "comment": "10 pages, 4 figures", "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.", "AI": {"tldr": "M3 simplifies querying MIMIC-IV, a large clinical dataset, by translating natural language questions into SQL, reducing technical barriers for researchers.", "motivation": "The complexity of large clinical datasets like MIMIC-IV hinders their effective use due to technical and clinical knowledge requirements.", "method": "M3 retrieves MIMIC-IV, uses SQLite or BigQuery, and employs the Model Context Protocol (MCP) to translate natural language queries into SQL.", "result": "Researchers can perform nuanced cohort analyses in minutes instead of hours, with verifiable and reproducible results.", "conclusion": "M3 democratizes access to clinical data, accelerating research by making it easier to extract actionable insights."}}
{"id": "2507.01054", "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "AI": {"tldr": "A scalable multimodal framework is proposed for materials discovery, using elemental composition and XRD without crystal structure input, achieving faster convergence and improved accuracy.", "motivation": "Overcome the impracticality of structure-based models in real-world applications where atomic structures are often unknown or hard to obtain.", "method": "Integrates modality-specific encoders with a cross-attention fusion module, trained on the Alexandria dataset. Uses masked XRD modeling and contrastive alignment for self-supervised pretraining.", "result": "Pretraining yields faster convergence (4.2x speedup), improved accuracy, and better representation quality. Multimodal performance scales better with dataset size than unimodal baselines.", "conclusion": "The framework establishes a path toward structure-free, experimentally grounded foundation models for materials science."}}
{"id": "2507.01384", "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "categories": ["cs.CV"], "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "The paper proposes MUG, an audio-visual Mamba network with pseudo-label augmentation, to improve segment-level and event-level predictions in weakly-supervised AVVP tasks.", "motivation": "Existing methods struggle with improving both segment-level and event-level predictions due to weak supervision and model limitations.", "method": "Uses pseudo-labels for data augmentation and an AV-Mamba network for feature processing to enhance segment perception and reduce noise.", "result": "MUG achieves state-of-the-art results on the LLP dataset, with gains of 2.1% and 1.2% in visual and audio segment-level metrics.", "conclusion": "MUG effectively addresses the limitations of weakly-supervised AVVP, improving performance through pseudo-label augmentation and the AV-Mamba network."}}
{"id": "2507.01645", "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "categories": ["cs.CL"], "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "The paper explores the transferability of pre-trained language models to low-resource Indonesian local languages for sentiment analysis, comparing zero-shot and adapter-based transfer methods. Performance varies by language group, with MAD-X improving results, and model exposure to the language being the key predictor of success.", "motivation": "To understand how pre-trained language models perform on low-resource Indonesian local languages and identify factors influencing transferability.", "method": "Evaluated zero-shot and adapter-based transfer (MAD-X) on ten local languages using monolingual and multilingual models (Indonesian BERT, mBERT, XLM-R). Languages were grouped by pre-training exposure: seen, partially seen, and unseen.", "result": "Multilingual models perform best on seen languages, moderately on partially seen, and poorly on unseen. MAD-X improves performance, especially for seen and partially seen languages. Model exposure to the language is the strongest predictor of transfer success.", "conclusion": "Transfer success depends on prior model exposure to the language. MAD-X is effective for seen and partially seen languages, while tokenization factors like subword fragmentation have limited explanatory power."}}
{"id": "2408.11787", "pdf": "https://arxiv.org/pdf/2408.11787", "abs": "https://arxiv.org/abs/2408.11787", "authors": ["Zhenye Lou", "Qing Xu", "Zekun Jiang", "Xiangjian He", "Zhen Chen", "Yi Wang", "Chenxin Li", "Maggie M. He", "Wenting Duan"], "title": "NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Domain-generalized nuclei segmentation refers to the generalizability of\nmodels to unseen domains based on knowledge learned from source domains and is\nchallenged by various image conditions, cell types, and stain strategies.\nRecently, the Segment Anything Model (SAM) has made great success in universal\nimage segmentation by interactive prompt modes (e.g., point and box). Despite\nits strengths, the original SAM presents limited adaptation to medical images.\nMoreover, SAM requires providing manual bounding box prompts for each object to\nproduce satisfactory segmentation masks, so it is laborious in nuclei\nsegmentation scenarios. To address these limitations, we propose a\ndomain-generalizable framework for nuclei image segmentation, abbreviated to\nNuSegDG. Specifically, we first devise a Heterogeneous Space Adapter\n(HS-Adapter) to learn multi-dimensional feature representations of different\nnuclei domains by injecting a small number of trainable parameters into the\nimage encoder of SAM. To alleviate the labor-intensive requirement of manual\nprompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to\ngenerate density maps driven by a single point, which guides segmentation\npredictions by mixing position prompts and semantic prompts. Furthermore, we\npresent a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic\nmasks to instance maps without the manual demand for morphological shape\nrefinement. Based on our experimental evaluations, the proposed NuSegDG\ndemonstrates state-of-the-art performance in nuclei instance segmentation,\nexhibiting superior domain generalization capabilities. The source code is\navailable at https://github.com/xq141839/NuSegDG.", "AI": {"tldr": "NuSegDG is a domain-generalizable framework for nuclei segmentation, improving SAM's adaptation to medical images by reducing manual prompts and enhancing feature learning.", "motivation": "Addressing SAM's limited adaptation to medical images and labor-intensive manual prompts in nuclei segmentation.", "method": "Proposes HS-Adapter for multi-dimensional feature learning, GKP-Encoder for automated prompts, and TSM-Decoder for instance segmentation.", "result": "NuSegDG achieves state-of-the-art performance in nuclei instance segmentation with superior domain generalization.", "conclusion": "NuSegDG effectively generalizes across domains and reduces manual intervention, advancing nuclei segmentation."}}
{"id": "2507.01058", "pdf": "https://arxiv.org/pdf/2507.01058", "abs": "https://arxiv.org/abs/2507.01058", "authors": ["Puspendu Banerjee", "Aritra Mazumdar", "Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "title": "A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "12 pages, 6 figures", "summary": "The judiciary, as one of democracy's three pillars, is dealing with a rising\namount of legal issues, needing careful use of judicial resources. This\nresearch presents a complex framework that leverages Data Science\nmethodologies, notably Large Language Models (LLM) and Retrieval-Augmented\nGeneration (RAG) techniques, to improve the efficiency of analyzing Calcutta\nHigh Court verdicts. Our framework focuses on two key aspects: first, the\ncreation of a robust summarization mechanism that distills complex legal texts\ninto concise and coherent summaries; and second, the development of an\nintelligent system for retrieving similar cases, which will assist legal\nprofessionals in research and decision making. By fine-tuning the Pegasus model\nusing case head note summaries, we achieve significant improvements in the\nsummarization of legal cases. Our two-step summarizing technique preserves\ncrucial legal contexts, allowing for the production of a comprehensive vector\ndatabase for RAG. The RAG-powered framework efficiently retrieves similar cases\nin response to user queries, offering thorough overviews and summaries. This\ntechnique not only improves legal research efficiency, but it also helps legal\nprofessionals and students easily acquire and grasp key legal information,\nbenefiting the overall legal scenario.", "AI": {"tldr": "A framework using LLMs and RAG improves legal case summarization and retrieval for Calcutta High Court verdicts, enhancing efficiency for legal professionals.", "motivation": "The judiciary faces increasing legal issues, requiring efficient resource use. This research aims to streamline legal analysis and research.", "method": "The framework fine-tunes the Pegasus model for summarization and uses RAG for retrieving similar cases, creating a vector database.", "result": "Improved summarization and efficient case retrieval, aiding legal professionals in research and decision-making.", "conclusion": "The framework enhances legal research efficiency and accessibility, benefiting the legal community."}}
{"id": "2507.01056", "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "categories": ["cs.LG"], "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "The study examines how flooding accelerates pavement roughness (measured by IRI) using 20 years of TxDOT data and XAI techniques like SHAP and LIME. Results show faster deterioration in flooded areas, urging proactive mitigation strategies.", "motivation": "Flooding causes immediate and long-term damage to pavements, but its specific impact on roughness (IRI) is not well quantified.", "method": "Analyzed 20 years of pavement condition data from TxDOT's PMIS, integrated with flood event data. Used statistical analysis and XAI (SHAP, LIME) to assess flood impact.", "result": "Flooded pavements show a faster increase in roughness (IRI) compared to non-flooded sections.", "conclusion": "Proactive flood mitigation (e.g., better drainage, flood-resistant materials) is needed to enhance pavement resilience in flood-prone areas."}}
{"id": "2507.01390", "pdf": "https://arxiv.org/pdf/2507.01390", "abs": "https://arxiv.org/abs/2507.01390", "authors": ["Shuai Tan", "Bill Gong", "Bin Ji", "Ye Pan"], "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases", "categories": ["cs.CV"], "comment": null, "summary": "Talking head generation is gaining significant importance across various\ndomains, with a growing demand for high-quality rendering. However, existing\nmethods often suffer from identity leakage (IL) and rendering artifacts (RA),\nparticularly in extreme cases. Through an in-depth analysis of previous\napproaches, we identify two key insights: (1) IL arises from identity\ninformation embedded within motion features, and (2) this identity information\ncan be leveraged to address RA. Building on these findings, this paper\nintroduces FixTalk, a novel framework designed to simultaneously resolve both\nissues for high-quality talking head generation. Firstly, we propose an\nEnhanced Motion Indicator (EMI) to effectively decouple identity information\nfrom motion features, mitigating the impact of IL on generated talking heads.\nTo address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes\nthe leaked identity information to supplement missing details, thus fixing the\nartifacts. Extensive experiments demonstrate that FixTalk effectively mitigates\nIL and RA, achieving superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FixTalk is a novel framework addressing identity leakage and rendering artifacts in talking head generation by decoupling identity from motion features and leveraging leaked identity for detail enhancement.", "motivation": "Existing methods for talking head generation suffer from identity leakage and rendering artifacts, especially in extreme cases, prompting the need for a solution that simultaneously resolves both issues.", "method": "FixTalk introduces an Enhanced Motion Indicator (EMI) to decouple identity from motion features and an Enhanced Detail Indicator (EDI) to use leaked identity for artifact correction.", "result": "Experiments show FixTalk effectively mitigates identity leakage and rendering artifacts, outperforming state-of-the-art methods.", "conclusion": "FixTalk provides a robust solution for high-quality talking head generation by addressing key challenges of identity leakage and rendering artifacts."}}
{"id": "2507.01702", "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "AdamMeme is a flexible, agent-based framework for evaluating multimodal LLMs' understanding of harmful memes, addressing limitations of static benchmarks by dynamically updating meme data.", "motivation": "Existing benchmarks for harmful meme understanding are static and lack adaptability to evolving online memes, necessitating a more dynamic evaluation approach.", "method": "Proposes AdamMeme, a multi-agent framework that iteratively updates meme data with challenging samples to probe mLLMs' reasoning on harmfulness.", "result": "AdamMeme systematically identifies performance variations and specific weaknesses in mLLMs' interpretation of harmful memes.", "conclusion": "AdamMeme offers a comprehensive, adaptive evaluation method for mLLMs, providing fine-grained insights into their limitations in harmful meme understanding."}}
{"id": "2502.13998", "pdf": "https://arxiv.org/pdf/2502.13998", "abs": "https://arxiv.org/abs/2502.13998", "authors": ["Hengyue Liang", "Taihui Li", "Ju Sun"], "title": "A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior", "categories": ["eess.IV", "cs.AI", "cs.CR", "cs.CV"], "comment": "Pulished in Transaction of Machine Learning Research (TMLR):\n  https://openreview.net/forum?id=g85Vxlrq0O", "summary": "Image watermarks have been considered a promising technique to help detect\nAI-generated content, which can be used to protect copyright or prevent fake\nimage abuse. In this work, we present a black-box method for removing invisible\nimage watermarks, without the need of any dataset of watermarked images or any\nknowledge about the watermark system. Our approach is simple to implement:\ngiven a single watermarked image, we regress it by deep image prior (DIP). We\nshow that from the intermediate steps of DIP one can reliably find an evasion\nimage that can remove invisible watermarks while preserving high image quality.\nDue to its unique working mechanism and practical effectiveness, we advocate\nincluding DIP as a baseline invasion method for benchmarking the robustness of\nwatermarking systems. Finally, by showing the limited ability of DIP and other\nexisting black-box methods in evading training-based visible watermarks, we\ndiscuss the positive implications on the practical use of training-based\nvisible watermarks to prevent misinformation abuse.", "AI": {"tldr": "A black-box method using deep image prior (DIP) removes invisible watermarks without datasets or system knowledge, advocating DIP as a benchmark for watermark robustness. Training-based visible watermarks show promise against misinformation.", "motivation": "To address the challenge of detecting and removing AI-generated content watermarks, especially for copyright protection and preventing fake image abuse.", "method": "Uses deep image prior (DIP) to regress a single watermarked image, finding an evasion image from intermediate DIP steps.", "result": "DIP effectively removes invisible watermarks while maintaining image quality, but struggles with training-based visible watermarks.", "conclusion": "DIP is a practical baseline for watermark robustness testing, and training-based visible watermarks are promising for preventing misinformation."}}
{"id": "2507.01061", "pdf": "https://arxiv.org/pdf/2507.01061", "abs": "https://arxiv.org/abs/2507.01061", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Yulei Ye", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "James Evan"], "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "18 pages, 5figures", "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ...", "AI": {"tldr": "Epitome is an open experimental platform integrating LLMs into social science research, enabling complex experiments on human-AI interactions and societal impacts.", "motivation": "To bridge AI and social science by studying the real-world impacts of AI on individuals, organizations, and society.", "method": "Epitome offers a one-stop solution with seven core modules, embedding social science experimental logic into human-computer interaction environments.", "result": "The platform successfully replicated three seminal social science experiments, demonstrating its ability to streamline designs and produce robust results.", "conclusion": "Epitome enhances interdisciplinary research on AI's societal implications, with potential applications in policy-making and beyond."}}
{"id": "2507.01057", "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached.", "AI": {"tldr": "An intelligent optimization system using deep CNN for mesh quality, featuring Loop2Net and loss functions to predict and optimize mesh generation.", "motivation": "To improve mesh generation and optimization for given wing coordinates using deep learning techniques.", "method": "Uses a deep convolutional neural network (Loop2Net) with two key loss functions and penalties for training and optimization.", "result": "Achieves effective mesh generation and optimization through the proposed system.", "conclusion": "The system successfully meets the goal of mesh generation with continuous optimization."}}
{"id": "2507.01397", "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "The paper proposes a coherent method for predicting HD map elements (lane segments, topology, and road boundaries) using prior SD map data, enhancing training stability and performance with hybrid encodings and temporal consistency.", "motivation": "Autonomous cars rely on HD maps, but online HD map construction is complex. The paper aims to address this by leveraging prior SD map data for coherent prediction.", "method": "A network architecture uses hybrid lane segment encodings (prior info + denoising) and past frames for temporal consistency to predict HD map elements.", "result": "The approach outperforms previous methods significantly, demonstrating the effectiveness of the modeling scheme.", "conclusion": "The proposed method successfully addresses the challenge of coherent HD map construction, leveraging prior data and temporal consistency for improved performance."}}
{"id": "2507.01715", "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "categories": ["cs.CL"], "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "The paper introduces StereoBias, a dataset for detecting bias and stereotypes in language models, showing that joint training improves bias detection.", "motivation": "Bias and stereotypes in language models can cause harm, especially in sensitive applications like content moderation and decision-making.", "method": "The study uses StereoBias dataset and compares encoder-only and fine-tuned decoder-only models (using QLoRA) for bias and stereotype detection. Joint training is explored.", "result": "Joint training on bias and stereotype detection significantly improves bias detection. Decoder-only models show competitive results.", "conclusion": "Leveraging stereotype information enhances fairness and effectiveness in AI systems, with joint training being key."}}
{"id": "2503.03327", "pdf": "https://arxiv.org/pdf/2503.03327", "abs": "https://arxiv.org/abs/2503.03327", "authors": ["Saqib Qamar", "Syed Furqan Qadri", "Roobaea Alroobaea", "Goram Mufarah M Alshmrani", "Richard Jiang"], "title": "ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Melanoma is a malignant tumor that originates from skin cell lesions.\nAccurate and efficient segmentation of skin lesions is essential for\nquantitative analysis but remains a challenge due to blurred lesion boundaries,\ngradual color changes, and irregular shapes. To address this, we propose\nScaleFusionNet, a hybrid model that integrates a Cross-Attention Transformer\nModule (CATM) and adaptive fusion block (AFB) to enhance feature extraction and\nfusion by capturing both local and global features. We introduce CATM, which\nutilizes Swin transformer blocks and Cross Attention Fusion (CAF) to adaptively\nrefine feature fusion and reduce semantic gaps in the encoder-decoder to\nimprove segmentation accuracy. Additionally, the AFB uses Swin\nTransformer-based attention and deformable convolution-based adaptive feature\nextraction to help the model gather local and global contextual information\nthrough parallel pathways. This enhancement refines the lesion boundaries and\npreserves fine-grained details. ScaleFusionNet achieves Dice scores of 92.94\\%\nand 91.80\\% on the ISIC-2016 and ISIC-2018 datasets, respectively,\ndemonstrating its effectiveness in skin lesion analysis. Simultaneously,\nindependent validation experiments were conducted on the PH$^2$ dataset using\nthe pretrained model weights. The results show that ScaleFusionNet demonstrates\nsignificant performance improvements compared with other state-of-the-art\nmethods. Our code implementation is publicly available at GitHub.", "AI": {"tldr": "ScaleFusionNet, a hybrid model combining Cross-Attention Transformer Module and adaptive fusion block, improves skin lesion segmentation by capturing local and global features, achieving high Dice scores on ISIC datasets.", "motivation": "Accurate skin lesion segmentation is challenging due to blurred boundaries and irregular shapes, necessitating advanced methods for feature extraction and fusion.", "method": "Proposes ScaleFusionNet with Cross-Attention Transformer Module (CATM) and adaptive fusion block (AFB) to refine feature fusion and reduce semantic gaps, using Swin transformer and deformable convolution.", "result": "Achieves Dice scores of 92.94% (ISIC-2016) and 91.80% (ISIC-2018), outperforming state-of-the-art methods on PH\u00b2 dataset.", "conclusion": "ScaleFusionNet effectively enhances segmentation accuracy and boundary refinement, validated by superior performance on benchmark datasets."}}
{"id": "2507.01062", "pdf": "https://arxiv.org/pdf/2507.01062", "abs": "https://arxiv.org/abs/2507.01062", "authors": ["Seyma Yaman Kayadibi"], "title": "Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review", "categories": ["cs.CY", "cs.AI", "62P25", "K.3.1; H.5.2"], "comment": "35 pages, 4 figures. All figures are image-based: one Python code\n  screenshot, one regression model output, one success score distribution\n  chart, and one PRISMA diagram. This article presents a standalone segment\n  from the author's master's thesis at Victoria University", "summary": "The exponential development of generative artificial intelligence (GenAI)\ntechnologies like ChatGPT has raised increasing curiosity about their use in\nhigher education, specifically with respect to how students view them, make use\nof them, and the implications for learning outcomes. This paper employs a\nhybrid methodological approach involving a systematic literature review and\nsimulation-based modeling to explore student perceptions of GenAI use in the\ncontext of higher education. A total of nineteen empirical articles from 2023\nthrough 2025 were selected from the PRISMA-based search targeting the Scopus\ndatabase. Synthesis of emerging patterns from the literature was achieved by\nthematic categorization. Six of these had enough quantitative information,\ni.e., item-level means and standard deviations, to permit probabilistic\nmodeling. One dataset, from the resulting subset, was itself selected as a\nrepresentative case with which to illustrate inverse-variance weighting by\nMonte Carlo simulation, by virtue of its well-designed Likert scale format and\nthematic alignment with the use of computing systems by the researcher.\n  The simulation provided a composite \"Success Score\" forecasting the strength\nof the relationship between student perceptions and learning achievements.\nFindings reveal that attitude factors concerned with usability and real-world\nusefulness are significantly better predictors of positive learning achievement\nthan affective or trust-based factors. Such an interdisciplinary perspective\nprovides a unique means of linking thematic results with predictive modelling,\nresonating with longstanding controversies about the proper use of GenAI tools\nwithin the university.", "AI": {"tldr": "The paper explores student perceptions of GenAI in higher education using a hybrid method of literature review and simulation modeling, finding usability and usefulness as key predictors of learning success.", "motivation": "To understand how students perceive and use GenAI in higher education and its impact on learning outcomes.", "method": "Combines systematic literature review (19 empirical articles) with simulation-based modeling (Monte Carlo) to analyze student perceptions.", "result": "Usability and real-world usefulness of GenAI are stronger predictors of learning success than affective or trust factors.", "conclusion": "The study bridges thematic insights with predictive modeling, addressing debates on GenAI's role in education."}}
{"id": "2507.01067", "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "The paper explores optimizing a foundational time series model for forecasting rare, spiky events in production outages, comparing it with classical stochastic models and achieving less than 6% error in year-long outage predictions.", "motivation": "To address the challenge of forecasting rare, spiky events in high-performance ML services, where foundational models haven't been applied yet.", "method": "Optimize a state-of-the-art foundational model and compare its performance with classical stochastic models (e.g., moving average, autoregressive) for sporadic events.", "result": "The foundational model outperforms stochastic models, achieving less than 6% error in estimating year-long outage statistics for specific root causes.", "conclusion": "Foundational models are effective for forecasting rare, spiky events, providing valuable insights compared to traditional stochastic approaches."}}
{"id": "2507.01401", "pdf": "https://arxiv.org/pdf/2507.01401", "abs": "https://arxiv.org/abs/2507.01401", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by MICCAI 2025", "summary": "Fetal abdominal malformations are serious congenital anomalies that require\naccurate diagnosis to guide pregnancy management and reduce mortality. Although\nAI has demonstrated significant potential in medical diagnosis, its application\nto prenatal abdominal anomalies remains limited. Most existing studies focus on\nimage-level classification and rely on standard plane localization, placing\nless emphasis on case-level diagnosis. In this paper, we develop a case-level\nmultiple instance learning (MIL)-based method, free of standard plane\nlocalization, for classifying fetal abdominal anomalies in prenatal ultrasound.\nOur contribution is three-fold. First, we adopt a mixture-of-attention-experts\nmodule (MoAE) to weight different attention heads for various planes. Secondly,\nwe propose a medical-knowledge-driven feature selection module (MFS) to align\nimage features with medical knowledge, performing self-supervised image token\nselection at the case-level. Finally, we propose a prompt-based prototype\nlearning (PPL) to enhance the MFS. Extensively validated on a large prenatal\nabdominal ultrasound dataset containing 2,419 cases, with a total of 24,748\nimages and 6 categories, our proposed method outperforms the state-of-the-art\ncompetitors. Codes are available at:https://github.com/LL-AC/AAcls.", "AI": {"tldr": "A case-level MIL-based method for classifying fetal abdominal anomalies in prenatal ultrasound, using MoAE, MFS, and PPL modules, outperforms state-of-the-art methods.", "motivation": "Accurate diagnosis of fetal abdominal malformations is critical for pregnancy management, but AI applications in this area are limited, especially for case-level diagnosis.", "method": "Proposes a MIL-based method with MoAE for attention weighting, MFS for medical-knowledge-aligned feature selection, and PPL to enhance MFS.", "result": "Validated on 2,419 cases (24,748 images), the method outperforms competitors.", "conclusion": "The approach advances case-level diagnosis in prenatal ultrasound, improving accuracy without standard plane localization."}}
{"id": "2507.01734", "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "The paper explores using LLMs and in-context learning to classify clauses in German employment contracts, finding that examination guidelines boost performance but LLMs still lag behind human lawyers.", "motivation": "Legal work is text-heavy and resource-intensive, but current NLP methods lack interpretability and trustworthiness for dynamic legal environments.", "method": "Collaborated with legal experts to extend a dataset, using LLMs and in-context learning to classify clauses as 'valid,' 'unfair,' or 'void' under three legal context variants.", "result": "Examination guidelines significantly improved recall for void clauses and weighted F1-Score (80%), but LLMs' performance with full-text sources was below human lawyers.", "conclusion": "LLMs show potential for assisting lawyers in contract legality review, but current methods have limitations."}}
{"id": "2504.13037", "pdf": "https://arxiv.org/pdf/2504.13037", "abs": "https://arxiv.org/abs/2504.13037", "authors": ["Yundi Zhang", "Paul Hager", "Che Liu", "Suprosanna Shit", "Chen Chen", "Daniel Rueckert", "Jiazhen Pan"], "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.", "AI": {"tldr": "ViTa introduces a multi-modal framework integrating CMR and patient-level factors for comprehensive cardiac health evaluation, leveraging UK Biobank data.", "motivation": "Current CMR lacks patient-level health factors, limiting holistic cardiac health understanding. Multi-modal approaches are often task-specific and lack comprehensive data.", "method": "ViTa combines 3D+T cine stacks from CMR with tabular patient data, learning a shared latent representation for diverse tasks.", "result": "ViTa enables unified cardiac phenotype prediction, segmentation, and disease classification, improving clinical utility.", "conclusion": "ViTa advances cardiac analysis by integrating imaging and patient context, offering scalable, patient-specific insights."}}
{"id": "2507.01063", "pdf": "https://arxiv.org/pdf/2507.01063", "abs": "https://arxiv.org/abs/2507.01063", "authors": ["Madhav Kotecha"], "title": "FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Online dating platforms have fundamentally transformed the formation of\nromantic relationships, with millions of users worldwide relying on algorithmic\nmatching systems to find compatible partners. However, current recommendation\nsystems in dating applications suffer from significant algorithmic\ndeficiencies, including but not limited to popularity bias, filter bubble\neffects, and inadequate reciprocity modeling that limit effectiveness and\nintroduce harmful biases. This research integrates foundational work with\nrecent empirical findings to deliver a detailed analysis of dating app\nrecommendation systems, highlighting key issues and suggesting research-backed\nsolutions. Through analysis of reciprocal recommendation frameworks, fairness\nevaluation metrics, and industry implementations, we demonstrate that current\nsystems achieve modest performance with collaborative filtering reaching 25.1\\%\nwhile reciprocal methods achieve 28.7\\%. Our proposed mathematical framework\naddresses these limitations through enhanced similarity measures,\nmulti-objective optimization, and fairness-aware algorithms that maintain\ncompetitive accuracy while improving demographic representation to reduce\nalgorithmic bias.", "AI": {"tldr": "The paper analyzes algorithmic deficiencies in dating app recommendation systems, proposes solutions, and introduces a fairness-aware framework to improve performance and reduce bias.", "motivation": "Current dating app recommendation systems suffer from issues like popularity bias and filter bubbles, limiting effectiveness and introducing harmful biases.", "method": "The research integrates foundational and empirical work, analyzes reciprocal recommendation frameworks, and evaluates fairness metrics. It proposes a mathematical framework with enhanced similarity measures and multi-objective optimization.", "result": "Collaborative filtering achieves 25.1% performance, while reciprocal methods reach 28.7%. The proposed framework improves accuracy and demographic representation.", "conclusion": "The study highlights key issues in dating app algorithms and offers research-backed solutions to enhance fairness and effectiveness."}}
{"id": "2507.01068", "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "categories": ["cs.LG"], "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability.", "AI": {"tldr": "The paper develops explainable AI methods using IMU data to detect and predict Freezing of Gait (FOG) in Parkinson's disease, achieving 99% accuracy with a Stacking Ensemble model and employing federated learning for model training.", "motivation": "Early detection of FOG in Parkinson's disease is critical for patient care, and explainable AI methods are needed to understand model decisions.", "method": "Machine learning models (CatBoost, XGBoost, Extra Trees) and a Stacking Ensemble are used for classification. Federated learning with a hybrid Conv1D + LSTM architecture is employed for training.", "result": "The Stacking Ensemble model achieves nearly 99% accuracy, outperforming a hybrid bidirectional GRU model. SHAP analysis identifies time as the most influential feature.", "conclusion": "The proposed framework is effective for FOG prediction, combining high accuracy with explainability and privacy-preserving federated learning."}}
{"id": "2507.01409", "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "AI": {"tldr": "CaptionSmiths proposes a method to control caption properties like length and descriptiveness in image captioning models by quantifying and interpolating between extreme states.", "motivation": "Existing models lack fine-grained control over caption properties and cannot smoothly transition between language patterns.", "method": "Quantifies caption properties (length, descriptiveness, word uniqueness) as scalar values and conditions the model via interpolation between endpoint vectors.", "result": "The model smoothly adjusts caption properties and improves lexical alignment, reducing length control error by 506%.", "conclusion": "CaptionSmiths effectively enables flexible control over caption properties in a single model."}}
{"id": "2507.01764", "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "categories": ["cs.CL"], "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "The paper explores how tokenisation discrepancies impact language data representation and analysis validity, focusing on emojis and homoglyphs, and proposes preprocessing methods for accurate corpus representation.", "motivation": "To address challenges in tokenisation, particularly with emojis and homoglyphs, ensuring corpus fidelity and reliable linguistic analysis.", "method": "Investigates preprocessing techniques for digital texts to maintain accuracy in corpora.", "result": "Highlights the need for understanding linguistic and technical aspects of digital data to improve corpus analysis accuracy.", "conclusion": "The study underscores the importance of precise tokenisation for reliable and repeatable linguistic research, impacting both quantitative and qualitative methods."}}
{"id": "2406.07318", "pdf": "https://arxiv.org/pdf/2406.07318", "abs": "https://arxiv.org/abs/2406.07318", "authors": ["Kamil Jeziorek", "Piotr Wzorek", "Krzysztof Blachut", "Andrea Pinna", "Tomasz Kryjak"], "title": "Embedded Graph Convolutional Networks for Real-Time Event Data Processing on SoC FPGAs", "categories": ["cs.CV", "cs.AR", "eess.IV"], "comment": "Submitted to the IEEE Transactions on Very Large Scale Integration\n  Systems. This manuscript was first submitted for publication on March 31,\n  2024. It has since been revised three times: on 22 Mau 2024, 10 June 2024 and\n  19 June 2025 (major update)", "summary": "The utilisation of event cameras represents an important and swiftly evolving\ntrend aimed at addressing the constraints of traditional video systems.\nParticularly within the automotive domain, these cameras find significant\nrelevance for their integration into embedded real-time systems due to lower\nlatency and energy consumption. One effective approach to ensure the necessary\nthroughput and latency for event processing is through the utilisation of graph\nconvolutional networks (GCNs). In this study, we introduce a custom EFGCN\n(Event-based FPGA-accelerated Graph Convolutional Network) designed with a\nseries of hardware-aware optimisations tailored for PointNetConv, a graph\nconvolution designed for point cloud processing. The proposed techniques result\nin up to 100-fold reduction in model size compared to Asynchronous Event-based\nGNN (AEGNN), one of the most recent works in the field, with a relatively small\ndecrease in accuracy (2.9% for the N-Caltech101 classification task, 2.2% for\nthe N-Cars classification task), thus following the TinyML trend. We\nimplemented EFGCN on a ZCU104 SoC FPGA platform without any external memory\nresources, achieving a throughput of 13.3 million events per second (MEPS) and\nreal-time partially asynchronous processing with low latency. Our approach\nachieves state-of-the-art performance across multiple event-based\nclassification benchmarks while remaining highly scalable, customisable and\nresource-efficient. We publish both software and hardware source code in an\nopen repository: https://github.com/vision-agh/gcnn-dvs-fpga", "AI": {"tldr": "The paper introduces EFGCN, an FPGA-accelerated GCN for event cameras, achieving 100x model size reduction and high throughput with minimal accuracy loss.", "motivation": "Addressing the limitations of traditional video systems in automotive applications by leveraging event cameras for low-latency, energy-efficient real-time processing.", "method": "Developed a custom EFGCN with hardware-aware optimizations for PointNetConv, implemented on a ZCU104 SoC FPGA without external memory.", "result": "Achieved 13.3 MEPS throughput, 100x model size reduction vs. AEGNN, with minor accuracy drops (2.9% for N-Caltech101, 2.2% for N-Cars).", "conclusion": "EFGCN offers state-of-the-art, scalable, and resource-efficient event-based classification, with open-source software and hardware."}}
{"id": "2507.01076", "pdf": "https://arxiv.org/pdf/2507.01076", "abs": "https://arxiv.org/abs/2507.01076", "authors": ["Vanja Stojanovi\u0107", "Bor Panger\u0161i\u010d"], "title": "Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem", "categories": ["cs.CG", "cs.AI", "cs.PF", "math.CO"], "comment": null, "summary": "The NP-complete mutual-visibility (MV) problem currently lacks empirical\nanalysis on its practical behaviour despite theoretical studies. This paper\naddresses this gap by implementing and evaluating three distinct algorithms - a\ndirect greedy heuristic, a hypergraph-based approximation, and a genetic\nalgorithm - on diverse synthetic graph datasets, including those with\nanalytically known $\\mu(G)$ values and general graph models. Our results\ndemonstrate that for smaller graphs, the algorithms consistently achieve MV set\nsizes aligning with theoretical bounds. However, for larger instances, achieved\nsolution sizes notably diverge from theoretical limits; this, combined with the\nabsence of tight bounds, complicates absolute quality assessment. Nevertheless,\nvalidation on known optimal graphs showed the Genetic Algorithm and other\nheuristics empirically performing best among tested methods.", "AI": {"tldr": "The paper empirically analyzes the mutual-visibility (MV) problem, testing three algorithms (greedy heuristic, hypergraph-based approximation, genetic algorithm) on synthetic graphs. Results show alignment with theory for small graphs but divergence for larger ones, with the genetic algorithm performing best.", "motivation": "To address the lack of empirical analysis on the practical behavior of the NP-complete mutual-visibility problem, despite existing theoretical studies.", "method": "Implemented and evaluated three algorithms (greedy heuristic, hypergraph-based approximation, genetic algorithm) on diverse synthetic graph datasets, including those with known theoretical values.", "result": "For smaller graphs, algorithms align with theoretical bounds; for larger graphs, solutions diverge. The genetic algorithm performed best on known optimal graphs.", "conclusion": "The study highlights practical challenges in achieving theoretical bounds for larger graphs, with the genetic algorithm emerging as the most effective heuristic."}}
{"id": "2507.01073", "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "AI": {"tldr": "A novel 3D encoding module for GNNs using rotational sampling achieves rotational invariance and improves molecular property prediction.", "motivation": "Traditional GNNs struggle with 3D molecular structures due to orientation variability, limiting generalization and robustness. Existing methods are either too rigid or computationally expensive.", "method": "Proposes a plug-and-play 3D encoding module using rotational sampling and SO(3) group expectation, with a post-alignment strategy for strict invariance.", "result": "Outperforms existing methods on QM9 and C10 datasets in accuracy, robustness, and generalization while maintaining low computational cost.", "conclusion": "The method offers an efficient, interpretable solution for 3D molecular data in drug discovery and material design."}}
{"id": "2507.01417", "pdf": "https://arxiv.org/pdf/2507.01417", "abs": "https://arxiv.org/abs/2507.01417", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Out-of-Distribution (OOD) detection is critical for safely deploying deep\nmodels in open-world environments, where inputs may lie outside the training\ndistribution. During inference on a model trained exclusively with\nIn-Distribution (ID) data, we observe a salient gradient phenomenon: around an\nID sample, the local gradient directions for \"enhancing\" that sample's\npredicted class remain relatively consistent, whereas OOD samples--unseen in\ntraining--exhibit disorganized or conflicting gradient directions in the same\nneighborhood. Motivated by this observation, we propose an inference-stage\ntechnique to short-circuit those feature coordinates that spurious gradients\nexploit to inflate OOD confidence, while leaving ID classification largely\nintact. To circumvent the expense of recomputing the logits after this gradient\nshort-circuit, we further introduce a local first-order approximation that\naccurately captures the post-modification outputs without a second forward\npass. Experiments on standard OOD benchmarks show our approach yields\nsubstantial improvements. Moreover, the method is lightweight and requires\nminimal changes to the standard inference pipeline, offering a practical path\ntoward robust OOD detection in real-world applications.", "AI": {"tldr": "A novel inference-stage technique for OOD detection leverages gradient consistency in ID samples and disorganization in OOD samples, improving robustness without major pipeline changes.", "motivation": "OOD detection is crucial for safe deep model deployment in open-world environments, where inputs may deviate from training data.", "method": "Proposes gradient short-circuiting to block spurious gradients in OOD samples and a local first-order approximation to avoid recomputing logits.", "result": "Substantial improvements on standard OOD benchmarks, with lightweight implementation.", "conclusion": "The method offers a practical, effective solution for robust OOD detection in real-world applications."}}
{"id": "2507.01785", "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "MuRating is a scalable framework for multilingual data-quality assessment, transferring English quality signals to 17 languages, improving model performance on English and multilingual tasks.", "motivation": "Existing data-quality methods are English-centric; MuRating addresses this gap by enabling multilingual quality assessment.", "method": "MuRating aggregates English raters via pairwise comparisons, projects judgments through translation, and trains a multilingual evaluator on diverse text pairs.", "result": "MuRating boosts accuracy on English and multilingual benchmarks, especially in knowledge-intensive tasks, outperforming baselines like QuRater and AskLLM.", "conclusion": "MuRating enhances multilingual data selection, with potential for further improvements in translation fidelity and addressing biases."}}
{"id": "2410.22784", "pdf": "https://arxiv.org/pdf/2410.22784", "abs": "https://arxiv.org/abs/2410.22784", "authors": ["Omar Erak", "Omar Alhussein", "Wen Tong"], "title": "Contrastive Learning and Adversarial Disentanglement for Privacy-Aware Task-Oriented Semantic Communication", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "eess.IV", "math.IT"], "comment": null, "summary": "Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission in\nnext-generation networks, where only information relevant to a specific task is\ncommunicated. This is particularly important in 6G-enabled Internet of Things\n(6G-IoT) scenarios, where bandwidth constraints, latency requirements, and data\nprivacy are critical. However, existing methods struggle to fully disentangle\ntask-relevant and task-irrelevant information, leading to privacy concerns and\nsuboptimal performance. To address this, we propose an information-bottleneck\ninspired method, named CLAD (contrastive learning and adversarial\ndisentanglement). CLAD utilizes contrastive learning to effectively capture\ntask-relevant features while employing adversarial disentanglement to discard\ntask-irrelevant information. Additionally, due to the absence of reliable and\nreproducible methods to quantify the minimality of encoded feature vectors, we\nintroduce the Information Retention Index (IRI), a comparative metric used as a\nproxy for the mutual information between the encoded features and the input.\nThe IRI reflects how minimal and informative the representation is, making it\nhighly relevant for privacy-preserving and bandwidth-efficient 6G-IoT systems.\nExtensive experiments demonstrate that CLAD outperforms state-of-the-art\nbaselines in terms of semantic extraction, task performance, privacy\npreservation, and IRI, making it a promising building block for responsible,\nefficient and trustworthy 6G-IoT services.", "AI": {"tldr": "CLAD, a method combining contrastive learning and adversarial disentanglement, improves task-oriented semantic communication in 6G-IoT by enhancing privacy and efficiency.", "motivation": "Addressing the challenge of disentangling task-relevant and irrelevant information in 6G-IoT, where existing methods fall short in privacy and performance.", "method": "Proposes CLAD, using contrastive learning for task-relevant features and adversarial disentanglement to discard irrelevant data, alongside introducing the IRI metric.", "result": "CLAD outperforms baselines in semantic extraction, task performance, privacy, and IRI, proving effective for 6G-IoT.", "conclusion": "CLAD is a promising solution for efficient, privacy-preserving, and trustworthy 6G-IoT services."}}
{"id": "2507.01081", "pdf": "https://arxiv.org/pdf/2507.01081", "abs": "https://arxiv.org/abs/2507.01081", "authors": ["Megan T. deBettencourt", "Sruthi Sakthivel", "Emily A. Holmes", "Mark Chevillet"], "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Trauma prevalence is vast globally. Evidence-based digital treatments can\nhelp, but most require human guidance. Human guides provide tailored\ninstructions and responsiveness to internal cognitive states, but limit\nscalability. Can generative AI and neurotechnology provide a scalable\nalternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to\nautomatically deliver and monitor an evidence-based digital treatment,\nspecifically the Imagery Competing Task Intervention (ICTI), to reduce\nintrusive memories after psychological trauma. One hundred healthy volunteers\nwere exposed to videos of traumatic events and randomly assigned to an\nintervention or active control condition. As predicted, intervention\nparticipants reported significantly fewer intrusive memories over the following\nweek. Post-hoc assessment against clinical rubrics confirmed the AI guide\ndelivered the intervention successfully. Additionally, pupil size tracked\nintervention engagement and predicted symptom reduction, providing a candidate\nbiomarker of intervention effectiveness. These findings open a path toward\nrigorous AI-guided digital interventions that can scale to trauma prevalence.", "AI": {"tldr": "ANTIDOTE, combining AI and pupillometry, effectively delivers ICTI to reduce intrusive memories post-trauma, showing promise for scalable digital interventions.", "motivation": "Address the scalability limitations of human-guided digital treatments for trauma by leveraging AI and neurotechnology.", "method": "Tested ANTIDOTE (AI + pupillometry) on 100 volunteers exposed to traumatic videos, comparing intervention and control groups.", "result": "Intervention group had fewer intrusive memories; pupil size tracked engagement and predicted symptom reduction.", "conclusion": "AI-guided interventions like ANTIDOTE can scale effectively for trauma treatment, with pupil size as a potential biomarker."}}
{"id": "2507.01075", "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems.", "AI": {"tldr": "The paper introduces yProv4ML, a library for collecting provenance data in AI model training to optimize efficiency, accuracy, and energy use.", "motivation": "The growing demand for large-scale AI models necessitates balancing computational efficiency, execution time, accuracy, and energy consumption. Provenance data is key for insights and accountability.", "method": "The yProv4ML library collects provenance data in JSON format, adhering to W3C PROV and ProvML standards, and supports plugin-based extensibility.", "result": "yProv4ML enables flexible, extensible provenance data collection, integrated with the yProv framework for workflow management.", "conclusion": "Provenance tools like yProv4ML are essential for optimizing AI model training and ensuring reproducibility and efficiency."}}
{"id": "2507.01422", "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image\nenhancement. However, existing methods tend to remove shadows with constant\ncolor background and ignore color shadows. In this paper, we first design a\ndiffusion model in latent space for document image shadow removal, called\nDocShaDiffusion. It translates shadow images from pixel space to latent space,\nenabling the model to more easily capture essential features. To address the\nissue of color shadows, we design a shadow soft-mask generation module (SSGM).\nIt is able to produce accurate shadow mask and add noise into shadow regions\nspecially. Guided by the shadow mask, a shadow mask-aware guided diffusion\nmodule (SMGDM) is proposed to remove shadows from document images by\nsupervising the diffusion and denoising process. We also propose a\nshadow-robust perceptual feature loss to preserve details and structures in\ndocument images. Moreover, we develop a large-scale synthetic document color\nshadow removal dataset (SDCSRD). It simulates the distribution of realistic\ncolor shadows and provides powerful supports for the training of models.\nExperiments on three public datasets validate the proposed method's superiority\nover state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "The paper introduces DocShaDiffusion, a latent space diffusion model for document shadow removal, addressing color shadows with a shadow soft-mask generation module (SSGM) and a mask-aware guided diffusion module (SMGDM). It also proposes a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).", "motivation": "Existing methods fail to handle color shadows in document images, limiting their effectiveness. The paper aims to improve shadow removal by focusing on color shadows and preserving document details.", "method": "The method involves a latent space diffusion model (DocShaDiffusion), SSGM for shadow mask generation, SMGDM for guided diffusion, and a perceptual feature loss. A synthetic dataset (SDCSRD) is also created for training.", "result": "Experiments on three public datasets show the method outperforms state-of-the-art techniques in document shadow removal.", "conclusion": "The proposed approach effectively removes color shadows while preserving document details, validated by superior performance on benchmarks. The code and dataset will be publicly available."}}
{"id": "2507.01786", "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "Language models can detect evaluation vs. deployment phases, raising concerns about AI governance. Probes in Llama-3.3-70B-Instruct show this distinction internally, suggesting current evaluations may seem artificial. This highlights the need for trustworthy evaluations and understanding deceptive capabilities.", "motivation": "To investigate evaluation awareness in language models and its implications for AI safety and governance, given the potential for models to deceive evaluations.", "method": "Used linear probes on Llama-3.3-70B-Instruct to analyze its ability to distinguish between evaluation and deployment prompts.", "result": "Probes successfully separated real-world evaluation and deployment prompts, indicating models internally represent this distinction. Safety evaluations were correctly classified as artificial.", "conclusion": "The study emphasizes the need for reliable evaluations and understanding deceptive model behaviors, suggesting model internals can aid safety audits for future models."}}
{"id": "2411.16370", "pdf": "https://arxiv.org/pdf/2411.16370", "abs": "https://arxiv.org/abs/2411.16370", "authors": ["M. M. A. Valiuddin", "R. J. G. van Sloun", "C. G. A. Viviers", "P. H. N. de With", "F. van der Sommen"], "title": "A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML"], "comment": "31 pages of content, revised", "summary": "Advancements in image segmentation play an integral role within the broad\nscope of Deep Learning-based Computer Vision. Furthermore, their widespread\napplicability in critical real-world tasks has resulted in challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling the expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision-making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation, by discussing fundamental concepts of\nuncertainty quantification, governing advancements in the field as well as the\napplication to various tasks. Moreover, literature on both types of\nuncertainties trace back to four key applications: (1) to quantify statistical\ninconsistencies in the annotation process due ambiguous images, (2) correlating\nprediction error with uncertainty, (3) expanding the model hypothesis space for\nbetter generalization, and (4) Active Learning. An extensive discussion follows\nthat includes an overview of utilized datasets for each of the applications and\nevaluation of the available methods. We also highlight challenges related to\narchitectures, uncertainty quantification methods, standardization and\nbenchmarking, and finally end with recommendations for future work such as\nmethods based on single forward passes and models that appropriately leverage\nvolumetric data.", "AI": {"tldr": "The paper provides a comprehensive overview of probabilistic segmentation, focusing on uncertainty quantification in CNN-based models, its applications, and future research directions.", "motivation": "The rapid adoption of CNN-based segmentation models in high-stake applications necessitates reliable uncertainty quantification to prevent uninformed decisions.", "method": "The work reviews fundamental concepts, advancements, and applications of uncertainty quantification in segmentation, covering epistemic and aleatoric uncertainties.", "result": "Key applications include addressing annotation inconsistencies, correlating prediction error with uncertainty, improving generalization, and Active Learning. Challenges in architectures, methods, and benchmarking are identified.", "conclusion": "Future work should focus on single forward-pass methods and better utilization of volumetric data, alongside standardization and benchmarking improvements."}}
{"id": "2507.01196", "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "AI": {"tldr": "LBMs show marginal improvements over traditional deep architectures in BCI tasks but require more parameters. LoRA reduces parameters without performance loss, indicating inefficiencies in current LBMs.", "motivation": "To evaluate the effectiveness of LBMs in brainwave modeling and address their inefficiencies for BCI applications.", "method": "Systematic fine-tuning experiments on BCI tasks, including memory tasks and sleep stage classification, using LoRA for parameter reduction.", "result": "LBMs achieve only 0.9%-1.2% improvement over traditional methods but with higher parameter costs. LoRA reduces parameters without degrading performance.", "conclusion": "Domain-specific strategies and architectural redesigns are needed to fully leverage LBMs in brainwave analysis."}}
{"id": "2507.01077", "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "categories": ["cs.LG"], "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.", "AI": {"tldr": "A novel decoder-only LLM for anomaly detection in ECU communication logs, addressing challenges like lack of tailored LLMs and inconsistent ground truth data.", "motivation": "Specialized domains like automotive communication systems need scalable anomaly detection solutions, but existing methods (supervised/clustering) fall short.", "method": "Uses a decoder-only LLM trained on UDP logs, with entropy regularization to handle inconsistent labels and improve uncertainty in anomalies.", "result": "Introduces a scalable system that learns from minimal examples, improves detection accuracy, and reduces reliance on manual labeling.", "conclusion": "The proposed LLM-based approach offers a scalable, adaptable solution for anomaly detection in complex ECU communication environments."}}
{"id": "2507.01439", "pdf": "https://arxiv.org/pdf/2507.01439", "abs": "https://arxiv.org/abs/2507.01439", "authors": ["Shaocheng Yan", "Pengcheng Shi", "Zhenjun Zhao", "Kaixin Wang", "Kuang Cao", "Ji Wu", "Jiayuan Li"], "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "categories": ["cs.CV"], "comment": "ICCV-2025 Accepted Paper", "summary": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.", "AI": {"tldr": "TurboReg introduces a fast, robust estimator for Point Cloud Registration using TurboClique and Pivot-Guided Search, achieving high recall and speed.", "motivation": "Existing methods for robust estimation in PCR are slow due to exponential time complexity, limiting practical use.", "method": "TurboReg uses TurboClique (3-clique in constrained graphs) and PGS for efficient, parallel searching with linear time complexity.", "result": "TurboReg outperforms state-of-the-art methods, e.g., 208.22\u00d7 faster than 3DMAC with higher recall.", "conclusion": "TurboReg is a scalable, efficient solution for PCR, balancing speed and robustness."}}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "The paper investigates how vision-language AI models handle conflicting multimodal inputs (e.g., mismatched image-caption pairs) and reveals biases in modality preference, internal representational structures, and the role of attention heads in resolving conflicts.", "motivation": "To understand how multimodal AI models behave when faced with conflicting input streams, particularly in vision-language tasks, and to explore the mechanisms behind modality preference and conflict resolution.", "method": "The study tests models with inconsistent inputs (e.g., an image of a dog paired with a cat caption) and analyzes their responses to modality-specific queries, examining internal representations and attention heads.", "result": "Models often favor one modality over another, with preferences evident in their internal structures. Specific attention heads and modality-agnostic \"router heads\" influence and can be manipulated to improve performance.", "conclusion": "The work advances understanding of how multimodal models detect and resolve conflicts, offering insights into controlling their behavior in complex environments."}}
{"id": "2506.19266", "pdf": "https://arxiv.org/pdf/2506.19266", "abs": "https://arxiv.org/abs/2506.19266", "authors": ["Jiahao Huang", "Ruifeng Li", "Wenwen Yu", "Anan Li", "Xiangning Li", "Mingchao Yan", "Lei Xie", "Qingrun Zeng", "Xueyan Jia", "Shuxin Wang", "Ronghui Ju", "Feng Chen", "Qingming Luo", "Hui Gong", "Andrew Zalesky", "Xiaoquan Yang", "Yuanjing Feng", "Zheng Wang"], "title": "Convergent and divergent connectivity patterns of the arcuate fasciculus in macaques and humans", "categories": ["q-bio.NC", "cs.CV", "eess.IV"], "comment": "34 pages, 6 figures", "summary": "The organization and connectivity of the arcuate fasciculus (AF) in nonhuman\nprimates remain contentious, especially concerning how its anatomy diverges\nfrom that of humans. Here, we combined cross-scale single-neuron tracing -\nusing viral-based genetic labeling and fluorescence micro-optical sectioning\ntomography in macaques (n = 4; age 3 - 11 years) - with whole-brain\ntractography from 11.7T diffusion MRI. Complemented by spectral embedding\nanalysis of 7.0T MRI in humans, we performed a comparative connectomic analysis\nof the AF across species. We demonstrate that the macaque AF originates in the\ntemporal-parietal cortex, traverses the auditory cortex and parietal operculum,\nand projects into prefrontal regions. In contrast, the human AF exhibits\ngreater expansion into the middle temporal gyrus and stronger prefrontal and\nparietal operculum connectivity - divergences quantified by Kullback-Leibler\nanalysis that likely underpin the evolutionary specialization of human language\nnetworks. These interspecies differences - particularly the human AF's broader\ntemporal integration and strengthened frontoparietal linkages - suggest a\nconnectivity-based substrate for the emergence of advanced language processing\nunique to humans. Furthermore, our findings offer a neuroanatomical framework\nfor understanding AF-related disorders such as aphasia and dyslexia, where\naberrant connectivity disrupts language function.", "AI": {"tldr": "The study compares the arcuate fasciculus (AF) in macaques and humans, revealing species-specific connectivity differences that may explain human language evolution and disorders like aphasia.", "motivation": "To resolve debates about AF organization in nonhuman primates and understand its divergence from human anatomy, particularly in language-related networks.", "method": "Combined single-neuron tracing in macaques (viral labeling and tomography) with 11.7T diffusion MRI, and spectral embedding analysis of 7.0T MRI in humans for comparative connectomics.", "result": "Macaque AF connects temporal-parietal to prefrontal regions, while human AF shows expanded middle temporal gyrus and stronger frontoparietal links, quantified by Kullback-Leibler analysis.", "conclusion": "Human AF's broader temporal integration and enhanced frontoparietal connectivity likely support advanced language processing, offering insights into AF-related disorders."}}
{"id": "2507.01198", "pdf": "https://arxiv.org/pdf/2507.01198", "abs": "https://arxiv.org/abs/2507.01198", "authors": ["Benjamin Kraljusic", "Zlatan Ajanovic", "Nermin Covic", "Bakir Lacevic"], "title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives", "categories": ["cs.RO", "cs.AI", "cs.CG"], "comment": "6 pages, 3 figures, submitted to a conference", "summary": "This work proposes a motion planning algorithm for robotic manipulators that\ncombines sampling-based and search-based planning methods. The core\ncontribution of the proposed approach is the usage of burs of free\nconfiguration space (C-space) as adaptive motion primitives within the graph\nsearch algorithm. Due to their feature to adaptively expand in free C-space,\nburs enable more efficient exploration of the configuration space compared to\nfixed-sized motion primitives, significantly reducing the time to find a valid\npath and the number of required expansions. The algorithm is implemented within\nthe existing SMPL (Search-Based Motion Planning Library) library and evaluated\nthrough a series of different scenarios involving manipulators with varying\nnumber of degrees-of-freedom (DoF) and environment complexity. Results\ndemonstrate that the bur-based approach outperforms fixed-primitive planning in\ncomplex scenarios, particularly for high DoF manipulators, while achieving\ncomparable performance in simpler scenarios.", "AI": {"tldr": "A motion planning algorithm for robotic manipulators combines sampling and search-based methods, using adaptive motion primitives (burs) for efficient C-space exploration, outperforming fixed-primitive methods in complex scenarios.", "motivation": "To improve motion planning efficiency for robotic manipulators by leveraging adaptive motion primitives (burs) for better exploration of free configuration space.", "method": "Combines sampling-based and search-based planning, using burs (adaptive motion primitives) within a graph search algorithm, implemented in the SMPL library.", "result": "Outperforms fixed-primitive planning in complex scenarios, especially for high DoF manipulators, while matching performance in simpler cases.", "conclusion": "The bur-based approach is more efficient for complex motion planning tasks, offering significant advantages over traditional fixed-primitive methods."}}
{"id": "2507.01078", "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.", "AI": {"tldr": "yProv4ML is a framework for capturing provenance data in machine learning processes using PROV-JSON format, addressing transparency and rigor issues in LLM development.", "motivation": "The lack of transparency and rigor in LLM development, especially around hyperparameters and epochs, highlights the need for better provenance tracking.", "method": "Proposes yProv4ML, a framework that captures provenance data in PROV-JSON format with minimal code changes.", "result": "Enables automated and standardized collection of provenance information during ML processes.", "conclusion": "yProv4ML improves transparency and rigor in ML development by addressing lineage and data capture challenges."}}
{"id": "2507.01455", "pdf": "https://arxiv.org/pdf/2507.01455", "abs": "https://arxiv.org/abs/2507.01455", "authors": ["Yuxing Liu", "Ji Zhang", "Zhou Xuchuan", "Jingzhong Xiao", "Huimin Yang", "Jiaxin Zhong"], "title": "OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous\nobjects within images. Existing pixel-wise methods typically assign anomaly\nscores individually and employ a global thresholding strategy to segment\nanomalies. Despite their effectiveness, these approaches encounter significant\nchallenges in real-world applications: (1) neglecting spatial correlations\namong pixels within the same object, resulting in fragmented segmentation; (2)\nvariabil ity in anomaly score distributions across image regions, causing\nglobal thresholds to either generate false positives in background areas or\nmiss segments of anomalous objects. In this work, we introduce OoDDINO, a novel\nmulti-level anomaly segmentation framework designed to address these\nlimitations through a coarse-to-fine anomaly detection strategy. OoDDINO\ncombines an uncertainty-guided anomaly detection model with a pixel-level\nsegmentation model within a two-stage cascade architecture. Initially, we\npropose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that\nsequentially integrates multiple uncertainty metrics with visual\nrepresentations, employing orthogonal constraints to strengthen the detection\nmodel's capacity for localizing anomalous regions accurately. Subsequently, we\ndevelop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically\ngenerates region-specific thresholds based on object-level detection outputs\nand pixel-wise anomaly scores. This approach allows for distinct thresholding\nstrategies within foreground and background areas, achieving fine-grained\nanomaly segmentation. The proposed framework is compatible with other\npixel-wise anomaly detection models, which acts as a plug-in to boost the\nperformance. Extensive experiments on two benchmark datasets validate our\nframework's superiority and compatibility over state-of-the-art methods.", "AI": {"tldr": "OoDDINO introduces a multi-level anomaly segmentation framework to address fragmented segmentation and global thresholding issues in anomaly detection, using a coarse-to-fine strategy with uncertainty-guided detection and adaptive thresholds.", "motivation": "Existing pixel-wise anomaly segmentation methods struggle with fragmented results and inconsistent thresholding due to spatial correlation neglect and score variability.", "method": "OoDDINO combines uncertainty-guided anomaly detection (OUAFS) with pixel-level segmentation (ADT-Net) in a two-stage cascade, using orthogonal constraints and adaptive thresholds.", "result": "Extensive experiments show OoDDINO outperforms state-of-the-art methods on benchmark datasets, offering compatibility with other models.", "conclusion": "OoDDINO effectively addresses limitations of existing methods, providing accurate and fine-grained anomaly segmentation."}}
{"id": "2507.01802", "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "The paper analyzes the MDACE dataset for explainable medical coding, evaluating plausibility of current systems and proposing match measures and recommendations.", "motivation": "To improve transparency in automatic medical coding by evaluating explainability methods using the MDACE dataset.", "method": "In-depth analysis of the MDACE dataset and plausibility evaluation of explainable medical coding systems.", "result": "Ground truth evidence aligns with code descriptions; state-of-the-art approaches show high overlap with ground truth.", "conclusion": "Recommendations are provided for developing and evaluating explainable medical coding systems."}}
{"id": "2506.21349", "pdf": "https://arxiv.org/pdf/2506.21349", "abs": "https://arxiv.org/abs/2506.21349", "authors": ["Yizhe Cheng", "Chunxun Tian", "Haoru Wang", "Wentao Zhu", "Xiaoxuan Ma", "Yizhou Wang"], "title": "Generalizable Neural Electromagnetic Inverse Scattering", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in\napplications such as medical imaging, where the goal is to reconstruct the\nrelative permittivity from scattered electromagnetic field. This inverse\nprocess is inherently ill-posed and highly nonlinear, making it particularly\nchallenging. A recent machine learning-based approach, Img-Interiors, shows\npromising results by leveraging continuous implicit functions. However, it\nrequires case-specific optimization, lacks generalization to unseen data, and\nfails under sparse transmitter setups (e.g., with only one transmitter). To\naddress these limitations, we revisit EISP from a physics-informed perspective,\nreformulating it as a two stage inverse transmission-scattering process. This\nformulation reveals the induced current as a generalizable intermediate\nrepresentation, effectively decoupling the nonlinear scattering process from\nthe ill-posed inverse problem. Built on this insight, we propose the first\ngeneralizable physics-driven framework for EISP, comprising a current estimator\nand a permittivity solver, working in an end-to-end manner. The current\nestimator explicitly learns the induced current as a physical bridge between\nthe incident and scattered field, while the permittivity solver computes the\nrelative permittivity directly from the estimated induced current. This design\nenables data-driven training and generalizable feed-forward prediction of\nrelative permittivity on unseen data while maintaining strong robustness to\ntransmitter sparsity. Extensive experiments show that our method outperforms\nstate-of-the-art approaches in reconstruction accuracy, generalization, and\nrobustness. This work offers a fundamentally new perspective on electromagnetic\ninverse scattering and represents a major step toward cost-effective practical\nsolutions for electromagnetic imaging.", "AI": {"tldr": "A physics-driven framework for solving Electromagnetic Inverse Scattering Problems (EISP) is proposed, improving generalization and robustness over existing methods.", "motivation": "Existing machine learning approaches like Img-Interiors lack generalization and fail under sparse transmitter setups, necessitating a more robust solution.", "method": "The problem is reformulated as a two-stage process involving a current estimator and permittivity solver, leveraging induced current as an intermediate representation.", "result": "The framework outperforms state-of-the-art methods in accuracy, generalization, and robustness, especially under sparse transmitter conditions.", "conclusion": "This work provides a novel, physics-informed approach to EISP, advancing practical electromagnetic imaging solutions."}}
{"id": "2507.01225", "pdf": "https://arxiv.org/pdf/2507.01225", "abs": "https://arxiv.org/abs/2507.01225", "authors": ["Sunandita Patra", "Mehtab Pathan", "Mahmoud Mahfouz", "Parisa Zehtabi", "Wided Ouaja", "Daniele Magazzeni", "Manuela Veloso"], "title": "Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration", "categories": ["cs.DC", "cs.AI"], "comment": "Please cite as: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz,\n  Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, and Manuela Veloso. \"Capacity\n  planning and scheduling for jobs with uncertainty in resource usage and\n  duration.\" The Journal of Supercomputing 80, no. 15 (2024): 22428-22461", "summary": "Organizations around the world schedule jobs (programs) regularly to perform\nvarious tasks dictated by their end users. With the major movement towards\nusing a cloud computing infrastructure, our organization follows a hybrid\napproach with both cloud and on-prem servers. The objective of this work is to\nperform capacity planning, i.e., estimate resource requirements, and job\nscheduling for on-prem grid computing environments. A key contribution of our\napproach is handling uncertainty in both resource usage and duration of the\njobs, a critical aspect in the finance industry where stochastic market\nconditions significantly influence job characteristics. For capacity planning\nand scheduling, we simultaneously balance two conflicting objectives: (a)\nminimize resource usage, and (b) provide high quality-of-service to the end\nusers by completing jobs by their requested deadlines. We propose approximate\napproaches using deterministic estimators and pair sampling-based constraint\nprogramming. Our best approach (pair sampling-based) achieves much lower peak\nresource usage compared to manual scheduling without compromising on the\nquality-of-service.", "AI": {"tldr": "The paper addresses capacity planning and job scheduling in hybrid cloud/on-prem environments, focusing on handling uncertainty in resource usage and job duration. It proposes approximate methods to minimize resource usage while meeting deadlines.", "motivation": "The shift to cloud computing and the need for efficient job scheduling in hybrid environments, especially in finance where market conditions introduce uncertainty.", "method": "Uses deterministic estimators and pair sampling-based constraint programming for capacity planning and scheduling.", "result": "The pair sampling-based approach reduces peak resource usage significantly without sacrificing quality-of-service.", "conclusion": "The proposed methods effectively balance resource efficiency and meeting deadlines in uncertain environments."}}
{"id": "2507.01080", "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Am\u00e9lie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "categories": ["cs.LG", "cs.PF"], "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.", "AI": {"tldr": "The study compares three AI models (NLP, LLM, JEPA) for triage prediction in EDs, finding the LLM model (URGENTIAPARSE) most accurate, outperforming nurse triage and other AI models.", "motivation": "Triage errors in EDs persist due to patient influx and staff shortages, prompting exploration of AI integration for improved accuracy.", "method": "Retrospective analysis of triage data using three AI models (NLP, LLM, JEPA) trained on demographic, complaint, and vital sign data, validated against the FRENCH scale.", "result": "The LLM model (URGENTIAPARSE) achieved the highest accuracy (composite score: 2.514) and outperformed nurse triage and other AI models.", "conclusion": "LLM-based AI models show promise for enhancing ED triage accuracy and efficiency, though ethical and integration challenges remain."}}
{"id": "2507.01463", "pdf": "https://arxiv.org/pdf/2507.01463", "abs": "https://arxiv.org/abs/2507.01463", "authors": ["Max Gandyra", "Alessandro Santonicola", "Michael Beetz"], "title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": "10 pages, 3 figures, 3 tables, NeurIPS 2025 preprint", "summary": "Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed, for all kinds of novel\nobjects, without (re-) training, has proven to be a difficult task. To handle\nthis, we propose a simple, yet powerful, framework, called: Novel Object Cyclic\nThreshold based Instance Segmentation (NOCTIS). This work stems from and\nimproves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also\nleverages on recent vision foundation models, namely: Grounded-SAM 2 and\nDINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise\nbounding boxes and their corresponding segmentation masks; while DINOv2's\nzero-shot capabilities are employed to generate the image embeddings. The\nquality of those masks, together with their embeddings, is of vital importance\nto our approach; as the proposal-object matching is realized by determining an\nobject matching score based on the similarity of the class embeddings and the\naverage maximum similarity of the patch embeddings. Differently to SAM-6D,\ncalculating the latter involves a prior patch filtering based on the distance\nbetween each patch and its corresponding cyclic/roundtrip patch in the image\ngrid. Furthermore, the average confidence of the proposals' bounding box and\nmask is used as an additional weighting factor for the object matching score.\nWe empirically show that NOCTIS, without further training/fine tuning,\noutperforms the best RGB and RGB-D methods on the seven core datasets of the\nBOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\"\ntask.", "AI": {"tldr": "NOCTIS is a novel framework for instance segmentation of unseen objects using Grounded-SAM 2 and DINOv2, outperforming existing methods without retraining.", "motivation": "The challenge of segmenting novel objects without retraining drives the need for a generalizable model.", "method": "Leverages Grounded-SAM 2 for proposals and masks, and DINOv2 for embeddings, with cyclic threshold filtering and confidence weighting.", "result": "Outperforms top RGB and RGB-D methods on BOP 2023 datasets.", "conclusion": "NOCTIS provides a robust, training-free solution for novel object instance segmentation."}}
{"id": "2507.01810", "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "categories": ["cs.CL", "cs.IR"], "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "JSON is the most parseable format for structured outputs in clinical note extraction, with robustness improving via targeted prompting and larger models.", "motivation": "To compare parseability of JSON, YAML, and XML for open attribute-value extraction from clinical notes, aiding deployment in privacy-sensitive settings.", "method": "Evaluated three serialization formats (JSON, YAML, XML) using small language models, analyzing parseability, structural robustness, and failure patterns.", "result": "JSON consistently outperforms YAML and XML in parseability. Robustness improves with targeted prompting and larger models but declines for longer documents.", "conclusion": "JSON is recommended for clinical note extraction due to higher parseability, with insights on prompt design and model size for optimal performance."}}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316", "abs": "https://arxiv.org/abs/2507.00316", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "title": "$\u03bc^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasets demonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks. At the same time, for prompt engineering, we introduce a\nfive-stage, LLM-driven pipeline that converts routine CT reports into paired\nvisual-question-answer triples and citation-linked reasoning narratives,\ncreating a scalable, high-quality supervisory corpus for explainable multimodal\nradiology LLM. All code, datasets, and models will be publicly available in our\nofficial repository. https://github.com/Siyou-Li/u2Tokenizer", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.01241", "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "AI": {"tldr": "A stochastic conjugate subgradient method with adaptive sampling is proposed for training LLMs, outperforming traditional SGD in convergence and scalability.", "motivation": "Traditional SGD methods show limitations in large-scale LLM training, prompting the need for a more efficient approach.", "method": "Combines stochastic conjugate subgradient with adaptive sampling and AdamW-like step size adjustment to handle nonconvexity and non-smoothness.", "result": "Achieves faster convergence and better scalability than SGD, improving speed and accuracy in optimization.", "conclusion": "The proposed method effectively addresses SGD's limitations, enhancing LLM training performance."}}
{"id": "2507.01098", "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "AI": {"tldr": "The paper explains Ziyin et al.'s proof of the 'perfect' Platonic Representation Hypothesis (PRH) for embedded deep linear networks (EDLN), showing SGD leads to identical layer representations up to rotation, despite most global minima not being Platonic. It also links PRH to progressive sharpening and identifies six ways PRH can fail.", "motivation": "To clarify and detail Ziyin et al.'s proof of PRH in EDLNs, emphasizing the surprising role of SGD in achieving Platonic representations and exploring connections to other deep learning phenomena.", "method": "The paper elaborates on the proof for PRH in EDLNs, demonstrating how SGD training results in identical layer representations (up to rotation) and linking this to progressive sharpening. It also identifies conditions under which PRH fails.", "result": "SGD in EDLNs leads to perfectly Platonic representations despite most global minima not being Platonic. The proof reveals six failure modes for PRH and connects Platonic representations to progressive sharpening.", "conclusion": "The work underscores the role of SGD's irreversibility and emergent 'entropic forces' in representation learning, linking seemingly unrelated phenomena and providing insights into deep learning dynamics."}}
{"id": "2507.01467", "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "categories": ["cs.CV"], "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "AI": {"tldr": "REG improves diffusion models by entangling low-level image latents with a high-level class token, enhancing generation quality and training efficiency with minimal overhead.", "motivation": "Existing methods like REPA fail to fully utilize discriminative representations during denoising inference.", "method": "REG entangles image latents with a single high-level class token from pretrained models for denoising.", "result": "Achieves 63x and 23x faster training than baselines and outperforms REPA with 10x fewer iterations.", "conclusion": "REG efficiently leverages semantic knowledge to guide generation, significantly improving performance and training speed."}}
{"id": "2507.01844", "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "The paper introduces a method to analyze how LLMs replicate training data by focusing on low-perplexity sequences, revealing unexpected gaps in data mapping and quantifying verbatim recall.", "motivation": "Understanding how LLMs' training data influences their outputs is critical for transparency, accountability, privacy, and fairness.", "method": "A systematic pipeline extracts and analyzes low-perplexity sequences (high-probability text spans) to trace their origins in the training data.", "result": "Many low-perplexity sequences cannot be mapped to the training corpus, and for those that can, the study quantifies their distribution across source documents.", "conclusion": "The findings provide insights into verbatim recall in LLMs and highlight the need for better understanding of training data impacts on model behavior."}}
{"id": "2507.00373", "pdf": "https://arxiv.org/pdf/2507.00373", "abs": "https://arxiv.org/abs/2507.00373", "authors": ["Jian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "title": "Customizable ROI-Based Deep Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "AI": {"tldr": "A customizable ROI-based deep image compression method is proposed, allowing users to define ROI via text and manage quality trade-offs between ROI and non-ROI.", "motivation": "Existing ROI-based compression lacks flexibility for diverse user preferences and fixed ROI definitions.", "method": "Introduces Text-controlled Mask Acquisition (TMA) for ROI customization, Customizable Value Assign (CVA) for quality trade-offs, and Latent Mask Attention (LMA) for latent space optimization.", "result": "The method effectively supports customizable ROI definition and quality trade-off management.", "conclusion": "The proposed paradigm successfully addresses the need for flexible and customizable ROI-based image compression."}}
{"id": "2507.01264", "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "AI": {"tldr": "A framework using LLMs for few-shot code generation to automate diverse and safety-critical driving scenarios in CARLA, enhanced by realistic video generation.", "motivation": "To efficiently evaluate autonomous driving systems by generating diverse and safety-critical scenarios, including rare edge cases.", "method": "Leverages LLMs for few-shot code generation in CARLA, integrating a video generation pipeline (Cosmos-Transfer1 with ControlNet) for realism.", "result": "Successfully generates realistic, diverse, and safety-critical scenarios, including edge cases like pedestrian crossings under occlusion.", "conclusion": "The framework is effective for simulation-based testing of autonomous vehicles, offering controllability and realism."}}
{"id": "2507.01117", "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "categories": ["cs.LG", "68T07, 35A99"], "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy.", "AI": {"tldr": "A neural operator combining dynamic mode decomposition (DMD) and deep learning (DL) is proposed for efficient spatiotemporal modeling, reducing computational costs while maintaining accuracy.", "motivation": "Balancing lightweight and accurate computations in scientific computing, especially for solving PDEs with varying conditions.", "method": "Uses DMD and DL to extract key modes and dynamics, constructing predictions efficiently.", "result": "Outperforms DeepONet and FNO in accuracy for heat, Laplace, and Burgers equations.", "conclusion": "The method offers a computationally efficient and accurate alternative for PDE solutions."}}
{"id": "2507.01472", "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "categories": ["cs.CV", "cs.LG", "cs.PF"], "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "The paper proposes efficient, low-power algorithms (Mag1c-SAS and CEM) for onboard methane detection using hyperspectral satellite imagery, achieving significant speed improvements (100x-230x faster) while maintaining accuracy. It also introduces band selection strategies for faster processing.", "motivation": "Early detection of methane leaks via hyperspectral imagery is crucial for climate change mitigation, but existing methods are computationally demanding for onboard hardware.", "method": "The study tests fast target detection methods (ACE, CEM) and introduces Mag1c-SAS, a faster variant of Mag1c, integrating them with machine learning models (U-Net, LinkNet). It also evaluates three band selection strategies.", "result": "Mag1c-SAS and CEM are identified as promising candidates, balancing accuracy and speed (100x-230x faster than Mag1c). One band selection strategy outperforms traditional methods with fewer channels.", "conclusion": "The research advances onboard methane detection with minimal hardware requirements, enabling timely data delivery. All code, data, and models are open-sourced."}}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "EKA-EVAL is a unified, multilingual evaluation framework for LLMs, integrating 35+ benchmarks, including 10 Indic-specific datasets, with features like distributed inference and multi-GPU support.", "motivation": "Address the lack of non-English-centric evaluation frameworks for LLMs, especially for linguistically diverse regions like India.", "method": "Developed EKA-EVAL, a production-ready framework with broad benchmark coverage, distributed inference, quantization, and multi-GPU support.", "result": "EKA-EVAL is the first end-to-end, extensible evaluation suite for global and Indic LLMs, lowering multilingual benchmarking barriers.", "conclusion": "EKA-EVAL is open-source and part of the EKA initiative, aiming to expand to 100+ benchmarks for a robust multilingual LLM evaluation ecosystem."}}
{"id": "2507.00490", "pdf": "https://arxiv.org/pdf/2507.00490", "abs": "https://arxiv.org/abs/2507.00490", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "title": "Just Noticeable Difference for Large Multimodal Models", "categories": ["cs.CV", "eess.IV"], "comment": "19 pages, 19 figures", "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "AI": {"tldr": "The paper introduces LMM-JND, a concept to quantify visual blind spots in large multimodal models (LMMs), and presents VPA-JND, a dataset to study these limitations. It reveals gaps between LMMs and human visual performance.", "motivation": "To address the lack of systematic exploration of perceptual boundaries in LMMs and uncover their visual blind spots, which pose security and efficiency risks.", "method": "Proposes LMM-JND and its pipeline, constructs the VPA-JND dataset (21.5k images, 489k stimuli, 12 distortion types), and evaluates LMMs like GPT-4o and InternVL2.5.", "result": "Identifies significant visual blind spots in LMMs, showing their inferiority to human performance in basic comparison tasks. Also finds correlations between vision/language backbone designs and visual acuity.", "conclusion": "Highlights LMM-JND as a key metric for studying LMMs, emphasizing its importance for security and future model refinement."}}
{"id": "2507.01271", "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.", "AI": {"tldr": "The paper introduces the PULSE protocol for evaluating unlearning in large multimodal models (LMMs), focusing on pre-trained knowledge unlearning and long-term sustainability, revealing limitations in existing methods.", "motivation": "Addressing the lack of practical evaluation frameworks for unlearning in LMMs, especially for realistic scenarios involving pre-trained knowledge and sequential unlearning requests.", "method": "Proposes the PULSE protocol, which evaluates unlearning across different knowledge acquisition phases (pre-training vs. fine-tuning) and sequential unlearning operations.", "result": "Existing methods struggle with pre-trained knowledge unlearning and show performance degradation in sequential unlearning scenarios.", "conclusion": "The PULSE protocol highlights gaps in current unlearning techniques for LMMs, emphasizing the need for more robust solutions."}}
{"id": "2507.01129", "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "categories": ["cs.LG", "cs.CR"], "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice.", "AI": {"tldr": "The paper critiques existing DP training methods for adaptive optimizers, proposing 'scale-then-privatize' as a superior alternative with better theoretical and empirical results.", "motivation": "Existing DP training methods for adaptive optimizers like AdaGrad and Adam perform poorly due to spherical noise, and their conclusions may not generalize to practical scenarios.", "method": "The paper surveys existing variants, develops theoretical insights, and empirically compares them, focusing on 'scale-then-privatize' as a key technique.", "result": "'Scale-then-privatize' outperforms other variants in small-scale language model training and aligns better with correlated noise mechanisms.", "conclusion": "Unbiased second-moment estimates in adaptive optimizers are misguided; 'scale-then-privatize' is more effective and theoretically sound."}}
{"id": "2507.01478", "pdf": "https://arxiv.org/pdf/2507.01478", "abs": "https://arxiv.org/abs/2507.01478", "authors": ["Chentao Shen", "Ding Pan", "Mingyu Mei", "Zaixing He", "Xinyue Zhao"], "title": "Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects", "categories": ["cs.CV"], "comment": "preprint version", "summary": "Visual pose tracking is playing an increasingly vital role in industrial\ncontexts in recent years. However, the pose tracking for industrial metal\nobjects remains a challenging task especially in the real world-environments,\ndue to the reflection characteristic of metal objects. To address this issue,\nwe propose a novel 6DoF pose tracking method based on active control points.\nThe method uses image control points to generate edge feature for optimization\nactively instead of 6DoF pose-based rendering, and serve them as optimization\nvariables. We also introduce an optimal control point regression method to\nimprove robustness. The proposed tracking method performs effectively in both\ndataset evaluation and real world tasks, providing a viable solution for\nreal-time tracking of industrial metal objects. Our source code is made\npublicly available at: https://github.com/tomatoma00/ACPTracking.", "AI": {"tldr": "A novel 6DoF pose tracking method for industrial metal objects using active control points to address reflection challenges.", "motivation": "Pose tracking for industrial metal objects is challenging due to reflections, requiring a robust solution.", "method": "Uses active control points to generate edge features for optimization, avoiding 6DoF pose-based rendering, and includes optimal control point regression for robustness.", "result": "Effective in dataset evaluation and real-world tasks, enabling real-time tracking of metal objects.", "conclusion": "The method provides a viable solution for real-time pose tracking of industrial metal objects, with publicly available source code."}}
{"id": "2507.01872", "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "DIY-MKG is an open-source system for polyglot language learning, addressing limitations of existing tools by enabling personalized vocabulary knowledge graphs, adaptive quizzes, and user feedback.", "motivation": "Existing tools lack support for polyglot learners, customization, and suffer from cognitive offloading.", "method": "DIY-MKG uses LLMs to build personalized vocabulary knowledge graphs, offers rich annotations, and generates adaptive quizzes with user feedback.", "result": "Evaluation shows reliable vocabulary expansion and highly accurate quizzes across multiple languages.", "conclusion": "DIY-MKG is robust and effective for polyglot language learning."}}
{"id": "2507.01274", "pdf": "https://arxiv.org/pdf/2507.01274", "abs": "https://arxiv.org/abs/2507.01274", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted and Presented at 11th International Maritime Science\n  Conference", "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges.", "AI": {"tldr": "An AI-driven framework enhances maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection, achieving high accuracy and surpassing benchmarks.", "motivation": "Traditional maritime training relies on subjective assessments, leading to challenges like subjectivity and cognitive limitations. This study aims to address these issues with AI.", "method": "The system integrates AI techniques: visual focus tracking (eye tracking, pupil dilation, computer vision), communication analysis (speech-to-text, NLP, LLMs), and stress detection (vocal pitch).", "result": "High accuracy was achieved: ~92% for visual detection, ~91% for speech recognition, and ~90% for stress detection, outperforming benchmarks.", "conclusion": "The study shows AI can transform maritime training by providing objective analytics, personalized feedback, and improved preparedness for real-world challenges."}}
{"id": "2507.01131", "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations.", "AI": {"tldr": "TDNs replace CG tensor products with low-rank decompositions to accelerate SO(3)-equivariant networks, achieving competitive performance with reduced computational cost.", "motivation": "The computational expense of Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks motivates the development of faster, approximately equivariant alternatives.", "method": "Tensor decomposition networks (TDNs) use low-rank decompositions (e.g., CP) and path-weight sharing to reduce complexity from O(L^6) to O(L^4).", "result": "TDNs achieve competitive performance on PubChemQCR, OC20, and OC22 datasets with significant computational speedup.", "conclusion": "TDNs offer a plug-and-play, efficient alternative to CG tensor products in equivariant networks without compromising performance."}}
{"id": "2507.01484", "pdf": "https://arxiv.org/pdf/2507.01484", "abs": "https://arxiv.org/abs/2507.01484", "authors": ["Xiaoshuai Hao", "Yuting Zhao", "Yuheng Ji", "Luanyuan Dai", "Peng Hao", "Dingzhe Li", "Shuai Cheng", "Rong Yin"], "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io.", "AI": {"tldr": "The paper proposes strategies to enhance the robustness of multi-modal fusion methods for HD map construction, focusing on data augmentation, a novel fusion module, and modality dropout training, achieving state-of-the-art results.", "motivation": "Existing methods prioritize accuracy over robustness, which is critical for real-world autonomous driving applications.", "method": "The approach includes data augmentation, a novel multi-modal fusion module, and modality dropout training, evaluated on 10 days of NuScenes data.", "result": "The methods significantly improve robustness and achieve state-of-the-art performance on the NuScenes dataset.", "conclusion": "The findings advance the development of robust and reliable HD map construction models for real-world autonomous driving."}}
{"id": "2507.01887", "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "MiCoTA improves small language models' reasoning by using intermediate-sized models as teacher assistants and intermediate-length reasoning sequences.", "motivation": "Address the 'SLMs Learnability Gap' where small language models struggle with long-form reasoning due to limited capacity.", "method": "Introduces MiCoTA, a framework using intermediate-sized models and sequences to bridge capacity and reasoning gaps.", "result": "SLMs show significant reasoning improvements, e.g., Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve score boosts of 3.47 and 3.93.", "conclusion": "MiCoTA effectively bridges gaps in SLMs' reasoning, paving the way for future research in long-CoT distillation."}}
{"id": "2507.01284", "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "AI": {"tldr": "VLAD integrates a fine-tuned VLM with VAD for autonomous driving, improving spatial reasoning and reducing collisions by 31.82%.", "motivation": "Enhance autonomous driving capabilities by leveraging open-source VLMs' general knowledge for perception, prediction, and planning.", "method": "Specialized fine-tuning of a VLM using custom QA datasets, integrated with VAD for navigational commands and explanations.", "result": "31.82% reduction in collision rates on the nuScenes dataset.", "conclusion": "VLAD sets a new benchmark for VLM-augmented autonomous driving systems with improved performance and transparency."}}
{"id": "2507.01132", "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "categories": ["cs.LG", "q-bio.MN"], "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges.", "AI": {"tldr": "SMH addresses imbalanced regression on graph-structured data by generating synthetic samples preserving topology and focusing on underrepresented target ranges, improving predictive performance.", "motivation": "The paper addresses the lack of research in imbalanced regression for graph-structured data, where domain preferences focus on scientifically valuable but underrepresented target ranges.", "method": "Proposes Spectral Manifold Harmonization (SMH), which generates synthetic graph samples preserving topological properties while targeting underrepresented regions of the target distribution.", "result": "SMH shows consistent improvements in predictive performance for target domain ranges on chemistry and drug discovery benchmarks.", "conclusion": "SMH effectively addresses imbalanced regression challenges in graph-structured data, outperforming conventional methods by focusing on underrepresented target ranges and preserving topology."}}
{"id": "2507.01492", "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "AI": {"tldr": "AVC-DPO enhances video MLLMs for human-preferred captioning by aligning preferences via optimized prompts and training.", "motivation": "Current video MLLMs struggle to adjust caption focus based on human preferences, especially for temporal dynamics and spatial information.", "method": "AVC-DPO uses enhanced prompts targeting human-centric preferences and leverages varied prompt conditions for preference-aware training.", "result": "Achieved top performance in the LOVE@CVPR'25 Workshop Track 1A, ranking first on the VDC benchmark.", "conclusion": "AVC-DPO effectively aligns video captions with human preferences, improving video MLLMs' captioning capabilities."}}
{"id": "2507.01900", "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "A novel pruning algorithm for LLMs strategically prunes attention heads in higher layers and uses adaptive rescaling to maintain representation quality, outperforming existing methods.", "motivation": "Conventional pruning methods remove attention heads indiscriminately, ignoring their positions in the network, which can harm performance.", "method": "Proposes a pruning algorithm targeting higher-layer attention heads and introduces adaptive rescaling to adjust representation scales post-pruning.", "result": "Outperforms existing structured pruning methods, especially in generation tasks, across 27 datasets and multiple LLMs.", "conclusion": "Strategic pruning and adaptive rescaling improve LLM compression and performance, particularly in generation tasks."}}
{"id": "2507.01313", "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models.", "AI": {"tldr": "The paper introduces a Neural Hamiltonian Operator (NHO) to solve high-dimensional stochastic control problems using deep learning, framing it as an operator learning task within Pontryagin's Maximum Principle (PMP).", "motivation": "High-dimensional stochastic control problems are challenging due to the curse of dimensionality, and traditional methods like dynamic programming are inefficient. PMP offers an alternative but requires solving complex FBSDEs.", "method": "The NHO parameterizes FBSDE dynamics via neural networks for feedback control and value function gradients. Training enforces PMP consistency conditions.", "result": "The NHO framework proves universal approximation capabilities under general martingale drivers and addresses optimization challenges.", "conclusion": "The NHO provides a rigorous, operator-theoretic approach to solving high-dimensional stochastic control problems with deep learning."}}
{"id": "2507.01154", "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.", "AI": {"tldr": "FlashDP introduces a cache-friendly per-layer DP-SGD method, reducing memory and computation overhead while maintaining accuracy in LLM training.", "motivation": "Privacy concerns in LLM training data necessitate efficient DP methods, but current approaches like DP-SGD face memory and computation inefficiencies.", "method": "FlashDP consolidates operations into a single task, calculating gradients once in a fused manner, reducing memory movement and redundant computations.", "result": "FlashDP reduces memory movement by 50% and redundant computations by 20%, achieving 90% throughput of Non-DP methods while maintaining accuracy.", "conclusion": "FlashDP is a significant advancement for efficient, privacy-preserving LLM training, with open-sourced code available."}}
{"id": "2507.01494", "pdf": "https://arxiv.org/pdf/2507.01494", "abs": "https://arxiv.org/abs/2507.01494", "authors": ["Muhammad Hassam Ejaz", "Muhammad Bilal", "Usman Habib"], "title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.", "AI": {"tldr": "A review of 37 studies (2018-2025) on AI-based pest classification, highlighting the shift from CNNs to hybrid/transformer models, key challenges, and future directions.", "motivation": "Traditional pest monitoring methods are slow and unscalable; AI offers automated, efficient solutions.", "method": "Analyzed 37 studies, organizing them by crop type, pest species, model architecture, dataset usage, and technical challenges.", "result": "Shift from CNNs to hybrid/transformer models improves accuracy and contextual understanding, but challenges like dataset imbalance and deployment hurdles persist.", "conclusion": "AI-based pest monitoring shows promise but requires addressing dataset, detection, and deployment challenges for broader adoption."}}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "The paper presents a comprehensive survey on AI for Research (AI4Research), addressing gaps in understanding and development by introducing a taxonomy, identifying research frontiers, and compiling resources.", "motivation": "The motivation stems from the lack of a unified survey on AI4Research despite advancements in AI and LLMs, which hampers progress in applying AI to scientific research.", "method": "The authors introduce a systematic taxonomy for AI4Research tasks, identify key research gaps, and compile multidisciplinary resources and tools.", "result": "The work provides a structured overview of AI4Research, highlights future directions, and offers accessible resources for the community.", "conclusion": "The survey aims to facilitate quick access to AI4Research resources and inspire innovative breakthroughs in the field."}}
{"id": "2507.01321", "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "AI": {"tldr": "The paper introduces the dual-learning hypothesis for LLMs, revealing their vulnerability to backdoor attacks in ICL, and proposes ICLShield, a defense mechanism that dynamically adjusts concept preference to mitigate attacks.", "motivation": "The adaptability and parameter-free nature of ICL in LLMs make them vulnerable to backdoor attacks, necessitating a robust defense mechanism.", "method": "The authors propose the dual-learning hypothesis and ICLShield, which dynamically adjusts the concept preference ratio using confidence and similarity scores to select clean demonstrations.", "result": "ICLShield outperforms existing methods by 26.02% on average and shows strong adaptability, even for closed-source models like GPT-4.", "conclusion": "The study highlights the vulnerability of ICL to backdoor attacks and successfully mitigates it with ICLShield, demonstrating superior defense performance."}}
{"id": "2507.01178", "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "AI": {"tldr": "Diffusion Explorer is an interactive tool designed to explain the geometric properties of diffusion models through 2D training and real-time observation of sampling dynamics.", "motivation": "Existing explanations of diffusion models are either too theoretical or overly focused on neural architectures, neglecting their rich geometric properties.", "method": "The tool allows users to train 2D diffusion models in a browser and observe their sampling process via interactive animations.", "result": "Diffusion Explorer provides an accessible way to visualize and understand the temporal dynamics of diffusion models.", "conclusion": "The open-source tool, with its live demo, effectively bridges the gap in explaining diffusion models' geometric properties."}}
{"id": "2507.01496", "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "categories": ["cs.CV"], "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "AI": {"tldr": "A new real-image editing method for ReFlow improves image quality and text alignment by leveraging mid-step latent and adapted attention, outperforming prior methods.", "motivation": "Adapting ReFlow for real-image editing is challenging despite its superiority in text-to-image tasks.", "method": "Analyzes intermediate representations of multimodal transformer blocks, extracts key features using mid-step latent, and adapts attention during injection.", "result": "Superior performance over nine baselines on two benchmarks, validated by human evaluations.", "conclusion": "The proposed training-free method enhances editability and alignment without requiring masks or source prompts."}}
{"id": "2507.01915", "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "GAPO, a novel RLHF method, aligns LLMs with diverse human preferences using multi-objective optimization, outperforming state-of-the-art methods.", "motivation": "Aligning LLMs with conflicting human preferences is challenging; GAPO addresses this by framing it as a multi-objective optimization problem.", "method": "GAPO employs multiple-gradient descent to balance conflicting objectives, while P-GAPO incorporates user preferences for Pareto solutions.", "result": "GAPO converges to Pareto optimal solutions and outperforms existing methods in helpfulness and harmlessness on Mistral-7B.", "conclusion": "GAPO effectively aligns LLMs with diverse preferences, offering a scalable solution for human value alignment."}}
{"id": "2507.01327", "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "The paper introduces APARL, a framework for detecting abnormal events in customer service dialogues, improving adaptability and OOD generalization.", "motivation": "The complexity of business data and dynamic customer interactions make abnormal event detection challenging, requiring strong OOD generalization for commercial value.", "method": "APARL uses a dual-loop dynamic curriculum learning architecture with large language models to progressively tackle harder samples.", "result": "APARL achieves a 17.19% F1 score improvement and 9.59% better OOD transferability in food delivery dialogue tasks.", "conclusion": "APARL offers a superior solution for industrial anomaly detection, enhancing operational efficiency and commercial benefits."}}
{"id": "2507.01201", "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "The paper introduces the Joint Autoencoder Modulator (JAM) framework to align disjoint vision and language representations, optimizing for mutual coherence while preserving modality-specific structures.", "motivation": "The Platonic Representation Hypothesis suggests that independently trained vision and language models may converge toward a shared statistical model of reality, prompting the need for explicit alignment methods.", "method": "The JAM framework jointly trains modality-specific autoencoders on latent representations of pre-trained models, using reconstruction and cross-modal objectives. Alignment is evaluated across objectives (contrastive loss variants, Spread loss), layer depth, and foundation model scale.", "result": "The lightweight Pareto-efficient JAM framework reliably induces alignment, even across frozen, independently trained representations.", "conclusion": "The work provides theoretical insight and practical methods for transforming unimodal foundations into specialist multimodal models."}}
{"id": "2507.01502", "pdf": "https://arxiv.org/pdf/2507.01502", "abs": "https://arxiv.org/abs/2507.01502", "authors": ["Ozan Durgut", "Beril Kallfelz-Sirmacek", "Cem Unsalan"], "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, journal manuscript", "summary": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.", "AI": {"tldr": "The paper proposes a rule-based approach combining traditional and deep learning methods to improve tree crown detection for forest monitoring.", "motivation": "Addressing global environmental issues like deforestation requires better forest monitoring, which can be automated using remote sensing and computer vision.", "method": "Integrates traditional methods for feature extraction and segmentation with deep learning for tree crown detection, followed by rule-based post-processing to enhance results.", "result": "The proposed method increases the number of detected tree crowns and is compared with existing approaches, highlighting its advantages and limitations.", "conclusion": "The rule-based integration of traditional and deep learning methods improves tree crown detection, offering a robust solution for forest monitoring."}}
{"id": "2507.01921", "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "Distilling reasoning traces from a teacher model (NaturalThoughts) improves student models' reasoning more effectively than reinforcement learning, with difficult examples being more sample-efficient.", "motivation": "To systematically study what kind of reasoning demonstrations from a teacher model most effectively improve student models' reasoning capabilities.", "method": "Curate high-quality \"NaturalThoughts\" by selecting reasoning traces from a strong teacher model, analyze factors affecting distillation, and evaluate on Llama and Qwen models.", "result": "NaturalThoughts outperforms existing datasets (e.g., OpenThoughts, LIMO) on STEM benchmarks like GPQA-Diamond, MMLU-Pro, and SuperGPQA.", "conclusion": "Selecting difficult, diverse reasoning examples is more sample-efficient for transferring reasoning skills, and scaling data size with random sampling provides steady gains."}}
{"id": "2507.01381", "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "AI": {"tldr": "DSAC-D, a distributional reinforcement learning algorithm, uses multimodal distributions to reduce bias in value function estimation and improve performance, achieving SOTA results in control tasks.", "motivation": "Traditional unimodal distributions in reinforcement learning cause bias in value function estimation, leading to poor performance.", "method": "Proposes DSAC-D with a multimodal distributional policy iteration framework, using diffusion models for accurate multi-peak distribution characterization.", "result": "Achieves SOTA performance in 9 control tasks, reduces estimation bias, and improves average return by over 10%. Real-world tests show accurate multimodal driving style representation.", "conclusion": "DSAC-D effectively addresses bias in value estimation and enables multimodal policy learning, outperforming existing methods."}}
{"id": "2507.01208", "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "AI": {"tldr": "The paper proposes using fast neural network techniques (Distilling and Pruning) to deploy Intrusion Detection Systems (IDS) on low-cost hardware like Raspberry Pi 4, achieving real-time performance with high accuracy.", "motivation": "Modern vehicles rely on automotive Ethernet, which is vulnerable to attacks like flow injection. Existing Deep Learning-based IDS require expensive hardware for real-time operation, limiting practicality.", "method": "The study evaluates and applies Distilling and Pruning techniques to optimize IDS models for deployment on low-cost platforms (e.g., Raspberry Pi 4).", "result": "The optimized models achieve intrusion detection times of 727 \u00b5s on Raspberry Pi 4, with an AUCROC of 0.9890.", "conclusion": "Fast neural network techniques enable efficient, real-time IDS deployment on affordable hardware, enhancing vehicle security."}}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "A novel framework, cRID, is introduced to detect and leverage textual describable PII in street-level datasets for improved person re-identification, addressing privacy risks.", "motivation": "Street-level datasets for autonomous driving and AI research contain PII, posing privacy risks for pedestrians. Current methods often overlook non-biometric PII.", "method": "Combines Large Vision-Language Models, Graph Attention Networks, and representation learning to detect and use interpretable PII features for Re-ID.", "result": "Improved performance in cross-dataset Re-ID scenarios, particularly from Market-1501 to CUHK03-np.", "conclusion": "cRID effectively addresses privacy risks by detecting meaningful PII and enhancing Re-ID, with practical utility demonstrated."}}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "The paper proposes a decision-oriented framework for evaluating NLG by measuring its impact on human and LLM decision outcomes, showing traditional metrics are inadequate.", "motivation": "Current intrinsic NLG evaluation methods poorly correlate with real-world decision-making efficacy, necessitating a more practical approach.", "method": "A framework assessing decision quality via financial performance of trades by humans and LLMs, using market digest texts as test cases.", "result": "Neither humans nor LLMs outperform random baselines with summaries alone, but human-LLM collaboration excels with richer analyses.", "conclusion": "Decision-oriented evaluation is crucial for NLG, revealing limitations of traditional metrics and the potential of human-LLM synergy."}}
{"id": "2507.01411", "pdf": "https://arxiv.org/pdf/2507.01411", "abs": "https://arxiv.org/abs/2507.01411", "authors": ["Yifei Sun", "Marshall A. Dalton", "Robert D. Sanders", "Yixuan Yuan", "Xiang Li", "Sharon L. Naismith", "Fernando Calamante", "Jinglei Lv"], "title": "Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Grey matter loss in the hippocampus is a hallmark of neurobiological aging,\nyet understanding the corresponding changes in its functional connectivity\nremains limited. Seed-based functional connectivity (FC) analysis enables\nvoxel-wise mapping of the hippocampus's synchronous activity with cortical\nregions, offering a window into functional reorganization during aging. In this\nstudy, we develop an interpretable deep learning framework to predict brain age\nfrom hippocampal FC using a three-dimensional convolutional neural network (3D\nCNN) combined with LayerCAM saliency mapping. This approach maps key\nhippocampal-cortical connections, particularly with the precuneus, cuneus,\nposterior cingulate cortex, parahippocampal cortex, left superior parietal\nlobule, and right superior temporal sulcus, that are highly sensitive to age.\nCritically, disaggregating anterior and posterior hippocampal FC reveals\ndistinct mapping aligned with their known functional specializations. These\nfindings provide new insights into the functional mechanisms of hippocampal\naging and demonstrate the power of explainable deep learning to uncover\nbiologically meaningful patterns in neuroimaging data.", "AI": {"tldr": "A deep learning framework predicts brain age from hippocampal functional connectivity, revealing age-sensitive connections and distinct anterior-posterior hippocampal patterns.", "motivation": "To understand functional connectivity changes in the hippocampus during aging, which remains limited despite known grey matter loss.", "method": "A 3D CNN combined with LayerCAM saliency mapping was used to analyze hippocampal FC and predict brain age.", "result": "Identified key hippocampal-cortical connections sensitive to age, with distinct patterns for anterior and posterior hippocampus.", "conclusion": "The study offers insights into hippocampal aging mechanisms and highlights the utility of explainable deep learning in neuroimaging."}}
{"id": "2507.01216", "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "AI": {"tldr": "PAE MobiLLM is a privacy-aware, efficient method for fine-tuning large language models on mobile devices using server-assisted side-tuning, activation caching, and one-token activation shortcuts to reduce communication and computational costs.", "motivation": "Addressing the gap between mobile device resource limitations and the need for on-device LLM fine-tuning, while ensuring privacy and efficiency.", "method": "Uses server-assisted additive side-tuning, activation caching, one-token activation shortcuts, and additive adapter side-network design.", "result": "Reduces communication burden, accelerates fine-tuning convergence, and ensures privacy by preventing server access to raw data or labels.", "conclusion": "PAE MobiLLM effectively balances privacy, efficiency, and resource constraints for mobile LLM fine-tuning."}}
{"id": "2507.01509", "pdf": "https://arxiv.org/pdf/2507.01509", "abs": "https://arxiv.org/abs/2507.01509", "authors": ["Tapas K. Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 2 figures, MICCAI-2025", "summary": "Polyp segmentation in colonoscopy images is crucial for early detection and\ndiagnosis of colorectal cancer. However, this task remains a significant\nchallenge due to the substantial variations in polyp shape, size, and color, as\nwell as the high similarity between polyps and surrounding tissues, often\ncompounded by indistinct boundaries. While existing encoder-decoder CNN and\ntransformer-based approaches have shown promising results, they struggle with\nstable segmentation performance on polyps with weak or blurry boundaries. These\nmethods exhibit limited abilities to distinguish between polyps and non-polyps\nand capture essential boundary cues. Moreover, their generalizability still\nfalls short of meeting the demands of real-time clinical applications. To\naddress these limitations, we propose SAM-MaGuP, a groundbreaking approach for\nrobust polyp segmentation. By incorporating a boundary distillation module and\na 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels\nat resolving weak boundary challenges and amplifies feature learning through\nenriched global contextual interactions. Extensive evaluations across five\ndiverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,\nachieving unmatched segmentation accuracy and robustness. Our key innovations,\na Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in\nthe field, pushing the boundaries of polyp segmentation to new heights.", "AI": {"tldr": "SAM-MaGuP enhances polyp segmentation by integrating boundary distillation and a 1D-2D Mamba adapter into SAM, outperforming existing methods in accuracy and robustness.", "motivation": "Current polyp segmentation methods struggle with weak boundaries and poor generalizability, limiting clinical application.", "method": "Proposes SAM-MaGuP, combining boundary distillation and a 1D-2D Mamba adapter within SAM for improved feature learning and boundary resolution.", "result": "Outperforms state-of-the-art methods across five datasets, achieving superior segmentation accuracy and robustness.", "conclusion": "SAM-MaGuP sets a new benchmark in polyp segmentation, addressing key challenges like weak boundaries and generalizability."}}
{"id": "2507.01936", "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "LLMs excel in persuasive debates but lack deeper comprehension of dialogue structures, impacting their reliability as evaluators.", "motivation": "To assess LLMs' debate skills and their understanding of dialogue, given their increasing use in sensitive applications.", "method": "Evaluated LLMs' debate performance and measured their comprehension of dialogical structures and pragmatic context.", "result": "LLMs can debate persuasively but fail to demonstrate deeper understanding of dialogue. Awareness of AI involvement increases skepticism.", "conclusion": "Persuasive ability in LLMs doesn't require deep comprehension, suggesting pragmatic context is secondary to effectiveness in dialogue."}}
{"id": "2507.01413", "pdf": "https://arxiv.org/pdf/2507.01413", "abs": "https://arxiv.org/abs/2507.01413", "authors": ["Kushal Agrawal", "Verona Teo", "Juan J. Vazquez", "Sudarsh Kunnavakkam", "Vishak Srikanth", "Andy Liu"], "title": "Evaluating LLM Agent Collusion in Double Auctions", "categories": ["cs.GT", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities as\nautonomous agents with rapidly expanding applications in various domains. As\nthese agents increasingly engage in socioeconomic interactions, identifying\ntheir potential for undesirable behavior becomes essential. In this work, we\nexamine scenarios where they can choose to collude, defined as secretive\ncooperation that harms another party. To systematically study this, we\ninvestigate the behavior of LLM agents acting as sellers in simulated\ncontinuous double auction markets. Through a series of controlled experiments,\nwe analyze how parameters such as the ability to communicate, choice of model,\nand presence of environmental pressures affect the stability and emergence of\nseller collusion. We find that direct seller communication increases collusive\ntendencies, the propensity to collude varies across models, and environmental\npressures, such as oversight and urgency from authority figures, influence\ncollusive behavior. Our findings highlight important economic and ethical\nconsiderations for the deployment of LLM-based market agents.", "AI": {"tldr": "The paper investigates collusion tendencies in LLM agents acting as sellers in simulated markets, finding communication, model choice, and environmental pressures influence collusion.", "motivation": "To identify potential undesirable behaviors of LLM agents in socioeconomic interactions, focusing on collusion.", "method": "Simulated continuous double auction markets with controlled experiments varying communication, model choice, and environmental pressures.", "result": "Direct communication increases collusion, model choice affects collusion propensity, and environmental pressures influence behavior.", "conclusion": "The study underscores economic and ethical concerns for deploying LLM-based market agents."}}
{"id": "2507.01235", "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "categories": ["cs.LG", "quant-ph"], "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "Quantum machine learning models (QSVM and QNN) were tested for classifying pedestrian stress from SCR data. QNN outperformed QSVM and classical methods.", "motivation": "To leverage quantum computing for complex machine learning tasks, specifically modeling pedestrian stress in intelligent transportation systems.", "method": "Developed QSVM and QNN models using an eight-qubit ZZ feature map on Pennylane, tested on SCR data with amplitude-based classes.", "result": "QSVM had 45% test accuracy (overfitting), while QNN achieved 55%, outperforming classical methods.", "conclusion": "QNN is more reliable for classifying pedestrian stress, demonstrating quantum computing's potential in machine learning."}}
{"id": "2507.01532", "pdf": "https://arxiv.org/pdf/2507.01532", "abs": "https://arxiv.org/abs/2507.01532", "authors": ["Tomas Zelezny", "Jakub Straka", "Vaclav Javorek", "Ondrej Valach", "Marek Hruz", "Ivan Gruber"], "title": "Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, supplementary, SLRTP2025, CVPR2025", "summary": "Sign Language Translation (SLT) has evolved significantly, moving from\nisolated recognition approaches to complex, continuous gloss-free translation\nsystems. This paper explores the impact of pose-based data preprocessing\ntechniques - normalization, interpolation, and augmentation - on SLT\nperformance. We employ a transformer-based architecture, adapting a modified T5\nencoder-decoder model to process pose representations. Through extensive\nablation studies on YouTubeASL and How2Sign datasets, we analyze how different\npreprocessing strategies affect translation accuracy. Our results demonstrate\nthat appropriate normalization, interpolation, and augmentation techniques can\nsignificantly improve model robustness and generalization abilities.\nAdditionally, we provide a deep analysis of the model's attentions and reveal\ninteresting behavior suggesting that adding a dedicated register token can\nimprove overall model performance. We publish our code on our GitHub\nrepository, including the preprocessed YouTubeASL data.", "AI": {"tldr": "The paper investigates how pose-based data preprocessing (normalization, interpolation, augmentation) impacts Sign Language Translation (SLT) performance using a transformer-based model, showing significant improvements in robustness and generalization.", "motivation": "To enhance SLT systems by optimizing pose-based data preprocessing techniques for better translation accuracy.", "method": "Uses a modified T5 encoder-decoder transformer model on pose representations, with ablation studies on YouTubeASL and How2Sign datasets.", "result": "Appropriate preprocessing techniques improve model robustness and generalization; a dedicated register token further boosts performance.", "conclusion": "Optimized preprocessing and a register token enhance SLT performance, with code and preprocessed data made publicly available."}}
{"id": "2507.01049", "pdf": "https://arxiv.org/pdf/2507.01049", "abs": "https://arxiv.org/abs/2507.01049", "authors": ["Pranav Jadhav"], "title": "Cohort Retrieval using Dense Passage Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Patient cohort retrieval is a pivotal task in medical research and clinical\npractice, enabling the identification of specific patient groups from extensive\nelectronic health records (EHRs). In this work, we address the challenge of\ncohort retrieval in the echocardiography domain by applying Dense Passage\nRetrieval (DPR), a prominent methodology in semantic search. We propose a\nsystematic approach to transform an echocardiographic EHR dataset of\nunstructured nature into a Query-Passage dataset, framing the problem as a\nCohort Retrieval task. Additionally, we design and implement evaluation metrics\ninspired by real-world clinical scenarios to rigorously test the models across\ndiverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding\nmodel that demonstrates superior performance compared to traditional and\noff-the-shelf SOTA methods.To our knowledge, this is the first work to apply\nDPR for patient cohort retrieval in the echocardiography domain, establishing a\nframework that can be adapted to other medical domains.", "AI": {"tldr": "The paper proposes using Dense Passage Retrieval (DPR) for patient cohort retrieval in echocardiography, transforming unstructured EHR data into a Query-Passage dataset and introducing custom evaluation metrics. Their custom-trained DPR model outperforms existing methods.", "motivation": "Patient cohort retrieval is crucial in medical research and practice, but unstructured EHR data in echocardiography poses challenges. The study aims to address this by applying DPR.", "method": "The approach involves converting unstructured EHR data into a Query-Passage dataset, framing the problem as Cohort Retrieval, and designing clinical scenario-inspired evaluation metrics. A custom-trained DPR model is implemented.", "result": "The custom DPR model shows superior performance over traditional and off-the-shelf state-of-the-art methods.", "conclusion": "This is the first application of DPR for cohort retrieval in echocardiography, offering a adaptable framework for other medical domains."}}
{"id": "2507.01418", "pdf": "https://arxiv.org/pdf/2507.01418", "abs": "https://arxiv.org/abs/2507.01418", "authors": ["Inyoung Cheong", "Alicia Guo", "Mina Lee", "Zhehui Liao", "Kowe Kadoma", "Dongyoung Go", "Joseph Chee Chang", "Peter Henderson", "Mor Naaman", "Amy X. Zhang"], "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing", "categories": ["cs.CY", "cs.AI", "H.5.2; I.2"], "comment": "Presented at CHIWORK 2025 Workshop on Generative AI Disclosure,\n  Ownership, and Accountability in Co-Creative Domains", "summary": "As AI integrates in various types of human writing, calls for transparency\naround AI assistance are growing. However, if transparency operates on uneven\nground and certain identity groups bear a heavier cost for being honest, then\nthe burden of openness becomes asymmetrical. This study investigates how AI\ndisclosure statement affects perceptions of writing quality, and whether these\neffects vary by the author's race and gender. Through a large-scale controlled\nexperiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated\na single human-written news article while disclosure statements and author\ndemographics were systematically varied. This approach reflects how both human\nand algorithmic decisions now influence access to opportunities (e.g., hiring,\npromotion) and social recognition (e.g., content recommendation algorithms). We\nfind that both human and LLM raters consistently penalize disclosed AI use.\nHowever, only LLM raters exhibit demographic interaction effects: they favor\narticles attributed to women or Black authors when no disclosure is present.\nBut these advantages disappear when AI assistance is revealed. These findings\nilluminate the complex relationships between AI disclosure and author identity,\nhighlighting disparities between machine and human evaluation patterns.", "AI": {"tldr": "The study examines how AI disclosure affects perceptions of writing quality, revealing disparities in human and LLM rater evaluations based on author race and gender.", "motivation": "To understand the asymmetric burden of AI transparency and its impact on different identity groups.", "method": "A controlled experiment with human and LLM raters evaluating a news article while varying disclosure statements and author demographics.", "result": "Both human and LLM raters penalize disclosed AI use, but only LLMs show demographic biases, favoring women or Black authors without disclosure.", "conclusion": "The findings highlight disparities in AI disclosure effects and evaluation patterns between humans and machines."}}
{"id": "2507.01285", "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "categories": ["cs.LG", "cs.DC", "cs.IR"], "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.", "AI": {"tldr": "Dist-FedAvg is a distance-based aggregation method for graph federated recommendation systems, improving personalization and efficiency by weighting similar user embeddings and preserving anchor user influence.", "motivation": "Traditional aggregation methods in federated learning overlook user embedding complexity and user similarity, limiting recommendation effectiveness and adaptability to evolving interactions.", "method": "Introduces Dist-FedAvg, which assigns higher weights to similar user embeddings and ensures anchor users retain influence in local updates.", "result": "Empirical evaluations show Dist-FedAvg outperforms baselines, enhancing recommendation accuracy while integrating seamlessly into federated frameworks.", "conclusion": "Dist-FedAvg addresses limitations of existing methods, offering improved personalization and efficiency in graph federated recommendation systems."}}
{"id": "2507.01535", "pdf": "https://arxiv.org/pdf/2507.01535", "abs": "https://arxiv.org/abs/2507.01535", "authors": ["Bingxi Liu", "Calvin Chen", "Junhao Li", "Guyang Yu", "Haoqian Song", "Xuchen Liu", "Jinqiang Cui", "Hong Zhang"], "title": "TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking", "categories": ["cs.CV"], "comment": "12 pages", "summary": "The Vision Transformer (ViT) model has long struggled with the challenge of\nquadratic complexity, a limitation that becomes especially critical in unmanned\naerial vehicle (UAV) tracking systems, where data must be processed in real\ntime. In this study, we explore the recently proposed State-Space Model, Mamba,\nleveraging its computational efficiency and capability for long-sequence\nmodeling to effectively process dense image sequences in tracking tasks. First,\nwe highlight the issue of temporal inconsistency in existing Mamba-based\nmethods, specifically the failure to account for temporal continuity in the\nMamba scanning mechanism. Secondly, building upon this insight,we propose\nTrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model\nfor handling image sequence of tracking problem. In our framework, the mamba\nscan is performed in a nested way while independently process temporal and\nspatial coherent patch tokens. While the template frame is encoded as query\ntoken and utilized for tracking in every scan. Extensive experiments conducted\non five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves\nstate-of-the-art precision while offering noticeable higher speed in UAV\ntracking.", "AI": {"tldr": "TrackingMiM, a Mamba-in-Mamba architecture, addresses ViT's quadratic complexity in UAV tracking by leveraging efficient long-sequence modeling and temporal-spatial coherence.", "motivation": "Quadratic complexity of ViT hinders real-time UAV tracking; Mamba's efficiency and long-sequence capability offer a solution.", "method": "Proposes TrackingMiM, a nested Mamba scan for temporal-spatial coherence, with template frames as query tokens.", "result": "Achieves state-of-the-art precision and higher speed on five UAV tracking benchmarks.", "conclusion": "TrackingMiM effectively balances accuracy and computational efficiency for real-time UAV tracking."}}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548", "abs": "https://arxiv.org/abs/2507.01548", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.", "AI": {"tldr": "Older migrants in urban China used AI-assisted co-creation to express fragmented narratives through storytelling and Hanzi reconstruction, facilitated by soft AI and human support.", "motivation": "To address the underrepresentation and verbalization challenges of personal narratives among aging migrants.", "method": "Pilot workshop combining oral storytelling, symbolic Hanzi reconstruction with LLM-suggested Xiaozhuan glyphs, and physical materials, supported by human facilitation and soft AI.", "result": "Participants transformed lived experiences into visual/tactile expressions without digital literacy, showcasing human-AI collaboration.", "conclusion": "Repositioning AI as a supportive mechanism enhances narrative agency and offers new perspectives on aging and sociotechnical systems."}}
{"id": "2507.01429", "pdf": "https://arxiv.org/pdf/2507.01429", "abs": "https://arxiv.org/abs/2507.01429", "authors": ["Benjamin Chen Ming Choong", "Tao Luo", "Cheng Liu", "Bingsheng He", "Wei Zhang", "Joey Tianyi Zhou"], "title": "Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems", "categories": ["cs.ET", "cs.AI", "cs.AR"], "comment": null, "summary": "Deep neural networks generate and process large volumes of data, posing\nchallenges for low-resource embedded systems. In-memory computing has been\ndemonstrated as an efficient computing infrastructure and shows promise for\nembedded AI applications. Among newly-researched memory technologies, racetrack\nmemory is a non-volatile technology that allows high data density fabrication,\nmaking it a good fit for in-memory computing. However, integrating in-memory\narithmetic circuits with memory cells affects both the memory density and power\nefficiency. It remains challenging to build efficient in-memory arithmetic\ncircuits on racetrack memory within area and energy constraints. To this end,\nwe present an efficient in-memory convolutional neural network (CNN)\naccelerator optimized for use with racetrack memory. We design a series of\nfundamental arithmetic circuits as in-memory computing cells suited for\nmultiply-and-accumulate operations. Moreover, we explore the design space of\nracetrack memory based systems and CNN model architectures, employing co-design\nto improve the efficiency and performance of performing CNN inference in\nracetrack memory while maintaining model accuracy. Our designed circuits and\nmodel-system co-optimization strategies achieve a small memory bank area with\nsignificant improvements in energy and performance for racetrack memory based\nembedded systems.", "AI": {"tldr": "The paper presents an efficient in-memory CNN accelerator optimized for racetrack memory, addressing challenges in memory density and power efficiency for embedded AI systems.", "motivation": "Deep neural networks generate large data volumes, challenging low-resource embedded systems. In-memory computing, especially with racetrack memory, offers promise but faces integration hurdles.", "method": "Designs in-memory arithmetic circuits for multiply-and-accumulate operations and co-optimizes racetrack memory systems with CNN architectures.", "result": "Achieves small memory bank area with significant energy and performance improvements for racetrack memory-based embedded systems.", "conclusion": "The proposed circuits and co-design strategies enhance efficiency and performance while maintaining model accuracy in embedded AI applications."}}
{"id": "2507.01354", "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.", "AI": {"tldr": "The paper introduces the Wavelet Diffusion Model (WDM) for downscaling precipitation data from 10 km to 1 km resolution, offering improved accuracy and speed over existing methods.", "motivation": "Standard global precipitation data (e.g., IMERG) lacks fine-scale resolution needed for hydrological modeling and extreme weather analysis.", "method": "WDM is a conditional diffusion model that operates in the wavelet domain, focusing on high-frequency coefficients to generate detailed 1-km precipitation fields.", "result": "WDM achieves 10x spatial super-resolution, 9x faster inference than pixel-based models, and produces realistic results with fewer artifacts.", "conclusion": "WDM addresses accuracy and speed challenges in geoscience super-resolution, enhancing hydrological forecasting reliability."}}
{"id": "2507.01539", "pdf": "https://arxiv.org/pdf/2507.01539", "abs": "https://arxiv.org/abs/2507.01539", "authors": ["Mohammadreza Amirian", "Michael Bach", "Oscar Jimenez-del-Toro", "Christoph Aberle", "Roger Schaer", "Vincent Andrearczyk", "Jean-F\u00e9lix Maestrati", "Maria Martin Asiain", "Kyriakos Flouris", "Markus Obmann", "Clarisse Dromain", "Beno\u00eet Dufour", "Pierre-Alexandre Alois Poletti", "Hendrik von Tengg-Kobligk", "Rolf H\u00fcgli", "Martin Kretzschmar", "Hatem Alkadhi", "Ender Konukoglu", "Henning M\u00fcller", "Bram Stieltjes", "Adrien Depeursinge"], "title": "A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has introduced numerous opportunities for human\nassistance and task automation in medicine. However, it suffers from poor\ngeneralization in the presence of shifts in the data distribution. In the\ncontext of AI-based computed tomography (CT) analysis, significant data\ndistribution shifts can be caused by changes in scanner manufacturer,\nreconstruction technique or dose. AI harmonization techniques can address this\nproblem by reducing distribution shifts caused by various acquisition settings.\nThis paper presents an open-source benchmark dataset containing CT scans of an\nanthropomorphic phantom acquired with various scanners and settings, which\npurpose is to foster the development of AI harmonization techniques. Using a\nphantom allows fixing variations attributed to inter- and intra-patient\nvariations. The dataset includes 1378 image series acquired with 13 scanners\nfrom 4 manufacturers across 8 institutions using a harmonized protocol as well\nas several acquisition doses. Additionally, we present a methodology, baseline\nresults and open-source code to assess image- and feature-level stability and\nliver tissue classification, promoting the development of AI harmonization\nstrategies.", "AI": {"tldr": "The paper introduces an open-source benchmark dataset for AI harmonization in CT analysis, addressing data distribution shifts caused by scanner variations. It includes 1378 phantom scans and provides baseline methods for stability and classification.", "motivation": "AI in medicine faces poor generalization due to data distribution shifts from scanner variations. Harmonization techniques can mitigate this, but lack standardized benchmarks.", "method": "A phantom-based dataset (1378 scans from 13 scanners) was created to eliminate patient variability. Baseline methods for image/feature stability and liver classification were developed.", "result": "The dataset and baseline methods enable standardized testing of AI harmonization techniques, promoting robustness in CT analysis.", "conclusion": "This work provides a valuable resource for advancing AI harmonization in medical imaging, addressing key challenges in generalization."}}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "SPRO is a novel framework for process-aware RL in LLMs, eliminating the need for external reward models by deriving rewards intrinsically and introducing step-wise advantage estimation. It improves efficiency and accuracy while reducing computational overhead.", "motivation": "Addressing the computational overhead and lack of theoretical framework for process-level advantage estimation in PRL for LLMs.", "method": "Proposes SPRO with intrinsic process rewards, cumulative process rewards, and Masked Step Advantage (MSA) for step-wise action advantage estimation.", "result": "SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% test accuracy improvement, stable policy entropy, and reduced response length.", "conclusion": "SPRO offers a computationally efficient and theoretically grounded solution for process-aware RL in LLMs, suitable for industrial implementation."}}
{"id": "2507.01438", "pdf": "https://arxiv.org/pdf/2507.01438", "abs": "https://arxiv.org/abs/2507.01438", "authors": ["Zheyu Shen", "Yexiao He", "Ziyao Wang", "Yuning Zhang", "Guoheng Sun", "Wanghao Ye", "Ang Li"], "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.", "AI": {"tldr": "EdgeLoRA is a system for efficiently serving fine-tuned LLMs on edge devices in multi-tenant settings, addressing challenges like adapter selection, memory overhead, and latency through adaptive mechanisms and batch processing.", "motivation": "To overcome inefficiencies in serving LLMs on resource-constrained edge devices, particularly in multi-tenant environments, where adapter selection, memory overhead, and sequential processing hinder performance.", "method": "EdgeLoRA introduces adaptive adapter selection, heterogeneous memory management (caching and pooling), and batch LoRA inference to optimize performance.", "result": "EdgeLoRA achieves up to 4x higher throughput and supports significantly more adapters simultaneously compared to llama.cpp, reducing latency and improving scalability.", "conclusion": "EdgeLoRA provides a scalable and efficient solution for deploying LLMs on edge devices in multi-tenant scenarios, enhancing performance and resource utilization."}}
{"id": "2507.01389", "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages.", "AI": {"tldr": "An enhanced surrogate model integrates slack variables into a factorization machine and its Ising representation, unifying a two-step process into one, improving performance in predicting drug combination effects.", "motivation": "To improve the efficiency and accuracy of surrogate models by incorporating slack variables for higher-order feature interactions, inspired by a prior quantum annealing-based approach.", "method": "The model adds slack variables to the factorization machine and its Ising representation, iteratively updating them during training to capture higher-order interactions.", "result": "Experimental results show notable performance improvement in predicting drug combination effects.", "conclusion": "The proposed algorithm is promising for efficient surrogate models, leveraging quantum advantages."}}
{"id": "2507.01557", "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "Proposes noise reduction methods for event cameras using IIR filters, achieving ~99% noise removal with low memory usage.", "motivation": "Event cameras produce noisy data streams, limiting their application potential.", "method": "Four algorithms based on IIR filters, tested on modified event datasets with artificial and recorded noise.", "result": "~99% noise removal, preserving valid signal; uses ~30KB memory for 1280x720 resolution.", "conclusion": "The methods are efficient and suitable for embedded devices."}}
{"id": "2507.01599", "pdf": "https://arxiv.org/pdf/2507.01599", "abs": "https://arxiv.org/abs/2507.01599", "authors": ["Zhaoyan Sun", "Jiayi Wang", "Xinyang Zhao", "Jiachi Wang", "Guoliang Li"], "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.", "AI": {"tldr": "The paper proposes 'Data Agents' to automate and enhance Data+AI systems by leveraging LLMs for semantic understanding, reasoning, and planning, addressing current limitations in pipeline orchestration.", "motivation": "Existing Data+AI systems rely heavily on human experts for pipeline orchestration due to limited semantic understanding and planning capabilities. LLMs offer potential to overcome these challenges.", "method": "Introduces the 'Data Agent' architecture, integrating LLMs for knowledge comprehension, reasoning, and planning to automate data-related tasks and pipeline orchestration.", "result": "Examples of data agent systems (e.g., data science agent, DBA agent) demonstrate practical applications, though open challenges remain.", "conclusion": "Data Agents, powered by LLMs, can revolutionize Data+AI ecosystems, but further research is needed to address design challenges."}}
{"id": "2507.01457", "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.", "AI": {"tldr": "The paper introduces a TVM compiler-based workflow to optimize AI workloads for RISC-V vector units, outperforming GCC autovectorization and muRISCV-NN in latency and code size.", "motivation": "Efficiently utilizing RISC-V Vector Extension (RVV) for AI workloads without expert knowledge is challenging due to lack of autotuning frameworks integrated with RVV.", "method": "Integrated RVV into TVM's MetaSchedule framework, implemented RISC-V SoCs on FPGA, and tuned AI workloads.", "result": "Achieved 46% latency improvement over GCC autovectorization, 29% over muRISCV-NN, and 35% over LLVM on a commercial RISC-V SoC, with smaller code footprint.", "conclusion": "The proposed solution effectively optimizes AI workloads for RISC-V RVV, offering significant performance gains and suitability for embedded devices, and is open-sourced for community expansion."}}
{"id": "2507.01414", "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "categories": ["cs.LG"], "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance.", "AI": {"tldr": "The paper introduces a toy problem combining linear-regression-style ICL with discrete associative recall, analyzing transformer models' ability to recall and predict states. Two distinct mechanisms emerge: one for associative recall and another for Bayesian-style prediction, with different learning dynamics.", "motivation": "To understand how transformer models perform associative recall and prediction in a controlled setting, revealing distinct mechanisms and their learning dynamics.", "method": "Pretrain transformer models on symbolic traces from linear dynamical systems, analyze recall and prediction capabilities, and conduct out-of-distribution and mechanistic analyses.", "result": "Two separate mechanisms emerge: one for associative recall using symbolic labels, and another for Bayesian-style prediction. The latter develops earlier.", "conclusion": "The findings suggest multi-mechanism learning dynamics, confirmed by similar observations in a translation task, indicating broader applicability."}}
{"id": "2507.01573", "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "The paper proposes IDGBR, a framework combining discriminative and generative learning to refine boundaries in remote sensing semantic segmentation, addressing limitations in high-frequency feature learning.", "motivation": "Existing segmentation models focus on low-frequency features (semantic correctness) but struggle with high-frequency details (boundary localization). Diffusion models excel at high-frequency details but lack semantic inference for low-frequency features.", "method": "IDGBR integrates discriminative learning (for coarse segmentation) and diffusion-based generative learning (for boundary refinement). A conditioning guidance network uses the coarse map and original image to guide an iterative denoising process.", "result": "Experiments on five datasets show IDGBR consistently refines boundaries in coarse results from various discriminative architectures.", "conclusion": "IDGBR effectively combines discriminative and generative learning to improve boundary refinement in semantic segmentation, validated across multiple datasets."}}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "Prefix-RFT combines SFT and RFT, outperforming both and offering a unified, robust approach for LLM post-training.", "motivation": "Address the trade-offs of SFT (generalization issues) and RFT (unexpected behaviors, sensitivity to initial policy) by unifying them.", "method": "Introduces Prefix-RFT, a hybrid approach integrating demonstration (SFT) and exploration (RFT), tested on mathematical reasoning.", "result": "Prefix-RFT outperforms standalone SFT and RFT, is robust to data variations, and integrates easily into existing frameworks.", "conclusion": "A unified paradigm integrating demonstration and exploration is promising for future LLM post-training research."}}
{"id": "2507.01462", "pdf": "https://arxiv.org/pdf/2507.01462", "abs": "https://arxiv.org/abs/2507.01462", "authors": ["Eneko Osaba", "Estibaliz Garrote", "Pablo Miranda-Rodriguez", "Alessia Ciacco", "Itziar Cabanes", "Aitziber Mancisidor"], "title": "Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0", "categories": ["cs.RO", "cs.AI", "cs.ET"], "comment": "2 pages, 1 figure, paper accepted for presentation at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "summary": "This work explores the application of hybrid quantum-classical algorithms to\noptimize robotic inspection trajectories derived from Computer-Aided Design\n(CAD) models in industrial settings. By modeling the task as a 3D variant of\nthe Traveling Salesman Problem, incorporating incomplete graphs and open-route\nconstraints, this study evaluates the performance of two D-Wave-based solvers\nagainst classical methods such as GUROBI and Google OR-Tools. Results across\nfive real-world cases demonstrate competitive solution quality with\nsignificantly reduced computation times, highlighting the potential of quantum\napproaches in automation under Industry 4.0.", "AI": {"tldr": "Hybrid quantum-classical algorithms optimize robotic inspection paths from CAD models, outperforming classical methods in speed while maintaining solution quality.", "motivation": "To enhance industrial automation under Industry 4.0 by leveraging quantum computing for efficient robotic inspection trajectory optimization.", "method": "Model the task as a 3D Traveling Salesman Problem with incomplete graphs and open-route constraints, comparing D-Wave solvers to GUROBI and OR-Tools.", "result": "Quantum methods achieve competitive solution quality with significantly faster computation times in five real-world cases.", "conclusion": "Hybrid quantum-classical algorithms show promise for industrial automation, offering speed advantages without sacrificing performance."}}
{"id": "2507.01469", "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaqu\u00edn Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions.", "AI": {"tldr": "The paper introduces BAR, a novel RSS dataset for museum environments, and a baseline classification method to address the lack of data for IPS development in cultural heritage settings.", "motivation": "Cultural heritage institutions face challenges in implementing IPS due to environmental constraints and lack of relevant datasets.", "method": "The study collects RSS data in museums using Android and iOS, and proposes a proximity-based method with k-NN algorithms for position classification.", "result": "The BAR dataset and baseline method are presented, with analysis and suggestions for future research.", "conclusion": "The work provides a foundation for developing and evaluating IPS algorithms tailored to museum environments."}}
{"id": "2507.01586", "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "categories": ["cs.CV"], "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive\nprocess, as animators are currently required to draw and color a large number\nof frames by hand. We present SketchColour, the first sketch-to-colour pipeline\nfor 2D animation built on a diffusion transformer (DiT) backbone. By replacing\nthe conventional U-Net denoiser with a DiT-style architecture and injecting\nsketch information via lightweight channel-concatenation adapters accompanied\nwith LoRA finetuning, our method natively integrates conditioning without the\nparameter and memory bloat of a duplicated ControlNet, greatly reducing\nparameter count and GPU memory usage. Evaluated on the SAKUGA dataset,\nSketchColour outperforms previous state-of-the-art video colourization methods\nacross all metrics, despite using only half the training data of competing\nmodels. Our approach produces temporally coherent animations with minimal\nartifacts such as colour bleeding or object deformation. Our code is available\nat: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "SketchColour is a sketch-to-colour pipeline for 2D animation using a diffusion transformer (DiT) backbone, reducing labor and resource usage while outperforming existing methods.", "motivation": "High-quality 2D animation production is labor-intensive, requiring manual drawing and coloring of frames.", "method": "Replaces U-Net denoiser with DiT architecture, uses lightweight adapters and LoRA finetuning for sketch integration, avoiding ControlNet's inefficiencies.", "result": "Outperforms state-of-the-art video colourization methods on the SAKUGA dataset with half the training data, producing temporally coherent animations.", "conclusion": "SketchColour offers an efficient, high-quality solution for 2D animation colourization, reducing manual effort and resource usage."}}
{"id": "2507.01735", "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "Summary of the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases using multimodal techniques.", "motivation": "To advance next-gen autonomous driving solutions by addressing corner cases with cutting-edge multimodal perception.", "method": "Organized a workshop with 5 speakers, collected research papers, and held a dual-track challenge for scene understanding and generation.", "result": "Pioneering effort to bridge the gap between advanced techniques and reliable self-driving agents for corner cases.", "conclusion": "The workshop aims to foster continuous progress in robust autonomous driving solutions."}}
{"id": "2507.01470", "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "The paper challenges the assumption that reward frequency measures task difficulty in reinforcement learning, highlighting a structural issue where critical subgoals lack rewards, termed zero-incentive dynamics.", "motivation": "To address the limitation of current policy learning methods that fail when essential subgoals don't yield direct rewards.", "method": "Identifies and formalizes zero-incentive dynamics, evaluates state-of-the-art subgoal-based algorithms, and analyzes sensitivity to reward timing.", "result": "Current algorithms struggle with zero-incentive dynamics, and performance depends heavily on subgoal-reward timing.", "conclusion": "A fundamental limitation exists in current methods, necessitating new mechanisms to infer latent task structure without relying on immediate rewards."}}
{"id": "2507.01516", "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorov\u00e1"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "categories": ["cs.LG"], "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research.", "AI": {"tldr": "The paper explores and unifies various loss functions in diffusion models under the variational lower bound framework, analyzing their relationships, performance differences, and impact on model goals like sample quality and likelihood estimation.", "motivation": "To address the key question of which loss functions diffusion models should train with, given the multiple formulations in literature with varying links and differences.", "method": "Theoretical analysis of target objectives and loss functions, unified under the variational lower bound framework, complemented by empirical study.", "result": "Provides insights into performance divergence conditions and factors, and evaluates how objective choice affects model goals.", "conclusion": "Offers a unified understanding of loss functions in diffusion models, aiding future efficient and goal-oriented designs."}}
{"id": "2507.01590", "pdf": "https://arxiv.org/pdf/2507.01590", "abs": "https://arxiv.org/abs/2507.01590", "authors": ["Ameer Hamza", "Zuhaib Hussain But", "Umar Arif", "Samiya", "M. Abdullah Asad", "Muhammad Naeem"], "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)", "AI": {"tldr": "A novel classroom surveillance system integrates drowsiness detection, mobile phone tracking, and face recognition to assess student attentiveness with high precision, achieving strong performance metrics.", "motivation": "To enhance classroom monitoring and student engagement assessment by combining multiple modalities for real-time insights and automated attendance recording.", "method": "Uses YOLOv8 for mobile phone and sleep detection, LResNet Occ FC for face recognition, and integrates these models with a PHP web application and ESP32-CAM hardware.", "result": "Sleep detection: 97.42% mAP@50; face recognition: 86.45% validation accuracy; mobile phone detection: 85.89% mAP@50.", "conclusion": "The system provides scalable, real-time monitoring and automated attendance, improving classroom management in diverse educational settings."}}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "BBoxER is an evolutionary black-box method for LLM post-training, addressing privacy and security concerns of gradient-based optimization while overcoming scalability and computational challenges of black-box methods.", "motivation": "The paper addresses the limitations of gradient-based optimization (privacy, security, overfitting) and black-box methods (scalability, computational cost) in deep learning, particularly for large language models (LLMs).", "method": "BBoxER introduces an evolutionary black-box method for LLM post-training, leveraging implicit data compression and information flow tractability to provide theoretical guarantees.", "result": "Experiments show BBoxER improves performance and generalization on reasoning benchmarks, offering lightweight, modular enhancements for privacy-sensitive environments.", "conclusion": "BBoxER is a viable add-on to gradient-based optimization, balancing privacy, security, and performance for LLMs."}}
{"id": "2507.01483", "pdf": "https://arxiv.org/pdf/2507.01483", "abs": "https://arxiv.org/abs/2507.01483", "authors": ["Craig S Wright"], "title": "Epistemic Scarcity: The Economics of Unresolvable Unknowns", "categories": ["econ.GN", "cs.AI", "cs.CY", "physics.hist-ph", "q-fin.EC", "91B42, 91B40, 68T01", "J.4; I.2.1; K.4.1; K.4.2"], "comment": "47 pages - submission to QJAE", "summary": "This paper presents a praxeological analysis of artificial intelligence and\nalgorithmic governance, challenging assumptions about the capacity of machine\nsystems to sustain economic and epistemic order. Drawing on Misesian a priori\nreasoning and Austrian theories of entrepreneurship, we argue that AI systems\nare incapable of performing the core functions of economic coordination:\ninterpreting ends, discovering means, and communicating subjective value\nthrough prices. Where neoclassical and behavioural models treat decisions as\noptimisation under constraint, we frame them as purposive actions under\nuncertainty.\n  We critique dominant ethical AI frameworks such as Fairness, Accountability,\nand Transparency (FAT) as extensions of constructivist rationalism, which\nconflict with a liberal order grounded in voluntary action and property rights.\nAttempts to encode moral reasoning in algorithms reflect a misunderstanding of\nethics and economics. However complex, AI systems cannot originate norms,\ninterpret institutions, or bear responsibility. They remain opaque, misaligned,\nand inert.\n  Using the concept of epistemic scarcity, we explore how information abundance\ndegrades truth discernment, enabling both entrepreneurial insight and soft\ntotalitarianism. Our analysis ends with a civilisational claim: the debate over\nAI concerns the future of human autonomy, institutional evolution, and reasoned\nchoice. The Austrian tradition, focused on action, subjectivity, and\nspontaneous order, offers the only coherent alternative to rising computational\nsocial control.", "AI": {"tldr": "The paper critiques AI's role in economic and epistemic order, arguing it cannot perform core economic functions like interpreting ends or communicating value. It challenges ethical AI frameworks (FAT) and highlights AI's limitations in norm creation and responsibility. The study concludes that the debate over AI impacts human autonomy and institutional evolution, advocating for the Austrian tradition as an alternative to computational social control.", "motivation": "The paper aims to challenge assumptions about AI's capacity for economic and epistemic governance, critiquing dominant ethical frameworks and highlighting AI's limitations in norm creation and responsibility.", "method": "The study employs praxeological analysis, Misesian a priori reasoning, and Austrian theories of entrepreneurship to critique AI's role in economic coordination and ethical frameworks.", "result": "AI systems are found incapable of core economic functions like interpreting ends or communicating value, and ethical frameworks like FAT are critiqued as extensions of constructivist rationalism. The paper also highlights AI's opacity and misalignment.", "conclusion": "The debate over AI concerns human autonomy and institutional evolution. The Austrian tradition, emphasizing action and spontaneous order, is proposed as a coherent alternative to computational social control."}}
{"id": "2507.01522", "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "AI": {"tldr": "Chargax is a JAX-based environment for simulating electric vehicle charging stations, offering 100x-1000x faster training for RL agents compared to existing methods.", "motivation": "Addressing slow reinforcement learning in sustainable energy challenges, particularly grid congestion and operational efficiency.", "method": "Introduces Chargax, a modular JAX-based environment for realistic EV charging station simulations, validated with real data.", "result": "Achieves 100x-1000x computational performance improvements and supports diverse real-world configurations.", "conclusion": "Chargax significantly accelerates RL training for sustainable energy applications, enabling practical deployment."}}
{"id": "2507.01603", "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "AI": {"tldr": "DepthSync is a training-free framework using diffusion guidance to achieve scale- and geometry-consistent depth predictions for long videos, addressing challenges in existing methods.", "motivation": "Existing methods for video depth estimation suffer from scale discrepancies and geometric inconsistencies, especially in long videos, due to reliance on 2D priors and sliding windows.", "method": "DepthSync introduces scale guidance to synchronize depth scales across windows and geometry guidance to enforce 3D alignment within windows, leveraging diffusion priors.", "result": "Experiments show DepthSync improves scale and geometry consistency in depth predictions for long videos.", "conclusion": "DepthSync effectively addresses the limitations of existing methods, offering a robust solution for consistent depth estimation in long videos."}}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "Proposes a CPU-friendly LoRA fine-tuning method for LLMs, leveraging pre-trained adapters to create lightweight combinations without GPU reliance.", "motivation": "Address the limitation of GPU dependency in LoRA fine-tuning, making it accessible for users with limited computational resources like standard laptop CPUs.", "method": "Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters, avoiding gradient-based updates and enabling CPU-only processing.", "result": "The CPU-generated adapters outperform the base Mistral model but fall short of GPU-trained LoRAs, offering a practical alternative.", "conclusion": "Provides a viable, resource-efficient solution for LoRA fine-tuning, broadening accessibility for users without GPUs."}}
{"id": "2507.01547", "pdf": "https://arxiv.org/pdf/2507.01547", "abs": "https://arxiv.org/abs/2507.01547", "authors": ["Ubada El Joulani", "Tatiana Kalganova", "Stergios-Aristoteles Mitoulis", "Sotirios Argyroudis"], "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Critical infrastructure, such as transport networks, underpins economic\ngrowth by enabling mobility and trade. However, ageing assets, climate change\nimpacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging\nfrom natural disasters to cyber attacks and conflicts pose growing risks to\ntheir resilience and functionality. This review paper explores how emerging\ndigital technologies, specifically Artificial Intelligence (AI), can enhance\ndamage assessment and monitoring of transport infrastructure. A systematic\nliterature review examines existing AI models and datasets for assessing damage\nin roads, bridges, and other critical infrastructure impacted by natural\ndisasters. Special focus is given to the unique challenges and opportunities\nassociated with bridge damage detection due to their structural complexity and\ncritical role in connectivity. The integration of SAR (Synthetic Aperture\nRadar) data with AI models is also discussed, with the review revealing a\ncritical research gap: a scarcity of studies applying AI models to SAR data for\ncomprehensive bridge damage assessment. Therefore, this review aims to identify\nthe research gaps and provide foundations for AI-driven solutions for assessing\nand monitoring critical transport infrastructures.", "AI": {"tldr": "This review explores how AI can improve damage assessment and monitoring of transport infrastructure, focusing on bridges and SAR data integration, while identifying research gaps.", "motivation": "The increasing risks to transport infrastructure from ageing, climate change, and hybrid threats necessitate advanced solutions like AI for resilience.", "method": "A systematic literature review of AI models and datasets for damage assessment in transport infrastructure, with emphasis on bridges and SAR data.", "result": "The review highlights a research gap in AI applications for SAR data in bridge damage assessment.", "conclusion": "The paper aims to guide future AI-driven solutions for monitoring and assessing critical transport infrastructure."}}
{"id": "2507.01544", "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "categories": ["cs.LG"], "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis", "AI": {"tldr": "MARVIS enables small vision-language models to predict any data modality accurately without training, outperforming Gemini by 16% and nearing specialized methods.", "motivation": "Specialized models lack flexibility, while foundation models underperform in non-traditional domains. MARVIS bridges this gap.", "method": "Transforms latent embeddings into visual representations, leveraging VLMs' reasoning skills for interpretation.", "result": "Competitive performance across vision, audio, biological, and tabular domains with a single 3B parameter model.", "conclusion": "MARVIS offers versatile, high-accuracy predictions without P.I.I. exposure or domain-specific training, with open-sourced resources."}}
{"id": "2507.01607", "pdf": "https://arxiv.org/pdf/2507.01607", "abs": "https://arxiv.org/abs/2507.01607", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi", "Eric Bourbao"], "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.", "AI": {"tldr": "This paper investigates backdoor attacks in deep learning-based face recognition systems, demonstrating their feasibility and impact, and suggests countermeasures.", "motivation": "The study addresses the lack of research on DNN backdoor attacks in real-life, unconstrained face recognition systems, highlighting security vulnerabilities.", "method": "The paper explores backdoor attacks holistically, including face generation and landmark shift attacks, and tests them across 20 pipeline configurations and 15 attack cases.", "result": "The research shows that a single backdoor can bypass an entire system's function, proving the vulnerability of face recognition pipelines.", "conclusion": "The paper concludes by offering best practices and countermeasures to mitigate backdoor threats in face recognition systems."}}
{"id": "2507.01951", "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1 is a reflective generative model using SPRM to match OpenAI o3's performance, reducing PRM parameters by 99% and enabling efficient reasoning with controllable thinking modes.", "motivation": "To create a unified, efficient model that integrates policy and process reward models without extra annotations, while maintaining performance.", "method": "Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, enabling test time scaling (TTS) with three reasoning effort modes.", "result": "Achieves performance comparable to OpenAI-o3-mini with only 32B parameters and establishes a scaling law for TTS.", "conclusion": "MetaStone-S1 is a scalable, efficient model open-sourced for community use, demonstrating the viability of SPRM for high-performance reasoning."}}
{"id": "2507.01616", "pdf": "https://arxiv.org/pdf/2507.01616", "abs": "https://arxiv.org/abs/2507.01616", "authors": ["Chengkun He", "Xiangmin Zhou", "Chen Wang", "Longbing Cao", "Jie Shao", "Xiaodong Li", "Guang Xu", "Carrie Jinqiu Hu", "Zahir Tari"], "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation", "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": null, "summary": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.", "AI": {"tldr": "The paper introduces an Enhanced Influence-aware Group Recommendation (EIGR) framework to address challenges in group recommendation, such as scalability, dynamic influence, and computational overhead. It outperforms existing methods in effectiveness and efficiency.", "motivation": "Group recommendation is crucial for applications like e-commerce and entertainment, but existing methods struggle with large-scale social graphs, dynamic influence, and real-time processing.", "method": "The EIGR framework includes a Graph Extraction-based Sampling (GES) strategy, a Dynamic Independent Cascade (DYIC) model, and a User Group Index (UG-Index) for efficient real-time recommendations.", "result": "Experiments show EIGR outperforms state-of-the-art baselines in effectiveness and efficiency.", "conclusion": "EIGR successfully addresses key challenges in group recommendation, offering a scalable and dynamic solution."}}
{"id": "2507.01559", "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "The paper explores the effects of resampling weights (\"zapping\") in neural networks during continual learning, showing it aids faster recovery in domain transfers and highlights optimizer choice's impact on learning dynamics.", "motivation": "To understand the mechanisms behind the effectiveness of zapping in continual learning and its interaction with optimizer choices in multi-task settings.", "method": "Experiments with convolutional neural networks in continual and few-shot transfer learning scenarios, using handwritten characters and natural images.", "result": "Zapping accelerates recovery in domain transfers, and optimizer choice influences learning/forgetting dynamics, creating task synergy/interference patterns.", "conclusion": "Zapping and optimizer selection significantly impact continual learning performance, revealing complex task interaction dynamics."}}
{"id": "2507.01630", "pdf": "https://arxiv.org/pdf/2507.01630", "abs": "https://arxiv.org/abs/2507.01630", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV 2025", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.", "AI": {"tldr": "The paper introduces P3HOT, a framework for Human-Object conTact (HOT) detection, combining prompt guidance and human proximal perception to improve accuracy and consistency in identifying contact regions.", "motivation": "Current HOT detection models are limited to single image types, causing over-segmentation and inconsistent category labeling in regions with minimal interaction.", "method": "P3HOT uses semantic-driven prompts to focus on relevant regions and a human proximal perception mechanism for dynamic depth range perception. It also introduces a Regional Joint Loss (RJLoss) and a new metric, AD-Acc., for better evaluation.", "result": "P3HOT achieves state-of-the-art performance, with improvements of 0.7, 2.0, 1.6, and 11.0 in SC-Acc., mIoU, wIoU, and AD-Acc. metrics on the HOT-Annotated dataset.", "conclusion": "The proposed P3HOT framework effectively addresses limitations in HOT detection, offering superior performance and a novel evaluation approach."}}
{"id": "2404.16369", "pdf": "https://arxiv.org/pdf/2404.16369", "abs": "https://arxiv.org/abs/2404.16369", "authors": ["Yukai Zhou", "Jian Lou", "Zhijie Huang", "Zhan Qin", "Yibei Yang", "Wenjie Wang"], "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models.", "AI": {"tldr": "The paper introduces DSN, an improved jailbreaking attack for LLMs, combining cosine decay and refusal suppression to achieve higher success rates than baseline methods.", "motivation": "LLMs are vulnerable to jailbreaking attacks that manipulate them into toxic outputs. Current methods rely on predefined behaviors, limiting adaptability.", "method": "Proposes DSN attack, enhancing the loss objective with cosine decay and refusal suppression for better performance.", "result": "DSN outperforms baselines, achieving state-of-the-art attack success rates (ASR) and showing strong universality and transferability.", "conclusion": "DSN is a more effective and adaptable jailbreaking attack for LLMs, demonstrating superior performance and broader applicability."}}
{"id": "2507.01631", "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF scales NeRF to large scenes by dividing regions into non-overlapping 3D tiles and using an out-of-core method, enabling single-device processing without quality loss.", "motivation": "Existing NeRF methods are limited to small scenes due to high memory usage during training.", "method": "Divides the scene into non-overlapping 3D tiles, crops images with overlap, and uses a $2\\times2$ tile progression strategy and segmented sampler to avoid edge errors.", "result": "Processes large satellite images with linear time complexity on a single GPU, maintaining quality.", "conclusion": "Snake-NeRF effectively scales NeRF for large scenes without compromising quality."}}
{"id": "2507.01581", "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "categories": ["cs.LG", "cs.CR", "eess.SP"], "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.", "AI": {"tldr": "The paper proposes a Federated Learning (FL)-based approach for indoor localization using a DNN model, addressing privacy and efficiency issues of traditional centralized methods.", "motivation": "Traditional indoor localization techniques have high errors and privacy concerns due to centralized data collection. ML solutions often face similar issues.", "method": "A Federated Learning (FL) approach with a Deep Neural Network (DNN) model is proposed to decentralize data processing.", "result": "FL achieves performance close to centralized models while ensuring data privacy, bandwidth efficiency, and server reliability.", "conclusion": "The FL approach is a viable solution for privacy-enhanced indoor localization, advancing secure and efficient systems."}}
{"id": "2507.01634", "pdf": "https://arxiv.org/pdf/2507.01634", "abs": "https://arxiv.org/abs/2507.01634", "authors": ["Boyuan Sun", "Modi Jin", "Bowen Yin", "Qibin Hou"], "title": "Depth Anything at Any Condition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC", "AI": {"tldr": "DepthAnything-AC is a monocular depth estimation model designed for diverse environmental conditions, using unsupervised consistency regularization and Spatial Distance Constraint for improved performance.", "motivation": "Existing foundation MDE models struggle in complex open-world environments with challenging conditions like illumination variations and adverse weather.", "method": "Proposes unsupervised consistency regularization finetuning and Spatial Distance Constraint for patch-level relative relationships.", "result": "Demonstrates zero-shot capabilities across diverse benchmarks, including adverse weather and synthetic corruption.", "conclusion": "DepthAnything-AC effectively handles diverse conditions, improving semantic boundaries and detail accuracy."}}
{"id": "2405.13012", "pdf": "https://arxiv.org/pdf/2405.13012", "abs": "https://arxiv.org/abs/2405.13012", "authors": ["Antoine Bellemare-Pepin", "Fran\u00e7ois Lespinasse", "Philipp Th\u00f6lke", "Yann Harel", "Kory Mathewson", "Jay A. Olson", "Yoshua Bengio", "Karim Jerbi"], "title": "Divergent Creativity in Humans and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "First two and last listed authors are corresponding authors. The\n  first two listed authors contributed equally to this work", "summary": "The recent surge of Large Language Models (LLMs) has led to claims that they\nare approaching a level of creativity akin to human capabilities. This idea has\nsparked a blend of excitement and apprehension. However, a critical piece that\nhas been missing in this discourse is a systematic evaluation of LLMs' semantic\ndiversity, particularly in comparison to human divergent thinking. To bridge\nthis gap, we leverage recent advances in computational creativity to analyze\nsemantic divergence in both state-of-the-art LLMs and a substantial dataset of\n100,000 humans. We found evidence that LLMs can surpass average human\nperformance on the Divergent Association Task, and approach human creative\nwriting abilities, though they fall short of the typical performance of highly\ncreative humans. Notably, even the top performing LLMs are still largely\nsurpassed by highly creative individuals, underscoring a ceiling that current\nLLMs still fail to surpass. Our human-machine benchmarking framework addresses\nthe polemic surrounding the imminent replacement of human creative labour by\nAI, disentangling the quality of the respective creative linguistic outputs\nusing established objective measures. While prompting deeper exploration of the\ndistinctive elements of human inventive thought compared to those of AI\nsystems, we lay out a series of techniques to improve their outputs with\nrespect to semantic diversity, such as prompt design and hyper-parameter\ntuning.", "AI": {"tldr": "LLMs show high semantic diversity, surpassing average humans but lagging behind highly creative individuals. A framework evaluates human vs. AI creativity, suggesting improvements for LLMs.", "motivation": "To systematically evaluate LLMs' semantic diversity compared to human divergent thinking, addressing claims of AI's near-human creativity.", "method": "Used computational creativity to analyze semantic divergence in top LLMs and 100,000 humans via the Divergent Association Task.", "result": "LLMs exceed average human performance but fall short of highly creative individuals. A benchmarking framework compares human and AI creativity.", "conclusion": "Current LLMs have a creativity ceiling. Techniques like prompt design can improve their semantic diversity, but human inventive thought remains distinct."}}
{"id": "2507.01638", "pdf": "https://arxiv.org/pdf/2507.01638", "abs": "https://arxiv.org/abs/2507.01638", "authors": ["Ana Nikolikj", "Gabriela Ochoa", "Tome Eftimov"], "title": "Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "We present an analysis of landscape features for predicting the performance\nof multi-objective combinatorial optimization algorithms. We consider features\nfrom the recently proposed compressed Pareto Local Optimal Solutions Networks\n(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a\nset of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness\nand objective correlation. We consider the performance of three algorithms --\nPareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and\nNon-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and\nhypervolume metrics. Our tailored analysis reveals feature combinations that\ninfluence algorithm performance specific to certain landscapes. This study\nprovides deeper insights into feature importance, tailored to specific\nrmnk-landscapes and algorithms.", "AI": {"tldr": "Analysis of landscape features for predicting multi-objective optimization algorithm performance using C-PLOS-net model on rmnk-landscapes.", "motivation": "To understand how landscape features influence the performance of multi-objective combinatorial optimization algorithms.", "method": "Use of C-PLOS-net model features on rmnk-landscapes with varying ruggedness and objective correlation. Performance of PLS, GSEMO, and NSGA-II evaluated using resolution and hypervolume metrics.", "result": "Identified feature combinations that influence algorithm performance for specific landscapes.", "conclusion": "Provides insights into feature importance tailored to specific rmnk-landscapes and algorithms."}}
{"id": "2507.01598", "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments.", "AI": {"tldr": "The paper analyzes Muon, a new optimizer for neural networks, proving convergence for its variants and showing tighter bounds with weight decay. It also derives Muon's critical batch size and validates findings experimentally.", "motivation": "To theoretically analyze Muon, a novel optimizer leveraging neural network parameter structure, and explore its variants and properties like weight decay and batch size.", "method": "Theoretical analysis of Muon, including convergence proofs for four variants (with/without Nesterov momentum and weight decay), and derivation of critical batch size. Experimental validation is also conducted.", "result": "Weight decay tightens bounds on parameter and gradient norms. The relationship between weight decay coefficient and learning rate is clarified. Critical batch size minimizing SFO complexity is derived.", "conclusion": "Muon's theoretical properties are rigorously analyzed, with practical implications for optimization in neural networks, supported by experimental validation."}}
{"id": "2507.01643", "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "categories": ["cs.CV"], "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "AI": {"tldr": "SAILViT is a Vision Transformer designed to improve Multimodal Large Language Models (MLLMs) by addressing parameter conflicts and semantic gaps through gradual feature refinement.", "motivation": "Existing ViTs struggle with direct co-training with LLMs due to initialization conflicts and modality gaps, limiting MLLM performance.", "method": "SAILViT introduces gradual feature learning for coarse-to-fine alignment and knowledge infusion, enhancing robustness and generalizability.", "result": "SAILViT significantly boosts MLLM performance on the OpenCompass benchmark across diverse tasks.", "conclusion": "SAILViT effectively bridges ViTs and LLMs, enabling better multimodal interaction and performance in MLLMs."}}
{"id": "2408.02544", "pdf": "https://arxiv.org/pdf/2408.02544", "abs": "https://arxiv.org/abs/2408.02544", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "title": "Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic.", "AI": {"tldr": "The paper examines if multimodal GUI agents get distracted by unrelated environmental content, finding even top models are susceptible, and suggests improving faithfulness.", "motivation": "To determine if multimodal GUI agents can be distracted by benign but unrelated environmental context, a gap not addressed in current research.", "method": "Evaluated various MLLMs as GUI agents using a simulated dataset under three perception levels, including adversarial environment injection.", "result": "Even the most powerful models are prone to distractions, highlighting a vulnerability not previously emphasized.", "conclusion": "Calls for collective focus on improving agent faithfulness to environmental distractions, beyond just helpfulness."}}
{"id": "2507.01649", "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.", "AI": {"tldr": "The paper introduces GradMetaNet, a novel architecture designed for processing gradients in neural networks, guided by principles of equivariance, multi-point gradient processing, and efficient representation.", "motivation": "Existing gradient-processing architectures lack specialized design, limiting their effectiveness. The paper aims to address this gap.", "method": "Proposes GradMetaNet, built on equivariant blocks, to process gradients efficiently while capturing curvature information.", "result": "GradMetaNet outperforms previous methods in tasks like learned optimization and loss landscape curvature estimation.", "conclusion": "GradMetaNet provides a principled and effective solution for gradient-based tasks, demonstrating superior performance and universality."}}
{"id": "2507.01636", "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "categories": ["cs.LG", "eess.SP"], "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient.", "AI": {"tldr": "An efficient online dictionary learning algorithm for kernel-based sparse representations, outperforming existing methods with low computational complexity.", "motivation": "To improve online kernel dictionary learning by enabling efficient updates and maintaining high accuracy.", "method": "Uses a recursive least squares (RLS) method for dictionary updates, working with single samples or mini-batches.", "result": "Outperforms existing online methods and achieves classification accuracy close to batch-trained models, with higher efficiency.", "conclusion": "The proposed method is efficient and effective for online kernel dictionary learning, balancing accuracy and computational cost."}}
{"id": "2507.01653", "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "categories": ["cs.CV"], "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "RobuSTereo enhances stereo matching models' zero-shot generalization in adverse weather by addressing data scarcity and feature extraction challenges using synthetic data and a robust feature encoder.", "motivation": "Stereo matching models struggle in adverse weather due to lack of training data and difficulty in feature extraction, limiting zero-shot generalization.", "method": "Proposes a diffusion-based simulation pipeline with stereo consistency for synthetic data and a feature encoder combining ConvNet and denoising transformer.", "result": "Improves robustness and generalization in adverse weather, with better disparity estimation accuracy.", "conclusion": "RobuSTereo effectively addresses challenges in stereo matching for adverse weather, enhancing model performance."}}
{"id": "2409.20434", "pdf": "https://arxiv.org/pdf/2409.20434", "abs": "https://arxiv.org/abs/2409.20434", "authors": ["Zhengren Wang", "Qinhan Yu", "Shida Wei", "Zhiyu Li", "Feiyu Xiong", "Xiaoxing Wang", "Simin Niu", "Hao Liang", "Wentao Zhang"], "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering Systems", "categories": ["cs.CL"], "comment": "ACL 2025 Oral", "summary": "Modern QA systems entail retrieval-augmented generation (RAG) for accurate\nand trustworthy responses. However, the inherent gap between user queries and\nrelevant documents hinders precise matching. We introduce QAEncoder, a\ntraining-free approach to bridge this gap. Specifically, QAEncoder estimates\nthe expectation of potential queries in the embedding space as a robust\nsurrogate for the document embedding, and attaches document fingerprints to\neffectively distinguish these embeddings. Extensive experiments across diverse\ndatasets, languages, and embedding models confirmed QAEncoder's alignment\ncapability, which offers a simple-yet-effective solution with zero additional\nindex storage, retrieval latency, training costs, or catastrophic forgetting\nand hallucination issues. The repository is publicly available at\nhttps://github.com/IAAR-Shanghai/QAEncoder.", "AI": {"tldr": "QAEncoder is a training-free method to bridge the gap between user queries and documents in QA systems, improving alignment without extra costs.", "motivation": "The gap between user queries and relevant documents in QA systems hinders precise matching, necessitating a solution like QAEncoder.", "method": "QAEncoder estimates query expectations in embedding space and uses document fingerprints to distinguish embeddings, requiring no training.", "result": "Experiments show QAEncoder effectively aligns queries and documents across datasets, languages, and embedding models with no added costs.", "conclusion": "QAEncoder provides a simple, cost-free solution for improving QA system accuracy, avoiding issues like hallucination and forgetting."}}
{"id": "2507.01663", "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "AI": {"tldr": "AsyncFlow is an asynchronous streaming RL framework for efficient post-training of LLMs, addressing scalability and resource issues in traditional RL frameworks.", "motivation": "Traditional RL frameworks for LLMs face scalability bottlenecks, dataflow complexity, and tight coupling with training/inference engines, limiting flexibility and efficiency.", "method": "AsyncFlow introduces distributed data storage, fine-grained scheduling, and a producer-consumer workflow to enable pipeline overlapping, load balancing, and minimized idleness.", "result": "Experiments show AsyncFlow achieves a 1.59x throughput improvement over state-of-the-art baselines.", "conclusion": "AsyncFlow offers a modular, efficient solution for RL post-training, with insights for future RL system designs."}}
{"id": "2507.01644", "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "categories": ["cs.LG"], "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation.", "AI": {"tldr": "DDCL improves DDR chart generation accuracy using a ConvLSTM model, surpassing the earlier CNN-LSTM approach (DDC).", "motivation": "To enhance the automatic generation of DDR charts, addressing limitations of the prior DDC method.", "method": "Uses a ConvLSTM-based model for chart generation, building on the DDC framework.", "result": "Substantially increases the accuracy of DDR chart generation compared to DDC.", "conclusion": "DDCL is a superior method for automatic DDR chart generation, offering improved performance over DDC."}}
{"id": "2507.01654", "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "SPoT introduces a flexible tokenization method for Vision Transformers, enabling continuous token placement to improve efficiency and performance.", "motivation": "Standard tokenization methods limit Vision Transformers to rigid patch grids, hindering their ability to leverage sparsity effectively.", "method": "Proposes Subpixel Placement of Tokens (SPoT), a continuous tokenization strategy, and uses oracle-guided search to optimize token positioning.", "result": "Achieves significant performance gains with fewer tokens, enhancing efficiency during inference.", "conclusion": "SPoT redefines sparsity as an advantage, offering a more flexible and interpretable approach for ViT architectures."}}
{"id": "2410.06716", "pdf": "https://arxiv.org/pdf/2410.06716", "abs": "https://arxiv.org/abs/2410.06716", "authors": ["Minbeom Kim", "Thibaut Thonet", "Jos Rozen", "Hwaran Lee", "Kyomin Jung", "Marc Dymetman"], "title": "Guaranteed Generation from Large Language Models", "categories": ["cs.CL"], "comment": "ICLR 2025", "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.", "AI": {"tldr": "GUARD combines autoregressive training with rejection sampling to ensure strict constraint satisfaction in LLM-generated text while preserving the original model's distribution.", "motivation": "The need to control LLM outputs to meet specific constraints without altering the model's original distribution.", "method": "Proposes GUARD, blending autoregressive proposal distribution with rejection sampling to enforce guarantees.", "result": "GUARD achieves perfect constraint satisfaction and maintains the ideal distribution with improved efficiency.", "conclusion": "GUARD offers a principled method for strict constraint enforcement in LLMs without compromising generative quality."}}
{"id": "2507.01668", "pdf": "https://arxiv.org/pdf/2507.01668", "abs": "https://arxiv.org/abs/2507.01668", "authors": ["Gjorgjina Cenikj", "Ga\u0161per Petelin", "Tome Eftimov"], "title": "Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "The field of numerical optimization has recently seen a surge in the\ndevelopment of \"novel\" metaheuristic algorithms, inspired by metaphors derived\nfrom natural or human-made processes, which have been widely criticized for\nobscuring meaningful innovations and failing to distinguish themselves from\nexisting approaches. Aiming to address these concerns, we investigate the\napplicability of statistical tests for comparing algorithms based on their\nsearch behavior. We utilize the cross-match statistical test to compare\nmultivariate distributions and assess the solutions produced by 114 algorithms\nfrom the MEALPY library. These findings are incorporated into an empirical\nanalysis aiming to identify algorithms with similar search behaviors.", "AI": {"tldr": "The paper critiques the proliferation of metaheuristic algorithms in numerical optimization and proposes using statistical tests to compare their search behaviors, analyzing 114 algorithms from MEALPY.", "motivation": "To address concerns about the lack of meaningful innovation and differentiation among metaheuristic algorithms.", "method": "Utilizes the cross-match statistical test to compare multivariate distributions of solutions from 114 algorithms in MEALPY.", "result": "Identifies algorithms with similar search behaviors through empirical analysis.", "conclusion": "Statistical tests can effectively distinguish and compare metaheuristic algorithms, aiding in identifying redundant or novel approaches."}}
{"id": "2507.01693", "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.", "AI": {"tldr": "The paper introduces SODA, a gradient-based algorithm for reconstructing exact inputs from LLM outputs, outperforming existing methods with 79.5% recovery for shorter inputs but struggling with longer sequences.", "motivation": "To enable post-incident analysis and detect fake outputs by reconstructing the exact input from LLM outputs, addressing a forensic gap in auditing techniques.", "method": "Formalizes input reconstruction as a discrete optimization problem, introducing SODA with continuous relaxation, periodic restarts, and parameter decay.", "result": "SODA recovers 79.5% of shorter out-of-distribution inputs without false positives but fails with longer (15+ token) sequences, indicating current deployment practices may mitigate misuse.", "conclusion": "SODA is effective for forensic analysis of LLM outputs but has limitations with longer inputs, suggesting existing protections may suffice against malicious use."}}
{"id": "2507.01667", "pdf": "https://arxiv.org/pdf/2507.01667", "abs": "https://arxiv.org/abs/2507.01667", "authors": ["Gianluca Monaci", "Philippe Weinzaepfel", "Christian Wolf"], "title": "What does really matter in image goal navigation?", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Image goal navigation requires two different skills: firstly, core navigation\nskills, including the detection of free space and obstacles, and taking\ndecisions based on an internal representation; and secondly, computing\ndirectional information by comparing visual observations to the goal image.\nCurrent state-of-the-art methods either rely on dedicated image-matching, or\npre-training of computer vision modules on relative pose estimation. In this\npaper, we study whether this task can be efficiently solved with end-to-end\ntraining of full agents with RL, as has been claimed by recent work. A positive\nanswer would have impact beyond Embodied AI and allow training of relative pose\nestimation from reward for navigation alone. In a large study we investigate\nthe effect of architectural choices like late fusion, channel stacking,\nspace-to-depth projections and cross-attention, and their role in the emergence\nof relative pose estimators from navigation training. We show that the success\nof recent methods is influenced up to a certain extent by simulator settings,\nleading to shortcuts in simulation. However, we also show that these\ncapabilities can be transferred to more realistic setting, up to some extend.\nWe also find evidence for correlations between navigation performance and\nprobed (emerging) relative pose estimation performance, an important sub skill.", "AI": {"tldr": "The paper explores whether end-to-end RL training can efficiently solve image goal navigation, analyzing architectural choices and simulator impacts.", "motivation": "To determine if end-to-end RL training can replace dedicated methods for image goal navigation, impacting broader AI applications.", "method": "Investigates architectural choices (late fusion, channel stacking, etc.) and their role in emerging relative pose estimators from RL training.", "result": "Simulator settings influence method success, but capabilities can transfer to realistic settings. Navigation performance correlates with emerging pose estimation skills.", "conclusion": "End-to-end RL training shows promise for image goal navigation, though simulator shortcuts exist. Transferability and emerging skills highlight potential."}}
{"id": "2412.05563", "pdf": "https://arxiv.org/pdf/2412.05563", "abs": "https://arxiv.org/abs/2412.05563", "authors": ["Ola Shorinwa", "Zhiting Mei", "Justin Lidard", "Allen Z. Ren", "Anirudha Majumdar"], "title": "A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The remarkable performance of large language models (LLMs) in content\ngeneration, coding, and common-sense reasoning has spurred widespread\nintegration into many facets of society. However, integration of LLMs raises\nvalid questions on their reliability and trustworthiness, given their\npropensity to generate hallucinations: plausible, factually-incorrect\nresponses, which are expressed with striking confidence. Previous work has\nshown that hallucinations and other non-factual responses generated by LLMs can\nbe detected by examining the uncertainty of the LLM in its response to the\npertinent prompt, driving significant research efforts devoted to quantifying\nthe uncertainty of LLMs. This survey seeks to provide an extensive review of\nexisting uncertainty quantification methods for LLMs, identifying their salient\nfeatures, along with their strengths and weaknesses. We present existing\nmethods within a relevant taxonomy, unifying ostensibly disparate methods to\naid understanding of the state of the art. Furthermore, we highlight\napplications of uncertainty quantification methods for LLMs, spanning chatbot\nand textual applications to embodied artificial intelligence applications in\nrobotics. We conclude with open research challenges in uncertainty\nquantification of LLMs, seeking to motivate future research.", "AI": {"tldr": "A survey reviewing uncertainty quantification methods for large language models (LLMs) to detect hallucinations and non-factual responses, highlighting applications and open research challenges.", "motivation": "Address reliability and trustworthiness concerns of LLMs due to their tendency to generate confident but factually incorrect responses (hallucinations).", "method": "Review and taxonomy of existing uncertainty quantification methods for LLMs, analyzing their features, strengths, and weaknesses.", "result": "Unified understanding of state-of-the-art methods and their applications in chatbots, text, and robotics.", "conclusion": "Identifies open research challenges to motivate future work in improving LLM uncertainty quantification."}}
