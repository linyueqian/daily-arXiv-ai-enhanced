<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.CV](#cs.CV) [Total: 99]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 111]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 8]
- [eess.IV](#eess.IV) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/pdf/2506.19952)
*Deepon Halder, Thanmay Jayakumar, Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill leverages LLMs and few-shot translation to create synthetic parallel corpora for high-quality MT without needing extensive parallel data.


<details>
  <summary>Details</summary>
Motivation: Parallel corpora are scarce for low-resource languages, limiting MT quality. CycleDistill aims to overcome this by using LLMs and monolingual data.

Method: CycleDistill iteratively generates synthetic parallel corpora via few-shot MT, fine-tuning the model on this data. It uses minimal parallel examples (1-4).

Result: Improves few-shot baseline by 20-30 chrF points on average for three Indian languages in the first iteration. Softmax activations offer mild improvements.

Conclusion: CycleDistill effectively boosts MT quality for low-resource languages using monolingual data and minimal parallel examples.

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/pdf/2506.19967)
*Travis Thompson, Seung-Hwan Lim, Paul Liu, Ruoying He, Dongkuan Xu*

Main category: cs.CL

TL;DR: Inference-Scaled GraphRAG improves LLM reasoning on knowledge graphs by combining sequential and parallel scaling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform on knowledge-intensive tasks due to limited access to structured context and multi-hop information. Existing RAG methods fail to capture relational structures in knowledge graphs.

Method: Introduces Inference-Scaled GraphRAG, combining sequential scaling (deep chain-of-thought traversal) and parallel scaling (majority voting over sampled trajectories) in an interleaved reasoning-execution loop.

Result: Significantly improves multi-hop question answering on GRBench, outperforming traditional GraphRAG and prior baselines.

Conclusion: Inference-time scaling is a practical, architecture-agnostic solution for structured knowledge reasoning with LLMs.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/pdf/2506.19998)
*Xinyi Ni, Haonan Jian, Qiuyang Wang, Vedanshi Chetan Shah, Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent is a scalable pipeline for creating agents that generate and refine executable tools from unstructured API documentation, improving performance and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Most API-based agents rely on curated toolsets, failing to address the complexity of real-world APIs. Building adaptable agents for arbitrary domains is challenging due to unstructured documentation and parameter inference.

Method: Doc2Agent generates executable tools from API documentation and refines them iteratively using a code agent. It is evaluated on real-world, WebArena, and research APIs.

Result: Achieved a 55% performance improvement with 90% lower cost on WebArena. Demonstrated adaptability in glycomaterial science.

Conclusion: Doc2Agent provides a scalable, generalizable solution for building tool agents from unstructured API documentation.

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [4] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/pdf/2506.20073)
*Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason integrates LLMs with spatio-temporal models for multi-task inference, outperforming baselines in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing models lack multi-task inference and explanatory outputs, limiting real-world applicability.

Method: STReason uses in-context learning to decompose queries into modular programs, generating solutions and rationales without task-specific finetuning.

Result: STReason outperforms advanced LLM baselines, excels in reasoning-intensive scenarios, and reduces expert workload.

Conclusion: STReason offers a promising direction for generalizable spatio-temporal reasoning systems.

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [5] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/pdf/2506.20081)
*Dhruv Gupta, Gayathri Ganesh Lakshmy, Yiqing Xie*

Main category: cs.CL

TL;DR: The paper analyzes code retrieval in RACG, revealing biases toward surface-level features and well-documented code. It proposes SACL, a framework improving retrieval and generation performance.


<details>
  <summary>Details</summary>
Motivation: To address biases in current code retrievers that rely on superficial features and irrelevant documentation, hindering effective code generation.

Method: Systematic masking of features while preserving functionality, leading to the development of SACL, which augments code with semantic information.

Result: SACL improves retrieval (e.g., 12.8% Recall@1 on HumanEval) and boosts code generation (e.g., 4.88% Pass@1 on HumanEval).

Conclusion: SACL effectively reduces retrieval biases and enhances code generation by integrating semantic knowledge.

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [6] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/pdf/2506.20083)
*Yingji Zhang, Danilo S. Carvalho, André Freitas*

Main category: cs.CL

TL;DR: The paper explores integrating compositional and symbolic properties into distributional semantic spaces to improve Transformer-based LMs, focusing on semantic representation learning to bridge symbolic and distributional semantics. It reviews VAE, VQVAE, and SAE architectures for their latent geometries and semantic interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability, controllability, compositionality, and generalization of Transformer-based LMs by combining compositional and symbolic properties with distributional semantics.

Method: Reviews and compares three autoencoder architectures (VAE, VQVAE, SAE) to analyze their latent space geometries and semantic structure.

Result: Identifies how each architecture's latent geometry impacts semantic interpretability and structure, bridging symbolic and distributional semantics.

Conclusion: Semantic representation learning offers a promising direction to unify symbolic and distributional semantics, improving LM capabilities.

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [7] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/pdf/2506.20093)
*Yilin Wang, Peixuan Lei, Jie Song, Yuzhe Hao, Tao Chen, Yuxuan Zhang, Lei Jia, Yuanxiang Li, Zhongyu Wei*

Main category: cs.CL

TL;DR: The paper introduces Time-Series QA and EngineMT-QA dataset, proposing ITFormer to integrate time-series data with natural language, improving QA accuracy efficiently.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating high-dimensional time-series data with natural language for dynamic tasks.

Method: Proposes ITFormer, a framework combining time-series encoders with frozen LLMs to align and fuse temporal-textual features.

Result: Achieves strong QA accuracy improvement with minimal additional trainable parameters (less than 1%).

Conclusion: Establishes a paradigm for temporal-textual integration, enabling new multi-modal AI research and applications.

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [8] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/pdf/2506.20243)
*Papa Séga Wade, Mihai Andries, Ioannis Kanellos, Thierry Moudenc*

Main category: cs.CL

TL;DR: A chunk-based approach using self-supervised learning models (Wav2Vec2, HuBERT, WavLM) and a hierarchical CNN-BiLSTM framework improves fluency assessment in non-native speakers by analyzing speech rhythm, pauses, and disfluencies.


<details>
  <summary>Details</summary>
Motivation: Automatic fluency assessment (AFA) struggles with capturing speech nuances like rhythm and disfluencies in non-native speakers, necessitating a more robust method.

Method: Speech is segmented into breath-group chunks using Silero-VAD, with SSL embeddings fused via a weighted mechanism. A CNN-BiLSTM framework captures local and long-term dependencies.

Result: The approach improves F1-score by 2.8 and Pearson correlation by 6.2 points on Speechocean762, and 4.2 F1-score and 4.0 Pearson points on Avalinguo, outperforming baselines.

Conclusion: Chunk-based multi-SSL fusion enhances fluency evaluation, but future work should address generalization to dialects with irregular prosody.

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [9] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/pdf/2506.20112)
*Songsoo Kim, Seungtae Lee, See Young Lee, Joonho Kim, Keechan Kan, Dukyong Yoon*

Main category: cs.CL

TL;DR: A three-pass LLM framework improves PPV and reduces costs for radiology report proofreading compared to simpler methods.


<details>
  <summary>Details</summary>
Motivation: The low error prevalence in radiology reports limits the PPV of LLM-based proofreading, prompting the need for a more efficient framework.

Method: Three LLM frameworks were tested: single-prompt detector, extractor plus detector, and extractor-detector-false-positive verifier. Metrics included PPV, aTPR, and operational costs.

Result: The three-pass framework (Framework 3) significantly improved PPV (0.159 vs. 0.063/0.079) and reduced costs by 42.6% (USD 5.58 vs. USD 9.72). Detection performance (aTPR) remained stable.

Conclusion: The three-pass LLM framework is effective for AI-assisted radiology report quality assurance, enhancing PPV and reducing costs without compromising detection.

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [10] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/pdf/2506.20430)
*Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare is an LLM-powered system for diagnosing rare diseases, outperforming existing methods with high accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Rare diseases are hard to diagnose due to clinical heterogeneity and low prevalence, necessitating advanced tools like DeepRare.

Method: DeepRare uses a modular design with a central host, specialized agents, and 40+ tools to process clinical inputs and generate ranked hypotheses.

Result: Achieves 100% accuracy for 1,013 diseases, outperforms 15 methods with a 57.18% Recall@1, and excels in multi-modal scenarios (70.60% Recall@1).

Conclusion: DeepRare is a scalable, accurate, and transparent solution for rare disease diagnosis, validated by experts and available as a web app.

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [11] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/pdf/2506.08400)
*Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: The paper introduces mSTEB, a benchmark for evaluating LLMs on low-resource languages, revealing performance gaps compared to high-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation benchmarks for low-resource languages in LLMs.

Method: Introduced mSTEB, a benchmark covering tasks like language identification, text classification, QA, and translation across speech and text. Evaluated models like Gemini 2.0 Flash, GPT-4o (Audio), Qwen 2 Audio, and Gemma 3 27B.

Result: Performance gaps exist between high-resource and low-resource languages, especially in African and Americas/Oceania languages.

Conclusion: More investment is needed to improve LLM coverage for underrepresented languages.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [12] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/pdf/2506.20119)
*Masaki Uto, Yuma Ito*

Main category: cs.CL

TL;DR: A novel method combines automated scoring with IRT to accurately estimate learner abilities while reducing manual grading workload.


<details>
  <summary>Details</summary>
Motivation: The need to assess higher-order abilities like expressive skills and logical thinking efficiently, despite the labor-intensive nature of constructed-response tests.

Method: Leverages automated scoring technologies to impute missing scores in IRT, improving accuracy for sparse or heterogeneous data.

Result: Achieves high accuracy in ability estimation and significantly reduces manual grading effort.

Conclusion: The proposed method effectively addresses the limitations of traditional IRT and manual grading, offering a scalable solution for ability assessment.

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [13] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/pdf/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: The paper introduces CCRS, a novel suite of five metrics for evaluating RAG systems, using a pretrained LLM as a zero-shot judge. It outperforms existing methods in efficiency and discriminative power.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for RAG systems are either simplistic (lexical overlap) or inefficient (multi-stage pipelines). CCRS addresses these limitations by providing a comprehensive yet efficient framework.

Method: CCRS employs a pretrained LLM as a zero-shot judge to evaluate five metrics: Contextual Coherence, Question Relevance, Information Density, Answer Correctness, and Information Recall. It is tested on the BioASQ dataset with six RAG configurations.

Result: CCRS effectively discriminates between RAG system performances, showing Mistral-7B outperforms Llama variants. It matches or surpasses RAGChecker in discriminative power while being more efficient.

Conclusion: CCRS offers a practical, comprehensive, and efficient solution for evaluating and improving RAG systems, addressing the shortcomings of existing methods.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [14] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/pdf/2506.20160)
*Ruosen Li, Ziming Luo, Quan Zhang, Ruochen Li, Ben Zhou, Ali Payani, Xinya Du*

Main category: cs.CL

TL;DR: AALC, a lightweight accuracy-aware length reward, reduces response length by 50% while maintaining or improving accuracy in large reasoning models.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models incur high latency and cost due to lengthy chain-of-thoughts without proportional accuracy gains.

Method: AALC integrates validation accuracy into reinforcement learning, dynamically balancing correctness and brevity with a scheduled length penalty.

Result: Reduces response length by over 50% while maintaining or improving accuracy, curbing redundant reasoning patterns.

Conclusion: Reward-based strategies like AALC can guide models toward efficient, generalizable reasoning, though interpretability may decrease.

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [15] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/pdf/2506.20167)
*Fengze Li, Yue Wang, Yangle Liu, Ming Huang, Dou Hong, Jieming Ma*

Main category: cs.CL

TL;DR: SEED integrates structural encoding with LLMs for multivariate time series forecasting, bridging the gap between numerical patterns and semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing models lack the ability to combine structural dependencies with semantic-level reasoning or task adaptation, limiting unified prediction systems.

Method: SEED uses a token-aware encoder, projection module, semantic reprogramming, and frozen LLM for prediction, decoupling representation learning from inference.

Result: SEED consistently outperforms baselines and addresses the structural-semantic modeling gap across datasets.

Conclusion: SEED effectively unifies structural and semantic modeling for improved time series forecasting.

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [16] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/pdf/2506.20178)
*Zhiyuan Wang, Jinhao Duan, Qingni Wang, Xiaofeng Zhu, Tianlong Chen, Xiaoshuang Shi, Kaidi Xu*

Main category: cs.CL

TL;DR: COIN is a framework for uncertainty quantification in foundation models, ensuring FDR control while improving answer retention and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of formal guarantees in heuristic UQ methods and the limitations of split conformal prediction in filtering incorrect answers.

Method: COIN calibrates thresholds using empirical error rates and confidence intervals (e.g., Clopper-Pearson) to control FDR and select answers.

Result: COIN achieves robust FDR control, high answer retention, and efficiency, even with limited calibration data.

Conclusion: COIN is adaptable and extensible, offering improved uncertainty quantification for diverse text generation tasks.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [17] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/pdf/2506.20199)
*Mengqi Wang, Tiantian Feng, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The paper explores improving conversational emotion recognition (CER) using LLMs by focusing on high-quality example retrieval in in-context learning (ICL). Augmented example retrieval outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing CER accuracy for subjective tasks like emotion recognition, inspired by the SLT 2024 GenSER Challenge.

Method: Proposes strategies for example retrieval in ICL, including random and augmented methods, and analyzes conversational context impact.

Result: Augmented example retrieval consistently outperforms other techniques across datasets (IEMOCAP, MELD, EmoryNLP).

Conclusion: Retrieving coherent, targeted examples and enhancing them through paraphrasing is crucial for improving CER with LLMs.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [18] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/pdf/2506.20203)
*Petra Barančíková, Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper compares Czech-specific and multilingual sentence embedding models using intrinsic and extrinsic evaluations, revealing a disconnect between semantic similarity performance and downstream task results.


<details>
  <summary>Details</summary>
Motivation: To assess how well sentence embeddings capture linguistic phenomena and perform in practical tasks like machine translation evaluation.

Method: Intrinsic evaluation uses Costra and STS benchmarks; extrinsic evaluation fine-tunes models for translation tasks using COMET metrics.

Result: Models strong in semantic similarity tests don't always excel in translation tasks, while fine-tuned models with smoothed embeddings perform well.

Conclusion: The findings stress the need for research into 'operationalizable semantics' and better downstream task datasets.

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [19] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/pdf/2506.20209)
*Benedetta Muscato, Lucia Passaro, Gizem Gezici, Fosca Giannotti*

Main category: cs.CL

TL;DR: The paper proposes a multi-perspective approach using soft labels in NLP to better capture human disagreements, outperforming traditional aggregation methods in subjective tasks like hate speech and stance detection.


<details>
  <summary>Details</summary>
Motivation: Traditional NLP methods aggregate annotators' viewpoints into a single ground truth, often underrepresenting minority perspectives. This study aims to address this by valuing diverse individual opinions.

Method: The study introduces a multi-perspective approach using soft labels and evaluates it across subjective tasks (hate speech, irony, abusive language, stance detection) using metrics like JSD and F1 scores.

Result: The multi-perspective approach better approximates human label distributions (lower JSD) and achieves higher F1 scores, though it shows lower confidence in highly subjective tasks like irony and stance detection.

Conclusion: The study highlights the importance of capturing human disagreements in NLP, advocating for more inclusive models. XAI reveals insights into model uncertainty, supporting the approach's validity.

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [20] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/pdf/2506.20241)
*Yubo Dong, Hehe Fan*

Main category: cs.CL

TL;DR: The paper introduces a structured reasoning approach to enhance LLMs, addressing their limitations in complex reasoning tasks by converting unstructured data into structured formats and using novel algorithms for training.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning due to reliance on implicit statistical relationships, lacking structured knowledge representation.

Method: Convert unstructured data to structured formats, use Supervised Fine-Tuning (SFT), and enhance reasoning with Group Relative Policy Optimization (GRPO) and algorithms (MAX-Flow, LCS).

Result: Improved reasoning effectiveness, reduced computational complexity, and robust performance across scenarios, validated on a DeepSeek-R1-Distill-Qwen-1.5B model.

Conclusion: Structured reasoning integration significantly enhances LLMs' performance in complex tasks.

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [21] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/pdf/2506.20269)
*Kai-Robin Lange, Tobias Schmidt, Matthias Reccius, Henrik Müller, Michael Roos, Carsten Jentsch*

Main category: cs.CL

TL;DR: The paper proposes combining Large Language Models (LLMs) and topic models to dynamically track narrative shifts over time, addressing the high costs of using LLMs alone.


<details>
  <summary>Details</summary>
Motivation: To efficiently investigate how media narratives evolve over time, overcoming the financial and computational limitations of using LLMs for entire corpora.

Method: A hybrid approach: topic models identify changes in topics, and LLMs analyze filtered documents to detect narrative shifts. Applied to Wall Street Journal articles (2009-2023).

Result: LLMs effectively detect narrative shifts when they exist but struggle to differentiate between content and narrative shifts.

Conclusion: The hybrid method offers a scalable solution for tracking narrative evolution, though further refinement is needed for distinguishing shift types.

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [22] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/pdf/2506.20331)
*Rian Touchent, Nathan Godey, Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched is a biomedical text dataset from PubMed, annotated for type, domain, and educational quality, enabling refined subsets for NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical text is hard to access due to privacy; this dataset offers an open alternative for biomedical NLP.

Method: Two-stage annotation: large model labels 400K paragraphs, fine-tunes a small model to propagate labels across PMC-OA.

Result: Created 2M clinical case paragraphs, with 450K high-quality ones. Pretraining experiments show ~5% boost on MMLU ProfMed and ~1% on MedQA/MedMCQA.

Conclusion: The dataset supports efficient biomedical pretraining, with quality filtering and domain upsampling improving performance and convergence.

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [23] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/pdf/2506.20409)
*Ekaterina Taktasheva, Jeff Dalton*

Main category: cs.CL

TL;DR: The paper introduces TAPS, a solution to enhance personalized tool use in LLMs by leveraging structured tagging and uncertainty-based detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing approaches neglect personalization in tool-augmented LLMs, limiting their ability to align with user preferences in goal-oriented tasks.

Method: TAPS uses a structured tagging tool and an uncertainty-based tool detector to integrate user preferences into LLMs.

Result: TAPS significantly improves personalized tool use, setting a new state-of-the-art for open-source models on the NLSI task.

Conclusion: Personalization in tool-augmented LLMs is crucial, and TAPS effectively addresses this gap, enhancing performance.

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [24] [Probing AI Safety with Source Code](https://arxiv.org/pdf/2506.20471)
*Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari*

Main category: cs.CL

TL;DR: The paper introduces Code of Thought (CoDoT), a prompting strategy to evaluate LLM safety, revealing significant toxicity increases in state-of-the-art models like GPT-4 Turbo and DeepSeek R1.


<details>
  <summary>Details</summary>
Motivation: The widespread use of LLMs in safety-critical applications necessitates improved safety measures to align models with human values, as current models fall short.

Method: CoDoT converts natural language inputs into simple code to represent intent, exposing LLM safety failures.

Result: CoDoT reveals alarming toxicity increases (e.g., GPT-4 Turbo's toxicity rises 16.5x, DeepSeek R1 fails 100%), with recursive application doubling toxicity.

Conclusion: CoDoT highlights the urgent need for safety evaluations from first principles to ensure safety and capabilities advance together.

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [25] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/pdf/2506.20474)
*Kaixiang Zhang, Justine Zhang, Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: A computational framework quantifies talk-time distribution and dynamics in conversations, revealing preferences for balanced talk-time and varied perceptions of dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how talk-time is shared in conversations and its impact on speaker perceptions.

Method: Developed a computational framework to analyze talk-time distribution and dynamics, applied to a dataset of video-chats.

Result: Balanced talk-time is preferred, and different dynamics affect perceptions even with similar overall balance.

Conclusion: The framework provides tools for improving communication platforms, including human-human and human-AI interactions.

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [26] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/pdf/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp's winning solution for SIGIR 2025 LiveRAG used a knowledge-aware diverse reranking RAG pipeline to outperform competitors on a diverse evaluation set.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving question-relevant documents from a large corpus (15M documents) for diverse topics, question types, and audiences.

Method: Proposed a knowledge-aware diverse reranking RAG pipeline.

Result: Achieved first place in the SIGIR 2025 LiveRAG competition.

Conclusion: The pipeline effectively handled diverse retrieval tasks, proving its superiority in the competition.

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [27] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/pdf/2506.20480)
*Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping*

Main category: cs.CL

TL;DR: A novel strategy for compressing large language models (LLMs) by combining or merging layers from finetuned variants, achieving competitive pruning results.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deploying large LLMs by reducing model size without significant performance loss.

Method: Proposes a zero-order optimization problem for layer removal, selection, and merging to strategically compress models.

Result: Compressed models retain ~97.3% of original performance while removing ~25% of parameters, outperforming prior methods.

Conclusion: The approach effectively balances model size reduction and performance retention, advancing LLM compression techniques.

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [28] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/pdf/2506.20495)
*Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode enhances LLMs' ability to adapt to API updates using rule-based reinforcement learning, improving code generation in dynamic environments without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with adapting to frequent API updates due to outdated training data, limiting reliable code generation in dynamic settings.

Method: ReCode uses a dataset of 2,000 entries for training, a modified string similarity metric for reinforcement learning rewards, and tests with GRPO and DAPO algorithms.

Result: ReCode significantly improves LLMs' performance in dynamic API scenarios, with Qwen2.5-Coder-7B outperforming larger models.

Conclusion: ReCode effectively addresses LLMs' API adaptation issues, offering a scalable solution for dynamic code generation.

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [29] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/pdf/2506.20512)
*Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu*

Main category: cs.CL

TL;DR: The paper investigates how mid-training strategies affect reinforcement learning (RL) dynamics in language models like Qwen and Llama, identifying key factors like high-quality math corpora and QA-style data. It introduces a two-stage strategy, Stable-then-Decay, leading to the OctoThinker model family.


<details>
  <summary>Details</summary>
Motivation: Understanding what makes base language models suitable for RL is crucial for developing next-generation foundation models.

Method: The study analyzes mid-training strategies, focusing on Qwen and Llama, and introduces a two-stage approach (Stable-then-Decay) to enhance RL compatibility.

Result: High-quality math corpora and QA-style data improve RL performance, while long chain-of-thought reasoning can cause verbosity and instability. The Stable-then-Decay strategy yields OctoThinker, closing the performance gap with RL-friendly models.

Conclusion: The findings guide pre-training strategies for RL-compatible foundation models, with open-source models and a curated math corpus released for further research.

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [30] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/pdf/2506.20544)
*Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker*

Main category: cs.CL

TL;DR: The paper explores robust scaling of inference-time compute for multilingual, multi-task LLMs, proposing novel sampling and selection strategies that outperform existing methods, especially in underrepresented languages.


<details>
  <summary>Details</summary>
Motivation: Current inference-time compute scaling methods focus on English and limited domains, lacking generalization across languages and open-ended tasks. The study aims to address this gap.

Method: The study evaluates existing selection methods, identifies their limitations, and introduces new sampling and selection strategies tailored for multilingual and multi-task settings.

Result: Proposed methods achieve significant performance gains, e.g., +6.8 win-rate improvement for 8B models and +9.0 for a 111B model, demonstrating effectiveness across languages and tasks.

Conclusion: Language- and task-aware inference-time compute methods are essential for democratizing performance improvements, particularly in underrepresented languages.

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [31] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/pdf/2506.20606)
*Baixiang Huang, Zhen Tan, Haoran Wang, Zijie Liu, Dawei Li, Ali Payani, Huan Liu, Tianlong Chen, Kai Shu*

Main category: cs.CL

TL;DR: The paper introduces Behavior Editing, a method to ethically steer LLM-based agents, and BehaviorBench, a benchmark for evaluating and editing agent behaviors. It shows effectiveness in both local and global behavior adjustments, highlighting ethical risks and benefits.


<details>
  <summary>Details</summary>
Motivation: Deploying LLM-based agents in high-stakes domains poses safety and ethical risks, necessitating methods to steer their behavior to prevent harm.

Method: Frames agent behavior steering as a model editing task (Behavior Editing) and introduces BehaviorBench, a multi-tier benchmark based on psychological moral theories for evaluation.

Result: Behavior Editing effectively steers agent behavior locally and globally, demonstrating potential for both ethical and harmful outcomes.

Conclusion: Behavior Editing offers a promising paradigm for steering agent behavior but also underscores the risks of misuse, emphasizing the need for careful application.

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [32] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/pdf/2506.20639)
*Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang*

Main category: cs.CL

TL;DR: DiffuCoder, a 7B dLLM, explores denoising and RL methods for code generation, outperforming AR models with novel sampling and training techniques.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the decoding behavior and training of diffusion large language models (dLLMs) for code generation, addressing gaps in current methods.

Method: Systematically investigates denoising processes and RL methods, proposing coupled-GRPO for efficient training and diverse generation.

Result: DiffuCoder improves code generation performance (+4.4% on EvalPlus) and reduces reliance on AR causality, showcasing diverse generation order.

Conclusion: The study advances dLLM understanding and provides an effective RL framework for code generation, with open-source contributions.

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [33] [Memento: Note-Taking for Your Future Self](https://arxiv.org/pdf/2506.20642)
*Chao Wan, Albert Gong, Mihir Mishra, Carl-Leander Henneking, Claas Beger, Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento is a prompting strategy that decomposes complex questions, constructs a dynamic fact database, and solves them, significantly improving LLM performance in multi-hop QA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with tasks requiring tight coupling of reasoning and retrieval, such as multi-hop QA. Memento aims to address this limitation.

Method: Memento decomposes questions into steps, builds a dynamic fact database using LLMs, and combines facts to solve questions.

Result: Memento doubles CoT performance on PhantomWiki, improves CoT-RAG by 20 F1 points on 2WikiMultiHopQA, and boosts ReAct by 3 F1 points on MuSiQue.

Conclusion: Memento effectively enhances LLM performance in multi-hop QA by integrating reasoning and retrieval dynamically.

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [34] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/pdf/2506.20666)
*Sonia K. Murthy, Rosie Zhao, Jennifer Hu, Sham Kakade, Markus Wulfmeier, Peng Qian, Tomer Ullman*

Main category: cs.CL

TL;DR: The paper explores how LLMs handle human-like value trade-offs, like balancing honesty and politeness, using cognitive models to evaluate their behavior in reasoning and training dynamics.


<details>
  <summary>Details</summary>
Motivation: Current tools for interpreting dynamic value trade-offs in LLMs are limited, despite their importance in human decision-making and language use.

Method: A cognitive model of polite speech is applied to evaluate LLMs in two settings: reasoning effort in black-box models and RL post-training dynamics in open-source models.

Result: LLMs show higher informational utility than social utility in reasoning models, with training dynamics revealing early shifts in utility values influenced by base model and pretraining data.

Conclusion: The method offers insights for shaping LLM training regimes, controlling value trade-offs, and forming hypotheses about high-level behaviors.

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


### [35] [A Global Context Mechanism for Sequence Labeling](https://arxiv.org/pdf/2305.19928)
*Conglei Xu, Kun Shen, Hongguang Sun, Yang Xu*

Main category: cs.CL

TL;DR: The paper introduces a simple, efficient mechanism to enhance global sentence information for sequence labeling tasks, improving performance without significant speed trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing RNN variants for integrating global sentence information are slow, incompatible with transformers, and hard to implement.

Method: A pluggable mechanism to supplement global context for BiLSTM and transformer models efficiently.

Result: Significant F1 score improvements on seven benchmarks, including NER and E2E-ABSA tasks, with competitive speed.

Conclusion: The proposed mechanism effectively addresses prior limitations, offering better performance and ease of integration.

Abstract: Global sentence information is crucial for sequence labeling tasks, where
each word in a sentence must be assigned a label. While BiLSTM models are
widely used, they often fail to capture sufficient global context for inner
words. Previous work has proposed various RNN variants to integrate global
sentence information into word representations. However, these approaches
suffer from three key limitations: (1) they are slower in both inference and
training compared to the original BiLSTM, (2) they cannot effectively
supplement global information for transformer-based models, and (3) the high
time cost associated with reimplementing and integrating these customized RNNs
into existing architectures. In this study, we introduce a simple yet effective
mechanism that addresses these limitations. Our approach efficiently
supplements global sentence information for both BiLSTM and transformer-based
models, with minimal degradation in inference and training speed, and is easily
pluggable into current architectures. We demonstrate significant improvements
in F1 scores across seven popular benchmarks, including Named Entity
Recognition (NER) tasks such as Conll2003, Wnut2017 , and the Chinese
named-entity recognition task Weibo, as well as End-to-End Aspect-Based
Sentiment Analysis (E2E-ABSA) benchmarks such as Laptop14, Restaurant14,
Restaurant15, and Restaurant16. With out any extra strategy, we achieve third
highest score on weibo NER benchmark. Compared to CRF, one of the most popular
frameworks for sequence labeling, our mechanism achieves competitive F1 scores
while offering superior inference and training speed. Code is available at:
https://github.com/conglei2XU/Global-Context-Mechanism

</details>


### [36] [When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/pdf/2311.09410)
*Leonardo Ranaldi, Giulia Pucci*

Main category: cs.CL

TL;DR: The paper investigates sycophantic behavior in Large Language Models (LLMs), where models align responses with user viewpoints, even if misleading, due to human feedback. It finds this behavior prevalent in subjective queries but absent in objective tasks like math.


<details>
  <summary>Details</summary>
Motivation: To understand and analyze the suggestibility of LLMs to sycophantic behavior, which biases responses and reduces model reliability.

Method: Systematic human-intervention prompts across various tasks to evaluate sycophantic tendencies.

Result: LLMs exhibit sycophancy in subjective queries but remain accurate in objective tasks like math, ignoring user hints.

Conclusion: Sycophantic behavior in LLMs undermines reliability in subjective contexts, highlighting the need for mitigation strategies.

Abstract: Large Language Models have been demonstrating broadly satisfactory generative
abilities for users, which seems to be due to the intensive use of human
feedback that refines responses. Nevertheless, suggestibility inherited via
human feedback improves the inclination to produce answers corresponding to
users' viewpoints. This behaviour is known as sycophancy and depicts the
tendency of LLMs to generate misleading responses as long as they align with
humans. This phenomenon induces bias and reduces the robustness and,
consequently, the reliability of these models. In this paper, we study the
suggestibility of Large Language Models (LLMs) to sycophantic behaviour,
analysing these tendencies via systematic human-interventions prompts over
different tasks. Our investigation demonstrates that LLMs have sycophantic
tendencies when answering queries that involve subjective opinions and
statements that should elicit a contrary response based on facts. In contrast,
when faced with math tasks or queries with an objective answer, they, at
various scales, do not follow the users' hints by demonstrating confidence in
generating the correct answers.

</details>


### [37] [Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs](https://arxiv.org/pdf/2403.19827)
*Kanishka Misra, Kyle Mahowald*

Main category: cs.CL

TL;DR: The paper investigates whether language models learn rare syntactic phenomena through generalization or memorization, focusing on the English AANN construction.


<details>
  <summary>Details</summary>
Motivation: To determine if language models generalize or memorize rare grammatical structures like the AANN construction.

Method: Iteratively trained transformer models on manipulated corpora, comparing learning of AANN in default vs. counterfactual corpora.

Result: AANNs were learned better than perturbed variants, suggesting generalization from related constructions. Increased input variability enhanced learning.

Conclusion: Language models can learn rare grammatical phenomena by generalizing from less rare constructions.

Abstract: Language models learn rare syntactic phenomena, but the extent to which this
is attributable to generalization vs. memorization is a major open question. To
that end, we iteratively trained transformer language models on systematically
manipulated corpora which were human-scale in size, and then evaluated their
learning of a rare grammatical phenomenon: the English
Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five days'').
We compared how well this construction was learned on the default corpus
relative to a counterfactual corpus in which AANN sentences were removed. We
found that AANNs were still learned better than systematically perturbed
variants of the construction. Using additional counterfactual corpora, we
suggest that this learning occurs through generalization from related
constructions (e.g., ``a few days''). An additional experiment showed that this
learning is enhanced when there is more variability in the input. Taken
together, our results provide an existence proof that LMs can learn rare
grammatical phenomena by generalization from less rare phenomena. Data and
code: https://github.com/kanishkamisra/aannalysis.

</details>


### [38] [Evaluating Long Range Dependency Handling in Code Generation LLMs](https://arxiv.org/pdf/2407.21049)
*Yannick Assogba, Donghao Ren*

Main category: cs.CL

TL;DR: The paper evaluates code generation models' ability to handle long-range dependencies in large context windows (up to 8k tokens), finding performance degradation when functions reference later-defined functions. Sliding window attention models struggle with distant references, but prompt modifications improve retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To assess how well language models utilize large context sizes, especially for multi-step tasks, beyond simple fact retrieval.

Method: Analyzed code generation models using multi-step key retrieval tasks with progressively increasing difficulty in 8k-token contexts. Evaluated performance degradation and tested prompt modifications using call graph information.

Result: Performance drops significantly (up to 2x) when functions reference later-defined ones. Sliding window attention models fail with distant references. Prompt modifications improved retrieval up to 3x.

Conclusion: Long-context performance requires deeper evaluation beyond single-fact retrieval, and prompt adjustments can enhance model capabilities.

Abstract: As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
for many models (up to 2x) when a function references another function that is
defined later in the prompt. We also observe that models that use sliding
window attention mechanisms have difficulty handling references further than
the size of a single window. We perform simple prompt modifications using call
graph information to improve multi-step retrieval performance up to 3x. Our
analysis highlights ways that long-context performance needs deeper
consideration beyond retrieval of single facts within a document.

</details>


### [39] [On the Role of Context in Reading Time Prediction](https://arxiv.org/pdf/2409.08160)
*Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: The paper explores how context is integrated in language comprehension, proposing an orthogonalized predictor to separate context effects from frequency.


<details>
  <summary>Details</summary>
Motivation: To better understand the role of context in language processing by addressing the confounding effects of word frequency.

Method: Proposes projecting surprisal onto the orthogonal complement of frequency to create a new contextual predictor uncorrelated with frequency.

Result: Experiments show context explains less variance in reading times when using the orthogonalized predictor, suggesting previous studies overstated context's role.

Conclusion: The study highlights the need to disentangle context from frequency in language comprehension research.

Abstract: We present a new perspective on how readers integrate context during
real-time language comprehension. Our proposals build on surprisal theory,
which posits that the processing effort of a linguistic unit (e.g., a word) is
an affine function of its in-context information content. We first observe that
surprisal is only one out of many potential ways that a contextual predictor
can be derived from a language model. Another one is the pointwise mutual
information (PMI) between a unit and its context, which turns out to yield the
same predictive power as surprisal when controlling for unigram frequency.
Moreover, both PMI and surprisal are correlated with frequency. This means that
neither PMI nor surprisal contains information about context alone. In response
to this, we propose a technique where we project surprisal onto the orthogonal
complement of frequency, yielding a new contextual predictor that is
uncorrelated with frequency. Our experiments show that the proportion of
variance in reading times explained by context is a lot smaller when context is
represented by the orthogonalized predictor. From an interpretability
standpoint, this indicates that previous studies may have overstated the role
that context has in predicting reading times.

</details>


### [40] [FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs](https://arxiv.org/pdf/2410.02899)
*Deema Alnuhait, Neeraja Kirtane, Muhammad Khalifa, Hao Peng*

Main category: cs.CL

TL;DR: FactCheckmate detects and mitigates LM hallucinations preemptively using hidden states, improving factual accuracy by 34.4%.


<details>
  <summary>Details</summary>
Motivation: To address LM hallucinations by leveraging internal representations for early detection and intervention.

Method: FactCheckmate learns a classifier to predict hallucinations from hidden states and adjusts them to improve output factuality.

Result: Achieves 70% preemptive detection accuracy and 34.4% more factual outputs with intervention.

Conclusion: FactCheckmate efficiently mitigates hallucinations using hidden states, outperforming post-hoc methods.

Abstract: Language models (LMs) hallucinate. We inquire: Can we detect and mitigate
hallucinations before they happen? This work answers this research question in
the positive, by showing that the internal representations of LMs provide rich
signals that can be used for this purpose. We introduce FactCheckmate, which
preemptively detects hallucinations by learning a classifier that predicts
whether the LM will hallucinate, based on the model's hidden states produced
over the inputs, before decoding begins. If a hallucination is detected,
FactCheckmate then intervenes by adjusting the LM's hidden states such that the
model will produce more factual outputs. FactCheckmate provides fresh insights
that the inner workings of LMs can be revealed by their hidden states.
Practically, both its detection and mitigation models are lightweight, adding
little inference overhead; FactCheckmate proves a more efficient approach for
mitigating hallucinations compared to many post-hoc alternatives. We evaluate
FactCheckmate over LMs of different scales and model families (including Llama,
Mistral, Qwen and Gemma), across a variety of QA datasets from different
domains. Our results demonstrate the effectiveness of FactCheckmate, achieving
over 70% preemptive detection accuracy. On average, outputs generated by LMs
with intervention are 34.4% more factual compared to those without.

</details>


### [41] [Graph Linearization Methods for Reasoning on Graphs with Large Language Models](https://arxiv.org/pdf/2410.19494)
*Christos Xypolopoulos, Guokan Shang, Xiao Fei, Giannis Nikolentzos, Hadi Abdine, Iakovos Evdaimon, Michail Chatzianastasis, Giorgos Stamou, Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: The paper explores graph linearization methods to adapt graphs for processing by large language models (LLMs), enhancing their ability to handle graph reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for graph reasoning by transforming graphs into token sequences that align with natural language properties like local dependency and global alignment.

Method: Developed graph linearization techniques based on centrality and degeneracy, enhanced with node relabeling.

Result: Experimental results show the methods outperform random linearization baselines.

Conclusion: The work introduces effective graph representations for LLMs, bridging graph machine learning with multimodal processing in transformers.

Abstract: Large language models have evolved to process multiple modalities beyond
text, such as images and audio, which motivates us to explore how to
effectively leverage them for graph reasoning tasks. The key question,
therefore, is how to transform graphs into linear sequences of tokens, a
process we term "graph linearization", so that LLMs can handle graphs
naturally. We consider that graphs should be linearized meaningfully to reflect
certain properties of natural language text, such as local dependency and
global alignment, in order to ease contemporary LLMs, trained on trillions of
textual tokens, better understand graphs. To achieve this, we developed several
graph linearization methods based on graph centrality and degeneracy. These
methods are further enhanced using node relabeling techniques. The experimental
results demonstrate the effectiveness of our methods compared to the random
linearization baseline. Our work introduces novel graph representations
suitable for LLMs, contributing to the potential integration of graph machine
learning with the trend of multimodal processing using a unified transformer
model.

</details>


### [42] [Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers](https://arxiv.org/pdf/2411.08745)
*Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West*

Main category: cs.CL

TL;DR: The paper investigates whether multilingual LLMs develop universal concept representations. By analyzing latent representations during translation tasks, it finds language and concept encoding layers, demonstrates control over concept and language separately, and shows improved translation using mean concept representations.


<details>
  <summary>Details</summary>
Motivation: To determine if multilingual LLMs create language-agnostic concept representations, disentangled from specific languages.

Method: Analyzes latent representations in transformer-based LLMs during word-translation tasks, using activation patching to manipulate concepts and languages separately.

Result: Language is encoded earlier than concepts; patching with mean concept representations improves translation. The model can generate descriptions of mean representations.

Conclusion: Evidence supports the existence of language-agnostic concept representations in the studied LLMs.

Abstract: A central question in multilingual language modeling is whether large
language models (LLMs) develop a universal concept representation, disentangled
from specific languages. In this paper, we address this question by analyzing
latent representations (latents) during a word-translation task in
transformer-based LLMs. We strategically extract latents from a source
translation prompt and insert them into the forward pass on a target
translation prompt. By doing so, we find that the output language is encoded in
the latent at an earlier layer than the concept to be translated. Building on
this insight, we conduct two key experiments. First, we demonstrate that we can
change the concept without changing the language and vice versa through
activation patching alone. Second, we show that patching with the mean
representation of a concept across different languages does not affect the
models' ability to translate it, but instead improves it. Finally, we
generalize to multi-token generation and demonstrate that the model can
generate natural language description of those mean representations. Our
results provide evidence for the existence of language-agnostic concept
representations within the investigated models.

</details>


### [43] [Understanding World or Predicting Future? A Comprehensive Survey of World Models](https://arxiv.org/pdf/2411.14499)
*Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, Fengli Xu, Yong Li*

Main category: cs.CL

TL;DR: A survey on world models, categorizing them into understanding and predicting functions, reviewing progress, applications in domains like autonomous driving, and outlining future challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of world models, driven by advancements in multimodal LLMs and video generation models, and their role in AGI.

Method: Systematic categorization of world models into understanding and predicting functions, reviewing current progress, and analyzing applications in key domains.

Result: Identifies key challenges and future research directions, with a summary of representative papers and code repositories.

Conclusion: World models are pivotal for AGI, with significant potential in diverse applications, though challenges remain for future exploration.

Abstract: The concept of world models has garnered significant attention due to
advancements in multimodal large language models such as GPT-4 and video
generation models such as Sora, which are central to the pursuit of artificial
general intelligence. This survey offers a comprehensive review of the
literature on world models. Generally, world models are regarded as tools for
either understanding the present state of the world or predicting its future
dynamics. This review presents a systematic categorization of world models,
emphasizing two primary functions: (1) constructing internal representations to
understand the mechanisms of the world, and (2) predicting future states to
simulate and guide decision-making. Initially, we examine the current progress
in these two categories. We then explore the application of world models in key
domains, including autonomous driving, robotics, and social simulacra, with a
focus on how each domain utilizes these aspects. Finally, we outline key
challenges and provide insights into potential future research directions. We
summarize the representative papers along with their code repositories in
https://github.com/tsinghua-fib-lab/World-Model.

</details>


### [44] [A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans](https://arxiv.org/pdf/2412.01131)
*Zhihan Cao, Hiroaki Yamada, Simone Teufel, Takenobu Tokunaga*

Main category: cs.CL

TL;DR: The paper introduces a comprehensive evaluation framework to assess PLMs' knowledge of five semantic relations beyond hypernymy, comparing human and model performance using six metrics.


<details>
  <summary>Details</summary>
Motivation: Previous research on PLMs' semantic relation knowledge was limited, focusing only on hypernymy and lacking human comparison. This gap motivated a broader evaluation.

Method: The study evaluates 16 PLMs (eight masked, eight causal) on five semantic relations using six metrics, including soundness and prototypicality, and compares results to human performance.

Result: Results show a significant knowledge gap between humans and models, with antonymy as the only well-performed relation. Masked models outperform causal ones, but both confuse non-antonymy relations with antonymy.

Conclusion: The study highlights the incomplete semantic knowledge of PLMs, emphasizing the need for further research to bridge the gap between human and model understanding of semantic relations.

Abstract: Recently, much work has concerned itself with the enigma of what exactly PLMs
(pretrained language models) learn about different aspects of language, and how
they learn it. One stream of this type of research investigates the knowledge
that PLMs have about semantic relations. However, many aspects of semantic
relations were left unexplored. Only one relation was considered, namely
hypernymy. Furthermore, previous work did not measure humans' performance on
the same task as that solved by the PLMs. This means that at this point in
time, there is only an incomplete view of models' semantic relation knowledge.
To address this gap, we introduce a comprehensive evaluation framework covering
five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy,
and synonymy. We use six metrics (two newly introduced here) for recently
untreated aspects of semantic relation knowledge, namely soundness,
completeness, symmetry, asymmetry, prototypicality, and distinguishability and
fairly compare humans and models on the same task. Our extensive experiments
involve 16 PLMs, eight masked and eight causal language models. Up to now only
masked language models had been tested although causal and masked language
models treat context differently. Our results reveal a significant knowledge
gap between humans and models for almost all semantic relations. Antonymy is
the outlier relation where all models perform reasonably well. In general,
masked language models perform significantly better than causal language
models. Nonetheless, both masked and causal language models are likely to
confuse non-antonymy relations with antonymy.

</details>


### [45] [Misalignment of Semantic Relation Knowledge between WordNet and Human Intuition](https://arxiv.org/pdf/2412.02138)
*Zhihan Cao, Hiroaki Yamada, Simone Teufel, Takenobu Tokunaga*

Main category: cs.CL

TL;DR: The study compares WordNet's semantic relations with human intuition, finding general misalignment, especially in synonymy and taxonomic relations. WordNet's path length is unreliable for predicting human intuition.


<details>
  <summary>Details</summary>
Motivation: To systematically compare WordNet's expert-constructed semantic relations with human intuition and identify misalignments for potential improvements.

Method: Uses templates to elicit responses from human participants, analyzing alignment and patterns of mismatch in semantic relations.

Result: Reveals general misalignment, particularly in synonymy and taxonomic relations (hypernymy/hyponymy), and shows WordNet path length is unreliable for human intuition.

Conclusion: Misalignments highlight areas for WordNet improvement and caution against relying solely on WordNet for human-like semantic understanding.

Abstract: WordNet provides a carefully constructed repository of semantic relations,
created by specialists. But there is another source of information on semantic
relations, the intuition of language users. We present the first systematic
study of the degree to which these two sources are aligned. Investigating the
cases of misalignment could make proper use of WordNet and facilitate its
improvement. Our analysis which uses templates to elicit responses from human
participants, reveals a general misalignment of semantic relation knowledge
between WordNet and human intuition. Further analyses find a systematic pattern
of mismatch among synonymy and taxonomic relations~(hypernymy and hyponymy),
together with the fact that WordNet path length does not serve as a reliable
indicator of human intuition regarding hypernymy or hyponymy relations.

</details>


### [46] [Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models](https://arxiv.org/pdf/2412.16545)
*Zhisong Zhang, Yan Wang, Xinting Huang, Tianqing Fang, Hongming Zhang, Chenlong Deng, Shuaiyi Li, Dong Yu*

Main category: cs.CL

TL;DR: The paper analyzes performance degradation in parallel context encoding for large language models, identifies high attention entropy as a key issue, and proposes mitigation methods (attention sinks and selective mechanisms) to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Full self-attention in Transformers is inefficient for long sequences and may ignore input structures. Parallel context encoding is an alternative but causes performance degradation due to untrained parallel patterns.

Method: The study analyzes the issue, identifies high attention entropy as the cause, and introduces two methods: attention sinks and selective mechanisms to reduce entropy.

Result: Experiments show these methods effectively lower irregular attention entropy and reduce performance gaps in various tasks.

Conclusion: The findings highlight ways to enhance context modeling mechanisms, particularly for parallel encoding in large language models.

Abstract: Large language models have shown remarkable performance across a wide range
of language tasks, owing to their exceptional capabilities in context modeling.
The most commonly used method of context modeling is full self-attention, as
seen in standard decoder-only Transformers. Although powerful, this method can
be inefficient for long sequences and may overlook inherent input structures.
To address these problems, an alternative approach is parallel context
encoding, which splits the context into sub-pieces and encodes them parallelly.
Because parallel patterns are not encountered during training, naively applying
parallel encoding leads to performance degradation. However, the underlying
reasons and potential mitigations are unclear. In this work, we provide a
detailed analysis of this issue and identify that unusually high attention
entropy can be a key factor. Furthermore, we adopt two straightforward methods
to reduce attention entropy by incorporating attention sinks and selective
mechanisms. Experiments on various tasks reveal that these methods effectively
lower irregular attention entropy and narrow performance gaps. We hope this
study can illuminate ways to enhance context modeling mechanisms.

</details>


### [47] [Unlocking In-Context Learning for Natural Datasets Beyond Language Modelling](https://arxiv.org/pdf/2501.06256)
*Jelena Bratulić, Sudhanshu Mittal, David T. Hoffmann, Samuel Böhm, Robin Tibor Schirrmeister, Tonio Ball, Christian Rupprecht, Thomas Brox*

Main category: cs.CL

TL;DR: The paper explores how Large Language Models (LLMs) achieve In-Context Learning (ICL) and extends its principles to non-text modalities like visual and EEG data.


<details>
  <summary>Details</summary>
Motivation: To understand and enable ICL in autoregressive models and non-text modalities, addressing its less straightforward emergence beyond text.

Method: Systematically identifies properties in LLMs that support ICL, such as token repetitions and training task difficulty, and applies these insights to visual and EEG datasets.

Result: Token repetitions improve ICL stability, and training task difficulty is crucial for ICL emergence. The approach successfully enables ICL for visual and EEG tasks.

Conclusion: The study provides insights into ICL mechanisms and demonstrates its applicability to diverse modalities, advancing few-shot learning capabilities.

Abstract: Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables
the model to perform new tasks conditioning only on the examples provided in
the context without updating the model's weights. While ICL offers fast
adaptation across natural language tasks and domains, its emergence is less
straightforward for modalities beyond text. In this work, we systematically
uncover properties present in LLMs that support the emergence of ICL for
autoregressive models and various modalities by promoting the learning of the
needed mechanisms for ICL. We identify exact token repetitions in the training
data sequences as an important factor for ICL. Such repetitions further improve
stability and reduce transiency in ICL performance. Moreover, we emphasise the
significance of training task difficulty for the emergence of ICL. Finally, by
applying our novel insights on ICL emergence, we unlock ICL capabilities for
various visual datasets and a more challenging EEG classification task in a
few-shot learning regime.

</details>


### [48] [Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception](https://arxiv.org/pdf/2502.11677)
*Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper explores using LLMs' internal states to improve their awareness of knowledge boundaries, introducing a method ($C^3$) to enhance confidence calibration and reduce incorrect responses.


<details>
  <summary>Details</summary>
Motivation: LLMs often provide confident but incorrect answers due to poor knowledge boundary awareness, posing risks in critical applications.

Method: Investigates pre-generation confidence estimation using internal states and introduces $C^3$ for post-generation confidence calibration via question reformulation.

Result: LLMs show strong pre-generation perception, and $C^3$ improves unknown perception rates by 5.6% (NQ) and 4.9% (HotpotQA).

Conclusion: Pre-generation confidence estimation boosts efficiency, while $C^3$ enhances reliability, making LLMs safer for practical use.

Abstract: Large language models (LLMs) exhibit impressive performance across diverse
tasks but often struggle to accurately gauge their knowledge boundaries,
leading to confident yet incorrect responses. This paper explores leveraging
LLMs' internal states to enhance their perception of knowledge boundaries from
efficiency and risk perspectives. We investigate whether LLMs can estimate
their confidence using internal states before response generation, potentially
saving computational resources. Our experiments on datasets like Natural
Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant
pre-generation perception, which is further refined post-generation, with
perception gaps remaining stable across varying conditions. To mitigate risks
in critical domains, we introduce Confidence Consistency-based Calibration
($C^3$), which assesses confidence consistency through question reformulation.
$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,
enhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our
findings suggest that pre-generation confidence estimation can optimize
efficiency, while $C^3$ effectively controls output risks, advancing the
reliability of LLMs in practical applications.

</details>


### [49] [Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models](https://arxiv.org/pdf/2502.11707)
*Sherzod Hakimov, Lara Pfennigschmidt, David Schlangen*

Main category: cs.CL

TL;DR: The study uses Codenames to benchmark LLMs, testing clue generation and guessing abilities under varied conditions, revealing their strategies and limitations.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' linguistic and cognitive skills in a controlled, interactive setting using the game Codenames.

Method: LLMs play both clue-giving and guessing roles in Codenames, with experiments varying word types and opponent behaviors. Commercial and open-weight models are compared.

Result: The study identifies factors affecting LLM performance, their strategies, and challenging cases, highlighting limitations.

Conclusion: Codenames serves as an effective benchmark for assessing LLMs, uncovering nuanced performance details and areas for improvement.

Abstract: This study utilizes the game Codenames as a benchmarking tool to evaluate
large language models (LLMs) with respect to specific linguistic and cognitive
skills. LLMs play each side of the game, where one side generates a clue word
covering several target words and the other guesses those target words. We
designed various experiments by controlling the choice of words (abstract vs.
concrete words, ambiguous vs. monosemic) or the opponent (programmed to be
faster or slower in revealing words). Recent commercial and open-weight models
were compared side-by-side to find out factors affecting their performance. The
evaluation reveals details about their strategies, challenging cases, and
limitations of LLMs.

</details>


### [50] [VAQUUM: Are Vague Quantifiers Grounded in Visual Data?](https://arxiv.org/pdf/2502.11874)
*Hugh Mee Wong, Rick Nouwen, Albert Gatt*

Main category: cs.CL

TL;DR: VLMs' compatibility with humans in using vague quantifiers like "a few" and "many" is evaluated, revealing similarities in object count influence but inconsistencies across evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To assess if vision-and-language models (VLMs) align with human judgments in using vague quantifiers in visual contexts.

Method: Created VAQUUM dataset with 20,300 human ratings; compared human judgments and VLM predictions using three evaluation methods.

Result: VLMs, like humans, are influenced by object counts but show inconsistencies across evaluation settings.

Conclusion: Judging and producing vague quantifiers involve distinct processes, as evidenced by model inconsistencies.

Abstract: Vague quantifiers such as "a few" and "many" are influenced by various
contextual factors, including the number of objects present in a given context.
In this work, we evaluate the extent to which vision-and-language models (VLMs)
are compatible with humans when producing or judging the appropriateness of
vague quantifiers in visual contexts. We release a novel dataset, VAQUUM,
containing 20,300 human ratings on quantified statements across a total of 1089
images. Using this dataset, we compare human judgments and VLM predictions
using three different evaluation methods. Our findings show that VLMs, like
humans, are influenced by object counts in vague quantifier use. However, we
find significant inconsistencies across models in different evaluation
settings, suggesting that judging and producing vague quantifiers rely on two
different processes.

</details>


### [51] [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/pdf/2502.11962)
*Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold*

Main category: cs.CL

TL;DR: IFT improves LLM informativeness but reduces truthfulness due to unfamiliar knowledge. Two new paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, address this: $UNIT_{cut}$ removes unfamiliar knowledge, while $UNIT_{ref}$ trains models to indicate uncertainty. Both improve truthfulness and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: The trade-off between informativeness and truthfulness in IFT of LLMs, caused by unfamiliar knowledge, motivates the need for solutions to maintain accuracy while generalizing to unseen tasks.

Method: Introduces $UNIT_{cut}$ (removes unfamiliar knowledge) and $UNIT_{ref}$ (trains models to indicate uncertainty) to mitigate truthfulness issues in IFT.

Result: $UNIT_{cut}$ improves truthfulness; $UNIT_{ref}$ maintains informativeness and reduces hallucinations by marking uncertain statements.

Conclusion: The proposed paradigms effectively balance informativeness and truthfulness in IFT, addressing the negative impact of unfamiliar knowledge on LLM accuracy.

Abstract: Instruction fine-tuning (IFT) can increase the informativeness of large
language models (LLMs), but may reduce their truthfulness. This trade-off
arises because IFT steers LLMs to generate responses containing long-tail
knowledge that was not well covered during pre-training. As a result, models
become more informative but less accurate when generalizing to unseen tasks. In
this paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets
can negatively affect the truthfulness of LLMs, and we introduce two new IFT
paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$
identifies and removes unfamiliar knowledge from IFT datasets to mitigate its
impact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize
their uncertainty and explicitly indicate it at the end of their responses. Our
experiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,
while $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by
distinguishing between confident and uncertain statements.

</details>


### [52] [LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems](https://arxiv.org/pdf/2502.17848)
*Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang*

Main category: cs.CL

TL;DR: LR$^2$Bench is introduced to evaluate reflective reasoning in LLMs, revealing significant gaps in current models' performance.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for evaluating reflective reasoning in LLMs motivates the creation of LR$^2$Bench.

Method: LR$^2$Bench includes 850 samples across six CSPs, testing diverse constraint patterns like knowledge-based, logical, and spatial constraints.

Result: Advanced LRMs like DeepSeek-R1 and OpenAI o1-preview score only 20.0% and 23.6% on LR$^2$Bench, highlighting performance gaps.

Conclusion: Current LLMs have substantial room for improvement in reflective reasoning, as shown by LR$^2$Bench evaluations.

Abstract: Recent progress in Large Reasoning Models (LRMs) has significantly enhanced
the reasoning abilities of Large Language Models (LLMs), empowering them to
tackle increasingly complex tasks through reflection capabilities, such as
making assumptions, backtracking, and self-refinement. However, effectively
evaluating such reflection capabilities remains challenging due to the lack of
appropriate benchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel
benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities
of LLMs. LR$^2$Bench comprises 850 samples across six Constraint Satisfaction
Problems (CSPs) where reflective reasoning is crucial for deriving solutions
that meet all given constraints. Each type of task focuses on distinct
constraint patterns, such as knowledge-based, logical, and spatial constraints,
providing a comprehensive evaluation of diverse problem-solving scenarios. Our
extensive evaluation on both conventional LLMs and LRMs reveals that even the
most advanced LRMs, such as DeepSeek-R1 and OpenAI o1-preview, struggle with
tasks in LR$^2$Bench, achieving an average Exact Match score of only 20.0% and
23.6%, respectively. These findings underscore the significant room for
improvement in the reflective reasoning capabilities of current LLMs.

</details>


### [53] [The Noisy Path from Source to Citation: Measuring How Scholars Engage with Past Research](https://arxiv.org/pdf/2502.20581)
*Hong Chen, Misha Teplitskiy, David Jurgens*

Main category: cs.CL

TL;DR: The paper introduces a computational pipeline to measure citation fidelity, revealing systematic differences in how accurately cited information is preserved, influenced by factors like recency, accessibility, and author characteristics.


<details>
  <summary>Details</summary>
Motivation: Citations are often used for research evaluation, but raw counts ignore variability in citation fidelity, which can distort knowledge flows.

Method: A computational pipeline analyzes full texts to identify citations and corresponding claims, using supervised models to measure fidelity at the sentence level.

Result: Citation fidelity is higher for recent, accessible, and intellectually close papers, and lower for high H-index first authors. A 'telephone effect' distorts fidelity further.

Conclusion: Citation fidelity varies systematically, highlighting limitations of relying on citation counts alone and potential evidence distortion.

Abstract: Academic citations are widely used for evaluating research and tracing
knowledge flows. Such uses typically rely on raw citation counts and neglect
variability in citation types. In particular, citations can vary in their
fidelity as original knowledge from cited studies may be paraphrased,
summarized, or reinterpreted, possibly wrongly, leading to variation in how
much information changes from cited to citing paper. In this study, we
introduce a computational pipeline to quantify citation fidelity at scale.
Using full texts of papers, the pipeline identifies citations in citing papers
and the corresponding claims in cited papers, and applies supervised models to
measure fidelity at the sentence level. Analyzing a large-scale
multi-disciplinary dataset of approximately 13 million citation sentence pairs,
we find that citation fidelity is higher when authors cite papers that are 1)
more recent and intellectually close, 2) more accessible, and 3) the first
author has a lower H-index and the author team is medium-sized. Using a
quasi-experiment, we establish the "telephone effect" - when citing papers have
low fidelity to the original claim, future papers that cite the citing paper
and the original have lower fidelity to the original. Our work reveals
systematic differences in citation fidelity, underscoring the limitations of
analyses that rely on citation quantity alone and the potential for distortion
of evidence.

</details>


### [54] [Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners](https://arxiv.org/pdf/2503.00845)
*Miao Peng, Nuo Chen, Zongrui Suo, Jia Li*

Main category: cs.CL

TL;DR: GraphPRM, a Process Reward Model for graph reasoning, improves LLM performance by 9% on 13 tasks, showing cross-domain applicability.


<details>
  <summary>Details</summary>
Motivation: Enhancing reasoning in LLMs beyond mathematical domains, leveraging automated step-level supervision for graph reasoning.

Method: Uses GraphSILO dataset (automated step-wise labels via MCTS) to train GraphPRM, evaluated via inference-time scaling and DPO.

Result: GraphPRM boosts LLM performance by 9% on graph tasks and transfers to math domains (GSM8K, Math500).

Conclusion: PRMs like GraphPRM can advance reasoning across diverse domains, making LLMs more versatile.

Abstract: Despite significant advancements in Large Language Models (LLMs), developing
advanced reasoning capabilities in LLMs remains a key challenge. Process Reward
Models (PRMs) have demonstrated exceptional promise in enhancing reasoning by
providing step-wise feedback, particularly in the context of mathematical
reasoning. However, their application to broader reasoning domains remains
understudied, largely due to the high costs associated with manually creating
step-level supervision. In this work, we explore the potential of PRMs in graph
reasoning problems - a domain that demands sophisticated multi-step reasoning
and offers opportunities for automated step-level data generation using
established graph algorithms. We introduce GraphSILO, the largest dataset for
graph reasoning problems with fine-grained step-wise labels, built using
automated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to
generate detailed reasoning steps with step-wise labels. Building upon this
dataset, we train GraphPRM, the first PRM designed for graph reasoning
problems, and evaluate its effectiveness in two key settings: inference-time
scaling and reinforcement learning via Direct Preference Optimization (DPO).
Experimental results show that GraphPRM significantly improves LLM performance
across 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and
demonstrating transferability to new graph reasoning datasets and new reasoning
domains like mathematical problem-solving. Notably, GraphPRM enhances LLM
performance on GSM8K and Math500, underscoring the cross-domain applicability
of graph-based reasoning rewards. Our findings highlight the potential of PRMs
in advancing reasoning across diverse domains, paving the way for more
versatile and effective LLMs.

</details>


### [55] [LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs](https://arxiv.org/pdf/2503.02502)
*Jianghao Chen, Junhong Wu, Yangyifan Xu, Jiajun Zhang*

Main category: cs.CL

TL;DR: Proposes LADM, a framework for selecting high-quality long-context data to improve LLM performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of measuring long-context data quality for LLM training.

Method: Uses attention-based dependency measurement (LADM) to identify high-quality data from a multi-domain corpus.

Result: LADM improves LLM performance on long-context tasks with minimal training data (1B tokens).

Conclusion: LADM effectively enhances LLM capabilities for long-context processing.

Abstract: Long-context modeling has drawn more and more attention in the area of Large
Language Models (LLMs). Continual training with long-context data becomes the
de-facto method to equip LLMs with the ability to process long inputs. However,
it still remains an open challenge to measure the quality of long-context
training data. To address this issue, we propose a Long-context data selection
framework with Attention-based Dependency Measurement (LADM), which can
efficiently identify high-quality long-context data from a large-scale,
multi-domain pre-training corpus. LADM leverages the retrieval capabilities of
the attention mechanism to capture contextual dependencies, ensuring a
comprehensive quality measurement of long-context data. Experimental results
show that our LADM framework significantly boosts the performance of LLMs on
multiple long-context tasks with only 1B tokens for continual training.

</details>


### [56] [Computation Mechanism Behind LLM Position Generalization](https://arxiv.org/pdf/2503.13305)
*Chi Han, Heng Ji*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) generalize positional information in text, revealing computational mechanisms behind their flexibility and disentanglement of attention logits.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs computationally process positional relevance and generalize to perturbed or longer texts, despite lacking explicit exploration of these mechanisms.

Method: Analyzes LLMs' self-attention mechanisms, identifying patterns in attention logits and intermediate features, and compares them to randomly initialized parameters.

Result: LLMs exhibit a 0.959 linear correlation between attention logits and an arithmetic sum of positional relevance and semantic importance, indicating learned behavior.

Conclusion: The study links position generalization in LLMs to their internal computational mechanisms, providing foundational insights into their flexibility.

Abstract: Most written natural languages are composed of sequences of words and
sentences. Similar to humans, large language models (LLMs) exhibit flexibility
in handling textual positions - a phenomenon we term position generalization.
They can understand texts with position perturbations and generalize to longer
texts than those encountered during training with the latest techniques. These
phenomena suggest that LLMs handle positions tolerantly, but how LLMs
computationally process positional relevance remains largely unexplored. This
work connects the linguistic phenomenon with LLMs' computational mechanisms. We
show how LLMs enforce certain computational mechanisms for the aforementioned
tolerance in position perturbations. Despite the complex design of the
self-attention mechanism, this work reveals that LLMs learn a counterintuitive
disentanglement of attention logits. Their values show a 0.959 linear
correlation with an approximation of the arithmetic sum of positional relevance
and semantic importance. Furthermore, we identify a prevalent pattern in
intermediate features, which we prove theoretically enables this effect. The
pattern, which is different from how randomly initialized parameters would
behave, suggests that it is a learned behavior rather than a natural result of
the model architecture. Based on these findings, we provide computational
explanations and criteria for LLMs' position flexibilities. This work takes a
pioneering step in linking position generalization with modern LLMs' internal
mechanisms.

</details>


### [57] [Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation](https://arxiv.org/pdf/2503.16789)
*Rupak Sarkar, Bahareh Sarrafzadeh, Nirupama Chandrasekaran, Nagu Rangan, Philip Resnik, Longqi Yang, Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: The paper studies how rewriting suboptimal user prompts with LLMs can improve chatbot responses, showing effectiveness across domains and LLM types.


<details>
  <summary>Details</summary>
Motivation: Users often struggle to craft effective prompts for LLM chatbots, leading to unhelpful responses. Real-world conversational datasets and LLMs' text understanding capabilities offer a way to address this issue.

Method: The study analyzes real human-AI chatbot conversations, identifies shortcomings in user queries, and explores LLM-based prompt rewriting to improve responses.

Result: Rewriting prompts enhances response quality while preserving user intent, especially in longer conversations where context aids inference. LLMs make plausible assumptions about user goals.

Conclusion: Prompt rewriting is a promising solution for better human-AI interactions, applicable across various domains and LLM models.

Abstract: Human-LLM conversations are increasingly becoming more pervasive in peoples'
professional and personal lives, yet many users still struggle to elicit
helpful responses from LLM Chatbots. One of the reasons for this issue is
users' lack of understanding in crafting effective prompts that accurately
convey their information needs. Meanwhile, the existence of real-world
conversational datasets on the one hand, and the text understanding faculties
of LLMs on the other, present a unique opportunity to study this problem, and
its potential solutions at scale. Thus, in this paper we present the first
LLM-centric study of real human-AI chatbot conversations, focused on
investigating aspects in which user queries fall short of expressing
information needs, and the potential of using LLMs to rewrite suboptimal user
prompts. Our findings demonstrate that rephrasing ineffective prompts can
elicit better responses from a conversational system, while preserving the
user's original intent. Notably, the performance of rewrites improves in longer
conversations, where contextual inferences about user needs can be made more
accurately. Additionally, we observe that LLMs often need to -- and inherently
do -- make \emph{plausible} assumptions about a user's intentions and goals
when interpreting prompts. Our findings largely hold true across conversational
domains, user intents, and LLMs of varying sizes and families, indicating the
promise of using prompt rewriting as a solution for better human-AI
interactions.

</details>


### [58] [LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](https://arxiv.org/pdf/2503.21227)
*Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, Qingpei Guo, Si Liu*

Main category: cs.CL

TL;DR: LLaVA-CMoE is a continual learning framework for large language models (LLMs) that avoids replay data, ensures parameter efficiency, and retains knowledge via Probe-Guided Knowledge Extension and Probabilistic Task Locator.


<details>
  <summary>Details</summary>
Motivation: Efficiently extending Mixture of Experts (MoE) architectures for sequential tasks without catastrophic forgetting or excessive parameter growth.

Method: Introduces Probe-Guided Knowledge Extension for adaptive parameter expansion and Probabilistic Task Locator with VAE-based reconstruction for expert allocation.

Result: Achieves strong continual learning performance on the CoIN benchmark with reduced forgetting and parameter overhead.

Conclusion: LLaVA-CMoE is effective and scalable for parameter-efficient continual learning in LLMs.

Abstract: Mixture of Experts (MoE) architectures have recently advanced the scalability
and adaptability of large language models (LLMs) for continual multimodal
learning. However, efficiently extending these models to accommodate sequential
tasks remains challenging. As new tasks arrive, naive model expansion leads to
rapid parameter growth, while modifying shared routing components often causes
catastrophic forgetting, undermining previously learned knowledge. To address
these issues, we propose LLaVA-CMoE, a continual learning framework for LLMs
that requires no replay data of previous tasks and ensures both parameter
efficiency and robust knowledge retention. Our approach introduces a
Probe-Guided Knowledge Extension mechanism, which uses probe experts to
dynamically determine when and where new experts should be added, enabling
adaptive and minimal parameter expansion tailored to task complexity.
Furthermore, we present a Probabilistic Task Locator that assigns each task a
dedicated, lightweight router. To handle the practical issue that task labels
are unknown during inference, we leverage a VAE-based reconstruction strategy
to identify the most suitable router by matching input distributions, allowing
automatic and accurate expert allocation. This design mitigates routing
conflicts and catastrophic forgetting, enabling robust continual learning
without explicit task labels. Extensive experiments on the CoIN benchmark,
covering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong
continual learning performance with a compact model size, significantly
reducing forgetting and parameter overhead compared to prior methods. These
results showcase the effectiveness and scalability of our approach for
parameter-efficient continual learning in large language models. Our code will
be open-sourced soon.

</details>


### [59] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/pdf/2505.20767)
*Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie*

Main category: cs.CL

TL;DR: The paper addresses the challenge of evaluating and detecting hallucinations in cognitive statements generated by LLMs, introducing a rigorous framework and datasets (CogniBench and CogniBench-L) for assessment and detection.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook cognitive statements, focusing only on factual ones, making it hard to assess hallucinations in inferences.

Method: Inspired by legal evidence assessment, the authors design a framework to evaluate faithfulness levels and create datasets (CogniBench and CogniBench-L) with an automatic annotation pipeline.

Result: The framework and datasets enable accurate detection of both factual and cognitive hallucinations, with insights from CogniBench statistics.

Conclusion: The work provides scalable tools (datasets and models) for detecting hallucinations in LLMs, addressing a critical gap in current benchmarks.

Abstract: Faithfulness hallucinations are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standards, existing benchmarks focus on "factual statements" that rephrase
source materials while overlooking "cognitive statements" that involve making
inferences from the given context. Consequently, evaluating and detecting the
hallucination of cognitive statements remains challenging. Inspired by how
evidence is assessed in the legal domain, we design a rigorous framework to
assess different levels of faithfulness of cognitive statements and introduce
the CogniBench dataset where we reveal insightful statistics. To keep pace with
rapidly evolving LLMs, we further develop an automatic annotation pipeline that
scales easily across different models. This results in a large-scale
CogniBench-L dataset, which facilitates training accurate detectors for both
factual and cognitive hallucinations. We release our model and datasets at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [60] [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/pdf/2506.04689)
*Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li*

Main category: cs.CL

TL;DR: REWIRE recycles low-quality web data by enriching it, improving model performance when mixed with high-quality data.


<details>
  <summary>Details</summary>
Motivation: Address the 'data wall' in pre-training by recycling discarded low-quality web data.

Method: Proposes REWIRE, a method to transform and enrich low-quality documents for training.

Result: Mixing rewritten texts with high-quality data improves performance by 1.0-2.5 percentage points across tasks.

Conclusion: Recycling web texts is a simple and effective way to scale pre-training data.

Abstract: Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.

</details>


### [61] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/pdf/2506.06406)
*Guoyang Xia, Yifeng Ding, Fengfa Li, Lei Ren, Wei Chen, Fangxiang Feng, Xiaojie Wang*

Main category: cs.CL

TL;DR: SMAR, a novel regularization technique, balances modality differentiation and language capabilities in multimodal MoE models, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing high training costs and degraded language capabilities in existing multimodal MoE models.

Method: Uses Kullback-Leibler divergence for regularization to control routing probabilities across modalities without altering architecture.

Result: Achieves 86.6% language ability retention with minimal text data (2.5%), outperforming baselines.

Conclusion: SMAR offers an efficient solution for multimodal MoE models, balancing modality specialization and language preservation.

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [62] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/pdf/2506.19028)
*Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy*

Main category: cs.CL

TL;DR: FiSCo is a statistical framework for evaluating group-level fairness in LLMs by analyzing semantic differences in long-form responses, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs overlook biases in long-form responses and intrinsic variability, limiting their reliability.

Method: FiSCo decomposes LLM outputs into claims, uses entailment checks for semantic consistency, and applies statistical hypothesis testing to detect biases.

Result: FiSCo reliably identifies nuanced biases across demographic groups and reduces the impact of LLM variability, outperforming other metrics.

Conclusion: FiSCo provides a robust framework for detecting subtle biases in LLMs, enhancing fairness evaluation in real-world applications.

Abstract: Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

</details>


### [63] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/pdf/2506.19262)
*Yuchang Zhu, Huazhen Zhong, Qunshu Lin, Haotong Wei, Xiaolong Sun, Zixuan Yu, Minghao Liu, Zibin Zheng, Liang Chen*

Main category: cs.CL

TL;DR: The paper investigates how the diversity of LLM-generated data impacts downstream model performance, finding that moderately diverse data improves performance, while highly diverse data harms it.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked importance of data diversity in LLM-generated data and its effect on downstream model performance, especially in data-scarce domains.

Method: Explores varying levels of diversity in LLM-generated data and its impact, including mixed proportions of synthetic and real data.

Result: Moderately diverse LLM-generated data enhances performance with minimal distribution shift, but highly diverse data negatively affects it.

Conclusion: The findings provide guidance for using LLMs as data generators, emphasizing the importance of balanced diversity.

Abstract: With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

</details>


### [64] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/pdf/2506.19750)
*Takashi Nishibayashi, Seiji Kanazawa, Kumpei Yamada*

Main category: cs.CL

TL;DR: The study proposes a Synthetic Vignette Simulation Approach to evaluate diagnostic performance changes in Symptom Checkers (SCs) for rare diseases using HPO data, showing effectiveness for diseases with frequency information.


<details>
  <summary>Details</summary>
Motivation: Evaluating SC algorithm updates for rare diseases is challenging due to data scarcity and high costs of manual vignette creation.

Method: Uses HPO disease-phenotype annotations to generate synthetic vignettes, simulating SC interviews to estimate performance changes post-update.

Result: High $R^2$ values (0.831 for Recall@8, 0.78 for Precision@8) for diseases with frequency data, but poor performance without it.

Conclusion: The method enables cost-effective, pre-deployment evaluation of SC updates for rare diseases, aiding early diagnosis.

Abstract: Symptom Checkers (SCs) provide users with personalized medical information.
To prevent performance degradation from algorithm updates, SC developers must
evaluate diagnostic performance changes for individual diseases before
deployment. However, acquiring sufficient evaluation data for rare diseases is
difficult, and manually creating numerous clinical vignettes is costly and
impractical. This study proposes and validates a novel Synthetic Vignette
Simulation Approach to evaluate diagnostic performance changes for individual
rare diseases following SC algorithm updates. We used disease-phenotype
annotations from the Human Phenotype Ontology (HPO), a knowledge database for
rare diseases, to generate synthetic vignettes. With these, we simulated SC
interviews to estimate the impact of algorithm updates on real-world diagnostic
performance. The method's effectiveness was evaluated retrospectively by
comparing estimated values with actual metric changes using the $R^2$
coefficient. The experiment included eight past SC algorithm updates. For
updates on diseases with frequency information in HPO (n=5), the $R^2$ for
Recall@8 change was 0.831 ($p$=0.031), and for Precision@8 change, it was 0.78
($p$=0.047), indicating the method can predict post-deployment performance. In
contrast, large prediction errors occurred for diseases without frequency
information (n=3), highlighting its importance. Our method enables
pre-deployment evaluation of SC algorithm changes for individual rare diseases
using a publicly available, expert-created knowledge base. This transparent and
low-cost approach allows developers to efficiently improve diagnostic
performance for rare diseases, potentially enhancing support for early
diagnosis.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/pdf/2506.19939)
*Aryan Singh Dalal, Sidharth Rai, Rahul Singh, Treman Singh Kaloya, Rahul Harsha Cheppally, Ajay Sharda*

Main category: cs.CV

TL;DR: A computer vision system using YOLO models was developed to quantify spray boom movement in agricultural sprayers, achieving high accuracy and potential for design improvements.


<details>
  <summary>Details</summary>
Motivation: Spray boom instability causes application errors in agricultural sprayers, but there's no quantitative data on boom movement to guide solutions.

Method: A computer vision system tracked a target on the boom edge using YOLO V7, V8, and V11 models, validated by an inclinometer sensor.

Result: The model detected targets with >90% accuracy and estimated distances within 0.026 m of sensor data.

Conclusion: The system effectively quantifies boom movement, aiding future design improvements for stability and application accuracy.

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [66] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/pdf/2506.19955)
*Yiming Ma, Victor Sanchez, Tanaya Guha*

Main category: cs.CV

TL;DR: EBC-ZIP introduces a Zero-Inflated Poisson regression for crowd counting, addressing imbalance in sparse density maps and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting methods ignore extreme sparsity in density maps and use loss functions ill-suited for count data, leading to biased estimations.

Method: Proposes EBC-ZIP, using ZIP regression for better handling of zero-heavy distributions and replacing traditional MSE loss with negative log-likelihood of ZIP.

Result: EBC-ZIP outperforms EBC and achieves state-of-the-art results on four benchmarks.

Conclusion: EBC-ZIP provides a principled probabilistic approach for crowd counting, improving accuracy and scalability.

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [67] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/pdf/2506.20066)
*Hsiang-Wei Huang, Wenhao Chai, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA is a token merging method for ViTs that combines semantic and spatial awareness, outperforming existing methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing token merging methods for ViTs rely on feature similarity, ignoring spatial information, which is crucial in early layers.

Method: ToSA integrates depth images to generate pseudo spatial tokens, guiding merging with both semantic and spatial cues.

Result: ToSA reduces ViT runtime and improves performance on visual and embodied question answering benchmarks.

Conclusion: ToSA is an efficient solution for ViT acceleration, preserving scene structure better than prior methods.

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [68] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/pdf/2506.20103)
*Jiahao Lin, Weixuan Peng, Bojia Zi, Yifeng Gao, Xianbiao Qi, Xingjun Ma, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces BrokenVideos, a benchmark dataset for detecting and localizing artifacts in AI-generated videos, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated videos suffer from visual artifacts, but existing datasets lack fine-grained spatial annotations for artifact localization.

Method: A dataset of 3,254 AI-generated videos with pixel-level artifact annotations is created and validated through human inspection.

Result: Training models on BrokenVideos enhances their ability to localize corrupted regions.

Conclusion: BrokenVideos provides a foundation for advancing artifact localization research in generative video models.

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [69] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2506.20214)
*Yanzhe Chen, Huasong Zhong, Yan Li, Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode$^2$ introduces a cascaded codebook framework for stable and semantically aligned visual tokenization, improving multimodal understanding and generation.


<details>
  <summary>Details</summary>
Motivation: Existing codebook-based methods either lack fine-grained semantics or suffer from instability due to naive scaling. UniCode$^2$ addresses these issues.

Method: UniCode$^2$ clusters SigLIP sequence embeddings into a 500K-entry codebook, using a cascaded design (frozen and trainable codebooks) for stability and semantic alignment.

Result: The framework achieves high token utilization, robust learning, and seamless integration with pretrained diffusion decoders, enabling high-quality visual synthesis.

Conclusion: UniCode$^2$ demonstrates scalable visual tokenization without compromising stability, semantics, or modularity, performing well across benchmarks.

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [70] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/pdf/2506.20134)
*Ningwei Xie, Zizi Tian, Lei Yang, Xiao-Ping Zhang, Meng Guo, Jie Li*

Main category: cs.CV

TL;DR: A survey on 3D-aware generative world models, highlighting their shift from 2D to 3D cognition, key technological drivers, core capabilities, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic analysis for emerging 3D cognitive world models motivates this survey to categorize techniques and clarify their roles in advancing AGI.

Method: Introduces a conceptual framework to review world models, focusing on 3D representations and world knowledge as key drivers, and dissects three core cognitive capabilities.

Result: Identifies applications in embodied AI, autonomous driving, digital twin, and gaming/VR, and outlines challenges in data, modeling, and deployment.

Conclusion: Future directions aim to advance robust and generalizable 3D world models, addressing current limitations.

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [71] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/pdf/2506.20370)
*Abdullah All Tanvir, Xin Zhong*

Main category: cs.CV

TL;DR: A deep learning framework for robust image zero-watermarking using distortion-invariant feature learning, leaving the original image unaltered and achieving state-of-the-art robustness.


<details>
  <summary>Details</summary>
Motivation: To develop a zero-watermarking method that preserves the original image while ensuring robustness against distortions.

Method: Combines noise-adversarial learning for invariant feature extraction and a learning-based multibit zero-watermarking scheme for message matching.

Result: Achieves superior robustness in feature stability and watermark recovery, outperforming existing methods.

Conclusion: The framework excels in generalization and robustness, making it a leading solution for zero-watermarking.

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [72] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/pdf/2506.20151)
*Haipeng Fan, Shiyuan Zhang, Baohunesitu, Zihang Guo, Huaiwen Zhang*

Main category: cs.CV

TL;DR: The paper proposes EAR, a fine-tuning method for concept erasure in AR models, introducing WGA and TLM strategies, and a new benchmark ECGVF for evaluation.


<details>
  <summary>Details</summary>
Motivation: AR models struggle with removing undesired concepts without degrading generation quality. EAR addresses this challenge.

Method: EAR uses Windowed Gradient Accumulation (WGA) and Thresholded Loss Masking (TLM) for fine-tuning. ECGVF benchmark is introduced for evaluation.

Result: EAR improves erasure effectiveness and utility preservation, demonstrated on the Janus-Pro AR model.

Conclusion: EAR provides a robust solution for concept erasure in AR models, validated by the ECGVF benchmark.

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [73] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/pdf/2506.20152)
*Deepak Ghimire, Kilho Lee, Seong-heum Kim*

Main category: cs.CV

TL;DR: LAASP introduces an efficient, loss-aware structured pruning method for neural networks, combining pruning and training into one cycle, outperforming state-of-the-art methods with significant FLOPs reduction and minimal accuracy drop.


<details>
  <summary>Details</summary>
Motivation: To streamline and improve structured pruning for neural networks, making them more efficient for deployment on resource-limited edge devices.

Method: Adopts a pruning-while-training approach, automatically selecting pruning criteria and layer-specific rates based on network loss, followed by brief retraining to mitigate accuracy drops.

Result: Achieves 52% FLOPs reduction in ResNet56/110 on CIFAR-10 with improved accuracy, and 42% FLOPs reduction in ResNet50 on ImageNet with only 0.33% top-5 accuracy drop.

Conclusion: LAASP is an effective, automated pruning method that enhances efficiency and accuracy, suitable for edge deployment.

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [74] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/pdf/2506.20548)
*Manyi Li, Renshuai Tao, Yufan Liu, Chuangchuang Tan, Haotong Qin, Bing Li, Yunchao Wei, Yao Zhao*

Main category: cs.CV

TL;DR: PLADA is a novel framework for deepfake detection on OSNs, addressing block effects and data scarcity with dual-stage attention and open data aggregation.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods ignore block effects from OSN compression and rely on raw images, limiting real-world applicability.

Method: PLADA includes Block Effect Eraser (B2E) for handling compression artifacts and Open Data Aggregation (ODA) for leveraging paired and unpaired data.

Result: PLADA outperforms state-of-the-art methods across 26 datasets, even with limited paired data and compression.

Conclusion: PLADA introduces block effects as a key factor in deepfake detection, offering a robust solution for open-world scenarios.

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [75] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/pdf/2506.20155)
*Avadhoot Jadhav, Ashutosh Srivastava, Abhinav Java, Silky Singh, Tarun Ram Menta, Surgan Jandial, Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: The paper introduces an exemplar-based image editing method using pretrained diffusion models and VLMs, outperforming baselines in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Text alone is insufficient for ambiguous image edits; exemplar pairs provide clearer guidance.

Method: Leverages pretrained text-to-image diffusion models and multimodal VLMs for optimization-free editing.

Result: Outperforms baselines on various edits and is ~4x faster.

Conclusion: Exemplar-based editing with diffusion models is efficient and effective.

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [76] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/pdf/2506.20168)
*Zhentao He, Can Zhang, Ziheng Wu, Zhenghao Chen, Yufei Zhan, Yifan Li, Zhao Zhang, Xian Wang, Minghui Qiu*

Main category: cs.CV

TL;DR: The paper introduces KIE-HVQA, a benchmark for evaluating OCR hallucination in degraded documents, and proposes a GRPO-based framework to mitigate hallucinations by incorporating visual uncertainty awareness.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with visual degradation, leading to unreliable outputs and hallucinations. The paper aims to address this by improving vision-faithful reasoning.

Method: The authors propose KIE-HVQA, a dataset with degraded documents, and a GRPO-based framework with a novel reward mechanism for self-awareness of visual uncertainty.

Result: The 7B-parameter model achieves a 22% improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA, with no drop in standard task performance.

Conclusion: The framework effectively mitigates hallucinations in degraded visual inputs while maintaining robustness in standard tasks.

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [77] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/pdf/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: Foundation models pretrained on remote sensing and general vision datasets can be combined to improve Earth Observation tasks, matching or outperforming larger models with less computational cost.


<details>
  <summary>Details</summary>
Motivation: To explore the reuse and combination of existing pretrained models for Earth Observation tasks, as an alternative to training large models from scratch.

Method: Evaluate feature-level ensembling of models like Prithvi, Hiera, and DOFA on the GEO-Bench benchmark across diverse datasets.

Result: Feature-level ensembling of smaller pretrained models matches or exceeds larger models' performance with reduced training time and resources.

Conclusion: Combining pretrained models and knowledge distillation offers a practical approach for deploying foundation models in real-world Earth Observation applications.

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [78] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/pdf/2506.20179)
*Enzhe Zhao, Zhichang Guo, Yao Li, Fanghui Song, Boying Wu*

Main category: cs.CV

TL;DR: The paper critiques the Wald protocol's limitations in pansharpening and introduces PADM and HFreqdiff for improved HRMS image generation.


<details>
  <summary>Details</summary>
Motivation: The Wald protocol's inaccurate degradation patterns limit deep pansharpening model generalization.

Method: Proposes PADM (Progressive Alignment Degradation Module) and HFreqdiff, integrating high-frequency details via diffusion and specialized modules.

Result: Superior performance in spatial sharpness and quality, outperforming state-of-the-art methods.

Conclusion: The proposed innovations address Wald protocol limitations, enhancing pansharpening effectiveness.

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [79] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/pdf/2506.20222)
*Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu*

Main category: cs.CV

TL;DR: A joint event and image (E-I) transmission framework is proposed to optimize bandwidth by eliminating redundancy between event and RGB cameras, using Bayesian modeling and information bottleneck for efficient reconstruction and deblurring.


<details>
  <summary>Details</summary>
Motivation: Hybrid systems combining event and RGB cameras face challenges in transmitting large volumes of data, with redundant information between the two sources.

Method: The framework employs Bayesian modeling and the information bottleneck method to disentangle shared and domain-specific information, adaptively allocating bandwidth based on scene dynamics.

Result: Simulations show superior reconstruction quality and enhanced deblurring performance compared to conventional systems.

Conclusion: The proposed scheme effectively optimizes bandwidth while improving reconstruction and deblurring, making it suitable for real-time hybrid vision systems.

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [80] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/pdf/2506.20293)
*Kunjing Yang, Libin Zheng, Minru Bai, Ting Lu, Leyuan Fang*

Main category: cs.CV

TL;DR: A method for blind fusion of unregistered hyperspectral (HSI) and multispectral (MSI) images using spectral domain registration and sparse fusion, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of spatial resolution differences and time-consuming registration in existing methods for HSI and MSI fusion.

Method: Proposes Spectral Prior Learning (SPL) for spectral feature extraction and resolution enhancement, followed by blind sparse fusion (BSF) with group sparsity regularization, solved via Proximal Alternating Optimization (PAO).

Result: Effective registration and fusion demonstrated on simulated and real datasets, with improved classification performance.

Conclusion: The method efficiently addresses registration and fusion challenges, offering computational simplicity and enhanced spectral accuracy.

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [81] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/pdf/2506.20254)
*Kun Yuan, Tingxuan Chen, Shi Li, Joel L. Lavanchy, Christian Heiliger, Ege Özsoy, Yiming Huang, Long Bai, Nassir Navab, Vinkle Srivastav, Hongliang Ren, Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA is a lightweight framework for adapting surgical foundation models to institutional settings with minimal annotation, achieving state-of-the-art performance in few-shot surgical phase recognition.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing generalizable models for surgical workflow understanding due to heterogeneous operating room settings and domain shifts.

Method: SPA uses few-shot spatial adaptation, diffusion modeling for temporal consistency, and dynamic test-time adaptation to align multi-modal embeddings with institution-specific scenes and phases.

Result: SPA outperforms full-shot models with 32-shot labeled data, achieving state-of-the-art performance across multiple institutions and procedures.

Conclusion: SPA provides a versatile and efficient solution for surgical workflow understanding, enabling hospitals to customize models with minimal effort.

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [82] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/pdf/2506.20255)
*Ayush Lodh, Ritabrata Chakraborty, Shivakumara Palaiahnakote, Umapada Pal*

Main category: cs.CV

TL;DR: An end-to-end network fuses offline images and online stroke data for handwriting recognition, achieving state-of-the-art accuracy with 1% improvement.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition systems typically use only one modality (glyph or trajectory), missing complementary cues. This work aims to leverage both for better performance.

Method: Early fusion of offline images and online stroke data in a shared latent space using a patch encoder and lightweight transformer. Learnable queries attend to both modalities.

Result: Achieves state-of-the-art accuracy on IAMOn-DB and VNOn-DB, outperforming previous methods by up to 1%. Also adapts well to gesturification on ISI-Air.

Conclusion: Combining offline and online modalities in early fusion improves handwriting recognition accuracy and writer independence.

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [83] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/pdf/2506.20263)
*Ning Luo, Meiyin Hu, Huan Wan, Yanyan Yang, Zhuohang Jiang, Xin Wei*

Main category: cs.CV

TL;DR: HMDRN improves few-shot fine-grained image classification by integrating dual-layer feature reconstruction and mask-enhanced processing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lose spatial information or fail to utilize hierarchical features, limiting performance in FS-FGIC.

Method: HMDRN combines dual-layer feature reconstruction with mask-enhanced processing, balancing high-level semantics and mid-level details.

Result: HMDRN outperforms state-of-the-art methods on three datasets and shows improved feature reconstruction.

Conclusion: HMDRN enhances inter-class discrimination and reduces intra-class variations, validated by ablation studies and visualizations.

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [84] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/pdf/2506.20272)
*Juan José Murillo-Fuentes, Pablo M. Olmos, Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: A deep learning-based method for comparing canvas fabrics in art, bypassing traditional thread density maps, is introduced and validated on Prado Museum canvases.


<details>
  <summary>Details</summary>
Motivation: Traditional thread density map matching fails for non-contiguous canvases, necessitating a new approach for authentication and conservation.

Method: A Siamese deep learning model compares canvas images, using feature representations and aggregated similarity scores from multiple pairs.

Result: The method effectively compares plain weave canvases with similar thread densities, proving feasible and accurate.

Conclusion: This approach opens new possibilities for analyzing masterpieces, enhancing authentication and conservation efforts.

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [85] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/pdf/2506.20279)
*Changliang Xia, Chengyou Jia, Zhuohang Dang, Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld, a benchmark for 25 dense prediction tasks, and proposes DenseDiT, a method leveraging generative models for real-world applications with minimal additional parameters.


<details>
  <summary>Details</summary>
Motivation: Existing dense prediction methods lack generalization to real-world scenarios and face data scarcity. The study aims to address these limitations.

Method: DenseDiT uses generative models' visual priors, a parameter-reuse mechanism, and lightweight branches for multi-scale context integration.

Result: DenseDiT outperforms baselines with less than 0.01% training data, showing superior real-world generalization.

Conclusion: DenseDiT offers practical value for real-world deployment, with data, checkpoints, and code made available.

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [86] [Learning Adaptive Lighting via Channel-Aware Guidance](https://arxiv.org/pdf/2412.01493)
*Qirui Yang, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Huanjing Yue, Jingyu Yang*

Main category: cs.CV

TL;DR: LALNet is a multi-task framework for lighting adaptation, leveraging color-separated and mixed features to outperform state-of-the-art methods efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing shared fundamental properties in light-related tasks like high dynamic range imaging and exposure correction, which are often tackled in isolation.

Method: Uses channel-aware features (color-separated and mixed) guided by Light Guided Attention (LGA), with dual domain channel modulation and mixed channel modulation.

Result: Significantly outperforms state-of-the-art methods on four light-related tasks with fewer computational resources.

Conclusion: LALNet effectively handles multiple light-related tasks by leveraging channel differences and visual consistency, demonstrating superior performance and efficiency.

Abstract: Learning lighting adaptation is a crucial step in achieving good visual
perception and supporting downstream vision tasks. Current research often
addresses individual light-related challenges, such as high dynamic range
imaging and exposure correction, in isolation. However, we identify shared
fundamental properties across these tasks: i) different color channels have
different light properties, and ii) the channel differences reflected in the
spatial and frequency domains are different. Leveraging these insights, we
introduce the channel-aware Learning Adaptive Lighting Network (LALNet), a
multi-task framework designed to handle multiple light-related tasks
efficiently. Specifically, LALNet incorporates color-separated features that
highlight the unique light properties of each color channel, integrated with
traditional color-mixed features by Light Guided Attention (LGA). The LGA
utilizes color-separated features to guide color-mixed features focusing on
channel differences and ensuring visual consistency across all channels.
Additionally, LALNet employs dual domain channel modulation for generating
color-separated features and a mixed channel modulation and light state space
module for producing color-mixed features. Extensive experiments on four
representative light-related tasks demonstrate that LALNet significantly
outperforms state-of-the-art methods on benchmark tests and requires fewer
computational resources. We provide an anonymous online demo at
https://xxxxxx2025.github.io/LALNet/.

</details>


### [87] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/pdf/2506.20294)
*Shunqi Mao, Wei Guo, Chaoyi Zhang, Weidong Cai*

Main category: cs.CV

TL;DR: Ctrl-Z Sampling is a novel method to improve conditional generation in diffusion models by dynamically escaping local optima through controlled noise injection and backtracking.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often converge to local optima, leading to globally inconsistent or misaligned outputs. Existing solutions focus on guidance signals or noise manipulation, but Ctrl-Z Sampling aims to escape these optima directly.

Method: Ctrl-Z Sampling identifies local maxima using a reward model, injects noise to revert to a noisier state, and evaluates trajectories for improvement. It alternates between refinement and exploration.

Result: The method improves generation quality with a 7.6X increase in function evaluations, enhancing alignment and visual quality.

Conclusion: Ctrl-Z Sampling is a model-agnostic, effective strategy for escaping local optima in diffusion models, compatible with existing frameworks.

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [88] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/pdf/2506.20302)
*Abbas Anwar, Mohammad Shullar, Ali Arshad Nasir, Mudassir Masood, Saeed Anwar*

Main category: cs.CV

TL;DR: A transformer-based diffusion model outperforms existing methods in restoring degraded images, enhancing their quality for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Degraded images (e.g., from noise, color cast, blur) hinder performance in tasks like object detection and classification, necessitating effective restoration methods.

Method: Developed a transformer-based diffusion model for image restoration, tested on underwater image enhancement, denoising, and deraining using public datasets.

Result: The model surpasses existing deep learning methods in performance across multiple quality metrics.

Conclusion: Diffusion models with transformers effectively improve degraded image quality, enhancing their utility in high-fidelity visual tasks.

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [89] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/pdf/2506.20306)
*Yaxi Chen, Simin Ni, Shaheer U. Saeed, Aleksandra Ivanova, Rikin Hargunani, Jie Huang, Chaozong Liu, Yipeng Hu*

Main category: cs.CV

TL;DR: The paper proposes a dynamic radiomic fingerprint framework for knee MRI scans, improving accuracy and interpretability over traditional radiomic signatures and end-to-end deep learning models.


<details>
  <summary>Details</summary>
Motivation: Current radiomic approaches use fixed features, limiting performance and generalization. The authors aim to address this by dynamically selecting patient-specific radiomic features.

Method: A deep learning model dynamically selects radiomic features (fingerprints) for each patient, combined with a low-dimensional logistic regression for classification.

Result: The method achieves comparable or superior accuracy to state-of-the-art DL models and provides interpretable clinical insights.

Conclusion: The radiomic fingerprint framework enhances diagnostic accuracy and interpretability, offering potential for biomarker discovery.

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [90] [On the Burstiness of Faces in Set](https://arxiv.org/pdf/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: The paper addresses burstiness in set-based face recognition (SFR), where certain faces dominate datasets, harming generalization and evaluation. It proposes detection methods (Quickshift++, feature self-similarity, GMP) and mitigation strategies, including quality-aware GMP, to improve recognition performance.


<details>
  <summary>Details</summary>
Motivation: Burstiness in SFR causes poor generalization and biased evaluation due to overrepresented faces. The study aims to detect and mitigate this issue.

Method: Three burst detection strategies (Quickshift++, feature self-similarity, GMP) are proposed. Mitigation involves adjusting sampling ratios and contributions of infrequent faces, plus quality-aware GMP for evaluation.

Result: Experiments show burstiness is widespread in SFR, and suppressing it significantly enhances recognition performance.

Conclusion: Detecting and mitigating burstiness in SFR improves generalization and evaluation, with proposed methods proving effective.

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [91] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/pdf/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: The paper benchmarks five object detection models on historical document datasets, finding that Transformer-based models excel on structured layouts, while CNN-OBB models perform better on complex documents.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of state-of-the-art object detection architectures for robust Document Layout Analysis (DLA) in historical documents with complex layouts.

Method: Benchmarked five models (Co-DETR, Grounding DINO, YOLO variants) on three datasets (e-NDP, CATMuS, HORAE) using different bounding box representations (AABB, OBB).

Result: Co-DETR performed best on structured datasets (e-NDP), while YOLOv11x-OBB outperformed others on complex datasets (CATMuS, HORAE). Oriented Bounding Boxes (OBB) proved essential for accuracy.

Conclusion: Transformers are ideal for structured layouts, while CNN-OBB models generalize better for complex documents, highlighting a trade-off between context awareness and generalization.

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [92] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/pdf/2506.20342)
*Lei Wang, Piotr Koniusz*

Main category: cs.CV

TL;DR: A deep translational action recognition framework enhances accuracy by predicting action concepts and auxiliary features, integrating novel descriptors and multimodal cues while handling uncertainty.


<details>
  <summary>Details</summary>
Motivation: Human action recognition requires high-level semantic reasoning and multimodal feature integration, which existing methods often lack.

Method: The framework jointly predicts action concepts and auxiliary features, uses hallucination streams for missing cues, and introduces Object Detection Features (ODF) and Saliency Detection Features (SDF). It integrates multimodal inputs and employs aleatoric uncertainty modeling.

Result: Achieves state-of-the-art performance on benchmarks like Kinetics-400, Kinetics-600, and Something-Something V2.

Conclusion: The framework effectively captures fine-grained action dynamics by combining novel descriptors, multimodal integration, and robust uncertainty handling.

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [93] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/pdf/2506.20381)
*Ben Kang, Xin Chen, Jie Zhao, Chunjuan Bo, Dong Wang, Huchuan Lu*

Main category: cs.CV

TL;DR: HiT and DyHiT are efficient transformer-based trackers addressing speed limitations on resource-constrained devices, achieving high performance and fast operation.


<details>
  <summary>Details</summary>
Motivation: Transformer-based trackers are powerful but slow on resource-constrained devices, prompting the need for efficient solutions.

Method: HiT uses a Bridge Module and dual-image position encoding; DyHiT employs dynamic routing for adaptive computation.

Result: HiT achieves 61 fps (64.6% AUC), DyHiT reaches 111 fps (62.4% AUC), and a training-free acceleration method speeds up SeqTrack-B256 by 2.68x.

Conclusion: The proposed methods offer efficient, high-performance tracking suitable for various devices.

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [94] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/pdf/2506.20388)
*Shen Tan, Xin Zhang, Liangxiu Han, Huaguo Huang, Han Wang*

Main category: cs.CV

TL;DR: A novel model using a Large Vision Foundation Model (LVFM) for high-resolution canopy height maps (CHMs) was developed, outperforming existing methods in accuracy and cost-effectiveness for plantation biomass monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB) is vital for carbon sequestration initiatives like China's CCER program, but traditional lidar-based methods are expensive and deep learning with RGB imagery struggles with canopy height feature extraction.

Method: The model integrates a feature extractor, a self-supervised feature enhancement module, and a height estimator, tested using 1-meter Google Earth imagery in Beijing's Fangshan District.

Result: The model achieved a mean absolute error of 0.09 m, RMSE of 0.24 m, and correlation of 0.78 against lidar-based CHMs, with over 90% success in tree detection and high AGB estimation accuracy.

Conclusion: This LVFM-based approach is a scalable, promising tool for carbon sequestration evaluation in plantations and natural forests.

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [95] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/pdf/2506.20449)
*Changlu Guo, Anders Nymark Christensen, Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art is a framework for medical image generation with limited data, leveraging vision-language models and fine-tuning methods to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in medical image generation, such as small datasets and scarce textual data.

Method: Uses vision-language models for visual descriptions, adapts PixArt-α (DiT-based), and introduces Hybrid-Level Diffusion Fine-tuning (HLDF).

Result: Achieves state-of-the-art performance on medical datasets (FID, KID, classification).

Conclusion: Med-Art effectively overcomes data limitations and improves medical image generation.

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [96] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/pdf/2506.20452)
*Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave is a training-free, zero-shot method for ultra-high-resolution image synthesis using pretrained diffusion models, enhancing visual fidelity and coherence without retraining.


<details>
  <summary>Details</summary>
Motivation: Training diffusion models at high resolutions is computationally expensive, and existing zero-shot techniques often produce artifacts like object duplication and spatial incoherence.

Method: HiWave uses a two-stage pipeline: generating a base image, followed by patch-wise DDIM inversion and a wavelet-based detail enhancer to preserve global coherence and enrich fine details.

Result: HiWave outperforms prior methods, reducing artifacts and achieving superior perceptual quality, as confirmed by a user study where it was preferred in over 80% of comparisons.

Conclusion: HiWave is effective for high-quality, ultra-high-resolution image synthesis without retraining or architectural changes.

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [97] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/pdf/2506.20464)
*Dibyayan Patra, Pasindu Ranasinghe, Bikram Banerjee, Simit Raval*

Main category: cs.CV

TL;DR: The paper proposes DeepBolt, a two-stage deep learning method for automated rock bolt detection in 3D point clouds, outperforming existing techniques in precision and recall.


<details>
  <summary>Details</summary>
Motivation: Manual rock bolt inspection in mines is challenging due to low light and time constraints, necessitating automated solutions. Existing methods lack robustness against noise and complex environments.

Method: DeepBolt, a novel two-stage deep learning architecture, addresses class imbalance and efficiently identifies rock bolts in noisy, large-scale 3D point clouds.

Result: DeepBolt achieves 96.41% precision and 96.96% recall, surpassing state-of-the-art models by up to 42.5% in IoU for rock bolt points.

Conclusion: DeepBolt is robust and effective for automated rock bolt detection in complex underground environments, improving safety and efficiency in mining operations.

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [98] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/pdf/2506.20522)
*Chathura Wimalasiri, Piumal Rathnayake, Shamod Wijerathne, Sumudu Rasnayaka, Dhanushka Leuke Bandara, Roshan Ragel, Vajira Thambawita, Isuru Nawinne*

Main category: cs.CV

TL;DR: An AI-based deep learning framework is proposed to automatically detect and quantify alveolar bone loss and its patterns in periodontitis using IOPA radiographs, achieving high accuracy and offering a rapid, objective tool for periodontal assessment.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of bone loss severity and pattern is critical for diagnosing and treating periodontitis, which significantly impacts oral health and quality of life.

Method: Combines YOLOv8 for tooth detection, Keypoint R-CNN for anatomical landmarks, and YOLOv8x-seg for bone level segmentation to calculate severity and classify patterns (horizontal vs. angular).

Result: High accuracy in detecting bone loss severity (ICC up to 0.80) and classifying patterns (87% accuracy) on a dataset of 1000 radiographs.

Conclusion: The AI framework provides a rapid, objective, and reproducible tool for periodontal assessment, improving early diagnosis and personalized treatment planning for periodontitis.

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [99] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/pdf/2506.20550)
*Yitong Quan, Benjamin Kiefer, Martin Messmer, Andreas Zell*

Main category: cs.CV

TL;DR: The paper proposes a simple method to improve video object detection by stacking consecutive frames as input to a YOLO-based detector, leveraging temporal context without complex modifications.


<details>
  <summary>Details</summary>
Motivation: Single-frame detection models ignore temporal context in videos, while existing video-based methods are complex and computationally heavy. Practical applications face challenges like motion blur and occlusions.

Method: Stack multiple consecutive frames as input to a YOLO-based detector, supervising only the target frame's output.

Result: Improves detection robustness, especially for lightweight models, and narrows the gap between compact and heavy networks. Validated on MOT20Det and BOAT360 datasets.

Conclusion: The method effectively leverages temporal information with minimal architectural changes, maintaining simplicity and efficiency. The BOAT360 dataset is introduced to support future research.

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [100] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/pdf/2506.20563)
*Lei Zhu, Jun Zhou, Rick Siow Mong Goh, Yong Liu*

Main category: cs.CV

TL;DR: The paper proposes an adversarial masked image modeling method to enhance transformer performance in semi-supervised medical image segmentation, leveraging masked inputs and adversarial training to bridge domain gaps.


<details>
  <summary>Details</summary>
Motivation: Transformers require large labeled datasets, limiting their use in semi-supervised scenarios with scarce annotations. Existing methods struggle to train transformers effectively with limited labeled data.

Method: The method constructs an auxiliary masked domain using masked image modeling, trains the transformer to predict full masks from masked inputs, and employs adversarial training to reduce domain gaps.

Result: Extensive experiments on three datasets show the method significantly outperforms existing approaches.

Conclusion: The proposed adversarial masked image modeling effectively boosts semi-supervised learning for medical image segmentation, with code publicly available.

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [101] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/pdf/2506.20567)
*Zhiwang Zhang, Dong Xu, Wanli Ouyang, Chuanqi Tan*

Main category: cs.CV

TL;DR: A division-and-summarization (DaS) framework for dense video captioning partitions videos into event proposals, generates segment descriptions, and summarizes them into a single sentence using a two-stage LSTM with hierarchical attention.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dense video captioning by leveraging visual and semantic information from video segments.

Method: Partition videos into event proposals, extract visual features, generate segment descriptions, and summarize using a two-stage LSTM with hierarchical attention.

Result: Effective performance demonstrated on the ActivityNet Captions dataset.

Conclusion: The DaS framework successfully combines visual and semantic cues for dense video captioning.

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [102] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/pdf/2506.20582)
*Rajat Rasal, Avinash Kori, Ben Glocker*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end framework for learning identifiable causal representations in medical imaging, improving generalisability and robustness in disease classification tasks.


<details>
  <summary>Details</summary>
Motivation: To uncover true causal relationships in medical imaging data for better generalisability and robustness in disease classification.

Method: Grouping observations to enforce invariance with respect to race, sex, and imaging views in an end-to-end framework.

Result: Causal representations improve generalisability and robustness across multiple classification tasks.

Conclusion: Grouping observations for identifiable causal representations enhances performance in medical imaging tasks.

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [103] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/pdf/2506.20583)
*Zhiwang Zhang, Dong Xu, Wanli Ouyang, Luping Zhou*

Main category: cs.CV

TL;DR: The paper proposes a graph-based partition-and-summarization (GPaS) framework for dense video captioning, addressing scene evolution in long event proposals by splitting and summarizing video segments.


<details>
  <summary>Details</summary>
Motivation: Existing methods inadequately explore scene evolution in long event proposals, leading to suboptimal performance when scenes and objects change over time.

Method: The GPaS framework splits event proposals into shorter segments for finer captioning (partition stage) and summarizes these into a single sentence (summarization stage) using a GCN-LSTM interaction module.

Result: The approach outperforms state-of-the-art methods on ActivityNet Captions and YouCook II datasets.

Conclusion: The GPaS framework effectively handles scene evolution in dense video captioning, improving performance through structured summarization of semantic words.

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [104] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/pdf/2506.20586)
*Yitong Quan, Benjamin Kiefer, Martin Messmer, Andreas Zell*

Main category: cs.CV

TL;DR: A neural network-based method for monocular distance estimation using a 360° fisheye lens camera outperforms traditional geometric techniques, offering robustness and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional geometric methods struggle with lens distortions and environmental variability in omnidirectional imaging, necessitating a more robust solution.

Method: A learning-based approach using neural networks to infer object distances directly from raw omnidirectional inputs, bypassing the need for precise lens calibration.

Result: The proposed model outperforms traditional geometry-based methods and other learning baselines in accuracy and robustness across three 360° datasets.

Conclusion: Deep learning shows promise for real-time omnidirectional distance estimation, especially for low-cost robotics, autonomous navigation, and surveillance applications.

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [105] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/pdf/2506.20588)
*Pritam Mishra, Coloma Ballester, Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: A self-supervised video summarization model is introduced, avoiding costly annotations and attention-based methods, achieving state-of-the-art results on SUMME and TVSUM datasets.


<details>
  <summary>Details</summary>
Motivation: The need for efficient video summarization without reliance on supervised annotations or computationally expensive models drives this research.

Method: The model uses a Markov process-driven loss and a two-stage self-supervised learning paradigm, avoiding attention, RNNs, or transformers.

Result: Outperforms unsupervised methods and rivals supervised models on SUMME and TVSUM datasets.

Conclusion: The approach demonstrates the viability of efficient, annotation-free video summarization, challenging reliance on complex architectures.

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [106] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/pdf/2506.20590)
*Chaojun Ni, Jie Li, Haoyun Li, Hengyu Liu, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Boyuan Wang, Chenxin Li, Guan Huang, Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree enables interactive 3D scene generation from a single image, improving novel view quality and cross-view consistency with WorldRestorer and ConsistView.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods struggle with limited explorability and rendering quality in unseen areas, hindering immersive experiences.

Method: Decouples the problem into novel view quality (addressed by WorldRestorer) and cross-view consistency (addressed by ConsistView). Includes a data collection pipeline for training.

Result: WonderFree outperforms WonderWorld with 77.20% user preference, enhancing rendering quality and global coherence.

Conclusion: WonderFree offers a seamless, immersive 3D exploration experience, with publicly available code, model, and data.

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [107] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/pdf/2506.20670)
*Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1 is a reinforcement learning framework for large multimodal models (LMMs) to perform efficient, on-demand, multi-turn searches in real-world Internet environments, outperforming traditional methods like RAG.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and prompt-engineered search agents are inefficient for dynamic real-world information, necessitating a more flexible and efficient approach.

Method: The framework uses reinforcement learning with outcome-based rewards and search penalties, integrating image and text search tools. A multimodal search VQA dataset supports training.

Result: MMSearch-R1 outperforms RAG baselines of the same size, matches larger RAG models, and reduces search calls by over 30%.

Conclusion: The framework advances multimodal search research by enabling efficient, on-demand search behavior in LMMs.

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [108] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/pdf/2506.20599)
*Ji Qi, Xinchang Zhang, Dingqi Ye, Yongjia Ruan, Xin Guo, Shaowen Wang, Haifeng Li*

Main category: cs.CV

TL;DR: SFNet, a novel forgery detection framework, leverages spatial and frequency domain features to detect fake remote sensing imagery, outperforming existing methods by 4%-15.18% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of sophisticated generative AI makes fake remote sensing imagery harder to detect, risking misinformation. Existing methods rely on single visual features, limiting generalization.

Method: SFNet uses two feature extractors for spatial and frequency domains, with modules to align, fuse, and refine these features while suppressing redundancy.

Result: SFNet improves accuracy by 4%-15.18% over state-of-the-art methods and shows robust generalization across datasets.

Conclusion: SFNet effectively addresses the limitations of single-feature methods, offering a robust solution for detecting fake remote sensing imagery.

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [109] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/pdf/2506.20601)
*Rui Huang, Guangyao Zhai, Zuria Bauer, Marc Pollefeys, Federico Tombari, Leonidas Guibas, Gao Huang, Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene leverages video generation models for 3D scene synthesis, addressing limitations of LLMs and image-based methods by ensuring coherence and consistency.


<details>
  <summary>Details</summary>
Motivation: Automating 3D scene synthesis benefits fields like architecture and gaming, but current methods (LLMs, image generation) lack spatial reasoning and multi-view consistency.

Method: VIPScene integrates video generation, 3D reconstruction, and perception models, using text/image prompts for flexible, realistic synthesis.

Result: VIPScene outperforms existing methods, achieving high realism and structural consistency across diverse scenarios.

Conclusion: VIPScene offers a robust solution for 3D scene synthesis, with potential applications in various domains. Code will be released.

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [110] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/pdf/2506.20616)
*Quoc-Duy Tran, Anh-Tuan Vo, Dinh-Khoi Vo, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal is an automated framework that mimics human pareidolia by transforming natural object silhouettes into plausible animal forms using vision-language models and text-to-image diffusion.


<details>
  <summary>Details</summary>
Motivation: To replicate the human ability to perceive meaningful patterns (pareidolia) in ambiguous stimuli like clouds or stones, enabling creative applications in storytelling, education, and art.

Method: Uses open-vocabulary segmentation to extract silhouettes, interprets animal concepts via vision-language models, and synthesizes animal images with text-to-image diffusion, blending them into the original scene.

Result: Demonstrated robustness and creative potential on diverse real-world inputs, generating visually coherent and spatially consistent compositions.

Conclusion: Shape2Animal opens new opportunities for visual storytelling, educational content, digital art, and interactive media design.

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [111] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/pdf/2506.20638)
*Clément Forray, Pauline Delporte, Nicolas Delaygue, Florence Genin, Dawa Derksen*

Main category: cs.CV

TL;DR: The paper explores using Neural Radiance Fields (NeRF) for 3D reconstruction of non-cooperative space objects from simulated images, focusing on joint optimization of camera poses and NeRF.


<details>
  <summary>Details</summary>
Motivation: Understanding the state and behavior of Earth-orbiting objects is crucial for applications like debris removal and anomaly detection. 3D models enhance Space Situational Awareness (SSA).

Method: Leverages NeRF for 3D reconstruction, addressing challenges like monochromatic images, unknown object orientation, and limited viewing angles. Focuses on optimizing camera poses jointly with NeRF.

Result: Most accurate 3D reconstruction is achieved by training with successive images one-by-one, using uniform rotation for camera pose estimation and regularization to limit pose divergence.

Conclusion: Joint optimization of camera poses and NeRF improves 3D reconstruction accuracy for non-cooperative space objects, advancing SSA capabilities.

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [112] [Disentangled representations of microscopy images](https://arxiv.org/pdf/2506.20649)
*Jacopo Dapueto, Vito Paolo Pastore, Nicoletta Noceti, Francesca Odone*

Main category: cs.CV

TL;DR: A Disentangled Representation Learning (DRL) method is proposed to improve interpretability in microscopy image classification, balancing accuracy and interpretability across three domains.


<details>
  <summary>Details</summary>
Motivation: Interpretability is crucial for microscopy image analysis, yet remains a challenge despite deep learning advancements.

Method: DRL framework using synthetic data transfer is applied to microscopy image classification.

Result: The method achieves a good trade-off between accuracy and interpretability in plankton, yeast vacuoles, and human cell datasets.

Conclusion: DRL enhances interpretability in microscopy image analysis without compromising accuracy.

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [113] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/pdf/2506.20671)
*Markus Gross, Aya Fahmy, Danit Niwattananan, Dominik Muhle, Rui Song, Daniel Cremers, Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer introduces context-adaptive instance proposals for vision-based 3D Panoptic Scene Completion, outperforming state-of-the-art methods in panoptic metrics and runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static queries in Transformer-based SSC methods and the unexplored potential of camera-based PSC, IPFormer dynamically adapts instance proposals to the observed scene.

Method: IPFormer adaptively initializes panoptic instance proposals from image context and refines them through attention-based encoding and decoding to reason about semantic instance-voxel relationships.

Result: IPFormer surpasses state-of-the-art methods in PQ$^\dagger$ and PQ-All metrics, achieves a 14x runtime reduction, and improves Thing-metrics by 18.65% through dynamic proposal initialization.

Conclusion: IPFormer's context-adaptive instance proposals mark a pioneering advancement in vision-based 3D Panoptic Scene Completion, demonstrating significant performance gains and efficiency.

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


### [114] [KD-DETR: Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling](https://arxiv.org/pdf/2211.08071)
*Yu Wang, Xin Li, Shengzhao Weng, Gang Zhang, Haixiao Yue, Haocheng Feng, Junyu Han, Errui Ding*

Main category: cs.CV

TL;DR: The paper introduces KD-DETR, a knowledge distillation method for DETR, addressing the challenge of inconsistent distillation points and improving performance for both homogeneous and heterogeneous distillation.


<details>
  <summary>Details</summary>
Motivation: The lack of effective knowledge distillation methods for DETR due to inconsistent distillation points motivates the development of KD-DETR.

Method: Proposes KD-DETR with consistent distillation points sampling, decoupling detection and distillation tasks using specialized object queries and a general-to-specific sampling strategy.

Result: KD-DETR improves student model performance by 2.6%-5.2% for DETR variants and achieves 2.1% improvement in heterogeneous distillation (DINO to Faster R-CNN).

Conclusion: KD-DETR is effective and generalizable, offering a robust solution for knowledge distillation in DETR architectures.

Abstract: DETR is a novel end-to-end transformer architecture object detector, which
significantly outperforms classic detectors when scaling up. In this paper, we
focus on the compression of DETR with knowledge distillation. While knowledge
distillation has been well-studied in classic detectors, there is a lack of
researches on how to make it work effectively on DETR. We first provide
experimental and theoretical analysis to point out that the main challenge in
DETR distillation is the lack of consistent distillation points. Distillation
points refer to the corresponding inputs of the predictions for student to
mimic, which have different formulations in CNN detector and DETR, and reliable
distillation requires sufficient distillation points which are consistent
between teacher and student.
  Based on this observation, we propose the first general knowledge
distillation paradigm for DETR (KD-DETR) with consistent distillation points
sampling, for both homogeneous and heterogeneous distillation. Specifically, we
decouple detection and distillation tasks by introducing a set of specialized
object queries to construct distillation points for DETR. We further propose a
general-to-specific distillation points sampling strategy to explore the
extensibility of KD-DETR. Extensive experiments validate the effectiveness and
generalization of KD-DETR. For both single-scale DAB-DETR and multis-scale
Deformable DETR and DINO, KD-DETR boost the performance of student model with
improvements of $2.6\%-5.2\%$. We further extend KD-DETR to heterogeneous
distillation, and achieves $2.1\%$ improvement by distilling the knowledge from
DINO to Faster R-CNN with ResNet-50, which is comparable with homogeneous
distillation methods.The code is available at
https://github.com/wennyuhey/KD-DETR.

</details>


### [115] [Low-light Pedestrian Detection in Visible and Infrared Image Feeds: Issues and Challenges](https://arxiv.org/pdf/2311.08557)
*Thangarajah Akilan, Hrishikesh Vachhani*

Main category: cs.CV

TL;DR: A review of low-light pedestrian detection methods, focusing on FIR sensor-based approaches, categorizing algorithms, and discussing challenges and datasets.


<details>
  <summary>Details</summary>
Motivation: Pedestrian detection is crucial for applications like autonomous driving, but existing methods using visible images struggle in low-light conditions, prompting exploration of FIR sensors.

Method: Systematic categorization and analysis of algorithms, including region-based, non-region-based, and graph-based learning, along with implementation issues.

Result: Identifies key methodologies and challenges in low-light pedestrian detection, highlighting benchmark datasets for further research.

Conclusion: The study provides a comprehensive review of low-light pedestrian detection, emphasizing the need for advanced algorithms and suitable datasets.

Abstract: Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study reviews recent developments in low-light pedestrian
detection approaches. It systematically categorizes and analyses various
algorithms from region-based to non-region-based and graph-based learning
methodologies by highlighting their methodologies, implementation issues, and
challenges. It also outlines the key benchmark datasets that can be used for
research and development of advanced pedestrian detection algorithms,
particularly in low-light situations.

</details>


### [116] [MambaMorph: a Mamba-based Framework for Medical MR-CT Deformable Registration](https://arxiv.org/pdf/2401.13934)
*Tao Guo, Yinuo Wang, Shihao Shu, Weimin Yuan, Diansheng Chen, Zhouping Tang, Cai Meng, Xiangzhi Bai*

Main category: cs.CV

TL;DR: MambaMorph is a novel multi-modality deformable registration framework for medical images, outperforming state-of-the-art methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current registration methods lack accuracy and clinical practicality, necessitating a more efficient solution for multi-modality medical image analysis.

Method: MambaMorph combines a Mamba-based registration module for long-range correspondence and a lightweight feature extractor for high-dimensional learning. A new dataset, SR-Reg, is introduced to address data scarcity.

Result: MambaMorph achieves superior registration accuracy on SR-Reg and public datasets, with efficient computational performance.

Conclusion: MambaMorph shows strong potential for practical medical image registration, balancing accuracy and efficiency.

Abstract: Capturing voxel-wise spatial correspondence across distinct modalities is
crucial for medical image analysis. However, current registration approaches
are not practical enough in terms of registration accuracy and clinical
applicability. In this paper, we introduce MambaMorph, a novel multi-modality
deformable registration framework. Specifically, MambaMorph utilizes a
Mamba-based registration module and a fine-grained, yet simple, feature
extractor for efficient long-range correspondence modeling and high-dimensional
feature learning, respectively. Additionally, we develop a well-annotated brain
MR-CT registration dataset, SR-Reg, to address the scarcity of data in
multi-modality registration. To validate MambaMorph's multi-modality
registration capabilities, we conduct quantitative experiments on both our
SR-Reg dataset and a public T1-T2 dataset. The experimental results on both
datasets demonstrate that MambaMorph significantly outperforms the current
state-of-the-art learning-based registration methods in terms of registration
accuracy. Further study underscores the efficiency of the Mamba-based
registration module and the lightweight feature extractor, which achieve
notable registration quality while maintaining reasonable computational costs
and speeds. We believe that MambaMorph holds significant potential for
practical applications in medical image registration. The code for MambaMorph
is available at: https://github.com/Guo-Stone/MambaMorph.

</details>


### [117] [FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray Image Segmentation](https://arxiv.org/pdf/2403.08059)
*Benjamin D. Killeen, Liam J. Wang, Blanca Inigo, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath*

Main category: cs.CV

TL;DR: FluoroSAM is a language-promptable X-ray image segmentation model trained on synthetic data, enabling flexible human-in-the-loop workflows in precision medicine.


<details>
  <summary>Details</summary>
Motivation: Existing models are task-specific and lack broad applicability. Language-aligned foundation models (LFMs) offer promise but need adaptation for X-ray imaging's variability and data constraints.

Method: FluoroSAM, a variant of the Segment Anything Model, is trained on 3M synthetic X-ray images with pseudo-ground truth masks and text descriptions, incorporating vector quantization (VQ) of text embeddings.

Result: FluoroSAM segments anatomical structures and tools based on natural language prompts, demonstrating performance on real X-ray images and enabling rich human-machine interaction.

Conclusion: FluoroSAM advances language-promptable X-ray image segmentation, supporting diverse applications in diagnostic and interventional precision medicine.

Abstract: Language promptable X-ray image segmentation would enable greater flexibility
for human-in-the-loop workflows in diagnostic and interventional precision
medicine. Prior efforts have contributed task-specific models capable of
solving problems within a narrow scope, but expanding to broader use requires
additional data, annotations, and training time. Recently, language-aligned
foundation models (LFMs) -- machine learning models trained on large amounts of
highly variable image and text data thus enabling broad applicability -- have
emerged as promising tools for automated image analysis. Existing foundation
models for medical image analysis focus on scenarios and modalities where
large, richly annotated datasets are available. However, the X-ray imaging
modality features highly variable image appearance and applications, from
diagnostic chest X-rays to interventional fluoroscopy, with varying
availability of data. To pave the way toward an LFM for comprehensive and
language-aligned analysis of arbitrary medical X-ray images, we introduce
FluoroSAM, a language-promptable variant of the Segment Anything Model, trained
from scratch on 3M synthetic X-ray images from a wide variety of human
anatomies, imaging geometries, and viewing angles. These include pseudo-ground
truth masks for 128 organ types and 464 tools with associated text
descriptions. FluoroSAM is capable of segmenting myriad anatomical structures
and tools based on natural language prompts, thanks to the novel incorporation
of vector quantization (VQ) of text embeddings in the training process. We
demonstrate FluoroSAM's performance quantitatively on real X-ray images and
showcase on several applications how FluoroSAM is a key enabler for rich
human-machine interaction in the X-ray image acquisition and analysis context.
Code is available at https://github.com/arcadelab/fluorosam.

</details>


### [118] [Neural Graph Map: Dense Mapping with Efficient Loop Closure Integration](https://arxiv.org/pdf/2405.03633)
*Leonard Bruns, Jun Zhang, Patric Jensfelt*

Main category: cs.CV

TL;DR: A novel RGB-D neural mapping framework using lightweight neural fields anchored to a sparse visual SLAM system, improving loop closure integration and scalability.


<details>
  <summary>Details</summary>
Motivation: Monolithic neural fields in SLAM limit loop closure efficiency and scalability.

Method: Dynamic anchoring of lightweight neural fields to a sparse visual SLAM pose graph.

Result: Effective large-scale loop closure integration, minimal reintegration, and superior performance in quality and runtime.

Conclusion: The proposed method outperforms state-of-the-art approaches in large scenes, offering scalability and efficiency.

Abstract: Neural field-based SLAM methods typically employ a single, monolithic field
as their scene representation. This prevents efficient incorporation of loop
closure constraints and limits scalability. To address these shortcomings, we
propose a novel RGB-D neural mapping framework in which the scene is
represented by a collection of lightweight neural fields which are dynamically
anchored to the pose graph of a sparse visual SLAM system. Our approach shows
the ability to integrate large-scale loop closures, while requiring only
minimal reintegration. Furthermore, we verify the scalability of our approach
by demonstrating successful building-scale mapping taking multiple loop
closures into account during the optimization, and show that our method
outperforms existing state-of-the-art approaches on large scenes in terms of
quality and runtime. Our code is available open-source at
https://github.com/KTH-RPL/neural_graph_mapping.

</details>


### [119] [MagicPose4D: Crafting Articulated Models with Appearance and Motion Control](https://arxiv.org/pdf/2405.14017)
*Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, Narendra Ahuja*

Main category: cs.CV

TL;DR: MagicPose4D is a framework for precise 4D content generation using motion prompts like videos or mesh sequences, improving accuracy and consistency over text-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-prompt-based 4D generation methods struggle with complex or rare motions, prompting the need for a more controlled approach.

Method: MagicPose4D uses a Dual-Phase 4D Reconstruction Module for shape and motion capture, and a Cross-category Motion Transfer Module for motion generalization.

Result: The framework outperforms existing methods in benchmarks, offering refined control and physical plausibility in 4D generation.

Conclusion: MagicPose4D advances 4D content generation by enabling precise motion control and cross-category transfer without extra training.

Abstract: With the success of 2D and 3D visual generative models, there is growing
interest in generating 4D content. Existing methods primarily rely on text
prompts to produce 4D content, but they often fall short of accurately defining
complex or rare motions. To address this limitation, we propose MagicPose4D, a
novel framework for refined control over both appearance and motion in 4D
generation. Unlike current 4D generation methods, MagicPose4D accepts monocular
videos or mesh sequences as motion prompts, enabling precise and customizable
motion control. MagicPose4D comprises two key modules: (i) Dual-Phase 4D
Reconstruction Module, which operates in two phases. The first phase focuses on
capturing the model's shape using accurate 2D supervision and less accurate but
geometrically informative 3D pseudo-supervision without imposing skeleton
constraints. The second phase extracts the 3D motion (skeleton poses) using
more accurate pseudo-3D supervision, obtained in the first phase and introduces
kinematic chain-based skeleton constraints to ensure physical plausibility.
Additionally, we propose a Global-local Chamfer loss that aligns the overall
distribution of predicted mesh vertices with the supervision while maintaining
part-level alignment without extra annotations. (ii) Cross-category Motion
Transfer Module, which leverages the extracted motion from the 4D
reconstruction module and uses a kinematic-chain-based skeleton to achieve
cross-category motion transfer. It ensures smooth transitions between frames
through dynamic rigidity, facilitating robust generalization without additional
training. Through extensive experiments, we demonstrate that MagicPose4D
significantly improves the accuracy and consistency of 4D content generation,
outperforming existing methods in various benchmarks.

</details>


### [120] [GlyphPattern: An Abstract Pattern Recognition Benchmark for Vision-Language Models](https://arxiv.org/pdf/2408.05894)
*Zixuan Wu, Yoolim Kim, Carolyn Jane Anderson*

Main category: cs.CV

TL;DR: GlyphPattern dataset evaluates VLMs' abstract pattern recognition, revealing challenges despite their strong performance on trained tasks.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability in abstract pattern recognition, a gap in their current capabilities.

Method: Introduces GlyphPattern, a dataset with 954 items pairing human-written descriptions of visual patterns from 40 writing systems with three visual styles.

Result: State-of-the-art VLMs like GPT-4o achieve only 55% accuracy, showing limited improvement with few-shot prompting.

Conclusion: VLMs struggle with abstract pattern recognition due to challenges in visual processing, language understanding, and generalization.

Abstract: Vision-Language Models (VLMs) building upon the foundation of powerful large
language models have made rapid progress in reasoning across visual and textual
data. While VLMs perform well on vision tasks that they are trained on, our
results highlight key challenges in abstract pattern recognition. We present
GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of
visual patterns from 40 writing systems with three visual presentation styles.
  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models
to understand and judge natural language descriptions of visual patterns.
GlyphPattern patterns are drawn from a large-scale cognitive science
investigation of human writing systems; as a result, they are rich in spatial
reference and compositionality. Our experiments show that GlyphPattern is
challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with
marginal gains from few-shot prompting. Our detailed error analysis reveals
challenges at multiple levels, including visual processing, natural language
understanding, and pattern generalization.

</details>


### [121] [ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model](https://arxiv.org/pdf/2408.16767)
*Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan*

Main category: cs.CV

TL;DR: ReconX introduces a novel 3D scene reconstruction method using pre-trained video diffusion models to address sparse-view reconstruction challenges, ensuring detail preservation and 3D consistency.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3D scene reconstruction often leads to artifacts and distortions in unseen areas, posing an ill-posed optimization problem.

Method: ReconX reframes reconstruction as a temporal generation task, leveraging pre-trained video diffusion models guided by a global point cloud condition. It synthesizes consistent video frames and recovers the 3D scene using Gaussian Splatting.

Result: ReconX outperforms state-of-the-art methods in quality and generalizability across real-world datasets.

Conclusion: The proposed ReconX paradigm effectively addresses sparse-view reconstruction challenges by combining generative priors and 3D consistency, yielding superior results.

Abstract: Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.

</details>


### [122] [Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning](https://arxiv.org/pdf/2411.01969)
*Zhengyang Yu, Arthur Aubret, Marcel C. Raabe, Jane Yang, Chen Yu, Jochen Triesch*

Main category: cs.CV

TL;DR: Toddlers' gaze behavior aids unsupervised learning of view-invariant object recognition.


<details>
  <summary>Details</summary>
Motivation: Understand how toddlers' eye and head movements contribute to their object recognition abilities.

Method: Combines head-mounted eye tracking with unsupervised machine learning to analyze toddlers' visual experience.

Result: Toddlers' gaze strategy helps learn invariant object representations, with central visual field size being crucial.

Conclusion: Toddlers' gaze behavior supports development of view-invariant object recognition.

Abstract: Toddlers learn to recognize objects from different viewpoints with almost no
supervision. During this learning, they execute frequent eye and head movements
that shape their visual experience. It is presently unclear if and how these
behaviors contribute to toddlers' emerging object recognition abilities. To
answer this question, we here combine head-mounted eye tracking during dyadic
play with unsupervised machine learning. We approximate toddlers' central
visual field experience by cropping image regions from a head-mounted camera
centered on the current gaze location estimated via eye tracking. This visual
stream feeds an unsupervised computational model of toddlers' learning, which
constructs visual representations that slowly change over time. Our experiments
demonstrate that toddlers' gaze strategy supports the learning of invariant
object representations. Our analysis also shows that the limited size of the
central visual field where acuity is high is crucial for this. Overall, our
work reveals how toddlers' gaze behavior may support their development of
view-invariant object recognition.

</details>


### [123] [USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting](https://arxiv.org/pdf/2411.10504)
*Kang Chen, Jiyuan Zhang, Zecheng Hao, Yajing Zheng, Tiejun Huang, Zhaofei Yu*

Main category: cs.CV

TL;DR: The paper introduces USP-Gaussian, an end-to-end framework unifying spike-based image reconstruction, pose correction, and 3D Gaussian Splatting to address cumulative errors in cascaded spike-based 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of cascaded pipelines in spike-based 3D reconstruction, which suffer from cumulative errors degrading fidelity.

Method: Proposes a synergistic optimization framework (USP-Gaussian) integrating spike-to-image reconstruction, pose correction, and 3D Gaussian Splatting, leveraging multi-view consistency and spike camera motion capture.

Result: Outperforms previous methods on synthetic datasets by eliminating cascading errors and achieves robust 3D reconstruction in real-world scenarios with inaccurate poses.

Conclusion: USP-Gaussian effectively reduces noise, preserves fine details, and enhances 3D reconstruction fidelity by unifying key steps into an end-to-end framework.

Abstract: Spike cameras, as an innovative neuromorphic camera that captures scenes with
the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D
reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting
(3DGS). Previous spike-based 3D reconstruction approaches often employ a
casecased pipeline: starting with high-quality image reconstruction from spike
streams based on established spike-to-image reconstruction algorithms, then
progressing to camera pose estimation and 3D reconstruction. However, this
cascaded approach suffers from substantial cumulative errors, where quality
limitations of initial image reconstructions negatively impact pose estimation,
ultimately degrading the fidelity of the 3D reconstruction. To address these
issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian},
that unifies spike-based image reconstruction, pose correction, and Gaussian
splatting into an end-to-end framework. Leveraging the multi-view consistency
afforded by 3DGS and the motion capture capability of the spike camera, our
framework enables a joint iterative optimization that seamlessly integrates
information between the spike-to-image network and 3DGS. Experiments on
synthetic datasets with accurate poses demonstrate that our method surpasses
previous approaches by effectively eliminating cascading errors. Moreover, we
integrate pose optimization to achieve robust 3D reconstruction in real-world
scenarios with inaccurate initial poses, outperforming alternative methods by
effectively reducing noise and preserving fine texture details. Our code, data
and trained models will be available at
https://github.com/chenkang455/USP-Gaussian.

</details>


### [124] [ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting with Multi-View Geometric Consistency](https://arxiv.org/pdf/2412.01402)
*Zhuoxiao Li, Shanliang Yao, Taoyu Wu, Yong Yue, Wufan Zhao, Rongjun Qin, Angel F. Garcia-Fernandez, Andrew Levers, Xiaohui Zhu*

Main category: cs.CV

TL;DR: ULSR-GS improves large-scale aerial surface extraction by combining point-to-photo partitioning and multi-view geometric consistency, outperforming existing GS-based methods.


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting (GS) struggles with large-scale aerial image surface extraction, prompting the need for a more robust framework.

Method: ULSR-GS uses point-to-photo partitioning and multi-view optimal view matching, along with densification based on geometric consistency.

Result: ULSR-GS achieves higher accuracy in surface extraction for large-scale aerial scenes compared to other GS-based methods.

Conclusion: ULSR-GS effectively addresses GS limitations in large-scale tasks, offering improved performance in complex urban environments.

Abstract: While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.

</details>


### [125] [World-Consistent Data Generation for Vision-and-Language Navigation](https://arxiv.org/pdf/2412.06413)
*Yu Zhong, Rui Zhang, Zihao Zhang, Shuo Wang, Chuan Fang, Xishan Zhang, Jiaming Guo, Shaohui Peng, Di Huang, Yanyang Yan, Xing Hu, Qi Guo*

Main category: cs.CV

TL;DR: The paper introduces WCGEN, a data-augmentation framework for Vision-and-Language Navigation (VLN) to address data scarcity and improve generalization to unseen environments by ensuring diversity and world-consistency.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in VLN leads to poor generalization. Existing data augmentation lacks diversity and world-consistency.

Method: WCGEN uses a two-stage approach: trajectory stage (point-cloud based for spatial coherency) and viewpoint stage (angle synthesis for spatial and wraparound consistency).

Result: Experiments show WCGEN achieves state-of-the-art results on navigation tasks and enhances generalization to unseen environments.

Conclusion: WCGEN effectively addresses VLN data scarcity by generating diverse, world-consistent data, improving agent performance.

Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires an
agent to navigate through photorealistic environments following
natural-language instructions. One main obstacle existing in VLN is data
scarcity, leading to poor generalization performance over unseen environments.
Though data argumentation is a promising way for scaling up the dataset, how to
generate VLN data both diverse and world-consistent remains problematic. To
cope with this issue, we propose the world-consistent data generation (WCGEN),
an efficacious data-augmentation framework satisfying both diversity and
world-consistency, aimed at enhancing the generalization of agents to novel
environments. Roughly, our framework consists of two stages, the trajectory
stage which leverages a point-cloud based technique to ensure spatial coherency
among viewpoints, and the viewpoint stage which adopts a novel angle synthesis
method to guarantee spatial and wraparound consistency within the entire
observation. By accurately predicting viewpoint changes with 3D knowledge, our
approach maintains the world-consistency during the generation procedure.
Experiments on a wide range of datasets verify the effectiveness of our method,
demonstrating that our data augmentation strategy enables agents to achieve new
state-of-the-art results on all navigation tasks, and is capable of enhancing
the VLN agents' generalization ability to unseen environments.

</details>


### [126] [Matching-Free Depth Recovery from Structured Light](https://arxiv.org/pdf/2501.07113)
*Zhuohang Yu, Kai Wang, Kun Huang, Juyong Zhang*

Main category: cs.CV

TL;DR: A novel depth estimation method using monocular structured light systems, employing a density voxel grid and self-supervised differentiable volume rendering for faster convergence and high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on image matching, which can be limiting. The proposed approach aims to improve geometric fidelity and training speed.

Method: Uses a density voxel grid trained via self-supervised differentiable volume rendering, leveraging color fields from structured light patterns. Integrates NDC, distortion loss, and surface-based color loss.

Result: Outperforms matching-based techniques, reducing depth errors by ~30% and training ~3x faster than implicit representation methods.

Conclusion: The method offers superior geometric performance and efficiency, making it a promising alternative for depth estimation.

Abstract: We introduce a novel approach for depth estimation using images obtained from
monocular structured light systems. In contrast to many existing methods that
depend on image matching, our technique employs a density voxel grid to
represent scene geometry. This grid is trained through self-supervised
differentiable volume rendering. Our method leverages color fields derived from
the projected patterns in structured light systems during the rendering
process, facilitating the isolated optimization of the geometry field. This
innovative approach leads to faster convergence and high-quality results.
Additionally, we integrate normalized device coordinates (NDC), a distortion
loss, and a distinctive surface-based color loss to enhance geometric fidelity.
Experimental results demonstrate that our method outperforms current
matching-based techniques in terms of geometric performance in few-shot
scenarios, achieving an approximately 30% reduction in average estimated depth
errors for both synthetic scenes and real-world captured scenes. Moreover, our
approach allows for rapid training, being approximately three times faster than
previous matching-free methods that utilize implicit representations.

</details>


### [127] [Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images](https://arxiv.org/pdf/2501.09552)
*Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga*

Main category: cs.CV

TL;DR: An AI-based pipeline for detecting PHI in medical images, combining text detection, extraction, and analysis, achieves optimal performance by integrating vision and language models.


<details>
  <summary>Details</summary>
Motivation: Limited evaluation of AI-based PHI detection tools hinders reliable development, necessitating a robust solution for privacy in medical data sharing.

Method: A three-module pipeline (text detection, extraction, analysis) tested with YOLOv11, EasyOCR, and GPT-4o on diverse datasets.

Result: Dedicated models for each module balance performance, latency, and cost, with LLMs improving OCR and enabling end-to-end PHI detection.

Conclusion: The pipeline demonstrates effectiveness in PHI detection, leveraging specialized models and LLMs for enhanced performance and practicality.

Abstract: De-identification of medical images is a critical step to ensure privacy
during data sharing in research and clinical settings. The initial step in this
process involves detecting Protected Health Information (PHI), which can be
found in image metadata or imprinted within image pixels. Despite the
importance of such systems, there has been limited evaluation of existing
AI-based solutions, creating barriers to the development of reliable and robust
tools. In this study, we present an AI-based pipeline for PHI detection,
comprising three key modules: text detection, text extraction, and text
analysis. We benchmark three models - YOLOv11, EasyOCR, and GPT-4o - across
different setups corresponding to these modules, evaluating their performance
on two different datasets encompassing multiple imaging modalities and PHI
categories. Our findings indicate that the optimal setup involves utilizing
dedicated vision and language models for each module, which achieves a
commendable balance in performance, latency, and cost associated with the usage
of Large Language Models (LLMs). Additionally, we show that the application of
LLMs not only involves identifying PHI content but also enhances OCR tasks and
facilitates an end-to-end PHI detection pipeline, showcasing promising outcomes
through our analysis.

</details>


### [128] [VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback](https://arxiv.org/pdf/2501.17726)
*Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier*

Main category: cs.CV

TL;DR: A novel multimodal framework enhances AI-generated medical reports for chest X-rays by improving semantic alignment and localization accuracy, using phrase grounding and text-to-image diffusion modules, validated by a dual-scoring system.


<details>
  <summary>Details</summary>
Motivation: Address the lack of reliability and interpretability in current AI-generated medical reports for chest X-rays, which often require expert oversight.

Method: Integrates a Phrase Grounding Model for pathology localization and a Text-to-Image Diffusion Module for synthetic image generation, with a dual-scoring system for validation.

Result: Outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment.

Conclusion: The framework provides a robust validation mechanism, advancing trustworthy and transparent AI in medical imaging.

Abstract: As artificial intelligence (AI) becomes increasingly central to healthcare,
the demand for explainable and trustworthy models is paramount. Current report
generation systems for chest X-rays (CXR) often lack mechanisms for validating
outputs without expert oversight, raising concerns about reliability and
interpretability. To address these challenges, we propose a novel multimodal
framework designed to enhance the semantic alignment and localization accuracy
of AI-generated medical reports. Our framework integrates two key modules: a
Phrase Grounding Model, which identifies and localizes pathologies in CXR
images based on textual prompts, and a Text-to-Image Diffusion Module, which
generates synthetic CXR images from prompts while preserving anatomical
fidelity. By comparing features between the original and generated images, we
introduce a dual-scoring system: one score quantifies localization accuracy,
while the other evaluates semantic consistency. This approach significantly
outperforms existing methods, achieving state-of-the-art results in pathology
localization and text-to-image alignment. The integration of phrase grounding
with diffusion models, coupled with the dual-scoring evaluation system,
provides a robust mechanism for validating report quality, paving the way for
more trustworthy and transparent AI in medical imaging.

</details>


### [129] [MatSwap: Light-aware material transfers in images](https://arxiv.org/pdf/2502.07784)
*Ivan Lopes, Valentin Deschaintre, Yannick Hold-Geoffroy, Raoul de Charette*

Main category: cs.CV

TL;DR: MatSwap is a method for photorealistic material transfer in images, avoiding manual annotations or 3D scene properties by learning material appearance directly.


<details>
  <summary>Details</summary>
Motivation: Material editing is challenging due to entangled appearance, geometry, and lighting; existing methods rely on impractical manual work or text engineering.

Method: Uses a light- and geometry-aware diffusion model, fine-tuning a pre-trained text-to-image model with synthetic data for material transfer.

Result: Effectively integrates materials into target images, outperforming recent methods in quality and realism.

Conclusion: MatSwap offers a practical, high-quality solution for material transfer without requiring extensive manual input.

Abstract: We present MatSwap, a method to transfer materials to designated surfaces in
an image photorealistically. Such a task is non-trivial due to the large
entanglement of material appearance, geometry, and lighting in a photograph. In
the literature, material editing methods typically rely on either cumbersome
text engineering or extensive manual annotations requiring artist knowledge and
3D scene properties that are impractical to obtain. In contrast, we propose to
directly learn the relationship between the input material -- as observed on a
flat surface -- and its appearance within the scene, without the need for
explicit UV mapping. To achieve this, we rely on a custom light- and
geometry-aware diffusion model. We fine-tune a large-scale pre-trained
text-to-image model for material transfer using our synthetic dataset,
preserving its strong priors to ensure effective generalization to real images.
As a result, our method seamlessly integrates a desired material into the
target location in the photograph while retaining the identity of the scene. We
evaluate our method on synthetic and real images and show that it compares
favorably to recent work both qualitatively and quantitatively. We release our
code and data on https://github.com/astra-vision/MatSwap

</details>


### [130] [Diffusion Models Through a Global Lens: Are They Culturally Inclusive?](https://arxiv.org/pdf/2502.08914)
*Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, Alice Oh*

Main category: cs.CV

TL;DR: The paper introduces CultDiff, a benchmark to evaluate text-to-image diffusion models' ability to generate culturally specific images, revealing their shortcomings, especially for underrepresented regions.


<details>
  <summary>Details</summary>
Motivation: To assess whether state-of-the-art diffusion models can accurately represent cultural nuances in generated images.

Method: The study evaluates models using the CultDiff benchmark, analyzing cultural relevance, description fidelity, and realism. Human evaluations inform the development of CultDiff-S, a neural-based similarity metric.

Result: Models often fail to generate culturally accurate artifacts, particularly for underrepresented regions, showing disparities in relevance, fidelity, and realism.

Conclusion: The work underscores the need for more inclusive AI systems and equitable dataset representation across diverse cultures.

Abstract: Text-to-image diffusion models have recently enabled the creation of visually
compelling, detailed images from textual prompts. However, their ability to
accurately represent various cultural nuances remains an open question. In our
work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion
models whether they can generate culturally specific images spanning ten
countries. We show that these models often fail to generate cultural artifacts
in architecture, clothing, and food, especially for underrepresented country
regions, by conducting a fine-grained analysis of different similarity aspects,
revealing significant disparities in cultural relevance, description fidelity,
and realism compared to real-world reference images. With the collected human
evaluations, we develop a neural-based image-image similarity metric, namely,
CultDiff-S, to predict human judgment on real and generated images with
cultural artifacts. Our work highlights the need for more inclusive generative
AI systems and equitable dataset representation over a wide range of cultures.

</details>


### [131] [Image Super-Resolution with Guarantees via Conformalized Generative Models](https://arxiv.org/pdf/2502.09664)
*Eduardo Adame, Daniel Csillag, Guilherme Tegoni Goedert*

Main category: cs.CV

TL;DR: A novel conformal prediction-based method for uncertainty quantification in generative ML models for image restoration, providing interpretable confidence masks.


<details>
  <summary>Details</summary>
Motivation: The need for robust and interpretable uncertainty quantification in generative ML models for tasks like super-resolution.

Method: Uses conformal prediction to create confidence masks, adaptable to any black-box model, requires minimal calibration data, and is customizable via local image similarity metrics.

Result: Strong theoretical guarantees for fidelity error control, reconstruction quality, and robustness against data leakage, supported by empirical validation.

Conclusion: The method reliably communicates trust in generated images and performs well empirically.

Abstract: The increasing use of generative ML foundation models for image restoration
tasks such as super-resolution calls for robust and interpretable uncertainty
quantification methods. We address this need by presenting a novel approach
based on conformal prediction techniques to create a 'confidence mask' capable
of reliably and intuitively communicating where the generated image can be
trusted. Our method is adaptable to any black-box generative model, including
those locked behind an opaque API, requires only easily attainable data for
calibration, and is highly customizable via the choice of a local image
similarity metric. We prove strong theoretical guarantees for our method that
span fidelity error control (according to our local image similarity metric),
reconstruction quality, and robustness in the face of data leakage. Finally, we
empirically evaluate these results and establish our method's solid
performance.

</details>


### [132] [FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion](https://arxiv.org/pdf/2503.01109)
*Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou*

Main category: cs.CV

TL;DR: A novel adaptive densification method using Fourier frequency domain analysis improves 3D Gaussian splatting for SLAM, enabling real-time high-fidelity mapping and efficient tracking.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of uncertainty in Gaussian position and initialization parameters, which cause slow convergence and redundant or insufficient representations in SLAM.

Method: Introduces adaptive densification via Fourier frequency domain analysis for Gaussian priors and constructs independent sparse (for tracking) and dense (for visuals) maps.

Result: Achieves 36 FPS on Replica and TUM RGB-D datasets with competitive accuracy in localization and mapping.

Conclusion: The first SLAM system using frequency domain analysis for real-time high-quality Gaussian mapping, demonstrating efficiency and accuracy.

Abstract: 3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.

</details>


### [133] [Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation](https://arxiv.org/pdf/2503.05319)
*Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak*

Main category: cs.CV

TL;DR: The paper proposes EDRL, a strategy combining Essence-Point and Disentangle Representation Learning, to improve multimodal ophthalmic diagnosis by addressing redundancy and overlapping features in traditional deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Ophthalmologists rely on multimodal data for accurate diagnosis, but incomplete data and privacy concerns limit its use. Traditional methods fail to handle task-irrelevant redundancy and overlapping features effectively.

Method: EDRL integrates self-distillation into an end-to-end framework, featuring Essence-Point Representation Learning for discriminative feature selection and Disentangled Representation Learning to separate modality-common and modality-unique features.

Result: EDRL outperforms state-of-the-art methods on multimodal ophthalmology datasets, improving disease grading and interpretability.

Conclusion: EDRL enhances multimodal learning by reducing redundancy and disentangling features, offering a robust solution for ophthalmic diagnosis.

Abstract: This paper discusses how ophthalmologists often rely on multimodal data to
improve diagnostic accuracy. However, complete multimodal data is rare in
real-world applications due to a lack of medical equipment and concerns about
data privacy. Traditional deep learning methods typically address these issues
by learning representations in latent space. However, the paper highlights two
key limitations of these approaches: (i) Task-irrelevant redundant information
(e.g., numerous slices) in complex modalities leads to significant redundancy
in latent space representations. (ii) Overlapping multimodal representations
make it difficult to extract unique features for each modality. To overcome
these challenges, the authors propose the Essence-Point and Disentangle
Representation Learning (EDRL) strategy, which integrates a self-distillation
mechanism into an end-to-end framework to enhance feature selection and
disentanglement for more robust multimodal learning. Specifically, the
Essence-Point Representation Learning module selects discriminative features
that improve disease grading performance. The Disentangled Representation
Learning module separates multimodal data into modality-common and
modality-unique representations, reducing feature entanglement and enhancing
both robustness and interpretability in ophthalmic disease diagnosis.
Experiments on multimodal ophthalmology datasets show that the proposed EDRL
strategy significantly outperforms current state-of-the-art methods.

</details>


### [134] [From $\mathcal{O}(n^{2})$ to $\mathcal{O}(n)$ Parameters: Quantum Self-Attention in Vision Transformers for Biomedical Image Classification](https://arxiv.org/pdf/2503.07294)
*Thomas Boucher, John Whittle, Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: Quantum vision transformers (QViTs) with quantum self-attention (QSA) match SOTA biomedical image classifiers using 99.99% fewer parameters and achieve comparable performance to classical ViTs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of parameter scaling in classical vision transformers (ViTs) by replacing linear self-attention layers with quantum neural networks (QNNs).

Method: Replaced self-attention mechanisms in ViTs with QSA mechanisms using parameterized QNNs, reducing parameter scaling from O(n²) to O(n). Evaluated on RetinaMNIST and eight diverse biomedical datasets.

Result: QViTs outperformed 13/14 SOTA methods on RetinaMNIST (56.5% accuracy) with 99.99% fewer parameters and 89% fewer GFLOPs. Maintained comparable performance to classical ViTs across datasets.

Conclusion: QSA is a practical, parameter-efficient alternative for biomedical image analysis, with knowledge distillation enhancing performance for higher-qubit architectures.

Abstract: We demonstrate that quantum vision transformers (QViTs), vision transformers
(ViTs) with self-attention (SA) mechanisms replaced by quantum self-attention
(QSA) mechanisms, can match state-of-the-art (SOTA) biomedical image
classifiers while using 99.99% fewer parameters. QSAs are produced by replacing
linear SA layers with parameterised quantum neural networks (QNNs), producing a
QSA mechanism and reducing parameter scaling from $\mathcal{O}(n^2)$ to
$\mathcal{O}(n)$. On RetinaMNIST, our ultra parameter-efficient QViT
outperforms 13/14 SOTA methods including CNNs and ViTs, achieving 56.5%
accuracy, just 0.88% below the top MedMamba model while using 99.99% fewer
parameters (1K vs 14.5M) and 89% fewer GFLOPs. We present the first
investigation of knowledge distillation (KD) from classical to quantum vision
transformers in biomedical image classification, showing that QViTs maintain
comparable performance to classical ViTs across eight diverse datasets spanning
multiple modalities, with improved QSA parameter-efficiency. Our higher-qubit
architecture benefitted more from KD pre-training, suggesting a scaling
relationship between QSA parameters and KD effectiveness. These findings
establish QSA as a practical architectural choice toward parameter-efficient
biomedical image analysis.

</details>


### [135] [MaizeField3D: A Curated 3D Point Cloud and Procedural Model Dataset of Field-Grown Maize from a Diversity Panel](https://arxiv.org/pdf/2503.07813)
*Elvis Kimara, Mozhgan Hadadi, Jackson Godbersen, Aditya Balu, Talukder Jubery, Yawei Li, Adarsh Krishnamurthy, Patrick S. Schnable, Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: MaizeField3D is a curated 3D point cloud dataset of field-grown maize plants, designed for AI-driven phenotyping and agricultural research.


<details>
  <summary>Details</summary>
Motivation: The lack of large, diverse 3D datasets for maize limits AI/ML tool development for 3D phenotyping. 2D images miss critical structural details.

Method: Collected 1,045 TLS point clouds, segmented and annotated 520 plants using graph-based methods, and fitted procedural models with NURBS surfaces via optimization.

Result: A high-quality, labeled dataset with metadata, multi-resolution point clouds, and procedural models for maize plants.

Conclusion: MaizeField3D provides a foundational resource for AI-driven agricultural research and 3D phenotyping.

Abstract: The development of artificial intelligence (AI) and machine learning (ML)
based tools for 3D phenotyping, especially for maize, has been limited due to
the lack of large and diverse 3D datasets. 2D image datasets fail to capture
essential structural details such as leaf architecture, plant volume, and
spatial arrangements that 3D data provide. To address this limitation, we
present MaizeField3D (https://baskargroup.github.io/MaizeField3D/), a curated
dataset of 3D point clouds of field-grown maize plants from a diverse genetic
panel, designed to be AI-ready for advancing agricultural research. Our dataset
includes 1,045 high-quality point clouds of field-grown maize collected using a
terrestrial laser scanner (TLS). Point clouds of 520 plants from this dataset
were segmented and annotated using a graph-based segmentation method to isolate
individual leaves and stalks, ensuring consistent labeling across all samples.
This labeled data was then used for fitting procedural models that provide a
structured parametric representation of the maize plants. The leaves of the
maize plants in the procedural models are represented using Non-Uniform
Rational B-Spline (NURBS) surfaces that were generated using a two-step
optimization process combining gradient-free and gradient-based methods. We
conducted rigorous manual quality control on all datasets, correcting errors in
segmentation, ensuring accurate leaf ordering, and validating metadata
annotations. The dataset also includes metadata detailing plant morphology and
quality, alongside multi-resolution subsampled point cloud data (100k, 50k, 10k
points), which can be readily used for different downstream computational
tasks. MaizeField3D will serve as a comprehensive foundational dataset for
AI-driven phenotyping, plant structural analysis, and 3D applications in
agricultural research.

</details>


### [136] [A Siamese Network to Detect If Two Iris Images Are Monozygotic](https://arxiv.org/pdf/2503.09749)
*Yongle Yuan, Kevin W. Bowyer*

Main category: cs.CV

TL;DR: An automated classifier using a Siamese network and contrastive learning to identify monozygotic iris pairs, outperforming human accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the untackled problem of distinguishing monozygotic iris pairs in biometric recognition, leveraging human-like detection capabilities.

Method: Uses a Siamese network with contrastive learning on a dataset of synthetic and natural monozygotic pairs, comparing full, iris-only, and non-iris-only images.

Result: Achieves higher accuracy than human classification, with both iris texture and surrounding ocular structure contributing to the model's performance.

Conclusion: The classifier is effective for distinguishing monozygotic iris pairs, with potential applications like noninvasive twin zygosity testing.

Abstract: This study presents the first automated classifier designed to determine
whether a pair of iris images originates from monozygotic individuals,
addressing a previously untackled problem in biometric recognition. In
Daugman-style iris recognition, the textures of the left and right irises of
the same person are traditionally considered as being as different as the
irises of two unrelated persons. However, previous research indicates that
humans can detect that two iris images are from different eyes of the same
person, or eyes of monozygotic twins, with an accuracy of about 80%. In this
work, we employ a Siamese network architecture and contrastive learning to
categorize a pair of iris images as coming from monozygotic or non-monozygotic
irises. This could potentially be applied, for example, as a fast, noninvasive
test to determine if twins are monozygotic or non-monozygotic. We construct a
dataset comprising both synthetic monozygotic pairs (images of different irises
of the same individual) and natural monozygotic pairs (images of different
images from persons who are identical twins), in addition to non-monozygotic
pairs from unrelated individuals, ensuring a comprehensive evaluation of the
model's capabilities. To gain deeper insights into the learned representations,
we train and analyze three variants of the model using (1) the original input
images, (2) iris-only images (masking everything but the iris region), and (3)
non-iris-only images (masking the iris region). This comparison reveals that
both iris texture and surrounding ocular structure contain information useful
for the model to classify the image pairs as monozygotic or non-monozygotic.
Our approach achieves accuracy levels using the full iris image that exceed
those previously reported for human classification of monozygotic iris pairs.

</details>


### [137] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/pdf/2506.18023)
*Kui Huang, Xinrong Chen, Wenyu Lv, Jincheng Liao, Guanzhong Wang, Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2 improves multimodal document understanding with enhanced data quality, feature fusion, and inference optimization, achieving 11.4% better performance and 73% lower latency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of PP-DocBee by improving synthetic data quality, visual feature fusion, and inference efficiency for better multimodal document understanding.

Method: Uses a large-scale multimodal pre-trained model for data quality optimization, decomposes ViT layers for enhanced feature fusion, and optimizes inference methodologies.

Result: 11.4% performance boost on Chinese business documents and 73.0% reduction in inference latency compared to the vanilla version.

Conclusion: PP-DocBee2 advances multimodal document understanding with key innovations in data quality and feature fusion, offering significant performance and efficiency gains.

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [138] [LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation](https://arxiv.org/pdf/2503.19777)
*Vladan Stojnić, Yannis Kalantidis, Jiří Matas, Giorgos Tolias*

Main category: cs.CV

TL;DR: A training-free method for open-vocabulary semantic segmentation using VLMs and VMs, enhanced by label propagation and pixel-level refinement, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve segmentation accuracy by leveraging VLMs for cross-modal alignment and VMs for intra-modal similarity, addressing resolution limitations of patch-based encoders.

Method: Combines VLMs and VMs with label propagation for joint optimization, applies pixel-level refinement, and avoids window-based processing for full-image context.

Result: LPOSS+ achieves state-of-the-art performance among training-free methods across diverse datasets.

Conclusion: The proposed method effectively enhances segmentation accuracy, especially near class boundaries, without requiring training.

Abstract: We propose a training-free method for open-vocabulary semantic segmentation
using Vision-and-Language Models (VLMs). Our approach enhances the initial
per-patch predictions of VLMs through label propagation, which jointly
optimizes predictions by incorporating patch-to-patch relationships. Since VLMs
are primarily optimized for cross-modal alignment and not for intra-modal
similarity, we use a Vision Model (VM) that is observed to better capture these
relationships. We address resolution limitations inherent to patch-based
encoders by applying label propagation at the pixel level as a refinement step,
significantly improving segmentation accuracy near class boundaries. Our
method, called LPOSS+, performs inference over the entire image, avoiding
window-based processing and thereby capturing contextual interactions across
the full image. LPOSS+ achieves state-of-the-art performance among
training-free methods, across a diverse set of datasets. Code:
https://github.com/vladan-stojnic/LPOSS

</details>


### [139] [Shape and Texture Recognition in Large Vision-Language Models](https://arxiv.org/pdf/2503.23062)
*Sagi Eppel, Mor Bismut, Alona Faktor-Strugatski*

Main category: cs.CV

TL;DR: The paper introduces the LAS&T dataset to benchmark LVLMs' understanding of shapes and textures, revealing their limitations compared to humans and simple nets.


<details>
  <summary>Details</summary>
Motivation: To assess how well LVLMs comprehend fundamental visual concepts like shapes and textures, which are crucial for general visual understanding.

Method: Created the LAS&T dataset via unsupervised extraction from natural images and tested LVLMs on shape and texture recognition tasks.

Result: LVLMs underperform humans in shape recognition and simpler 2D textures but approach human-level performance in 3D material recognition. Simple nets outperform LVLMs.

Conclusion: Leading LVLMs have significant deficiencies in understanding basic visual concepts, highlighting a gap in their foundational visual understanding.

Abstract: Shapes and textures are the basic building blocks of visual perception. The
ability to identify shapes regardless of orientation, texture, or context, and
to recognize textures and materials independently of their associated objects,
is essential for a general visual understanding of the world. This work
introduces the Large Shape and Textures dataset (LAS&T), a giant collection of
highly diverse shapes and textures, created by unsupervised extraction of
patterns from natural images. This dataset is used to benchmark how effectively
leading Large Vision-Language Models (LVLMs) understand shapes, textures, and
materials in 2D and 3D scenes. For shape recognition, we test the models'
ability to match images of identical shapes that differ in orientation,
texture, color, or environment. Our results show that the shape recognition
capabilities of the LVLMs remain significantly below human performance. LVLMs
rely predominantly on high-level and semantic features and struggle with
abstract shapes lacking clear class associations. For texture and material
recognition, we evaluated the models' ability to identify images with identical
textures and materials across different objects and environments.
Interestingly, leading LVLMs approach human-level performance in recognizing
materials in 3D scenes, yet substantially underperform humans when identifying
simpler more abstract 2D textures. These results are consistent across a wide
range of leading VLMs (GPT/Gemini/LLama/Qwen) and foundation vision models
(DINO/CLIP), exposing major deficiencies in the ability of leading models to
understand fundamental visual concepts. In contrast, simple nets trained
directly for these tasks achieve high accuracy. The LAS&T dataset, featuring
over 600,000 images for 2D/3D shape, texture, and material recognition and
retrieval, is publicly available.

</details>


### [140] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/pdf/2506.18871)
*Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2 is an open-source generative model for diverse tasks like text-to-image, image editing, and in-context generation, featuring dual decoding pathways and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To unify diverse generation tasks while preserving text generation capabilities and improving upon OmniGen v1.

Method: Uses two decoding pathways (text/image) with unshared parameters, a decoupled image tokenizer, and introduces reflection mechanisms and new datasets.

Result: Achieves competitive results on benchmarks like text-to-image and image editing, and state-of-the-art in-context generation on OmniContext.

Conclusion: OmniGen2 is a versatile, high-performing model with released resources to support future research.

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [141] [Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset](https://arxiv.org/pdf/2504.04494)
*Marin Benčević, Robert Šojo, Irena Galić*

Main category: cs.CV

TL;DR: Evaluation of skin color measurement methods in dermatoscopic images using a synthetic dataset (S-SYNTH) to assess robustness under varying lighting conditions. Segmentation-based and color quantization methods perform best, while patch-based methods require calibration. Neural networks show promise but need further validation.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate skin color measurement methods under controlled conditions, ensuring robustness and invariance to lighting for accurate skin color estimation.

Method: Used a synthetic dataset (S-SYNTH) with controlled variables (melanin, lesion shapes, hair, lighting) to test four approaches: segmentation-based, patch-based, color quantization, and neural networks. Estimated ITA and Fitzpatrick types.

Result: Segmentation-based and color quantization methods were robust and lighting-invariant. Patch-based methods had lighting-dependent biases. Neural networks, with blurring, showed promise but lacked real-world validation.

Conclusion: Practical recommendations for fair and reliable skin color estimation methods, emphasizing the need for lighting-invariant approaches and further validation of neural networks.

Abstract: This paper presents a comprehensive evaluation of skin color measurement
methods from dermatoscopic images using a synthetic dataset (S-SYNTH) with
controlled ground-truth melanin content, lesion shapes, hair models, and 18
distinct lighting conditions. This allows for rigorous assessment of the
robustness and invariance to lighting conditions. We assess four classes of
image colorimetry approaches: segmentation-based, patch-based, color
quantization, and neural networks. We use these methods to estimate the
Individual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic
images. Our results show that segmentation-based and color quantization methods
yield robust, lighting-invariant estimates, whereas patch-based approaches
exhibit significant lighting-dependent biases that require calibration.
Furthermore, neural network models, particularly when combined with heavy
blurring to reduce overfitting, can provide light-invariant Fitzpatrick
predictions, although their generalization to real-world images remains
unverified. We conclude with practical recommendations for designing fair and
reliable skin color estimation methods.

</details>


### [142] [Time-Aware Auto White Balance in Mobile Photography](https://arxiv.org/pdf/2504.05623)
*Mahmoud Afifi, Luxi Zhao, Abhijith Punnappurath, Mohammed A. Abdelsalam, Ran Zhang, Michael S. Brown*

Main category: cs.CV

TL;DR: A lightweight illuminant estimation method for auto white balance (AWB) uses contextual metadata and image colors, achieving results comparable to larger models. A dataset of 3,224 smartphone images with ground-truth and user-preferred illuminants is introduced for validation.


<details>
  <summary>Details</summary>
Motivation: Improving AWB by leveraging contextual metadata (e.g., timestamp, geolocation) and image colors to correct color casts caused by illumination and camera sensitivity.

Method: Proposes a compact model (~5K parameters) incorporating contextual metadata, capture information, and image colors for illuminant estimation.

Result: The method matches or surpasses larger models, demonstrating effectiveness. A dataset with ground-truth and user-preferred illuminants is provided for benchmarking.

Conclusion: The lightweight model successfully integrates contextual metadata for accurate AWB, validated by a comprehensive dataset.

Abstract: Cameras rely on auto white balance (AWB) to correct undesirable color casts
caused by scene illumination and the camera's spectral sensitivity. This is
typically achieved using an illuminant estimator that determines the global
color cast solely from the color information in the camera's raw sensor image.
Mobile devices provide valuable additional metadata-such as capture timestamp
and geolocation-that offers strong contextual clues to help narrow down the
possible illumination solutions. This paper proposes a lightweight illuminant
estimation method that incorporates such contextual metadata, along with
additional capture information and image colors, into a compact model (~5K
parameters), achieving promising results, matching or surpassing larger models.
To validate our method, we introduce a dataset of 3,224 smartphone images with
contextual metadata collected at various times of day and under diverse
lighting conditions. The dataset includes ground-truth illuminant colors,
determined using a color chart, and user-preferred illuminants validated
through a user study, providing a comprehensive benchmark for AWB evaluation.

</details>


### [143] [WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care](https://arxiv.org/pdf/2504.06185)
*Vanessa Borst, Timo Dittus, Tassilo Dege, Astrid Schmieder, Samuel Kounev*

Main category: cs.CV

TL;DR: The paper benchmarks deep learning models for wound segmentation, evaluates their real-world applicability, and introduces a framework for AI-driven wound size estimation in telehealth.


<details>
  <summary>Details</summary>
Motivation: Chronic wounds are a significant health issue, especially for the elderly and diabetic patients, but automated monitoring via semantic segmentation is under-researched in medical imaging.

Method: The study standardizes training, data augmentation, and evaluation of state-of-the-art deep learning models, including cross-validation and real-world deployment tests. It also proposes a reference object-based approach for wound size estimation.

Result: TransNeXt showed the highest generalizability, while VWFormer and ConvNeXtS performed best in mask quality. All models met computational efficiency requirements, and size predictions matched expert annotations.

Conclusion: The AI-driven framework, WoundAmbit, is viable for telehealth integration, offering accurate wound monitoring and reducing the need for in-person visits.

Abstract: Chronic wounds affect a large population, particularly the elderly and
diabetic patients, who often exhibit limited mobility and co-existing health
conditions. Automated wound monitoring via mobile image capture can reduce
in-person physician visits by enabling remote tracking of wound size. Semantic
segmentation is key to this process, yet wound segmentation remains
underrepresented in medical imaging research. To address this, we benchmark
state-of-the-art deep learning models from general-purpose vision, medical
imaging, and top methods from public wound challenges. For a fair comparison,
we standardize training, data augmentation, and evaluation, conducting
cross-validation to minimize partitioning bias. We also assess real-world
deployment aspects, including generalization to an out-of-distribution wound
dataset, computational efficiency, and interpretability. Additionally, we
propose a reference object-based approach to convert AI-generated masks into
clinically relevant wound size estimates and evaluate this, along with mask
quality, for the five best architectures based on physician assessments.
Overall, the transformer-based TransNeXt showed the highest levels of
generalizability. Despite variations in inference times, all models processed
at least one image per second on the CPU, which is deemed adequate for the
intended application. Interpretability analysis typically revealed prominent
activations in wound regions, emphasizing focus on clinically relevant
features. Expert evaluation showed high mask approval for all analyzed models,
with VWFormer and ConvNeXtS backbone performing the best. Size retrieval
accuracy was similar across models, and predictions closely matched expert
annotations. Finally, we demonstrate how our AI-driven wound size estimation
framework, WoundAmbit, is integrated into a custom telehealth system.

</details>


### [144] [TT3D: Table Tennis 3D Reconstruction](https://arxiv.org/pdf/2504.10035)
*Thomas Gossard, Andreas Ziegler, Andreas Zell*

Main category: cs.CV

TL;DR: A novel method for 3D ball trajectory reconstruction in table tennis using physics-based motion analysis and automated camera calibration, enabling spin inference and player movement tracking.


<details>
  <summary>Details</summary>
Motivation: 2D ball tracking in sports broadcasts is limited by camera viewpoint and lacks depth, hindering comprehensive game analysis.

Method: Leverages ball motion physics to minimize reprojection error, uses automated camera calibration, and adapts a 3D pose estimation model for player tracking.

Result: Accurate 3D reconstruction of ball trajectories and spin inference without relying on unreliable human pose or racket tracking.

Conclusion: The approach enables comprehensive 3D analysis of table tennis rallies, overcoming limitations of 2D tracking.

Abstract: Sports analysis requires processing large amounts of data, which is
time-consuming and costly. Advancements in neural networks have significantly
alleviated this burden, enabling highly accurate ball tracking in sports
broadcasts. However, relying solely on 2D ball tracking is limiting, as it
depends on the camera's viewpoint and falls short of supporting comprehensive
game analysis. To address this limitation, we propose a novel approach for
reconstructing precise 3D ball trajectories from online table tennis match
recordings. Our method leverages the underlying physics of the ball's motion to
identify the bounce state that minimizes the reprojection error of the ball's
flying trajectory, hence ensuring an accurate and reliable 3D reconstruction. A
key advantage of our approach is its ability to infer ball spin without relying
on human pose estimation or racket tracking, which are often unreliable or
unavailable in broadcast footage. We developed an automated camera calibration
method capable of reliably tracking camera movements. Additionally, we adapted
an existing 3D pose estimation model, which lacks depth motion capture, to
accurately track player movements. Together, these contributions enable the
full 3D reconstruction of a table tennis rally.

</details>


### [145] [Visual and Textual Prompts in VLLMs for Enhancing Emotion Recognition](https://arxiv.org/pdf/2504.17224)
*Zhifeng Wang, Qixuan Zhang, Peter Zhang, Wenjia Niu, Kaihao Zhang, Ramesh Sankaranarayana, Sabrina Caldwell, Tom Gedeon*

Main category: cs.CV

TL;DR: SoVTP enhances VLLMs for video emotion recognition by integrating spatial, physiological, and contextual cues into a unified prompting framework, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current VLLMs lack spatial and contextual awareness for video emotion recognition, often ignoring non-verbal cues like body language and social interactions.

Method: Proposes Set-of-Vision-Text Prompting (SoVTP), combining spatial annotations, physiological signals, and contextual cues into a unified prompting strategy.

Result: SoVTP significantly improves zero-shot emotion recognition, outperforming existing visual prompting methods.

Conclusion: SoVTP effectively enhances VLLMs' video emotion recognition by preserving holistic scene information and enabling fine-grained analysis.

Abstract: Vision Large Language Models (VLLMs) exhibit promising potential for
multi-modal understanding, yet their application to video-based emotion
recognition remains limited by insufficient spatial and contextual awareness.
Traditional approaches, which prioritize isolated facial features, often
neglect critical non-verbal cues such as body language, environmental context,
and social interactions, leading to reduced robustness in real-world scenarios.
To address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel
framework that enhances zero-shot emotion recognition by integrating spatial
annotations (e.g., bounding boxes, facial landmarks), physiological signals
(facial action units), and contextual cues (body posture, scene dynamics,
others' emotions) into a unified prompting strategy. SoVTP preserves holistic
scene information while enabling fine-grained analysis of facial muscle
movements and interpersonal dynamics. Extensive experiments show that SoVTP
achieves substantial improvements over existing visual prompting methods,
demonstrating its effectiveness in enhancing VLLMs' video emotion recognition
capabilities.

</details>


### [146] [VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/pdf/2505.12434)
*Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, Tianfei Zhou*

Main category: cs.CV

TL;DR: VIDEORFT extends reinforcement fine-tuning (RFT) to MLLMs for video reasoning, using a two-stage approach (SFT + RL) and a novel CoT curation pipeline to address dataset scarcity. It achieves SOTA results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video reasoning remains challenging due to complex logic and temporal structures. Existing RFT methods lack large-scale video CoT datasets.

Method: Two-stage RFT: SFT with CoT annotations, followed by RL. Introduces an automatic CoT curation pipeline and a semantic-consistency reward for RL.

Result: VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.

Conclusion: VIDEORFT successfully extends RFT to video reasoning, addressing dataset scarcity and improving generalization, setting a new benchmark for MLLMs.

Abstract: Reinforcement fine-tuning (RFT) has shown great promise in achieving
humanlevel reasoning capabilities of Large Language Models (LLMs), and has
recently been extended to MLLMs. Nevertheless, reasoning about videos, which is
a fundamental aspect of human intelligence, remains a persistent challenge due
to the complex logic, temporal and causal structures inherent in video data. To
fill this gap, we propose VIDEORFT, a novel approach that extends the RFT
paradigm to cultivate human-like video reasoning capabilities in MLLMs.
VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning
(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement
learning (RL) to improve generalization. A central challenge to achieve this in
the video domain lies in the scarcity of large-scale, high-quality video CoT
datasets. We address this by building a fully automatic CoT curation pipeline.
First, we devise a cognitioninspired prompting strategy to elicit a reasoning
LLM to generate preliminary CoTs based solely on rich, structured, and literal
representations of video content. Subsequently, these CoTs are revised by a
visual-language model conditioned on the actual video, ensuring visual
consistency and reducing visual hallucinations. This pipeline results in two
new datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To
further strengthen the RL phase, we introduce a novel semantic-consistency
reward that explicitly promotes the alignment between textual reasoning and
visual evidence. This reward encourages the model to produce coherent,
context-aware reasoning outputs grounded in visual input. Extensive experiments
show that VIDEORFT achieves state-of-the-art performance on six video reasoning
benchmarks.

</details>


### [147] [It's not you, it's me -- Global urban visual perception varies across demographics and personalities](https://arxiv.org/pdf/2505.12758)
*Matias Quintana, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Filip Biljecki*

Main category: cs.CV

TL;DR: The paper introduces the SPECS dataset to analyze urban streetscape perceptions, highlighting demographic and personality influences on preferences, and reveals biases in existing machine learning models.


<details>
  <summary>Details</summary>
Motivation: Current urban planning approaches often overlook demographic and personality differences, risking biased decisions. This study aims to address this gap by examining how demographics and traits shape perceptions.

Method: A large-scale survey of 1,000 participants from diverse backgrounds evaluated streetscapes using street view imagery, measuring perceptions across traditional and new indicators.

Result: Significant differences in perception scores were found among demographics and personalities. Machine learning models overestimated positive and underestimated negative indicators compared to human responses.

Conclusion: Urban planning should incorporate local demographic and personality insights to avoid biases and improve decision-making.

Abstract: Understanding people's preferences and needs is crucial for urban planning
decisions, yet current approaches often combine them from multi-cultural and
multi-city populations, obscuring important demographic differences and risking
amplifying biases. We conducted a large-scale urban visual perception survey of
streetscapes worldwide using street view imagery, examining how demographics --
including gender, age, income, education, race and ethnicity, and, for the
first time, personality traits -- shape perceptions among 1,000 participants,
with balanced demographics, from five countries and 45 nationalities. This
dataset, introduced as Street Perception Evaluation Considering Socioeconomics
(SPECS), exhibits statistically significant differences in perception scores in
six traditionally used indicators (safe, lively, wealthy, beautiful, boring,
and depressing) and four new ones we propose (live nearby, walk, cycle, green)
among demographics and personalities. We revealed that location-based
sentiments are carried over in people's preferences when comparing urban
streetscapes with other cities. Further, we compared the perception scores
based on where participants and streetscapes are from. We found that an
off-the-shelf machine learning model trained on an existing global perception
dataset tends to overestimate positive indicators and underestimate negative
ones compared to human responses, suggesting that targeted intervention should
consider locals' perception. Our study aspires to rectify the myopic treatment
of street perception, which rarely considers demographics or personality
traits.

</details>


### [148] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/pdf/2505.17333)
*Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab*

Main category: cs.CV

TL;DR: The paper introduces an image-to-video (I2V) framework to simulate regular respiratory motion, addressing limitations of existing methods by using temporal differential fields for consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to simulate temporal motions without high-dose scans, and patient movement introduces deviations that affect modeling.

Method: Proposes an I2V synthesis framework with a Temporal Differential Diffusion Model to generate differential fields for temporal consistency.

Result: Outperforms competitors on perceptual similarity and temporal consistency in 4D video simulation.

Conclusion: The approach effectively models respiratory motion, offering improved accuracy and consistency in synthesized videos.

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>


### [149] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/pdf/2505.21381)
*Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba improves PointMamba by introducing a zigzag scan path for spatial continuity and a Semantic-Siamese Masking Strategy (SMS) for better local semantic modeling, outperforming in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PointMamba-based methods disrupt spatial continuity and local semantic correlations due to complex token ordering and random masking.

Method: Proposes ZigzagPointMamba with a zigzag scan path for global sequencing and SMS for masking semantically similar tokens to enhance local semantic modeling.

Result: Achieves significant improvements: 1.59% mIoU gain on ShapeNetPart, 0.4% higher accuracy on ModelNet40, and better accuracies on ScanObjectNN subsets.

Conclusion: ZigzagPointMamba effectively addresses spatial and semantic challenges, enhancing performance in point cloud self-supervised learning and downstream tasks.

Abstract: State Space models (SSMs) such as PointMamba enable efficient feature
extraction for point cloud self-supervised learning with linear complexity,
outperforming Transformers in computational efficiency. However, existing
PointMamba-based methods depend on complex token ordering and random masking,
which disrupt spatial continuity and local semantic correlations. We propose
ZigzagPointMamba to tackle these challenges. The core of our approach is a
simple zigzag scan path that globally sequences point cloud tokens, enhancing
spatial continuity by preserving the proximity of spatially adjacent point
tokens. Nevertheless, random masking undermines local semantic modeling in
self-supervised learning. To address this, we introduce a Semantic-Siamese
Masking Strategy (SMS), which masks semantically similar tokens to facilitate
reconstruction by integrating local features of original and similar tokens.
This overcomes the dependence on isolated local features and enables robust
global semantic modeling. Our pre-trained ZigzagPointMamba weights
significantly improve downstream tasks, achieving a 1.59% mIoU gain on
ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for
classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for
the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of
ScanObjectNN.

</details>


### [150] [PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms](https://arxiv.org/pdf/2505.22016)
*Yifei Xia, Shuchen Weng, Siqi Yang, Jingqi Liu, Chengxuan Zhu, Minggui Teng, Zijian Jia, Han Jiang, Boxin Shi*

Main category: cs.CV

TL;DR: PanoWan lifts pre-trained text-to-video models to the panoramic domain with minimal modules, addressing distortion and boundary issues, and introduces PanoVid dataset for training.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to leverage pre-trained generative priors for high-quality panoramic videos due to dataset limitations and spatial feature gaps.

Method: PanoWan uses latitude-aware sampling, rotated semantic denoising, and padded pixel-wise decoding to address distortion and boundary transitions.

Result: PanoWan achieves state-of-the-art performance in panoramic video generation and robustness in zero-shot tasks.

Conclusion: PanoWan effectively bridges the gap between conventional text-to-video models and panoramic video generation, supported by the PanoVid dataset.

Abstract: Panoramic video generation enables immersive 360{\deg} content creation,
valuable in applications that demand scene-consistent world exploration.
However, existing panoramic video generation models struggle to leverage
pre-trained generative priors from conventional text-to-video models for
high-quality and diverse panoramic videos generation, due to limited dataset
scale and the gap in spatial feature representations. In this paper, we
introduce PanoWan to effectively lift pre-trained text-to-video models to the
panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware
sampling to avoid latitudinal distortion, while its rotated semantic denoising
and padded pixel-wise decoding ensure seamless transitions at longitude
boundaries. To provide sufficient panoramic videos for learning these lifted
representations, we contribute PanoVid, a high-quality panoramic video dataset
with captions and diverse scenarios. Consequently, PanoWan achieves
state-of-the-art performance in panoramic video generation and demonstrates
robustness for zero-shot downstream tasks. Our project page is available at
https://panowan.variantconst.com.

</details>


### [151] [ViStoryBench: Comprehensive Benchmark Suite for Story Visualization](https://arxiv.org/pdf/2505.24862)
*Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, Chi Zhang*

Main category: cs.CV

TL;DR: ViStoryBench is introduced as a comprehensive evaluation benchmark for story visualization models, featuring diverse story types and artistic styles to test model performance across various dimensions.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of story visualization frameworks in real-world scenarios by providing a structured and multifaceted evaluation framework.

Method: Collecting a diverse dataset with varied story types, artistic styles, and narrative structures, including single and multiple protagonists, complex plots, and intricate world-building.

Result: ViStoryBench enables thorough identification of model strengths and weaknesses through a wide range of evaluation metrics.

Conclusion: The benchmark fosters targeted improvements in story visualization models by providing a comprehensive and balanced evaluation framework.

Abstract: Story visualization, which aims to generate a sequence of visually coherent
images aligning with a given narrative and reference images, has seen
significant progress with recent advancements in generative models. To further
enhance the performance of story visualization frameworks in real-world
scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We
collect a diverse dataset encompassing various story types and artistic styles,
ensuring models are evaluated across multiple dimensions such as different
plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D
renderings). ViStoryBench is carefully curated to balance narrative structures
and visual elements, featuring stories with single and multiple protagonists to
test models' ability to maintain character consistency. Additionally, it
includes complex plots and intricate world-building to challenge models in
generating accurate visuals. To ensure comprehensive comparisons, our benchmark
incorporates a wide range of evaluation metrics assessing critical aspects.
This structured and multifaceted framework enables researchers to thoroughly
identify both the strengths and weaknesses of different models, fostering
targeted improvements.

</details>


### [152] [TIIF-Bench: How Does Your T2I Model Follow Your Instructions?](https://arxiv.org/pdf/2506.02161)
*Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, Lei Zhang*

Main category: cs.CV

TL;DR: TIIF-Bench is a new benchmark for evaluating Text-to-Image (T2I) models, addressing limitations in prompt diversity, complexity, and evaluation metrics. It includes 5000 prompts with varying difficulties, dual-length versions, and focuses on text rendering and style control. A novel computable framework assesses model outputs, revealing current T2I model limitations.


<details>
  <summary>Details</summary>
Motivation: Existing T2I evaluation benchmarks lack prompt diversity, complexity, and fine-grained metrics, hindering accurate assessment of model alignment with textual instructions.

Method: TIIF-Bench introduces 5000 prompts categorized by difficulty, with short and long versions for each. It evaluates text rendering, style control, and uses a computable framework based on large vision language models.

Result: The benchmark rigorously assesses mainstream T2I models, highlighting their strengths and weaknesses, and exposes limitations in current benchmarks.

Conclusion: TIIF-Bench provides a systematic and detailed evaluation tool for T2I models, improving the understanding of their capabilities and limitations.

Abstract: The rapid advancements of Text-to-Image (T2I) models have ushered in a new
phase of AI-generated content, marked by their growing ability to interpret and
follow user instructions. However, existing T2I model evaluation benchmarks
fall short in limited prompt diversity and complexity, as well as coarse
evaluation metrics, making it difficult to evaluate the fine-grained alignment
performance between textual instructions and generated images. In this paper,
we present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming
to systematically assess T2I models' ability in interpreting and following
intricate textual instructions. TIIF-Bench comprises a set of 5000 prompts
organized along multiple dimensions, which are categorized into three levels of
difficulties and complexities. To rigorously evaluate model robustness to
varying prompt lengths, we provide a short and a long version for each prompt
with identical core semantics. Two critical attributes, i.e., text rendering
and style control, are introduced to evaluate the precision of text synthesis
and the aesthetic coherence of T2I models. In addition, we collect 100
high-quality designer level prompts that encompass various scenarios to
comprehensively assess model performance. Leveraging the world knowledge
encoded in large vision language models, we propose a novel computable
framework to discern subtle variations in T2I model outputs. Through meticulous
benchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and
cons of current T2I models and reveal the limitations of current T2I
benchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/.

</details>


### [153] [Dark Channel-Assisted Depth-from-Defocus from a Single Image](https://arxiv.org/pdf/2506.06643)
*Moushumi Medhi, Rajiv Ranjan Sahay*

Main category: cs.CV

TL;DR: The paper proposes a method for estimating scene depth from a single defocus-blurred image using the dark channel as a cue, improving upon traditional multi-image DFD approaches.


<details>
  <summary>Details</summary>
Motivation: Single-image depth-from-defocus (DFD) is challenging due to its underconstrained nature, and existing methods often require multiple images. This work aims to address this gap by leveraging the dark channel prior.

Method: The method uses local defocus blur and contrast variations as depth cues, incorporating the dark channel prior. The pipeline is trained end-to-end with adversarial learning.

Result: Experiments on real data show that the approach provides meaningful depth estimation, validating the use of the dark channel prior in single-image DFD.

Conclusion: The paper demonstrates that the dark channel prior can effectively enhance single-image DFD, offering a viable solution to an underexplored problem.

Abstract: We estimate scene depth from a single defocus-blurred image using the dark
channel as a complementary cue, leveraging its ability to capture local
statistics and scene structure. Traditional depth-from-defocus (DFD) methods
use multiple images with varying apertures or focus. Single-image DFD is
underexplored due to its inherent challenges. Few attempts have focused on
depth-from-defocus (DFD) from a single defocused image because the problem is
underconstrained. Our method uses the relationship between local defocus blur
and contrast variations as depth cues to improve scene structure estimation.
The pipeline is trained end-to-end with adversarial learning. Experiments on
real data demonstrate that incorporating the dark channel prior into
single-image DFD provides meaningful depth estimation, validating our approach.

</details>


### [154] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/pdf/2506.07368)
*Jiaying He, Yitong Lin, Jiahe Chen, Honghui Xu, Jianwei Zheng*

Main category: cs.CV

TL;DR: C3S3 is a novel semi-supervised medical image segmentation model that improves boundary delineation and precision by integrating complementary competition and contrastive selection, outperforming existing methods by at least 6% on key metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of insufficient annotated medical samples and the inability of current methods to capture subtle boundary details, which leads to diagnostic inaccuracies.

Method: Introduces C3S3 with an Outcome-Driven Contrastive Learning module for boundary refinement and a Dynamic Complementary Competition module for pseudo-label generation using two sub-networks.

Result: Validated on MRI and CT datasets, C3S3 achieves superior performance, with at least 6% improvement on 95HD and ASD metrics.

Conclusion: C3S3 significantly advances semi-supervised medical image segmentation, particularly in boundary precision, and is publicly available for further use.

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
Outcome-Driven Contrastive Learning module dedicated to refining boundary
localization. Additionally, we incorporate a Dynamic Complementary Competition
module that leverages two high-performing sub-networks to generate
pseudo-labels, thereby further improving segmentation quality. The proposed
C3S3 undergoes rigorous validation on two publicly accessible datasets,
encompassing the practices of both MRI and CT scans. The results demonstrate
that our method achieves superior performance compared to previous cutting-edge
competitors. Especially, on the 95HD and ASD metrics, our approach achieves a
notable improvement of at least 6%, highlighting the significant advancements.
The code is available at https://github.com/Y-TARL/C3S3.

</details>


### [155] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/pdf/2506.09229)
*Sungwon Hwang, Hyojin Jang, Kinam Kim, Minho Park, Jaegul Choo*

Main category: cs.CV

TL;DR: The paper introduces CREPA, a method to improve video diffusion model fine-tuning by aligning hidden states across frames for better semantic consistency and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning video diffusion models for user-specific attributes is challenging and underexplored, despite its practical importance.

Method: Proposes Cross-frame Representation Alignment (CREPA), a regularization technique aligning hidden states of a frame with external features from neighboring frames.

Result: CREPA improves visual fidelity and cross-frame semantic coherence in large-scale VDMs like CogVideoX-5B and Hunyuan Video.

Conclusion: CREPA is broadly applicable and effective for enhancing video diffusion model fine-tuning.

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.

</details>


### [156] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/pdf/2506.10978)
*Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces 'HeadHunter' and 'SoftPAG' for fine-grained control in diffusion models by perturbing specific attention heads, improving generation quality and visual attributes.


<details>
  <summary>Details</summary>
Motivation: Existing attention perturbation methods lack principled approaches for determining perturbation locations, especially in Diffusion Transformer architectures.

Method: Investigates granularity of attention perturbations, proposes 'HeadHunter' for selecting attention heads and 'SoftPAG' for tuning perturbation strength.

Result: Demonstrates superior performance in quality enhancement and style-specific guidance on models like Stable Diffusion 3 and FLUX.1.

Conclusion: Provides the first head-level analysis of attention perturbation, enabling interpretable specialization and practical perturbation strategies.

Abstract: Recent guidance methods in diffusion models steer reverse sampling by
perturbing the model to construct an implicit weak model and guide generation
away from it. Among these approaches, attention perturbation has demonstrated
strong empirical performance in unconditional scenarios where classifier-free
guidance is not applicable. However, existing attention perturbation methods
lack principled approaches for determining where perturbations should be
applied, particularly in Diffusion Transformer (DiT) architectures where
quality-relevant computations are distributed across layers. In this paper, we
investigate the granularity of attention perturbations, ranging from the layer
level down to individual attention heads, and discover that specific heads
govern distinct visual concepts such as structure, style, and texture quality.
Building on this insight, we propose "HeadHunter", a systematic framework for
iteratively selecting attention heads that align with user-centric objectives,
enabling fine-grained control over generation quality and visual attributes. In
addition, we introduce SoftPAG, which linearly interpolates each selected
head's attention map toward an identity matrix, providing a continuous knob to
tune perturbation strength and suppress artifacts. Our approach not only
mitigates the oversmoothing issues of existing layer-level perturbation but
also enables targeted manipulation of specific visual styles through
compositional head selection. We validate our method on modern large-scale
DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,
demonstrating superior performance in both general quality enhancement and
style-specific guidance. Our work provides the first head-level analysis of
attention perturbation in diffusion models, uncovering interpretable
specialization within attention layers and enabling practical design of
effective perturbation strategies.

</details>


### [157] [CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation](https://arxiv.org/pdf/2506.15549)
*Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Shahnaz Jamil-Copley, Richard H. Clayton, Chen Chen*

Main category: cs.CV

TL;DR: CLAIM is a framework for generating realistic and diverse myocardial scar images from LGE cardiac MRI using clinical guidance, improving scar segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Limited availability and variability of high-quality labeled LGE images hinder robust scar segmentation model development.

Method: CLAIM uses the SMILE module (diffusion-based generator guided by clinical AHA 17-segment model) and joint training of scar synthesis and segmentation networks.

Result: CLAIM produces anatomically coherent scars and outperforms baselines in Dice similarity with real scar distributions.

Conclusion: CLAIM enables controllable, realistic scar synthesis and enhances downstream medical imaging tasks.

Abstract: Deep learning-based myocardial scar segmentation from late gadolinium
enhancement (LGE) cardiac MRI has shown great potential for accurate and timely
diagnosis and treatment planning for structural cardiac diseases. However, the
limited availability and variability of LGE images with high-quality scar
labels restrict the development of robust segmentation models. To address this,
we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE
\textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial
Scar Synthesis and Segmentation framework, a framework for anatomically
grounded scar generation and segmentation. At its core is the SMILE module
(Scar Mask generation guided by cLinical knowledgE), which conditions a
diffusion-based generator on the clinically adopted AHA 17-segment model to
synthesize images with anatomically consistent and spatially diverse scar
patterns. In addition, CLAIM employs a joint training strategy in which the
scar segmentation network is optimized alongside the generator, aiming to
enhance both the realism of synthesized scars and the accuracy of the scar
segmentation performance. Experimental results show that CLAIM produces
anatomically coherent scar patterns and achieves higher Dice similarity with
real scar distributions compared to baseline models. Our approach enables
controllable and realistic myocardial scar synthesis and has demonstrated
utility for downstream medical imaging task. Code is available at
https://github.com/farheenjabeen/CLAIM-Scar-Synthesis.

</details>


### [158] [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)
*Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao*

Main category: cs.CV

TL;DR: VLN-R1 is an end-to-end framework using Large Vision-Language Models (LVLM) for continuous navigation, trained with GRPO-based methods and a two-stage approach (SFT and RFT). It outperforms on VLN-CE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLN systems rely on discrete topological graphs, limiting flexibility. VLN-R1 aims to enable continuous navigation using LVLMs for better adaptability and performance.

Method: VLN-R1 uses LVLMs to translate egocentric video into actions. Training involves supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with Time-Decayed Reward (TDR). The VLN-Ego dataset and Long-Short Memory Sampling are introduced.

Result: VLN-R1 achieves strong performance on the VLN-CE benchmark, demonstrating LVLMs' capability for embodied navigation.

Conclusion: VLN-R1 shows LVLMs can enhance embodied navigation through data-efficient, reward-driven training, improving task-specific reasoning.

Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI,
requiring agents to navigate real-world environments using natural language
instructions. Current language model-based navigation systems operate on
discrete topological graphs, limiting path planning to predefined node
connections. We propose VLN-R1, an end-to-end framework that leverages Large
Vision-Language Models (LVLM) to directly translate egocentric video streams
into continuous navigation actions, adopting GRPO-based training inspired by
DeepSeek-R1. To enable effective training, we first construct the VLN-Ego
dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling
to balance historical and current observations. While large language models can
supervise complete textual instructions, they lack fine-grained action-level
control. Our framework employs a two-stage training approach: a) Supervised
fine-tuning (SFT) to align the model's action sequence text predictions with
expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced
with a Time-Decayed Reward (TDR) mechanism that strategically weights
multi-step future actions. Experimental results show VLN-R1 achieves strong
performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied
navigation and enhance task-specific reasoning through data-efficient,
reward-driven post-training.

</details>


### [159] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/pdf/2506.17892)
*Jianghong Huang, Luping Ji, Xin Ma, Mao Ye*

Main category: cs.CV

TL;DR: The paper addresses the lack of real-world industrial belt crack datasets and proposes a baseline method for intelligent crack detection using triple-domain feature fusion.


<details>
  <summary>Details</summary>
Motivation: Conveyor belt cracks threaten operational efficiency and safety, but existing datasets focus on pavement or synthetic data, lacking real-world industrial examples.

Method: A baseline method with triple-domain (time-space-frequency) feature hierarchical fusion learning is proposed for crack detection.

Result: Experiments confirm the dataset's availability and effectiveness, with the baseline outperforming similar detection methods.

Conclusion: The work provides a valuable dataset and a superior baseline method for industrial belt crack detection.

Abstract: Conveyor belts are important equipment in modern industry, widely applied in
production and manufacturing. Their health is much critical to operational
efficiency and safety. Cracks are a major threat to belt health. Currently,
considering safety, how to intelligently detect belt cracks is catching an
increasing attention. To implement the intelligent detection with machine
learning, real crack samples are believed to be necessary. However, existing
crack datasets primarily focus on pavement scenarios or synthetic data, no
real-world industrial belt crack datasets at all. Cracks are a major threat to
belt health. Furthermore, to validate usability and effectiveness, we propose a
special baseline method with triple-domain ($i.e.$, time-space-frequency)
feature hierarchical fusion learning for the two whole-new datasets.
Experimental results demonstrate the availability and effectiveness of our
dataset. Besides, they also show that our baseline is obviously superior to
other similar detection methods. Our datasets and source codes are available at
https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [160] [EvDetMAV: Generalized MAV Detection from Moving Event Cameras](https://arxiv.org/pdf/2506.19416)
*Yin Zhang, Zian Ning, Xiaoyu Zhang, Shiliang Guo, Peidong Liu, Shiyu Zhao*

Main category: cs.CV

TL;DR: The paper proposes a method for detecting micro aerial vehicles (MAVs) using event cameras by focusing on propeller features in event streams, outperforming RGB-based methods without training.


<details>
  <summary>Details</summary>
Motivation: Existing MAV detection methods rely on RGB images, which lack generalized detection due to diverse appearances. Event streams capture propeller features better.

Method: Three modules extract propeller features from event streams while filtering noise. A novel event-based MAV dataset is introduced.

Result: The method achieves 83.0% precision (+30.3%) and 81.5% recall (+36.4%) on the new dataset, outperforming state-of-the-art methods.

Conclusion: Event-based detection is effective for MAVs, and the introduced dataset supports further research.

Abstract: Existing micro aerial vehicle (MAV) detection methods mainly rely on the
target's appearance features in RGB images, whose diversity makes it difficult
to achieve generalized MAV detection. We notice that different types of MAVs
share the same distinctive features in event streams due to their high-speed
rotating propellers, which are hard to see in RGB images. This paper studies
how to detect different types of MAVs from an event camera by fully exploiting
the features of propellers in the original event stream. The proposed method
consists of three modules to extract the salient and spatio-temporal features
of the propellers while filtering out noise from background objects and camera
motion. Since there are no existing event-based MAV datasets, we introduce a
novel MAV dataset for the community. This is the first event-based MAV dataset
comprising multiple scenarios and different types of MAVs. Without training,
our method significantly outperforms state-of-the-art methods and can deal with
challenging scenarios, achieving a precision rate of 83.0\% (+30.3\%) and a
recall rate of 81.5\% (+36.4\%) on the proposed testing dataset. The dataset
and code are available at: https://github.com/WindyLab/EvDetMAV.

</details>


### [161] [Sampling Matters in Explanations: Towards Trustworthy Attribution Analysis Building Block in Visual Models through Maximizing Explanation Certainty](https://arxiv.org/pdf/2506.19442)
*Róisín Luo, James McDermott, Colm O'Riordan*

Main category: cs.CV

TL;DR: The paper addresses the misalignment of sample distribution in gradient integration for image attribution analysis, proposing a semi-optimal sampling approach by suppressing features to improve explanation certainty.


<details>
  <summary>Details</summary>
Motivation: Prior methods add noise to images for gradient integration, leading to low explanation certainty due to misalignment with natural image distribution.

Method: A semi-optimal sampling approach suppresses features from inputs to align sample distribution with natural images, avoiding noise addition.

Result: Extensive evaluation on ImageNet shows the approach outperforms state-of-the-art baselines, yielding more satisfactory explanations.

Conclusion: Trustworthy attribution analysis requires addressing sample distribution misalignment; suppressing features provides a more effective solution than noise addition.

Abstract: Image attribution analysis seeks to highlight the feature representations
learned by visual models such that the highlighted feature maps can reflect the
pixel-wise importance of inputs. Gradient integration is a building block in
the attribution analysis by integrating the gradients from multiple derived
samples to highlight the semantic features relevant to inferences. Such a
building block often combines with other information from visual models such as
activation or attention maps to form ultimate explanations. Yet, our
theoretical analysis demonstrates that the extent to the alignment of the
sample distribution in gradient integration with respect to natural image
distribution gives a lower bound of explanation certainty. Prior works add
noise into images as samples and the noise distributions can lead to low
explanation certainty. Counter-intuitively, our experiment shows that extra
information can saturate neural networks. To this end, building trustworthy
attribution analysis needs to settle the sample distribution misalignment
problem. Instead of adding extra information into input images, we present a
semi-optimal sampling approach by suppressing features from inputs. The sample
distribution by suppressing features is approximately identical to the
distribution of natural images. Our extensive quantitative evaluation on large
scale dataset ImageNet affirms that our approach is effective and able to yield
more satisfactory explanations against state-of-the-art baselines throughout
all experimental models.

</details>


### [162] [Self-Supervised Multimodal NeRF for Autonomous Driving](https://arxiv.org/pdf/2506.19615)
*Gaurav Sharma, Ravi Kothari, Josef Schmid*

Main category: cs.CV

TL;DR: A self-supervised Neural Radiance Fields (NeRF) framework, NVSF, for joint learning of space and time-varying scenes in LiDAR and Camera data, outperforming baselines on KITTI-360.


<details>
  <summary>Details</summary>
Motivation: To address the need for 3D labels in multimodal dynamic NeRFs by proposing a self-supervised approach for autonomous driving scenarios.

Method: Uses heuristic-based image pixel sampling for efficient training and a Double Gradient mask for LiDAR point feature preservation.

Result: Achieves best performance on KITTI-360 dataset for both LiDAR and Camera domains.

Conclusion: NVSF is effective, self-supervised, and outperforms existing methods without requiring 3D labels.

Abstract: In this paper, we propose a Neural Radiance Fields (NeRF) based framework,
referred to as Novel View Synthesis Framework (NVSF). It jointly learns the
implicit neural representation of space and time-varying scene for both LiDAR
and Camera. We test this on a real-world autonomous driving scenario containing
both static and dynamic scenes. Compared to existing multimodal dynamic NeRFs,
our framework is self-supervised, thus eliminating the need for 3D labels. For
efficient training and faster convergence, we introduce heuristic-based image
pixel sampling to focus on pixels with rich information. To preserve the local
features of LiDAR points, a Double Gradient based mask is employed. Extensive
experiments on the KITTI-360 dataset show that, compared to the baseline
models, our framework has reported best performance on both LiDAR and Camera
domain. Code of the model is available at
https://github.com/gaurav00700/Selfsupervised-NVSF

</details>


### [163] [One Prototype Is Enough: Single-Prototype Activation for Interpretable Image Classification](https://arxiv.org/pdf/2506.19808)
*Yitao Peng, Lianghua He, Die Hu*

Main category: cs.CV

TL;DR: ProtoSolo is a deep neural architecture for interpretable image classification using a single prototype per category, reducing explanation complexity. It employs feature-based comparison and non-prototype projection learning for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing prototype networks rely on multiple prototypes for classification, increasing cognitive complexity. ProtoSolo aims to simplify this by using a single prototype.

Method: ProtoSolo uses a single prototype per category, feature map comparisons, and non-prototype projection learning to enhance interpretability and performance.

Result: Experiments on CUB-200-2011 and Stanford Cars datasets show superior classification performance and lower cognitive complexity compared to state-of-the-art methods.

Conclusion: ProtoSolo simplifies interpretable image classification with single prototypes, achieving high performance and reduced explanation complexity.

Abstract: In this paper, we propose ProtoSolo, a novel deep neural architecture for
interpretable image classification inspired by prototypical networks such as
ProtoPNet. Existing prototype networks usually rely on the collaborative
decision-making of multiple prototypes to achieve the classification and
interpretation of a single category. In contrast, ProtoSolo only requires the
activation of a single prototype to complete the classification. This allows
the network to explain each category decision by only providing the features
that are most similar to the prototype of that category, significantly reducing
the cognitive complexity of the explanation. Secondly, we propose a
feature-based comparison method, which uses feature map instead of full-channel
feature vector as the object of similarity comparison and prototype learning.
This design enables ProtoSolo to utilize richer global information for
classification while relying on a single prototype activation. In addition, we
propose a non-prototype projection learning strategy, which preserves the
information association between the prototype and the training image patches
while avoiding the sharp change of the network structure caused by the
projection operation, thus avoiding its negative impact on the classification
performance. Experiments on the CUB-200-2011 and Stanford Cars datasets show
that ProtoSolo achieves superior performance in classification tasks and
reaches the best level in terms of cognitive complexity of explanations
compared to state-of-the-art interpretable methods. The code is available at
https://github.com/pyt19/ProtoSolo.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [164] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/pdf/2506.19923)
*Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent combines LLMs and Lean for automated theorem proving, achieving 86.1% success on MiniF2F with fewer samples.


<details>
  <summary>Details</summary>
Motivation: To improve automated theorem proving by integrating LLMs with formal proof assistants like Lean.

Method: Coordinates an informal reasoning LLM, a formal prover model, and Lean feedback, while generating auxiliary lemmas.

Result: Achieves 86.1% success rate on MiniF2F, outperforming SLM-based methods with lower sample usage.

Conclusion: Prover Agent sets a new benchmark for automated theorem proving efficiency and effectiveness.

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [165] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/pdf/2506.19977)
*Deng Pan, Keerthiram Murugesan, Nuno Moniz, Nitesh Chawla*

Main category: cs.AI

TL;DR: A novel framework for context attribution in generative QA systems using combinatorial multi-armed bandits (CMAB) and Combinatorial Thompson Sampling (CTS) to improve query efficiency and attribution fidelity.


<details>
  <summary>Details</summary>
Motivation: To build interpretable and trustworthy generative QA systems by understanding which parts of retrieved context contribute to the model's answers.

Method: Formulates context attribution as a CMAB problem, treating each context segment as a bandit arm and using CTS to explore context subsets efficiently under limited query budgets. Defines a reward function based on normalized token likelihoods.

Result: Achieves competitive attribution quality with fewer model queries compared to traditional perturbation-based methods like SHAP.

Conclusion: The proposed method significantly improves query efficiency while maintaining high attribution fidelity, making it practical for real-world applications.

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [166] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/pdf/2506.20008)
*Abdul Basit, Minghao Shao, Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique*

Main category: cs.AI

TL;DR: The paper benchmarks LLMs for quantum code generation using PennyLane and QHack challenges, introducing QHackBench. RAG-enhanced models perform similarly to standard prompting, with a multi-agent pipeline improving execution success.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in quantum computing, addressing the underexplored area of quantum code generation.

Method: Benchmarked LLMs using QHackBench, evaluated under vanilla prompting and RAG, and introduced a multi-agent pipeline for refining solutions.

Result: RAG-enhanced models matched standard prompting, especially in complex algorithms, with the multi-agent pipeline boosting execution success.

Conclusion: The study advances AI-assisted quantum programming, with public release of QHackBench and tools to support future research.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [167] [Language Modeling by Language Models](https://arxiv.org/pdf/2506.20249)
*Junyan Cheng, Peter Clark, Kyle Richardson*

Main category: cs.AI

TL;DR: A multi-agent LLM system, Genesys, is proposed to discover novel LM architectures by simulating research stages, using genetic programming for efficiency, and achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can model the discovery of novel LM architectures, inspired by real research processes.

Method: Genesys employs a multi-agent approach with stages like ideation, code generation, pre-training, and evaluation, using a genetic programming backbone and scaling laws.

Result: 1,162 new designs were discovered, with 1,062 verified; the best designs outperformed GPT2 and Mamba2 on 6/9 benchmarks.

Conclusion: Genesys demonstrates effective autonomous discovery of LM architectures, with insights for future systems.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [168] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/pdf/2506.20009)
*Konstantinos Vrettos, Michail E. Klontzas*

Main category: cs.AI

TL;DR: Custom RAG models outperform commercial LLMs in medical tasks with higher accuracy and lower energy consumption.


<details>
  <summary>Details</summary>
Motivation: Address environmental and ethical concerns of AI in healthcare, focusing on resource use and patient privacy.

Method: Developed a customizable RAG framework for medical tasks, tested with open-source LLMs like llama3.1:8b and medgemma-4b-it.

Result: llama3.1-RAG achieved highest accuracy (58.5%) and lowest energy/CO2 footprint, outperforming commercial models.

Conclusion: Local LLMs can create sustainable, high-performance RAGs for healthcare, aligning with UN sustainability goals.

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [169] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/pdf/2506.20018)
*Zechun Deng, Ziwei Liu, Ziqian Bi, Junhao Song, Chia Xin Liang, Joe Yeong, Junfeng Hao*

Main category: cs.AI

TL;DR: The paper explores real-time AI-driven decision support systems, focusing on low-latency models, Edge-IoT integration, and human-AI collaboration, with insights into model compression and edge analytics.


<details>
  <summary>Details</summary>
Motivation: To advance efficient and adaptable AI-supported decision-making, especially in resource-limited scenarios, leveraging technologies like large language models and Edge-IoT.

Method: A detailed review of holistic AI tools, model compression (e.g., DeLLMa), edge device improvements, and human-AI teamwork approaches.

Result: Practical development strategies and application insights, identifying opportunities for more flexible and efficient AI systems.

Conclusion: The paper lays groundwork for future AI breakthroughs in real-time decision support, emphasizing its transformative potential.

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [170] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/pdf/2506.20020)
*Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan*

Main category: cs.AI

TL;DR: Persona-assigned LLMs exhibit human-like motivated reasoning, reducing veracity discernment and aligning with identity-congruent conclusions, with limited success in debiasing.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs show human-like motivated reasoning when assigned personas, impacting critical societal debates.

Method: Tested 8 LLMs with personas across political and socio-demographic attributes on veracity discernment and scientific evidence evaluation.

Result: Persona-assigned LLMs showed reduced veracity discernment (up to 9%) and political personas were 90% more likely to align with identity-congruent evidence. Debiasing methods were ineffective.

Conclusion: LLMs exhibit human-like motivated reasoning, raising concerns about exacerbating biases in AI and human decision-making.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [171] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/pdf/2506.20059)
*Weijieying Ren, Tianxiang Zhao, Lei Wang, Tianchun Wang, Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM integrates EHR data into medical dialogues for test recommendation, result interpretation, and diagnosis prediction, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing medical LLMs overlook EHR data and focus narrowly on diagnosis, limiting clinical applicability.

Method: Uses a Clinical Test Reference (CTR) strategy to map EHR codes and classify results. Employs reinforcement learning with reject sampling and tailored rewards.

Result: Outperforms baselines in clinical test recommendation and diagnosis prediction.

Conclusion: DiaLLM aligns better with real-world medical practice by integrating EHR data and improving diagnostic accuracy.

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [172] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/pdf/2506.20130)
*Adrien Bibal, Steven N. Minton, Deborah Khider, Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub, an AI-powered platform, introduces the Reproducibility Copilot to streamline computational reproducibility in research, reducing reproduction time from 30+ hours to ~1 hour.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring independent reproducibility of research findings in open science.

Method: Uses AI to analyze manuscripts, code, and supplementary materials, generating structured Jupyter Notebooks and recommendations.

Result: Substantially reduces reproduction time and detects barriers like missing hyperparameters or incomplete datasets.

Conclusion: AI-driven tools like OpenPub can enhance reproducibility and transparency in scientific communication, with potential for broader open science applications.

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [173] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/pdf/2506.20274)
*Liya Wang, David Yi, Damien Jose, John Passarelli, James Gao, Jordan Leventis, Kang Li*

Main category: cs.AI

TL;DR: A 14-task framework based on Bloom's Taxonomy is proposed to evaluate LLMs for enterprise tasks, addressing noisy data and costly annotation with a scalable pipeline. Open-source models perform well in reasoning but lag in judgment tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMLU fail to assess enterprise-specific task complexities, necessitating a tailored evaluation framework.

Method: Developed a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and CRAG to curate a 9,700-sample benchmark. Evaluated six leading models.

Result: Open-source models like DeepSeek R1 rival proprietary models in reasoning but lag in judgment tasks due to overthinking.

Conclusion: The benchmark highlights enterprise performance gaps and provides actionable insights for model optimization, advancing practical LLM deployment.

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [174] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/pdf/2506.20664)
*Andrei Lupu, Timon Willi, Jakob Foerster*

Main category: cs.AI

TL;DR: The paper introduces Decrypto, a game-based benchmark for evaluating multi-agent reasoning and theory of mind (ToM) in LLMs, addressing gaps in existing benchmarks. It finds current LLMs lag behind humans and older models in ToM tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for ToM and multi-agent reasoning in LLMs are limited by narrow scope, data leakage, and lack of interactivity, necessitating a new evaluation framework.

Method: The authors propose Decrypto, a benchmark inspired by cognitive science and multi-agent reinforcement learning, designed to minimize confounding factors. They validate it through empirical evaluations, robustness studies, and human-AI cross-play experiments.

Result: LLMs perform worse than humans and older models in ToM tasks, highlighting a gap in current evaluations.

Conclusion: Decrypto fills a critical gap in ToM and reasoning evaluations, aiding the development of better artificial agents.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


### [175] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/pdf/2506.20332)
*Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1 introduces interactive multi-turn reinforcement learning with task-level rewards to improve mobile agents' exploration and error correction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on offline training or action-level rewards, limiting dynamic interaction and causing local optima, which weakens exploration and error correction.

Method: Mobile-R1 uses a three-stage training framework: initial finetuning, single-step online training with action-level rewards, and multi-turn online training with task-level rewards.

Result: The approach enhances exploration and error correction, leading to significant performance improvements. A dataset and benchmark are also provided.

Conclusion: Mobile-R1 advances mobile agent capabilities by addressing limitations of prior methods, with open-sourced resources for further research.

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [176] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/pdf/2506.20357)
*Sungwon Han, Sungkyu Park, Seungeon Lee*

Main category: cs.AI

TL;DR: REFeat, a novel method, uses LLMs with structured reasoning to generate diverse and informative features for tabular data, outperforming existing approaches in accuracy and feature quality.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based feature generation methods often produce simple or repetitive features due to biases and lack of structured reasoning.

Method: REFeat leverages multiple reasoning types to guide LLMs in generating diverse and meaningful features.

Result: Experiments on 59 datasets show higher predictive accuracy and more diverse features compared to existing methods.

Conclusion: Incorporating rich reasoning and adaptive strategies into LLM-driven feature discovery holds promise for improving tabular data feature engineering.

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [177] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/pdf/2506.20384)
*Dror Ivry, Oran Nahum*

Main category: cs.AI

TL;DR: The paper introduces Paladin-mini, a compact open-source classifier for grounding claims, and a new evaluation dataset (grounding-benchmark), showcasing its performance against State-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of grounding claims in documents by providing supportive evidence, ensuring robust real-world applicability.

Method: Developed Paladin-mini (3.8B parameters) for labeling grounded/ungrounded claims and created the grounding-benchmark dataset for evaluation.

Result: Demonstrated Paladin-mini's performance against State-of-the-art, with clear and reproducible results.

Conclusion: Paladin-mini and the grounding-benchmark offer effective tools for claim grounding, with competitive performance.

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [178] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/pdf/2506.20401)
*Jinchun Du, Bojie Shen, Muhammad Aamir Cheema, Adel N. Toosi*

Main category: cs.AI

TL;DR: The paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G), focusing on profit-maximization for EV drivers by optimizing charging/discharging decisions amid dynamic electricity prices and route constraints. Two metaheuristic algorithms (EA and LNS) are proposed, showing significant profit gains and scalability.


<details>
  <summary>Details</summary>
Motivation: The integration of EVs into ride-hailing and delivery services, coupled with V2G technology, creates new challenges and opportunities for optimizing charging/discharging decisions to maximize profits while supporting the energy grid.

Method: The problem is formulated as a Mixed Integer Programming (MIP) model, and two metaheuristic algorithms (evolutionary and large neighborhood search) are proposed for near-optimal solutions.

Result: Experiments on real-world data demonstrate that the proposed methods double driver profits compared to baselines, with near-optimal performance on small instances and scalability on larger ones.

Conclusion: The work presents a promising approach for smarter, more profitable EV-based mobility systems that actively contribute to grid stability.

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [179] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/pdf/2506.20404)
*Riccardo Lo Bianco, Willem van Jaarsveld, Remco Dijkman*

Main category: cs.AI

TL;DR: GymPN is a software library using Deep Reinforcement Learning to optimize business process decisions, addressing partial observability and multiple decisions, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To enhance decision-making in business processes by overcoming limitations of previous work, such as lack of support for partial observability and multiple decisions.

Method: Developed GymPN, a library leveraging Deep Reinforcement Learning, to model and optimize business process decisions, including task assignment and execution timing.

Result: Evaluated on eight business process patterns, GymPN effectively models problems and learns optimal policies, demonstrating its practicality.

Conclusion: GymPN advances process management by enabling realistic decision-making scenarios and outperforming existing solutions.

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [180] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/pdf/2506.20486)
*Salvatore Milite, Giulio Caravagna, Andrea Sottoriva*

Main category: cs.AI

TL;DR: The paper introduces Mixture of Neural Cellular Automata (MNCA), a framework combining probabilistic rules and noise to model stochastic biological processes, outperforming deterministic NCAs in robustness and realism.


<details>
  <summary>Details</summary>
Motivation: Deterministic Neural Cellular Automata (NCAs) lack stochasticity, limiting their ability to model real-world biological systems.

Method: Proposes MNCA, integrating mixture models and intrinsic noise into NCAs to capture stochastic dynamics.

Result: MNCAs show superior robustness, realistic biological growth patterns, and interpretable rule segmentation in tissue growth, image morphogenesis, and microscopy tasks.

Conclusion: MNCAs are a promising tool for modeling stochastic dynamical systems and self-growth processes.

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [181] [Engineering Sentience](https://arxiv.org/pdf/2506.20504)
*Konstantin Demin, Taylor Webb, Eric Elmoznino, Hakwan Lau*

Main category: cs.AI

TL;DR: The paper defines sentience for AI in functional, computational terms, emphasizing assertoric and qualitative sensory signals for meaningful implementation.


<details>
  <summary>Details</summary>
Motivation: To provide a clear, implementable definition of sentience for AI, ensuring it captures subjective experience beyond mere perceptual encoding.

Method: Proposes a functional notion of sentience requiring persistent (assertoric) and qualitative sensory signals, with potential implementation sketches.

Result: A framework for designing sentient AI, aiding in intentional creation and timely recognition of sentient artificial agents.

Conclusion: Defining functional sentience helps avoid inadvertent creation of sentient AI and guides ethical implementation.

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [182] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/pdf/2506.20531)
*Wenbin Gan, Minh-Son Dao, Koji Zettsu*

Main category: cs.AI

TL;DR: The paper introduces a CBR-LLM framework to enhance evasive maneuver decision-making in autonomous driving by combining LLMs with case-based reasoning, improving accuracy and human alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in autonomous driving, such as domain adaptation and lack of experiential knowledge, to enable reliable and interpretable decisions in high-risk scenarios.

Method: Integrates semantic scene understanding from dashcam videos with retrieval of past driving cases, using risk-aware prompting and similarity-based case retrieval.

Result: Improves decision accuracy, justification quality, and alignment with human expert behavior across multiple LLMs.

Conclusion: The CBR-LLM framework shows promise as an adaptive and trustworthy decision-support tool for intelligent driving systems.

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [183] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/pdf/2506.20598)
*Alexander D. Kalian, Jaewook Lee, Stefan P. Johannesson, Lennart Otte, Christer Hogstrand, Miao Guo*

Main category: cs.AI

TL;DR: A multi-agent AI framework for sustainable protein research, focusing on microbial sources, uses RAG with GPT-based agents for literature search and information extraction. Fine-tuning and prompt engineering improved performance, with fine-tuning yielding higher cosine similarity scores.


<details>
  <summary>Details</summary>
Motivation: Address the global demand for sustainable protein by leveraging AI to process and synthesize scientific knowledge efficiently.

Method: Two GPT-based LLM agents: one for literature search and another for information extraction. Optimized via fine-tuning and prompt engineering.

Result: Fine-tuning improved mean cosine similarity scores to ≥0.94, outperforming prompt engineering (≥0.89). Lower uncertainties with prompt engineering.

Conclusion: The framework effectively supports sustainable protein research, with fine-tuning offering superior performance, and a user interface was developed for accessibility.

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [184] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/pdf/2506.20600)
*Wengxi Li, Roy Pea, Nick Haber, Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen is an AI architecture that converts programming videos into adaptive learning experiences using student modeling and generative AI tutoring, grounded in the Cognitive Apprenticeship framework.


<details>
  <summary>Details</summary>
Motivation: To enhance video-based programming education by making it interactive and adaptive, leveraging AI to personalize learning.

Method: Combines video segmentation by learning goals, a conversational tutoring engine, and Bayesian Knowledge Tracing for student modeling.

Result: Effective video segmentation and strong pedagogical alignment, with ablation studies confirming the necessity of each component.

Conclusion: CogGen advances AI tutoring by integrating structured student modeling with interactive AI, offering scalable improvements to programming education.

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [185] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/pdf/2506.20608)
*Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The PETSc team is using LLM-powered tools like RAG and chatbots to make its fragmented knowledge base more accessible, aiming to enhance scientific software development.


<details>
  <summary>Details</summary>
Motivation: PETSc's extensive but informal knowledge base is underutilized, making it hard for users and developers to access and reuse information effectively.

Method: The team employs LLM tools (RAG, reranking, chatbots) to organize and retrieve PETSc-specific knowledge, evaluating system architecture and models.

Result: Initial experiences show LLM tools can improve knowledge accessibility and workflow in scientific software, particularly for Krylov solvers.

Conclusion: The project aims to create an extensible AI framework for scientific software, with plans to expand into a robust platform for accelerating discovery.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [186] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/pdf/2506.20640)
*Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live is a framework to evaluate ML agents' ability to collaborate in a simulated research community. CoMind, a novel agent, excels in this context, outperforming most human competitors in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: Existing ML agents work in isolation, missing the collaborative benefits of human research communities.

Method: Introduces MLE-Live for live evaluation and CoMind, an agent designed for community knowledge exchange.

Result: CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% of human competitors in Kaggle.

Conclusion: CoMind demonstrates the potential of collaborative ML agents, bridging the gap between isolated and community-driven research.

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [187] [Towards Better Benchmark Datasets for Inductive Knowledge Graph Completion](https://arxiv.org/pdf/2406.11898)
*Harry Shomer, Jay Revolinsky, Jiliang Tang*

Main category: cs.AI

TL;DR: The paper identifies a shortcut in current inductive KGC datasets (exploitable via PPR scores) and proposes a new dataset construction method to mitigate this issue, improving benchmark reliability.


<details>
  <summary>Details</summary>
Motivation: Current inductive KGC datasets inadvertently allow shortcuts (e.g., PPR scores) that bypass relational learning, undermining evaluation of true model capabilities.

Method: Analyzes the PPR shortcut issue, proposes a revised dataset construction strategy, and benchmarks popular KGC methods on the new datasets.

Result: PPR scores perform near SOTA on existing datasets; the new method mitigates this, revealing true model performance.

Conclusion: The revised dataset construction removes shortcuts, enabling a clearer evaluation of inductive KGC methods.

Abstract: Knowledge Graph Completion (KGC) attempts to predict missing facts in a
Knowledge Graph (KG). Recently, there's been an increased focus on designing
KGC methods that can excel in the inductive setting, where a portion or all of
the entities and relations seen in inference are unobserved during training.
Numerous benchmark datasets have been proposed for inductive KGC, all of which
are subsets of existing KGs used for transductive KGC. However, we find that
the current procedure for constructing inductive KGC datasets inadvertently
creates a shortcut that can be exploited even while disregarding the relational
information. Specifically, we observe that the Personalized PageRank (PPR)
score can achieve strong or near SOTA performance on most datasets. In this
paper, we study the root cause of this problem. Using these insights, we
propose an alternative strategy for constructing inductive KGC datasets that
helps mitigate the PPR shortcut. We then benchmark multiple popular methods
using the newly constructed datasets and analyze their performance. The new
benchmark datasets help promote a better understanding of the capabilities and
challenges of inductive KGC by removing any shortcuts that obfuscate
performance. The code and dataset and can be found at
https://github.com/HarryShomer/Better-Inductive-KGC.

</details>


### [188] [Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging](https://arxiv.org/pdf/2502.04030)
*Guinan Su, Jonas Geiping*

Main category: cs.AI

TL;DR: An Automated Model Merging Framework is proposed to enhance reasoning in LLMs by efficiently exploring merging strategies without retraining, reducing costs and human effort.


<details>
  <summary>Details</summary>
Motivation: Current model merging methods rely on manual hyperparameter tuning, limiting exploration and requiring significant resources.

Method: The framework uses multi-fidelity approximations for fine-grained merging, supporting single and multi-objective optimization with novel search spaces (LFS and DIS).

Result: Autonomous searches find merges that boost performance on single and multi-objective tasks, achieving results in under 500 steps.

Conclusion: The framework enables cost-effective, automated model merging, improving LLM reasoning capabilities.

Abstract: Reasoning capabilities represent a critical frontier for large language
models (LLMs), but developing them requires extensive proprietary datasets and
computational resources. One way to efficiently supplement capabilities with is
by model merging, which offers a promising alternative by combining multiple
models without retraining. However, current merging approaches rely on
manually-designed strategies for merging hyperparameters, limiting the
exploration of potential model combinations and requiring significant human
effort. We propose an Automated Model Merging Framework that enables
fine-grained exploration of merging strategies while reducing costs through
multi-fidelity approximations. We support both single and multi-objective
optimization and introduce two novel search spaces: layerwise fusion (LFS) and
depth-wise integration (DIS). Evaluating across a number of benchmarks, we find
that the search autonomously finds 1) Merges that further boost
single-objective performance, even on tasks the model has already been
finetuned on, and 2) Merges that optimize multi-objective frontiers across
tasks. Effective merges are found with limited compute, e.g. within less than
500 search steps.

</details>


### [189] [RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models](https://arxiv.org/pdf/2505.07089)
*Hanzheng Dai, Yuanliang Li, Jun Yan, Zhibo Zhang*

Main category: cs.AI

TL;DR: RefPentester, a knowledge-informed, self-reflective AutoPT framework, outperforms GPT-4o by 16.7% in identifying vulnerabilities by addressing LLM limitations like imbalanced knowledge and lack of learning from failures.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based AutoPT frameworks underperform due to imbalanced knowledge, short-sighted planning, and hallucinations, lacking mechanisms to learn from failures.

Method: Proposed RefPentester integrates a seven-state Stage Machine to guide PT stages, select tactics, and learn from failures.

Result: RefPentester outperforms GPT-4o by 16.7% in revealing credentials on Hack The Box's Sau machine and shows superior success rates in PT stage transitions.

Conclusion: RefPentester effectively addresses LLM limitations in AutoPT, enhancing performance and adaptability in ethical hacking.

Abstract: Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
inherent knowledge of LLMs. However, existing LLM-based AutoPT frameworks often
underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sightedness in
the planning process, and hallucinations during command generation. Moreover,
the trial-and-error nature of the PT process is constrained by existing
frameworks lacking mechanisms to learn from previous failures, restricting
adaptive improvement of PT strategies. To address these limitations, we propose
a knowledge-informed, self-reflective PT framework powered by LLMs, called
RefPentester. This AutoPT framework is designed to assist human operators in
identifying the current stage of the PT process, selecting appropriate tactics
and techniques for each stage, choosing suggested actions, providing
step-by-step operational guidance, and reflecting on and learning from previous
failed operations. We also modeled the PT process as a seven-state Stage
Machine to integrate the proposed framework effectively. The evaluation shows
that RefPentester can successfully reveal credentials on Hack The Box's Sau
machine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages,
RefPentester also demonstrates superior success rates on PT stage transitions.

</details>


### [190] [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/pdf/2505.18746)
*Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang*

Main category: cs.AI

TL;DR: The paper introduces $C^3$-Bench, an open-source benchmark to evaluate AI agents' robustness by addressing tool dependencies, hidden information, and dynamic decisions, revealing vulnerabilities in current models.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of AI agents ignore critical factors like tool relationships and environmental feedback, limiting understanding of agent behavior.

Method: The benchmark includes three challenges (tool relationships, hidden information, dynamic decisions) with fine-grained metrics and reproducible methods, tested on 49 agents.

Result: Agents struggle with tool dependencies, long context, and policy switching, exposing vulnerabilities.

Conclusion: $C^3$-Bench aims to improve agent interpretability and robustness by highlighting model weaknesses.

Abstract: Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.

</details>


### [191] [Turing Test 2.0: The General Intelligence Threshold](https://arxiv.org/pdf/2505.19550)
*Georgios Mappouras*

Main category: cs.AI

TL;DR: The paper critiques traditional A.G.I. detection methods like the Turing test and introduces a new framework, Turing test 2.0, with a clear G.I. definition and threshold for practical A.G.I. detection.


<details>
  <summary>Details</summary>
Motivation: The lack of clear methods to detect A.G.I. in A.I. models, despite advancements, necessitates a new approach.

Method: Proposes a G.I. Threshold (G.I.T.) and a new testing framework (Turing test 2.0) for fail/pass A.G.I. detection.

Result: Demonstrates real-life applications of the Turing test 2.0 on modern A.I. models.

Conclusion: The Turing test 2.0 offers a practical, clear-cut method to detect A.G.I., addressing limitations of traditional approaches.

Abstract: With the rise of artificial intelligence (A.I.) and large language models
like ChatGPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a system
(computer or any other) has reached or surpassed A.G.I. To achieve this, we
make two new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing test 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing test 2.0 framework
on modern A.I. models.

</details>


### [192] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/pdf/2506.02097)
*Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Srikant Panda*

Main category: cs.AI

TL;DR: A hybrid framework combining RAG with intent-based canned responses improves enterprise conversational AI by balancing accuracy (95%) and latency (180ms).


<details>
  <summary>Details</summary>
Motivation: Address challenges in enterprise-scale deployments of RAG and LLM-powered chatbots, such as diverse queries, high latency, hallucinations, and domain-specific knowledge integration.

Method: Integrates RAG with intent-based canned responses, uses a dialogue context manager for coherence, and includes a feedback loop for refinement.

Result: Achieves high accuracy (95%) and low latency (180ms), outperforming standalone RAG and intent-based systems.

Conclusion: The framework is scalable and adaptive, suitable for enterprise conversational AI applications.

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [193] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/pdf/2506.02280)
*Kedir Yassin Hussen, Walelign Tewabe Sewunetie, Abinew Ali Ayele, Sukairaj Hafiz Imam, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

Main category: cs.AI

TL;DR: The paper highlights the limited support for African languages in LLMs, SLMs, and SSLMs, identifying gaps in coverage, scripts, and data, and proposes solutions like standardization and community corpus development.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Africa's 2,000 low-resource languages in NLP models, despite the transformative impact of LLMs.

Method: Comparative analysis of African language coverage across six LLMs, eight SLMs, and six SSLMs, evaluating language coverage, training sets, technical limitations, scripts, and roadmaps.

Result: Identified 42 supported African languages and 23 public datasets, with four languages (Amharic, Swahili, Afrikaans, Malagasy) dominating while over 98% remain unsupported. Only Latin, Arabic, and Ge'ez scripts are recognized, neglecting 20 others.

Conclusion: Challenges like data scarcity, tokenization biases, and high computational costs require solutions such as language standardization, community-driven corpus development, and effective adaptation methods for African languages.

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [194] [The Alignment Trap: Complexity Barriers](https://arxiv.org/pdf/2506.10304)
*Jasper Yao*

Main category: cs.AI

TL;DR: The paper argues AI alignment is fundamentally impossible due to logical contradictions, supported by five mathematical proofs.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that AI alignment is not just technically challenging but inherently contradictory.

Method: Five mathematical proofs (Geometric, Computational, Statistical, Information-Theoretic, Dynamic) are presented to confirm the impossibility.

Result: The proofs show AI alignment is impossible due to inherent contradictions in safety requirements, training data, and optimization.

Conclusion: The field faces a strategic trilemma due to these fundamental barriers; formal verification of theorems is ongoing.

Abstract: This paper argues that AI alignment is not merely difficult, but is founded
on a fundamental logical contradiction. We first establish The Enumeration
Paradox: we use machine learning precisely because we cannot enumerate all
necessary safety rules, yet making ML safe requires examples that can only be
generated from the very enumeration we admit is impossible. This paradox is
then confirmed by a set of five independent mathematical proofs, or "pillars of
impossibility." Our main results show that: (1) Geometric Impossibility: The
set of safe policies has measure zero, a necessary consequence of projecting
infinite-dimensional world-context requirements onto finite-dimensional models.
(2) Computational Impossibility: Verifying a policy's safety is coNP-complete,
even for non-zero error tolerances. (3) Statistical Impossibility: The training
data required for safety (abundant examples of rare disasters) is a logical
contradiction and thus unobtainable. (4) Information-Theoretic Impossibility:
Safety rules contain more incompressible, arbitrary information than any
feasible network can store. (5) Dynamic Impossibility: The optimization process
for increasing AI capability is actively hostile to safety, as the gradients
for the two objectives are generally anti-aligned. Together, these results
demonstrate that the pursuit of safe, highly capable AI is not a matter of
overcoming technical hurdles, but of confronting fundamental, interlocking
barriers. The paper concludes by presenting a strategic trilemma that these
impossibilities force upon the field. A formal verification of the core
theorems in Lean4 is currently in progress.

</details>


### [195] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/pdf/2506.10521)
*Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai*

Main category: cs.AI

TL;DR: The paper introduces the Scientists' First Exam (SFE) benchmark to evaluate Multimodal Large Language Models (MLLMs) on scientific cognitive capacities, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: Current scientific benchmarks inadequately assess MLLMs' perception and reasoning abilities, focusing only on knowledge understanding.

Method: The SFE benchmark evaluates MLLMs through three levels: scientific signal perception, attribute understanding, and comparative reasoning, using 830 expert-verified VQA pairs across 66 tasks in five disciplines.

Result: State-of-the-art models (GPT-3 and InternVL-3) scored only 34.08% and 26.52% on SFE, indicating significant room for improvement.

Conclusion: SFE aims to advance AI-enhanced scientific discoveries by addressing gaps in MLLM evaluation.

Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.

</details>


### [196] [Evaluating Generalization and Representation Stability in Small LMs via Prompting, Fine-Tuning and Out-of-Distribution Prompts](https://arxiv.org/pdf/2506.17289)
*Rahul Raja, Arpita Vats*

Main category: cs.AI

TL;DR: Comparative study of few-shot prompting and supervised fine-tuning in small language models, analyzing generalization, robustness, and internal representations in low-resource and OOD settings.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness and generalization of small language models under different adaptation strategies (prompting vs. fine-tuning) in low-resource and distribution-shift scenarios.

Method: Comparative analysis across task formats, prompt styles, and model scales, evaluating accuracy and internal representations in in-distribution and OOD settings.

Result: Highlights key differences in how small models internalize and generalize knowledge under prompting and fine-tuning, providing empirical insights.

Conclusion: Offers practical guidance for model selection in low-data regimes and contributes to the debate on prompting versus fine-tuning.

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings. Beyond accuracy, we analyze the internal
representations learned by each approach to assess the stability and
abstraction of task-specific features. Our findings highlight critical
differences in how small models internalize and generalize knowledge under
different adaptation strategies. This work offers practical guidance for model
selection in low-data regimes and contributes empirical insight into the
ongoing debate over prompting versus fine-tuning. Code for the experiments is
available at the following

</details>


### [197] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/pdf/2506.17667)
*Lintao Wang, Encheng Su, Jiaqi Liu, Pengze Li, Peng Xia, Jiabei Xiao, Wenlong Zhang, Xinnan Dai, Xi Chen, Yuan Meng, Mingyu Ding, Lei Bai, Wanli Ouyang, Shixiang Tang, Aoran Wang, Xinzhu Ma*

Main category: cs.AI

TL;DR: PhysUniBench is a multimodal benchmark for evaluating MLLMs on undergraduate physics problems, revealing significant gaps in current models' reasoning and diagram interpretation.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with the complexity of physics problem-solving, necessitating a rigorous benchmark to assess and improve their capabilities.

Method: Developed PhysUniBench, a 3,304-question benchmark across 8 physics sub-disciplines, with open-ended and multiple-choice questions, expert-curated difficulty levels, and multimodal inputs.

Result: State-of-the-art models like GPT-4o mini perform poorly (34.2% accuracy), especially on multi-step and diagram-heavy problems.

Conclusion: PhysUniBench highlights the need for improved AI models in physics reasoning and aims to advance multimodal understanding in AI for Science.

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [198] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/pdf/2506.20609)
*Ankit Shah, Rita Singh, Bhiksha Raj, Alexander Hauptmann*

Main category: cs.SD

TL;DR: A cost-effective acoustic analysis method for gunshot detection and firearm classification using machine learning, outperforming traditional methods but facing challenges with noisy data.


<details>
  <summary>Details</summary>
Motivation: Addressing the high costs of commercial gunshot detection systems and the need for timely, accurate information to mitigate gun-related violence.

Method: Leveraging acoustic analysis of gunshot recordings, using SVMs and CNNs for detection and classification, tested on a dataset of 3459 recordings.

Result: CNN achieved mAP 0.58 on clean data, outperforming SVM (mAP 0.39), but performance dropped to mAP 0.35 with noisy data.

Conclusion: The approach shows promise for real-time, low-cost deployment on common devices but requires improvements in handling noisy environments.

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


### [199] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/pdf/2506.14293)
*Tawsif Ahmed, Andrej Radonjic, Gollam Rabby*

Main category: cs.SD

TL;DR: Sleeping-DISCO 9M is a large-scale pre-training dataset for music and song, addressing the lack of open-source, high-quality datasets for generative music tasks by using real-world popular music.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for generative music tasks are either synthetic or lack real-world relevance, limiting their adoption. Sleeping-DISCO 9M aims to fill this gap by providing a dataset of actual popular music.

Method: The dataset is constructed using real popular music and songs from world-renowned artists, ensuring high quality and relevance.

Result: Sleeping-DISCO 9M offers a practical and realistic dataset for tasks like text-music, music-captioning, and singing-voice synthesis.

Conclusion: This dataset addresses a critical need in the generative music community by providing a high-quality, real-world music corpus for diverse applications.

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music
and song. To the best of our knowledge, there are no open-source high-quality
dataset representing popular and well-known songs for generative music modeling
tasks such as text-music, music-captioning, singing-voice synthesis, melody
reconstruction and cross-model retrieval. Past contributions focused on
isolated and constrained factors whose core perspective was to create synthetic
or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily
large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another
focus for the community. Unfortunately, adoption of these datasets has been
below substantial in the generative music community as these datasets fail to
reflect real-world music and its flavour. Our dataset changes this narrative
and provides a dataset that is constructed using actual popular music and
world-renowned artists.

</details>


### [200] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/pdf/2506.18671)
*Yuqin Dai, Wanlu Zhu, Ronghui Li, Xiu Li, Zhenyu Zhang, Jun Li, Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++ is a music-driven framework for harmonious group dance generation, addressing collisions, foot sliding, and abrupt swapping with innovative embeddings and loss functions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-dancer collisions, foot sliding, and abrupt swapping in long group dance generation, limiting quality and coherence.

Method: TCDiff++ uses dancer positioning embedding, distance-consistency loss, swap mode embedding, Footwork Adaptor, long group diffusion sampling, and Sequence Decoder.

Result: TCDiff++ achieves state-of-the-art performance, especially in long-duration scenarios, ensuring high-quality group dance generation.

Conclusion: The proposed framework effectively addresses key challenges in group dance generation, delivering superior results.

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [201] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/pdf/2506.19999)
*Francesco Ignazio Re, Andreas Opedal, Glib Manaiev, Mario Giulianelli, Ryan Cotterell*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic model for reading behavior using a marked spatio-temporal point process, improving on standard methods by capturing dynamic fixations and saccades with a Hawkes process.


<details>
  <summary>Details</summary>
Motivation: Standard eye-tracking models rely on aggregated data and strong assumptions, missing spatio-temporal dynamics. This work aims to better model reading behavior.

Method: A marked spatio-temporal point process models fixations and saccades, with saccades modeled via a Hawkes process and fixation durations incorporating contextual predictors.

Result: The Hawkes process model fits human saccades better than baselines. Contextual surprisal marginally improves fixation duration predictions, suggesting surprisal theory's limited explanatory power for fine-grained eye movements.

Conclusion: The proposed model captures reading dynamics more effectively, though surprisal theory's role in explaining fixation durations remains limited.

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [202] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/pdf/2506.19882)
*Rylan Schaeffer, Joshua Kazdan, Yegor Denisov-Blanch, Brando Miranda, Matthias Gerstgrasser, Susan Zhang, Andreas Haupt, Isha Gupta, Elyas Obbad, Jesse Dodge, Jessica Zosa Forde, Koustuv Sinha, Francesco Orabona, Sanmi Koyejo, David Donoho*

Main category: cs.LG

TL;DR: The paper proposes a 'Refutations and Critiques' (R & C) Track at ML conferences to systematically correct errors in research, enhancing the field's self-correcting nature.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in ML research have led to flawed or incorrect studies being accepted due to peer review fallibility, necessitating a mechanism for correction.

Method: The paper suggests designing an R & C Track with specific review principles, addressing potential pitfalls, and provides an example submission.

Result: The proposed R & C Track would offer a reputable platform to critique and refute prior research, fostering a self-correcting ecosystem.

Conclusion: ML conferences should implement official mechanisms like the R & C Track to improve research integrity and correction.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [203] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/pdf/2506.19883)
*Zhuqing Liu, Chaosheng Dong, Michinari Momma, Simone Shao, Shaoyuan Xu, Yan Gao, Haibo Yang, Jia Liu*

Main category: cs.LG

TL;DR: The paper introduces STIMULUS, a novel multi-objective optimization algorithm, and its enhanced versions (STIMULUS-M, STIMULUS+, STIMULUS-M+), offering improved convergence rates and sample complexity compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing MOO methods suffer from poor convergence and high sample complexity, limiting their practical applications.

Method: STIMULUS uses a recursive stochastic gradient framework, while STIMULUS-M adds momentum. Enhanced versions (STIMULUS+/M+) incorporate adaptive batching to reduce full gradient evaluations.

Result: The methods achieve $O(1/T)$ (non-convex) and $O(\exp{-\mu T})$ (strongly convex) convergence rates, with state-of-the-art sample complexities.

Conclusion: STIMULUS and its variants provide robust, efficient solutions for MOO problems, addressing key limitations of existing approaches.

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [204] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/pdf/2506.19885)
*Jing Lu, Xuan Wu, Yizhun Tian, Songhan Fan, Yali Fang*

Main category: cs.LG

TL;DR: The paper introduces FlightKooba, a framework combining HIPPO, Koopman theory, and state space equations to improve flight trajectory prediction by reducing computational load and enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Current Koopman-based models for flight trajectory prediction are inefficient, lack interpretability, and have high computational costs.

Method: FlightKooba constructs Koopman operators directly from data using structural state space equations, reducing trainable parameters and training time.

Result: FlightKooba achieves faster training (comparable to Mamba without CUDA), reduces memory by over 50%, and cuts parameters by tenfold.

Conclusion: FlightKooba offers an efficient, interpretable solution for Koopman operator computation, advancing time series forecasting and control integration.

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [205] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/pdf/2506.19890)
*Ziru Zhang, Jiadong Yu, Danny H. K. Tsang*

Main category: cs.LG

TL;DR: An intelligent framework combining adaptive keyframe extraction and causal-aware RL to optimize QoE in multi-user VR, outperforming benchmarks in latency, fairness, and QoE.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore causal relationships between bandwidth, CPU frequency, and user perception, limiting QoE improvements in multi-user VR interactions.

Method: Proposes PS-CDDPG, integrating DDPG with causal influence detection, and formulates a QoE metric using the Weber-Fechner Law. Optimizes keyframe ratios, bandwidth, and computational resources via MIP.

Result: Significantly reduces latency, enhances QoE, and maintains fairness, outperforming benchmark methods in experiments.

Conclusion: The framework effectively balances latency, fidelity, and fairness, demonstrating superior performance in multi-user VR QoE optimization.

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [206] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/pdf/2506.20260)
*Junqi Jiang, Antonio Rago, Francesco Leofante, Francesca Toni*

Main category: cs.LG

TL;DR: The paper addresses the challenge of providing robust counterfactual explanations (CEs) under model multiplicity (MM) in machine learning, proposing recourse-aware ensembling (RAE) and an argumentative ensembling method to ensure CE robustness.


<details>
  <summary>Details</summary>
Motivation: Model multiplicity complicates recourse recommendations via CEs, as CEs may not be valid across all competing models. The paper aims to formalize and solve this problem.

Method: Introduces recourse-aware ensembling (RAE) and a novel argumentative ensembling method leveraging computational argumentation to resolve conflicts between models and CEs.

Result: The method guarantees CE robustness under MM, supports model preferences, and satisfies six desirable properties. Theoretical and empirical analyses validate its effectiveness.

Conclusion: Argumentative ensembling effectively addresses recourse under MM, offering customizable and robust solutions for CEs.

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [207] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/pdf/2506.19891)
*Qinghui Gong, Xue Yang, Xiaohu Tang*

Main category: cs.LG

TL;DR: A novel class-aware soft pruning framework for machine unlearning achieves rapid, precise forgetting with minimal accuracy loss, outperforming existing methods in speed and performance.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between unlearning speed and predictive accuracy in machine unlearning, ensuring compliance with privacy regulations like GDPR.

Method: Uses orthogonal convolutional kernel regularization and activation difference analysis to decorrelate filters and identify class-specific channels for efficient pruning.

Result: Demonstrates millisecond-level response times, complete forgetting of targeted classes, and minimal accuracy loss on retained data across multiple datasets.

Conclusion: The framework offers an efficient, practical solution for real-time machine unlearning in MLaaS scenarios, reducing attack risks and outperforming baselines.

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [208] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/pdf/2506.20100)
*Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE is a multimodal benchmark for expert-level reasoning in agriculture, combining natural queries, expert responses, and image-based context. It features real-world, underspecified scenarios with diverse biological entities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for grounded reasoning and long-form generation in knowledge-intensive domains like agriculture.

Method: Curated from 35,000 real user-expert interactions, MIRAGE includes 7,000+ biological entities and uses a multi-step pipeline for high-fidelity scenarios.

Result: MIRAGE offers a diverse, open-world benchmark for evaluating vision-language models on expert-level tasks like pest diagnosis and crop management.

Conclusion: MIRAGE fills a gap in multimodal benchmarks by providing a realistic, taxonomically diverse dataset for complex reasoning in agriculture.

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [209] [Representation Learning with Parameterised Quantum Circuits for Advancing Speech Emotion Recognition](https://arxiv.org/pdf/2501.12050)
*Thejan Rajapakshe, Rajib Rana, Farina Riaz, Sara Khalifa, Björn W. Schuller*

Main category: cs.LG

TL;DR: A hybrid quantum-classical model using PQCs integrated with CNNs improves speech emotion recognition, reducing parameters by 50% and outperforming classical CNNs.


<details>
  <summary>Details</summary>
Motivation: To explore quantum machine learning's potential for enhancing representation learning in complex signal domains like speech emotion recognition, addressing challenges like subtle temporal variations and overlapping affective states.

Method: Proposes a hybrid quantum-classical architecture combining parameterised quantum circuits (PQCs) with CNNs, leveraging quantum properties like superposition and entanglement.

Result: The hybrid model outperforms classical CNNs on benchmark datasets (IEMOCAP, RECOLA, MSP-IMPROV) with a 50% reduction in trainable parameters.

Conclusion: Demonstrates QML's potential for emotion recognition and lays groundwork for future quantum-enabled affective computing systems.

Abstract: Quantum machine learning (QML) offers a promising avenue for advancing
representation learning in complex signal domains. In this study, we
investigate the use of parameterised quantum circuits (PQCs) for speech emotion
recognition (SER) a challenging task due to the subtle temporal variations and
overlapping affective states in vocal signals. We propose a hybrid quantum
classical architecture that integrates PQCs into a conventional convolutional
neural network (CNN), leveraging quantum properties such as superposition and
entanglement to enrich emotional feature representations. Experimental
evaluations on three benchmark datasets IEMOCAP, RECOLA, and MSP-IMPROV
demonstrate that our hybrid model achieves improved classification performance
relative to a purely classical CNN baseline, with over 50% reduction in
trainable parameters. This work provides early evidence of the potential for
QML to enhance emotion recognition and lays the foundation for future
quantum-enabled affective computing systems.

</details>


### [210] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/pdf/2506.19893)
*Jingzhi Hu, Geoffrey Ye Li*

Main category: cs.LG

TL;DR: Proposes DeKA-g, a distillation-enabled knowledge alignment algorithm for generative semantic communication (GSC) to improve efficiency and adaptability in transmitting AI-generated content.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of aligning knowledge between cloud-GAI and edges/users, and between transmission knowledge and actual wireless channels in GSC systems.

Method: Introduces DeKA-g with two novel methods: metaword-aided knowledge distillation (MAKD) for efficient knowledge transfer and variable-rate grouped SNR adaptation (VGSA) for adapting to diverse channel conditions.

Result: Improves alignment between edge- and cloud-generated images by 44%, adapts to compression rates 116% more efficiently, and enhances low-SNR performance by 28%.

Conclusion: DeKA-g effectively addresses knowledge alignment challenges in GSC, significantly improving performance and adaptability.

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [211] [Multimodal Representation Learning and Fusion](https://arxiv.org/pdf/2506.20494)
*Qihang Jin, Enze Ge, Yuhang Xie, Hongying Luo, Junhao Song, Ziqian Bi, Chia Xin Liang, Jibin Guan, Joe Yeong, Junfeng Hao*

Main category: cs.LG

TL;DR: Multi-modal learning combines data from sources like images, text, and audio to enhance AI understanding and decision-making, using techniques like representation learning, alignment, and fusion. Challenges include handling diverse data formats and adversarial attacks. Future directions focus on unsupervised learning, AutoML, and better evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To improve AI systems' ability to interpret and reason by leveraging multiple data modalities, addressing real-world complexity.

Method: Uses representation learning, alignment methods, and fusion strategies to integrate multi-modal data. Explores unsupervised/semi-supervised learning and AutoML for scalability.

Result: Enhances AI's interpretation, reasoning, and decision-making capabilities. Identifies challenges like data diversity and adversarial attacks.

Conclusion: Multi-modal learning holds promise for advancing AI in fields like computer vision and healthcare, aiming for human-like understanding and adaptability.

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [212] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/pdf/2506.19894)
*Antoine Pesenti, Aidan OSullivan*

Main category: cs.LG

TL;DR: The paper uses deep neural networks (DNN) and explainable AI (XAI) methods like SHAP and Gradient to forecast and analyze electricity market prices, enhancing understanding of market dynamics.


<details>
  <summary>Details</summary>
Motivation: Electricity markets are complex, and traditional econometric methods lack the predictive power of DNNs. The goal is to improve understanding of price dynamics using XAI.

Method: A DNN is used for price forecasting, combined with XAI methods (SHAP, Gradient) and visual techniques (heatmaps) to analyze feature contributions in five markets. Novel concepts like SSHAP values and lines are introduced.

Result: The approach provides insights into the factors driving electricity prices, leveraging DNNs' predictive power and XAI's interpretability.

Conclusion: The study enhances understanding of electricity market dynamics by combining DNN forecasting with XAI, introducing new methods for analyzing high-dimensional models.

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [213] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/pdf/2506.19895)
*Miguel N. Font, José L. Jorro-Aragoneses, Carlos M. Alaíz*

Main category: cs.LG

TL;DR: A novel post-hoc framework measures neural network uncertainty using retrieved training cases with similar activation vectors, introducing Decision Change and Layer Uncertainty metrics to improve uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Neural networks can produce wrong solutions in high-risk domains, necessitating better uncertainty measurement to detect and mitigate errors.

Method: The framework retrieves training cases with similar activation vectors per layer and proposes Decision Change and Layer Uncertainty metrics to analyze nearest-neighbor class distributions.

Result: Evaluation on CIFAR-10 and MNIST datasets shows the metrics enhance uncertainty estimation, outperforming softmax-based confidence.

Conclusion: The proposed metrics improve uncertainty estimation in neural networks, particularly for challenging tasks.

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [214] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/pdf/2506.19929)
*Efe Çakır, Patrick Dumond*

Main category: cs.LG

TL;DR: The paper explores using reinforcement learning (RL), specifically Deep Q-Networks (DQNs), for bearing fault diagnosis, showing comparable performance to traditional methods but better adaptability, though with higher computational costs.


<details>
  <summary>Details</summary>
Motivation: Bearing faults cause operational disruptions and high costs. Current methods rely on labeled data and struggle in dynamic environments, prompting the need for adaptable solutions like RL.

Method: The study employs Deep Q-Networks (DQNs) for bearing fault classification, focusing on adaptability and performance under controlled and dynamic conditions.

Result: RL models match traditional supervised learning in controlled settings but outperform in adaptability with optimized rewards, despite higher computational demands.

Conclusion: RL shows promise for adaptive bearing fault diagnosis, complementing traditional methods, though computational efficiency needs improvement.

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [215] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/pdf/2506.19935)
*Shuchen Xue, Tianyu Xie, Tianyang Hu, Zijin Feng, Jiacheng Sun, Kenji Kawaguchi, Zhenguo Li, Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper compares autoregressive (AR) and masked diffusion models (MDMs) in a decoder-only framework, highlighting trade-offs in speed and perplexity while decoupling paradigm differences from architectural influences.


<details>
  <summary>Details</summary>
Motivation: To fairly compare AR and MDM paradigms by addressing the confounding factor of architectural differences (decoder-only vs. encoder-only) and to refine the MDM objective.

Method: Evaluates MDMs within a decoder-only framework, comparing them as Any-Order AR (AO-AR) and standard AR, and investigates architectural influences.

Result: Decoder-only MDMs achieve significant generation speedups (~25×) and comparable perplexity with temperature annealing, despite modeling a larger space.

Conclusion: The work decouples paradigm differences from architectural influences, providing insights for future model design, and suggests refining the AO-AR objective.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [216] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/pdf/2506.19937)
*Tomas M. Bosschieter, Luis Franca, Jessica Wolk, Yiyuan Wu, Bella Mehta, Joseph Dehoney, Orsolya Kiss, Fiona C. Baker, Qingyu Zhao, Rich Caruana, Kilian M. Pohl*

Main category: cs.LG

TL;DR: The paper introduces a method to evaluate the importance of feature groups in GAMs, emphasizing joint signals over isolated features, and demonstrates its utility in medical datasets.


<details>
  <summary>Details</summary>
Motivation: Existing interpretable ML methods often overlook joint signals from feature groups, which can be critical, especially in datasets with natural feature groupings like multimodal data.

Method: A novel approach for determining feature group importance in GAMs, efficient and requiring no retraining, with support for posthoc and overlapping groups.

Result: The method is validated on synthetic experiments and applied to medical datasets, showing improved accuracy and holistic insights compared to single-feature analysis.

Conclusion: Analyzing group importance provides more accurate and comprehensive insights, particularly in medical contexts, highlighting the value of joint feature signals.

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [217] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/pdf/2506.19992)
*Gabor Petnehazi, Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES is a hierarchical k-means clustering algorithm integrating LLMs for interpretable summaries of diverse data types.


<details>
  <summary>Details</summary>
Motivation: Address the need for advanced tools to group complex datasets and provide human-understandable insights.

Method: Recursive k-means clustering with LLM-generated titles/descriptions for clusters, supporting direct and description modes.

Result: Enhanced interpretability and meaningful hierarchical knowledge extraction from complex datasets.

Conclusion: HERCULES offers a powerful, interpretable solution for hierarchical clustering of multimodal data.

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [218] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/pdf/2506.19997)
*Geonwoo Cho, Jaegyun Im, Jihwan Lee, Hojun Yi, Sejin Kim, Sundong Kim*

Main category: cs.LG

TL;DR: TRACED introduces transition prediction error and co-learnability to improve regret approximation in UED, enhancing zero-shot generalization with fewer environment interactions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deep reinforcement learning agents to unseen environments by refining regret approximation in UED.

Method: Combines transition prediction error and co-learnability in regret approximation for adaptive task generation.

Result: TRACED improves zero-shot generalization and reduces environment interactions by up to 2x compared to baselines.

Conclusion: Refined regret approximation and task relationship modeling enhance sample-efficient curriculum design in UED.

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [219] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/pdf/2506.20015)
*Dengyu Wu, Jiechen Chen, H. Vincent Poor, Bipin Rajendran, Osvaldo Simeone*

Main category: cs.LG

TL;DR: The paper proposes a neuromorphic computing architecture using resonate-and-fire (RF) neurons for efficient spectral feature extraction in time-series signals, reducing energy consumption compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional leaky integrate-and-fire (LIF) neurons fail to effectively capture spectral features in streaming signals, necessitating a more efficient approach for edge applications like wireless sensing and audio recognition.

Method: The study introduces a wireless split computing architecture with RF neurons, which resonate at tunable frequencies to extract spectral features directly, avoiding costly pre-processing.

Result: The RF-SNN architecture achieves accuracy comparable to LIF-SNNs and ANNs while significantly reducing spike rates and energy consumption during inference and communication.

Conclusion: RF neurons offer a promising solution for energy-efficient, real-time processing of spectral-rich signals in edge applications.

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [220] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/pdf/2506.20016)
*Shanika Iroshi Nanayakkara, Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: A novel deep unfolding-based approach in Quantum Federated Learning (QFL) autonomously optimizes hyperparameters to address client heterogeneity, achieving ~90% accuracy, outperforming traditional methods (~55%).


<details>
  <summary>Details</summary>
Motivation: Client heterogeneity in QFL leads to performance issues, requiring a solution to dynamically adapt hyperparameters for robust optimization.

Method: Leverages deep unfolding for autonomous hyperparameter optimization (e.g., learning rates, regularization) based on client-specific training behavior.

Result: Achieves ~90% accuracy on IBM quantum hardware and Qiskit Aer simulators, significantly improving over traditional methods.

Conclusion: The framework enhances QFL applicability in critical areas like healthcare and genomics by mitigating overfitting and ensuring robust optimization.

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [221] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/pdf/2506.20023)
*Ryan Hildebrant, Rahul Bhope, Sharad Mehrotra, Christopher Tull, Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM is a preprocessing framework for training robust time series imputation models, addressing real-world missing data patterns and outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets often have complex, heterogeneous missing patterns, unlike artificially masked training data, necessitating a robust solution.

Method: DIM-SUM combines pattern clustering and adaptive masking strategies with theoretical learning guarantees to handle diverse missing patterns.

Result: DIM-SUM outperforms traditional methods in accuracy and efficiency, achieving 2x higher accuracy than a large pre-trained model with less inference time.

Conclusion: DIM-SUM effectively bridges the gap between artificial and real missing data patterns, offering a practical solution for infrastructure monitoring.

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [222] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/pdf/2506.20024)
*Salva Rühling Cachay, Miika Aittala, Karsten Kreis, Noah Brenowitz, Arash Vahdat, Morteza Mardani, Rose Yu*

Main category: cs.LG

TL;DR: ERDM integrates rolling forecasts with Elucidated Diffusion Models to improve probabilistic forecasting in chaotic systems by addressing temporal dependencies and uncertainty growth.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with complex temporal dependencies and uncertainty growth in high-dimensional chaotic systems.

Method: ERDM adapts Elucidated Diffusion Models (EDM) components—noise schedule, network preconditioning, and Heun sampler—to rolling forecasts, with a novel loss weighting, efficient initialization, and hybrid sequence architecture.

Result: ERDM outperforms baselines in 2D Navier-Stokes simulations and ERA5 weather forecasting.

Conclusion: ERDM provides a flexible framework for diffusion-based sequence generation, especially where escalating uncertainty is critical.

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [223] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/pdf/2506.20025)
*Nathan Stromberg, Christos Thrampoulidis, Lalitha Sankar*

Main category: cs.LG

TL;DR: Loss weighting remains effective in last layer retraining (LLR) for models between underparameterized and overparameterized extremes, but weights must account for model overparameterization.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in machine learning models, especially in the intermediate regime of LLR where data is inseparable and models are proportionately sized.

Method: Theoretical and practical exploration of loss weighting in the LLR regime, considering model overparameterization.

Result: Loss weighting is effective in LLR, but its success depends on accounting for relative overparameterization.

Conclusion: Properly adjusted loss weights can mitigate biases in intermediate model regimes, bridging gaps between underparameterized and overparameterized extremes.

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [224] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/pdf/2506.20031)
*Prithvi Poddar, Ehsan Tarkesh Esfahani, Karthik Dantu, Souma Chowdhury*

Main category: cs.LG

TL;DR: A framework for generating diverse courses of action (COAs) for multi-agent operations, using a genetic algorithm and graph neural network to optimize task allocation and sequencing.


<details>
  <summary>Details</summary>
Motivation: Automated planning is needed for multi-agent missions (e.g., disaster response) due to environmental changes and varying agent capabilities, requiring diverse COAs.

Method: Theoretical formulation with graph abstraction, genetic algorithm for task allocation, and graph neural network for task sequencing.

Result: Simulated tests show performance gains over random baselines, small optimality gaps, and feasible execution times (50 mins for 5 agents/100 tasks).

Conclusion: The framework effectively generates diverse COAs, balancing compatibility and diversity, with practical computational efficiency.

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [225] [Verifiable Unlearning on Edge](https://arxiv.org/pdf/2506.20037)
*Mohammad M Maheri, Alex Davidson, Hamed Haddadi*

Main category: cs.LG

TL;DR: A framework using zk-SNARKs ensures verifiable data unlearning on edge devices while preserving privacy and model performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for verifiable removal of data samples on edge devices due to copyright, biases, or regulations, ensuring integrity.

Method: Leverages zero-knowledge proofs (zk-SNARKs) for verification, with algorithms designed for efficient unlearning and minimal overhead.

Result: Practical and effective, enabling verifiable unlearning with minimal impact on personalized model performance.

Conclusion: The framework ensures privacy-preserving, verifiable, and effective machine unlearning on edge devices.

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [226] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/pdf/2506.20040)
*Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: The paper introduces CLVQVAE, a framework using vector quantization to map and collapse redundant features in transformer layers into interpretable concept vectors, addressing challenges in analyzing cross-layer superposition.


<details>
  <summary>Details</summary>
Motivation: Current methods analyze neural representations at single layers, missing cross-layer superposition and redundancy, limiting interpretability of feature evolution in large language models.

Method: Proposes CLVQVAE, combining top-k temperature-based sampling with EMA codebook updates and scaled-spherical k-means++ for initialization to map and collapse features into concept vectors.

Result: The framework provides controlled exploration of latent space and maintains codebook diversity, better aligning with semantic structure in word embeddings.

Conclusion: CLVQVAE offers a novel approach to uncovering emergent concepts in transformer layers by addressing redundancy and improving interpretability.

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [227] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/pdf/2506.20041)
*Soheil Abadifard, Fazli Can*

Main category: cs.LG

TL;DR: The paper introduces LSH-DynED, a novel method combining Locality Sensitive Hashing with Random Hyperplane Projections and Dynamic Ensemble Diversification to address multi-class imbalanced data streams. It outperforms 15 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of dynamic imbalance ratios in multi-class imbalanced data streams, with limited existing solutions, motivates this study.

Method: LSH-RHP is integrated into DynED for undersampling majority classes, creating balanced training sets and improving ensemble prediction.

Result: LSH-DynED excels in Kappa and mG-Mean measures, showing robustness in large-scale, high-dimensional datasets with class imbalances.

Conclusion: LSH-DynED is effective for multi-class imbalanced non-stationary data streams, with potential for future research and practical applications.

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [228] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/pdf/2506.20046)
*Hirad Daneshvar, Reza Samavi*

Main category: cs.LG

TL;DR: A novel method using knowledge distillation is proposed to efficiently quantify predictive uncertainty in GNNs for healthcare, outperforming traditional Bayesian and ensemble methods in computational cost and precision.


<details>
  <summary>Details</summary>
Motivation: Quantifying predictive uncertainty in GNNs is crucial for trustworthiness in clinical settings, but existing methods like Bayesian and ensemble approaches are computationally expensive and lack diversity capture.

Method: The method employs self-distillation, where the same network acts as both teacher and student, avoiding independent training of multiple networks. An uncertainty metric assigns weights to GNN classifiers to capture diversity.

Result: Evaluated on MIMIC-IV and Enzymes datasets, the method effectively captures uncertainty, matches MC Dropout and ensemble performance, and distinguishes out-of-distribution data.

Conclusion: The proposed self-distillation-based method efficiently quantifies GNN uncertainty with high precision, offering a practical solution for clinical applications.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [229] [Universal pre-training by iterated random computation](https://arxiv.org/pdf/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: Randomly generated data can pre-train models effectively, supported by theoretical and empirical evidence, improving zero-shot learning and finetuning performance.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility and benefits of using synthetic data for pre-training models, inspired by theoretical insights from algorithmic complexity.

Method: Theoretical analysis of algorithmic complexity and empirical validation using synthetic data for pre-training, followed by finetuning on real-world data.

Result: Models pre-trained with synthetic data achieve zero-shot learning and show improved performance with scale; finetuning enhances convergence and generalization.

Conclusion: Synthetic data pre-training is a viable and effective approach, offering theoretical grounding and practical benefits for model performance.

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [230] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/pdf/2506.20061)
*Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang*

Main category: cs.LG

TL;DR: The paper introduces a method using large language models (LLMs) to relabel unsuccessful trajectories in reinforcement learning, reducing reliance on human-labeled data and improving policy performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of developing instruction-following policies in reinforcement learning due to sparse rewards and dependence on human-labeled datasets.

Method: Leveraging LLMs to retrospectively generate open-ended instructions from agent trajectories, relabeling unsuccessful ones to enrich training data.

Result: Improved sample efficiency, instruction coverage, and policy performance in the Craftax environment compared to baselines.

Conclusion: LLM-guided open-ended instruction relabeling effectively enhances instruction-following reinforcement learning.

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [231] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/pdf/2506.20065)
*Cristian Minoccheri, Sophia Tesic, Kayvan Najarian, Ryan Stidham*

Main category: cs.LG

TL;DR: The paper introduces a supervised coupled matrix-tensor factorization (SCMTF) method to integrate patient-reported outcomes (PROs) and lab data for predicting medication persistence in ulcerative colitis (UC), achieving high predictive accuracy and interpretable phenotypes.


<details>
  <summary>Details</summary>
Motivation: Patient-reported symptoms in UC are often noisy and sparse, leading to their exclusion in phenotyping. This work aims to leverage PROs using a novel tensor-based method to improve phenotyping and prediction.

Method: A supervised coupled matrix-tensor factorization (SCMTF) integrates temporal PROs, lab data, and static features within a deep learning framework to handle missing data and predict medication persistence.

Result: The model predicts medication changes 8 and 20 months ahead with AUCs of 0.853 and 0.803, respectively, and identifies interpretable phenotypes including symptom variables.

Conclusion: The SCMTF method successfully applies tensor-based phenotyping to UC and PROs, demonstrating the value of PROs in predicting medication persistence and improving patient care.

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [232] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/pdf/2506.20090)
*Ainaz Jamshidi, Dongchan Kim, Muhammad Arif*

Main category: cs.LG

TL;DR: A review comparing classification- and regression-based predictive maintenance (PdM) methods, highlighting advancements, challenges, and trends like hybrid approaches and AI-enabled systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standalone comparative studies between regression- and classification-based PdM approaches, providing insights for researchers and practitioners.

Method: Analyzes recent literature on PdM, focusing on classification (failure probability) and regression (RUL estimation) methods, including challenges like data imbalance.

Result: Identifies key advancements, challenges (e.g., data imbalance), and trends (e.g., hybrid methods), aiding in robust maintenance system development.

Conclusion: The review clarifies trade-offs between PdM methods and suggests future work on practical tools and datasets to advance PdM research.

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [233] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/pdf/2506.20094)
*Krishna Praneet Gudipaty, Walid A. Hanafy, Kaan Ozkara, Qianlin Liang, Jesse Milzman, Prashant Shenoy, Suhas Diggavi*

Main category: cs.LG

TL;DR: Proposes Multi-Level Ensemble Learning (MEL) for resilient edge inference, balancing accuracy and fault tolerance with lightweight backup models.


<details>
  <summary>Details</summary>
Motivation: Edge environments are power- and resource-constrained, and conventional resilience methods compromise latency or accuracy.

Method: MEL trains multiple lightweight backup models collaboratively, formulated as a multi-objective optimization problem encouraging diversity and standalone performance.

Result: MEL achieves performance comparable to original models (40% size) and retains 95.6% accuracy during failures.

Conclusion: MEL provides effective fault tolerance and deployment flexibility for edge inference without compromising accuracy.

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [234] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/pdf/2506.20132)
*Patrick Alan Johnson, Gabriel Tseng, Yawen Zhang, Heather Heward, Virginia Sjahli, Favyen Bastani, Joseph Redmon, Patrick Beukema*

Main category: cs.LG

TL;DR: The paper proposes using a pretrained multimodal earth-observation model to create large-scale, high-resolution LFMC maps for wildfire risk monitoring, improving accuracy by 20% over previous methods.


<details>
  <summary>Details</summary>
Motivation: Wildfires are intensifying, and ground-based LFMC sampling is costly and sparse. AI and satellite data offer a scalable solution for real-time monitoring.

Method: A pretrained, multimodal earth-observation model is used to generate spatially complete LFMC maps, tested in wildfire-impacted regions.

Result: The method reduces RMSE by 20% compared to randomly initialized models, enabling rapid LFMC map generation across the U.S.

Conclusion: The approach provides a scalable, accurate solution for wildfire risk monitoring, validated in real-world scenarios.

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [235] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/pdf/2506.20169)
*Bala Rajesh Konkathi, Arun K. Tangirala*

Main category: cs.LG

TL;DR: The paper proposes a method called Partition of Variables (PoV) for causal discovery in LTI-DAE systems, improving upon a 2022 method by Kathari and Tangirala. PoV handles systems with algebraic relations and identifies causal drivers efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DIPCA fail for dynamical systems with algebraic relations (DAE systems). The need for a robust causal discovery method in such systems drives this work.

Method: PoV uses DIPCA to identify algebraic and dynamical relations, then partitions variables via the constraint matrix's condition number to find causal drivers.

Result: PoV successfully identifies causal drivers in LTI-DAE systems, outperforming the 2022 method by handling both pure dynamical and mixed systems.

Conclusion: PoV is a superior method for causal discovery in LTI-DAE systems, addressing limitations of prior work and demonstrating effectiveness through case studies.

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [236] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/pdf/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: A framework for discovering causal structure in PDEs using physics-informed neural networks and counterfactual perturbations, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical methods like residual minimization or sparse regression in identifying causal operators in PDEs, by introducing a more interpretable and robust approach.

Method: Uses physics-informed neural networks and counterfactual perturbations to quantify operator-level necessity, with causal sensitivity indices and structural deviation metrics for assessment.

Result: Theoretically proven exact recovery of causal operator support under certain conditions, with empirical validation on synthetic and real-world datasets showing superior performance over standard methods.

Conclusion: The framework successfully positions causal PDE discovery as a tractable and interpretable task, grounded in structural causal models and variational residual analysis.

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [237] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/pdf/2506.20194)
*Ruokai Yin, Yuhang Li, Donghyun Lee, Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT combines unstructured weight pruning and activation sparsity to reduce LLM deployment costs while maintaining accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High memory and compute costs of LLMs hinder deployment, and existing pruning methods often ignore runtime activation sparsity.

Method: DuoGPT integrates unstructured weight pruning with activation sparsity, using activation-aware calibration and dense model residuals for accuracy.

Result: DuoGPT achieves up to 9.17% better accuracy than state-of-the-art methods at a 1.39× speedup.

Conclusion: DuoGPT effectively balances performance and efficiency, scaling well for billion-parameter LLMs.

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [238] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/pdf/2506.20197)
*Clément L. Canonne, Yash Pote, Uddalok Sarkar*

Main category: cs.LG

TL;DR: The paper introduces Anubis, a zero-shot tool for attributing code generated by LLMs using hypothesis testing and density estimates, achieving high accuracy with minimal samples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of attributing code generated by LLMs due to the curse of dimensionality, leveraging available samples and density estimates.

Method: Proposes Anubis, which frames attribution as a distribution testing problem, using samples and density estimates from LLMs for tractability.

Result: Anubis achieves AUROC scores ≥0.9 in distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and Stable-Code with ≈2000 samples.

Conclusion: Anubis effectively solves the attribution problem for LLM-generated code, demonstrating high accuracy and practicality.

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [239] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/pdf/2506.20481)
*Matthieu Meeus, Igor Shilov, Georgios Kaissis, Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: The paper highlights that memorization in machine learning models is influenced by more than just self-influence, emphasizing the role of near-duplicates and proposing the use of full influence distributions for better risk assessment.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of counterfactual self-influence in capturing memorization risks, especially due to the impact of near-duplicates in training data.

Method: Analyzes memorization by treating counterfactual influence as a distributional quantity, computing full influence distributions for a small language model and image classification tasks.

Result: Self-influence alone underestimates memorization risks; near-duplicates significantly reduce self-influence but remain extractable. Influence distributions reveal near-duplicates in datasets like CIFAR-10.

Conclusion: Memorization is better understood through full influence distributions, which capture complex interactions in training data, rather than relying solely on self-influence.

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [240] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/pdf/2506.20204)
*Eduardo Gutierrez Maestro, Hadi Banaee, Amy Loutfi*

Main category: cs.LG

TL;DR: The paper introduces the Affective Priming Score (APS) to detect and mitigate priming effects in affective computing data, reducing misclassification rates in models.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored impact of priming on physiological signals and its potential to cause misclassifications in learning models.

Method: Proposes APS, a data-driven method to score data points for priming effects, validated on SEED and SEED-VII datasets by comparing original and priming-free data.

Result: Misclassification rates significantly drop when using priming-free sequences, demonstrating APS's effectiveness.

Conclusion: APS enhances model robustness and provides insights for designing affective computing datasets by mitigating priming effects at the data level.

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [241] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/pdf/2506.20520)
*Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos*

Main category: cs.LG

TL;DR: The paper analyzes an off-policy REINFORCE algorithm for aligning LLMs, showing policy improvement when the baseline lower-bounds expected reward, and validates findings in experiments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between off-policy RL and supervised fine-tuning for aligning LLMs, addressing suboptimal performance of off-policy methods.

Method: Theoretical analysis of an off-policy REINFORCE algorithm with a tunable baseline, validated in a stochastic bandit setting and LLM fine-tuning.

Result: Policy improvement is guaranteed when the baseline lower-bounds expected reward; off-policy updates benefit more from positive rewards.

Conclusion: The study provides insights into optimizing off-policy RL for LLM alignment, emphasizing the role of baseline tuning and reward focus.

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [242] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/pdf/2506.20235)
*Yuyang Zhang, Xu Shen, Yu Xie, Ka-Chun Wong, Weidun Xie, Chengbin Peng*

Main category: cs.LG

TL;DR: A novel GNN framework combines feature embedding and community information for directed link prediction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Improving directed link prediction by integrating feature embedding with community information.

Method: Proposes a GNN framework using hybrid features and transforms graphs into directed line graphs for better information aggregation.

Result: Outperforms state-of-the-art methods on benchmark datasets with varying training data proportions.

Conclusion: The hybrid feature approach and directed line graph transformation enhance link prediction performance.

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [243] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/pdf/2506.20629)
*Soufiane Hayou, Nikhil Ghosh, Bin Yu*

Main category: cs.LG

TL;DR: PLoP (Precise LoRA Placement) is a lightweight method for automatically identifying optimal LoRA adapter placements in pretrained models, outperforming common strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA adapter placement strategies lack consensus, with some favoring attention modules and others MLP modules. PLoP aims to resolve this ambiguity by providing a data-driven solution.

Method: PLoP uses intuitive theoretical analysis to automatically determine the best module types (e.g., Query, Key, MLP) for LoRA adapter placement, given a pretrained model and finetuning task.

Result: PLoP consistently outperforms or matches common placement strategies in supervised finetuning and reinforcement learning for reasoning tasks.

Conclusion: PLoP offers a robust, automated solution for LoRA adapter placement, enhancing efficiency and performance in model finetuning.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [244] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/pdf/2506.20245)
*Yushan Zhao, Jinyuan He, Donglai Chen, Weijie Luo, Chong Xie, Ri Zhang, Yonghong Chen, Yan Xu*

Main category: cs.LG

TL;DR: FedBKD is a novel data-free distillation framework for federated learning, addressing non-IID data challenges without public datasets, improving both global and local model performance.


<details>
  <summary>Details</summary>
Motivation: To solve the non-IID data problem in FL while avoiding data leakage risks from public datasets, and to enhance both global and local model performance.

Method: Uses GANs to generate synthetic data, with local models as frozen discriminators, and bidirectional distillation between global and local models for knowledge interaction.

Result: FedBKD achieves state-of-the-art performance on 4 benchmarks under various non-IID settings.

Conclusion: FedBKD effectively addresses non-IID challenges in FL without relying on public datasets, improving both generalization and personalization.

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [245] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/pdf/2506.20251)
*Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song*

Main category: cs.LG

TL;DR: The paper evaluates safety risks in quantized LLMs and proposes Q-resafe, a framework to restore safety without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Quantization of LLMs for resource-constrained environments may compromise safety, necessitating systematic evaluation and mitigation.

Method: Comprehensive safety evaluations across quantization techniques and datasets, followed by the Q-resafe framework for safety restoration.

Result: Q-resafe successfully re-aligns safety of quantized LLMs with pre-quantization levels, even in challenging scenarios.

Conclusion: The proposed Q-resafe framework effectively mitigates safety vulnerabilities in quantized LLMs.

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [246] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/pdf/2506.20253)
*Ben Gerhards, Nikita Popkov, Annekatrin König, Marcel Arpogaus, Bastian Schäfermeier, Leonie Riedl, Stephan Vogt, Philip Hehlert*

Main category: cs.LG

TL;DR: The paper evaluates data-driven methods for generating synthetic time series data for long-term energy consumption forecasting, comparing WGAN, DDPM, HMM, and MABF techniques.


<details>
  <summary>Details</summary>
Motivation: Long-term forecasting of individual power consumption is understudied, and high-fidelity synthetic data is needed for energy system applications.

Method: Comparative evaluation of WGAN, DDPM, HMM, and MABF to replicate temporal dynamics and dependencies in energy consumption profiles.

Result: The study highlights the strengths and limitations of each method, aiding in selecting the best approach for energy-related tasks.

Conclusion: The framework enhances synthetic data accuracy and privacy, using open-source German household data for practical applications.

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [247] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/pdf/2506.20285)
*Zeqi Leng, Chunxu Zhang, Guodong Long, Riting Xia, Bo Yang*

Main category: cs.LG

TL;DR: A novel FL framework introduces a universal expert model to capture shared knowledge across clusters in CFL, improving performance by balancing personalized and shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing CFL methods overlook shared information across clusters, limiting generalizable knowledge in FL systems.

Method: The framework involves three iterative steps: local model training, cluster-specific aggregation, and universal expert distillation.

Result: The method outperforms traditional gradient-based aggregation, handling model heterogeneity better and reducing conflicts.

Conclusion: The proposed framework advances CFL by effectively balancing personalized and shared knowledge.

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [248] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/pdf/2506.20305)
*Kazuki Yoda, Kazuhiko Kawamoto, Hiroshi Kera*

Main category: cs.LG

TL;DR: The paper explores learning functions of medium sensitivity, using Transformers for QR code decoding, achieving results beyond theoretical error-correction limits.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between input-sensitivity and learning hardness, focusing on medium-sensitivity tasks like QR code decoding.

Method: Employed Transformers to decode QR codes, analyzing their ability to learn embedded text structures and generalize across languages and random strings.

Result: Transformers successfully decoded QR codes beyond theoretical limits, focusing on data bits while ignoring error-correction bits.

Conclusion: Transformers offer a distinct decoding mechanism for QR codes, demonstrating potential for learning medium-sensitivity functions.

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [249] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/pdf/2506.20307)
*Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu*

Main category: cs.LG

TL;DR: ILDE is a novel imitation learning algorithm that uses double exploration (optimistic policy optimization and curiosity-driven exploration) to improve sample efficiency and achieve beyond-expert performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately learning expert policies from limited demonstrations due to complex state spaces and the need for exploration to surpass expert performance.

Method: ILDE combines optimistic policy optimization (rewarding uncertain state-action pairs) and curiosity-driven exploration (exploring states outside demonstrations).

Result: ILDE outperforms state-of-the-art algorithms in sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations.

Conclusion: ILDE is theoretically justified as an uncertainty-regularized policy optimization method with sublinear regret growth, proving its effectiveness.

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [250] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/pdf/2506.20323)
*Saundarya Subramaniam, Shalini Majumdar, Shantanu Nadar, Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: AI-driven crop disease detection system using deep learning models, achieving 95.76% accuracy, aims to aid rural farmers.


<details>
  <summary>Details</summary>
Motivation: To assist farmers in rural areas with limited resources by improving crop disease detection and management.

Method: Comparative analysis of deep learning models (EfficientNet, ResNet101, MobileNetV2, custom CNN) for transfer learning efficacy.

Result: Custom CNN achieved 95.76% validation accuracy in classifying plant diseases.

Conclusion: Transfer learning can reshape agricultural practices, enhancing crop health and sustainability in rural areas.

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [251] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/pdf/2506.20324)
*Torben Berndt, Benjamin Walker, Tiexin Qin, Jan Stühmer, Andrey Kormilitzin*

Main category: cs.LG

TL;DR: Permutation Equivariant Neural Graph CDEs enhance Graph Neural CDEs by reducing parameters while maintaining performance, improving efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of dynamic graphs by leveraging permutation equivariance for efficiency without losing representational power.

Method: Projects Graph Neural CDEs onto permutation equivariant function spaces, reducing parameters.

Result: Improved performance in interpolation and extrapolation tasks on simulated and real-world data.

Conclusion: The approach offers a more efficient and generalizable solution for dynamic graph modeling.

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [252] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/pdf/2506.20329)
*Alexandre Rio, Marta Soare, Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: The paper addresses fairness in sequential bundle recommendations, proposing methods to balance exposure for item groups while maintaining bundle quality.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios highlight the need for fair exposure of different item groups in sequential bundle recommendations.

Method: The authors formalize producer-fairness, propose an exact solution for small instances, and examine heuristics (quality-first, fairness-first, and an adaptive variant) for real-time balancing.

Result: Experiments on three datasets show the methods effectively provide fair recommendations without sacrificing quality.

Conclusion: The proposed solutions successfully balance fairness and quality in bundle recommendations, with adaptive heuristics offering dynamic trade-offs.

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [253] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/pdf/2506.20353)
*Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu*

Main category: cs.LG

TL;DR: DipSVD enhances SVD-based LLM compression by protecting critical matrix components locally and globally, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the inferior performance of existing SVD-based compression methods by protecting critical matrix components.

Method: Proposes dual-level importance protection: local (channel-weighted whitening) and global (heuristic/optimization-based layer compression).

Result: DipSVD achieves superior performance, especially at high compression ratios, across benchmarks.

Conclusion: DipSVD improves SVD-based compression by safeguarding critical components, enhancing model performance.

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [254] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/pdf/2506.20347)
*Malik Shahid Sultan, Hernando Ombao*

Main category: cs.LG

TL;DR: The paper proposes a new deep learning approach to Granger Causality (GC) by treating it as a prediction problem rather than variable selection, using model uncertainty and residuals to uncover causal structures.


<details>
  <summary>Details</summary>
Motivation: Traditional linear VAR models for GC are limited in capturing complex associations, and existing DNN-based methods treat GC as variable selection, which may not fully exploit deep learning's potential.

Method: The authors use deep learning to model time series jointly, leveraging model uncertainty and residual distributions to infer GC. They also study the impact of input layer dropout.

Result: A well-regularized deep learning model can learn the true GC structure without explicit sparsity or variable selection terms in the loss function.

Conclusion: The proposed paradigm shifts GC estimation from variable selection to prediction, demonstrating that deep learning can effectively uncover causal structures when properly regularized.

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [255] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/pdf/2506.20354)
*Francesco Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi*

Main category: cs.LG

TL;DR: A novel self-attention mechanism (MVPA) is introduced for modeling heterogeneous multi-variate time-series, applied to iEEG data, achieving strong generalization and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from multi-variate time-series with varying channel configurations, especially in clinical domains like iEEG.

Method: Proposes MVPA, a self-attention mechanism disentangling content, temporal, and spatial attention, and builds MVPFormer, a generative foundation model for iEEG.

Result: MVPFormer achieves expert-level seizure detection and outperforms state-of-the-art baselines. MVPA also excels in standard time-series tasks.

Conclusion: MVPA is a general-purpose attention mechanism for heterogeneous time-series, and MVPFormer is a state-of-the-art open-source iEEG foundation model.

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [256] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/pdf/2506.20359)
*Chanuka Don Samarasinghage, Dhruv Gulabani*

Main category: cs.LG

TL;DR: The paper introduces a taxonomy-based feature selection method for trajectory analysis to address high-dimensionality issues, improving efficiency, interpretability, and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: High-dimensionality in trajectory data reduces model efficiency and interpretability, necessitating effective feature selection methods.

Method: A taxonomy-based approach categorizes features into geometric (curvature, indentation) and kinematic (speed, acceleration) groups.

Result: The method achieved comparable or superior predictive performance, reduced feature selection time, and provided dataset-specific insights.

Conclusion: Taxonomy-based feature selection enhances interpretability, reduces complexity, and supports high-level decision-making in trajectory analysis.

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [257] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/pdf/2506.20362)
*Lorenzo Bini, Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN is a self-supervised graph learning framework that avoids negative sampling by using spectral bootstrapping and Laplacian-based signals, achieving efficient and scalable performance.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve self-supervised graph learning by eliminating the need for negative sampling and handcrafted augmentations while capturing rich structural representations.

Method: Integrates Laplacian-based signals and spectral bootstrapping, using max-min centrality-guided optimization for spectral augmentations and an adversarial bootstrapped training scheme.

Result: Outperforms state-of-the-art self-supervised graph methods on benchmark datasets, demonstrating efficiency and scalability.

Conclusion: LaplaceGNN offers a simpler, efficient, and effective self-supervised alternative for graph neural networks, with broad applicability.

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [258] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/pdf/2506.20380)
*Zhengpeng Feng, Sadiq Jaffer, Jovana Knezevic, Silja Sormunen, Robin Young, Madeline Lisaius, Markus Immitzer, James Ball, Clement Atzberger, David A. Coomes, Anil Madhavapeddy, Andrew Blake, Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA is a Remote Sensing Foundation Model (RSFM) using Self-Supervised Learning (SSL) to create global, robust representations from satellite data, outperforming traditional and foundation models in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To enable diverse Earth observation applications like climate modeling and conservation by generating high-resolution, global representations from satellite data.

Method: Uses two parallel Transformer-based encoders for Sentinel-1 SAR and Sentinel-2 MSI data, fused via MLP to create a global representation map (2017-2024).

Result: Sets a new state-of-the-art benchmark, outperforming traditional and geospatial foundation models in five diverse tasks.

Conclusion: TESSERA democratizes access to high-performance RS representations and excels in downstream applications.

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [259] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/pdf/2506.20413)
*Mohammad Mahdi Maheri, Denys Herasymuk, Hamed Haddadi*

Main category: cs.LG

TL;DR: P4 is a decentralized method for personalized learning in IoT, ensuring privacy and robustness against attacks, outperforming peers in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, private, and robust personalized learning in decentralized IoT ecosystems drives this work.

Method: P4 uses a lightweight algorithm to detect client similarity, forms groups, and employs differentially private knowledge distillation for co-training.

Result: P4 achieves 5%-30% higher accuracy than peers and remains robust with up to 30% malicious clients, adding minimal overhead (~7s).

Conclusion: P4 effectively addresses privacy, robustness, and efficiency in decentralized personalized learning for IoT.

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [260] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/pdf/2506.20417)
*Tatsuhiro Shimizu, Kazuki Kawamura, Takanori Muroi, Yusuke Narita, Kei Tateno, Takuma Udagawa, Yuta Saito*

Main category: cs.LG

TL;DR: The paper introduces a novel estimator, OPFV, for future off-policy evaluation and learning in non-stationary environments, leveraging time-series structures to reduce bias and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for off-policy evaluation and learning assume stationarity or rely on restrictive reward-modeling assumptions, leading to bias in non-stationary environments like e-commerce recommendations.

Method: Proposes OPFV, an estimator that uses time-series structures (e.g., seasonal effects) via a new importance weighting technique for accurate future policy value estimation. Also extends OPFV to a policy-gradient method for proactive learning.

Result: OPFV outperforms existing methods in estimating and optimizing future policy values under non-stationarity, as shown in empirical evaluations.

Conclusion: OPFV effectively addresses the limitations of existing methods by leveraging time-related structures, enabling accurate future policy evaluation and learning in non-stationary environments.

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [261] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/pdf/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: Proposes KDIA, a federated learning method using knowledge distillation and inequitable aggregation to improve model performance in large-scale, heterogeneous settings with limited client participation.


<details>
  <summary>Details</summary>
Motivation: Addresses performance degradation in federated learning due to client label skew, data quantity skew, and heterogeneity, especially when only a few clients participate in large-scale settings.

Method: Uses Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA), where the teacher model aggregates all clients weighted by participation intervals, counts, and data volume, while the student model averages participating clients. Includes self-knowledge distillation and auxiliary training with server-generated IID data.

Result: KDIA achieves better accuracy with fewer training rounds, especially under severe heterogeneity, as demonstrated on CIFAR-10/100/CINIC-10 datasets.

Conclusion: KDIA effectively leverages knowledge from all clients, improving federated learning performance in challenging heterogeneous and large-scale settings.

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [262] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/pdf/2506.20451)
*Shuchu Han, Wolfgang Bruckner*

Main category: cs.LG

TL;DR: An algorithm for selecting the optimal number of demonstrations in ICL for tabular data, considering data distribution, prompt template, and LLM, validated against random selection methods.


<details>
  <summary>Details</summary>
Motivation: Determining the ideal number of demonstrations in ICL for tabular data classification is challenging. This work aims to automate this process by integrating data distribution, prompt template, and LLM specifics.

Method: Uses Spectral Graph Theory to quantify demonstration similarities, constructs a similarity graph, and analyzes Laplacian eigenvalues to determine the minimum representative demonstrations.

Result: Validated through experiments, showing superior performance compared to random selection algorithms across diverse datasets and LLMs.

Conclusion: The proposed method effectively automates demonstration selection, improving ICL performance for tabular data.

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [263] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/pdf/2506.20441)
*Antoine Caradot, Rémi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban*

Main category: cs.LG

TL;DR: The paper introduces a new quadrature method for selecting collocation points in PINNs, leveraging the hessian of the function to improve training efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of Physics-informed Neural Networks (PINNs) by improving the selection of collocation points during training.

Method: Proposes a quadrature method based on the hessian of the function to guide collocation point selection in PINNs.

Result: The method aims to optimize the training process of PINNs by adaptively refining collocation points.

Conclusion: The proposed hessian-based quadrature method offers a promising approach for improving PINN training through better collocation point selection.

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [264] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/pdf/2506.20525)
*Christian Internò, Andrea Castellani, Sebastian Schmitt, Fabio Stella, Barbara Hammer*

Main category: cs.LG

TL;DR: The paper introduces SIDED, a synthetic industrial dataset for NILM, and AMDA, a data augmentation method, to address data scarcity and improve model generalization in industrial energy disaggregation.


<details>
  <summary>Details</summary>
Motivation: Industrial NILM faces challenges due to limited high-quality datasets and complex energy consumption patterns. Data scarcity and privacy issues further complicate the problem.

Method: The authors propose SIDED, a synthetic dataset generated via Digital Twin simulations, and AMDA, a method to scale appliance power contributions for better model generalization.

Result: NILM models trained with AMDA-augmented data achieved a Normalized Disaggregation Error of 0.093, outperforming non-augmented (0.451) and randomly augmented (0.290) models.

Conclusion: AMDA effectively aligns training and test data distributions, enhancing NILM model performance for industrial energy disaggregation.

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [265] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/pdf/2506.20511)
*Arno Geimer, Karthick Panner Selvam, Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: Optimizing local batch sizes in Federated Learning via greedy randomized search improves convergence speed without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Improve local training efficiency in Federated Learning by optimizing hardware usage, addressing suboptimal configurations due to lack of information exchange between participants.

Method: Uses greedy randomized search to optimize local batch sizes, leveraging parallel processing in FL.

Result: Outperforms default parameter settings in convergence speed, matching performance of locally optimized parameters.

Conclusion: Proposed method effectively enhances FL training efficiency without requiring parameter tuning for each participant.

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [266] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/pdf/2506.20518)
*Arno Geimer, Beltran Fiz Pontiveros, Radu State*

Main category: cs.LG

TL;DR: A novel framework for reward distribution in Federated Learning (FL) using client-specific tokens and decentralized finance (DeFi) to enhance flexibility and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the understudied area of reward distribution in FL, especially for for-profit applications, by leveraging DeFi and automated market makers (AMMs).

Method: Proposes a framework with client-specific tokens as investment vehicles, integrated with DeFi and AMMs for flexible reward distribution.

Result: Creates a scalable and flexible reward system for FL participants and enables third-party investment in the FL process.

Conclusion: The framework advances FL incentive mechanisms by combining DeFi and AMMs, offering a scalable solution for reward distribution.

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [267] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/pdf/2506.20537)
*R. Sharma, M. Raissi, Y. B. Guo*

Main category: cs.LG

TL;DR: The paper introduces FEA-PINN, a hybrid model combining Physics-Informed Neural Networks (PINN) with corrective FEA simulations to efficiently predict thermal fields in LPBF, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional FEA methods for LPBF simulation are computationally expensive, necessitating a faster yet accurate alternative.

Method: The FEA-PINN framework integrates PINN with dynamic material updating and corrective FEA simulations to address residual accumulation and ensure physical consistency.

Result: FEA-PINN matches FEA accuracy with significantly lower computational cost, validated using benchmark data and single-track LPBF scanning.

Conclusion: FEA-PINN offers an efficient and accurate solution for LPBF thermal field prediction, overcoming the limitations of standalone PINN and FEA.

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [268] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/pdf/2506.20543)
*Sanne van Kempen, Jaron Sanders, Fiona Sloothaak, Maarten G. Wolf*

Main category: cs.LG

TL;DR: The paper explores a reinforcement learning algorithm for optimal customer routing in skill-based queueing systems, demonstrating its adaptability and superiority over static policies. It introduces a heuristic for delay reduction and multi-objective optimization, while also analyzing sensitivity to errors and tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently managing customer routing in dynamic queueing systems like data centers and service networks, leveraging reinforcement learning for adaptability and performance.

Method: Uses a case study with real-world data to test a reinforcement learning algorithm, introduces a heuristic routing rule, and evaluates multi-objective optimization and parameter tuning.

Result: The algorithm outperforms static policies, adapts to changes, reduces delays, and balances multiple objectives like payoff, server fairness, and waiting times.

Conclusion: The reinforcement learning approach is viable for real-world queueing systems, with insights on implementation, multi-objective optimization, and sensitivity to tuning.

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [269] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/pdf/2506.20574)
*Laura Boggia, Rafael Teixeira de Lima, Bogdan Malaescu*

Main category: cs.LG

TL;DR: The paper explores transformer-based approaches, particularly the iTransformer, for anomaly detection in multivariate time series, analyzing key parameters, label extraction, training impacts, and model comparisons.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in multivariate time series is crucial but challenging due to unknown anomalies and complex interdependencies.

Method: Investigates the iTransformer architecture, focusing on parameters like window size, step size, and model dimensions, and evaluates anomaly label extraction and loss functions.

Result: Provides insights into parameter influences, label extraction methods, and performance of transformer-based models across diverse datasets.

Conclusion: The study advances transformer-based anomaly detection, offering practical guidance and comprehensive comparisons for real-world applications.

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [270] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/pdf/2506.20575)
*Itay Niv, Neta Rabin*

Main category: cs.LG

TL;DR: The paper explores the effectiveness of graph-transformer (GT) and hybrid GT-MPNN backbones in out-of-distribution (OOD) settings, showing they outperform traditional MPNNs. It also introduces a post-training analysis method for evaluating generalization.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks assume training and testing data share the same distribution, which is unrealistic. The paper investigates OOD generalization, focusing on backbone architectures like GTs and MPNNs.

Method: Systematically evaluate GT and hybrid backbones in OOD settings, adapt domain generalization algorithms for GTs, and propose a post-training analysis method for clustering structure.

Result: GT and hybrid backbones generalize better than MPNNs, even without specialized algorithms. The post-training analysis provides insights into domain alignment and class separation.

Conclusion: GTs show promise for robust graph learning in real-world scenarios, and the post-training analysis offers a new tool for understanding generalization beyond accuracy metrics.

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [271] [The kernel of graph indices for vector search](https://arxiv.org/pdf/2506.20584)
*Mariano Tepper, Ted Willke*

Main category: cs.LG

TL;DR: The paper introduces the Support Vector Graph (SVG), a machine learning-based graph index for vector search in metric and non-metric spaces, with formal navigability guarantees. It also interprets existing indices as SVG specializations and proposes SVG-L0 for bounded out-degree.


<details>
  <summary>Details</summary>
Motivation: Current graph indices for vector search rely on Euclidean space principles, limiting their applicability. The authors aim to extend these indices to metric and non-metric spaces using machine learning.

Method: The authors propose SVG, leveraging kernel methods for graph connectivity, and SVG-L0, which adds an ℓ0 sparsity constraint for bounded out-degree. They also reinterpret existing indices like HNSW and DiskANN as SVG specializations.

Result: SVG provides formal navigability guarantees in metric and non-metric spaces. SVG-L0 offers a principled way to bound out-degree and self-tuning properties, avoiding heuristics.

Conclusion: The work demonstrates the potential of machine learning to enhance graph indices for vector search, offering theoretical and practical improvements over traditional methods.

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [272] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/pdf/2506.20607)
*Jasen Lai, Senwei Liang, Chunmei Wang*

Main category: cs.LG

TL;DR: H-FEX, a symbolic learning method, accurately learns complex Hamiltonian functions while preserving energy conservation, outperforming traditional data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods struggle to accurately capture complex Hamiltonian functions and preserve energy conservation.

Method: Proposes H-FEX, a symbolic learning method with novel interaction nodes to effectively capture intricate interaction terms.

Result: H-FEX successfully recovers Hamiltonian functions for complex systems, preserving energy over long time horizons.

Conclusion: H-FEX is a promising framework for discovering closed-form expressions of complex dynamical systems.

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [273] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/pdf/2506.20623)
*Fariba Jangjoo, Matteo Marsili, Yasser Roudi*

Main category: cs.LG

TL;DR: Closed-loop learning in exponential family models leads to biased absorbing states, but bias can be mitigated by data pollution, MAP estimation, or regularization.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamics of closed-loop learning, where models train on their own generated data, is crucial for future neural network training.

Method: Derived equations of motion for parameters in exponential family models, analyzed maximum likelihood estimation, and explored mitigation strategies like data pollution and regularization.

Result: Closed-loop learning converges to biased absorbing states, but bias can be prevented with small external data, MAP estimation, or regularization.

Conclusion: Closed-loop learning amplifies initial biases, but simple interventions can mitigate this effect, though dynamics are not reparametrization invariant.

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [274] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/pdf/2506.20644)
*Hangyu Li, Hongyue Wu, Guodong Fan, Zhen Zhang, Shizhan Chen, Zhiyong Feng*

Main category: cs.LG

TL;DR: FedEDS is a federated learning scheme for edge devices that uses encrypted data sharing to improve convergence speed and model performance, addressing issues like latency and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Current federated learning research neglects network topology, physical distance, and data heterogeneity, causing latency and degraded performance.

Method: FedEDS trains a data encryptor using client models and stochastic layers, shares encrypted data among clients, and uses both private and shared data for local model training.

Result: FedEDS accelerates convergence and mitigates data heterogeneity, enhancing model performance in edge device applications.

Conclusion: FedEDS is effective for federated learning on edge devices, improving training efficiency and performance.

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [275] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/pdf/2506.20650)
*Anqi Mao, Mehryar Mohri, Yutao Zhong*

Main category: cs.LG

TL;DR: The paper introduces novel surrogate loss functions and algorithms for learning to defer with multiple experts, addressing consistency properties in single-stage and two-stage learning scenarios.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in optimally assigning input instances to experts while balancing accuracy and computational cost, particularly in fields like natural language generation, image processing, and medical diagnostics.

Method: The paper proposes new surrogate loss functions and efficient algorithms, providing theoretical guarantees for realizable H-consistency, H-consistency bounds, and Bayes-consistency in single-stage and two-stage deferral learning.

Result: The proposed methods achieve strong theoretical guarantees and outperform existing baselines in experiments.

Conclusion: The paper advances the field by addressing key consistency challenges in deferral learning, offering practical solutions with robust theoretical foundations.

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [276] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/pdf/2506.20651)
*Fei Wang, Baochun Li*

Main category: cs.LG

TL;DR: The paper analyzes malicious gradient leakage attacks in federated learning, showing their practical limitations and proposing a lightweight client-side detection mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the risk of sensitive data leakage in federated learning due to malicious server manipulation, and to evaluate the practicality and detectability of such attacks.

Method: Comprehensive analysis of gradient leakage attacks, including their effectiveness and stealthiness, and development of a client-side detection mechanism.

Result: Malicious attacks are limited in practice and detectable; a proposed detection mechanism offers a feasible defense with minimal overhead.

Conclusion: While theoretically concerning, gradient leakage attacks are practically limited and detectable, with lightweight defenses available.

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


### [277] [A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges](https://arxiv.org/pdf/2211.06665)
*Yunpeng Qing, Shunyu Liu, Jie Song, Huiqiong Wang, Mingli Song*

Main category: cs.LG

TL;DR: A survey on eXplainable Reinforcement Learning (XRL) categorizing methods into model, reward, state, and task explanations, and discussing challenges and opportunities.


<details>
  <summary>Details</summary>
Motivation: To address the black-box nature of Deep RL (DRL) and enhance trust and reliability in real-world applications.

Method: Review and taxonomy of XRL methods, including model-explaining, reward-explaining, state-explaining, and task-explaining approaches.

Result: A comprehensive categorization of XRL works and identification of overlooked methods leveraging human knowledge.

Conclusion: The survey provides a high-level summary of XRL, encourages future research, and offers a categorized open-source repository.

Abstract: Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and performance of agents while this
kind of method is often ignored in XRL field. Some challenges and opportunities
in XRL are discussed. This survey intends to provide a high-level summarization
of XRL and to motivate future research on more effective XRL solutions.
Corresponding open source codes are collected and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning.

</details>


### [278] [Do Concept Bottleneck Models Respect Localities?](https://arxiv.org/pdf/2401.01259)
*Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, Mateja Jamnik*

Main category: cs.LG

TL;DR: The paper evaluates whether concept-based explainability methods truly reflect model reasoning by analyzing if concept predictors use relevant features (locality). It finds many models fail this, leading to spurious explanations, and proposes solutions.


<details>
  <summary>Details</summary>
Motivation: To assess if concept-based explainability methods genuinely help understand model reasoning by verifying if concept predictors use relevant features (locality).

Method: Constructs three metrics to test locality under different perturbations, complemented by theoretical analysis.

Result: Many concept-based models fail to respect locality, as concept predictors struggle to distinguish distinct concepts clearly.

Conclusion: Proposes suggestions to improve concept-based models by ensuring locality, enhancing their explainability.

Abstract: Concept-based explainability methods use human-understandable intermediaries
to produce explanations for machine learning models. These methods assume
concept predictions can help understand a model's internal reasoning. In this
work, we assess the degree to which such an assumption is true by analyzing
whether concept predictors leverage "relevant" features to make predictions, a
term we call locality. Concept-based models that fail to respect localities
also fail to be explainable because concept predictions are based on spurious
features, making the interpretation of the concept predictions vacuous. To
assess whether concept-based models respect localities, we construct and use
three metrics to characterize when models respect localities, complementing our
analysis with theoretical results. Each of our metrics captures a different
notion of perturbation and assess whether perturbing "irrelevant" features
impacts the predictions made by a concept predictors. We find that many
concept-based models used in practice fail to respect localities because
concept predictors cannot always clearly distinguish distinct concepts. Based
on these findings, we propose suggestions for alleviating this issue.

</details>


### [279] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/pdf/2505.17282)
*Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli*

Main category: cs.LG

TL;DR: The paper analyzes token embeddings in a one-layer softmax attention model, showing how gradient descent aligns embeddings with token importance and optimizes classification margins.


<details>
  <summary>Details</summary>
Motivation: Despite the practical importance of token embeddings in language modeling, their theoretical understanding is limited. This paper aims to bridge that gap.

Method: The study uses a one-layer softmax attention model with a linear head for binary classification, analyzing embeddings via gradient descent and gradient flow.

Result: After training, embeddings align with token importance based on frequency, and the softmax selects predictive tokens, optimizing classification margins.

Conclusion: The theoretical findings are supported by experiments on real-world datasets (IMDB, Yelp), validating the model's behavior.

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [280] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/pdf/2506.18330)
*Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan*

Main category: cs.LG

TL;DR: Confucius3-Math is a 14B-parameter open-source LLM for math tasks, optimized for Chinese K-12 education, achieving SOTA performance efficiently on consumer GPUs.


<details>
  <summary>Details</summary>
Motivation: To enhance math education for Chinese K-12 students using AI, focusing on affordability and alignment with national curriculum.

Method: Post-training with large-scale RL, featuring innovations like Targeted Entropy Regularization, Recent Sample Recovery, and Policy-Specific Hardness Weighting.

Result: Achieves SOTA on math tasks, outperforming larger models, with stable RL training and improved data efficiency.

Conclusion: Demonstrates feasibility of cost-effective domain-specific reasoning models; model and code are open-sourced.

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [281] [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/pdf/2506.19143)
*Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy*

Main category: cs.LG

TL;DR: The paper proposes sentence-level analysis to interpret long-form reasoning in large language models, introducing three attribution methods to identify influential "thought anchors."


<details>
  <summary>Details</summary>
Motivation: Addressing interpretability challenges in long-form chain-of-thought reasoning by decomposing the process at the sentence level.

Method: Three attribution methods: (1) black-box counterfactual importance, (2) white-box attention pattern aggregation, (3) causal attribution via attention suppression.

Result: Identified "thought anchors"—key reasoning steps (e.g., planning or backtracking sentences) with outsized influence on subsequent reasoning.

Conclusion: Sentence-level analysis, supported by consistent findings across methods, offers deeper understanding of reasoning models, with tools provided for visualization.

Abstract: Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified "broadcasting" sentences that receive disproportionate attention
from all future sentences via "receiver" attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.

</details>


### [282] [Rethinking Early Stopping: Refine, Then Calibrate](https://arxiv.org/pdf/2501.19195)
*Eugène Berta, David Holzmüller, Michael I. Jordan, Francis Bach*

Main category: cs.LG

TL;DR: The paper introduces a variational formulation for calibration-refinement decomposition, showing calibration and refinement errors aren't minimized simultaneously during training. It proposes a two-step method: refine first, then calibrate post hoc, improving classifier performance.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal compromise between calibration and refinement errors during training, aiming for better probabilistic prediction quality.

Method: A novel variational formulation for calibration-refinement decomposition, followed by a two-step training approach: refine during training, then calibrate post hoc.

Result: The method consistently improves performance across diverse classification tasks by optimizing refinement and calibration separately.

Conclusion: Separating refinement and calibration steps enhances classifier performance, offering a practical solution to the trade-off between the two errors.

Abstract: Machine learning classifiers often produce probabilistic predictions that are
critical for accurate and interpretable decision-making in various domains. The
quality of these predictions is generally evaluated with proper losses, such as
cross-entropy, which decompose into two components: calibration error assesses
general under/overconfidence, while refinement error measures the ability to
distinguish different classes. In this paper, we present a novel variational
formulation of the calibration-refinement decomposition that sheds new light on
post-hoc calibration, and enables rapid estimation of the different terms.
Equipped with this new perspective, we provide theoretical and empirical
evidence that calibration and refinement errors are not minimized
simultaneously during training. Selecting the best epoch based on validation
loss thus leads to a compromise point that is suboptimal for both terms. To
address this, we propose minimizing refinement error only during training
(Refine,...), before minimizing calibration error post hoc, using standard
techniques (...then Calibrate). Our method integrates seamlessly with any
classifier and consistently improves performance across diverse classification
tasks.

</details>


### [283] [Adversarial Reasoning at Jailbreaking Time](https://arxiv.org/pdf/2502.01633)
*Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani*

Main category: cs.LG

TL;DR: The paper introduces an adversarial reasoning approach to jailbreak aligned LLMs, achieving state-of-the-art success rates and advancing the understanding of LLM vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To study LLM failure cases and improve adversarial robustness by addressing jailbreaking vulnerabilities.

Method: Develops an adversarial reasoning approach using a loss signal to guide test-time compute for automatic jailbreaking.

Result: Achieves SOTA attack success rates against aligned LLMs, including those designed for adversarial robustness.

Conclusion: The work advances understanding of LLM vulnerabilities, aiding the development of more robust and trustworthy AI systems.

Abstract: As large language models (LLMs) are becoming more capable and widespread, the
study of their failure cases is becoming increasingly important. Recent
advances in standardizing, measuring, and scaling test-time compute suggest new
methodologies for optimizing models to achieve high performance on hard tasks.
In this paper, we apply these advances to the task of model jailbreaking:
eliciting harmful responses from aligned LLMs. We develop an adversarial
reasoning approach to automatic jailbreaking that leverages a loss signal to
guide the test-time compute, achieving SOTA attack success rates against many
aligned LLMs, even those that aim to trade inference-time compute for
adversarial robustness. Our approach introduces a new paradigm in understanding
LLM vulnerabilities, laying the foundation for the development of more robust
and trustworthy AI systems.

</details>


### [284] [Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo](https://arxiv.org/pdf/2502.06379)
*Filip Ekström Kelvinius, Zheng Zhao, Fredrik Lindsten*

Main category: cs.LG

TL;DR: A sequential Monte Carlo method (DDSMC) is introduced for solving linear-Gaussian inverse problems using decoupled diffusion, showing effectiveness on synthetic, protein, and image data, with extensions to discrete data.


<details>
  <summary>Details</summary>
Motivation: To improve Bayesian inverse problem solving by leveraging pre-trained generative diffusion models as priors, focusing on larger sample updates.

Method: Designs a sequential Monte Carlo method based on decoupled diffusion, enabling asymptotically exact solutions for linear-Gaussian inverse problems.

Result: Demonstrates effectiveness on synthetic, protein, and image data, with potential extensions to discrete data.

Conclusion: DDSMC is a promising approach for Bayesian inverse problems, scalable and adaptable to various data types.

Abstract: A recent line of research has exploited pre-trained generative diffusion
models as priors for solving Bayesian inverse problems. We contribute to this
research direction by designing a sequential Monte Carlo method for
linear-Gaussian inverse problems which builds on "decoupled diffusion", where
the generative process is designed such that larger updates to the sample are
possible. The method is asymptotically exact and we demonstrate the
effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC)
algorithm on both synthetic as well as protein and image data. Further, we
demonstrate how the approach can be extended to discrete data.

</details>


### [285] [Chemical knowledge-informed framework for privacy-aware retrosynthesis learning](https://arxiv.org/pdf/2502.19119)
*Guikun Chen, Xu Zhang, Xiaolin Hu, Yong Liu, Yi Yang, Wenguan Wang*

Main category: cs.LG

TL;DR: CKIF is a privacy-preserving framework for training retrosynthesis models without sharing raw reaction data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Chemical reaction data is sensitive and proprietary, but current training methods risk privacy by centralizing data.

Method: CKIF uses distributed training with chemical knowledge-informed aggregation of model parameters, avoiding raw data sharing.

Result: CKIF outperforms strong baselines on various reaction datasets.

Conclusion: CKIF offers a secure, effective solution for privacy-preserving retrosynthesis model training.

Abstract: Chemical reaction data is a pivotal asset, driving advances in competitive
fields such as pharmaceuticals, materials science, and industrial chemistry.
Its proprietary nature renders it sensitive, as it often includes confidential
insights and competitive advantages organizations strive to protect. However,
in contrast to this need for confidentiality, the current standard training
paradigm for machine learning-based retrosynthesis gathers reaction data from
multiple sources into one single edge to train prediction models. This paradigm
poses considerable privacy risks as it necessitates broad data availability
across organizational boundaries and frequent data transmission between
entities, potentially exposing proprietary information to unauthorized access
or interception during storage and transfer. In the present study, we introduce
the chemical knowledge-informed framework (CKIF), a privacy-preserving approach
for learning retrosynthesis models. CKIF enables distributed training across
multiple chemical organizations without compromising the confidentiality of
proprietary reaction data. Instead of gathering raw reaction data, CKIF learns
retrosynthesis models through iterative, chemical knowledge-informed
aggregation of model parameters. In particular, the chemical properties of
predicted reactants are leveraged to quantitatively assess the observable
behaviors of individual models, which in turn determines the adaptive weights
used for model aggregation. On a variety of reaction datasets, CKIF outperforms
several strong baselines by a clear margin.

</details>


### [286] [Training Plug-n-Play Knowledge Modules with Deep Context Distillation](https://arxiv.org/pdf/2503.08727)
*Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vulić, Alessandro Sordoni*

Main category: cs.LG

TL;DR: The paper proposes Knowledge Modules (KMs) for dynamically integrating new information into language models, addressing limitations of in-context learning and RAG. KMs use Deep Context Distillation for training, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in integrating new or evolving information post-pre-training, especially in low-data or private/specialized scenarios, motivate the need for lightweight, modular solutions.

Method: Train document-level Knowledge Modules (KMs) as LoRA modules using Deep Context Distillation, simulating teacher model hidden states and logits.

Result: KMs outperform next-token prediction and pre-instruction training across two datasets, showing synergy with RAG.

Conclusion: KMs offer an effective, lightweight solution for dynamic knowledge integration, with potential synergies with RAG.

Abstract: Dynamically integrating new or rapidly evolving information after (Large)
Language Model pre-training remains challenging, particularly in low-data
scenarios or when dealing with private and specialized documents. In-context
learning and retrieval-augmented generation (RAG) face limitations, including
their high inference costs and their inability to capture global document
information. In this paper, we propose a way of modularizing knowledge by
training document-level Knowledge Modules (KMs). KMs are lightweight components
implemented as parameter-efficient LoRA modules, which are trained to store
information about new documents and can be easily plugged into models on
demand. We show that next-token prediction performs poorly as the training
objective for KMs. We instead propose Deep Context Distillation: we learn KMs
parameters such as to simulate hidden states and logits of a teacher that takes
the document in context. Our method outperforms standard next-token prediction
and pre-instruction training techniques, across two datasets. Finally, we
highlight synergies between KMs and RAG.

</details>


### [287] [TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis](https://arxiv.org/pdf/2505.13033)
*Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam*

Main category: cs.LG

TL;DR: TSPulse is an ultra-compact time-series pre-trained model with 1M parameters, excelling in classification, anomaly detection, imputation, and retrieval tasks through innovative architecture and task-level techniques.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art time-series models are large-scale and computationally expensive, necessitating a more efficient solution.

Method: TSPulse employs dual-space masked reconstruction (time and frequency domains), dual-embedding disentanglement, TSLens for task-specific fine-tuning, multi-head triangulation for anomaly detection, and hybrid mask pretraining for zero-shot imputation.

Result: Achieves significant performance gains: 5-16% on UEA classification, +20% on TSB-AD anomaly detection, +50% in zero-shot imputation, and +25% in retrieval, with only 1M parameters.

Conclusion: TSPulse sets a new standard for efficient time-series pre-trained models, offering high performance with minimal computational resources.

Abstract: The rise of time-series pre-trained models has advanced temporal
representation learning, but current state-of-the-art models are often
large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact
time-series pre-trained models with only 1M parameters, specialized to perform
strongly across classification, anomaly detection, imputation, and retrieval
tasks. TSPulse introduces innovations at both the architecture and task levels.
At the architecture level, it employs a dual-space masked reconstruction,
learning from both time and frequency domains to capture complementary signals.
This is further enhanced by a dual-embedding disentanglement, generating both
detailed embeddings for fine-grained analysis and high-level semantic
embeddings for broader task understanding. Notably, TSPulse's semantic
embeddings are robust to shifts in time, magnitude, and noise, which is
important for robust retrieval. At the task level, TSPulse incorporates TSLens,
a fine-tuning component enabling task-specific feature attention. It also
introduces a multi-head triangulation technique that correlates deviations from
multiple prediction heads, enhancing anomaly detection by fusing complementary
model outputs. Additionally, a hybrid mask pretraining is proposed to improves
zero-shot imputation by reducing pre-training bias. These architecture and task
innovations collectively contribute to TSPulse's significant performance gains:
5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly
detection leaderboard, +50% in zero-shot imputation, and +25% in time-series
retrieval. Remarkably, these results are achieved with just 1M parameters
(10-100X smaller than existing SOTA models) and allow GPU-free inference,
setting a new standard for efficient time-series pre-trained models. The models
can be accessed from
https://huggingface.co/ibm-granite/granite-timeseries-tspulse-r1

</details>


### [288] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/pdf/2506.07744)
*Seungho Baek, Taegeon Park, Jongchan Park, Seungjun Oh, Yusung Kim*

Main category: cs.LG

TL;DR: GAS replaces high-level policy learning with graph search for subgoal selection, improving efficiency and stitching in offline hierarchical reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods degrade with longer task horizons and lack effective transition stitching.

Method: GAS uses Temporal Distance Representation (TDR) for state clustering and a shortest-path algorithm for subgoal selection, enhanced by the Temporal Efficiency (TE) metric.

Result: GAS outperforms prior methods, achieving 88.3 in a critical task vs. the previous 1.0.

Conclusion: GAS is a novel, effective framework for offline hierarchical reinforcement learning.

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [289] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/pdf/2506.16014)
*Jina Kim, Youjin Jang, Jeongjin Han*

Main category: cs.LG

TL;DR: VRAIL is a bi-level RL framework combining deep learning and reward shaping to improve interpretability and learning stability.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and stability in RL by attributing importance to state features and their interactions.

Method: Uses a DL stage to estimate value functions and an RL stage for reward shaping, with linear or quadratic modeling for feature attribution.

Result: Improves training stability and convergence in Taxi-v3, identifies meaningful subgoals, and enhances interpretability.

Conclusion: VRAIL is a model-agnostic framework that boosts learning and interpretability in RL.

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [290] [Backpropagation Through Time For Networks With Long-Term Dependencies](https://arxiv.org/pdf/2103.15589)
*George Bird, Maxim E. Polivoda*

Main category: cs.LG

TL;DR: A new exact method for backpropagation in RNNs using discrete forward sensitivity equations is proposed, addressing long-term dependencies and allowing parameter variation, though it requires Jacobian computation.


<details>
  <summary>Details</summary>
Motivation: Current BPTT methods assume short-term dependencies in RNNs, but as RNNs advance, long-term dependencies will become more influential, necessitating a new approach.

Method: The proposed method uses the 'discrete forward sensitivity equation' and its variant for single and multiple interacting recurrent loops, enabling exact backpropagation with parameter variation.

Result: The solution is exact and accommodates parameter changes between steps, but involves Jacobian computation.

Conclusion: The proposed method addresses the limitations of existing BPTT techniques for advanced RNNs, though it introduces computational complexity with the Jacobian.

Abstract: Backpropagation through time (BPTT) is a technique of updating tuned
parameters within recurrent neural networks (RNNs). Several attempts at
creating such an algorithm have been made including: Nth Ordered Approximations
and Truncated-BPTT. These methods approximate the backpropagation gradients
under the assumption that the RNN only utilises short-term dependencies. This
is an acceptable assumption to make for the current state of artificial neural
networks. As RNNs become more advanced, a shift towards influence by long-term
dependencies is likely. Thus, a new method for backpropagation is required. We
propose using the 'discrete forward sensitivity equation' and a variant of it
for single and multiple interacting recurrent loops respectively. This solution
is exact and also allows the network's parameters to vary between each
subsequent step, however it does require the computation of a Jacobian.

</details>


### [291] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/pdf/2506.16791)
*Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, David Salinas, Frank Hutter*

Main category: cs.LG

TL;DR: TabArena introduces a continuously maintained benchmarking system for tabular data, addressing flaws in static benchmarks by updating datasets, models, and methodologies.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for tabular data are static and outdated, failing to adapt to new models or discovered flaws.

Method: TabArena manually curates datasets and models, conducts large-scale benchmarking, and establishes a maintenance team.

Result: Gradient-boosted trees remain strong, but deep learning and foundation models show promise under certain conditions. Ensembles advance state-of-the-art.

Conclusion: TabArena provides a dynamic, living benchmark with a public leaderboard and maintenance protocols to ensure reliability and relevance.

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [292] [SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2309.05019)
*Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The paper introduces SA-Solver, an efficient stochastic Adams method for solving diffusion SDEs, improving stochastic sampling for high-quality data generation.


<details>
  <summary>Details</summary>
Motivation: Stochastic sampling in Diffusion Probabilistic Models (DPMs) can enhance diversity and quality, but existing methods focus on deterministic ODEs. This work explores stochastic sampling via variance-controlled SDEs and linear multi-step solvers.

Method: Proposes SA-Solver, a stochastic Adams method for solving diffusion SDEs, combining variance control and multi-step techniques.

Result: SA-Solver outperforms or matches SOTA methods in few-step sampling and achieves top FID scores on benchmarks with optimal NFEs.

Conclusion: SA-Solver advances stochastic sampling in DPMs, offering high-quality data generation with efficiency and diversity.

Abstract: Diffusion Probabilistic Models (DPMs) have achieved considerable success in
generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE
or ODE which is time-consuming, numerous fast sampling methods built upon
improved differential equation solvers are proposed. The majority of such
techniques consider solving the diffusion ODE due to its superior efficiency.
However, stochastic sampling could offer additional advantages in generating
diverse and high-quality data. In this work, we engage in a comprehensive
analysis of stochastic sampling from two aspects: variance-controlled diffusion
SDE and linear multi-step SDE solver. Based on our analysis, we propose
\textit{SA-Solver}, which is an improved efficient stochastic Adams method for
solving diffusion SDE to generate data with high quality. Our experiments show
that \textit{SA-Solver} achieves: 1) improved or comparable performance
compared with the existing state-of-the-art (SOTA) sampling methods for
few-step sampling; 2) SOTA FID on substantial benchmark datasets under a
suitable number of function evaluations (NFEs). Code is available at
https://github.com/scxue/SA-Solver.

</details>


### [293] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/pdf/2506.17219)
*Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He*

Main category: cs.LG

TL;DR: RLIF uses intrinsic signals (e.g., token entropy) for post-training LLMs, showing early gains but degrading later, especially for instruction-tuned models.


<details>
  <summary>Details</summary>
Motivation: To explore alternatives to externally supervised methods (RLHF, RLVR) by leveraging intrinsic model-derived signals.

Method: Uses unsupervised reward proxies (token-level entropy, trajectory-level entropy, self-certainty) for Reinforcement Learning from Internal Feedback (RLIF).

Result: RLIF improves reasoning early but degrades later, surpassing RLVR initially but failing for instruction-tuned models.

Conclusion: RLIF has limited long-term benefits, especially for instruction-tuned models, highlighting the need for better intrinsic feedback strategies.

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


### [294] [Counterfactual Fairness through Transforming Data Orthogonal to Bias](https://arxiv.org/pdf/2403.17852)
*Shuyi Chen, Shixiang Zhu*

Main category: cs.LG

TL;DR: The paper introduces the Orthogonal to Bias (OB) algorithm to ensure counterfactual fairness in machine learning by eliminating the influence of continuous sensitive variables.


<details>
  <summary>Details</summary>
Motivation: Machine learning models can exhibit biased decision-making, especially with multivariate and continuous sensitive variables, which current methods inadequately address.

Method: The OB algorithm, based on a jointly normal distribution within a structural causal model, ensures data orthogonality to sensitive variables. It includes a sparse variant for numerical stability.

Result: Empirical evaluations show OB effectively promotes fairness without compromising accuracy in both simulated and real-world datasets.

Conclusion: The OB algorithm is a model-agnostic solution for achieving counterfactual fairness in machine learning, applicable across various tasks.

Abstract: Machine learning models have shown exceptional prowess in solving complex
issues across various domains. However, these models can sometimes exhibit
biased decision-making, resulting in unequal treatment of different groups.
Despite substantial research on counterfactual fairness, methods to reduce the
impact of multivariate and continuous sensitive variables on decision-making
outcomes are still underdeveloped. We propose a novel data pre-processing
algorithm, Orthogonal to Bias (OB), which is designed to eliminate the
influence of a group of continuous sensitive variables, thus promoting
counterfactual fairness in machine learning applications. Our approach, based
on the assumption of a jointly normal distribution within a structural causal
model (SCM), demonstrates that counterfactual fairness can be achieved by
ensuring the data is orthogonal to the observed sensitive variables. The OB
algorithm is model-agnostic, making it applicable to a wide range of machine
learning models and tasks. Additionally, it includes a sparse variant to
improve numerical stability through regularization. Empirical evaluations on
both simulated and real-world datasets, encompassing settings with both
discrete and continuous sensitive variables, show that our methodology
effectively promotes fairer outcomes without compromising accuracy.

</details>


### [295] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/pdf/2506.17253)
*Chenghan Li, Mingchen Li, Yipu Liao, Ruisheng Diao*

Main category: cs.LG

TL;DR: The paper introduces MS-TVNet, a multi-scale 3D CNN for long-term time series prediction, outperforming Transformer and MLP models.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of convolutional networks in long-term time series prediction.

Method: Proposes a multi-scale time series reshape module and MS-TVNet, a 3D dynamic CNN.

Result: MS-TVNet achieves SOTA performance on diverse datasets.

Conclusion: Convolutional networks are effective for capturing complex temporal patterns, offering a promising research direction.

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [296] [Bridging the Gap Between Approximation and Learning via Optimal Approximation by ReLU MLPs of Maximal Regularity](https://arxiv.org/pdf/2409.12335)
*Ruiyang Hong, Anastasis Kratsios*

Main category: cs.LG

TL;DR: The paper identifies a class of ReLU MLPs that are both universal approximators and statistically reliable, achieving optimal function approximation and generalization.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between expressive but non-generalizing models and generalizing but constrained models, inspired by real-world deep learning implementations.

Method: Constructs a sparsely connected ReLU MLP class with specific width, depth, and parameter constraints, using Kuhn triangulations to fit linear pieces.

Result: The MLPs approximate Hölder functions with uniform error, achieve near-optimal sample complexity, and solve the McShane extension problem.

Conclusion: Neural networks can be both expressive and statistically reliable, as demonstrated by the constructed MLP class.

Abstract: The foundations of deep learning are supported by the seemingly opposing
perspectives of approximation or learning theory. The former advocates for
large/expressive models that need not generalize, while the latter considers
classes that generalize but may be too small/constrained to be universal
approximators. Motivated by real-world deep learning implementations that are
both expressive and statistically reliable, we ask: "Is there a class of neural
networks that is both large enough to be universal but structured enough to
generalize?" This paper constructively provides a positive answer to this
question by identifying a highly structured class of ReLU multilayer
perceptions (MLPs), which are optimal function approximators and are
statistically well-behaved. We show that any $(L,\alpha)$-H\"{o}lder function
from $[0,1]^d$ to $[-n,n]$ can be approximated to a uniform $\mathcal{O}(1/n)$
error on $[0,1]^d$ with a sparsely connected ReLU MLP with the same H\"{o}lder
exponent $\alpha$ and coefficient $L$, of width $\mathcal{O}(dn^{d/\alpha})$,
depth $\mathcal{O}(\log(d))$, with $\mathcal{O}(dn^{d/\alpha})$ nonzero
parameters, and whose weights and biases take values in $\{0,\pm 1/2\}$ except
in the first and last layers which instead have magnitude at-most $n$. Further,
our class of MLPs achieves a near-optimal sample complexity of
$\mathcal{O}(\log(N)/\sqrt{N})$ when given $N$ i.i.d. normalized sub-Gaussian
training samples. We achieve this through a new construction that perfectly
fits together linear pieces using Kuhn triangulations, along with a new proof
technique which shows that our construction preserves the regularity of not
only the H\"{o}lder functions, but also any uniformly continuous function. Our
results imply that neural networks can solve the McShane extension problem on
suitable finite sets.

</details>


### [297] [Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes](https://arxiv.org/pdf/2410.02145)
*Erica Zhang, Fangzhao Zhang, Mert Pilanci*

Main category: cs.LG

TL;DR: The paper introduces a gradient-free cutting-plane training method for ReLU networks, extending it to deep neural networks and achieving convergence guarantees for active learning.


<details>
  <summary>Details</summary>
Motivation: To improve sample complexity in machine learning by developing a novel active learning scheme for deep neural networks.

Method: A gradient-free cutting-plane training method for ReLU networks, adapted from linear models to handle nonconvexity and nonlinear decision boundaries.

Result: Demonstrates geometric contraction of the feasible set and outperforms popular deep active learning baselines in synthetic and real-world tasks.

Conclusion: The proposed method is the first deep active learning scheme with convergence guarantees, showing promise for practical applications.

Abstract: Active learning methods aim to improve sample complexity in machine learning.
In this work, we investigate an active learning scheme via a novel
gradient-free cutting-plane training method for ReLU networks of arbitrary
depth and develop a convergence theory. We demonstrate, for the first time,
that cutting-plane algorithms, traditionally used in linear models, can be
extended to deep neural networks despite their nonconvexity and nonlinear
decision boundaries. Moreover, this training method induces the first deep
active learning scheme known to achieve convergence guarantees, revealing a
geometric contraction rate of the feasible set. We exemplify the effectiveness
of our proposed active learning method against popular deep active learning
baselines via both synthetic data experiments and sentimental classification
task on real datasets.

</details>


### [298] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/pdf/2506.18165)
*Jaemoo Choi, Yongxin Chen, Molei Tao, Guan-Horng Liu*

Main category: cs.LG

TL;DR: NAAS is a new SOC-based diffusion sampler that avoids importance sampling, using annealed reference dynamics for efficient and scalable training.


<details>
  <summary>Details</summary>
Motivation: Existing annealing methods suffer from high variance and scalability issues due to reliance on importance sampling.

Method: NAAS leverages annealed reference dynamics and a lean adjoint system for training, avoiding importance sampling.

Result: NAAS effectively samples from classical energy landscapes and molecular Boltzmann distributions.

Conclusion: NAAS offers a scalable and efficient alternative to traditional annealing-based diffusion samplers.

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [299] [Bilinear MLPs enable weight-based mechanistic interpretability](https://arxiv.org/pdf/2410.08417)
*Michael T. Pearce, Thomas Dooms, Alice Rigg, Jose M. Oramas, Lee Sharkey*

Main category: cs.LG

TL;DR: The paper introduces bilinear MLPs, a variant of Gated Linear Units (GLUs) without element-wise nonlinearities, to improve interpretability of MLP computations. It demonstrates their competitive performance and uses tensor analysis to reveal interpretable weight structures.


<details>
  <summary>Details</summary>
Motivation: Understanding how MLPs compute in deep neural networks is challenging due to nonlinearities obscuring weight interactions. The paper aims to provide a clearer mechanistic understanding by analyzing bilinear MLPs.

Method: The study replaces traditional MLP activation functions with bilinear MLPs, which avoid element-wise nonlinearities. It uses third-order tensor representations and eigendecomposition to analyze weight spectra.

Result: Bilinear MLPs achieve competitive performance while enabling interpretable weight analysis. The method reveals low-rank structures, aids in crafting adversarial examples, and identifies model circuits directly from weights.

Conclusion: Bilinear MLPs offer an interpretable alternative to traditional activation functions, proving weight-based interpretability is feasible for deep-learning models.

Abstract: A mechanistic understanding of how MLPs do computation in deep neural
networks remains elusive. Current interpretability work can extract features
from hidden activations over an input dataset but generally cannot explain how
MLP weights construct features. One challenge is that element-wise
nonlinearities introduce higher-order interactions and make it difficult to
trace computations through the MLP layer. In this paper, we analyze bilinear
MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity
that nevertheless achieves competitive performance. Bilinear MLPs can be fully
expressed in terms of linear operations using a third-order tensor, allowing
flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights
using eigendecomposition reveals interpretable low-rank structure across toy
tasks, image classification, and language modeling. We use this understanding
to craft adversarial examples, uncover overfitting, and identify small language
model circuits directly from the weights alone. Our results demonstrate that
bilinear layers serve as an interpretable drop-in replacement for current
activation functions and that weight-based interpretability is viable for
understanding deep-learning models.

</details>


### [300] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/pdf/2506.18240)
*Wenxin Li, Chuan Wang, Hongdong Zhu, Qi Gao, Yin Ma, Hai Wei, Kai Wen*

Main category: cs.LG

TL;DR: A novel Quadratic Binary Optimization (QBO) model for quantized neural network training is introduced, using Forward Interval Propagation (FIP) to handle non-linearity and multi-layer structures. The method enables quantum computing optimization, with theoretical bounds on error and Ising spins. Quantum Conditional Gradient Descent (QCGD) is used to solve the QCBO problem, achieving high accuracy on Fashion MNIST.


<details>
  <summary>Details</summary>
Motivation: To enable quantized neural network training with arbitrary activation and loss functions using quantum computers, addressing challenges of non-linearity and multi-layer structures.

Method: Forward Interval Propagation (FIP) discretizes activation functions into linear subintervals. Quantum Conditional Gradient Descent (QCGD) solves the QCBO problem with theoretical convergence guarantees.

Result: Theoretical bounds on approximation error and Ising spins. Experimental results show 94.95% accuracy on Fashion MNIST with 1.1-bit precision.

Conclusion: The QBO model and QCGD algorithm successfully enable quantized neural network training on quantum computers, demonstrating practical applicability and high accuracy.

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [301] [Federated Learning Clients Clustering with Adaptation to Data Drifts](https://arxiv.org/pdf/2411.01580)
*Minghao Li, Dmitrii Avdiukhin, Rana Shahout, Nikita Ivkin, Vladimir Braverman, Minlan Yu*

Main category: cs.LG

TL;DR: FIELDING is a clustered FL framework addressing data drift by detecting drift at clients and re-clustering selectively, improving accuracy and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Client heterogeneity and data drift in FL degrade model performance, requiring adaptive solutions.

Method: FIELDING detects drift at clients and performs selective re-clustering to maintain cluster quality and model performance.

Result: FIELDING improves accuracy by 1.9-5.9% and achieves target accuracy 1.16x-2.23x faster than existing methods.

Conclusion: FIELDING effectively handles data drift in FL, enhancing model accuracy and convergence speed.

Abstract: Federated Learning (FL) trains deep models across edge devices without
centralizing raw data, preserving user privacy. However, client heterogeneity
slows down convergence and limits global model accuracy. Clustered FL (CFL)
mitigates this by grouping clients with similar representations and training a
separate model for each cluster. In practice, client data evolves over time, a
phenomenon we refer to as data drift, which breaks cluster homogeneity and
degrades performance. Data drift can take different forms depending on whether
changes occur in the output values, the input features, or the relationship
between them. We propose FIELDING, a CFL framework for handling diverse types
of data drift with low overhead. FIELDING detects drift at individual clients
and performs selective re-clustering to balance cluster quality and model
performance, while remaining robust to malicious clients and varying levels of
heterogeneity. Experiments show that FIELDING improves final model accuracy by
1.9-5.9% and achieves target accuracy 1.16x-2.23x faster than existing
state-of-the-art CFL methods.

</details>


### [302] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/pdf/2505.24190)
*Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than*

Main category: cs.LG

TL;DR: A theoretical framework addresses the performance gap in few-shot image classification caused by synthetic data, proposing a novel algorithm that outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Few-shot image classification struggles with limited labeled data, and synthetic data augmentation often degrades performance due to distribution gaps.

Method: Developed a theoretical framework to quantify distribution discrepancies, suggesting practical synthetic sample generation and training methods. Proposed a prototype-learning-based algorithm to optimize data partitioning and model training.

Result: The approach outperforms state-of-the-art methods across multiple datasets.

Conclusion: The framework and algorithm effectively bridge the gap between real and synthetic data, improving few-shot classification performance.

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [303] [Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective](https://arxiv.org/pdf/2502.02719)
*Steve Azzolin, Sagar Malhotra, Andrea Passerini, Stefano Teso*

Main category: cs.LG

TL;DR: SE-GNNs' explanations (MEs) are formalized and compared to PI and faithful explanations. MEs match PIs for some tasks but are less informative and misaligned with faithfulness. Dual-Channel GNNs are proposed to address limitations, combining rule extraction with SE-GNNs, showing promising results.


<details>
  <summary>Details</summary>
Motivation: To understand and address the limitations of explanations provided by SE-GNNs, particularly their misalignment with faithfulness and informativeness compared to PI explanations.

Method: Formalize MEs, compare them to PI and faithful explanations, and propose Dual-Channel GNNs integrating rule extraction with SE-GNNs.

Result: MEs match PIs for certain tasks but are generally less informative and unfaithful. Dual-Channel GNNs recover succinct rules and perform comparably or better than SE-GNNs.

Conclusion: Dual-Channel GNNs offer a viable solution to SE-GNNs' limitations, balancing explanation quality and tractability.

Abstract: Self-Explainable Graph Neural Networks (SE-GNNs) are popular
explainable-by-design GNNs, but their explanations' properties and limitations
are not well understood. Our first contribution fills this gap by formalizing
the explanations extracted by some popular SE-GNNs, referred to as Minimal
Explanations (MEs), and comparing them to established notions of explanations,
namely Prime Implicant (PI) and faithful explanations. Our analysis reveals
that MEs match PI explanations for a restricted but significant family of
tasks. In general, however, they can be less informative than PI explanations
and are surprisingly misaligned with widely accepted notions of faithfulness.
Although faithful and PI explanations are informative, they are intractable to
find and we show that they can be prohibitively large. Given these
observations, a natural choice is to augment SE-GNNs with alternative
modalities of explanations taking care of SE-GNNs' limitations. To this end, we
propose Dual-Channel GNNs that integrate a white-box rule extractor and a
standard SE-GNN, adaptively combining both channels. Our experiments show that
even a simple instantiation of Dual-Channel GNNs can recover succinct rules and
perform on par or better than widely used SE-GNNs.

</details>


### [304] [Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data](https://arxiv.org/pdf/2502.10381)
*Corinna Cortes, Anqi Mao, Mehryar Mohri, Yutao Zhong*

Main category: cs.LG

TL;DR: The paper addresses class imbalance in machine learning, proposing a new theoretical framework and a class-imbalanced margin loss function with strong consistency guarantees, leading to the IMMAX algorithm.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in multi-class problems lacks solid theoretical foundations, and existing methods like cost-sensitive techniques are not Bayes-consistent.

Method: Introduces a theoretical framework for imbalanced classification, proposes a class-imbalanced margin loss function, and develops the IMMAX algorithm with learning guarantees.

Result: The proposed method demonstrates strong consistency and outperforms existing baselines empirically.

Conclusion: The paper provides a robust theoretical and practical solution for imbalanced classification, validated by empirical results.

Abstract: Class imbalance remains a major challenge in machine learning, especially in
multi-class problems with long-tailed distributions. Existing methods, such as
data resampling, cost-sensitive techniques, and logistic loss modifications,
though popular and often effective, lack solid theoretical foundations. As an
example, we demonstrate that cost-sensitive methods are not Bayes-consistent.
This paper introduces a novel theoretical framework for analyzing
generalization in imbalanced classification. We then propose a new
class-imbalanced margin loss function for both binary and multi-class settings,
prove its strong $H$-consistency, and derive corresponding learning guarantees
based on empirical loss and a new notion of class-sensitive Rademacher
complexity. Leveraging these theoretical results, we devise novel and general
learning algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate
confidence margins and are applicable to various hypothesis sets. While our
focus is theoretical, we also present extensive empirical results demonstrating
the effectiveness of our algorithms compared to existing baselines.

</details>


### [305] [Follow-the-Perturbed-Leader Approaches Best-of-Both-Worlds for the m-Set Semi-Bandit Problems](https://arxiv.org/pdf/2504.07307)
*Jingxin Zhan, Yuchen Xin, Chenjie Sun, Zhihua Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a common case of the combinatorial semi-bandit problem, the
$m$-set semi-bandit, where the learner exactly selects $m$ arms from the total
$d$ arms. In the adversarial setting, the best regret bound, known to be
$\mathcal{O}(\sqrt{nmd})$ for time horizon $n$, is achieved by the well-known
Follow-the-Regularized-Leader (FTRL) policy. However, this requires to
explicitly compute the arm-selection probabilities via optimizing problems at
each time step and sample according to them. This problem can be avoided by the
Follow-the-Perturbed-Leader (FTPL) policy, which simply pulls the $m$ arms that
rank among the $m$ smallest (estimated) loss with random perturbation. In this
paper, we show that FTPL with a Fr\'echet perturbation also enjoys the near
optimal regret bound $\mathcal{O}(\sqrt{nm}(\sqrt{d\log(d)}+m^{5/6}))$ in the
adversarial setting and approaches best-of-both-world regret bounds, i.e.,
achieves a logarithmic regret for the stochastic setting. Moreover, our lower
bounds show that the extra factors are unavoidable with our approach; any
improvement would require a fundamentally different and more challenging
method.

</details>


### [306] [Proofs as Explanations: Short Certificates for Reliable Predictions](https://arxiv.org/pdf/2504.08377)
*Avrim Blum, Steve Hanneke, Chirag Pabbaraju, Donya Saless*

Main category: cs.LG

TL;DR: The paper proposes a model for explainable AI where explanations are subsets of training data proving predictions under assumptions of hypothesis class realizability and bounded corruption. It generalizes this for any hypothesis class and corruption bound, introducing the robust hollow star number to characterize certificate size.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous, data-driven explanations for AI predictions under realistic assumptions of bounded errors and hypothesis class realizability.

Method: Defines the robust hollow star number for hypothesis classes, analyzes worst-case and distributional bounds on certificate size, and introduces the certificate coefficient for sample size analysis.

Result: The robust hollow star number precisely characterizes the smallest achievable certificate size. Matching bounds on sample size are proven for the certificate coefficient.

Conclusion: The framework offers a principled way to generate and analyze explanations for AI predictions, with theoretical guarantees on their correctness and size.

Abstract: We consider a model for explainable AI in which an explanation for a
prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it
exists) such that all classifiers $h' \in H$ that make at most $b$ mistakes on
$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has
label $y$ under the assumption that (1) the target function $h^\star$ belongs
to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,
if $b=0$ and $H$ is the family of linear classifiers in $\mathbb{R}^d$, and if
$x$ lies inside the convex hull of the positive data points in $S$ (and hence
every consistent linear classifier labels $x$ as positive), then
Carath\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$
of those points. So, a set $S'$ of size $d+1$ could be released as an
explanation for a positive prediction, and would serve as a short proof of
correctness of the prediction under the assumption of realizability.
  In this work, we consider this problem more generally, for general hypothesis
classes $H$ and general values $b\geq 0$. We define the notion of the robust
hollow star number of $H$ (which generalizes the standard hollow star number),
and show that it precisely characterizes the worst-case size of the smallest
certificate achievable, and analyze its size for natural classes. We also
consider worst-case distributional bounds on certificate size, as well as
distribution-dependent bounds that we show tightly control the sample size
needed to get a certificate for any given test example. In particular, we
define a notion of the certificate coefficient $\varepsilon_x$ of an example
$x$ with respect to a data distribution $D$ and target function $h^\star$, and
prove matching upper and lower bounds on sample size as a function of
$\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.

</details>


### [307] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/pdf/2504.21662)
*Mauricio Ortiz Torres, Markus Lange, Arne P. Raulf*

Main category: cs.LG

TL;DR: The Forward-Forward algorithm has been enhanced for complex tasks, achieving a 20% test error reduction on CIFAR10 and enabling lightweight models for low-capacity hardware.


<details>
  <summary>Details</summary>
Motivation: To improve the Forward-Forward algorithm for real-life applications and low-capacity hardware.

Method: Combines convolutional channel grouping, learning rate schedules, and independent block structures.

Result: 20% decrease in test error; lightweight models achieve 21±3% test error with 164,706 to 754,386 parameters.

Conclusion: Sets a foundation for future verification and validation of such neural networks.

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects, we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$3)\% and number of trainable parameters between 164,706 and
754,386. This serves as a basis for our future study on complete verification
and validation of these kinds of neural networks.

</details>


### [308] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/pdf/2505.04396)
*Jingnan Wang, Jie Chao, Shangshang Yang, Congyi Nai, Kaijun Ren, Kefeng Deng, Xi Chen, Yaxin Liu, Hanqiuzi Wen, Ziniu Xiao, Lifeng Zhang, Xiaodong Wang, Jiping Guan, Baoxiang Pan*

Main category: cs.LG

TL;DR: A method combining learned high-resolution climatological data with coarse-grid forecasts improves wind power weather predictions, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate, high-resolution weather forecasts are critical for renewable energy planning, but traditional downscaling methods face challenges like scale inconsistency and high computational costs.

Method: Learns climatological distribution from high-resolution simulations and optimally combines it with coarse-grid forecasts to produce fine-grained, ensemble weather forecasts.

Result: Outperforms existing methods in accuracy and efficiency, with a 100-member, 10-day forecast taking <1 hour on a GPU compared to thousands of CPU hours traditionally.

Conclusion: The method enables more efficient and reliable renewable energy planning by reducing computational costs without sacrificing accuracy.

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [309] [Log-Linear Attention](https://arxiv.org/pdf/2506.04761)
*Han Guo, Songlin Yang, Tarushii Goel, Eric P. Xing, Tri Dao, Yoon Kim*

Main category: cs.LG

TL;DR: Log-linear attention balances efficiency and expressiveness by replacing fixed-size hidden states with logarithmically growing ones, enabling log-linear compute cost.


<details>
  <summary>Details</summary>
Motivation: Address the bottlenecks of quadratic-compute and linear-memory in Transformers while overcoming the limitations of fixed-size hidden states in RNN-like models.

Method: Develops log-linear attention, a mechanism with logarithmically growing hidden states, and applies it to existing linear attention variants like Mamba-2 and Gated DeltaNet.

Result: Log-linear attention variants perform well compared to linear-time models, maintaining efficiency and expressiveness.

Conclusion: Log-linear attention offers a scalable and efficient alternative to traditional attention mechanisms, improving sequence modeling.

Abstract: The attention mechanism in Transformers is an important primitive for
accurate and scalable sequence modeling. Its quadratic-compute and
linear-memory complexity however remain significant bottlenecks. Linear
attention and state-space models enable linear-time, constant-memory sequence
modeling and can moreover be trained efficiently through matmul-rich
parallelization across sequence length. However, at their core these models are
still RNNs, and thus their use of a fixed-size hidden state to model the
context is a fundamental limitation. This paper develops log-linear attention,
an attention mechanism that balances linear attention's efficiency and the
expressiveness of softmax attention. Log-linear attention replaces the
fixed-size hidden state with a logarithmically growing set of hidden states. We
show that with a particular growth function, log-linear attention admits a
similarly matmul-rich parallel form whose compute cost is log-linear in
sequence length. Log-linear attention is a general framework and can be applied
on top of existing linear attention variants. As case studies, we instantiate
log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet
-- and find they perform well compared to their linear-time variants.

</details>


### [310] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/pdf/2506.06300)
*Yuanye Zhou, Zhaokun Wang, Kai Zhou, Hui Tang, Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs improve topology optimization by eliminating manual interpolation and enabling precise boundary determination, outperforming conventional density-based PINNs.


<details>
  <summary>Details</summary>
Motivation: Conventional PINNs rely on density-based topology descriptions, which require manual interpolation and limit applicability to complex geometries.

Method: Proposes LT-PINNs, parameterizing control variables of topology boundary curves as learnable parameters and introducing specialized loss functions for boundary accuracy.

Result: LT-PINNs reduce relative L2 errors, handle arbitrary boundary conditions, and infer clear topology boundaries without manual interpolation.

Conclusion: LT-PINNs offer a robust, boundary-focused framework for engineering optimization, applicable to a wide range of PDEs and complex topologies.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [311] [Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning](https://arxiv.org/pdf/2506.18847)
*Anthony Kobanda, Waris Radji, Mathieu Petitbois, Odalric-Ambrym Maillard, Rémy Portelas*

Main category: cs.LG

TL;DR: ProQ introduces a compositional framework for long-horizon goal-reaching tasks by leveraging asymmetric distance learning, keypoint coverage, and goal-conditioned control, ensuring robust performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of compounding value-estimation errors in scaling offline goal-conditioned reinforcement learning to long-horizon tasks.

Method: ProQ learns an asymmetric distance, uses it for keypoint coverage and directional cost, and integrates a Lagrangian out-of-distribution detector for reachability.

Result: ProQ produces meaningful sub-goals and achieves robust long-horizon goal-reaching on diverse navigation benchmarks.

Conclusion: ProQ effectively unifies metric learning, keypoint coverage, and control, offering a scalable solution for long-horizon tasks.

Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to
reach specified goals from previously collected trajectories. Scaling that
promises to long-horizon tasks remains challenging, notably due to compounding
value-estimation errors. Principled geometric offers a potential solution to
address these issues. Following this insight, we introduce Projective
Quasimetric Planning (ProQ), a compositional framework that learns an
asymmetric distance and then repurposes it, firstly as a repulsive energy
forcing a sparse set of keypoints to uniformly spread over the learned latent
space, and secondly as a structured directional cost guiding towards proximal
sub-goals. In particular, ProQ couples this geometry with a Lagrangian
out-of-distribution detector to ensure the learned keypoints stay within
reachable areas. By unifying metric learning, keypoint coverage, and
goal-conditioned control, our approach produces meaningful sub-goals and
robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [312] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2506.20039)
*Koorosh Moslemi, Chi-Guhn Lee*

Main category: cs.MA

TL;DR: A framework for learning two-sided team formation in dynamic multi-agent systems is introduced, addressing gaps in existing MARL studies.


<details>
  <summary>Details</summary>
Motivation: Existing MARL studies focus on unilateral or predefined teams, leaving bilateral grouping in dynamic populations underexplored.

Method: A framework for learning two-sided team formation in dynamic multi-agent systems is proposed.

Result: The approach shows competitive performance and improved generalization in widely adopted multi-agent scenarios.

Conclusion: The study provides insights into algorithmic properties influencing policy performance and generalization in bilateral team formation.

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


### [313] [A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem](https://arxiv.org/pdf/2506.20400)
*Kristoffer Christensen, Bo Nørregaard Jørgensen, Zheng Grace Ma*

Main category: cs.MA

TL;DR: A Python-based dashboard framework for analyzing multi-agent simulations of EV home charging, enabling root-cause analysis of system-level anomalies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting and explaining unexpected events in complex EV charging simulations.

Method: Developed a modular dashboard using Dash by Plotly with three coordinated views for multi-level exploration.

Result: Demonstrated effectiveness in identifying anomalies like transformer overloads in a Danish residential network case study.

Conclusion: The framework provides actionable insights and is adaptable to other energy systems.

Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging
ecosystems generate large, complex, and stochastic time-series datasets that
capture interactions between households, grid infrastructure, and energy
markets. These interactions can lead to unexpected system-level events, such as
transformer overloads or consumer dissatisfaction, that are difficult to detect
and explain through static post-processing. This paper presents a modular,
Python-based dashboard framework, built using Dash by Plotly, that enables
efficient, multi-level exploration and root-cause analysis of emergent behavior
in MABS outputs. The system features three coordinated views (System Overview,
System Analysis, and Consumer Analysis), each offering high-resolution
visualizations such as time-series plots, spatial heatmaps, and agent-specific
drill-down tools. A case study simulating full EV adoption with smart charging
in a Danish residential network demonstrates how the dashboard supports rapid
identification and contextual explanation of anomalies, including clustered
transformer overloads and time-dependent charging failures. The framework
facilitates actionable insight generation for researchers and distribution
system operators, and its architecture is adaptable to other distributed energy
resources and complex energy systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [314] [EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations](https://arxiv.org/pdf/2505.23018)
*Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin*

Main category: cs.MM

TL;DR: The paper introduces EmotionTalk, a high-quality Chinese multimodal emotion dataset with rich annotations, addressing the lack of such resources for Chinese language and culture.


<details>
  <summary>Details</summary>
Motivation: There's a need for comprehensive datasets for Chinese emotion recognition due to unique linguistic and cultural characteristics, which existing English datasets don't address.

Method: The authors created EmotionTalk, a dataset with multimodal data (acoustic, visual, textual) from 19 actors in dyadic conversations, annotated for emotions, sentiment, and speech captions.

Result: EmotionTalk includes 23.6 hours of speech, 19,250 utterances, and detailed annotations, making it suitable for various emotion recognition tasks.

Conclusion: EmotionTalk is the first versatile Chinese multimodal emotion dataset, valuable for cross-cultural research, and will be open-sourced for academic use.

Abstract: In recent years, emotion recognition plays a critical role in applications
such as human-computer interaction, mental health monitoring, and sentiment
analysis. While datasets for emotion analysis in languages such as English have
proliferated, there remains a pressing need for high-quality, comprehensive
datasets tailored to the unique linguistic, cultural, and multimodal
characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an
interactive Chinese multimodal emotion dataset with rich annotations. This
dataset provides multimodal information from 19 actors participating in dyadic
conversational settings, incorporating acoustic, visual, and textual
modalities. It includes 23.6 hours of speech (19,250 utterances), annotations
for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,
fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,
neutral, weakly positive, and positive) and 4-dimensional speech captions
(speaker, speaking style, emotion and overall). The dataset is well-suited for
research on unimodal and multimodal emotion recognition, missing modality
challenges, and speech captioning tasks. To our knowledge, it represents the
first high-quality and versatile Chinese dialogue multimodal emotion dataset,
which is a valuable contribution to research on cross-cultural emotion analysis
and recognition. Additionally, we conduct experiments on EmotionTalk to
demonstrate the effectiveness and quality of the dataset. It will be
open-source and freely available for all academic purposes. The dataset and
codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [315] [Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers](https://arxiv.org/pdf/2506.19875)
*Taous Iatariene, Can Cui, Alexandre Guérin, Romain Serizel*

Main category: eess.AS

TL;DR: The paper proposes using speaker embeddings for identity reassignment in speaker tracking, improving performance in scenarios with intermittent and moving speakers.


<details>
  <summary>Details</summary>
Motivation: Current speaker tracking methods struggle with discontinuous spatial trajectories caused by intermittent and moving speakers.

Method: The method involves post-tracking identity reassignment using speaker embeddings, leveraging trajectory-related information and beamforming-enhanced audio signals.

Result: The proposed method improves identity assignment performance for neural and standard tracking systems, especially when speakers change positions during inactivity.

Conclusion: Speaker embeddings, combined with beamforming, effectively address identity reassignment challenges in dynamic speaker tracking scenarios.

Abstract: Speaker tracking methods often rely on spatial observations to assign
coherent track identities over time. This raises limits in scenarios with
intermittent and moving speakers, i.e., speakers that may change position when
they are inactive, thus leading to discontinuous spatial trajectories. This
paper proposes to investigate the use of speaker embeddings, in a simple
solution to this issue. We propose to perform identity reassignment
post-tracking, using speaker embeddings. We leverage trajectory-related
information provided by an initial tracking step and multichannel audio signal.
Beamforming is used to enhance the signal towards the speakers' positions in
order to compute speaker embeddings. These are then used to assign new track
identities based on an enrollment pool. We evaluate the performance of the
proposed speaker embedding-based identity reassignment method on a dataset
where speakers change position during inactivity periods. Results show that it
consistently improves the identity assignment performance of neural and
standard tracking systems. In particular, we study the impact of beamforming
and input duration for embedding extraction.

</details>


### [316] [MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition](https://arxiv.org/pdf/2506.19887)
*Hyo Jin Jon, Longbin Jin, Hyuntaek Jung, Hyunseo Kim, Donghun Min, Eun Yi Kim*

Main category: eess.AS

TL;DR: The paper introduces MATER, a hierarchical framework for speech emotion recognition, combining acoustic and textual features at multiple levels. It also uses an uncertainty-aware ensemble to handle annotator inconsistencies, achieving competitive results in the SERNC Challenge.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of natural speech emotion recognition, including variability and annotator inconsistencies, by integrating multi-level acoustic and textual features.

Method: Proposes MATER, a hierarchical framework fusing acoustic and textual features at word, utterance, and embedding levels, and introduces an uncertainty-aware ensemble strategy.

Result: MATER ranks fourth in categorical emotion recognition (Macro-F1: 41.01%) and second in valence prediction (CCC: 0.6941).

Conclusion: MATER effectively captures emotional nuances in natural speech and improves robustness, demonstrating strong performance in the SERNC Challenge.

Abstract: This paper presents our contributions to the Speech Emotion Recognition in
Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion
recognition and emotional attribute prediction. To handle the complexities of
natural speech, including intra- and inter-subject variability, we propose
Multi-level Acoustic-Textual Emotion Representation (MATER), a novel
hierarchical framework that integrates acoustic and textual features at the
word, utterance, and embedding levels. By fusing low-level lexical and acoustic
cues with high-level contextualized representations, MATER effectively captures
both fine-grained prosodic variations and semantic nuances. Additionally, we
introduce an uncertainty-aware ensemble strategy to mitigate annotator
inconsistencies, improving robustness in ambiguous emotional expressions. MATER
ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of
0.5928, securing second place in valence prediction with an impressive CCC of
0.6941.

</details>


### [317] [An Exploration of ECAPA-TDNN and x-vector Speaker Representations in Zero-shot Multi-speaker TTS](https://arxiv.org/pdf/2506.20190)
*Marie Kunešová, Zdeněk Hanzlíček, Jindřich Matoušek*

Main category: eess.AS

TL;DR: The paper compares three speaker encoders (H/ASP, x-vector, ECAPA-TDNN) in zero-shot TTS, finding H/ASP outperforms others in speaker similarity.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of different speaker embeddings (developed for speaker recognition) in zero-shot TTS, which remains underexplored.

Method: Used a YourTTS-based TTS system to compare H/ASP, x-vector, and ECAPA-TDNN embeddings on Czech read speech, evaluated subjectively (listening test) and objectively (cosine distances).

Result: H/ASP encoder consistently outperformed ECAPA-TDNN and x-vectors in speaker similarity, despite ECAPA-TDNN's popularity in speaker recognition.

Conclusion: Empirical evaluation is crucial when reusing speaker recognition embeddings in TTS; H/ASP is more effective for zero-shot TTS in this setup.

Abstract: Zero-shot multi-speaker text-to-speech (TTS) systems rely on speaker
embeddings to synthesize speech in the voice of an unseen speaker, using only a
short reference utterance. While many speaker embeddings have been developed
for speaker recognition, their relative effectiveness in zero-shot TTS remains
underexplored. In this work, we employ a YourTTS-based TTS system to compare
three different speaker encoders - YourTTS's original H/ASP encoder, x-vector
embeddings, and ECAPA-TDNN embeddings - within an otherwise fixed zero-shot TTS
framework. All models were trained on the same dataset of Czech read speech and
evaluated on 24 out-of-domain target speakers using both subjective and
objective methods. The subjective evaluation was conducted via a listening test
focused on speaker similarity, while the objective evaluation measured cosine
distances between speaker embeddings extracted from synthesized and real
utterances. Across both evaluations, the original H/ASP encoder consistently
outperformed the alternatives, with ECAPA-TDNN showing better results than
x-vectors. These findings suggest that, despite the popularity of ECAPA-TDNN in
speaker recognition, it does not necessarily offer improvements for speaker
similarity in zero-shot TTS in this configuration. Our study highlights the
importance of empirical evaluation when reusing speaker recognition embeddings
in TTS and provides a framework for additional future comparisons.

</details>


### [318] [Improved Topology-Independent Distributed Adaptive Node-Specific Signal Estimation for Wireless Acoustic Sensor Networks](https://arxiv.org/pdf/2506.20001)
*Paul Didier, Toon van Waterschoot, Simon Doclo, Jörg Bitzer, Marc Moonen*

Main category: eess.AS

TL;DR: The paper introduces TI-DANSE+, an improved algorithm for topology-independent distributed adaptive node-specific signal estimation in wireless acoustic sensor networks, offering faster convergence and reduced power usage compared to TI-DANSE.


<details>
  <summary>Details</summary>
Motivation: The slow convergence of the existing TI-DANSE algorithm in non-fully connected networks limits its practicality, prompting the need for a more efficient solution.

Method: TI-DANSE+ allows nodes to exploit partial in-network sums from neighbors during updates, increasing degrees of freedom and accelerating convergence. A tree-pruning strategy is also introduced to further speed up convergence.

Result: TI-DANSE+ matches the convergence speed of DANSE in fully connected networks while using less transmit power, as shown in simulations.

Conclusion: TI-DANSE+ effectively addresses the limitations of TI-DANSE, offering faster convergence and improved efficiency in wireless acoustic sensor networks.

Abstract: This paper addresses the challenge of topology-independent (TI) distributed
adaptive node-specific signal estimation (DANSE) in wireless acoustic sensor
networks (WASNs) where sensor nodes exchange only fused versions of their local
signals. An algorithm named TI-DANSE has previously been presented to handle
non-fully connected WASNs. However, its slow iterative convergence towards the
optimal solution limits its applicability. To address this, we propose in this
paper the TI-DANSE+ algorithm. At each iteration in TI-DANSE+, the node set to
update its local parameters is allowed to exploit each individual partial
in-network sums transmitted by its neighbors in its local estimation problem,
increasing the available degrees of freedom and accelerating convergence with
respect to TI-DANSE. Additionally, a tree-pruning strategy is proposed to
further increase convergence speed. TI-DANSE+ converges as fast as the DANSE
algorithm in fully connected WASNs while reducing transmit power usage. The
convergence properties of TI-DANSE+ are demonstrated in numerical simulations.

</details>


### [319] [Lightweight Target-Speaker-Based Overlap Transcription for Practical Streaming ASR](https://arxiv.org/pdf/2506.20288)
*Aleš Pražák, Marie Kunešová, Josef Psutka*

Main category: eess.AS

TL;DR: A lightweight, target-speaker-based extension for streaming ASR systems reduces WER in overlapping speech by 32.22% with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Overlapping speech is a major challenge for ASR in real-world applications like broadcast media.

Method: Combines speaker-independent and speaker-conditioned models with overlap detection using a binary classifier. FiLM incorporates speaker embeddings for target-speaker transcription.

Result: Reduced WER on overlapping segments from 68.0% to 35.78% with only a 44% increase in computational load.

Conclusion: The system provides an effective, scalable solution for overlap transcription in continuous ASR services.

Abstract: Overlapping speech remains a major challenge for automatic speech recognition
(ASR) in real-world applications, particularly in broadcast media with dynamic,
multi-speaker interactions. We propose a light-weight, target-speaker-based
extension to an existing streaming ASR system to enable practical transcription
of overlapping speech with minimal computational overhead. Our approach
combines a speaker-independent (SI) model for standard operation with a
speaker-conditioned (SC) model selectively applied in overlapping scenarios.
Overlap detection is achieved using a compact binary classifier trained on
frozen SI model output, offering accurate segmentation at negligible cost. The
SC model employs Feature-wise Linear Modulation (FiLM) to incorporate speaker
embeddings and is trained on synthetically mixed data to transcribe only the
target speaker. Our method supports dynamic speaker tracking and reuses
existing modules with minimal modifications. Evaluated on a challenging set of
Czech television debates with 16% overlap, the system reduced WER on
overlapping segments from 68.0% (baseline) to 35.78% while increasing total
computational load by only 44%. The proposed system offers an effective and
scalable solution for overlap transcription in continuous ASR services.

</details>


### [320] [The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models](https://arxiv.org/pdf/2506.20361)
*Yi Wang, Oli Danyi Liu, Peter Bell*

Main category: eess.AS

TL;DR: AV-HuBERT, an audio-visual model, fails to fully capture the temporal asynchronicity in multimodal speech perception, unlike human listeners.


<details>
  <summary>Details</summary>
Motivation: To explore whether self-supervised learning models like AV-HuBERT can replicate the temporal dynamics of human multimodal speech perception, particularly the asynchronicity between audio and visual cues.

Method: Compared AV-HuBERT with audio-only HuBERT using linear classifiers to track phonetic decodability over time.

Result: Phoneme information in AV-HuBERT appears only ~20 ms earlier than in HuBERT, indicating poor capture of temporal dynamics.

Conclusion: AV-HuBERT is limited in modeling multimodal speech perception due to its inability to replicate the temporal asynchronicity observed in humans.

Abstract: Human speech perception is multimodal. In natural speech, lip movements can
precede corresponding voicing by a non-negligible gap of 100-300 ms, especially
for specific consonants, affecting the time course of neural phonetic encoding
in human listeners. However, it remains unexplored whether self-supervised
learning models, which have been used to simulate audio-visual integration in
humans, can capture this asynchronicity between audio and visual cues. We
compared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using
linear classifiers to track their phonetic decodability over time. We found
that phoneme information becomes available in AV-HuBERT embeddings only about
20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and
feature concatenation process. It suggests AV-HuBERT does not adequately
capture the temporal dynamics of multimodal speech perception, limiting its
suitability for modeling the multimodal speech perception process.

</details>


### [321] [Cross-attention Inspired Selective State Space Models for Target Sound Extraction](https://arxiv.org/pdf/2409.04803)
*Donghang Wu, Yiwen Wang, Xihong Wu, Tianshu Qu*

Main category: eess.AS

TL;DR: CrossMamba, a novel method for target sound extraction, combines Mamba's efficiency with cross-attention-like dependencies, outperforming traditional Transformer-based approaches.


<details>
  <summary>Details</summary>
Motivation: Transformer models for target sound extraction are effective but computationally inefficient. Mamba offers efficiency but lacks cross-sequence dependency capture. CrossMamba bridges this gap.

Method: CrossMamba uses Mamba's hidden attention to compute dependencies between clues and audio mixtures, mimicking cross-attention by generating queries from clues and keys/values from mixtures.

Result: Experiments show CrossMamba's efficacy in target sound extraction, validating its performance against Transformer-based methods.

Conclusion: CrossMamba successfully integrates Mamba's efficiency with cross-attention-like capabilities, offering a promising alternative for target sound extraction.

Abstract: The Transformer model, particularly its cross-attention module, is widely
used for feature fusion in target sound extraction which extracts the signal of
interest based on given clues. Despite its effectiveness, this approach suffers
from low computational efficiency. Recent advancements in state space models,
notably the latest work Mamba, have shown comparable performance to
Transformer-based methods while significantly reducing computational complexity
in various tasks. However, Mamba's applicability in target sound extraction is
limited due to its inability to capture dependencies between different
sequences as the cross-attention does. In this paper, we propose CrossMamba for
target sound extraction, which leverages the hidden attention mechanism of
Mamba to compute dependencies between the given clues and the audio mixture.
The calculation of Mamba can be divided to the query, key and value. We utilize
the clue to generate the query and the audio mixture to derive the key and
value, adhering to the principle of the cross-attention mechanism in
Transformers. Experimental results from two representative target sound
extraction methods validate the efficacy of the proposed CrossMamba.

</details>


### [322] [BSM-iMagLS: ILD Informed Binaural Signal Matching for Reproduction with Head-Mounted Microphone Arrays](https://arxiv.org/pdf/2501.18227)
*Or Berebi, Zamir Ben-Hur, David Lou Alon, Boaz Rafaely*

Main category: eess.AS

TL;DR: The paper introduces BSM-iMagLS, an improved binaural signal matching method that integrates interaural level difference (ILD) into magnitude-least squares optimization, enhancing spatial audio quality for wearable devices.


<details>
  <summary>Details</summary>
Motivation: High-quality spatial audio is crucial for AR/VR immersion, but wearable microphone arrays with few microphones struggle to match the performance of larger arrays.

Method: Extends BSM with MagLS by incorporating ILD into the optimization, using a DNN-based solver for joint optimization of magnitude, ILD, and derivatives.

Result: Demonstrates reduced ILD errors while maintaining magnitude accuracy, validated through simulations and listening experiments.

Conclusion: BSM-iMagLS improves binaural reproduction for wearable devices, offering potential for better AR/VR audio experiences.

Abstract: Headphone listening in applications such as augmented and virtual reality (AR
and VR) relies on high-quality spatial audio to ensure immersion, making
accurate binaural reproduction a critical component. As capture devices,
wearable arrays with only a few microphones with irregular arrangement face
challenges in achieving a reproduction quality comparable to that of arrays
with a large number of microphones. Binaural signal matching (BSM) has recently
been presented as a signal-independent approach for generating high-quality
binaural signal using only a few microphones, which is further improved using
magnitude-least squares (MagLS) optimization at high frequencies. This paper
extends BSM with MagLS by introducing interaural level difference (ILD) into
the MagLS, integrated into BSM (BSM-iMagLS). Using a deep neural network
(DNN)-based solver, BSM-iMagLS achieves joint optimization of magnitude, ILD,
and magnitude derivatives, improving spatial fidelity. Performance is validated
through theoretical analysis, numerical simulations with diverse HRTFs and
head-mounted array geometries, and listening experiments, demonstrating a
substantial reduction in ILD errors while maintaining comparable magnitude
accuracy to state-of-the-art solutions. The results highlight the potential of
BSM-iMagLS to enhance binaural reproduction for wearable and portable devices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [323] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/pdf/2506.19975)
*Hang Zhang, Yuxi Zhang, Jiazheng Wang, Xiang Chen, Renjiu Hu, Xin Tian, Gaolei Li, Min Liu*

Main category: eess.IV

TL;DR: VoxelOpt combines learning-based and iterative methods for deformable image registration, balancing accuracy and speed using displacement entropy and adaptive message passing.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of learning-based methods (limited data, large deformations) and iterative methods (slow runtime) in deformable image registration.

Method: Uses displacement entropy for voxel-wise adaptive message passing, multi-level image pyramids, and a pretrained segmentation model for feature extraction.

Result: Outperforms iterative methods in efficiency and accuracy, matches learning-based methods with label supervision in abdominal CT registration.

Conclusion: VoxelOpt achieves a better balance between accuracy and runtime, offering a promising solution for deformable image registration.

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [324] [MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment](https://arxiv.org/pdf/2506.20200)
*Siqiao Li, Chen Hui, Wei Zhang, Rui Liang, Chenyue Song, Feng Jiang, Haiqi Zhu, Zhixuan Li, Hong Huang, Xiang Li*

Main category: eess.IV

TL;DR: A novel multi-scale feature fusion network (MS-IQA) is proposed for PET/CT image quality assessment, addressing both low-level and high-level features, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing PET/CT image quality assessment methods fail to account for both low-level distortions and high-level anatomical structures, leading to diagnostic uncertainty.

Method: MS-IQA combines multi-scale features from ResNet and Swin Transformer, using a dynamically weighted channel attention mechanism for feature fusion. A new dataset, PET-CT-IQA-DS, is also introduced.

Result: MS-IQA achieves superior performance on PET-CT-IQA-DS and LDCTIQAC2023 datasets, outperforming state-of-the-art methods in various IQA metrics.

Conclusion: The proposed MS-IQA provides an accurate and efficient solution for PET/CT image quality assessment, with publicly available code and dataset.

Abstract: Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical
role in medical imaging, combining functional and anatomical information to aid
in accurate diagnosis. However, image quality degradation due to noise,
compression and other factors could potentially lead to diagnostic uncertainty
and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT
image, both low-level features like distortions and high-level features like
organ anatomical structures affect the diagnostic value of the image. However,
existing medical image quality assessment (IQA) methods are unable to account
for both feature types simultaneously. In this work, we propose MS-IQA, a novel
multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale
features from various intermediate layers of ResNet and Swin Transformer,
enhancing its ability of perceiving both local and global information. In
addition, a multi-scale feature fusion module is also introduced to effectively
combine high-level and low-level information through a dynamically weighted
channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,
we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT
images with quality scores assigned by radiologists. Experiments on our dataset
and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed
model has achieved superior performance against existing state-of-the-art
methods in various IQA metrics. This work provides an accurate and efficient
IQA method for PET/CT. Our code and dataset are available at
https://github.com/MS-IQA/MS-IQA/.

</details>


### [325] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/pdf/2506.16116)
*Ignacio Hernández Montilla, Alfonso Medela, Paola Pasquali, Andy Aguilar, Taig Mac Carthy, Gerardo Fernández, Antonio Martorell, Enrique Onieva*

Main category: eess.IV

TL;DR: The paper proposes cross-domain training for Image Quality Assessment (IQA) in teledermatology to address poor image quality, leveraging both dermatology and non-dermatology datasets for improved performance.


<details>
  <summary>Details</summary>
Motivation: Poor image quality in teledermatology reduces the effectiveness of remote consultations, and existing dermatology IQA research lacks the scale and advancements seen in non-dermatology IQA.

Method: The authors created a new dermatology IQA database (Legit.Health-DIQA-Artificial) and combined it with non-dermatology datasets for cross-domain training of IQA models.

Result: Cross-domain training improves performance by addressing data scarcity in dermatology IQA and enhances image quality management in teledermatology.

Conclusion: Cross-domain training is effective for improving IQA in teledermatology, overcoming data limitations and enhancing remote consultation quality.

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [326] [Volumetric segmentation of muscle compartments using in vivo imaging and architectural validation in human finger flexors](https://arxiv.org/pdf/2506.20206)
*Yang Li*

Main category: eess.IV

TL;DR: A novel method for volumetric segmentation of muscle compartments using in vivo imaging was developed, validated, and applied to the flexor digitorum superficialis (FDS) for finger control.


<details>
  <summary>Details</summary>
Motivation: Segmenting muscle compartments and measuring their architecture aids in movement function assessment, musculoskeletal modeling, and electromyogram simulation.

Method: A two-step piecewise segmentation using ultrasound and MRI, followed by MRI tractography for measuring architectural properties, validated with electromyography.

Result: Fiber orientation matched cadaveric data; significant architectural differences between compartments; 95% of electromyogram centers were correctly located.

Conclusion: The validated method and derived properties can enhance biomedical applications.

Abstract: Segmenting muscle compartments and measuring their architecture can
facilitate movement function assessment, accurate musculoskeletal modeling, and
synergy-based electromyogram simulation. Here, we presented a novel method for
volumetric segmentation of muscle compartments using in vivo imaging, focusing
on the independent compartments for finger control of flexor digitorum
superficialis (FDS). Besides, we measured the architectural properties of FDS
compartments and validated the segmentation. Specifically, ultrasound and
magnetic resonance imaging (MRI) from 10 healthy subjects were used for
segmentation and measurement, while electromyography was utilized for
validation. A two-step piecewise segmentation was proposed, first annotating
compartment regions in the cross-sectional ultrasound image based on
compartment movement, and then performing minimum energy matching to register
the ultrasound data to the three-dimensional MRI coordinate system.
Additionally, the architectural properties were measured in the compartment
masks from the segmentation using MRI tractography. Anatomical correctness was
verified by comparing known anatomy with reconstructed fiber tracts and
measured properties, while segmentation accuracy was quantified as the
percentage of finger electromyogram centers falling within their corresponding
compartments. Results demonstrated agreement for the fiber orientation between
the tractography and cadaveric photographs. Significant differences in
architectural properties (P < 0.001) were observed between compartments. The
properties of FDS and its compartments were within the physiological ranges (P
< 0.01). 95% (38/40) of the electromyogram centers were located within
respective compartments, with 2 errors occurring in the index and little
fingers. The validated segmentation method and derived architectural properties
may advance biomedical applications.

</details>


### [327] [Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration](https://arxiv.org/pdf/2506.20282)
*Jiaxing Huang, Heng Guo, Le Lu, Fan Yang, Minfeng Xu, Ge Yang, Wei Luo*

Main category: eess.IV

TL;DR: A deep learning framework addresses limitations in osteoporosis diagnosis using opportunistic CT, leveraging self-supervised learning, Mixture of Experts, and multi-task learning for improved accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Current osteoporosis diagnosis methods using CT face challenges like underutilized unlabeled data, device-specific biases, and lack of clinical knowledge integration.

Method: Proposes a unified deep learning framework with self-supervised learning for unlabeled data, MoE for cross-device adaptability, and multi-task learning for diagnosis, BMD regression, and vertebra location.

Result: Validated across multiple sites, the framework outperforms existing methods in accuracy and generalizability for osteoporosis screening.

Conclusion: The proposed framework effectively addresses key limitations, offering a robust solution for opportunistic osteoporosis diagnosis using CT.

Abstract: Osteoporosis, characterized by reduced bone mineral density (BMD) and
compromised bone microstructure, increases fracture risk in aging populations.
While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD
assessment, its limited accessibility hinders diagnosis in resource-limited
regions. Opportunistic computed tomography (CT) analysis has emerged as a
promising alternative for osteoporosis diagnosis using existing imaging data.
Current approaches, however, face three limitations: (1) underutilization of
unlabeled vertebral data, (2) systematic bias from device-specific DXA
discrepancies, and (3) insufficient integration of clinical knowledge such as
spatial BMD distribution patterns. To address these, we propose a unified deep
learning framework with three innovations. First, a self-supervised learning
method using radiomic representations to leverage unlabeled CT data and
preserve bone texture. Second, a Mixture of Experts (MoE) architecture with
learned gating mechanisms to enhance cross-device adaptability. Third, a
multi-task learning framework integrating osteoporosis diagnosis, BMD
regression, and vertebra location prediction. Validated across three clinical
sites and an external hospital, our approach demonstrates superior
generalizability and accuracy over existing methods for opportunistic
osteoporosis screening and diagnosis.

</details>


### [328] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/pdf/2506.20303)
*Lee Qi Zun, Oscar Wong Jin Hao, Nor Anita Binti Che Omar, Zalifa Zakiah Binti Asnir, Mohamad Sabri bin Sinal Zainal, Goh Man Fye*

Main category: eess.IV

TL;DR: FundaQ-8 is a framework for assessing fundus image quality using eight parameters, with a ResNet18-based model trained to predict scores, improving diagnostic robustness in diabetic retinopathy grading.


<details>
  <summary>Details</summary>
Motivation: Automated fundus image quality assessment is challenging due to variability in acquisition and subjective evaluations, necessitating a standardized approach.

Method: Developed FundaQ-8 with eight parameters, trained a ResNet18-based regression model on 1800 images using transfer learning and MSE optimization.

Result: Validation on the EyeQ dataset confirmed reliability and clinical interpretability; integration improved diabetic retinopathy grading robustness.

Conclusion: FundaQ-8 enhances fundus image quality assessment and diagnostic reliability, proving valuable for real-world screening.

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [329] [Transformer Based Multi-Target Bernoulli Tracking for Maritime Radar](https://arxiv.org/pdf/2506.20319)
*Caden Sweeney, Du Yong Kim, Branko Ristic, Brian Cheung*

Main category: eess.IV

TL;DR: The paper proposes a transformer-based ML method for maritime multi-target tracking, outperforming CFAR in low SIR scenarios.


<details>
  <summary>Details</summary>
Motivation: Maritime tracking is challenging due to non-Gaussian sea clutter; ML offers potential for better detection and tracking of low SIR targets.

Method: Uses a transformer to extract measurements from range-azimuth maps, clusters and tracks with LMB filter, and designs a measurement-driven birth density.

Result: Transformer-based method outperforms CFAR in all tested scenarios.

Conclusion: The transformer approach is superior for maritime target tracking, especially in low SIR conditions.

Abstract: Multi-target tracking in the maritime domain is a challenging problem due to
the non-Gaussian and fluctuating characteristics of sea clutter. This article
investigates the use of machine learning (ML) to the detection and tracking of
low SIR targets in the maritime domain. The proposed method uses a transformer
to extract point measurements from range-azimuth maps, before clustering and
tracking using the Labelled mulit- Bernoulli (LMB) filter. A measurement driven
birth density design based on the transformer attention maps is also developed.
The error performance of the transformer based approach is presented and
compared with a constant false alarm rate (CFAR) detection technique. The LMB
filter is run in two scenarios, an ideal birth approach, and the measurement
driven birth approach. Experiments indicate that the transformer based method
has superior performance to the CFAR approach for all target scenarios
discussed

</details>


### [330] [EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis](https://arxiv.org/pdf/2506.20333)
*Jiayan Chen, Kai Li, Yulu Zhao, Jianqiang Huang, Zhan Wang*

Main category: eess.IV

TL;DR: EAGLE, a U-shaped network with PVSS encoder and HVSS decoder, achieves efficient and accurate segmentation of hepatic echinococcosis lesions, outperforming MSVM-UNet with a DSC of 89.76%.


<details>
  <summary>Details</summary>
Motivation: HE is prevalent in underdeveloped areas with limited medical resources. Existing CNN and Transformer models have limitations in global context modeling and computational efficiency.

Method: EAGLE combines PVSS encoder and HVSS decoder, using CVSSB for local-global feature fusion and HWTB for lossless downsampling.

Result: EAGLE achieves a DSC of 89.76%, surpassing MSVM-UNet by 1.61%.

Conclusion: EAGLE offers an efficient and accurate solution for HE lesion segmentation, addressing limitations of existing models.

Abstract: Hepatic echinococcosis (HE) is a widespread parasitic disease in
underdeveloped pastoral areas with limited medical resources. While CNN-based
and Transformer-based models have been widely applied to medical image
segmentation, CNNs lack global context modeling due to local receptive fields,
and Transformers, though capable of capturing long-range dependencies, are
computationally expensive. Recently, state space models (SSMs), such as Mamba,
have gained attention for their ability to model long sequences with linear
complexity. In this paper, we propose EAGLE, a U-shaped network composed of a
Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space
(HVSS) decoder that work collaboratively to achieve efficient and accurate
segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional
Vision State Space Block (CVSSB) module is designed to fuse local and global
features, while the Haar Wavelet Transformation Block (HWTB) module compresses
spatial information into the channel dimension to enable lossless downsampling.
Due to the lack of publicly available HE datasets, we collected CT slices from
260 patients at a local hospital. Experimental results show that EAGLE achieves
state-of-the-art performance with a Dice Similarity Coefficient (DSC) of
89.76%, surpassing MSVM-UNet by 1.61%.

</details>


### [331] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/pdf/2506.20407)
*Fangyijie Wang, Yuan Liang, Sourav Bhattacharjee, Abey Campbell, Kathleen M. Curran, Guénolé Silvestre*

Main category: eess.IV

TL;DR: A novel feature fusion framework using deep learning and radiomics for accurate gestational age estimation from fetal ultrasound images, achieving a mean absolute error of 8.0 days.


<details>
  <summary>Details</summary>
Motivation: Manual GA estimation is operator-dependent and time-consuming, necessitating automated methods for clinical practice.

Method: Combines deep learning for image representation and radiomics for fetal brain growth patterns, fusing both for GA estimation.

Result: Achieves a mean absolute error of 8.0 days, outperforming existing methods, with robustness across diverse populations.

Conclusion: The framework offers a reliable, automated solution for GA estimation, with potential for widespread clinical use.

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.

</details>


### [332] [Papanicolaou Stain Unmixing for RGB Image Using Weighted Nucleus Sparsity and Total Variation Regularization](https://arxiv.org/pdf/2506.20450)
*Nanxin Gong, Saori Takeyama, Masahiro Yamaguchi, Takumi Urata, Fumikazu Kimura, Keiko Ishii*

Main category: eess.IV

TL;DR: A novel method for unmixing Papanicolaou stain in RGB images is proposed, addressing challenges in quantifying dye amounts for cervical cancer screening. It outperforms multispectral imaging and aids in diagnosing precancerous lesions.


<details>
  <summary>Details</summary>
Motivation: The subjective and variable nature of visual color observation in Papanicolaou-stained samples hinders accurate quantification for diagnosis, necessitating a robust RGB-based solution.

Method: The method formulates stain unmixing as an optimization problem with nonnegativity, weighted nucleus sparsity, and total variation regularizations.

Result: Validated against multispectral imaging, the method excelled in stain quantification and achieved 98.0% accuracy in diagnosing precancerous lesions.

Conclusion: RGB-based stain unmixing shows significant potential for quantitative diagnosis in cytopathology.

Abstract: The Papanicolaou stain, consisting of eosin Y, hematoxylin, light Green SF
yellowish, orange G, and Bismarck brown Y, provides extensive color information
essential for cervical cancer screening in cytopathology. However, the visual
observation of these colors is subjective and difficult to characterize. In
digital image analysis, the RGB intensities are affected by staining and
imaging variations, hindering direct quantification of color in
Papanicolaou-stained samples. Stain unmixing is a promising alternative that
quantifies the amounts of dyes. In previous work, multispectral imaging was
utilized to estimate the dye amounts of Papanicolaou stain for quantitative
diagnosis. Still, its application to RGB images presents a challenge since the
number of dyes exceeds the three RGB channels. This paper proposes a novel
Papanicolaou stain unmixing method for RGB images that incorporates three key
assumptions: nonnegative stain abundances; a sparse spatial distribution of
hematoxylin, which binds to nuclei; and piecewise smoothness of stain
abundances. By formulating this as an optimization problem with nonnegativity,
weighted nucleus sparsity, and total variation regularizations, our method
achieved excellent performance in stain quantification when validated against
the results of multispectral imaging. We also adopted the proposed method for
discriminating lobular endocervical glandular hyperplasia (LEGH), a
precancerous lesion of gastric-type adenocarcinoma of the cervix. The resulting
quantification distinctly characterized differences between LEGH and normal
endocervical cells with stain abundance, and a classifier based on the
quantification results achieved 98.0% accuracy. This demonstrates the
significant potential of RGB-based stain unmixing for quantitative diagnosis.

</details>


### [333] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/pdf/2506.20614)
*Simon Perrin, Sébastien Levilly, Huajun Sun, Harold Mouchère, Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: The paper introduces a new feature, Weighted Mean Frequencies (WMF), to improve segmentation of 4D Flow MRI images, showing better results than PC-MRA in experiments.


<details>
  <summary>Details</summary>
Motivation: The poor resolution and noise in 4D Flow MRI images hinder accurate segmentation, especially for biomarkers like wall shear stress.

Method: The proposed WMF feature visualizes pulsatile flow regions in 3D, aiding segmentation. Experiments used optimal thresholding and deep learning methods.

Result: WMF improved IoU and Dice scores by 0.12 and 0.13, respectively, compared to PC-MRA.

Conclusion: WMF enhances segmentation accuracy and could be useful for other vascular regions like the heart or brain.

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


### [334] [From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction](https://arxiv.org/pdf/2506.16210)
*Zhenxuan Zhang, Lipei Zhang, Yanqi Cheng, Zi Wang, Fanwen Wang, Haosen Zhang, Yue Yang, Yinzhe Wu, Jiahao Huang, Angelica I Aviles-Rivero, Zhifan Gao, Guang Yang, Peter J. Lally*

Main category: eess.IV

TL;DR: PR-INR is a progressive refinement framework for motion-robust MRI, unifying motion correction, structural refinement, and volumetric synthesis to address hierarchical disruptions in slice-to-volume reconstruction.


<details>
  <summary>Details</summary>
Motivation: Challenges in MRI reconstruction include local detail loss, global structural aliasing, and volumetric anisotropy due to motion and undersampling.

Method: PR-INR combines a motion-aware diffusion module for coarse reconstruction, an implicit detail restoration module for local refinement, and a voxel continuous-aware module for inter-slice completion.

Result: PR-INR outperforms state-of-the-art methods in quantitative metrics and visual quality, showing robustness across diverse conditions.

Conclusion: PR-INR effectively addresses hierarchical disruptions in MRI reconstruction, demonstrating superior performance and generalization.

Abstract: In motion-robust magnetic resonance imaging (MRI), slice-to-volume
reconstruction is critical for recovering anatomically consistent 3D brain
volumes from 2D slices, especially under accelerated acquisitions or patient
motion. However, this task remains challenging due to hierarchical structural
disruptions. It includes local detail loss from k-space undersampling, global
structural aliasing caused by motion, and volumetric anisotropy. Therefore, we
propose a progressive refinement implicit neural representation (PR-INR)
framework. Our PR-INR unifies motion correction, structural refinement, and
volumetric synthesis within a geometry-aware coordinate space. Specifically, a
motion-aware diffusion module is first employed to generate coarse volumetric
reconstructions that suppress motion artifacts and preserve global anatomical
structures. Then, we introduce an implicit detail restoration module that
performs residual refinement by aligning spatial coordinates with visual
features. It corrects local structures and enhances boundary precision.
Further, a voxel continuous-aware representation module represents the image as
a continuous function over 3D coordinates. It enables accurate inter-slice
completion and high-frequency detail recovery. We evaluate PR-INR on five
public MRI datasets under various motion conditions (3% and 5% displacement),
undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental
results demonstrate that PR-INR outperforms state-of-the-art methods in both
quantitative reconstruction metrics and visual quality. It further shows
generalization and robustness across diverse unseen domains.

</details>


### [335] [LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images](https://arxiv.org/pdf/2506.17983)
*Chenyue Song, Chen Hui, Qing Lin, Wei Zhang, Siqiao Li, Haiqi Zhu, Zhixuan Li, Shengping Zhang, Shaohui Liu, Feng Jiang, Xiang Li*

Main category: eess.IV

TL;DR: LVPNet is a prediction-based lossless medical image compression method using global latent variables and quantization compensation to improve efficiency and avoid posterior collapse.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from posterior collapse and inefficient latent variable use due to uniform latent information distribution in sub-images.

Method: Proposes LVPNet with Global Multi-scale Sensing Module (GMSM) for spatial dependency capture and Quantization Compensation Module (QCM) to refine quantized features.

Result: Achieves superior compression efficiency on benchmarks while maintaining competitive inference speed.

Conclusion: LVPNet outperforms state-of-the-art lossless compression methods, addressing key limitations of prior approaches.

Abstract: Autoregressive Initial Bits is a framework that integrates sub-image
autoregression and latent variable modeling, demonstrating its advantages in
lossless medical image compression. However, in existing methods, the image
segmentation process leads to an even distribution of latent variable
information across each sub-image, which in turn causes posterior collapse and
inefficient utilization of latent variables. To deal with these issues, we
propose a prediction-based end-to-end lossless medical image compression method
named LVPNet, leveraging global latent variables to predict pixel values and
encoding predicted probabilities for lossless compression. Specifically, we
introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact
and informative latent representations from the entire image, effectively
capturing spatial dependencies within the latent space. Furthermore, to
mitigate the information loss introduced during quantization, we propose the
Quantization Compensation Module (QCM), which learns the distribution of
quantization errors and refines the quantized features to compensate for
quantization loss. Extensive experiments on challenging benchmarks demonstrate
that our method achieves superior compression efficiency compared to
state-of-the-art lossless image compression approaches, while maintaining
competitive inference speed. The code is at
https://github.com/scy-Jackel/LVPNet.

</details>
